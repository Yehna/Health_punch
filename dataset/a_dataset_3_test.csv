document,summary,id
"          The advancement in the field of Computer Vision ~ and Natural Language Processing ~ over the last decade, has introduced several interesting machine learning techniques. %problems more convenient.          The problems such as object detection~, segmentation~, and  image classification~ in CV, and machine translation~, question answering~, biomedical and clinical text mining~ , speech recognition~ in NLP, are being solved much more efficiently than ever before. This has facilitated the researchers to indulge into solving interdisciplinary problems that demand knowledge of both the fields.                   Visual Question Answering ~ has emerged as one such problem. In VQA, the task is poised as questions being asked with respect to an image, where the machine needs to learn and generate answers of such questions based on the learned features of the input image. In contrast to the typical CV tasks which largely focus on %have singular and          solving problems such as %restricted problems ~ and Inception-Resnet-v2~, respectively. We fuse the representations together and pass it to the specific answer prediction model at the leaf node. For the task of question classification in the root node, we propose a question segregation technique. We use Support Vector Machine ~ as the classifier with hand-engineered and word frequency-based features for QS. We use the machine learning technique for QS, as the rule-based strategy suffers from the problem of defining too many rules that may not extend to other datasets~. The following examples from RAdiology Data ~ show the difficulty of the rule-based approach in the medical domain.                                                      Careful analysis of the question reveals that the first example expects a descriptive type answer that is to list out the facts that indicate kidney hemorrhage , while the second example expects to confirm the presence/absence of spleen . The presence of such anomalies in the question acts as a hindrance in the formation of robust rules for the classification of questions into their correct type.                  We perform all our experiments in the RAD and  ImageCLEF2018 VQA-Med 2018  datasets, as they perfectly capture the problem statement that we intend to solve. Detailed discussion on the dataset can be found in Section. Experimental evaluation demonstrates promising results, showing the effectiveness of our proposed approach. %'s efficiency.          Additionally, error analysis of the system's outputs %error analysis          shows the future direction in this research area by addressing different kinds of errors.          The organization of this paper is as follows. We first discuss the related work in VQA. Then we present the details of the methodologies that we implemented to solve our specific problem. In particular, we explain our proposed HQS-VQA models in detail. Basically, we discussed the technique used for the question segregation module and the VQA components used to generate the query-specific answers. Details of experiments along with the evaluation results and necessary analysis are reported. %We then perform the experiments and show the results with qualitative and quantitative analysis.          Finally, we conclude and provide the future directions of our work.       \subsection{Motivation}          The motivation behind our work are stemmed from the following facts: %of the medical visual question answering are listed as follows:                                           \end{adjustwidth}         \end{table} \item         We identify this need, and propose a  SVM-based question segregation technique to segregate the questions. We then use this information to propose a hierarchical deep multi-modal network to generate the answers.   \end{itemize}     \subsection{Contributions}                                               The major challenges of the VQA-Med are closely related to the general VQA and QA in the medical domain and we see a lot of interesting solutions evolving over time. We present the survey with respect to the related datasets and methods in the following subsections.                 A number of research projects have been initiated for the development of benchmark datasets to promote the works in the medical domain. The Genomic corpus released as part of the TREC~ task is one of the benchmark datasets for the medical QA task. It focuses exclusively on scientific papers. However, a small number of questions in the dataset are not sufficient to evaluate the efficiency of the large-scale QA systems. This constraint led to the release of several other datasets, such as Question Answering for Machine Reading Evaluation ~ and Biomedical Semantic Indexing and Question Answering ~. The QA4MRE consists of the Biomedical Text on Alzheimer's Disease, while BioASQ gathers information from various heterogeneous sources to address real-life questions from the biomedical experts. A number of datasets, such as MRI-DIR~, fastMRI~, and a few more ~ focused on different medical tasks, are also available.         However, the images in the VQA-Med dataset have different modalities and contain radiological markings such as short information, tags, etc. It may also contain a stack of sub-images that is not the case with the existing medical datasets. In addition, general VQA datasets~ are task-specific, unlike VQA-Med, where a question can be asked about any disease from any part of the body.                MS-COCO~ is the most popular dataset that contains real world images along with their corresponding captions. Most of the VQA datasets~ were created using the image captions from MS-COCO. Visual Question Answering ~ is one of them. It is the most significant and commonly used dataset for the VQA task which was published as a part of the VQA challenge. The VQA 1.0 dataset's primary issue was its inherent bias and language priors too had a major impact on the responses to the questions that inspired the design of the second version of this dataset. VQA 2.0~ is a larger and more balanced dataset.              In this work, we use the RAD ~ and CLEF18 ~ medical VQA datasets, which are  different from the existing VQA datasets. The obvious reason is their focus on the medical domain, which offers distinguishing challenges. The images, questions, and answers must be clinically relevant in order to be a part of this dataset which is not a constraint in the VQA datasets.            The building of phrases for sentences is another distinction. Most of the Medical-VQA sentences are complex with a lot of medical terms while being simple and straightforward in the stated datasets. Another difference is the incomparable size of the dataset, medical domain data resources are limited compared to the general domain data resources which are usually huge . The number of reference answers, which is just one, is another drawback of this dataset.                        VQA tasks are primarily based on three key components: generating representations of images and questions; passing these inputs through a neural network to produce a co-dependent embedding, and then generating the correct response. Fig illustrates this framework where the key components can take a wide variety of forms.                     VQA systems differ from each other in the way they fuse multi-modal information. Although most open-ended VQA algorithms used the classification mechanism, this strategy can only produce answers seen during training. The multi-word response is generated one word at a time using LSTM~. The response generated, however, is still restricted to words seen in the course of training.          For question encoding, most methods for VQA uses a variant of Recurrent Neural Network ~. RNNs are capable of handling sequence problems, but when RNN processes lengthy sequences, context data is easily ignored. LSTM's ~ proposal mitigated the long-distance dependency issue. In addition, the researchers also discovered that the respective route from the decoder to the encoder will be reduced if the input sequence is inverted, contributing to network memory. The Bi-LSTM~ model combines the above two points and improves the results. The Gated Recurrent Unit ~ is also notable, and widely used, simplification of the LSTM. As for the image feature extraction, Convolutional Neural Network ~ are used where VGG-net~ and deep residual networks ~ are the most popular choice.               The application of attention on the image can help to improve the performance of the model by discarding the irrelevant parts of the image. So, attention mechanisms~ are usually incorporated in the models so that they may learn to attend to the important regions of the input image. However, attending image is not enough but question attention is important too as most of the words in the question may be irrelevant so simultaneous integration of both question and image attention is advised~. The fundamental concept behind all these attentive models is that for answering a specific question, certain visual areas in an image and certain words in a question provides more information than others. The Stacked Attention Network ~ and the Dynamic Memory Network ~ used image features from a CNN feature map's spatial grid. In~ an attention layer is specified by a single layer of weights using the question and image feature defined to calculate attention distribution across image locations. Using a weighted sum, this allocation is then applied to the CNN feature map to pool across spatial feature locations. It creates a global representation of the image that highlights certain spatial regions.                            VQA depends on the image and question being processed together. This was achieved earlier by using simplified methods such as concatenation or element-wise product, but these methods fail to capture the complex interactions between these two modalities. Later, multi-modal bi-linear pooling was proposed where the idea was to approximate the outer product between the two features, enabling a much deeper interaction between them. Similar concepts have been shown to work well to improve the fine-grained image recognition~. Multimodal Compact Bilinear ~ is the most significant VQA technique used in bilinear pooling. It calculates the outer product in a reduced dimensional space instead of explicit calculation to minimize the number of parameters to be learned. Then this is used to predict the relevant spatial features according to the question. The major change was the use of MCB for feature fusion instead of element-wise multiplication.          Methods for Medical-VQA must be different from general VQA as the size of the datasets is incomparable. The other challenge with Medical-VQA is to balance the number of image features  with the number of clinical features  in the deep learning network to avoid drowning out of the clinical features. Attention-based on bounding box too cannot be applied directly as medical images lack the bounding box information. For medical imaging, there are many computer-aided diagnostic systems~. Most of them, however, deal with single disease problems and focused primarily on easily identifiable areas such as the lungs and skin. In contrast to these systems, Medical-VQA deals with multiple diseases at the same time apart from handling multiple body parts which are difficult for machines to learn.       {Recently, the ImageCLEF introduced the challenge of Medical Domain Visual Question Answering, VQA-Med 2018\footnote{} . The system submitted by  achieved the best performance  in VQA-Med 2018 for medical visual question answering. They built their best performing systems using ResNet-152 for image feature extraction and Multimodal Factorized High-order   for language-vision fusion. \citet{zhou2018employing} utilized the Inception-Resnet-v2 and Bi-LSTM for image and question representation, respectively. They used the inter-attention mechanism to fuse the language and vision features. Their best performing system stood second among all the submitted systems in the challenge. The third best system submitted by \citet{abacha2018nlm} uses the pre-trained VGG-16 model for image feature extraction and LSTM for question representation. They utilized the stack attention network to fuse the question and image features. In the second edition\footnote{} of VQA-Med, \citet{yan2019zhejiang} submitted the best system for medical visual question answering. The proposed approach utilized the BERT  for question representation and pre-trained VGG-16 model for image representation. They fused the question and image features using MFB mechanism.     }       here were only limited datasets  were released as part of the VQA-Med 2018 and challenge received a total of  runs from  participating teams. In the second edition of the VQA-Med 2019\footnote{}                Inherently, questions follow a temporal sequence and naturally cluster into different types. This question-type information is very important to predict the response regardless of the image. Authors in~ use a similar approach where they first identify the question-type and use this information for answer generation. Our work, however, isolates the learning path based on question-type rather than using this knowledge as a feature. This type of information can also affect model performance as some of the VQA models perform better than others for certain types of questions. Therefore, these models can be intelligently combined to leverage their varied strengths. We propose a simple model with a question segregation module which segregates the learning path based on the question types  to reap the benefits of question-type dedicated models. We use Inception-Resnet to encode image feature and Bi-LSTM for question feature creation.     In this paper we developed a theory for how LSTM language models can capture power law temporal dependencies. We showed that this theory predicts the distribution of timescales in LSTM language models trained on both natural and formal languages. We also found that explicit multi-timescale models that are forced to follow this theoretical distribution give better performance, particularly over very long timescales. Finally, we show evidence that information dependent on different timescales is routed through specific units, demonstrating that the unit timescales are highly interpretable. This enhanced interpretability makes it possible to use LSTM activations to predict brain data, as in , and estimate processing timescales for different brain regions . These results highlight the importance of theoretical modeling and understanding of how language models capture dependencies over multiple timescales.  \subsubsection*{Acknowledgments} We would like to thank Shailee Jain for valuable feedback on the manuscript and useful discussions, and the anonymous reviewers for their insights and suggestions. Funding support for this work came from the Burroughs Wellcome Fund Career Award at the Scientific Interface , Intel Research Award, and Alfred P. Sloan Foundation Research Fellowship.      anthology    \clearpage       anthology       
","              The major challenges of the VQA-Med are closely related to the general VQA and QA in the medical domain and we see a lot of interesting solutions evolving over time. We present the survey with respect to the related datasets and methods in the following subsections.                 A number of research projects have been initiated for the development of benchmark datasets to promote the works in the medical domain. The Genomic corpus released as part of the TREC~ task is one of the benchmark datasets for the medical QA task. It focuses exclusively on scientific papers. However, a small number of questions in the dataset are not sufficient to evaluate the efficiency of the large-scale QA systems. This constraint led to the release of several other datasets, such as Question Answering for Machine Reading Evaluation ~ and Biomedical Semantic Indexing and Question Answering ~. The QA4MRE consists of the Biomedical Text on Alzheimer's Disease, while BioASQ gathers information from various heterogeneous sources to address real-life questions from the biomedical experts. A number of datasets, such as MRI-DIR~, fastMRI~, and a few more ~ focused on different medical tasks, are also available.         However, the images in the VQA-Med dataset have different modalities and contain radiological markings such as short information, tags, etc. It may also contain a stack of sub-images that is not the case with the existing medical datasets. In addition, general VQA datasets~ are task-specific, unlike VQA-Med, where a question can be asked about any disease from any part of the body.                MS-COCO~ is the most popular dataset that contains real world images along with their corresponding captions. Most of the VQA datasets~ were created using the image captions from MS-COCO. Visual Question Answering ~ is one of them. It is the most significant and commonly used dataset for the VQA task which was published as a part of the VQA challenge. The VQA 1.0 dataset's primary issue was its inherent bias and language priors too had a major impact on the responses to the questions that inspired the design of the second version of this dataset. VQA 2.0~ is a larger and more balanced dataset.              In this work, we use the RAD ~ and CLEF18 ~ medical VQA datasets, which are  different from the existing VQA datasets. The obvious reason is their focus on the medical domain, which offers distinguishing challenges. The images, questions, and answers must be clinically relevant in order to be a part of this dataset which is not a constraint in the VQA datasets.            The building of phrases for sentences is another distinction. Most of the Medical-VQA sentences are complex with a lot of medical terms while being simple and straightforward in the stated datasets. Another difference is the incomparable size of the dataset, medical domain data resources are limited compared to the general domain data resources which are usually huge . The number of reference answers, which is just one, is another drawback of this dataset.                        VQA tasks are primarily based on three key components: generating representations of images and questions; passing these inputs through a neural network to produce a co-dependent embedding, and then generating the correct response. Fig illustrates this framework where the key components can take a wide variety of forms.                     VQA systems differ from each other in the way they fuse multi-modal information. Although most open-ended VQA algorithms used the classification mechanism, this strategy can only produce answers seen during training. The multi-word response is generated one word at a time using LSTM~. The response generated, however, is still restricted to words seen in the course of training.          For question encoding, most methods for VQA uses a variant of Recurrent Neural Network ~. RNNs are capable of handling sequence problems, but when RNN processes lengthy sequences, context data is easily ignored. LSTM's ~ proposal mitigated the long-distance dependency issue. In addition, the researchers also discovered that the respective route from the decoder to the encoder will be reduced if the input sequence is inverted, contributing to network memory. The Bi-LSTM~ model combines the above two points and improves the results. The Gated Recurrent Unit ~ is also notable, and widely used, simplification of the LSTM. As for the image feature extraction, Convolutional Neural Network ~ are used where VGG-net~ and deep residual networks ~ are the most popular choice.               The application of attention on the image can help to improve the performance of the model by discarding the irrelevant parts of the image. So, attention mechanisms~ are usually incorporated in the models so that they may learn to attend to the important regions of the input image. However, attending image is not enough but question attention is important too as most of the words in the question may be irrelevant so simultaneous integration of both question and image attention is advised~. The fundamental concept behind all these attentive models is that for answering a specific question, certain visual areas in an image and certain words in a question provides more information than others. The Stacked Attention Network ~ and the Dynamic Memory Network ~ used image features from a CNN feature map's spatial grid. In~ an attention layer is specified by a single layer of weights using the question and image feature defined to calculate attention distribution across image locations. Using a weighted sum, this allocation is then applied to the CNN feature map to pool across spatial feature locations. It creates a global representation of the image that highlights certain spatial regions.                            VQA depends on the image and question being processed together. This was achieved earlier by using simplified methods such as concatenation or element-wise product, but these methods fail to capture the complex interactions between these two modalities. Later, multi-modal bi-linear pooling was proposed where the idea was to approximate the outer product between the two features, enabling a much deeper interaction between them. Similar concepts have been shown to work well to improve the fine-grained image recognition~. Multimodal Compact Bilinear ~ is the most significant VQA technique used in bilinear pooling. It calculates the outer product in a reduced dimensional space instead of explicit calculation to minimize the number of parameters to be learned. Then this is used to predict the relevant spatial features according to the question. The major change was the use of MCB for feature fusion instead of element-wise multiplication.          Methods for Medical-VQA must be different from general VQA as the size of the datasets is incomparable. The other challenge with Medical-VQA is to balance the number of image features  with the number of clinical features  in the deep learning network to avoid drowning out of the clinical features. Attention-based on bounding box too cannot be applied directly as medical images lack the bounding box information. For medical imaging, there are many computer-aided diagnostic systems~. Most of them, however, deal with single disease problems and focused primarily on easily identifiable areas such as the lungs and skin. In contrast to these systems, Medical-VQA deals with multiple diseases at the same time apart from handling multiple body parts which are difficult for machines to learn.       {Recently, the ImageCLEF introduced the challenge of Medical Domain Visual Question Answering, VQA-Med 2018\footnote{} . The system submitted by  achieved the best performance  in VQA-Med 2018 for medical visual question answering. They built their best performing systems using ResNet-152 for image feature extraction and Multimodal Factorized High-order   for language-vision fusion. \citet{zhou2018employing} utilized the Inception-Resnet-v2 and Bi-LSTM for image and question representation, respectively. They used the inter-attention mechanism to fuse the language and vision features. Their best performing system stood second among all the submitted systems in the challenge. The third best system submitted by \citet{abacha2018nlm} uses the pre-trained VGG-16 model for image feature extraction and LSTM for question representation. They utilized the stack attention network to fuse the question and image features. In the second edition\footnote{} of VQA-Med, \citet{yan2019zhejiang} submitted the best system for medical visual question answering. The proposed approach utilized the BERT  for question representation and pre-trained VGG-16 model for image representation. They fused the question and image features using MFB mechanism.     }       here were only limited datasets  were released as part of the VQA-Med 2018 and challenge received a total of  runs from  participating teams. In the second edition of the VQA-Med 2019\footnote{}                Inherently, questions follow a temporal sequence and naturally cluster into different types. This question-type information is very important to predict the response regardless of the image. Authors in~ use a similar approach where they first identify the question-type and use this information for answer generation. Our work, however, isolates the learning path based on question-type rather than using this knowledge as a feature. This type of information can also affect model performance as some of the VQA models perform better than others for certain types of questions. Therefore, these models can be intelligently combined to leverage their varied strengths. We propose a simple model with a question segregation module which segregates the learning path based on the question types  to reap the benefits of question-type dedicated models. We use Inception-Resnet to encode image feature and Bi-LSTM for question feature creation.",0
"  The rapid development of science and technology in the world has created a vast amount of data. In particular, the growth of social networks continuously creates a huge amount of comments and posts which are valuable sources to exploit and analyze in the digital era. Text classification is a prerequisite for such works as analyzing user opinion in the network environment, filtering and removing malicious information, and detecting criminal risk. With great potential, text classification has attracted much attention from experts in the natural language processing community worldwide. In English, we easily search for a range of text classification publications in many fields. However, relatively few researches have been done on Vietnamese text. Most published articles focus on binary classification. However, a large amount of information today requires analysis in many more aspects . The lack of knowledge and techniques for the Vietnamese language makes us decide to conduct this research to classify multi-class text for Vietnamese social media datasets. These datasets are provided from the VLSP share-task and publications on text classification. In particular, there are various social media textual datasets such as UIT-VSMEC for emotion recognition  and UIT-VSFC for students' feedback classification  and HSD-VLSP for hate speech detection . These are the datasets with multi-label and imbalance between the labels that have been published recently. They are suitable for the requirements that we would like to study.  The emergence of deep neural networks  and word embeddings have made text classification more efficient. Pre-trained word embeddings accurately capture semantics to assist deep learning models improve the efficiency of classification. In this study, we implement deep learning models such as CNN , LSTM  and their variants to solve classification problems. Besides, we implement the BERT model , which is a state-of-the-art model in many natural language processing tasks in recent years. BERT is trained through the transformer閳ユ獨 two-dimensional context . BERT is in contrast to previous deep learning models that looked at a text sequence from left to right or combined left-to-right and right-to-left training. To improve the word representation, we create a normalized words dictionary, which helps recognize words included in pre-trained embedding but is not represented due to misspellings.  As a result, CNN model combined with fastText's pre-trained embedding , has been remarkably performance on Vietnamese social media datasets. Our study also proves the efficiency of BERT on Vietnamese students' feedback dataset. Besides, we combine single models to increase the efficiency of the classification. As a result, our ensemble model accomplishes higher results than the single model. Compared to previous studies done on the datasets, our models achieve better results.     Nowadays, many organizations realize the importance of sentiment analysis for consumer's feedback. Through this feedback, they can evaluate the quality of their services or products and devise appropriate strategies. In order to predict the genre and rating of films through viewer ratings, Varshit battu and his collaborators  conducted research on viewer comment data collected from many websites. They implemented various classification methods on their dataset to evaluate the effectiveness of methods. As a result, the CNN model achieved high results in many different languages.  The detection of emotions in texts has become an essential task in natural language processing. Su et al. \shortcite{Su} studied the text emotional recognition problem based on semantic word vector and emotional word vector of the input text. Their proposed method used the LSTM model for emotion recognition by modeling the input text's contextual emotion evolution. They use five-fold cross-validation to evaluate the performance of their proposed method. As a result, their model achieved recognition accuracy of 70.66\  better than the CNN-based method which was implemented in the same dataset.   In addition, hate speech detection is increasingly concerned because of the explosion of social networks. There has been an amount of successful research in this field. To complete the offensive task of categorizing tweets that were announced by SemEval competition in 2019. Nikolov and Radivchev. \shortcite{nikolov-radivchev-2019-nikolov} used different approaches and models towards offensive tweet classification. Their paper presented pre-processing data methods for tweets as well as techniques for tackling imbalanced class distribution in the provided test data. Their experiments show that the BERT model proved its outstanding advantages in text classification. Not only did it outperform conventional models on the validation set, but also based on the results from the test set, it did not cause the over-fitting issue.   In Vietnam, there have been some studies efforts for text classification tasks, as well as contributing Vietnamese data for the research community. Pham et al. \shortcite{pham2017nnvlp} announced a neural network-based toolkit namely NNVLP for essential Vietnamese language processing tasks, including part-of-speech tagging, chunking, named entity recognition. This toolkit achieved state-of-the-art results on these three tasks. With the two of UIT-VSMEC  and UIT-VSFC  datasets we used in this study, their authors performed classification tasks using a variety of deep learning methods. On the  UIT-VSMEC dataset, Ho et al. \shortcite{ho2019emotion} used Random Forest, SVM, LSTM, and CNN models to classify emotions of comments. They achieved 59.74\  with seven labels and 66.48\  with six labels by using the CNN model. With the  UIT-VSFC dataset, Nguyen et al. \shortcite{8606837} gained the highest result by the BiLSTM model with 92.03\  on sentiment and 89.62\  on the topic label.    The above studies have shown the superiority of the deep learning models in text classification, which is the premise for us to apply them to the Vietnamese datasets in this study. By modifying the models and implementing new models, we aim to bring better results for these Vietnamese social media datasets.      In this work, we focus on the problem of mitigating gender bias in neural dialogue models. We propose an adversarial training framework Debiased-Chat to reduce the bias of a dialogue model during the training process. With the help of a disentanglement model, we design an adversarial learning framework that trains dialogue models to cleverly include unbiased gender features and exclude biased gender features in responses. Experiments on two human conversation datasets demonstrate that our model successfully mitigates gender bias in dialogue models and outperforms baselines by producing more engaging, diverse, and gender-specific responses. In the future, we will investigate debiasing retrieval-based dialogue models and more complicated pipeline-based dialogue systems.    File emnlp2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small}    This is not strictly necessary, and may be commented out,   but it will improve the layout of the manuscript,   and will typically save some space. \usepackage{microtype}   \aclfinalcopy   Uncomment this line for the final submission  \def\aclpaperid{***}    Enter the acl Paper ID here   \setlength\titlebox{5cm}   You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.  \newcommand\BibTeX{B\TeX}  \title{Instructions for EMNLP 2020 Proceedings}  \author{First Author \\   Affiliation / Address line 1 \\   Affiliation / Address line 2 \\   Affiliation / Address line 3 \\   \texttt{email@domain} \\\And   Second Author \\   Affiliation / Address line 1 \\   Affiliation / Address line 2 \\   Affiliation / Address line 3 \\   \texttt{email@domain} \\}  \date{}    
"," Nowadays, many organizations realize the importance of sentiment analysis for consumer's feedback. Through this feedback, they can evaluate the quality of their services or products and devise appropriate strategies. In order to predict the genre and rating of films through viewer ratings, Varshit battu and his collaborators  conducted research on viewer comment data collected from many websites. They implemented various classification methods on their dataset to evaluate the effectiveness of methods. As a result, the CNN model achieved high results in many different languages.  The detection of emotions in texts has become an essential task in natural language processing. Su et al. \shortcite{Su} studied the text emotional recognition problem based on semantic word vector and emotional word vector of the input text. Their proposed method used the LSTM model for emotion recognition by modeling the input text's contextual emotion evolution. They use five-fold cross-validation to evaluate the performance of their proposed method. As a result, their model achieved recognition accuracy of 70.66\  better than the CNN-based method which was implemented in the same dataset.   In addition, hate speech detection is increasingly concerned because of the explosion of social networks. There has been an amount of successful research in this field. To complete the offensive task of categorizing tweets that were announced by SemEval competition in 2019. Nikolov and Radivchev. \shortcite{nikolov-radivchev-2019-nikolov} used different approaches and models towards offensive tweet classification. Their paper presented pre-processing data methods for tweets as well as techniques for tackling imbalanced class distribution in the provided test data. Their experiments show that the BERT model proved its outstanding advantages in text classification. Not only did it outperform conventional models on the validation set, but also based on the results from the test set, it did not cause the over-fitting issue.   In Vietnam, there have been some studies efforts for text classification tasks, as well as contributing Vietnamese data for the research community. Pham et al. \shortcite{pham2017nnvlp} announced a neural network-based toolkit namely NNVLP for essential Vietnamese language processing tasks, including part-of-speech tagging, chunking, named entity recognition. This toolkit achieved state-of-the-art results on these three tasks. With the two of UIT-VSMEC  and UIT-VSFC  datasets we used in this study, their authors performed classification tasks using a variety of deep learning methods. On the  UIT-VSMEC dataset, Ho et al. \shortcite{ho2019emotion} used Random Forest, SVM, LSTM, and CNN models to classify emotions of comments. They achieved 59.74\  with seven labels and 66.48\  with six labels by using the CNN model. With the  UIT-VSFC dataset, Nguyen et al. \shortcite{8606837} gained the highest result by the BiLSTM model with 92.03\  on sentiment and 89.62\  on the topic label.    The above studies have shown the superiority of the deep learning models in text classification, which is the premise for us to apply them to the Vietnamese datasets in this study. By modifying the models and implementing new models, we aim to bring better results for these Vietnamese social media datasets.",1
" In recent years, Transformers  have defined state-of-the-art performance on a variety of NLP tasks, including machine translation  and language modeling. While large Transformer models can learn uniquely rich representations, they are also highly overparameterized . Several studies have therefore attempted to prune Transformers during or after training while retaining as much performance as possible . Some methods have been fairly successful, achieving compression ratios up to 10 depending on the downstream task.  Looking beyond task performance, however, it remains unclear how widely-used pruning methods affect a model's learned representations. For example, a pruned Transformer may translate text at the same BLEU, but does pruning affect the model in ways unaccounted for by this metric?  Motivated by this question, we apply recent analysis techniques to study the representations of increasingly sparse Transformers trained on MT. We perform magnitude pruning in an iterative, lottery-ticket fashion to identify Transformers at competitive sparsities with no drop in task performance . We examine the internal structures of our models as sparsity increases, specifically addressing the following questions:    Using iterative magnitude pruning , we train an En-De Transformer that retains 99.4\% of BLEU at 66.4\% sparsity. During IMP, we obtain eight Transformer models at varying levels of sparsity, along with the original unpruned model. We probe these models' representations for learned linguistic knowledge on eighteen auxiliary syntactic and semantic tasks . We then perform an unsupervised comparison of the representations and attention distributions between dense and sparse models, adopting metrics posed in \citet{wu_similarity_2020}. Our key conclusions are as follows:      Much work has attempted to reduce the parameter count of dominant Transformer-based architectures . Several papers prune BERT , either via structured removal of layers and attention heads  or unstructured pruning of individual weights . Structured head pruning has also been applied to NMT , in which BLEU is used to quantify effective compression. Recent work from \citet{yu_playing_2020} uses iterative magnitude pruning to identify lottery tickets for NMT, retaining 99\  of BLEU at 67\  sparsity for Transformer-Big. To our knowledge, they achieve the highest net pruning ratio on translation with no drop in performance.  While most such studies are primarily considered with maximizing sparsity, a subset of them address other questions. \citet{gordon2020compressing} weight prune BERT and finetune on GLUE tasks to identify how much sparsity each task can accommodate. \citet{prasanna2020bert} prune heads while finetuning BERT on GLUE tasks, and identify which heads are masked most often. They use pruning as an analysis technique to identify `good' or `bad' BERT subnetworks. Similarly, \citet{michel_are_2019} and \citet{voita_analyzing_2019} prune heads to identify which types of attention are most relevant to performance. However, these studies focus only on task performance, leaving other behavioral differences between dense and sparse models unexplored.  Relevant methods of analyzing representations in NLP include probing classifiers, which evaluate model representations on supervised tasks for morphology , syntax , and/or semantics . For Transformers, some work has directly examined the attention module . These analyses include inference of functional annotations for particular heads , or assessment of attention's ability to perform unsupervised syntax tree prediction . Recent work has also applied high-dimensional similarity analysis methods to compare learned representations within or across models . For instance, \citet{bau2018identifying} identify recurring neurons across NMT models, interpret their functions, and control their activations. A broader survey of such literature is covered by \citet{belinkov2018analysis}. We leverage some of these representation analysis methods to study and compare sparse and dense Transformers, which, to our knowledge, previous work has not addressed.    We introduce energy-based re-ranking  to improve the performance of autoregressive neural machine translation. Still, the performance gap between the output of EBR and oracle re-ranker is significant. This gap indicates that the Joint-EBM model introduced in this paper cannot perfectly distinguish the samples of target sentences for each source sentence. Exploring different energy models for Joint-EBM is the target of our future work to reduce this gap.      proreweighted NMT  which augments autoregressive NMT  with energy-based models. We introduced a training algorithm for the energy-based model for translation tasks and experimentally show the effectiveness of ERNMT in single-source and multi-source translation tasks. Specifically, we showed that multi-source ERNMT consistently improves the performanc of the BaseNMT: .   \clearpage 
","  Much work has attempted to reduce the parameter count of dominant Transformer-based architectures . Several papers prune BERT , either via structured removal of layers and attention heads  or unstructured pruning of individual weights . Structured head pruning has also been applied to NMT , in which BLEU is used to quantify effective compression. Recent work from \citet{yu_playing_2020} uses iterative magnitude pruning to identify lottery tickets for NMT, retaining 99\  of BLEU at 67\  sparsity for Transformer-Big. To our knowledge, they achieve the highest net pruning ratio on translation with no drop in performance.  While most such studies are primarily considered with maximizing sparsity, a subset of them address other questions. \citet{gordon2020compressing} weight prune BERT and finetune on GLUE tasks to identify how much sparsity each task can accommodate. \citet{prasanna2020bert} prune heads while finetuning BERT on GLUE tasks, and identify which heads are masked most often. They use pruning as an analysis technique to identify `good' or `bad' BERT subnetworks. Similarly, \citet{michel_are_2019} and \citet{voita_analyzing_2019} prune heads to identify which types of attention are most relevant to performance. However, these studies focus only on task performance, leaving other behavioral differences between dense and sparse models unexplored.  Relevant methods of analyzing representations in NLP include probing classifiers, which evaluate model representations on supervised tasks for morphology , syntax , and/or semantics . For Transformers, some work has directly examined the attention module . These analyses include inference of functional annotations for particular heads , or assessment of attention's ability to perform unsupervised syntax tree prediction . Recent work has also applied high-dimensional similarity analysis methods to compare learned representations within or across models . For instance, \citet{bau2018identifying} identify recurring neurons across NMT models, interpret their functions, and control their activations. A broader survey of such literature is covered by \citet{belinkov2018analysis}. We leverage some of these representation analysis methods to study and compare sparse and dense Transformers, which, to our knowledge, previous work has not addressed.",2
"  In rule-based machine translation , a linguist formalises linguistic knowledge into lexicons and grammar rules, which is used by the system to analyse sentences in the source language and translate them. While this approach does not require any parallel corpora for training and grants control over the translations created by the system, the process of encoding linguistic knowledge requires a great amount of expert time. Notable examples of RBMT systems are the original, rule-based Systran , Lucy LT  and the Apertium platform .  Instead, corpus-based machine translation  systems learn to translate from examples, usually in the form of sentence-level aligned corpora. On the one hand, this approach is generally computationally more expensive and offers limited control over the generated translations. Furthermore, it is not feasible for language pairs that have limited to no available parallel resources. On the other hand, if parallel resources are available, it boasts a much higher coverage of the targeted language pair. Examples of corpus-based MT paradigms are phrase-based statistical machine translation   and neural machine translation  .  In this work, we focused on leveraging RBMT knowledge for improving the performance of NMT systems in an under-resourced scenario. Namely, we used the information provided by Lucy LT, an RBMT system where the linguistic knowledge is formalised by human linguists as computational grammars, monolingual and bilingual lexicons. Grammars are collections of transformations to annotated trees. Monolingual lexicons are collections of lexical entries, where each lexical entry is a set of feature-value pairs containing morphological, syntactic and semantic information. Bilingual lexicon entries include source-target lexical correspondences and, optionally, contextual conditions and actions. The Lucy LT system divides the translation process into three sequential phases: analysis, transfer, and generation. During the analysis phase, the source sentence is morphologically analysed using a lexicon that identifies each surface form and all its plausible morphological readings. Next, the Lucy LT chart parser together with an analysis grammar consisting of augmented syntactic rules extracts the underlying syntax tree structure and annotates it. The transfer and generation grammars are then applied in succession on that tree, which undergoes multiple annotations and transformations that add information about the equivalences in the target language and adapt the source language structures to the appropriate ones in the target language. Finally, the terminal nodes of the generation tree are assembled into the translated sentence. We focused on the analysis phase, with a special interest for two of the features used: the morphological category  and the inflexion class  or classes of the lexical entries.   %%% NE/TERM Additionally, we focused on two language phenomena that are easily addressable when using RBMT but present a challenge when using corpus-based MT: named entities and terminological expressions.  A named entity  is a word or a sequence of words that unequivocally refer to a real-world object, such as proper nouns, toponyms, numbers or dates. In the context of MT, NEs present different challenges. For example, if an English sentence starts with the word Smith, we do not know a priori if we are dealing with the name of a profession, that will have to be translated, or a proper noun that may have to be left untranslated, or maybe transliterated to a different script. A second issue may arise when using subword units: while word-level models may accidentally preserve an out-of-vocabulary NE, the subword level model will generate a  translation for it. NEs are one of the main out-of-vocabulary word classes, which often cause translation problems that seriously affect the meaning of the sentence .  Similarly, a terminological expression can consist of a single word or a sequence of words that may have a different meaning depending on the context or domain they appear. Hence, the translation for the term might be different depending on the context or domain. Moreover, different contexts and domains may impose additional restrictions on the language used, such as different modes or the use of active or passive voice, and the presence of particular terminology may suggest that a translation is not acceptable even if the meaning of the source sentence is preserved. Accurate terminology translation is crucial to produce adequate translations .  In this work we extend and further analyse the injection of morphological information technique that we proposed in a previous word  and we propose an approach to NEs and terminology that does not rely on any particular technology and can be applied to any MT approach using any kind of resource to detect and translate the NEs and terminological expressions.  To test our proposed approach, we focused on English-Spanish , English-Basque, English-Irish and English-Simplified Chinese language pairs in an under-resourced scenario, using corpora with around one million parallel entries per language pair and domain. Additional test sets that contain several examples of terms, NEs and rich morphology have also been selected and used to further explore the performance of the proposed approaches. Results suggest that, while obtaining results that are not statistically significantly different than the baseline in several scenarios, the proposed approaches show appropriate behaviours such as keeping the passive voice characteristic of some domains.   %Results suggested that adding morphological information to the source language is as effective as using subword units in this particular setting.     In this section, we present the existing work on incorporating linguistic, terminological and NE information into NMT systems.     Several approaches have been proposed to incorporate linguistic knowledge into MT models in order to improve translation quality. One of the approaches is to include the knowledge as features or extra tokens for the model. For example, morphological features, part of speech  tags and syntactic dependency labels  were proven to improve translation quality when translating between English and German and English to Romanian. A different approach used interleaved CCG supertags within the target word sequence , comparing favourably to multi-task learning when translating from German and Romanian to English. Information can also be added to the target side by replacing it with a linearised and lexicalised constituency tree , which shows improved word reordering when translating from German, Czech and Russian to English both in automatic and small-scale human evaluation.  A second approach is to modify the architecture of the recurrent neural network to capture linguistic knowledge. The encoder of the NMT ensemble was replaced with a graph convolutional network, that places no rigid constraints on the structure of the sentence , which showed improvements when using syntactic dependency trees for the source language translating from English to German and Czech. An alternative approach modified the encoder to process tree-based syntactic representations of the source language, and the attention to be able to address both sentences and phrases , which improved results for English to Japanese translation.  A different approach is to use multi-task learning to improve translation quality by adding information from similar tasks, such as POS tagging. For example, two decoders were used to predict lemmas and factors  independently  when translating from English to French, which led to increased vocabulary coverage. Another approach generated both the translation of the sentence, tagged the POS of the source sentence, and recognised NEs in the source language . Different architectures that shared encoders, attention mechanisms and even decoders were used, showing improvements of all individual tasks when translating from German to English  Finally, different subword unit strategies have been tested. Generating compositional representations of the input words by using an auxiliary recurrent neural network  showed improved results compared to systems using byte-pair encoding when translating from morphologically rich languages  to English. Another alternative used morpheme-based segmentation , which compared favourably to byte-pair encoding when translating English to Hindi, English to Bengali and Bengali to Hindi; what is more, a combination of both strategies showed even better results. Other representations, such as linguistically motivated or frequency-based word segmentation methods , were also explored when using NMT, RBMT and PBSMT.  It has also been investigated whether the encoder of NMT models learns syntactic information from the source sentence  when performing three different tasks: translating from English to French and English to German, generating a linearised constitutional tree from English, and auto-encoding from English to permutated English. The authors found that different types of syntactic information are captured in different layers.    Several strategies have been tested for dealing with NE translation.  For example, identifying NEs before translating and replacing the tokens with special tags or translating the NE using an external translation model . This model showed performance improvements over the baseline model when translating sentences with person names from Simplified Chinese to English.  A different approach used alignment information to align source and target language NEs before translating . As using information from both sides can help improving NE tagging, the model showed improvements over the baseline when translating from Simplified Chinese to English.  Addressing multi-word NEs by using additional features to indicate where each NE starts and ends was also investigated , which showed improvements when translating from English to Japanese, Romanian and Bulgarian.  Similarly, terminology translation has been approached in different ways.  there are works focused on terminology translation in PBSMT.  The use of a cache-based model within PBSMT capable of combining both an static phrase table and language model with smaller, dynamic-loaded extensions  compared favourably both to the baseline model and an XML-based markup mode that enables enforcing the translation of some tokens in the sentence  when translating between English and Italian and English and German.  A mechanism named guided NMT decoding , similar in concept to the XML-based markup for PBSMT, was also tested, comparing favourably to baseline models, both in English to German translation and automatic post-editing.\footnote{ That is, translating from English that is likely to have low adequacy, usually MT hypotheses, to post-edited, more adequate English.} This model was only able to guide the decoder, but not to enforce the restrictions; hence, a multi-stack approach using finite-state acceptor to enforce the constraints was proposed , showing improved results when translating from English to German and Simplified Chinese in scenarios using gold tokens and phrases present in the reference but not produced by the baseline system, or dictionaries. This information would be present in translation memories and glossaries provided by a possible customer.  Finally, an approach that encodes the information encoded in knowledge graphs, i.e.\ terminological expressions and NEs, as embeddings that are then concatenated to the word embeddings was tested , showing improved results for English to German translation. Additionally, the performance of SMT and NMT have been explored when translating terminology without context, both using baseline and domain adapted models , showing that BPE-based NMT models benefit the most from domain adaptation.      If requested by reviewers    Adds a feature , and target-language terms next to the source-language terms in the source sentence to promote copy behaviour. Outperforms baseline for English to German with small computing cost    create pseudo-parallel in-domain corpora using both supervised  and unsupervised  embedding-based lexicon induction, showing improvements on all domains both with bidirectional LSTM and transformer models when translating English to German        Finally, data selection has been used to improve the performance of the trained models, reduce the computational cost of training, or both . Thought, to the best of our knowledge, applying data selection to the selection of targeted tests sets that frequently exhibit the studied feature to attain a higher insight of the performance of the model has not been previously explored in the literature.       This paper presented a multi-view co-teaching network that is able to learn from sparse, noisy interaction data for job-resume matching.  We considered two views for developing the matching algorithm, namely text- and relation-based models.  Furthermore, the two models were integrated into a unified approach that was able to combine their merits for this task. We designed two strategies for model integration, namely representation enhancement and data enhancement.  Representation enhancement referred to the sharing of the learned parameters or representations across the two models; data enhancement referred to the process of filtering or re-weighting training instances according to their quality, which was implemented by the co-teaching algorithms. Extensive experiments showed that the proposed approach is able to achieve a better matching performance from  sparse and noisy interaction data by comparing with several competitive baselines.     In this paper, we only focus on the macro interaction behaviors, \ie the acceptation of interview or rejection. While, it is intuitive that other kinds of micro interactive actions should be also useful to the matching task, such as click or dwell time. We will investigate into this topic and develop a more comprehensive interaction model. Besides, we will also consider applying our approach to more categories and study the domain adaptation problem across different categories. 
"," In this section, we present the existing work on incorporating linguistic, terminological and NE information into NMT systems.     Several approaches have been proposed to incorporate linguistic knowledge into MT models in order to improve translation quality. One of the approaches is to include the knowledge as features or extra tokens for the model. For example, morphological features, part of speech  tags and syntactic dependency labels  were proven to improve translation quality when translating between English and German and English to Romanian. A different approach used interleaved CCG supertags within the target word sequence , comparing favourably to multi-task learning when translating from German and Romanian to English. Information can also be added to the target side by replacing it with a linearised and lexicalised constituency tree , which shows improved word reordering when translating from German, Czech and Russian to English both in automatic and small-scale human evaluation.  A second approach is to modify the architecture of the recurrent neural network to capture linguistic knowledge. The encoder of the NMT ensemble was replaced with a graph convolutional network, that places no rigid constraints on the structure of the sentence , which showed improvements when using syntactic dependency trees for the source language translating from English to German and Czech. An alternative approach modified the encoder to process tree-based syntactic representations of the source language, and the attention to be able to address both sentences and phrases , which improved results for English to Japanese translation.  A different approach is to use multi-task learning to improve translation quality by adding information from similar tasks, such as POS tagging. For example, two decoders were used to predict lemmas and factors  independently  when translating from English to French, which led to increased vocabulary coverage. Another approach generated both the translation of the sentence, tagged the POS of the source sentence, and recognised NEs in the source language . Different architectures that shared encoders, attention mechanisms and even decoders were used, showing improvements of all individual tasks when translating from German to English  Finally, different subword unit strategies have been tested. Generating compositional representations of the input words by using an auxiliary recurrent neural network  showed improved results compared to systems using byte-pair encoding when translating from morphologically rich languages  to English. Another alternative used morpheme-based segmentation , which compared favourably to byte-pair encoding when translating English to Hindi, English to Bengali and Bengali to Hindi; what is more, a combination of both strategies showed even better results. Other representations, such as linguistically motivated or frequency-based word segmentation methods , were also explored when using NMT, RBMT and PBSMT.  It has also been investigated whether the encoder of NMT models learns syntactic information from the source sentence  when performing three different tasks: translating from English to French and English to German, generating a linearised constitutional tree from English, and auto-encoding from English to permutated English. The authors found that different types of syntactic information are captured in different layers.    Several strategies have been tested for dealing with NE translation.  For example, identifying NEs before translating and replacing the tokens with special tags or translating the NE using an external translation model . This model showed performance improvements over the baseline model when translating sentences with person names from Simplified Chinese to English.  A different approach used alignment information to align source and target language NEs before translating . As using information from both sides can help improving NE tagging, the model showed improvements over the baseline when translating from Simplified Chinese to English.  Addressing multi-word NEs by using additional features to indicate where each NE starts and ends was also investigated , which showed improvements when translating from English to Japanese, Romanian and Bulgarian.  Similarly, terminology translation has been approached in different ways.  there are works focused on terminology translation in PBSMT.  The use of a cache-based model within PBSMT capable of combining both an static phrase table and language model with smaller, dynamic-loaded extensions  compared favourably both to the baseline model and an XML-based markup mode that enables enforcing the translation of some tokens in the sentence  when translating between English and Italian and English and German.  A mechanism named guided NMT decoding , similar in concept to the XML-based markup for PBSMT, was also tested, comparing favourably to baseline models, both in English to German translation and automatic post-editing.\footnote{ That is, translating from English that is likely to have low adequacy, usually MT hypotheses, to post-edited, more adequate English.} This model was only able to guide the decoder, but not to enforce the restrictions; hence, a multi-stack approach using finite-state acceptor to enforce the constraints was proposed , showing improved results when translating from English to German and Simplified Chinese in scenarios using gold tokens and phrases present in the reference but not produced by the baseline system, or dictionaries. This information would be present in translation memories and glossaries provided by a possible customer.  Finally, an approach that encodes the information encoded in knowledge graphs, i.e.\ terminological expressions and NEs, as embeddings that are then concatenated to the word embeddings was tested , showing improved results for English to German translation. Additionally, the performance of SMT and NMT have been explored when translating terminology without context, both using baseline and domain adapted models , showing that BPE-based NMT models benefit the most from domain adaptation.      If requested by reviewers    Adds a feature , and target-language terms next to the source-language terms in the source sentence to promote copy behaviour. Outperforms baseline for English to German with small computing cost    create pseudo-parallel in-domain corpora using both supervised  and unsupervised  embedding-based lexicon induction, showing improvements on all domains both with bidirectional LSTM and transformer models when translating English to German        Finally, data selection has been used to improve the performance of the trained models, reduce the computational cost of training, or both . Thought, to the best of our knowledge, applying data selection to the selection of targeted tests sets that frequently exhibit the studied feature to attain a higher insight of the performance of the model has not been previously explored in the literature.",3
" % no  Spoken Language Understanding  technology plays a crucial part in goal-oriented dialogue systems. It typically involves intent detection  and slot filling  tasks. As the names imply, intent detection aims to identify users閳 intents, while slot filling focuses on capturing semantic constituents from user utterances  . As shown in Fig., given a user query 閳ユイook a restaurant on next fall for 5閳ユ, which is sampled from the SNIPS dataset , intent BookRestaurant is assigned to the whole sentence, and each token in the sentence corresponds to one specific slot type. Due to the process interdependence between SLU and subsequent dialogue components, such as the dialogue manager and the natural language generator, performance on these two tasks, i.e., ID and SF, determines the upper limit for the utility of such dialogue system .      Intuitively, intent detection and slot filling are associated with each other  , which can be observed in Fig.. For instance, when the intent of an utterance is 	extit{PlayMusic, the slots of the utterance are more likely to be artist rather than cuisine, and vice versa  % .  . As the accumulation of annotated queries, the co-occurrence characteristic between slot tags and intent labels can become more prominent and perceptible, providing hints about the mutual dependence of ID and SF. Hence, it is promising to achieve a complementary effect by modeling the two tasks in a joint fashion and sharing knowledge between them. %  proposed using CNN based triangular CRF for joint intent detection and slot filling.  % Some works  simply rely on the shared parameters to model this co-occurrence characteristic in an implicit way.  Some works  proposed to model intent-slot relation by sharing parameters, outperforming previous separated models by a large margin. % With the rise of RNN-based methods and attention mechanisms, the practice of working the relationship between intents and slots into joint models is likely to get more sophisticated. More recently, gate mechanism and attention mechanism were also introduced to the RNN-based models   , which provides a new perspective for joint ID and SF modeling.  %  proposed using a slot-gated mechanism to enhance slot filling performance with intent information. To take one step further,  proposed a Stack-Propagation Framework to incorporate token-level intent information to better guide the slot prediction process. This stacking neural network model could provide better interpretability than the slot-gated mechanism.  However, these methods still suffer from various limitations.  For one thing, local context information is not fully exploited in their models, ignoring the intuition that local context is a useful architectural inductive prior for SF. For another thing, most methods fail to take full advantage of the supervised signals due to their implicit or unidirectional modeling style of the intent-slot relations.  Those limitations will hinder the further improvement of SLU systems, especially the overall accuracy, which highly depends on the joint performance of ID and SF.    In this work, we propose a novel Parallel Interactive Network  to address above issues. For the first issue, a Gaussian self-attentive encoder is introduced to better capture local structure and contextual information at each token, which incorporates valuable inductive prior knowledge for SF. For the second issue, we design a Intent2Slot module and a Slot2Intent module  to model the bidirectional information flow between SF and ID. Specifically, inspired by the Dual Process Theory  in neurocognitive science, we divide the information processing in these modules into two stages: the implicit interaction stage and the explicit interaction stage. These two stages correspond to two different processing styles in which the human brain operates: implicit , unconscious learning and explicit , conscious learning. In the implicit interaction stage, the relationships between intents and slots are implicitly captured in the parameters of the shared encoder, which is then utilized by the intuitive decoders to obtain token-level intent distribution and slot label distribution. In the explicit interaction stage, those distribution information obtained in former stage is explicitly utilized by rational decoders to reduce the solution space. Finally, a cooperation mechanism, which comprehensively considers information from above two stages, is performed to reduce the prediction bias and thereby improve the precision and accuracy of model predictions.  To verify the effectiveness of our proposed method, we conduct experiments on two real-world datasets, i.e., ATIS  and SNIPS , which are popularly used as benchmarks in recent works. Empirical results show that our method achieves competent performance on intent error rate, slot F1-score, and sentence-level semantic frame accuracy compared with other baselines. % 閹存垿妫潻妯瑰▏閻⑩暈ert娴ｆ粈璐熸０鍕唲缂佸啯膩閸ㄥ绱濇潻娑楃濮濄儲褰侀崡鍥︾啊濡崇烽惃鍕冮悳鑸 In addition, Bidirectional Encoder Representation from Transformer   is explored to further improve the performance of our model.  In summary, the key contributions are as follows:       In recent years, most researchers treat intent detection as a Semantic Utterance Classification  problem . The early traditional method is to use rule-based templates  to match the most appropriate intent, which is extremely inflexible and effort-consuming if intent categories changed.  Some statistical-based methods,  such as Naive Bayes  , Adaboost , Support Vector Machine   and logistic regression , have also been explored to improve the performance of intent detection. But these approaches still have difficulty in understanding the deep semantics of user utterances due to the ambiguity and irregularity of user expressions. With the rise of deep learning techniques, many neural network based models are proposed to better solve this classification problem.  attempted to use Deep belief networks  in call routing classification.  proposed using CNN to extract features of sentences and has achieved excellent results.  More recently,  adopted GRU and LSTM to capture the long-range dependency between words, which showed excellent performance on the intent detection problem. Besides,  demonstrated that capsule network could also be applied to this task.    Typically, slot filling is often regarded as a sequence labelling task. Compared with sentence-level intent detection, word-level slot filling heavily relies on fine-grained semantic features. Traditional rule-based approaches directly extract semantic concepts from the user utterance based on hand-crafted rules . They are in the throes of poor generalization and bad transferability. Another line of works centered on feature-based supervised learning algorithms , such as Maximum Entropy Models , Hidden Markov Models  , and Conditional Random Fields  . Though achieving better performance than rule-based models, they still suffered from designing elaborate features, which is obviously time-consuming and tedious. Recently, DL-based models for SF have taken a leading role and achieved state-of-the-art results. Bi-directional and hybrid RNN , deep LSTM , RNN-EM , attention-based encoder-decoder , CNN , and joint pointer and attention  are some typical works of this research direction.    Intuitively, intents and slots represent the semantics of user actions from different granularity perspectives. Hence, there have been some works attempting to jointly model intent detection and slot filling.  Hence, how to better model the interactions between slots and intents is the crux of the matter.  proposed using an attention-based neural network to jointly model the two tasks. Without directly exchanging information between intents and slots, it simply relied on a joint loss function to ``implicitly'' consider both cues.   introduced a slot-gated mechanism to improve slot filling by conditioning on the learned intent results.  proposed using intent-augmented embedding as a special gate function to guide the process of slot filling.  proposed a Stack-Propagation framework to explicitly incorporate token-level intent information to slot filling, which can further reduce the error propagation. However, these unidirectional interaction approaches ignored the fact that slot filling results are also instructive to the intent detection task. Thus it is necessary to consider the cross-feeding interactions between intents and slots for these two tasks. For instance,  proposed a bi-directional interrelated model to achieve the effects of mutual enhancement.  adopted a hierarchical capsule network to leverage the hierarchical relationships among words, slots, and intents in an utterance.  In contrast to these lines of work, we propose to explicitly model the mutual support in a parallel manner, which is more straightforward and does better in capturing the complicated interdependence among slots and intents. Moreover, empirical results on two benchmark datasets demonstrate the effectiveness of our method, which can further improve the performance of SLU systems.      In this work, we explored the use of rule-based machine translation  knowledge to improve the performance of neural machine translation  models in an under-resourced scenario, showing that the models had limited ability to learn from the external information.   adding morphological information to the source language is as effective as using subword units in this particular setting.   We also found that RBMT translations were often adequate but both BLEU and TER poorly reflected this, often scoring worse than incorrect NMT-generated translations.  We also tested different approaches to inject named entities  and terminological expressions contained in the RBMT model to NMT. The approaches treat the NMT model as a black box, that is, in such a way that there is no need to know or modify the inner workings of the system, thus being applicable to any model, implementation and architecture. Only the approaches injecting terminology in word-based models improved the baseline, albeit not statistically significantly. In some scenarios, the use of some approaches led to translations that, while not having a significantly different automatic evaluation score, appear to be closer to the style of the targeted text; namely, in the case of terminology translation, some strategies managed to retain the passive voice of the corpus.    One of the paths of our future work will further focus on a more sophisticated extraction of RBMT knowledge. Namely, we plan to use the transfer rules to improve the performance of the NMT model. One of the paths of our future work will further focus on the extraction of RBMT knowledge and the inclusion of transfer rules to improve the performance of the NMT model. The model that was trained following the structure with the parse tree was not able to properly deal with the information, and generally performed worse than the rest; integrating this information differently might produce better results.   Use a second encoder with the RBMT output as input.  A second path is using approaches that modify the architecture of the neural network. For example, using multiple encoders to take both the source sentence and the output of the RBMT system. This approach has been used to improve the performance of NMT. As previously mentioned, corpus-based MT gives limited control over the output to the user, especially when dealing with homographs and terminology; instead, RBMT gives total control. Combining the source sentence with the RBMT output that contains the user-selected translations might lead to improvements in domain-specific or low resource scenarios.    A second improvement path would be using multiple encoders. This approach has been used to improve the performance NMT, but, in our scenario, one of the inputs would be the output of the RBMT system. As previously mentioned, corpus-based machine translation gives limited control over the output to the user, specially when dealing with homographs and terminology; instead, RBMT gives total control. Combining the source sentence with the RBMT output that contains the user-selected translations might lead to improvements in domain-specific or low resource scenarios.   Use of other sources of information . Finally, we also plan to leverage information contained in other freely available RBMT systems, such as Apertium, that contains features similar to the ones used in this work.   While Apertium is a shallow-transfer system,>Apertium is now deep transfer  meaning that there is less syntactic information, features similar to the ones used in this work are available in Apertium.                     
","    In recent years, most researchers treat intent detection as a Semantic Utterance Classification  problem . The early traditional method is to use rule-based templates  to match the most appropriate intent, which is extremely inflexible and effort-consuming if intent categories changed.  Some statistical-based methods,  such as Naive Bayes  , Adaboost , Support Vector Machine   and logistic regression , have also been explored to improve the performance of intent detection. But these approaches still have difficulty in understanding the deep semantics of user utterances due to the ambiguity and irregularity of user expressions. With the rise of deep learning techniques, many neural network based models are proposed to better solve this classification problem.  attempted to use Deep belief networks  in call routing classification.  proposed using CNN to extract features of sentences and has achieved excellent results.  More recently,  adopted GRU and LSTM to capture the long-range dependency between words, which showed excellent performance on the intent detection problem. Besides,  demonstrated that capsule network could also be applied to this task.    Typically, slot filling is often regarded as a sequence labelling task. Compared with sentence-level intent detection, word-level slot filling heavily relies on fine-grained semantic features. Traditional rule-based approaches directly extract semantic concepts from the user utterance based on hand-crafted rules . They are in the throes of poor generalization and bad transferability. Another line of works centered on feature-based supervised learning algorithms , such as Maximum Entropy Models , Hidden Markov Models  , and Conditional Random Fields  . Though achieving better performance than rule-based models, they still suffered from designing elaborate features, which is obviously time-consuming and tedious. Recently, DL-based models for SF have taken a leading role and achieved state-of-the-art results. Bi-directional and hybrid RNN , deep LSTM , RNN-EM , attention-based encoder-decoder , CNN , and joint pointer and attention  are some typical works of this research direction.    Intuitively, intents and slots represent the semantics of user actions from different granularity perspectives. Hence, there have been some works attempting to jointly model intent detection and slot filling.  Hence, how to better model the interactions between slots and intents is the crux of the matter.  proposed using an attention-based neural network to jointly model the two tasks. Without directly exchanging information between intents and slots, it simply relied on a joint loss function to ``implicitly'' consider both cues.   introduced a slot-gated mechanism to improve slot filling by conditioning on the learned intent results.  proposed using intent-augmented embedding as a special gate function to guide the process of slot filling.  proposed a Stack-Propagation framework to explicitly incorporate token-level intent information to slot filling, which can further reduce the error propagation. However, these unidirectional interaction approaches ignored the fact that slot filling results are also instructive to the intent detection task. Thus it is necessary to consider the cross-feeding interactions between intents and slots for these two tasks. For instance,  proposed a bi-directional interrelated model to achieve the effects of mutual enhancement.  adopted a hierarchical capsule network to leverage the hierarchical relationships among words, slots, and intents in an utterance.  In contrast to these lines of work, we propose to explicitly model the mutual support in a parallel manner, which is more straightforward and does better in capturing the complicated interdependence among slots and intents. Moreover, empirical results on two benchmark datasets demonstrate the effectiveness of our method, which can further improve the performance of SLU systems.",4
" Task-oriented dialogue systems are designed to help users achieve predefined goals, such as booking restaurants or movie recommendations via natural language interactions. These systems are deeply connected with external Knowledge Bases  since the system responses are guided by the output from the KB and the dialogue history.   The current state-of-the-arts are end-to-end pipelined systems that rely on Dialogue State Tracking  and Speech Act  annotations. Aside from the annotation cost, which is knowingly high, these pipelined systems must predict a valid DST for querying the KB, execute the query, generate a response template, and finally fulfill it with the retrieved information. The resulting systems are usually overly complicated, and they require multiple steps, including a direct interaction with the KB.   On the other end of the spectrum, there are end-to-end trainable models that use both the KB and the dialogue history as input, and they directly generate system responses. Most of the implementations use either the Gold KB as input or an intermediate API call to retrieve part of the KB . These systems require at least the DST annotation for generating the API calls or to select the gold KB. Moreover, even with the most advanced transformer architecture, end-to-end models struggle when the input becomes too large. For example, in MWOZ, there are 22K entities just for one of the domains. Interested readers can refer to Appendix C for an overview of different task-oriented methodologies.  On the other hand, \citet{petroni2019language} discovered a simple yet effective way to query factual knowledge from BERT. Later on, \citet{roberts2020much} fine-tuned a pre-trained language model, T5, on just question-answers pairs, without letting the model access any external context or knowledge. These results suggest that the actual knowledge is stored in the model parameters. However, in task-oriented dialogue systems, KB entities do not appear in news articles or Wikipedia, e.g., hotel addresses or postcodes, and thus the aforementioned methods cannot be straightforwardly applied, especially when the KB dynamically changes .  In this paper, we propose a method to store the KB directly into the model parameters using a novel Knowledge Embedded  approach. The resulting model does not use any DST or template responses, nor a KB as input at the inference time, and it can be used in dynamically changing KBs via fine-tuning. The KE approach consists of a newly defined user goal query that generates equivalents KE dialogues from the KB  using minimal annotation effort. Figure shows a high level overview of our approach. To verify the effectiveness of our proposed methodology, we extensively experiment, using both automatic and human metrics, in five task-oriented datasets with small, medium, and large KBs. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all five datasets.  % Additionally, we show that end-to-end models can perform as well as pipelined modularized systems that uses both DST and S-ACT.     \paragraph{Dialogue Systems} are categorized into chit-chat and task-oriented; in this paper we focus on the latter. Task-oriented dialogue systems are further classified into: modularized, retrieval end-to-end and hybrid.  To the best of our knowledge, these methods use either DST/S-ACT annotations, template responses, or all/partial KB as the input to the model, where instead we only use the dialogue history.   Recently, several task-oriented dialogue models are introduced to tackle the resource scarcity challenges in target domains and target languages, and large pre-trained language models are shown to possess the capability to quickly adapt to task-oriented dialogue tasks by using only a few data samples.  \paragraph{Data Augmentation} is a widely used technique to improve both robustness and performance. Task-oriented dialogue systems have been explored to improve DST, Natural Language Understanding , intent classification and hybrid end-to-end systems. These data augmentation methods aim to improve the final performance of the given task, e.g., zero-shot performance, template response, etc., where instead, our proposed approach aims to store the KB into the model parameters.   \paragraph{Agenda-Based User Simulation} builds an interactive system that models the user turns rather than the system. User simulators are designed to cover all possible user queries while keeping a diverse and fluent user interaction. This enables models to learn a better dialogue policy via interaction, and it is especially useful in scenarios in where few or no data is available. In our work, instead, we use all the possible user goal queries to generate dialogues directly, instead of creating a reinforcement learning loop to train the model.   \paragraph{Language Models as Knowledge Bases} has been used for encoding common sense knowledge into transformers.  improved story generation by training a Language Model with knowledge triples converted into sentences using predefined templates. Differently, we extract templates from real data, and we aim to store the KB into the models parameters to be able to extract knowledge directly, instead of improving common sense generation. Moreover, several studies tried to extract or use large pre-trained models, e.g. BERT, as knowledge bases.     In this paper, we propose a novel Parallel Interactive Network  for jointly modeling intent detection and slot filling. In our model, a Gaussian self-attentive encoder is first introduced to better capture the local context information of utterance, then two modules are introduced to model the mutual guidance between ID and SF. Finally, a cooperation mechanism is proposed to further improve the performance and robustness of our proposed PIN. Experiment results on two benchmark datasets show that the proposed PIN achieves competent performance compared with other baselines, demonstrating the effectiveness of our proposed PIN. In addition, by incorporating the pre-trained language model BERT, our method achieves the state-of-the-art among all comparison approaches.  For our future work, we will extend our model to handle cold start problem where few data samples are provided for training process.       conference papers do not normally have an appendix     use section* for acknowledgment 
"," \paragraph{Dialogue Systems} are categorized into chit-chat and task-oriented; in this paper we focus on the latter. Task-oriented dialogue systems are further classified into: modularized, retrieval end-to-end and hybrid.  To the best of our knowledge, these methods use either DST/S-ACT annotations, template responses, or all/partial KB as the input to the model, where instead we only use the dialogue history.   Recently, several task-oriented dialogue models are introduced to tackle the resource scarcity challenges in target domains and target languages, and large pre-trained language models are shown to possess the capability to quickly adapt to task-oriented dialogue tasks by using only a few data samples.  \paragraph{Data Augmentation} is a widely used technique to improve both robustness and performance. Task-oriented dialogue systems have been explored to improve DST, Natural Language Understanding , intent classification and hybrid end-to-end systems. These data augmentation methods aim to improve the final performance of the given task, e.g., zero-shot performance, template response, etc., where instead, our proposed approach aims to store the KB into the model parameters.   \paragraph{Agenda-Based User Simulation} builds an interactive system that models the user turns rather than the system. User simulators are designed to cover all possible user queries while keeping a diverse and fluent user interaction. This enables models to learn a better dialogue policy via interaction, and it is especially useful in scenarios in where few or no data is available. In our work, instead, we use all the possible user goal queries to generate dialogues directly, instead of creating a reinforcement learning loop to train the model.   \paragraph{Language Models as Knowledge Bases} has been used for encoding common sense knowledge into transformers.  improved story generation by training a Language Model with knowledge triples converted into sentences using predefined templates. Differently, we extract templates from real data, and we aim to store the KB into the models parameters to be able to extract knowledge directly, instead of improving common sense generation. Moreover, several studies tried to extract or use large pre-trained models, e.g. BERT, as knowledge bases.",5
" Topic models, such as Latent Dirichlet Allocation  , aim to discover underlying topics and semantic structures from text collections. Due to its interpretability and effectiveness, LDA has been extended to many Natural Language Processing  tasks . Most of these models employ mean-field variational inference or collapsed Gibbs sampling  for model inference as a result of their intractable posteriors. However, such inference algorithms are model specific and require dedicated derivations.  To address such limitation, neural topic models with black-box inference have been explored, with more flexible training schemes. Inspired by variational autoencoder  , \citet{miao2016nvdm} proposed Neural Variational Document Model which interprets the latent code in VAE as topics. Following this way, \citet{srivastava2017prodlda} adopted the logistic normal prior rather than Gaussian to mimic the simplex properties of topic distribution. Logistic normal is a Laplace approximation to the Dirichlet distribution . However, logistic normal can not exhibit multiple peaks at the vertices of the simplex as the Dirichlet distribution. Therefore, it is less capable of capturing the multi-modality which is crucial for topic modeling .  To overcome such limitation, \citet{wang2019atm} proposed Adversarial-neural Topic Model , a topic model based on Generative Adversarial Networks   and sampling topics directly from the Dirichlet distribution to impose a Dirichlet prior. ATM employs a generator transforming randomly sampled topic distributions to word distributions, and an adversarially trained discriminator estimating the probability that a word distribution came from the training data rather than the generator. Although ATM was shown to be effective in discovering coherent topics, it can not be used to induce the topic distribution given a document due to the absence of a topic inference module. Such limitation hinders its application to downstream tasks, such as text classification. Moreover, ATM fails to deal with document labels which can help extract more coherent topics. For example, a document labeled as `sports' more likely belongs to topics such as `basketball' or `football' rather than `economics' or `politics'.  To address such limitations of ATM, we propose a novel neural topic modeling approach, named Topic Modeling with Cycle-consistent Adversarial Training . In ToMCAT, topic modeling is cast into the transformation between topic distributions and word distributions. Specifically, the transformation from topic distributions to word distributions is used to interpret topics, and the reverse transformation is used to infer underlying topics for a given document. Under such formulation, ToMCAT employs a generator to transform topic distributions randomly sampled from the Dirichlet prior into the corresponding word distributions, and an encoder to reversely transform documents represented as word distributions into their topic distributions. To encourage the generator/encoder to produce more realistic target samples, discriminators for word/topic distributions are introduced to enable adversarial training. Additional cycle-consistency constraints are utilized to align the learning of the encoder and the generator to prevent them from contradicting each other. Furthermore, for documents with labels, we propose sToMCAT that introduces an extra classifier to regularize the topic modeling process.  The main contributions of the paper are:     Our work is related to neural topic modeling and unsupervised style transfer.   Recent advances on deep generative models, such as VAEs  and GANs , attract much research interest in the NLP community.  Based on VAE, Neural Variational Document Model   encodes documents with variational posteriors in the latent topic space. NVDM employs Gaussian as the prior distribution of latent topics. Instead, \citet{srivastava2017prodlda} proposed that Dirichlet distribution is a more appropriate prior for multinomial topic distributions, and constructed a Laplace approximation of Dirichlet to enable reparameterisation . Furthermore, the word-level mixture is replaced with a weighted product of experts . Later, a non-parametric neural topic model utilizing stick-breaking construction was presented in . There are some attempts in incorporating supervised information into neural topic modeling. For example, \citet{card-etal-2018-neural} extended the Sparse Additive Generative Model  in the neural framework and incorporated document metadata such as document labels into the modeling process.  Apart from VAE-based approaches, Adversarial-neural Topic Model  ) was proposed to model topics with GANs. The generator of ATM projects randomly sampled topic distributions to word distributions, and is adversarially trained with a discriminator that tries to distinguish real and generated word distributions. Moreover, \citet{wang-etal-2019-open} extended ATM for open-domain event extraction by representing an event as a combination of an entity distribution, a location distribution, a keyword distribution and a date distribution. Such joint distributions are adversarially learned in a similar manner as ATM. The proposed ToMCAT is partly inspired by ATM but differs in its capability of inferring document-specific topic distributions and incorporating supervision information. BAT  is an extension to ATM that employs bidirectional adversarial training  for document-specific topic distribution inference. Although BAT similarly utilizes an adversarial training objective to guide the learning of topic distribution, there are some major differences. Apart from different adversarial losses, ToMCAT also incorporates two cycle-consistency constraints which encourage the model to generate informative representations and are shown to be crucial for generating coherent topics as in our experiments.   Style transfer, aiming at transforming representations from one style to another, has been found many interesting applications, such as image and text style transfer. However, training data paired between different styles are not available for many tasks. To solve this problem, \citet{zhu2017cycle} imposed cycle-consistency constraints to align mappings between two styles and proposed CycleGAN for unsupervised image style translation. Similarly, DiscoGAN  was proposed to discover the relations between different image styles and transformed images from one style to another without paired data. In the NLP field, \citet{lee2018scalable} developed a CycleGAN-based approach to transfer the sentiment style  of the text.  Inspired by CycleGAN, Our work views topic modeling as unsupervised distribution transfer and follows the framework of CycleGAN.    In this paper, we propose a novel approach for making use of an early fusion classification model to improve late fusion retrieval models. The early fusion model is used to supervised data mining that augments the training data for the later model. The proposed approach mines 53\ ~ and 12\ ~ more examples for MultiRQA-NQ and MultiRQA-SQuAD, respectively. The resulting retrieval models improve +8.6\  and +1.0\  on P@1 on NQ and SQuAD, respectively. The current pipeline assumes there exists annotated in-domain question answer pairs to train the cross-attention model. With a strong general purpose cross-attention model, our supervised data mining method could be modified to train in-domain retrieval models without gold question answer pairs.  We leave this direction to the future work.  
"," Our work is related to neural topic modeling and unsupervised style transfer.   Recent advances on deep generative models, such as VAEs  and GANs , attract much research interest in the NLP community.  Based on VAE, Neural Variational Document Model   encodes documents with variational posteriors in the latent topic space. NVDM employs Gaussian as the prior distribution of latent topics. Instead, \citet{srivastava2017prodlda} proposed that Dirichlet distribution is a more appropriate prior for multinomial topic distributions, and constructed a Laplace approximation of Dirichlet to enable reparameterisation . Furthermore, the word-level mixture is replaced with a weighted product of experts . Later, a non-parametric neural topic model utilizing stick-breaking construction was presented in . There are some attempts in incorporating supervised information into neural topic modeling. For example, \citet{card-etal-2018-neural} extended the Sparse Additive Generative Model  in the neural framework and incorporated document metadata such as document labels into the modeling process.  Apart from VAE-based approaches, Adversarial-neural Topic Model  ) was proposed to model topics with GANs. The generator of ATM projects randomly sampled topic distributions to word distributions, and is adversarially trained with a discriminator that tries to distinguish real and generated word distributions. Moreover, \citet{wang-etal-2019-open} extended ATM for open-domain event extraction by representing an event as a combination of an entity distribution, a location distribution, a keyword distribution and a date distribution. Such joint distributions are adversarially learned in a similar manner as ATM. The proposed ToMCAT is partly inspired by ATM but differs in its capability of inferring document-specific topic distributions and incorporating supervision information. BAT  is an extension to ATM that employs bidirectional adversarial training  for document-specific topic distribution inference. Although BAT similarly utilizes an adversarial training objective to guide the learning of topic distribution, there are some major differences. Apart from different adversarial losses, ToMCAT also incorporates two cycle-consistency constraints which encourage the model to generate informative representations and are shown to be crucial for generating coherent topics as in our experiments.   Style transfer, aiming at transforming representations from one style to another, has been found many interesting applications, such as image and text style transfer. However, training data paired between different styles are not available for many tasks. To solve this problem, \citet{zhu2017cycle} imposed cycle-consistency constraints to align mappings between two styles and proposed CycleGAN for unsupervised image style translation. Similarly, DiscoGAN  was proposed to discover the relations between different image styles and transformed images from one style to another without paired data. In the NLP field, \citet{lee2018scalable} developed a CycleGAN-based approach to transfer the sentiment style  of the text.  Inspired by CycleGAN, Our work views topic modeling as unsupervised distribution transfer and follows the framework of CycleGAN.",6
" % {\color{red}jiaqi: outlines}  % \ys{Need to put more Covid information here. Logic is that we need to do so for covid 19 rather than we have the information and then we can do so. }   In this work, we report the system architecture and results of the team TEST\_POSITIVE in the competition of W-NUT 2020 sharred Task-3: extracting COVID-19 event from Twitter.   Since February 2020, the pandemic COVID-19 has been spreading all over the world, posing a significant threat to mankind in every aspect. The information sharing about a pandemic has been critical in stopping virus spreading. With the recent advance of social networks and machine learning, we are able to automatically detect potential events of COVID cases, and identify key information to prepare ahead.  % \kenneth{I would probably make it explicit that ``this paper reports the system architecture and results of the team ABC in XYZ competition at IMWUT 2020''.}  % Users share a wide range of information on social networks. Large platforms, such as Twitter and Facebook, provide sufficient user-generated content for natural language processing applications. For example, massive tweet data posted by users have nourished a variety of applications, e.g. sentiment analysis ~, disaster monitoring ~, event extraction ~ and etc.  We are interested in COVID-19 related event extraction from tweets. With the prevalence of coronavirus, Twitter has been a valuable source of news and information. Twitter users share COVID-19 related topics about personal narratives and news on social media . The information could be helpful for doctors, epidemiologists, and policymakers in controlling the pandemic. However, manual extracting useful information from tremendous amount of tweets is impossible. Hence, we aim to develop a system to automatically extract structured knowledge from Twitter.  % \ys{According to Chieh-Yang, using global model solved the issue of limited annotation, while using the various types of tasks to use all event data to do the training.} Extracting COVID-19 related events from Twitter is non-trivial due to the following challenges: \\  How to deal with limited annotations in heterogeneous events and subtasks?. The creation of the annotated data relies completely on human labors, and thus only a limited amount of data can be obtained in each event categories. There are a variety types of events and subtasks. % Due to the sparsity of the positive samples,  % \cc{why it is due to the sparsity of positve samples} the annotation cannot scale up properly and thus only a limited amount of data can be obtained. % The training dataset relies on manual annotation. Hence, we can only obtain a limited number of training data.  Many existing works solve these low resource problem by different approaches, inlcuding crowdsourcing , unsupervised training , or multi-task learning . Here we adopt multi-task training paradigm to benefit from the inter-event and intra-event  information sharing. In this way, \ours learns a shared embedding network globally from all events data. In this way, we implicitly augment the dataset by global training and fine-tuning the language model.    % because our events and subtasks share similarities % to make use of the fundamental relations across different subtasks and events in learning a global embedding network. %  Heterogeneous types of events and subtasks.  How to make type-aware predictions? Existing work  did not encode the information of different subtask types into the model, while it could be useful in suggesting the candidate slot entity type. In order to make type-aware predictions, we propose a NER-based post-processing procedure in the end of \ours pipeline. We use NER to automatically tag the candidate slots and remove the candidate whose entity type does not match the corresponding subtask type. For example, as shown in Figure, in subtask ``Who'', ``my wife's grandmother'' is a valid candidate slot, while ``old persons home'', tagged as location entity, would be replaced with ``Not Specified'' during the post-processing.   % UK閳 will not be a valid slot for the subtask 閳ユ辅ho閳,as 閳ユ辅ho閳 would require a human-related descrip-tion but 閳ユ矾K閳 is tagged as location-related entityby NER. %   %   % and trains   % tackles each event separately and trains multiple models for different events.   % \cy{Didn't see the reason for this to become a challenge.}  % To tackle the aforementioned challenges, we propose \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model.  % Built upon a joint event multi-task learning framework, \ours benefits from the training data across all the event types.  % In this way, we implicitly augment the dataset by global training and fine-tuning the embedding parameters.  % Furthermore, we design a type-aware post-processing step to automatically remove the predictions whose entities do not match the corresponding subtask types by leveraging the named entity recognition  .  % For example, ``UK'' will not be a valid slot for the subtask ``who'', as ``who'' would require a human-related description but ``UK'' is tagged as location-related entity by NER. % \kenneth{This example is quite confusing. Need to make this more clear.} % For example, if a predicted slot for subtask ``who'' is tagged with a location related entity, we invalidate the prediction by ``Not Specified''.  %  In summary, \ours is enabled by the following technical contributions:\\ %     %    % covid-19 wide spreading  % To automatically extract structured knowledge on events % related to COVID-19 from Twitter is useful for epidemiologists, journalist or policymakers.    % Challenges:  Noisy text in Twitter;  Limited training data;   % In this work, we propose a joint event multi-task learning model for noisy text slot filling tasks with limited training data.  % Our Contributions: %    % s      \paragraph*{Event Extraction from Twitter}    Research on Twitter event extraction can be categorized into domain specific and open domain approaches.  Impressive efforts have been made to detect events from Twitter. Existing works include domain specific event extraction and open domain event extraction. For domain specific extraction, approaches mainly focus on extracting a particular type of events, including natural disasters , traffic events , user mobility behaviors , and etc. The open domain scenario is more challenging and usually relies on unsupervised approaches. Existing works usually create clusters with event-related keywords , or named entities . Additionally, \citet{ritter2012open} and \citet{zhou2015unsupervised} design general pipelines to extract and categorize events in supervised and unsupervised manner respectively.     cluster events from tweets using NE mentions as central terms of the clusters. Similarly,  create clusters based on cosine similarity among tweets.  propose a non-parametric Bayesian Mixture Model with word embeddings to create event clusters.  exploit NE mentions in tweets and context to create event graphs, which are further used for clustering.    There has been a variety of works in extracting stuctured knowledge from Twitter data.  use a latent variable model for discovering important event categories.  propose a probabilistic model to discover individual users' mobility behaviors including who, where, when and what.  detects events by exploring the textual and temporal components from user-generated tweets.    designs a general pipeline process of filtering, extraction and categorization framework in unsupervised manner to explore events from tweets.  Different from previous works, we deal with COVID-19 related event extraction in particular. \citet{zong2020extracting} provide a BERT baseline for the same task. But we create a unified framework to learn simultaneously for different categories of events and subtasks.    \kenneth{Is this new? Anyone done something similar before ?}   Moreover, \ours is specially designed to extract the COVID-19 related events from Twitter.    propose a general unsupervised framework to explore events   {\color{red}jiaqi}    \paragraph*{Type-aware Slot Filling} \citet{yang2016cmuml} formulate entity type constraints and use integer linear programming to combine them with relation classification. \citet{adel2019type} propose to integrate entity and relation classes in convolutional neural networks and learn the correlation from data.  We propose a NER-based post-processing technique for type-aware slot filling.   \kenneth{This is probably the 2nd time you use the word ``NER-based'', what exactly does this mean? What's the intuition?} By filtering out entity mis-matched predictions, \ours can efficiently boost the performance with minimum hand-crafted rules.     \paragraph*{COVID-19 Twitter Analysis} With the quarantine situation, people can share thoughts and make comments about COVID-19 on Twitter. It has become a research source for researchers to explore and study. \citet{singh2020first} show that Twitter conversations indicate a spatio-temporal relationship between information flow and new cases of COVID-19. There is some work about COVID-19 datasets. \citet{banda2020large} provide a large-scale curated dataset of over 152 million tweets. \citet{chen2020covid} collect tweets and forms a multilingual COVID-19 Twitter dataset. Based on the collected data, \citet{jahanbin2020using} propose a model to predict COVID-19 breakout by monitoring and tracking information on Twitter. Though there are some works about COVID-19 tweets analyisis , the work about automatically extracting structured knowledge of COVID-19 events from tweets is still limited.      {\color{red}jiaqi}       We have introduced Graph Topic Model, a neural topic model that incorporates corpus-level neighboring context using graph convolutions to enrich document representations and facilitate the topic inference. Both quantitative and qualitative results are presented in the experiments to demonstrate the effectiveness of the proposed approach. In the future, we would like to extend GTM to corpora with explicit doc-doc interactions, e.g., scientific documents with citations or social media posts with user relationships. Replacing GCN in GTM with more advanced graph neural networks is another promising research direction.   
","    \paragraph*{Event Extraction from Twitter}    Research on Twitter event extraction can be categorized into domain specific and open domain approaches.  Impressive efforts have been made to detect events from Twitter. Existing works include domain specific event extraction and open domain event extraction. For domain specific extraction, approaches mainly focus on extracting a particular type of events, including natural disasters , traffic events , user mobility behaviors , and etc. The open domain scenario is more challenging and usually relies on unsupervised approaches. Existing works usually create clusters with event-related keywords , or named entities . Additionally, \citet{ritter2012open} and \citet{zhou2015unsupervised} design general pipelines to extract and categorize events in supervised and unsupervised manner respectively.     cluster events from tweets using NE mentions as central terms of the clusters. Similarly,  create clusters based on cosine similarity among tweets.  propose a non-parametric Bayesian Mixture Model with word embeddings to create event clusters.  exploit NE mentions in tweets and context to create event graphs, which are further used for clustering.    There has been a variety of works in extracting stuctured knowledge from Twitter data.  use a latent variable model for discovering important event categories.  propose a probabilistic model to discover individual users' mobility behaviors including who, where, when and what.  detects events by exploring the textual and temporal components from user-generated tweets.    designs a general pipeline process of filtering, extraction and categorization framework in unsupervised manner to explore events from tweets.  Different from previous works, we deal with COVID-19 related event extraction in particular. \citet{zong2020extracting} provide a BERT baseline for the same task. But we create a unified framework to learn simultaneously for different categories of events and subtasks.    \kenneth{Is this new? Anyone done something similar before ?}   Moreover, \ours is specially designed to extract the COVID-19 related events from Twitter.    propose a general unsupervised framework to explore events   {\color{red}jiaqi}    \paragraph*{Type-aware Slot Filling} \citet{yang2016cmuml} formulate entity type constraints and use integer linear programming to combine them with relation classification. \citet{adel2019type} propose to integrate entity and relation classes in convolutional neural networks and learn the correlation from data.  We propose a NER-based post-processing technique for type-aware slot filling.   \kenneth{This is probably the 2nd time you use the word ``NER-based'', what exactly does this mean? What's the intuition?} By filtering out entity mis-matched predictions, \ours can efficiently boost the performance with minimum hand-crafted rules.     \paragraph*{COVID-19 Twitter Analysis} With the quarantine situation, people can share thoughts and make comments about COVID-19 on Twitter. It has become a research source for researchers to explore and study. \citet{singh2020first} show that Twitter conversations indicate a spatio-temporal relationship between information flow and new cases of COVID-19. There is some work about COVID-19 datasets. \citet{banda2020large} provide a large-scale curated dataset of over 152 million tweets. \citet{chen2020covid} collect tweets and forms a multilingual COVID-19 Twitter dataset. Based on the collected data, \citet{jahanbin2020using} propose a model to predict COVID-19 breakout by monitoring and tracking information on Twitter. Though there are some works about COVID-19 tweets analyisis , the work about automatically extracting structured knowledge of COVID-19 events from tweets is still limited.      {\color{red}jiaqi}",7
"  In the era of digitization, most businesses are turning towards leveraging artificial intelligence  techniques to exploit the information contained in business documents. Traditional information extraction  approaches utilize Natural Language Processing  methods to process the information from documents expressed in the form of natural language text . However, documents contain rich multi-modal information that includes both text and the document layout. The document layout organises the textual information into different formats such as sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues are also indicated through figures/charts/logos etc. and the overall document page appearance. In general, information in a document spans over multiple pages which gives rise to a variety of complex document layouts that can be observed in scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing and understanding these documents is a challenging endeavor and requires a multi-disciplinary perspective combining NLP, computer vision , and knowledge-representation to learn a generic document representation suitable for different downstream applications .  Recent approaches towards document analysis have explored frameworks that utilize information from document text, document layout and document image in different capacities  for specific document tasks.  have proposed joint training of document text and structure for the task of IE from form-like documents, while  combine text and image information for the task of semantic segmentation of documents. Their proposed frameworks optimize the network performance with respect to downstream task which are not suitable for other tasks. To address this limitation,  proposed a pre-training technique based on the BERT transformer architecture , to combine text and layout information from scanned documents. They showcase applicability of their pre-trained network on different downstream tasks further utilizing the image information during fine-tuning for each task. Although  presents a pre-trained framework to learn document representation, there are two limitations to their approach -  the framework only allows for single page documents and  proposed pre-training tasks cannot utilize image information for learning document representation. In the real-world scenario, multi-page documents are common with different pages potentially containing different information across text, layout, and image dimensions. Also, the page image captures the overall layout beyond the appearance of text tokens in the document. Thus, for serving different documents tasks, a unified pre-training framework that learns a generic document representation from all three modalities and works on multi-page documents is necessary.     In this paper, we propose such a generic document representation learning framework that takes as input the document text, layout, and image information applicable to different document tasks. Specifically, we encode the multi-modal document information as -  text and position embeddings similar to BERT   text token 2D position embeddings to capture the layout,  text token image embeddings to capture their appearance, and  document page image and position embeddings to learn the document representation capable of handling multi-page documents. In order to handle large token sequences courtesy of multi-page documents, we utilize the Longformer model proposed by  as the backbone of our framework which introduces an attention mechanism that scales linearly with the sequence length. Following the work of , we utilize the Masked Visual Language Modelling  task and a document classification task that enforces the joint pre-training of all the input embeddings. To further ensure the network learns from the image embeddings, we introduce two additional self-supervised pre-training tasks in our framework -  document topic modeling  and  document shuffle prediction . Similar to the work of , we mine the latent topics from the document text and train our framework to predict the topic distribution using only the document page image embeddings for the task of DTM. On the other hand, DSP involves shuffling the page image order while keeping the other embeddings intact for randomly sampled documents during training to identify if the document is tampered with. While DSP task enforces the joint pre-training of the image embeddings with the text and layout embeddings, DTM task helps to learn richer page image embeddings. As explored by different approaches in prior art , we employ a multi-task learning framework to simultaneously train multiple objectives of the different pre-training tasks to learn shared representations across the text, layout, and image modalities of the documents. We train our network on the publicly available ArXiv dataset  which contains millions of research articles spanning a variety of STEM domains such as mathematics, physics, computer science, etc.  Fig.  signifies the applicability of our pre-trained embeddings for different document tasks. We evaluate the performance of our framework on the following tasks and datasets -  Form Understanding and IE from scanned forms    Document Classification    Table Token Classification  and  Document Retrieval . We conduct an exhaustive set of experiments to analyze the performance of our pre-trained embeddings against state-of-the-art  baselines and ablations of our framework. We're able to beat the SOTA baselines trained on comparable dataset size and network parameters for most of these tasks. In summary, the main contributions of this work are:  % We're able to beat the SOTA performance for certain tasks and achieve comparable performance in other cases utilizing only the pre-trained embeddings for fine-tuning on each task. In summary, the main contributions of this work are      In recent years, different prior-art approaches have explored the document semantics, visual appearance as well as its layout to have a granular understanding of the document information necessary to solve problems such as information extraction, semantic segmentation, layout analysis, table structure detection, etc.  introduce a document representation that encodes the character level textual information while preserving the 2D document layout. They train a fully convolutional encoder-decoder network that learns from this input representation to extract semantic information from invoices. For similar task of information extraction from invoices,  propose a convolutional network that learns both semantic and structural information from scanned invoices by creating a gridded text representation that preserves the spatial relationship among the text tokens. Contrary to these approaches,  utilizes the knowledge of key fields to be extracted from a document to generate candidates and learn their dense representation that also encodes information from its positional neighbors. For analysing the tables in scanned documents,  propose different modifications to standard CNN network architectures such as VGGNet  used for classification and Faster R-CNN  for object detection in images to recognise tables and identify their structure. To segment key regions in scientific articles,  propose to take a pixel-wise semantic segmentation approach. They use a multi-modal encoder-decoder network architecture that takes as input both the text and image embeddings. To learn a generic representation for supporting different tasks such as document image classification and document information extraction,  propose to utilize the BERT transformer architecture to encode text as well as layout information to learn pre-trained embeddings and further utilize image information to fine-tune for a specific task. Most of the approaches in prior-art utilize the multi-modal document information from single page documents and extending their applicability to multi-page documents needs further exploration. Further, these approaches rely on limited labeled data, thus, exploring self-supervised learning to leverage large unlabeled datasets also needs exploration. We attempt to address these limitations in this paper.     Traditional approaches for document analysis address different problems such as document classification , document summarisation , document retrieval  etc. Documents, in general, are comprised of both semantic and structural information. However, these approaches taken to solve the traditional problems in document analysis restrict themselves to a single modality, either document text or document image.       In recent years, different prior-art approaches have focussed on taking a multi-modal approach for document understanding. Exploring the document semantics as well as structure allow to learn a granular understanding of the document information necessary to solve problems such as information extraction, semantic segmentation, layout analysis, table structure detection etc. These approaches fundamentally involve analysing document structure in addition to the primary modality of document text or document image.  introduce a document representation that encodes the character level textual information while preserving the 2D document layout. They train a fully convolutional encoder-decoder network that learns from this input representation to extract semantic information from invoices. For similar task of information extraction from invoices,  propose a convolutional network that learns both semantic and structural information from scanned invoices by creating a gridded text representation that preserves the spatial relationship among the text tokens. Contrary to these approaches,  utilize the knowledge of key fields to be extracted from a document to generate candidates and learn their dense representation that encodes information from its positional neighbors. For analysing the tables in scanned documents,  propose different modifications to standard CNN network architectures such as VGGNet  used for classification and Faster R-CNN  for object detection in images to recognise tables and identify their structure. Similarly,  propose to augment the Faster R-CNN object detection network architecture  with contextual features about the document pages and region bounding boxes to segment key regions in scientific articles. On the contrary,  propose to solve this as a pixel-wise semantic segmentation task utilising a multi-modal encoder-decoder network architecture that takes as input both the text and image embeddings. To learn a generic representation for supporting different tasks such as document image classification and document information extraction,  propose to utilise the BERT transformer architecture  to encode text as well as layout information to learn pre-trained embeddings and further utilise image information to fine-tune for a specific task. Most of the approaches in prior-art utilise the multi-modal document information from single page documents and extending their applicability to multi-page documents needs further exploration.          In this work, we build \ours upon a joint event multi-task learning framework. We use NER-based post-processing to generate type-aware predictions. The results show \ours significantly boosts the performance of extracting COVID-19 events from noisy tweets over BERT and CT-BERT baselines. In the future, we would like to extend \ours to open domain event extraction tasks, which is more challenging and requires a more general pipeline.    \kenneth{Say one or two sentence about future work. What's next?}      \clearpage          
","  In recent years, different prior-art approaches have explored the document semantics, visual appearance as well as its layout to have a granular understanding of the document information necessary to solve problems such as information extraction, semantic segmentation, layout analysis, table structure detection, etc.  introduce a document representation that encodes the character level textual information while preserving the 2D document layout. They train a fully convolutional encoder-decoder network that learns from this input representation to extract semantic information from invoices. For similar task of information extraction from invoices,  propose a convolutional network that learns both semantic and structural information from scanned invoices by creating a gridded text representation that preserves the spatial relationship among the text tokens. Contrary to these approaches,  utilizes the knowledge of key fields to be extracted from a document to generate candidates and learn their dense representation that also encodes information from its positional neighbors. For analysing the tables in scanned documents,  propose different modifications to standard CNN network architectures such as VGGNet  used for classification and Faster R-CNN  for object detection in images to recognise tables and identify their structure. To segment key regions in scientific articles,  propose to take a pixel-wise semantic segmentation approach. They use a multi-modal encoder-decoder network architecture that takes as input both the text and image embeddings. To learn a generic representation for supporting different tasks such as document image classification and document information extraction,  propose to utilize the BERT transformer architecture to encode text as well as layout information to learn pre-trained embeddings and further utilize image information to fine-tune for a specific task. Most of the approaches in prior-art utilize the multi-modal document information from single page documents and extending their applicability to multi-page documents needs further exploration. Further, these approaches rely on limited labeled data, thus, exploring self-supervised learning to leverage large unlabeled datasets also needs exploration. We attempt to address these limitations in this paper.     Traditional approaches for document analysis address different problems such as document classification , document summarisation , document retrieval  etc. Documents, in general, are comprised of both semantic and structural information. However, these approaches taken to solve the traditional problems in document analysis restrict themselves to a single modality, either document text or document image.       In recent years, different prior-art approaches have focussed on taking a multi-modal approach for document understanding. Exploring the document semantics as well as structure allow to learn a granular understanding of the document information necessary to solve problems such as information extraction, semantic segmentation, layout analysis, table structure detection etc. These approaches fundamentally involve analysing document structure in addition to the primary modality of document text or document image.  introduce a document representation that encodes the character level textual information while preserving the 2D document layout. They train a fully convolutional encoder-decoder network that learns from this input representation to extract semantic information from invoices. For similar task of information extraction from invoices,  propose a convolutional network that learns both semantic and structural information from scanned invoices by creating a gridded text representation that preserves the spatial relationship among the text tokens. Contrary to these approaches,  utilize the knowledge of key fields to be extracted from a document to generate candidates and learn their dense representation that encodes information from its positional neighbors. For analysing the tables in scanned documents,  propose different modifications to standard CNN network architectures such as VGGNet  used for classification and Faster R-CNN  for object detection in images to recognise tables and identify their structure. Similarly,  propose to augment the Faster R-CNN object detection network architecture  with contextual features about the document pages and region bounding boxes to segment key regions in scientific articles. On the contrary,  propose to solve this as a pixel-wise semantic segmentation task utilising a multi-modal encoder-decoder network architecture that takes as input both the text and image embeddings. To learn a generic representation for supporting different tasks such as document image classification and document information extraction,  propose to utilise the BERT transformer architecture  to encode text as well as layout information to learn pre-trained embeddings and further utilise image information to fine-tune for a specific task. Most of the approaches in prior-art utilise the multi-modal document information from single page documents and extending their applicability to multi-page documents needs further exploration.",8
"  Discourse coherence has been the subject of much research in Computational Linguistics thanks to its widespread applications . Most current methods can be described as either stemming from explicit representations based on the Centering Theory , or deep learning approaches that learn without the use of hand-crafted linguistic features.  Our work explores a third research avenue based on the Rhetorical Structure Theory  . We hypothesize that texts of low/high coherence tend to adhere to different discourse structures. Thus, we pose that using even silver-standard RST features should help in separating coherent texts from incoherent ones. This stems from the definition of the coherence itself - as the writer of a document needs to follow specific rules for building a clear narrative or argument structure in which the role of each constituent of the document should be appropriate with respect to its local and global context, and even existing discourse parsers should be able to predict a plausible structure that is consistent across all coherent documents. However, if a parser has difficulty interpreting a given document, it will be more likely to produce unrealistic trees with improbable patterns of discourse relations between constituents. This idea was first explored by \citeauthor{feng-etal-2014-impact} \shortcite{feng-etal-2014-impact}, who followed an approach similar to \citeauthor{Barzilay-Entity-Grid} \shortcite{Barzilay-Entity-Grid} by estimating entity transition likelihoods, but instead using discourse relations  that entities participate in as opposed to their grammatical roles. Their method achieved significant improvements in performance even when using silver-standard discourse trees, showing potential in the use of parsed RST features for classifying textual coherence.       Our work, however, is the first to develop and test a neural approach to leveraging RST discourse representations in coherence evaluation. Furthermore, \citet{feng-etal-2014-impact} only tested their proposal on the sentence permutation task, which involves ranking a sentence-permuted text against the original. As noted by \citet{lai-grammerly}, this is not an accurate proxy for realistic coherence evaluation. We evaluate our method on their more realistic Grammarly Corpus Of Discourse Coherence , where the model needs to classify a naturally produced text into one of three levels of coherence. Our contributions involve:  RST-Recursive, an RST-based neural tree-recursive method for coherence evaluation that achieves 2\% below the state of the art performance on the GCDC while having 62\% fewer parameters.  When ensembled with the current state of the art, namely Parseq , we achieve a notable improvement over the plain ParSeq model.  We demonstrate the usefulness of silver-standard RST features in coherence classification, and establish our results as a lower-bound for performance improvements to be gained using RST features.       Centering Theory  states that subsequent sentences in coherent texts are likely to continue to focus on the same entities  as within the previous sentences. Building on top of this, \citet{Barzilay-Entity-Grid} were the first to propose the Entity-Grid model that constructs a two-dimensional array  for a text of  sentences and  entities, which are used to estimate transition probabilities for entity occurrence patterns. More recently, \citet{elsner-charniak-2011-extending} extended Entity-Grid using entity-specific features, while \citet{tien-nguyen-joty-2017-neural} used a Convolutional Neural Network  on top of Entity-Grid to learn more hierarchical patterns.   On the other hand, feature-free deep neural techniques have dominated recent research. \citeauthor{li-jurafsky-2017-neural} \shortcite{li-jurafsky-2017-neural} applied Recurrent Neural Networks  to model the coherent generation of the next sentence given the current sentence and vice-versa. \citeauthor{mesgar-strube-2018-neural} \shortcite{mesgar-strube-2018-neural} constructed a local coherence model that encodes patterns of changes on how adjacent sentences within the text are semantically related. Recently, \citet{moon-etal-2019-unified} used a multi-component model to capture both local and global coherence perturbations. \citeauthor{lai-grammerly} \shortcite{lai-grammerly} developed a hierarchical neural architecture named ParSeq with three stacked LSTM Networks, designed to encode the coherence at sentence, paragraph and document levels.    RST describes the structure of a text in the following way: first, the text is segmented into elementary discourse units , which describe spans of text constituting clauses or clause-like units . Second, the EDUs are recursively structured into a tree hierarchy where each node defines an RST relation between the constituting sub-trees. The sub-tree with the central purpose is called the nucleus, and the one bearing secondary intent is called the satellite while a connective discourse relation is assigned to both. An example of a 閳ユ笜ucleus-satellite"" relation pairing is presented in Figure  where a claim is followed by the evidence for the claim; RST posits an 閳ユ窊vidence閳 relation between these two spans with the left sub-tree being the 閳ユ笜ucleus"" and the right sub-tree as 閳ユ笩atellite"".     We present a multi-modal pre-training framework that utilizes multi-task learning to learn a generic document representation. Our framework encodes the visual, layout and textual information and supports real-world multi-page documents. Our network is pre-trained on the publically available Arxiv dataset utilizing self-supervised tasks that promote learning multi-modal shared representations. We fine-tune our pre-trained network to showcase state-of-the-art performance on different document tasks such as document classification, information extraction and document retrieval. In future, we will investigate pre-training on large datasets such as PublayNet  to analyze the performance gain for different tasks and further explore new architecture designs that will enable document image tasks such as object detection/segmentation using our framework.     We present a multi-modal neural network architecture that utilizes multi-task learning to learn a generic document representation. Our proposed architecture can encode multiple pages while encoding the visual, layout and textual components ubiquitous in real-world PDF documents. We further finetune our architecture across various downstream tasks, and compare our results with existing baselines. Our model significantly outperforms existing baselines in FUNSD, while attains comparable scores in RVL-CDIP, even when pretrained on a much smaller dataset compared to LayoutLM. We also demonstrate that our model is capable table token detection and document retrieval tasks. Novel to our approach, our architecture can utilize visual, layout \& textual components during pretraining and hence can generalize better even when pretrained on a smaller dataset. We also introduce two novel pretraining tasks that helps to learn richer visual representations and enforces joint representation learning for both visual and language modalities. Hence, our model pretrained on all the four pretraining tasks acheives the highest performance across all downstream tasks. We also conduct an ablation to demonstrate the efficacy of the two proposed tasks.     In future research, we will investigate pretraining our architecture on a larger subset of the Arxiv dataset and use the larger PublayNet dataset .   Add more future work \section{Ethical Impact}  The framework proposed in this paper for learning a generic document representation enables a system to read, understand and interpret digital documents. Such a framework is applicable in a variety of enterprise settings. Typical enterprise applications depend on experts to put in hours of work in collecting, filtering, reading, searching and analysing business documents to mine useful insights for business. Common examples include government officers validating user submitted documents for passport application, loan officers analysing user business documents to ascertain income status of the owner, corporate lawyers analysing contracts to identify loopholes etc. For all of these different scenarios, the upside to using our proposed framework is huge since it dramatically reduces the manual effort for all these different experts in conducting their routine tasks. For e.g., our framework fine-tuned on a dataset of passport applications is capable of analysing and extracting all the submitted fields by the applicant in their application. A system based on our framework deployed with the concerned government agency would assist its officials to quickly go through all the fields and approve/reject the application. Additionally, the officials do not need to acquire any specialised skills or undergo training to understand how the system works. On the other hand, it is difficult to come up with a scenario where our proposed framework can be ill-used without malicious intent. Users can potentially utilize our framework to mine personal information of applicants/employees from enterprise documents. For e.g., a corporate human resources  officer could keep a database of all applicants by mining their personal information from submitted resumes by using our framework that is fine-tuned on a dataset of resumes. Hence, in our opinion, the proposed framework enables decision making for different users by providing document insights which can be used to have both a positive or negative impact. \section{Introduction}  In the era of digitization, most businesses are turning towards leveraging artificial intelligence  techniques to exploit the information contained in business documents. Traditional information extraction  approaches utilize Natural Language Processing  methods to process the information from documents expressed in the form of natural language text . However, documents contain rich multi-modal information that includes both text and the document layout. The document layout organises the textual information into different formats such as sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues are also indicated through figures/charts/logos etc. and the overall document page appearance. In general, information in a document spans over multiple pages which gives rise to a variety of complex document layouts that can be observed in scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing and understanding these documents is a challenging endeavor and requires a multi-disciplinary perspective combining NLP, computer vision , and knowledge-representation to learn a generic document representation suitable for different downstream applications .  Recent approaches towards document analysis have explored frameworks that utilize information from document text, document layout and document image in different capacities  for specific document tasks.  have proposed joint training of document text and structure for the task of IE from form-like documents, while  combine text and image information for the task of semantic segmentation of documents. Their proposed frameworks optimize the network performance with respect to downstream task which are not suitable for other tasks. To address this limitation,  proposed a pre-training technique based on the BERT transformer architecture , to combine text and layout information from scanned documents. They showcase applicability of their pre-trained network on different downstream tasks further utilizing the image information during fine-tuning for each task. Although  presents a pre-trained framework to learn document representation, there are two limitations to their approach -  the framework only allows for single page documents and  proposed pre-training tasks cannot utilize image information for learning document representation. In the real-world scenario, multi-page documents are common with different pages potentially containing different information across text, layout, and image dimensions. Also, the page image captures the overall layout beyond the appearance of text tokens in the document. Thus, for serving different documents tasks, a unified pre-training framework that learns a generic document representation from all three modalities and works on multi-page documents is necessary.     In this paper, we propose such a generic document representation learning framework that takes as input the document text, layout, and image information applicable to different document tasks. Specifically, we encode the multi-modal document information as -  text and position embeddings similar to BERT   text token 2D position embeddings to capture the layout,  text token image embeddings to capture their appearance, and  document page image and position embeddings to learn the document representation capable of handling multi-page documents. In order to handle large token sequences courtesy of multi-page documents, we utilize the Longformer model proposed by  as the backbone of our framework which introduces an attention mechanism that scales linearly with the sequence length. Following the work of , we utilize the Masked Visual Language Modelling  task and a document classification task that enforces the joint pre-training of all the input embeddings. To further ensure the network learns from the image embeddings, we introduce two additional self-supervised pre-training tasks in our framework -  document topic modeling  and  document shuffle prediction . Similar to the work of , we mine the latent topics from the document text and train our framework to predict the topic distribution using only the document page image embeddings for the task of DTM. On the other hand, DSP involves shuffling the page image order while keeping the other embeddings intact for randomly sampled documents during training to identify if the document is tampered with. While DSP task enforces the joint pre-training of the image embeddings with the text and layout embeddings, DTM task helps to learn richer page image embeddings. As explored by different approaches in prior art , we employ a multi-task learning framework to simultaneously train multiple objectives of the different pre-training tasks to learn shared representations across the text, layout, and image modalities of the documents. We train our network on the publicly available ArXiv dataset  which contains millions of research articles spanning a variety of STEM domains such as mathematics, physics, computer science, etc.  Fig.  signifies the applicability of our pre-trained embeddings for different document tasks. We evaluate the performance of our framework on the following tasks and datasets -  Form Understanding and IE from scanned forms    Document Classification    Table Token Classification  and  Document Retrieval . We conduct an exhaustive set of experiments to analyze the performance of our pre-trained embeddings against state-of-the-art  baselines and ablations of our framework. We're able to beat the SOTA baselines trained on comparable dataset size and network parameters for most of these tasks. In summary, the main contributions of this work are:    We're able to beat the SOTA performance for certain tasks and achieve comparable performance in other cases utilizing only the pre-trained embeddings for fine-tuning on each task. In summary, the main contributions of this work are   \section{Introduction}  In the era of digitization, most businesses are turning towards leveraging artificial intelligence  techniques to exploit the information contained in business documents. Traditional information extraction  approaches utilize Natural Language Processing  methods to process the information from documents expressed in the form of natural language text . However, documents contain rich multi-modal information that includes both text and the document structure. The document structure organises the textual information into different formats such as sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues are also indicated through figures/charts/logos, etc. In general, information in the documents spans over multiple pages and the different document structures give rise to a variety of complex document layouts which can be observed in scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing and understanding these documents is a challenging endeavor and requires a multi-disciplinary perspective combining NLP, computer vision , and knowledge-representation to learn a generic document representation suitable for different downstream applications .  Although traditional approaches to document processing involve analysing the textual information  for document classification, summarisation etc., recent approaches have explored frameworks that utilize information from text, document structure and document image in different capacities  for specific downstream tasks.  has proposed joint training of document text and structure for the task of IE from form-like documents, while  combine text and image information for the task of semantic segmentation of documents. These approaches propose a framework with the objective of optimizing the network performance w.r.t. the downstream task and do not learn a generic document representation applicable to different downstream tasks. To address these limitations,  proposed a pre-training technique based on the BERT transformer architecture , to combine text and structure information from scanned documents. They incorporate modifications to the BERT pre-training tasks to make it suitable for training on documents and further showcase applicability on different downstream tasks utilizing the image information during fine-tuning for each task. Although  presents a pre-trained framework to learn document representation, there are two limitations to their approach -  the framework only allows for single page documents and  proposed pre-training tasks cannot utilize image information for learning document representation. In the real-world scenario, multi-page documents are common with different pages potentially containing different information across text, structure, and image dimensions. Also, the page image contains important visual cues about different document elements such as tables/figures/charts, etc. and the overall layout beyond the appearance of text tokens in the document. Thus, for serving different documents tasks, a unified pre-training framework that learns a generic document representation from all three modalities and works on multi-page documents is necessary.     In this paper, we propose such a generic document representation learning framework that takes as input the document text, structure, and image information applicable to different document tasks. Specifically, we encode the multi-modal document information as -  text and position embeddings similar to BERT   text token 2D position embeddings to capture the structure,  text token image embeddings to capture their appearance, and  document page image and position embeddings to learn the document representation capable of handling multi-page documents. In order to handle large token sequences courtesy of multi-page documents, we utilize the Longformer model proposed by  as the backbone of our framework which introduces an attention mechanism that scales linearly with the sequence length. Following the work of , we utilize the Masked Visual Language Modelling  task that enforces joint pre-training of the text, structure, and page embeddings and a document classification task that enforces the joint pre-training of all the input embeddings. To further ensure the network learns from the overall page image embeddings, we introduce two additional self-supervised pre-training tasks in our framework -  topic modeling  and  document shuffle prediction . Similar to the work of , we mine the latent topics from the document text and train our framework to predict the topic distribution using only the document page image embeddings for the task of DTM. On the other hand, DSP involves shuffling the page image order while keeping the other embeddings intact for randomly sampled documents during training to identify if the document is tampered with. Both of these tasks enforce the joint pre-training of the page image embeddings with the text and structure embeddings. As explored by different approaches in prior art , we employ a multi-task learning framework to simultaneously train multiple objectives of the different pre-training tasks to learn shared representations across the text, structure, and image modalities of the documents. We train our network on the publicly available ArXiv dataset  which contains millions of research articles spanning a variety of STEM domains such as mathematics, physics, computer science, etc.  Fig.  signifies the applicability of our pre-trained embeddings for different document tasks. We evaluate the performance of our framework on the following tasks and datasets -  Form Understanding and IE from scanned forms    Document Classification    Table Token Classification  and  Document Retrieval . We conduct an exhaustive set of experiments to analyze the performance of our pre-trained embeddings against state-of-the-art  baselines and ablations of our framework. We're able to beat the SOTA baselines trained on comparable dataset size and network parameters for most of these tasks. In summary, the main contributions of this work are:    We're able to beat the SOTA performance for certain tasks and achieve comparable performance in other cases utilizing only the pre-trained embeddings for fine-tuning on each task. In summary, the main contributions of this work are   \def\year{2021}\relax  File: formatting-instructions-latex-2021.tex  release 2021.1 \documentclass[letterpaper]{article}   DO NOT CHANGE THIS  \usepackage[switch]{lineno}  \usepackage{aaai21}    DO NOT CHANGE THIS \usepackage{times}    DO NOT CHANGE THIS \usepackage{helvet}   DO NOT CHANGE THIS \usepackage{courier}    DO NOT CHANGE THIS \usepackage[hyphens]{url}    DO NOT CHANGE THIS \usepackage{graphicx}   DO NOT CHANGE THIS \usepackage{fixltx2e} \urlstyle{rm}   DO NOT CHANGE THIS \def\UrlFont{\rm}    DO NOT CHANGE THIS \usepackage{natbib}    DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption}   DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing    DO NOT CHANGE THIS \setlength{\pdfpagewidth}{8.5in}    DO NOT CHANGE THIS \setlength{\pdfpageheight}{11in}    DO NOT CHANGE THIS  \nocopyright   PDF Info Is REQUIRED.   For /Author, add all authors within the parentheses, separated by commas. No accents or commands.   For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses. \pdfinfo{ /Title   /Author  /TemplateVersion  }  Leave this   /Title    Put your actual complete title  within the parentheses in mixed case   Leave the space between \Title and the beginning parenthesis alone   /Author    Put your actual complete list of authors  within the parentheses in mixed case.   Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,   remove them. \usepackage{amsfonts} \copyrighttext{}   DISALLOWED PACKAGES   \usepackage{authblk} -- This package is specifically forbidden   \usepackage{balance} -- This package is specifically forbidden   \usepackage{color    \usepackage{CJK} -- This package is specifically forbidden   \usepackage{float} -- This package is specifically forbidden   \usepackage{flushend} -- This package is specifically forbidden   \usepackage{fontenc} -- This package is specifically forbidden   \usepackage{fullpage} -- This package is specifically forbidden   \usepackage{geometry} -- This package is specifically forbidden   \usepackage{grffile} -- This package is specifically forbidden   \usepackage{hyperref} -- This package is specifically forbidden   \usepackage{navigator} -- This package is specifically forbidden      \indentfirst} -- This package is specifically forbidden   \layout} -- This package is specifically forbidden   \multicol} -- This package is specifically forbidden   \nameref} -- This package is specifically forbidden   \usepackage{savetrees} -- This package is specifically forbidden   \usepackage{setspace} -- This package is specifically forbidden   \usepackage{stfloats} -- This package is specifically forbidden   \usepackage{tabu} -- This package is specifically forbidden   \usepackage{titlesec} -- This package is specifically forbidden   \usepackage{tocbibind} -- This package is specifically forbidden   \usepackage{ulem} -- This package is specifically forbidden   \usepackage{wrapfig} -- This package is specifically forbidden   DISALLOWED COMMANDS   \nocopyright -- Your paper will not be published if you use this command   \addtolength -- This command may not be used   \balance -- This command may not be used   \baselinestretch -- Your paper will not be published if you use this command   \clearpage -- No page breaks of any kind may be used for the final version of your paper   \columnsep -- This command may not be used    -- No page breaks of any kind may be used for the final version of your paper   \pagebreak -- No page breaks of any kind may be used for the final version of your paperr   \pagestyle -- This command may not be used   \tiny -- This is not an acceptable font size.    \usepackage{multirow}  \usepackage[switch]{lineno}  \setcounter{secnumdepth}{2}  May be changed to 1 or 2 if section numbers are desired.    The file aaai21.sty is the style file for AAAI Press   proceedings, working notes, and technical reports.      Title    Your title must be in mixed case, not sentence case.   That means all verbs ,   nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while   articles, conjunctions, and prepositions are lower case unless they   directly follow a colon or long dash    \title{Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning}   \author {         Author       Anonymous authors \\   }   \iffalse  Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it       \iffalse  Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it \title{Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning} \author {       Authors         Subhojeet Pramanik\thanks{Equal Contribution}\textsuperscript{\rm 1},         Shashank Mujumdar\textsuperscript{*\rm 2},         Hima Patel\textsuperscript{\rm 2}\\ } \affiliations{         \textsuperscript{1}IBM Cloud, India \\         \textsuperscript{2}IBM Research, India\\         \{subhojeet,shamujum,himapatel\}@in.ibm.com         }   \fi            
","    Centering Theory  states that subsequent sentences in coherent texts are likely to continue to focus on the same entities  as within the previous sentences. Building on top of this, \citet{Barzilay-Entity-Grid} were the first to propose the Entity-Grid model that constructs a two-dimensional array  for a text of  sentences and  entities, which are used to estimate transition probabilities for entity occurrence patterns. More recently, \citet{elsner-charniak-2011-extending} extended Entity-Grid using entity-specific features, while \citet{tien-nguyen-joty-2017-neural} used a Convolutional Neural Network  on top of Entity-Grid to learn more hierarchical patterns.   On the other hand, feature-free deep neural techniques have dominated recent research. \citeauthor{li-jurafsky-2017-neural} \shortcite{li-jurafsky-2017-neural} applied Recurrent Neural Networks  to model the coherent generation of the next sentence given the current sentence and vice-versa. \citeauthor{mesgar-strube-2018-neural} \shortcite{mesgar-strube-2018-neural} constructed a local coherence model that encodes patterns of changes on how adjacent sentences within the text are semantically related. Recently, \citet{moon-etal-2019-unified} used a multi-component model to capture both local and global coherence perturbations. \citeauthor{lai-grammerly} \shortcite{lai-grammerly} developed a hierarchical neural architecture named ParSeq with three stacked LSTM Networks, designed to encode the coherence at sentence, paragraph and document levels.    RST describes the structure of a text in the following way: first, the text is segmented into elementary discourse units , which describe spans of text constituting clauses or clause-like units . Second, the EDUs are recursively structured into a tree hierarchy where each node defines an RST relation between the constituting sub-trees. The sub-tree with the central purpose is called the nucleus, and the one bearing secondary intent is called the satellite while a connective discourse relation is assigned to both. An example of a 闁炽儲绗渦cleus-satellite"" relation pairing is presented in Figure  where a claim is followed by the evidence for the claim; RST posits an 闁炽儲绐妚idence闁 relation between these two spans with the left sub-tree being the 闁炽儲绗渦cleus"" and the right sub-tree as 闁炽儲绗゛tellite"".",9
"  The Transformer translation model , which has outperformed previous RNN/CNN based sequence-to-sequence models, is based on multi-head attention networks. The multi-head attention mechanism, which computes several scaled dot-product attention in parallel, can be more efficiently parallelized at sequence level than RNNs , while addressing the drawback of CNNs  which can only model contexts inside a fixed window.  Even though the advantages in parallelization of the multi-head attention mechanism, recent studies  suggest that the computation the scaled dot-product attention is not sufficiently efficient, especially when handling very long sequences, due to the quadratic increasing size of the attention matrix.  In this paper, we study to accelerate the inference of the scaled dot-product attention in another perspective. Specifically, we propose to learn a hard retrieval attention which only attends to one position in the sequence rather than all tokens to simplify the computation of the scaled dot-product attention. Since the hard attention mechanism only attends to one token, the matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can be achieved by a simple and efficient retrieval operation.  Our contributions are as follows:       Most previous research aims to efficiently enable the self-attention mechanism modeling very long sequences.  accelerate the decoder self-attention with the average attention network.  introduce the notion of recurrence into deep self-attention network to model very long term dependency efficiently.  combine low rank approximate and parameters sharing to construct a tensorized Transformer.  replace dot-product attention by one that uses locality-sensitive hashing and use reversible residual layers instead of the standard residuals.  propose a dimension-wise attention mechanism to reduce the attention complexity.  express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products.  approximate the self-attention mechanism by a low-rank matrix.  introduce an attention mechanism that scales linearly with sequence length.  introduce sparse factorizations of the attention matrix.  On using hard  attention for machine translation,  selectively focuses on a small window of context smoothed by Gaussian distribution, while our approach learns explicit one-on-one alignment. For self-attentional sentence encoding,  train hard attention mechanisms which select a subset of tokens via policy gradient.  investigate the selective self-attention networks implemented with Gumble-Sigmoid.     Recent years extensively studies the automatic medical code assignment. Neural clinical text encoding models use CNNs to extract local features and RNNs to preserve sequential dependency. This paper combines both by using dilated convolution. The dilated convolutional attention network  consists of dilated convolution layers, residual connections, and the label attention layer. The DCAN model obeys the causal constraint of sequence encoding and learns rich representations to capture label-aware importance. Through experiments on the MIMIC-III dataset, our model shows better predictive performance than the state-of-the-art methods.   
","  Most previous research aims to efficiently enable the self-attention mechanism modeling very long sequences.  accelerate the decoder self-attention with the average attention network.  introduce the notion of recurrence into deep self-attention network to model very long term dependency efficiently.  combine low rank approximate and parameters sharing to construct a tensorized Transformer.  replace dot-product attention by one that uses locality-sensitive hashing and use reversible residual layers instead of the standard residuals.  propose a dimension-wise attention mechanism to reduce the attention complexity.  express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products.  approximate the self-attention mechanism by a low-rank matrix.  introduce an attention mechanism that scales linearly with sequence length.  introduce sparse factorizations of the attention matrix.  On using hard  attention for machine translation,  selectively focuses on a small window of context smoothed by Gaussian distribution, while our approach learns explicit one-on-one alignment. For self-attentional sentence encoding,  train hard attention mechanisms which select a subset of tokens via policy gradient.  investigate the selective self-attention networks implemented with Gumble-Sigmoid.",10
" Neural Machine Translation  has opened up new opportunities in transfer learning from high-resource to low-resource language pairs . While transfer learning has shown great promise, the transfer between languages with different scripts brings additional challenges. For a successful transfer of the embedding layer, both the parent and the child model should use the same or a partially overlapping vocabulary . It is common to merge the two vocabularies by aligning identical subwords and randomly assigning the remaining subwords from the child vocabulary to positions in the parent vocabulary .   This works well for transfer between languages that use the same script, but if the child language is written in an unseen script, most vocabulary positions are replaced by random subwords. This significantly reduces the transfer from the embedding layer. \citet{gheini2019universal} argue that romanization can improve transfer to languages with unseen scripts. However, romanization can also introduce information loss that might hurt translation quality. In our work, we study the usefulness of romanization for transfer from many-to-many multilingual MT models to low-resource languages with different scripts. Our contributions are the following:       Initial work on transfer learning for NMT has assumed that the child language is known in advance and that the parent and child model can use a shared vocabulary . \citet{lakew2018transfer} argue that this is not feasible in most real-life scenarios and propose using a dynamic vocabulary. Most studies have since opted to replace unused parts of the parent vocabulary with unseen subwords from the child vocabulary ; others use various methods to align embedding spaces . Recently, \citet{aji-etal-2020-neural} showed that transfer of the embedding layer is only beneficial if there is an overlap between the parent and child vocabulary such that embeddings for identical subwords can be aligned. Such alignments are very rare if the child language uses an unseen script.  \citet{gheini2019universal} train a universal vocabulary on multiple languages by romanizing languages written in a non-Latin script. Their many-to-one parent model can be transferred to new source languages without exchanging the vocabulary. In our work, we extend this idea to many-to-many translation settings using subsequent deromanization of the output. We study the trade-off between a greater vocabulary overlap and information loss as a result of romanization. Based on experiments on a diverse set of low-resource languages, we show that romanization is helpful for model transfer to related languages with different scripts.     We propose to accelerate the inference of the scaled dot-product attention by learning a hard retrieval attention which only attends to one token in the sentence rather than all tokens. With the one-on-one hard attention matrix, the matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can be replaced by a simple and efficient retrieval operation.  Our hard retrieval attention mechanism can accelerate both long and short sequences and is  times fast as the scaled dot-product attention. In our experiments on a wide range of machine translation tasks, we demonstrate that using the hard retrieval attention for cross attention networks can lead to competitive performance.  
"," Initial work on transfer learning for NMT has assumed that the child language is known in advance and that the parent and child model can use a shared vocabulary . \citet{lakew2018transfer} argue that this is not feasible in most real-life scenarios and propose using a dynamic vocabulary. Most studies have since opted to replace unused parts of the parent vocabulary with unseen subwords from the child vocabulary ; others use various methods to align embedding spaces . Recently, \citet{aji-etal-2020-neural} showed that transfer of the embedding layer is only beneficial if there is an overlap between the parent and child vocabulary such that embeddings for identical subwords can be aligned. Such alignments are very rare if the child language uses an unseen script.  \citet{gheini2019universal} train a universal vocabulary on multiple languages by romanizing languages written in a non-Latin script. Their many-to-one parent model can be transferred to new source languages without exchanging the vocabulary. In our work, we extend this idea to many-to-many translation settings using subsequent deromanization of the output. We study the trade-off between a greater vocabulary overlap and information loss as a result of romanization. Based on experiments on a diverse set of low-resource languages, we show that romanization is helpful for model transfer to related languages with different scripts.",11
" Machine learning  models used in practice today are predominantly supervised models and rely on large datasets labeled for training. However, the cost of collecting and maintaining labeled training data remains a bottleneck for training high-capacity supervised models. Data programming aims to address the difficulty of collecting labeled data by using a programmatic approach to weak supervision by heuristics, where domain experts are expected to provide data programs  incorporating their domain knowledge. Prior work on data programming focuses on modeling and aggregating labeling functions written manually or generated automatically to denoise labeling functions.  % However, little is known about user experience  % in writing labeling functions and how to improve it.   Writing data programs can be, however, challenging and time consuming.  Most domain experts or lay users have no or little programming literacy, and even for those who are proficient programmers, it is often difficult to convert domain knowledge to a set of rules by writing programs.       %  By extending data programming with programming by example, we bridge the gap between scalable training data generation and domain experts. To address these challenges, we introduce data programming by demonstration ,  a new framework that aims to make creating labeling functions  easier by learning them from users' interactive visual demonstrations. DPBD moves the burden of writing labeling functions to an intelligent synthesizer while enabling users to steer the synthesis process at multiple semantic levels, from providing rationales relevant for their labeling choices to interactively filtering the proposed functions. DPBD draws from two lines of prior research; programming by demonstration  or example , e.g.,, which aims to make programming easier by synthesizing them based on user interactions or input and output examples, and  interactive learning from user-provided features or rationales .    We operationalize our framework with \system, an interactive system that enables more accessible data programming to create labeled training datasets for document classification. \system automatically generates  document level labeling rules from  span-level annotations and their relations on specific examples provided by users. Through a user study conducted with  10 data scientists, we evaluate  \system alongside manual data programming using Snorkel. We measure the predictive performances of models created by participants for two  common labeling tasks, sentiment classification and spam detection. We also elicit ratings and qualitative feedback from participants on multiple measures, including  ease of use, ease of learning, expressivity, and overall satisfaction.  We find \system facilitates more accessible creation of labeling functions without a loss in the quality of learned labeling  models.   Tagging or token level classification in text documents is another widely used task that can benefit from DPBD. Here we also briefly discuss our work in progress on \tagruler, a DPBD system that learns token labeling functions through user interaction to create training datasets for tagging models.   % Tagging or span-level classification in text documents is another widely used task that can benefit from DPBD. Here we also briefly discuss our work in progress on \tagruler, a DPBD system that enables the interactive generation of token labeling functions in order to create labeled training data for tagging models.       % On the other hand, \tagruler synthesizes token classification  rules based users.   In summary, we contribute  DPBD, a general data independent framework for learning labeling rules by interactive demonstration;  \system, an interactive system operationalizing our framework for document classification tasks; and  a comparative user study conducted with data scientists in performing real world tasks to evaluate \system and conventional data programming. We have made our research artifacts, including the \system code and demo, publicly available~.   %  along with the materials and anonymized results of the user study %  \documentclass[sigconf]{acmart} \usepackage[moderate]{savetrees} \usepackage{booktabs} % For formal tables \usepackage{listings} \usepackage{latexsym} \usepackage[sets]{cryptocode} \usepackage{amsmath} \usepackage{amssymb} \usepackage{graphicx} \usepackage{setspace} \usepackage{fullpage} \usepackage{xspace} \usepackage{xcolor} \usepackage{caption} \usepackage{subfigure} \usepackage{courier} \usepackage{enumitem} \usepackage[font=normal,skip=2pt]{caption} \usepackage{times} \usepackage{microtype} \usepackage{balance} % to better equalize the last page \usepackage{xcolor} \usepackage[hang,flushmargin]{footmisc} \setlength{\textfloatsep}{8pt plus 2pt minus 2.0pt} \setlength{\intextsep}{3.0pt plus 1.0pt minus 1.0pt}  % \textfloatsep: 20.0pt plus 2.0pt minus 4.0pt; % \floatsep: 12.0pt plus 2.0pt minus 2.0pt; % \intextsep: 12.0pt plus 2.0pt minus 2.0pt.  \renewcommand{\UrlFont}{\ttfamily\small} \renewcommand % where to search for the images % Copyright \setcopyright{none} \acmConference[]{}{}  %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%\acmSubmissionID{123-A56-BU3}  %% %% The majority of ACM publications use numbered citations and %% references.  The command \citestyle{authoryear} switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%\citestyle{acmauthoryear}  %% %% end of the preamble, start of the body of the document source. %Conference  %\acmYear{1997} %\copyrightyear{2016}   %\acmArticle{4} %\acmPrice{15.00}  %% These commands are optional %%\acmBooktitle{Transactions of the ACM Woodstock conference} %\editor{Jennifer B. Sartor} %\editor{Theo D'Hondt} %\editor{Wolfgang De Meuter} \definecolor{tomato}{rgb}{1,0.2,0} \definecolor{turqoise}{rgb}{0.03, 0.91, 0.87} \definecolor{grey}{rgb}{0.4,0.4,0.4} \newif\ifnotes \notestrue \DeclareRobustCommand{\cagatay}[1]{\ifnotes{\small[\textcolor{grey}{\c{C}a\u{g}atay:}\textcolor{tomato}{#1}]}\fi} \DeclareRobustCommand{\sara}[1]{\ifnotes{\small[\textcolor{grey}{Sara:}\textcolor{turqoise}{#1}]}\fi} \DeclareRobustCommand{\subhead}[1]{#1} \DeclareRobustCommand{\system}{\mbox{\sc Ruler}\xspace} \DeclareRobustCommand{\ruler}{\mbox{\sc Ruler}\xspace} \DeclareRobustCommand{\tagruler}{\mbox{\sc TagRuler}\xspace}  \DeclareRobustCommand{\snorkel}{\mbox{\sc Snorkel}\xspace} \DeclareRobustCommand{\babblelabble}{\mbox{\sc BabbleLabble}\xspace} \DeclareRobustCommand{\thenum}{ten\xspace} \newcommand{\eat}[1]{} \newcommand{\example}[1]{{\underline{Example:} #1\qed}} \newcommand{\stitle}[1]{\smallskip {#1}} \newcommand{\sstitle}[1]{\smallskip {\underline{#1}}} \DeclareRobustCommand{\subhead}[1]{#1}  \newcommand{\squishlist}{      }      \renewcommand{\shortauthors}{} \settopmatter{printacmref=false,printfolios=true,printccs=false}       We build on prior work on weak supervision, programming by demonstration, and learning from feature annotations by users.  \subhead{Weak Supervision} To reduce the cost of labeled data collection, weak supervision methods leverage noisy, limited, or low precision sources such as crowdsourcing, distant supervision, and user-defined heuristics to gather large training data for supervised learning. Data programming is a programmatic approach to weak supervision using heuristics, where labeling functions provided by domain experts are used to create training data at scale.      and train ML models using probabilistic labels.  \subhead{Program Synthesis by Demonstration} Automated synthesis of programs that satisfy a given specification is a classical artificial intelligence  problem. Generating programs by examples or demonstration is an instance of this problem. The terms programming by example , or programming by demonstration  have often been used interchangeably, though their adoption and exact meaning might diverge across fields and applications.  PBD systems aim to empower  end user programming in order to improve user productivity .    .  One of the core research questions in PBD is how to generalize from seen examples or demonstrations. To generalize, PBD systems need to resolve the semantic meaning of user actions over relevant  items.  Prior approaches incorporate a spectrum of user involvement, from making no inference  to using AI models with no or minimal user involvement, to synthesize a generalized program . Our framework takes a hybrid approach within the spectrum above and combines inference and statistical ranking along with interactive demonstration.   \subhead{Learning from Feature Annotations} Prior  work proposes methods for learning from user provided features, rationales, and natural language explanations. BabbleLabble uses a rule-based parser  to turn natural language explanations into labeling functions and aggregates these functions using Snorkel.  \system also learns labeling functions from high level imprecise explanations and aggregates them using the Snorkel framework. However, \system  enables users to supply their rationales through interactive visual demonstrations, reducing the cognitive load to formalize one's intuition.      We analyzed the value of romanization for transferring multilingual models to low-resource languages with different scripts. While we cannot recommend romanization as the default strategy for multilingual models and transfer learning across scripts because of the information loss inherent to it, we find that it benefits transfer between related languages that use different scripts. The \texttt{uconv} romanization tool outperforms \texttt{uroman} because it preserves more information encoded in the original script and consequently causes less information loss. Furthermore, we demonstrated that romanization can also be successful on the target side if followed by an additional, learned deromanization step. We hope that our results provide valuable insights for future work in transfer learning and practical applications for low-resource languages with unseen scripts.  
"," We build on prior work on weak supervision, programming by demonstration, and learning from feature annotations by users.  \subhead{Weak Supervision} To reduce the cost of labeled data collection, weak supervision methods leverage noisy, limited, or low precision sources such as crowdsourcing, distant supervision, and user-defined heuristics to gather large training data for supervised learning. Data programming is a programmatic approach to weak supervision using heuristics, where labeling functions provided by domain experts are used to create training data at scale.      and train ML models using probabilistic labels.  \subhead{Program Synthesis by Demonstration} Automated synthesis of programs that satisfy a given specification is a classical artificial intelligence  problem. Generating programs by examples or demonstration is an instance of this problem. The terms programming by example , or programming by demonstration  have often been used interchangeably, though their adoption and exact meaning might diverge across fields and applications.  PBD systems aim to empower  end user programming in order to improve user productivity .    .  One of the core research questions in PBD is how to generalize from seen examples or demonstrations. To generalize, PBD systems need to resolve the semantic meaning of user actions over relevant  items.  Prior approaches incorporate a spectrum of user involvement, from making no inference  to using AI models with no or minimal user involvement, to synthesize a generalized program . Our framework takes a hybrid approach within the spectrum above and combines inference and statistical ranking along with interactive demonstration.   \subhead{Learning from Feature Annotations} Prior  work proposes methods for learning from user provided features, rationales, and natural language explanations. BabbleLabble uses a rule-based parser  to turn natural language explanations into labeling functions and aggregates these functions using Snorkel.  \system also learns labeling functions from high level imprecise explanations and aggregates them using the Snorkel framework. However, \system  enables users to supply their rationales through interactive visual demonstrations, reducing the cognitive load to formalize one's intuition.",12
"      Deep neural networks are typically trained on a large amount of a single task data through a time-consuming optimization phase. This assumes that the distribution over data points is fixed. However, such neural models do not scale to complex, realistic environments and are prone to distributional shifts and adversarial data points. Online learning on the other hand does not make any distributional assumption and naturally involves an adversarial scenario. However, due to the larger number of training parameters and non-convex optimization landscape, the deep neural networks are hard to train in online settings.     % where the data points are made available over time in an streaming fashion.                \vskip -0.45in     \end{wrapfigure}          Meta-learning  has emerged as a promising technique for fast training of deep neural networks by acquiring and transferring knowledge across different tasks through a learned learning algorithm. This work proposes a meta-learning approach to learn sequential adaptation algorithms for deep neural networks. We introduce a sparse variant of Meta Networks to perform an online and continual fast adaptation of deep neural networks over a data stream with non-stationary distribution.           In Sparse Meta Networks , fast-weights are generated sparsely at each step by a meta-learner and accumulated across multiple steps. When the sparse fast-weights are accumulated in this way, across different tasks, they all together act as an mixture of multiple experts in a single Sparse-MetaNet model. Such sparsely generated recurrent fast-weights are not only computationally efficient; and thus can be applied with large scale deep neural networks, but also crucial to maintain a far past memory over the streaming data.           To demonstrate the effectiveness of our approach, we introduce a new vision based benchmark called Online Cifar. In the Online Cifar setup, Sparse-MetaNet shows a better flexibility and a less catastrophic interference, and achieves the best classification accuracy compared with gradient based baselines. We also evaluate Sparse-MetaNet on Wisconsin Card Sorting Test , a simple online reinforcement learning problem adapted from the human cognitive test and large scale language modelling benchmarks. When used along with Transformer-XL for adaptive language modelling, our Sparse-MetaNet achieves 1.00 bpc on enwik8 and 22.67 perplexity on WikiText-103 datasets, improving upon the original Transformer-XL result of 1.06 bpc and 24.0 perplexity, respectively.                 \vskip -0.45in     \end{wrapfigure}            There is a very broad line of work on online learning with convex functions. One of the earliest online learning methods is the Perceptron algorithm. Other more sophisticated methods are kernel Perceptron, Winnow, second order Perceptron and online gradient descent. The online gradient descent algorithm can readily be applied to deep neural networks to learn from a streaming data. However, multiple iterative passes over each data point is normally required due to the non-convex optimization landscape and the training of deep neural networks with the online gradient updates is usually difficult.          Recurrent neural networks with an external memory  system exhibit a  sequential adaptation capability when the data stream is provided in a custom format. The present work is along the similar line with an advantage that our Sparse-MetaNet approach is generic and can be applied to an arbitrary neural network architecture. More recently,  explored a similar setup as ours with a focus on online reinforcement learning in changing environments. Their approach involves maintaining a mixture of neural network models over the entire data stream and this can be quite expensive to scale to large neural networks. A more complex online meta-learning scenario was also introduced based on the MAML algorithm where the meta-learner is continually trained while we in this work focus on meta-learning a learned online learning algorithm that can generalize on unseen test data stream.          More broadly, our approach falls into the category of memory-based meta-learning. Meta-learning has been extensively studied in few-shot learning setup where a meta-learning algorithm assumes an access to  a distribution of tasks with a few labelled examples each.      Unlike online learning, task identities are known and the examples are not strictly provided in an sequential order in few-shot learning. Moreover, the online learning setup does not restrict the number of examples to be only a few. Therefore, a learned online learning algorithm should be capable of a fast adaptation when there is only a few examples available from a new distribution, but also scalable to a larger data regime to be able to continually improve its underlying model. The later can be challenging for learned gradient-based learning algorithms due to short horizon biases introduced during training. Specially, the learned learning rate, an important hyper-parameter for gradient-based training does not generalize across different data regimes.          Continual learning investigates the catastrophic interference in neural networks in a similar setup as online learning. However, the prior continual learning algorithms have mainly been focused on a small handful set of tasks. The Online Cifar benchmark introduced in this work offers an interesting testbed to evaluate continual learning approaches on a large task stream.    Accessibility is a key to wider adoption of any technology and machine learning is no exception. Here  we introduced  data programming by demonstration , a general human-in-the-loop framework that aims to ease writing labeling functions, improving the accessibility and efficiency of data programming.  We then presented \system, a DPBD system, for easily generating labeling functions to create training datasets for document-level classification tasks. \system converts user rationales interactively expressed as span-level annotations and relations among them to labeling rules using the DPBD framework. We also reported our progress in developing \tagruler, a second DPBD system focusing on labeling functions for tagging.   Through a user study with 10 data scientists performing real world labeling tasks for classification, we evaluated \system together with conventional data programming and found that \system enables more accessible data programming without loss in the performance of labeling models created. We believe DPBD systems will be useful for data scientists as well as subject matter experts. We release \system as open source software to support future applications and extended research.   \system prioritizes accessibility over expressivity. Is this trade-off inevitable? The expressivity of \system can be enhanced by extended semantic and syntactic analysis of the document  context of user demonstrations. Enabling manual revision of synthesized labeling functions at multiple levels of abstraction can be also useful. In this context, further improving the expressivity of \system through use cases without diminishing its accessibility is an important area of future research. Deriving additional insights into how users  with limited or no  programming proficiency  would use \system is another area of future work, and open sourcing \system is a step forward in this direction. Future research also includes developing fast search and ranking algorithms and experimenting with different active learning strategies to effectively search and navigate the vast joint space of labeling functions and data examples.     In this paper we presented \system, a data programming by demonstration  system for easily generating labeling functions to create training datasets for document-level classification tasks. \system converts user rationales interactively expressed as span-level annotations and relations to  labeling rules using the DPBD framework.  DPDB is a general  human-in-the-loop framework that aims to ease writing labeling functions, improving the accessibility and efficiency of data programming.  Through a user study with 10 data scientists performing real world labeling tasks for classification, we evaluated \system together with conventional data programming and found that \system enables more accessible data programming without loss in the performance of labeling models created. Results of our study  also suggested that, even for  skilled programmers, the majority of functions they write can be captured more easily through visual interactions using our system. We release \system as an open source software to support future applications and extended research.  \section{Evaluation} We evaluate \system alongside manual data programming using Snorkel. Our goal is to better understand the trade-offs afforded by each method. To this end, we conducted a user study with data scientists and measured  their task performance accuracy in completing two labeling tasks. In addition to task performance, we also analyzed the accessibility and  expressivity of both methods using the qualitative feedback elicited from participants.   Note that \system can be used by programmers and non-programmer domain experts alike, but a fair comparison with Snorkel requires proficiency in conventional programming.   \subhead{Participants}  We recruited 10 participants with Python programming experience through our professional network. All participants had significant programming experience . Their experience with Python programming ranged from  to  years with an average of  years .    \subhead{Experimental Design}  We carried out the study using  a within-subjects experiment design, where all participants performed tasks using both conditions .  The sole independent variable controlled was the method of creating labeling functions. We counterbalanced the order in which the tools were used, as well as which classification task we performed with which tool.   \subhead{Tasks and Procedure} We asked participants to write  labeling functions for two prevalent labeling tasks: spam detection and sentiment classification.  They performed these two tasks on  YouTube Comments and Amazon Reviews, respectively. Participants received 15 mins of instruction on how to use each tool, using a topic classification task  over a newsgroup dataset as an example. We asked participants to write as many functions as they considered necessary for the goal of the task.  There were given 30 mins to complete each task and we recorded the labeling functions they created and these functions' individual and aggregate performances.  After completing both tasks, participants also filled out an exit survey, providing their qualitative feedback.  For the manual programming condition, we provided a Jupyter notebook interface based on the Snorkel tutorial. The notebook had a section for writing functions, a section with diverse analysis tools, and a section to train a logistic regression model on the labels generated.    \section{Evaluation}   We evaluate \system alongside manual data programming using Snorkel.   Our goal is to better understand the trade-offs afforded by each method. To this end, we conducted a user study with 10 data scientists and measured  their task performance accuracy in completing two labeling tasks. In addition to task performance, we also analyzed the accessibility and  expressivity of both methods using the qualitative feedback elicited from participants.     \subhead{Participants}  We recruited participants with Python programming experience through our professional network . Note that \system can be used by programmers and non-programmer domain experts alike, but a fair comparison with Snorkel requires proficiency in conventional programming.  All participants had significant programming experience . Their experience with Python programming ranged from  to  years with an average of  years .      \subhead{Experimental Design}  We carried out the study using  a within-subjects experiment design, where all participants performed tasks using both conditions .  The sole independent variable controlled was the method of creating labeling functions. We counterbalanced the order in which the tools were used, as well as which classification task we performed with which tool.     \subhead{Tasks and Procedure} We asked participants to write  labeling functions for two prevalent labeling tasks: spam detection and sentiment classification.  They performed these two tasks on  YouTube Comments and Amazon Reviews, respectively. Participants received 15 mins of instruction on how to use each tool, using a topic classification task  over a newsgroup dataset as an example. We asked participants to write as many functions as they considered necessary for the goal of the task.  There were given 30 mins to complete each task and we recorded the labeling functions they created and these functions' individual and aggregate performances.  After completing both tasks, participants also filled out an exit survey, providing their qualitative feedback.    For the manual programming condition, we iteratively developed a Jupyter notebook interface based on the Snorkel tutorial. We provided a section for writing functions, a section with diverse analysis tools, and a section to train a logistic regression model on the labels they had generated.     .      \section{Evaluation}   We evaluate our framework against a baseline of manual programming of labeling functions . Our primary goal is to better understand the trade offs afforded by each method based on the quantitative performance and the qualitative feedback by participants. To this end, we conducted a user study with \thenum participants and measured their task performance accuracy in two labeling tasks on two different corpora, YouTube Comments and Amazon Reviews. In addition to task performance, we also analyzed the accessibility, expressivity, and interpretability of both methods using the qualitative feedback elicited from participants and our observations gathered during the study sessions.     \subhead{Experimental Design} We wanted to be sure that each user had an opportunity to try both tools so that they could fairly compare the two, while still minimizing knowledge transfer between tasks. We also wanted to evaluate these methods on different types of tasks.  To achieve this, we divided the participant pool into two random groups each with five participants in each. We then randomly assigned task/tool pairings to each group.  To avoid ordering effects, we counterbalanced the presentation of tasks within each group.     The sole independent variable we controlled was the method of creating labeling function,  which had two conditions; \snorkel, and \system. Note that these two tools  correspond to two different forms of creating labeling functions, manual , and using visual interactive demonstration, respectively.      Babble Labble mention   In a pilot version of the study we also tested Babble Labble, a system for generation labeling functions from natural language explanations. In general our participants performed worse and found the system less expressive, which is why we have omitted it from this study. Our takeaway is that Babble may be useful for collecting functions at scale from, for example, crowdsource workers, but is less suited for an individual machine learning engineer or domain expert.     \subhead{Participants}  For a fair comparison, we wanted to make sure participants were skilled programmers, as well as familiar with training machine learning models and able to interpret statistics like precision and recall. Because of the difficulty of recruiting subjects with this skill set, we recruited participants who are employees or interns from our lab.  None of the participants were involved in this work.    Although we believe Ruler can be used by programmers and non-programmer domain experts alike, because the purpose of this study was to compare Ruler to existing methods, we only recruited programmers skilled at Python.     Participants had either ``research scientist'' or  ``software engineer''  as a job title. Five of them held PhDs and one held BS, all in computer science.     \todo{Mention titles, prev experience of participants}    All participants had significant programming experience . Their experience with Python programming ranged from  to  years with an average of  years .  Only two participants had used data programming in the past, but all had experience training supervised models and collecting training data.    \subhead{Tasks and Data} We asked participants to write  labeling functions for two prevalent labeling tasks, spam detection and sentiment classification.  They performed these two tasks on  YouTube Comments and Amazon Reviews, respectively. We asked participants to write as many functions as they considered necessary for the goal of the task.  They were given 30 mins to complete each task. Participants were also tutored for 15 mins on writing labeling functions using a topic  classification  task on a newsgroup dataset.     \subhead{Procedure}  Before the experiment began, users were asked to complete a questionnaire that elicited information on their educational background and programming and model development experiences. This way we could ensure that our treatment groups were reasonably balanced across several dimensions.     Each user was scheduled to complete two sessions, never on the same day  and not more than 3 days apart. These sessions were conducted over zoom and began with a 15 minute tutorial to learn how to use the tool by practicing on the newsgroup dataset.   Next, the user was given 30 minutes to complete their assigned task. They were allowed to ask questions and access the internet as desired.   Before each tutorial and task, the user was given as much time as they wanted to read a task description consisting of a short paragraph describing the task and 5 examples from each class.     We cannot expect a 30 minute experiment to be a realistic representation of what generating training data labels is like, but it is likely a good approximation of what the first 30 minutes of generating training data is like. Given constrained resources, we consider this the best method of evaluation.    Throughout each task, we recorded the labeling functions created by participants and these functions' individual and aggregate performances on each task. At the end of the session, participants completed an exit survey to provide their qualitative feedback. After the second such session, the user was asked to complete a final survey comparing the two tools.    For instance,  if spam detection was assigned to be the first task to be completed for a user, and Snorkel the first tool, then in the first session the participant would complete a Snorkel tutorial, then the spam detection task using Snorkel and a survey. On a later day, they would complete a Ruler tutorial, followed by the sentiment analysis task using Ruler, then a survey about Ruler, and a survey comparing the two tools.      \cagatay{If space permits, consider adding a figure illustrating the experiment flow.}  \sara{consider adding figure showing users' experience}\section{DPBD Framework} \stitle{Problem Statement} Given a dataset  of data records and a set of labels ,  we aim to develop a framework that enables human labelers to interactively assign a label from  for each data record efficiently sampled from   , while demonstrating their rationales for label assignments through visual interaction. Given a triplet  of a data record, a visual interaction from the labeler, and the label assigned,  we want this framework to effectively synthesize and propose labeling rules  for the labeler to choose from.  Finally, we want the framework to optimally aggregate all the chosen rules  in order to create a labeled training set from   with probabilistic labels in order to subsequently train discriminative models on it.  \stitle{Framework Overview} The data programming by demonstration  framework   has two input sources: the human labeler, and the data that is to be labeled. The labeler is the subject matter expert who has sufficient domain understanding to extract useful signals from data. Given a dataset, our framework enables the labeler to label each record with a categorical label, while providing their labeling rationales by interactively marking relevant parts of the record and specifying semantics and relationships among them. The output is a labeling model, which is trained to automatically produce labels for the large set of unlabeled data. The DPBD framework has four main components, labeling interface,  synthesizer, modeler, and active sampler.    The labeler interacts with data via the labeling interface. The labeling interface    records the labeler's interaction and compiles the interaction into a labeling rule. The    synthesizer synthesizes labeling rules and translates those chosen by the labeler into   program functions. Third, the selected functions are passed to the modeler, which    builds a labeling model by optimally aggregating the generated functions. Until a certain    stopping criterion is met  or the labeler decides to exit,   the active sampler selects the next data record to present the labeler.     In the rest of this section, we describe the details of each component.   The labeling interface is the workplace where the labeler encodes domain knowledge into labeling rules. It provides a way to express noisy explanations for labeling decisions using a visual interaction language,  which allows the user to express domain knowledge without having to formalize their ideas into computer programs or natural language explanations. This allows for more focus on patterns in the data while abstracting away any implementation concerns.  \stitle{Generalized Labeling Model} Inspired by the entity-relationship model in database modeling, the generalized labeling model  models the data records with concepts and relationships.  The GLM views the data record as a series of tokens, where a token is a continuous subset of a record with no semantics attached.  For example, in text data, a token can be any span  of the data record; in an image data record, it would be a 2D region, rectangular or free form; and in an audio data record, it would be a 1D window of the data record .  A concept is a group of tokens that the labeler believes share common semantics. For instance, over text data, the labeler might define a concept of positive adjectives consisting of a set of tokens, each of which can imply a positive review.  When labeling audio data, the labeler might create a concept to aggregate all clips that express excitement, or of a specific speaker.  This abstraction allows the user to teach the GLM which generalizations are relevant to the task. A relationship represents a binary correlation between token-token, token-concept, or concept-concept. Some examples are membership , co-existence , and positional .     \stitle{Mapping GLM Elements to Operations} Given the GLM specification described above, our framework also defines the operations that can be applied on GLM elements. Table lists the GLM elements and the corresponding operations. The implementation of both the labeling interface and the operations described in Table would vary across data types and token definitions. To add expressivity, the GLM may also perform transformations over the set of tokens, as we describe in the next section.   \stitle{Compiling Operations into Labeling Rules} Once the labeler finishes annotating an example using the provided operations, and selects a label, the tokens are extracted from the annotation and used  as the initial set of conditions from which to build rules. The synthesizer combines these conditions into labeling rules by selecting subsets of the conditions to be combined with different conjunctive formulas, according to the relationships the user has annotated. The synthesizer extends the initial set of labeling rules and presents the extended labeling rules for the labeler to select from, choosing desired ones based on  domain knowledge.  A labeling rule serves as an intermediate language, interpretable by both the labeler and the synthesizer. In our framework, we adopt the notation of domain relational calculus to represent these rules, which can be expressed as:  . The variable \texttt{tokens} is a sequence of tokens with existential quantification, and  \texttt{conditions} is a conjunctive formula over boolean predicates that is tested over \texttt{tokens} on a data record.   The predicates are first-order expressions, and each can be expressed as a tuple .  is an optional transformation function on a token identifier, a process of mapping the raw token to more generalized forms. Some example transformations are word lemmatization in text labeling, speech-to-text detection in audio labeling, or object recognition in image labeling.    is a token, while  is can be either token, literal or a set. If  denotes a token, the transformation function  may also apply to .   is an operator whose type depends on the type of .  If  is a token or literal,   detects a positional or an equality relationship. Otherwise, if  is a set,  is one of the set operators  . Since the \texttt{conditions} is in the conjunctive form, the order of labeler's interactions does not matter.  \example{ Consider the following review for the binary sentiment classification  task:  \texttt{This book was so great! I loved and read it so many times that I will soon have to buy a new copy.}   If the labeler thinks this data record has a positive sentiment, she can express her decision rationale using GLM. First, she may select two tokens that are related to the sentiment: \texttt{book} and \texttt{great}. Assume there are two concepts the labeler previously created:  \texttt{itembook, electronics}; and  \texttt{padjwonderful}. The labeler realizes the token \texttt{great} can be generalized by the \texttt{padj} concept, which means that the labeling rule will still be valid if this token is replaced by any tokens in the concept, so she adds this token to the concept.  Finally, the labeler creates a positional relationship from \texttt{book} to token \texttt{great} to indicate that they appear in the same sentence, before completing the labeling process. These operations compile into the labeling rule . }  This rule is sent to the synthesizer for expansion and program synthesis.   Given the compiled labeling rule from the labeling interface, the synthesizer extends one single labeling rule from labeler's interaction to a set of more general labeling rules; and translates those labeling rules into computer programs. It is straightforward to translate the rules into executable computer programs , so in this section, we focus on how to synthesize the extended labeling rules.  Given the labeling rule compiled from a labeler's interaction, the synthesizer generates more labeling rules while optimizing two competing goals: maximizing generalization, so that more  data can be accurately labeled; and maximizing the coverage of the labeler's interaction, simply because labeler's interaction is the most valuable signal for labeling based on the domain knowledge. Of course, the larger the set of annotations in an interaction, the larger the set of labeling functions that can be synthesized. To keep rule selection as easy as possible for the user, in this case we prioritize rules that cover more of the interaction, assuming that there is little redundancy.  in the labeler's interaction.  We achieve generalization of the given rules using the following heuristics:  substituting tokens with concepts;  replacing general coexistence relationships with position-specific ones; and  applying the available transformations over the tokens .    Since the labeling rule in GLM has conjunctive conditions, Algorithm  generalizes each predicate in the conditions.    Inside, Line to Line substitute token with concept.   Line can be implemented explicitly by matching token to concept set, as well as sophisticated data-dependent processing via transformation .   For example, in our system for text labeling , in addition to matching values with labeler defined concepts, we also apply named-entity recognition  where the named-entities are implicit concepts that a token can be a member of.    Line to Line replace the positional with co-occurrence relationship by removing the condition that specifies the positional context.   The conditions for extended labeling rules is a conjunctive combination of single predicates, one from each extended condition set .   In addition, for special cases of binary labeling, the algorithm also considers the rule which flips over the label by adding negation to the conditions .     Once the extended rules are generated, the rules are ranked by their generalization score---a measurement of how applicable a certain rule is. We define a data-independent generalization score for a labeling rule  as: .     Intuitively,  is calculated by counting how many different data instances that  can be used.  It prefers labeling rules using large sets to match tokens in the data record.  \example{ Continuing with our Amazon review example, the synthesizer can derive the following labeling rules from  using these heuristics:   Labeling rule~ is generated using heuristics  and .  Labeling rule~ and~ are synthesized by using heuristics  and , respectively. Note that labeling rule~ is more general than~ and~ because all data records that can be labeled by~ and~ will be labeled the same way using labeling rule~.  Labeling rules~ are due to flipping over the binary label with heuristics .  }   Once the extended labeling rules are generated, the labeler can help confirm the validity in order to achieve faster convergence. The top-k candidates ranked by the generalization score are displayed in the labeling interface for the labeler to accept or reject.   The modeler component trains a model that can be used to automatically annotate unlabeled datasets. Naively aggregating the labeling functions  can be either inaccurate , or does not scale with a large set of unlabeled data. Instead, the modeler encapsulates the ideas from traditional data programming to first build a generative model to denoise labeling functions, and then train a discriminative model to leverage other features beyond what are expressed by the labeling functions.   To improve the model quality at faster rates, our framework uses an active sampler to choose the next data record for labeling.  The active sampler can be plugged in with any custom active learning policy.  By default, it selects the data record  with the highest entropy :   where  is the probability that example  belongs to class , as predicted by the trained label model.     \section{DPBD Framework}   \stitle{Problem Statement}   Given a dataset  of data records and a set of labels ,  we aim to develop a framework that enables human labelers to interactively assign a label from  for each data record efficiently sampled from   , while demonstrating their rationales for label assignments through visual interaction. Given a triplet  of a data record, a visual interaction from the labeler, and the label assigned,  we want this framework to effectively synthesize and propose labeling rules  for the labeler to choose from.    Finally, we want the framework to optimally aggregate all the chosen rules  in order to create a labeled training set from   with probabilistic labels in order to subsequently train discriminative models on it.    \stitle{Framework Overview}   The data programming by demonstration  framework   has two input sources: the human labeler, and the data that is to be labeled. The labeler is the subject matter expert who has sufficient domain understanding to extract useful signals from data. Given a dataset, our framework enables the labeler to label each record with a categorical label, while providing their labeling rationales by interactively marking relevant parts of the record and specifying semantics and relationships among them.   The output is a labeling model, which is trained to automatically produce labels for the large set of unlabeled data.      Inherited from the traditional data programming~, our framework also assumes that a set of labeled data is available for tuning model hyperparameters.   The DPBD framework has four main components. The labeler interacts with data via the labeling interface. The labeling interface records the labeler's interaction and compiles the interaction into a labeling rule.   The synthesizer synthesizes labeling rules and translates those chosen by the labeler into program functions. Third, the selected functions are passed to the modeler, which builds a labeling model by optimally aggregating the generated functions. Until a certain stopping criterion is met  or the labeler decides to exit, the active sampler selects the next data record to present the labeler.       In the rest of this section, we describe the details of each component.       The labeling interface is the workplace where the labeler encodes domain knowledge into labeling rules. It provides a way to express noisy explanations for labeling decisions using a visual interaction language,  which allows the user to express domain knowledge without having to formalize their ideas into computer programs or natural language explanations. This allows for more focus on patterns in the data while abstracting away any implementation concerns.       Inspired by the entity-relationship model in database modeling, the generalized labeling model  models the data records with concepts and relationships.  The GLM views the data record as a series of tokens,   where a token is a continuous subset of a record with no semantics attached.    For example, in text data, a token can be any span  of the data record; in an image data record, it would be a 2D region, rectangular or free form; and in an audio data record, it would be a 1D window of the data record .      A concept is a group of tokens that the labeler believes share common semantics. For instance, over text data, the labeler might define a concept of positive adjectives consisting of a set of tokens, each of which can imply a positive review.    When labeling audio data, the labeler might create a concept to aggregate all clips that express excitement, or of a specific speaker.  This abstraction allows the user to teach the GLM which generalizations are relevant to the task.    A relationship represents a binary correlation between token-token, token-concept, or concept-concept. Some examples are membership , co-existence , and positional .           \subhead{Mapping GLM Elements to Operations}   Given the GLM specification described above, our framework also defines the operations that can be applied on GLM elements. Table lists the GLM elements and the corresponding operations. The implementation of both the labeling interface and the operations described in Table would vary across data types and token definitions. To add expressivity, the GLM may also perform transformations over the set of tokens, as we describe in the next section.     \subhead{Compiling Operations into Labeling Rules}   Once the labeler finishes annotating an example using the provided operations, and selects a label, the tokens are extracted from the annotation and used    as the initial set of conditions from which to build rules.   The synthesizer combines these conditions into labeling rules by selecting subsets of the conditions to be combined with different conjunctive formulas, according to the relationships the user has annotated.   The synthesizer extends the initial set of labeling rules and presents the extended labeling rules for the labeler to select from, choosing desired ones based on  domain knowledge.    A labeling rule serves as an intermediate language, interpretable by both the labeler and the synthesizer. In our framework, we adapt the notation of domain relational calculus to represent these rules, which can be expressed as:    .   The variable \texttt{tokens} is a sequence of tokens with existential quantification, and    \texttt{conditions} is a conjunctive formula over boolean predicates that is tested over \texttt{tokens} on a data record.     The predicates are first-order expressions, and each can be expressed as a tuple .  is an optional transformation function on a token identifier, a process of mapping the raw token to more generalized forms. Some example transformations are word lemmatization for text labeling, speech-to-text detection in audio labeling, or object recognition in image labeling.      is a token, while  is can be either token, literal or a set.   If  denotes a token, the transformation function  may also apply to .     is an operator whose type depends of the type of .  If  is a token or literal,   detects a positional or an equality relationship. Otherwise, if  is a set,  is one of the set operators  . Since the \texttt{conditions} is in the conjunctive form, the order of labeler's interactions does not matter.    \example{   Consider the binary sentiment classification  task on Amazon review data. Observe the following review:    \texttt{This book was so great! I loved and read it so many times that I will soon have to buy a new copy.}     If the labeler thinks this data record has a positive sentiment, she can express her decision rationale using GLM.   First, she may select two tokens that are related to the sentiment: \texttt{book} and \texttt{great}. Assume there are two concepts the labeler previously created:  \texttt{itembook, electronics}; and    \texttt{padjwonderful}. The labeler realizes the token \texttt{great} can be generalized by the \texttt{padj} concept, which means that the labeling rule will still be valid if this token is replaced by any tokens in the concept, so she adds this token to the concept.    Finally, the labeler creates a positional relationship from \texttt{book} to token \texttt{great} to indicate that they appear in the same sentence, before completing the labeling process.   These operations compile into the labeling rule   .   }    This rule is sent to the synthesizer for expansion and program synthesis.        Given the compiled labeling rule from the labeling interface, the synthesizer extends one single labeling rule from labeler's interaction to a set of more general labeling rules; and translates those labeling rules into computer programs.   It is straightforward to translate the rules into executable computer programs , so in this section, we focus on how to synthesize the extended labeling rules.    Given the labeling rule compiled from a labeler's interaction, the synthesizer generates more labeling rules while optimizing two competing goals: maximizing generalization, so that more  data can be accurately labeled; and maximizing the coverage of the labeler's interaction, simply because labeler's interaction is the most valuable signal for labeling based on the domain knowledge.   Of course, the larger the set of annotations in an interaction, the larger the set of labeling functions that can be synthesized. To keep rule selection as easy as possible for the user, in this case we prioritize rules that cover more of the interaction, assuming that there is little redundancy.  in the labeler's interaction.    We achieve generalization of the given rules using the following heuristics:    substituting tokens with concepts;  replacing general co-existence relationships with position-specific ones; and  applying the available transformations over the tokens .      Since the labeling rule in GLM has conjunctive conditions, Algorithm  generalizes each predicate in the conditions.      Inside, Line to Line substitute token with concept.     Line can be implemented explicitly by matching token to concept set, as well as sophisticated data-dependent processing via transformation .     For example, in our system for text labeling , in addition to matching values with labeler defined concepts, we also apply named-entity recognition  where the named-entities are implicit concepts that a token can be a member of.      Line to Line replace the positional with co-occurrence relationship by removing the condition that specifies the positional context.     The conditions for extended labeling rules is a conjunctive combination of single predicates, one from each extended condition set .     In addition, for special case of binary labeling, the algorithm also considers the rule which flips over the label by adding negation to the conditions .         Once the extended rules are generated, the rules are ranked by their generalization score---a measurement of how applicable a certain rule is.   We define a data-independent generalization score for a labeling rule  as: .         Intuitively,  is calculated by counting how many different data instances that  can be used.    It prefers labeling rules using large sets to match tokens in the data record.    \example{   Continuing with our Amazon review example, the synthesizer can derive the following labeling rules from  using these heuristics:       Labeling rule~ is generated using heuristics  and .  Labeling rule~ and~ are synthesized by using heuristics  and , respectively.   Note that labeling rule~ is more general than~ and~ because all data records that can be labeled by~ and~ will be labeled the same way using labeling rule~.    Labeling rules~ are due to flipping over the binary label with heuristics .    }     Once the extended labeling rules are generated, the labeler can help confirm the validity in order to achieve faster convergence.   The top-k candidates ranked by the generalization score are displayed in the labeling interface for the labeler to accept or reject.       The modeler component trains a model that can be used to automatically annotate unlabeled datasets.   Naively aggregating the labeling functions  can be either inaccurate , or does not scale with large set of unlabeled data.    This is simply because labeling functions are noisy: they may overlap, conflict and even depends with each other, and can only provide limited signals in weak supervision.   Instead, the modeler encapsulates the ideas from traditional data programming to first build a generative model to denoise labeling functions, and then train a discriminative model to leverage other features beyond what are expressed by the labeling functions.       To improve the model quality at faster rates, our framework uses an active sampler to choose the next data record for labeling.    The active sampler can plug in any custom active learning policy.  By default, it selects the data record  with the highest    entropy :   where  is the probability that example  belongs to class , as predicted by the trained label model. \section{Introduction} Machine learning  models used in practice today are predominantly supervised models and rely on large datasets labeled for training. However, the cost of collecting and maintaining labeled training data remains a bottleneck for training high-capacity supervised models. Data programming aims to address the difficulty of collecting labeled data by using a programmatic approach to weak supervision by heuristics, where domain experts are expected to provide data programs  incorporating their domain knowledge. Prior work on data programming focuses on modeling and aggregating labeling functions written manually or generated automatically to denoise labeling functions.    However, little is known about user experience    in writing labeling functions and how to improve it.   Writing data programs can be, however, challenging and time consuming.  Most domain experts or lay users have no or little programming literacy, and even for those who are proficient programmers, it is often difficult to convert domain knowledge to a set of rules by writing programs.          By extending data programming with programming by example, we bridge the gap between scalable training data generation and domain experts. To address these challenges, we introduce data programming by demonstration ,  a new framework that aims to make creating labeling functions  easier by learning them from users' interactive visual demonstrations. DPBD moves the burden of writing labeling functions to an intelligent synthesizer while enabling users to steer the synthesis process at multiple semantic levels, from providing rationales relevant for their labeling choices to interactively filtering the proposed functions. DPBD draws from two lines of prior research; programming by demonstration  or example , e.g.,, which aims to make programming easier by synthesizing them based on user interactions or input and output examples, and  interactive learning from user-provided features or rationales .    We operationalize our framework with \system, an interactive system that enables more accessible data programming to create labeled training datasets for document classification. \system automatically generates  document level labeling rules from  span-level annotations and their relations on specific examples provided by users. Through a user study conducted with  10 data scientists, we evaluate  \system alongside manual data programming using Snorkel. We measure the predictive performances of models created by participants for two  common labeling tasks, sentiment classification and spam detection. We also elicit ratings and qualitative feedback from participants on multiple measures, including  ease of use, ease of learning, expressivity, and overall satisfaction.  We find \system facilitates more accessible creation of labeling functions without a loss in the quality of learned labeling  models.   Tagging or token level classification in text documents is another widely used task that can benefit from DPBD. Here we also briefly discuss our work in progress on \tagruler, a DPBD system that learns token labeling functions through user interaction to create training datasets for tagging models.     Tagging or span-level classification in text documents is another widely used task that can benefit from DPBD. Here we also briefly discuss our work in progress on \tagruler, a DPBD system that enables the interactive generation of token labeling functions in order to create labeled training data for tagging models.         On the other hand, \tagruler synthesizes token classification  rules based users.   In summary, we contribute  DPBD, a general data independent framework for learning labeling rules by interactive demonstration;  \system, an interactive system operationalizing our framework for document classification tasks; and  a comparative user study conducted with data scientists in performing real world tasks to evaluate \system and conventional data programming. We have made our research artifacts, including the \system code and demo, publicly available~   where to search for the images   Copyright \setcopyright{none} \acmConference[]{}{}        Submission ID.    Use this when submitting an article to a sponsored event. You'll    receive a unique submission ID from the organizers    of the event, and this ID should be used as the parameter to this command.   \acmSubmissionID{123-A56-BU3}        The majority of ACM publications use numbered citations and    references.  The command \citestyle{authoryear} switches to the    ""author year"" style.       If you are preparing content for an event    sponsored by ACM SIGGRAPH, you must use the ""author year"" style of    citations and references.    Uncommenting    the next command will enable that style.   \citestyle{acmauthoryear}        end of the preamble, start of the body of the document source.  Conference   \acmYear{1997}  \copyrightyear{2016}    \acmArticle{4}  \acmPrice{15.00}     These commands are optional   \acmBooktitle{Transactions of the ACM Woodstock conference}  \editor{Jennifer B. Sartor}  \editor{Theo D'Hondt}  \editor{Wolfgang De Meuter} \definecolor{tomato}{rgb}{1,0.2,0} \definecolor{turqoise}{rgb}{0.03, 0.91, 0.87} \definecolor{grey}{rgb}{0.4,0.4,0.4} \newif\ifnotes \notestrue \DeclareRobustCommand{\cagatay}[1]{\ifnotes{\small[{\c{C}a\u{g}atay:}{#1}]}\fi} \DeclareRobustCommand{\sara}[1]{\ifnotes{\small[{Sara:}{#1}]}\fi} \DeclareRobustCommand{\subhead}[1]{#1} \DeclareRobustCommand{\system}{\mbox{\sc Ruler}\xspace} \DeclareRobustCommand{\ruler}{\mbox{\sc Ruler}\xspace} \DeclareRobustCommand{\tagruler}{\mbox{\sc TagRuler}\xspace}  \DeclareRobustCommand{\snorkel}{\mbox{\sc Snorkel}\xspace} \DeclareRobustCommand{\babblelabble}{\mbox{\sc BabbleLabble}\xspace} \DeclareRobustCommand{\thenum}{ten\xspace} \newcommand{\eat}[1]{} \newcommand{\example}[1]{{\underline{Example:} #1\qed}} \newcommand{\stitle}[1]{\smallskip {#1}} \newcommand{\sstitle}[1]{\smallskip {\underline{#1}}} \DeclareRobustCommand{\subhead}[1]{#1}  \newcommand{\squishlist}{      }      \renewcommand{\shortauthors}{} \settopmatter{printacmref=false,printfolios=true,printccs=false}  \begin{document}  \title[]{Data Programming by Demonstration:\\A Framework for Interactively Learning Labeling Functions}  \author{Sara Evensen} \affiliation{    \institution{Megagon Labs} }   \email{} \author{Chang Ge} \authornote{Work done during internship at Megagon Labs.} \affiliation{    \institution{University of Waterloo} }   \email{} \author{Dongjin Choi} \authornotemark[1] \affiliation{   \institution{Georgia Tech} }   \email{} \author{\c{C}a\u{g}atay Demiralp} \affiliation{   \institution{Megagon Labs} }   \email{}    \renewcommand{\shortauthors}{B. Trovato et al.}    \renewcommand{\shortauthors}{Evensen et al.}      \keywords{}  \maketitle  \pagestyle{plain}                    
","     There is a very broad line of work on online learning with convex functions. One of the earliest online learning methods is the Perceptron algorithm. Other more sophisticated methods are kernel Perceptron, Winnow, second order Perceptron and online gradient descent. The online gradient descent algorithm can readily be applied to deep neural networks to learn from a streaming data. However, multiple iterative passes over each data point is normally required due to the non-convex optimization landscape and the training of deep neural networks with the online gradient updates is usually difficult.          Recurrent neural networks with an external memory  system exhibit a  sequential adaptation capability when the data stream is provided in a custom format. The present work is along the similar line with an advantage that our Sparse-MetaNet approach is generic and can be applied to an arbitrary neural network architecture. More recently,  explored a similar setup as ours with a focus on online reinforcement learning in changing environments. Their approach involves maintaining a mixture of neural network models over the entire data stream and this can be quite expensive to scale to large neural networks. A more complex online meta-learning scenario was also introduced based on the MAML algorithm where the meta-learner is continually trained while we in this work focus on meta-learning a learned online learning algorithm that can generalize on unseen test data stream.          More broadly, our approach falls into the category of memory-based meta-learning. Meta-learning has been extensively studied in few-shot learning setup where a meta-learning algorithm assumes an access to  a distribution of tasks with a few labelled examples each.      Unlike online learning, task identities are known and the examples are not strictly provided in an sequential order in few-shot learning. Moreover, the online learning setup does not restrict the number of examples to be only a few. Therefore, a learned online learning algorithm should be capable of a fast adaptation when there is only a few examples available from a new distribution, but also scalable to a larger data regime to be able to continually improve its underlying model. The later can be challenging for learned gradient-based learning algorithms due to short horizon biases introduced during training. Specially, the learned learning rate, an important hyper-parameter for gradient-based training does not generalize across different data regimes.          Continual learning investigates the catastrophic interference in neural networks in a similar setup as online learning. However, the prior continual learning algorithms have mainly been focused on a small handful set of tasks. The Online Cifar benchmark introduced in this work offers an interesting testbed to evaluate continual learning approaches on a large task stream.",13
"  The advent of open-source software and question and answering websites contributed for improving the way developers produce code. Nowadays, code search permeates the development activities. Developers can spend 15\% of their time searching online for how a piece of code works, how to fix a bug, and how to use an API . According to \citet{sadowski-how-developers-search-for-code-case-study:2015}, at Google, developers search for code 12 times a day, clicking on 2 to 3 results in average per search session.    Most developers use general-purpose search engines  to look for code , which uses page rank and other indexes tactics that are not optimized for searching code. Then, general-purpose search engines do not adequately find code snippets unless they have accompanying descriptions. According to \citet{masudur-developers-use-google-code-retrieval:2018}, developers spend more time, visit more pages, and change queries more often when they are doing code-related searches. In particular, newcomers to a project can greatly benefit from semantic search since they face a variety of entrance barriers .   GitHub, a popular source code hosting platform, has attempted to build a semantic code search. They extracted millions of lines of code from its repositories and matched each code snippet to a docstring. The final results were not satisfactory as the tool could find a relevant code snippet only if the user provided a query that matched the docstring description . According to \citet{cambronero-deep-code-search-2019}, users' intents were better matched to questions collected from question-answering sites related to programming, e.g., Stack Overflow. Those sites allow users to ask a question and approve the best answer for it. Other users vote for the most helpful answer and mark the wrong or not helpful ones. Those collective actions curate and organize information.  Initial code search studies were based on deductive-logic rules and manually extracted features . The recent success of artificial neural networks has shifted recent works to a machine learning-based approach. \citet{cambronero-deep-code-search-2019} coined a name, neural code search, i.e., code search based on neural networks.  Recent works applied neural networks to summarize and retrieve code snippets. \citet{cambronero-deep-code-search-2019} proposed a neural network with attention mechanism and \citet{Gu-deep-code-search:2018} presented a recurrent neural network. Our novel approach is based on Convolutional Neural Networks . For the best of our knowledge, CNNs have not yet been used to search for code, but have achieved good results in selecting answers . CNNs prioritize local interactions  and its translation invariant, which are important traits for our task.   In our study, we answer the following research questions:        We summarize the difference between our work and related work in Table. Most works differ on how they combine the word embeddings to obtain a sentence embedding. Recent works  used Skip-gram  and adopted simpler architectures for sentence embedding than previous work .         Previous work  that used GitHub data extracted the methods from the source code and matched them to docstring descriptions. Works that used Stack Overflow data  paired question titles to the accepted answers' code snippet. For the search corpus, some works  adopted a GitHub corpus with millions of pieces of code, while others  retrieved code snippets from a small sample of 50 randomly selected code snippets. These studies  are commonly evaluated using questions collected from Stack Overflow.   Although our architecture is more complex, since it requires more parameters and time to train, than \citet{cambronero-deep-code-search-2019}'s  architecture , we can train our model offline. We matched question title to code snippets collected from Stack Overflow following \citet{Allamanis-bimodal-source-code-natural-language:2015} and \citet{iyer-etal-2016-summarizing}  work.     The only universal learning algorithm that we are aware of is how humans learn. Human learning is robust and flexible -- it relies on causality, has an ability of fast and sequential adaptation and balances memory encoding and active forgetting, across a large number of familiar and unfamiliar scenarios. Meta-learning offers a promising computational paradigm to learn such a universal learning algorithm in a data-driven way.  In this work, we proposed a meta-learning approach to learn a sequential adaptation algorithm for arbitrary deep neural network architectures. Our approach performs sequential adaptation with a bounded compute and memory across changing environment and tasks. The proposed Online Cifar setup can serve as a useful benchmark for studying flexible models and algorithms that go beyond the fixed distribution regime.   In the current state of the Sparse-MetaNet method, a sparsity mask is sampled from a fixed distribution. A future work should explore learning-based approaches for a conditional mask distribution, so that a Sparse-MetaNet model can selectively encode a fast-weight memory from past gradients. The current work has a limited focus on the catastrophic interference issue in neural networks. A future work can extend the Sparse-MetaNet approach for mitigating this issue.  
","  We summarize the difference between our work and related work in Table. Most works differ on how they combine the word embeddings to obtain a sentence embedding. Recent works  used Skip-gram  and adopted simpler architectures for sentence embedding than previous work .         Previous work  that used GitHub data extracted the methods from the source code and matched them to docstring descriptions. Works that used Stack Overflow data  paired question titles to the accepted answers' code snippet. For the search corpus, some works  adopted a GitHub corpus with millions of pieces of code, while others  retrieved code snippets from a small sample of 50 randomly selected code snippets. These studies  are commonly evaluated using questions collected from Stack Overflow.   Although our architecture is more complex, since it requires more parameters and time to train, than \citet{cambronero-deep-code-search-2019}'s  architecture , we can train our model offline. We matched question title to code snippets collected from Stack Overflow following \citet{Allamanis-bimodal-source-code-natural-language:2015} and \citet{iyer-etal-2016-summarizing}  work.",14
" % Background Collecting a sufficient amount of electronic health records is a challenging task with various factors . Due to this problem, researchers in the medical field are often provided with only a small amount of data given. Owing to the fact that deep learning techniques perform better on large amounts of data, a number of studies using machine learning techniques have been conducted to solve specific medical problems, regarding a limited number of data . Dementia is also one of many medical symptoms facing this situation.  % Alzheimer's Dementia Dementia, a syndrome in which there is deterioration in cognitive function beyond what might be expected from normal ageing, is mostly affected by Alzheimer閳ユ獨 Disease . % Although studies with Dementia also faces the problem with lacking dataset,  There were previous researches with various approaches to recognize Alzheimer's Dementia , which has shown excellent performance. % However, the dataset used in these works were more adequete with quantity than the one used in this paper. However, datasets used in these works were sufficient with quantity than the one used in this paper.  % The ADReSS challenge The ADReSS challenge  at INTERSPEECH 2020 hosts two tasks: Alzheimer閳ユ獨 Dementia  classification and Mini Mental Status Examination  regression, while providing a refined dataset. The dataset is equally balanced of AD and non-AD participants with the metadata of age and gender. % Each data is a conversation between a participant and an investigator composed of acoustic and textual information. % Each data is a conversation between a participant and an investigator where a participant spontaneously describes the picture given by an investigator. % Each data is a conversation where a participant spontaneously describes the picture given by an investigator with acoustic and textual modality. Each data is a conversation in which participants, in both audio and text modalities, spontaneously describes the picture given by the investigator. % proposing work Participants of the challenge are suggested to solve hosted tasks using only the given data, where the numbers of train and test data are 108 and 48, respectively.  For recognizing AD with small amounts of data, we determined it would be beneficial to use both acoustic and textual features. % why? % we thought it would be best to use as many information as possible for recognizing AD 闉氭帾鐓遍瀬婵庢簜鎼 姘氭棄闈栨棶? Furthermore, we leverage models pre-trained on large scale datasets as feature extractor to get better representation. To this end, this paper focus on exploiting various multi-modal features, and design suitable network architecture. % 闇嬨倢妫 闆﹥妲 闆尗姣勯湆 鑷ф粚娈 鑷у嫴鐏ラ爟姗佺煀 闈广倠鐛忛爟姗佽荡 We compare 3 and 4 different acoustic and textual features, respectively, and use the hand-crafted  feature and part-of-speech  tagging as additional inputs. The usage of POS and HC is influenced by previous research, which has approved that using these features gained from transcript can improve the performance . The proposed network is a modified version of Convolutional Recurrent Neural Network ; capable of computing conversations with variable lengths, and implemented with methods to fit with a small amount of data. Also, the model is able to compute using the acoustic feature only, without any metadata, which can be efficient considering the real-world situation. Our experimental results show using features of the pre-trained network leads to performance gain than that of raw, and regression results imply the potential of network classifying classes of cognitive impairment based on MMSE score.      While the proposed model can cope with additional inputs such as visual modality, the ADReSS challenge only offers acoustic and textual modalities. Thus, we primarily focus on the network with bimodal inputs.  The overview of our model is as  Figure. In case of unimodal, the network has the same structure, except that only a single modality feature is input.    An input dialogue consists of its utterances and an extracted HC feature. Each utterance comes along with an acoustic and a textual feature, and a speaker index. The speaker index is a binary feature denoting an investigator or a participant, where it is extended as the size of the largest size of input feature dimension, 1024 in our case, by a single fully connected layer. Input features smaller than 1024 are also expanded the same way by a fully connected layer.    Dropout We apply dropout  to the input features before they are inserted into the network. This way, the model can be provided with more opportunity to learn independent representations, because each dimension can convey significant information, especially for the features extracted from pre-trained models.     The proposed network is a modified version of CRNN, where an attention layer is a forefront layer of the network, and fully connected layers followed after the recurrent layer. Here, we use a bidirectional Long Short-Term Memory Network   as the recurrent network.   Figure  is an overview of the model architecture.   Attention Layer Each modality input is individually inserted and computed through an attention layer. Our attention layer is implemented as the Scaled Dot-Product Attention mechanism introduced in . We use a self-attention mechanism, where an individual feature is used as a query, key, and value during the attentional computation.    Convolutional Neural Network  Outputs of the attention layer and embedded speaker index of a single utterance are channel-wise concatenated then inserted into the one-dimensional Convolutional Neural Network . After a convolutional layer expands channel dimension to 32, 6 Squeeze-and-Excitation   blocks are followed in the CNN. Each SE block consists of 2 convolution layers with a SE layer in between them. The last convolutional layer of every 2 SE blocks reduces feature dimension by convolutional stride factor of 4 and increments channel dimension. The expanding sizes of the channel dimension are 128, 512, 1024 respectively. Ultimately, CNN outputs 1024-dimensional channels with a global max pooled value.    Squeeze-and-Excitation block闉 convolutional layer鎼 闈奉剣姣勯爟. SE-block闉欳lassification task闉愭劤鍔 闉栧姙鍨爟 闈硅啒鐖滈灇 鐡崐鎳嶉澒 闈瑰姜濮 闋勩儸鍎婇灇 鏃崐瀵戦爟.    bidirectional Long Short-Term Memory Network  After every utterance from the input dialogue is each computed through the CNN, the processed utterance embeddings are sequentially inputted into the bi-LSTM. The recurrent network consists of 3 bi-LSTM layers with 512 hidden units. Ultimately, the recurrent network outputs the max-pooled state from the results of the last layer閳ユ獨 hidden states and is concatenated with HC.    decision layer Three fully connected  layers follow after the bi-LSTM layers. Both the first two FC layers are followed by a rectified linear unit  activation and reduce the input dimension by a factor of 4. The last activation function for classification and regression tasks are softmax and sigmoid, respectively.   We use sigmoid activation for the last FC layer.  Ground truth MMSE score is scaled from 0 to 1 for regression loss computation.          Batch We use different numbers of utterances per batch during the training phase for the network to have opportunities to interpret various sequences of dialogue. The size is randomly selected between 5 and the minimum number of utterances among the dialogues in each batch. Since the minimum number of utterances of dialogue in the training data is 7, it was reasonable to set the minimum length to 5. If the length is too short, the network could be vulnerable to utterances with less meaningful data such as the investigator閳ユ獨 閳ユ笝kay閳 or 閳ユ笗hm閳. A single batch is used during the inference phase to analyze every utterance in an input dialogue.    Loss & hyperparams Our training loss for classification and regression tasks are binary cross-entropy error and mean squared error, respectively. The total cost function is a summation of these two values. We use the Adam optimizer  with a learning rate of 0.0002 and momentum parameters 1 = 0.5, 2 = 0.9.     This report describes Brown University's entry to the TREC 2019 Deep Learning Track, in which we produced the final ranking of a set of 1000 candidate passages for given queries. Our method aims at enriching the meaning and surface form of a query by expanding it with similar queries, in the hopes that during the subsequent ranking process, the expanded query would provide extra semantic information or vocabulary overlap that would facilitate the retrieval of more relevant documents. \\  We found this retrieval method to be promising in terms of retrieval results, albeit with significant margins for future improvement. A natural focus point of future work is improving the semantic similarity between generated queries and the original query. In this work, we simply use the top 3 output beams in terms of estimated log-likelihood. However, different metrics could be used to re-order and prioritize a larger number of generated outputs. In addition, further investigation can be carried out in terms of various ways of synthesizing the query information or condensing the documents' representation.  
"," While the proposed model can cope with additional inputs such as visual modality, the ADReSS challenge only offers acoustic and textual modalities. Thus, we primarily focus on the network with bimodal inputs.  The overview of our model is as  Figure. In case of unimodal, the network has the same structure, except that only a single modality feature is input.    An input dialogue consists of its utterances and an extracted HC feature. Each utterance comes along with an acoustic and a textual feature, and a speaker index. The speaker index is a binary feature denoting an investigator or a participant, where it is extended as the size of the largest size of input feature dimension, 1024 in our case, by a single fully connected layer. Input features smaller than 1024 are also expanded the same way by a fully connected layer.    Dropout We apply dropout  to the input features before they are inserted into the network. This way, the model can be provided with more opportunity to learn independent representations, because each dimension can convey significant information, especially for the features extracted from pre-trained models.     The proposed network is a modified version of CRNN, where an attention layer is a forefront layer of the network, and fully connected layers followed after the recurrent layer. Here, we use a bidirectional Long Short-Term Memory Network   as the recurrent network.   Figure  is an overview of the model architecture.   Attention Layer Each modality input is individually inserted and computed through an attention layer. Our attention layer is implemented as the Scaled Dot-Product Attention mechanism introduced in . We use a self-attention mechanism, where an individual feature is used as a query, key, and value during the attentional computation.    Convolutional Neural Network  Outputs of the attention layer and embedded speaker index of a single utterance are channel-wise concatenated then inserted into the one-dimensional Convolutional Neural Network . After a convolutional layer expands channel dimension to 32, 6 Squeeze-and-Excitation   blocks are followed in the CNN. Each SE block consists of 2 convolution layers with a SE layer in between them. The last convolutional layer of every 2 SE blocks reduces feature dimension by convolutional stride factor of 4 and increments channel dimension. The expanding sizes of the channel dimension are 128, 512, 1024 respectively. Ultimately, CNN outputs 1024-dimensional channels with a global max pooled value.    Squeeze-and-Excitation block闂 convolutional layer閹 闂堝鍓ｅВ鍕垷. SE-block闂夋lassification task闂夋劖鍔ら崝 闂夋牕濮欓崹顒勭垷 闂堢鍟掗悥婊堢亣 閻☆垰宕愰幊宥夋緬 闂堢懓濮滄慨 闂嬪嫨鍎搁崕濠囩亣 閺冾喖宕愮垫垿鐖.    bidirectional Long Short-Term Memory Network  After every utterance from the input dialogue is each computed through the CNN, the processed utterance embeddings are sequentially inputted into the bi-LSTM. The recurrent network consists of 3 bi-LSTM layers with 512 hidden units. Ultimately, the recurrent network outputs the max-pooled state from the results of the last layer闁炽儲鐛 hidden states and is concatenated with HC.    decision layer Three fully connected  layers follow after the bi-LSTM layers. Both the first two FC layers are followed by a rectified linear unit  activation and reduce the input dimension by a factor of 4. The last activation function for classification and regression tasks are softmax and sigmoid, respectively.   We use sigmoid activation for the last FC layer.  Ground truth MMSE score is scaled from 0 to 1 for regression loss computation.          Batch We use different numbers of utterances per batch during the training phase for the network to have opportunities to interpret various sequences of dialogue. The size is randomly selected between 5 and the minimum number of utterances among the dialogues in each batch. Since the minimum number of utterances of dialogue in the training data is 7, it was reasonable to set the minimum length to 5. If the length is too short, the network could be vulnerable to utterances with less meaningful data such as the investigator闁炽儲鐛 闁炽儲绗漦ay闁 or 闁炽儲绗梙m闁. A single batch is used during the inference phase to analyze every utterance in an input dialogue.    Loss & hyperparams Our training loss for classification and regression tasks are binary cross-entropy error and mean squared error, respectively. The total cost function is a summation of these two values. We use the Adam optimizer  with a learning rate of 0.0002 and momentum parameters 1 = 0.5, 2 = 0.9.",15
"   Transformer  is one of the state-of-the-art approaches for Neural Machine Translation , and hence, being widely accepted. For example, in WMT19 machine translation tasks, it is reported that 80\% of submitted systems have adopted the Transformer architecture . Note that high translation quality of Transformer models entails a large number of parameters. Moreover, the Transformer model is inherently much slower than conventional machine translation approaches  mainly due to the auto-regressive inference scheme  incrementally generating each token. As a result, deploying the Transformer model to mobile devices with limited resources involves numerous practical implementation issues.  To address such implementation challenges with little degradation in translation quality, we study a low-bit quantization strategy for Transformer to accomplish high-performance on-device NMT. We note that most previous studies to compress Transformer models utilize uniform quantization . While uniform quantization may be effective for memory footprint savings, it would face various issues to improve inference time and to maintain reasonable BLEU score. For example, even integer arithmetic units for inference operations present limited speed up  and resulting BLEU score of quantized Transformer can be substantially degraded with low-bit quantization such as INT4 .  While determining the number of quantization bits for Transformer, it is crucial to consider that each component of Transformer may exhibit varied sensitivity of quantization error toward degradation in translation quality . Accordingly, a mixed precision quantization can be suggested as an effort to assign different numbers of quantization bits depending on how each component after quantization is sensitive to the loss function. In addition, as we illustrate later, even assigning different quantization bits for each row of an embedding block can further reduce the overall number of quantization bits of the entire Transformer model. Our proposed quantization strategy, thus, provides a finer-grained mixed precision approach compared to previous methods, such as  that consider layer-wise or matrix-wise mixed precision.  % One important aspect is that each block in Transformer contributes to the inference computation and the translation accuracy differently. Transformer consists of three major blocks: embedding, encoder, and decoder. The embedding block has a huge number of parameters due to its dependence on the vocabulary size, easily in scale of tens of thousands. On the contrary, the matrices in encoder and decoder are relatively small since they are independent of the vocabulary size.  As a result, embedding block causes a major memory and latency consumption. Since the decoding steps are not parallelizable at inference time, it also contributes largely to the inference computation.  % In consideration of these, we propose a mixed precision quantization strategy for Transformer quantization with efficient inference computation and reasonable accuracy loss. Accommodating distinguished implementation properties  of each component in Transformer, we propose the following methodologies to decide precision of a block: 1) in the case of embedding block, statistical importance of each word is taken into account and 2) for encoder and decoder blocks, sensitivity of each quantized sub-layer is considered. The main contributions of this paper are as follows:          To explore more light-weighted architecture of Transformer, some researches adopt architectural search methods and suggest Transformer variants that are more efficient to compute and/or have smaller model size . However, our focus is devising a compression method for an already trained Transformer model.     citation闉 闉濋灈鎰层偧 闉氭﹤顕爟姗傚 鐡村眾姣 plural鎼 姘氭棄闈栫摯宀嗕粓 reviewer 1闉 闉濆嫴绉 comment 闆碱剣鏅為洿顭庡礉闇涚紕鎯灃. 鏀撮附鐏呴澒 citet闇屻倢娼 plural鎼 姘氭顶鐭婇洯 鐡跨娋闀块瀫鎯﹀闆. Previous researches proposed various model compression techniques to reduce the size of Transformer models.  \citet{state-of-sparsity} apply pruning  to eliminate redundant weights of Transformer and report that higher pruning rates lead to greater BLEU score degradation. As for pruning, achieving inference speed up is more challenging because unstructured pruning method is associated with irregular data formats, and hence, low parallelism .  Uniform quantization for Transformer is explored within reasonable degradation in BLEU score at INT8, while BLEU score can be severely damaged at low bit-precision such as INT4 . In order to exploit efficient integer arithmetic units with uniformly quantized models, activations need to be quantized as well . Furthermore, probability mapping operations in Transformer, such as layer norm. and softmax, could exhibit significant amount of error in computational results with low precision data type .    We compare Transformer quantization methods in Table .       For the classification task, the best test accuracy using only acoustic input is 72.92\ , while using both modality results in 81.25\ . For the regression task, we achieved an RMSE score of 3.7749 . Additionally, our 5-fold cross-validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment, categorized by the MMSE score, with an accuracy of 78.70\ .  This paper demonstrates extracted features from pre-trained networks are satisfactory for handling small amounts of data, to recognize Alzheimer's Dementia. The proposed model can compute variable lengths of dialogue and also introduce productive methods to fit the network with a little amount of data. Furthermore, our model does not require any metadata and also can perform well without transcript, which may be practical in real-world situations. Our test result outperforms baseline's with both tasks, and our regression results imply the potential of network classifying classes of cognitive impairment based on the MMSE score.    validation 闆碱煃纭犻瀽鎰冲妧 unimodal鎼 鑷 modality闉 闆介爟 鐡寸摚鑷ｆ post 鐡撮灊鏇ф殔 鐡寸摚鑷, audio闉愭劤鍔闆 姣电儎绋婄摽 text闉愭劤鍔闆 闊规顒呯, 鏀 姘氭﹤瀵戦灇 鐡村眾姣庤嚙 闉涘牗妫冮浖. 闉氭帾銈 鐡跨娊鐗戦爠鍫ㄦ綁 闇 bimodal network鑷 闇 modality姣 merge 闋冩﹤鈹愰澒 闉庡牗顣伴灇 鏀垫數鍕虫綁 姘氬姙鐏涢爟姗冾潊 闉庡﹫纰 闇呮﹤鐗 闋冩瑬濮烽爟姗冩３ 闈瑰姜濮 闉濋爟姗佺殰 闉涘牗妫冮浖銈婄 鏂兼棈娴爟 闈 闉涘牕濮呴浛, 闉氭帾銈 姘氣晥顫呴爟姗傚 姘囶煄宓 鐢戭剣鈥滄 future work闉欒導顢 姝嗗嫶鏋嗛爟 闈 闉涘牗娼 鐡村喒婢婇浖. 闇涚紕纰 闉濅緟鏌庢挨 闈 闉涘牗娼 闇    VGGish & Transformer-XL   VGGish wrong - 18   Transformer-XL wrong - 5    bimodial wrong - 9     future work to improve multi-modal network   Some modifications in the model architecture can be done to merge different modalities with beneficial effects on each other as future work. There were validation samples with no overlapping error results of the unimodal network of each modality, where the bimodal network using the same modality features above was able to reach the accurate answer for these typical samples. Yet, some samples that the unimodal networks could deduce correctly were wrong by the bimodal network. Accordingly, mechanisms effectively fusioning divergent features can be applied in expectation of performance gain  .    For future work, some modifications in the model architecture can be done to merge different modalities with mechanisms effectively fusioning divergent features can be applied in expectation of performance gain  .  For future work, with the expectation of performance gain, mechanisms effectively fusioning different modality features   can be applied in the model architecture.        Acknowledgements 
","     To explore more light-weighted architecture of Transformer, some researches adopt architectural search methods and suggest Transformer variants that are more efficient to compute and/or have smaller model size . However, our focus is devising a compression method for an already trained Transformer model.     citation闂 闂夋繈鐏堥幇灞傚仹 闂夋碍锕ら顓㈢垷濮楀倸顫 閻℃潙鐪惧В plural閹 濮樻碍妫勯棃鏍懐瀹鍡曠矒 reviewer 1闂 闂夋繂瀚寸粔 comment 闂嗙⒈鍓ｉ弲鐐烘纯椤骸绀夐棁娑氱磿閹噣鐏. 閺鎾檮閻忓懘婢 citet闂囧被鍊㈠ plural閹 濮樻碍椤堕惌濠囨疮 閻¤法濞嬮梹鍧楃幆锕顎橀梿. Previous researches proposed various model compression techniques to reduce the size of Transformer models.  \citet{state-of-sparsity} apply pruning  to eliminate redundant weights of Transformer and report that higher pruning rates lead to greater BLEU score degradation. As for pruning, achieving inference speed up is more challenging because unstructured pruning method is associated with irregular data formats, and hence, low parallelism .  Uniform quantization for Transformer is explored within reasonable degradation in BLEU score at INT8, while BLEU score can be severely damaged at low bit-precision such as INT4 . In order to exploit efficient integer arithmetic units with uniformly quantized models, activations need to be quantized as well . Furthermore, probability mapping operations in Transformer, such as layer norm. and softmax, could exhibit significant amount of error in computational results with low precision data type .    We compare Transformer quantization methods in Table .",16
" The rapid progression of generative models in both computer vision  and natural language processing  has led to the increasing likelihood of realistic-looking news articles generated by Artificial Intelligence . The malicious use of such technology could present a major societal problem. \citet{zellers2019defending} report that humans are easily deceived by its AI-generated propaganda. By manipulating such technology, adversaries would be able to disseminate large amounts of online disinformation rapidly. While it is promising that the pretrained generative models themselves are our best defense , it is often challenging to be aware of the models utilized by adversaries beforehand. More importantly, it ignores the fact that news articles are often accompanied by images with captions .   %We argue that such visual context provides vital clues for discriminating against machine-generated articles.    In this paper, we present the first line of defence against neural fake news with images and captions. To the best of our knowledge, we are the first to address this challenging and realistic problem. Premised on the assumption that the adversarial text generator is unknown beforehand, we propose to evaluate articles based on the semantic consistency between the linguistic and visual components. While state-of-the-art approaches in bidirectional image-sentence retrieval  have leveraged visual-semantic consistency to great success on standard datasets such as MSCOCO  and Flickr30K , we show in Appendix they are not able to reason effectively about objects in an image and named entities present in the caption or article body. This is due to discrepancies in the distribution of these datasets, as captions in the standard datasets usually contain general terms including woman or dog as opposed to named entities such as Mrs Betram and a Golden Retriever, which are commonly contained in news article captions. Moreover, images are often not directly related to the articles they are associated with. For example, in Figure , the article contains mentions of the British Prime Minister. Yet, it only contains an image of the United Kingdom flag.   To circumvent this problem, we present DIDAN, a simple yet surprisingly effective approach which exploits possible semantic inconsistencies between the text and image/captions to detect machine-generated articles. For example, notice that the article and caption in Fig. actually mention different Prime Ministers. Besides evaluating the semantic relevance of images and captions to the article, DIDAN also exploits the co-occurrences of named entities in the article and captions to determine the authenticity score. The authenticity score can be thought of as the probability that an article is human-generated. We adopt a learning paradigm commonly used in image-sentence retrieval where models are trained to reason about dissimilarities between images and non-matching captions. In this instance, negative samples constitute articles and non-corresponding image-caption pairs. Not only is this a reasonable approach when the adversarial generative model is unknown, we show empirically that it is crucial to detecting machine-generated articles with high confidence even with access to machine-generated samples during training. More importantly, this means that DIDAN is easily trained on the abundance of online news articles without additional costly annotations.  To study this threat, we construct the NeuralNews dataset which contains both human and machine-generated articles. These articles contain a title, the main body as well as images and captions. The human-generated articles are sourced from the GoodNews  dataset. Using the same titles and main article bodies as context, we use GROVER  to generate articles. Instead of using GAN-generated images which are easy to detect even without exposure to them during training time , we consider the much harder setting where the articles are completed with the original images. We include both real and generated captions which are generated with the SOTA entity-aware image captioning model . We present results and findings from a series of empirical as well as user study experiments. In the user study experiments, we use 4 types of articles including real and generated news to determine what humans are most susceptible to. The insights derived from these findings help identify the possible weaknesses that adversaries can exploit to produce neural fake news and serve as a valuable reference for defending against this threat. Last but not least, our experimental results provide a competitive baseline for future research in this area.  In summary,  our contributions are multi-fold:       GROVER  draws on recent improvements in neural text generation  to generate realistic-looking articles complete with metadata such as title and publication date but without images. Interestingly, it also serves as the best form of defense against its own generated propaganda.  show that the GPT-2 model can be manipulated to generate fake reviews to deceive online shoppers. Corroborating the findings by , they also report that pretrained language models such as GROVER and GPT-2 are unable to accurately detect fake reviews. To combat effectively against the dissemination of neural disinformation,  propose a promising direction of reverse engineering the configurations of neural language models to identify detectable tokens. Last but not least,  introduce an approach to generate image captions based on contextual information derived from news articles. Such progress points towards the inevitability of large-scale dissemination of generated propaganda and the significance of this task.    In recent years, the introduction of Generative Adversarial Networks  has led to unprecedented progress in image and video generation. While most of these have focused on generating images from text as well as video translation, such models can easily be exploited to generate disinformation which can be devastating to privacy and national security . In response to this growing threat,  propose a forensic approach to identify fake videos by modeling people's facial expressions and speaking movements. In a similar vein to ,  seek to exploit visual artifacts to detect face manipulations and deepfakes.  Encouragingly, \citet{wang2019detecting} show that neural networks can easily learn to detect generated images even without exposure to training samples from those generators.     Visual-semantic consistency forms the backbone of SOTA bidirectional image-sentence retrieval approaches .  However, as we show in Appendix, these methods do not perform well on images and captions from news articles since they are unable to effectively take advantage of named entity information.  Instead, we use Canonical Correlation Analysis  as our baseline as it has proven to be effective in realistic vision-language tasks.  Similar in spirit to generating fake news with large neural language models, neural visual-semantic models  can easily be exploited to generate multimodal neural disinformation.    In this work, we analyze each block and sub-layer of the Transformer and propose an extremely low-bit quantization strategy for Transformer architecture. Our 2.6-bit quantized Transformer model achieves 11.8 model compression ratio with reasonable -0.5 BLEU. We also achieve the compression ratio of 8.3 in memory footprints and 3.5 speed up on a mobile device .       \clearpage   
","  GROVER  draws on recent improvements in neural text generation  to generate realistic-looking articles complete with metadata such as title and publication date but without images. Interestingly, it also serves as the best form of defense against its own generated propaganda.  show that the GPT-2 model can be manipulated to generate fake reviews to deceive online shoppers. Corroborating the findings by , they also report that pretrained language models such as GROVER and GPT-2 are unable to accurately detect fake reviews. To combat effectively against the dissemination of neural disinformation,  propose a promising direction of reverse engineering the configurations of neural language models to identify detectable tokens. Last but not least,  introduce an approach to generate image captions based on contextual information derived from news articles. Such progress points towards the inevitability of large-scale dissemination of generated propaganda and the significance of this task.    In recent years, the introduction of Generative Adversarial Networks  has led to unprecedented progress in image and video generation. While most of these have focused on generating images from text as well as video translation, such models can easily be exploited to generate disinformation which can be devastating to privacy and national security . In response to this growing threat,  propose a forensic approach to identify fake videos by modeling people's facial expressions and speaking movements. In a similar vein to ,  seek to exploit visual artifacts to detect face manipulations and deepfakes.  Encouragingly, \citet{wang2019detecting} show that neural networks can easily learn to detect generated images even without exposure to training samples from those generators.     Visual-semantic consistency forms the backbone of SOTA bidirectional image-sentence retrieval approaches .  However, as we show in Appendix, these methods do not perform well on images and captions from news articles since they are unable to effectively take advantage of named entity information.  Instead, we use Canonical Correlation Analysis  as our baseline as it has proven to be effective in realistic vision-language tasks.  Similar in spirit to generating fake news with large neural language models, neural visual-semantic models  can easily be exploited to generate multimodal neural disinformation.",17
"  In the past few decades, knowledge graph construction and applications have been rapidly developed and achieved significant outcomes.  For better relevancy in web search, Google has been leveraging knowledge graph that represents real-world entities and their relationships to one another since 2012. %, there are also a large amount of publicly available knowledge graphs, such as freebase, Dbpedia, YAGO that have been constructed and used to many real-world intelligent applications. To identify those entities from text, named entity recognition  techniques have been extensively studied and applied in many areas  including e-commerce search . Such NER systems usually work with a well defined ontology to classify tokens in a sequence of words . A comprehensive and domain-specific PT ontology is beneficial to product search and discovery in an e-commerce platform . At The Home Depot , PT ontology has been used tremendously by the online search to improve query understanding and product retrieval. For example, Figure  shows a snippet of our PT ontology that consists of known PT classes. The PTs in the ontology serve as the entity reference for the NER task  as well as the classes for SKU-PT mapping  on the catalog side that facilitates the retrieval of relevant products.  %. Kutiyanawala et al. also proposed an product ontology framework created specially for e-commerce search and retrieval .  %comprehensive and domain-specific Ontology is required in order to better understand customers閳 intent and account for the expanding catalog. The Ontology enrichment has been proved effective to boost search relevancy. For example, given the customer query ""shower curtain hook"", the system would also return some ""shower curtain"" products since it failed to infer the proper product type due to the lack of knowledge. By introducing a new product type ""shower curtain hook"", the system is able to remove the noise and provide more relevant results. %  % \end{equation*}  %     \[ %   z = \overbrace[1pt][5pt]{ge}^{brand}\ \overbrace{7.3\:cu\:ft}^{dim} \quad\overbrace{dryer}^{product}\quad\overbrace{gas}^{attribute} %   \] %In the domain of e-commerce, a strong and well-structured knowledge graph also plays pivotal roles for both business to business  communications and customer search and navigation experience.   %A structured and standardized product ontology which define product description, catalog formats and business documents support electric data exchange between vendors and buyers.  %The Home Depot  is a world leading home improvement retailer for customers and business. Orange Graph  is the repository and access point for THD domain-specific knowledge, which includes rich product information, project information and their relationships. By adopting well-structured knowledge graph, a high-level of search quality, project-based buying features, marketing and customer services can be offered at THD e-commerce and enterprise systems.  % } %  %  % \end{table} Discovering valid PTs is a key task to build or expand a PT ontology with a fundamental challenge regarding the definition of a PT. % given it's a concept instead of fact. A PT can be defined from the demand side as atomic keywords/phrase that describes what customers look for  or from the supply side as a semantic tag/label that uniquely identifies a product. Within THD, we also have practical guidelines to distinguish between valid and invalid PTs like  %Product type  is an essential component of a PT ontology.  %it is widely used in e-commerce domain to group the similar products together. For instance, consider Appliances category, our goal is to discover distinct types of refrigerators which in this case it could be: ""Side By Side"", ""French Door"", etc.  %Although there are different definitions for a valid PT, In this paper, we define a valid PT as a leaf-level description of an entity.   no common attributes like color, brand, material, style etc in PTs  and  it requires significant differences in the form, functionality or usage location to make a new PT comparing to existing ones . %Another determiner for whether adding a token to a product type makes it a new product type  is if the addition of the new token changes the form, function or usage location. In our example, cordless doesn't change it for drill, while utility does for sink.   Obviously, neither the definition is definite nor the guidelines are exhaustive enough and there are always complicated cases and exceptions in which human judgement based on knowledge in merchandising, customer preference or just common sense is required. %without involving human knowledge which is usually expensive in term of time and monetary cost. %automatically determine if a candidate .  %Although aforementioned definition would generally help to distinguish between valid and invalid PTs, there are several challenges in this task  %as depicted in Table.  %First and foremost, it is crucial to determine a right level of granularity for discovered PTs. Very generic PTs are generally ambiguous as they could be attributable to a broad set of products with different use cases. For example, PT chairs can be ambiguous as it can comprise outdoor chairs, office chairs, dining chairs and each of these chairs types has a different usage location. %Specifically, domain experts have great advantage in  For example, a generic PT range can be broken down into more granular ones by fuel type like gas range, electric range or by other attribute like induction range, convention range. The word ""wood"" is material in wood rolling pin while is about usage in wood glue.   % Moreover, it is often subjective to determine in what level of granularity PT discovery should be stopped and based on what criteria a generic PT should be broken down into more granular PTs. For instance, given a generic PT ranges we can break it down by fuel type  or features . In this example, we can consider one of them as the PT and the other one as an attribute; alternatively they can be combined and construct a more granular PT.  % Another challenge is to automatically identify if a token in a PT is an attribute or not. As an example, consider wood rolling pin and wood glue; token wood in the latter change the use case of the glue, while in the former is a material.     However, leveraging human knowledge in large scale problems is usually timely and expensive.  To reduce such cost, this paper proposes  %The main contribution of this paper is as follows: proposing  an active learning framework that minimizes human effort in PT discovery by 1) identifying high quality candidates using phrase mining and user behavior. 2) limiting number of PT candidates for human validation.  %%%%%%%     Recently, incorporating structured human knowledge encapsulated in KG is proven to be very effective in various applications . In this section we discuss some of the related works in the area of KG construction and completion. A technique to extract the information with no pre-defined ontology is proposed in . The authors utilized semi-supervised label propagation approach to collect data and train a classifier to extract entities relations. While there are several generic knowledge bases, one of the challenges associated with domain-specific KG construction and completion is lack of publicly available knowledge base in that particular domain. To address aforementioned issue, a salable methodology is studied to expand the KG by integrating a domain-specific KG with a general domain one  . The authors employed graph neural network to automatically align the entities in multiple knowledge bases. In the domain of e-commerce, several unsupervised techniques have been used to generate a commercial product-brand knowledge base by leveraging customer behavior and search terms . In addition,  provided a comprehensive comparison between generic KGs and product KGs. In this work, a self-attention based model utilized customer behavior data  and product's content information  to learn product embedding and discovered the relationships between the e-commerce products. More recently, product KGs have been widely used to improve e-commerce search performance.  presented unsupervised and supervised approaches to identify e-commerce product types from searched queries. They demonstrated a performance comparison between diverse approaches:  unsupervised product type and attribute identification directly from queries in an unsupervised fashion  leveraged labeled data and trained convolutional neural networks to identify product type token in a query  trained a named entity recognition model similar to the model described by  to detect the product types from the queries. In this work, we introduce an active learning approach to discover new product types by mining data from products' catalog and query logs.              This work introduces an advanced GA for hyperparameter optimization and applies it to machine translation optimization. We demonstrate that optimization of hyperparameters via a GA can outperform a random selection of hyperparameters. Specifically, outperform is defined by the ability of the algorithm to arrive at the goal with less individuals added. Finally, we propose future research directions which are expected to provide additional gains in the efficacy of GAs.  
","  Recently, incorporating structured human knowledge encapsulated in KG is proven to be very effective in various applications . In this section we discuss some of the related works in the area of KG construction and completion. A technique to extract the information with no pre-defined ontology is proposed in . The authors utilized semi-supervised label propagation approach to collect data and train a classifier to extract entities relations. While there are several generic knowledge bases, one of the challenges associated with domain-specific KG construction and completion is lack of publicly available knowledge base in that particular domain. To address aforementioned issue, a salable methodology is studied to expand the KG by integrating a domain-specific KG with a general domain one  . The authors employed graph neural network to automatically align the entities in multiple knowledge bases. In the domain of e-commerce, several unsupervised techniques have been used to generate a commercial product-brand knowledge base by leveraging customer behavior and search terms . In addition,  provided a comprehensive comparison between generic KGs and product KGs. In this work, a self-attention based model utilized customer behavior data  and product's content information  to learn product embedding and discovered the relationships between the e-commerce products. More recently, product KGs have been widely used to improve e-commerce search performance.  presented unsupervised and supervised approaches to identify e-commerce product types from searched queries. They demonstrated a performance comparison between diverse approaches:  unsupervised product type and attribute identification directly from queries in an unsupervised fashion  leveraged labeled data and trained convolutional neural networks to identify product type token in a query  trained a named entity recognition model similar to the model described by  to detect the product types from the queries. In this work, we introduce an active learning approach to discover new product types by mining data from products' catalog and query logs.",18
" Distributional word representations trained on large-scale corpora are widely used in modern natural language processing  systems, which aims to describe the meaning of words and sentences with vectorized representations . Recent studies  addressed the state-of-the-art word embedding performance on various NLP tasks, where start to focus on how to evaluate the performance between different word embeddings accurately. However, \citet{Tsvetkov15} and \citet{Chiu16} have demonstrated that even for the same word embedding, most of the existing evaluation methods do not provide the constantly correlative results between intrinsic evaluation and extrinsic evaluation. Therefore, evaluating the performance of word embeddings with a unified metric is challenging in NLP tasks.  \citet{Hollenstein19} proposed a new evaluation framework called CogniVal, which applied traditional neural networks for regression and considered both intrinsic and extrinsic measurements based on collected human natural language processing-related cognitive data sources across three modalities: electroencephalography , functional magnetic resonance imaging , and eye-tracking. CogniVal is potentially identified as a pioneer of multi-modal cognitive word embedding evaluation framework, which conducts vectorized word embeddings evaluation by predicting how much they reflect the semantic representations against cognitive data sources that recorded when human processing natural language.   However, CogniVal framework ignored to measure some characteristics of human physiological signals. Specifically, all three modalities  of cognitive data used in their experiment featuring with non-stationary and non-linear motions . Inspired by \citet{Zekri08,Bodyanskiy13}, we assume that neural networks and fuzzy systems as computational intelligence methods are suitable tools for modelling expert knowledge and dealing with uncertain non-linear processes or non-stationary time series in a dynamic system, because approximate reasoning characteristics of fuzzy systems could present a practical model to handle uncertainty and disturbances in real data for complex hybrid non-linear or non-stationary problems . For this reason, we proposed a fuzzy-based neural network  framework for evaluating word embeddings with cognitive datasets, name CogniFNN, which expects to enhance the quality of evaluating the performance of word embeddings with cognitive data sources , and achieve a higher ratio of significant results with random word embeddings as well.   \paragraph{Contributions} The main contributions of our study are shown as follows:         \citet{Mitchell08} initiated introduced a neural based computational model to predict the fMRI activation when subjects are given the representation of word stimuli. Following this work, \citet{Jelodar10} proposed a similar model to Mitchell et al., but used different word embedding  to solve the ambiguity issues in fMRI dataset and improves the accuracy of processing cognition-language data. Later on, \citet{Wehbe14} have conducted an extensive study on evaluating the performance of brain activation patterns at sentence level rather than an isolated word, and \citet{Fernandino15} proposed a multiple regression model with sensory-motor experience based attributes as elements of the word vector to predict neural activation pattern for lexical concepts. Moreover, \citet{Sogaard16} has used the eye-tracking data source which is another modality of cognitive data to evaluate word embeddings against continuous text stimuli along with the fMRI data.    More recently, as the success of the neural network based approach for learning word representations, a study of whether word embedding models might simulate in part how the human brain process natural language has become a trend. Hence, \citet{Anderson17} proposed a deep convolutions neural networks model to evaluate the prediction of brain activation patterns, which was using Word2Vec as word embedding to compare the text-based word representation with image-based models. However, the lack of proper training data has become a significant reason why evaluating vector-space based word embedding models by using human cognitive data source has not been popularized so far , which means these related works mentioned above mainly focus on the single modality of recording signals from a small individual cognitive data source, without the universality of the word embeddings evaluation framework. To solve this problem, \citet{Hollenstein19} developed CogniVal, a neural network based regression model pioneered predicting cognitive language processing data against various modalities of recording human signals EEG, fMRI, and eye-tracking. Furthermore, the CogniVal is used to evaluate the ability of how well embeddings can predict human processing data against various modalities of recording human signals  to counteract the noisiness of the data.     However, these approaches above mostly focused on the collection or integration of related cognitive datasets, and none of the them tried to solve the non-linear and non-stationary problems of these signals. Hence, in this work we developed a CogniFNN framework to extract non-linear and non-stationary characteristics of human language processing-related physiological signals. To the best of our knowledge, this is the first attempt at using fuzzy neural networks to improve the comprehensive evaluation of cognitive wording embeddings.     In this work, we propose an active learning framework for product type discovery that leverage domain expertise in an efficient way.  The effectiveness of the framework is demonstrated by the quality and coverage of the resulting product types in the experiments as well as the positive business impact.  Experiment results also show that training data denoising is significantly beneficial to method performance. There are two kinds of future work including: 1) Feature engineering of PT classifier by exploiting more textual and/or image data 2) Design a denoise procedure and add it as an additional component into the framework.       
","  \citet{Mitchell08} initiated introduced a neural based computational model to predict the fMRI activation when subjects are given the representation of word stimuli. Following this work, \citet{Jelodar10} proposed a similar model to Mitchell et al., but used different word embedding  to solve the ambiguity issues in fMRI dataset and improves the accuracy of processing cognition-language data. Later on, \citet{Wehbe14} have conducted an extensive study on evaluating the performance of brain activation patterns at sentence level rather than an isolated word, and \citet{Fernandino15} proposed a multiple regression model with sensory-motor experience based attributes as elements of the word vector to predict neural activation pattern for lexical concepts. Moreover, \citet{Sogaard16} has used the eye-tracking data source which is another modality of cognitive data to evaluate word embeddings against continuous text stimuli along with the fMRI data.    More recently, as the success of the neural network based approach for learning word representations, a study of whether word embedding models might simulate in part how the human brain process natural language has become a trend. Hence, \citet{Anderson17} proposed a deep convolutions neural networks model to evaluate the prediction of brain activation patterns, which was using Word2Vec as word embedding to compare the text-based word representation with image-based models. However, the lack of proper training data has become a significant reason why evaluating vector-space based word embedding models by using human cognitive data source has not been popularized so far , which means these related works mentioned above mainly focus on the single modality of recording signals from a small individual cognitive data source, without the universality of the word embeddings evaluation framework. To solve this problem, \citet{Hollenstein19} developed CogniVal, a neural network based regression model pioneered predicting cognitive language processing data against various modalities of recording human signals EEG, fMRI, and eye-tracking. Furthermore, the CogniVal is used to evaluate the ability of how well embeddings can predict human processing data against various modalities of recording human signals  to counteract the noisiness of the data.     However, these approaches above mostly focused on the collection or integration of related cognitive datasets, and none of the them tried to solve the non-linear and non-stationary problems of these signals. Hence, in this work we developed a CogniFNN framework to extract non-linear and non-stationary characteristics of human language processing-related physiological signals. To the best of our knowledge, this is the first attempt at using fuzzy neural networks to improve the comprehensive evaluation of cognitive wording embeddings.",19
" 	 	 	Reinforcement Learning~ methods are increasingly being used for solving sequential decision-making problems from natural language inputs, like text-based games chat-bots and personal conversation assistants. In this work, we focus on Text-Based Games~, which require solving goals like ``Obtain coin from the kitchen'', based on a natural language description of the agent's observation of the environment. To interact with the environment, the agent issues text-based action commands~ upon which it receives a reward signal used for training the RL agent. 	 	 	 	 	 	 	 	% generalization problem 	Traditional text-based RL methods focus on the problems of partial observability and large action spaces. However, the topic of generalization to unseen TBGs is less explored in the literature.  We show that previous RL methods for TBGs often show poor generalization to unseen test games. We hypothesize that such overfitting is caused due to the presence of irrelevant tokens in the observation text, which might lead to action memorization. 	% ~(eg. every time agent.  	To alleviate this problem, we propose CREST, which first trains an overfitted base model on the original observation text in training games using Q-learning. Subsequently, we apply observation pruning such that, for each episode of the training games, we remove the observation tokens that are not semantically related to the base policy's action tokens. Finally, we re-train a bootstrapped policy on the pruned observation text using Q-learning that improves generalization by removing irrelevant tokens. Figure shows an illustrative example of our method. Experimental results on TextWorld games show that our proposed method generalizes to unseen games using almost x-x fewer training games compared to SOTA methods; and features significantly faster learning. 	 	 	 	 	     Please check here!! 	 	LSTM-DQN is the first work on text-based RL combining natural language representation learning and deep Q-learning. 	LSTM-DRQN is the state-of-the-art on TextWorld CoinCollector games, and addresses the issue of partial observability by using memory units in the action scorer.  	\citet{fulda2017can} proposed a method for affordance extraction via word embeddings trained on a Wikipedia corpus.  	AE-DQN~ -- which is a combination of a Deep RL algorithm with an action eliminating network for sub-optimal actions -- was proposed by Zahavy et al..  	Recent methods use various heuristics to learn better state representations for efficiently solving complex TBGs. 	 	 	 	 	 	  In this paper, we proposed a CogniFNN framework using fuzzy-based neural networks to explore the non-linear and non-stationary characteristics of physiological signals for improving the evaluation performance of word embeddings against cognitive datasets which recorded when subjects were understanding natural language . Our findings showed that CogniFNN achieved smaller prediction errors and higher significant ratios on both context-independent  and context-sensitive  word embeddings against 15 cognitive data sources across EEG, fMRI and eye-tracking. Our contributions could be a useful evaluation strategy which is beneficial to the exhaustive investigation on word embedding evaluations with corresponding cognitive features.         
","    Please check here!! 	 	LSTM-DQN is the first work on text-based RL combining natural language representation learning and deep Q-learning. 	LSTM-DRQN is the state-of-the-art on TextWorld CoinCollector games, and addresses the issue of partial observability by using memory units in the action scorer.  	\citet{fulda2017can} proposed a method for affordance extraction via word embeddings trained on a Wikipedia corpus.  	AE-DQN~ -- which is a combination of a Deep RL algorithm with an action eliminating network for sub-optimal actions -- was proposed by Zahavy et al..  	Recent methods use various heuristics to learn better state representations for efficiently solving complex TBGs.",20
" As a key step in constructing a knowledge graph, relation extraction is a task to extract the relation between the entities expressed in a sentence.  Previous work has largely focused on intra-sentence binary relation extraction, where the goal is to extract the relation between an entity pair in the sentence.   However, some relations require more than two entities and may span multiple sentences, which is defined as n-ary cross-sentence relation extraction. As the example shown in Table, the relation ``educate'' includes four entities, the person's ""name``, ""academic degree``, ""academic major`` and ""school``. In addition, this relation spans in four sentences in the example. Some prior works have applied a supervised learning approach to tackle this task, but they require large-scale labeled training data.  \end{table}  To obtain large-scale annotated data, some work assumes that if the consecutive sentences  contain the entities that have a relation in a knowledge base, these sentences as a whole describe that relation.  This assumption is referred to as distant supervision in the n-ary cross-sentence relation extraction task.  Even though methods based on distant supervision can quickly annotate sentences, they still have two main limitations: 1) they suffer from a noisy labeling problem;  2) the strong distant supervision assumption does not consider the non-consecutive sentences, which reduces the generalizability of the trained model. As the example shown in Table, the sentences at the 18th and 20th positions describe the fact but are not labeled using distant supervision because they are not consecutive. The first sentence is incorrectly labeled and is a noisy labeled data, which describes Alan Turing's work instead of his education.  To address the first limitation, we propose to train a sentence distribution estimator , which is a two-level agent reinforcement learning model. This provides a well-trained model that can select the high-quality labeled sentence groups and alleviate the impact of noisy data. There are previous works on applying reinforcement learning  to remove binary intra-sentence noisy data and achieve state-of-the-art  performance. When applying RL for n-ary cross-sentence relation extraction, a key challenge is that the RL model should not only learn sentence features, but also know the context and relation between each sentence. In this paper, the process of selecting sentences is not only influenced by the feature of the sentence itself, but also by the indicators we defined , which measure the semantic relationship between sentences. Moreover, whether a sentence is selected in a state or not is going to affect the decision of the next state. This state transition property provides the ability to choose the best combination of sentences in each sentence group.  To address the second limitation,  we relax the strong distant supervision assumption that lies at the heart of prior work by replacing it with a weaker distant supervision assumption. The assumption is that the sentence that has at least one main entity or two supplementary entities is annotated with the relation of these entities. We follow the Wikidata Knowledge Base scheme, where the main entity is the ``value'' of each fact and the supplementary entity is the ``qualifer'' of each fact. This assumption introduces some non-consecutive sentences and we propose a novel universal relation extractor to encode both consecutive and non-consecutive sentence groups. This relation extractor has a self-attention and soft attention mechanism layer, which compares the similarity between the word-level features and the relation query vectors. The relation extractor also encodes each sentence via a Piece-wise Convolution Neural Network  layer. The PCNN output is used to learn how the information transforms through sentences via a non-linear transformation layer.    Dependency shortest path has been applied with other pre-processing features for n-ary cross-sentence relation extraction. With the rise of deep learning, some work encoded the dependency shortest path via graph neural networks. Peng et al. applied Graph-LSTM to encode the dependency shortest path and link each path. One dependency shortest path usually requires two Graph-LSTMs. Song et al. proposed the Graph-state LSTM so that only one Graph-LSTM is needed to encode a path. Some work also implemented Bi-LSTM directly to encode the whole sentence sequences without requiring any preprocessing . The LSTM-CNN model they proposed achieved a better performance on the PubMed dataset, but it cannot encode long sequences. Recently, this model has been improved by deploying a multi-head attention layer. The model is also enhanced by incorporating prior knowledge from a pre-trained Knowledge Base.  The large-scale data used in these approaches are automatically labeled via distant supervision. As discussed in previous literature, distant supervision always introduces noisy incorrectly labeled data. In the binary relation extraction task, this problem is addressed by using a weaker distant supervision assumption. This assumption takes all labeled sentences with the same entity pairs into a bag and assumes that only one sentence in this bag is correctly labeled. Some work also trained an extra selector model, which selected the correctly labeled sentences as the training data of the relation extraction model. Most selectors are reinforcement learning -based models. However, these reinforcement learning-based selectors cannot be applied to n-ary cross-sentence relation extraction task, which is more challenging than the binary intra-sentence relation extraction.  To the best of our knowledge, our proposed work is the first to apply reinforcement learning on the n-ary cross-sentence relation extraction task. We propose an RL-based two-level agent selector  to select the correctly labeled sentence group. We also propose a weaker distant supervision assumption to label both consecutive and non-consecutive sentences. To encode them both, a novel universal relation extractor model is proposed, which is a hybrid approach of attention mechanism and context learning process of sentence features.    	 	 	We present a method for improving generalization in TBGs using irrelevant token removal from observation texts. Our bootstrapped model trained on the salient observation tokens obtains generalization performance similar to SOTA methods, with x-x fewer training games, due to better generalization; and shows accelerated convergence.  	In this paper, we have restricted our analysis to TBGs that feature similar domain distributions in training and test games. In the future, we wish to handle the topic of generalization in the presence of domain differences such as novel objects, and goal statements in test games that were not seen in training. 	  	 	 	 	 	 	 	 	 	 	
"," Dependency shortest path has been applied with other pre-processing features for n-ary cross-sentence relation extraction. With the rise of deep learning, some work encoded the dependency shortest path via graph neural networks. Peng et al. applied Graph-LSTM to encode the dependency shortest path and link each path. One dependency shortest path usually requires two Graph-LSTMs. Song et al. proposed the Graph-state LSTM so that only one Graph-LSTM is needed to encode a path. Some work also implemented Bi-LSTM directly to encode the whole sentence sequences without requiring any preprocessing . The LSTM-CNN model they proposed achieved a better performance on the PubMed dataset, but it cannot encode long sequences. Recently, this model has been improved by deploying a multi-head attention layer. The model is also enhanced by incorporating prior knowledge from a pre-trained Knowledge Base.  The large-scale data used in these approaches are automatically labeled via distant supervision. As discussed in previous literature, distant supervision always introduces noisy incorrectly labeled data. In the binary relation extraction task, this problem is addressed by using a weaker distant supervision assumption. This assumption takes all labeled sentences with the same entity pairs into a bag and assumes that only one sentence in this bag is correctly labeled. Some work also trained an extra selector model, which selected the correctly labeled sentences as the training data of the relation extraction model. Most selectors are reinforcement learning -based models. However, these reinforcement learning-based selectors cannot be applied to n-ary cross-sentence relation extraction task, which is more challenging than the binary intra-sentence relation extraction.  To the best of our knowledge, our proposed work is the first to apply reinforcement learning on the n-ary cross-sentence relation extraction task. We propose an RL-based two-level agent selector  to select the correctly labeled sentence group. We also propose a weaker distant supervision assumption to label both consecutive and non-consecutive sentences. To encode them both, a novel universal relation extractor model is proposed, which is a hybrid approach of attention mechanism and context learning process of sentence features.",21
" Healthcare information systems store huge volumes of electronic health records  that contain detailed visit information about patients over a period of time. The data is structured in three levels from top to bottom: the patient journey, the individual visit and the medical code. Fig. provides a typical example of this structure. An anonymous patient visits his/her doctor, a pathology lab and is admitted to the hospital on different days. The procedures and diagnoses performed at each of these visits are recorded as industry-standard medical codes. Each medical code, i.e. International Classification of Diseases  and Current Procedure Terminology , at the lowest level, records an independent observation while the set of codes at a higher level can depict the medical conditions of a patient at a given time point. At the top level, all occurrences of medical events at different time-stamps are chained together as a patient journey, which offers more informative details. Predicting sequential medical outcomes based on a patient's journey, such as hospital re-admissions and diagnoses, is a core research task that significantly benefits for healthcare management by hospitals and governments. For example, re-admission statistics could be used to measure the quality of care; Diagnoses can be used to understand more fully a patient's problems and relevant medical research. However, researchers have encountered many challenges in their attempts to represent patient journeys and predict medical outcomes from EHR data with the characteristics of temporality, high-dimensionality and irregularity.   Recurrent neural networks  have been widely used to analyze sequential data, unsurprisingly including medical events modelling for clinical prediction. For example, Choi et al. proposed a multi-level representation learning, which integrates visits and medical concepts based on visit sequences and the co-occurrence of medical concepts. They indirectly exploited an RNN to embed the visit sequences into a patient representation for downstream prediction tasks. Some other research works directly employed RNNs to model time-ordered patient visits for predicting diagnoses.  However, when the length of the patient visit sequence grows, such RNN-based models are restricted by the less expressive power of RNNs, such as vanishing gradient and forgetfulness.  However, such RNN-based models are constrained by forgetfulness, i.e., their predictive power drops significantly when the sequence of patient visits grows too long.  To memorize historical records, LSTM and GRU have been developed to utilize memory and gate mechanism for mitigating these issues.  To go further, Song et al. proposed to utilise attention mechanism in a deep framework to model sequential medical events.  It is worth noting that sequences of medical events are often found to be lengthy, especially when a patient suffers from chronic disease. Hence, due to the restricted ability of RNNs for long-term dependency modeling , the traditional RNNs, even with memory cells and gates, usually underperform in the cases of a long sequence of medical events. In light of this, a neural model that can overcome the performance bottleneck of RNN-based models is particularly desirable for medical predictions based on longitudinal EHR data.   %%%%%%%%  WHAT THE RELATION BETWEEN SHEN2018DISAN AND THIS ONE?? in a Directional self-attention networks can alleviate long sequence problems to improve the accuracy of predictions, as these models can be trained on all available input information - past and future.. CAN WE COME TO THE CONCLUSION: ONE OF CONTRIBUTION IS WE HAVE FULLY CONSIDERED ALL MEDICAL EVENTS COMPARING TO OTHER WORKS THAT CAN ONLY PARTIALLY CONSIDER.  % Recently, attention mechanism has been integrated into RNNs to model sequential EHRs data, which achieves good prediction accuracy. Although the attention-based RNNs relatively improves the prediction performance, the limitations of RNNs weaken the advantage of attention mechanism. In natural language processing , a sole attention mechanism has been used to construct a sequence to sequence model that achieves a state-of-the-art quality score on the neural machine translation  task. The attention mechanism has more flexibility in sequence length than RNN, and is more task/data-driven when modeling dependencies. Unlike sequential models, its computation can be easily and significantly accelerated by existing distributed/parallel computing schemes. However, to the best of our knowledge, a neural net entirely based on attention has not been designed for patient journey in EHRs data.  Most recently, attention mechanisms have sprung to the fore as effective integrations with RNNs for modeling sequential EHR data. So far, these approaches have shown satisfactory prediction accuracy, but some argue that the power of attention in an RNN is limited by weaknesses in the RNN itself . In particular, Vaswani et al. used a sole attention mechanism, i.e., multi-head attention and self-attention, to construct a sequence-to-sequence model for neural machine translation tasks and achieved a state-of-the-art quality score. And according to  Shen et al., self-attention mechanism allows for more flexibility in sequence lengths than RNNs and is more task/data-driven when modeling contextual dependencies. Unlike recurrent models, attention procedure is easy to compute and the computation can also be significantly accelerated with distributed/parallel computing schemes.  For example, Song et al. proposed to employ 1D CNN  to model local context and use attention mechanism  to capture long-term dependency for sequential medical events.  However, when applied to EHR data instead of regular sequential data , the current attention models cannot appropriately deal with some aspects of EHR data, such as arbitrary time-stamps and hierarchical data format.  Hence, to the best of our knowledge, a neural network-based entirely on attention has never been designed for analytics with EHR data.   To bridge the gap in this literature and address some of the open issues listed above, we propose a novel attention mechanism called Masked Encoder  for temporal context fusion. It uses self-attention to capture contextual information and temporal dependencies between a patient's visits.  Then, we propose an end-to-end neural network, called Bidirectional temporal encoder Network , to predict medical outcomes by leveraging a learned representation of the patient journey,  where the representation is generated solely by the proposed attention mechanism, MasEnc. BiteNet constructs a multi-level self-attention network to represent visits and patient journeys simultaneously, using attention pooling and stacked MasEnc layers. It is worth noting that, compared to the existed RNN-based methods, BiteNet can yield better prediction performance for long sequences of medical records.   Experiments conducted on two supervised prediction and two unsupervised clustering tasks with real-world EHR datasets demonstrate that the proposed BiteNet model is superior to prior state-of-the-art baseline methods.   To summarize, our main contributions are:   % The remainders of this paper are organized as follows. Section reviews related studies. In Section, we briefly discuss some preliminary, and details about our model are presented in Section. In Section, we demonstrate the experimental results conducted on real-world datasets. Lastly, we conclude our study in Section.%and outline our future work  %     Applying deep learning to healthcare analytical tasks has recently attracted enormous interest in healthcare informatics. Recurrent neural network  has been widely used to model medical events in sequential EHR data for clinical prediction tasks. For instance, Choi et al. indirectly exploit an RNN to embed the visit sequences into a patient representation by multi-level representation learning to integrate visits and medical codes based on visit sequences and the co-occurrence of medical codes. Other research works have, however, used RNNs directly to model time-ordered patient visits for predicting diagnoses. The Convolutional Neural Network  also has been exploited to represent a patient journey. For example, Nguyen et al. transforms a record into a sequence of discrete elements separated by coded time gaps, and then employ CNN to detect and combine predictive local clinical motifs to stratify the risk. Song et al. use CNN in code level to learn visit embedding. These RNN- and CNN-based models follow ideas of processing sentences in documents from NLP to treat a patient journey as a document and a visit as a sentence, which only has a sequential relationship, while two arbitrary visits in one patient journey may be separated by different time intervals, an important factor in longitudinal studies. Attention-based neural networks have been exploited successfully in healthcare tasks to model sequential EHR data and have been shown to improve predictive   performance.    We proposed  a sentence distribution estimator to alleviate the impact of noisy distant supervision labeled data for n-ary cross-sentence relation extraction;   a weaker distant supervision assumption, which considers non-consecutive sentences; and  a universal relation extractor, which is a hybrid model of attention mechanism and non-linear transformation layer that encodes both non-consecutive and consecutive sentence groups. The experiments showed that the proposed model reduces the impact of noisy data and achieves significantly better performance for n-ary cross sentence relation extraction compared to SotA models.  
","   Applying deep learning to healthcare analytical tasks has recently attracted enormous interest in healthcare informatics. Recurrent neural network  has been widely used to model medical events in sequential EHR data for clinical prediction tasks. For instance, Choi et al. indirectly exploit an RNN to embed the visit sequences into a patient representation by multi-level representation learning to integrate visits and medical codes based on visit sequences and the co-occurrence of medical codes. Other research works have, however, used RNNs directly to model time-ordered patient visits for predicting diagnoses. The Convolutional Neural Network  also has been exploited to represent a patient journey. For example, Nguyen et al. transforms a record into a sequence of discrete elements separated by coded time gaps, and then employ CNN to detect and combine predictive local clinical motifs to stratify the risk. Song et al. use CNN in code level to learn visit embedding. These RNN- and CNN-based models follow ideas of processing sentences in documents from NLP to treat a patient journey as a document and a visit as a sentence, which only has a sequential relationship, while two arbitrary visits in one patient journey may be separated by different time intervals, an important factor in longitudinal studies. Attention-based neural networks have been exploited successfully in healthcare tasks to model sequential EHR data and have been shown to improve predictive   performance.",22
"   Systematic Generalization has been characterized as the capacity to understand and produce a potentially infinite number of novel combinations from known components . For example, in Figure, a model could be exposed to a set of facts , but not to all the possible facts that can be inferred by combination of the known components . More recent work has examined systematic generalization in terms of the ability of ``a model to manipulate concepts in new combinations after being trained on all concepts, but only on a limited set of their combinations'' . This view of systematic generalization shifts emphasis from reasoning to learning. %If a model is able to perfectly accomplish a task by leveraging existing facts to infer new ones, we deem the model is generalizing systematically. Here we examine systematic generalization through measuring the ability of a model to reason about new inference step combinations despite being trained on a limited subset of them. %, and conditioning upon a small subset of active relationships at inference time.   Recent developments in natural language processing  have shown that Transformer  Language Models  are able to capture linguistic knowledge , and yield state-of-the-art performance for many NLP tasks , including but not limited to answering reading comprehension questions  and generating factual knowledge  with little to no task supervision. These models are optimized on large corpora to predict the next words or a set of masked words in a sentence. While yielding impressive results, it is not clear if TLMs rely on many superficial patterns in the data or if they actually learn re-usable skills, enabling them to generalize to new tasks by leveraging the compositionality of those skills . Training on massive data can give certain advantages with respect to understanding the meanings of words, but we conjecture that such data gives models less experience with reasoning over inference chains.    In our work, we study the less understood issues related to how well TLMs are able to perform long chains of reasoning. In particular, we use TLMs for the task of theorem proving, where facts and proofs are specified in natural language. Using theorem proving, we test if TLMs can generate interpretable proofs with logically consistent language modeling as their main objective. % In this setting, language models have various attractive properties: they require no logical rule engineering while still being interpretable, do not need human annotations, and are easy to extend to more data. % Language models have many advantages over theorem provers: they require no rule engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. In particular, we study their behavior as logical reasoners on text by analyzing the generated proofs and the final answer. This setup allows us to evaluate the reasoning and generalization capabilities of TLMs. Recent work such as  suggest that language models can be treated as knowledge bases. This directly motivates us to investigate if language models can also learn certain reasoning strategies. Studying these abilities can give us insights to facilitate the use of such models as dynamic knowledge bases that could infer new knowledge even if it is not seen during pre-training.  For natural language theorem proving, we use the question answering CLUTRR benchmark suite  to perform controlled studies. This dataset is of interest because:  the compositional nature of tasks involved make it well suited for evaluating systematic generalization, and  each question--answer pair is accompanied by a proof that can be used to explain how to arrive at the answer. %Our goal is not to obtain state-of-the-art results on this dataset, rather, We use this dataset as a medium to understand the reasoning capacity of TLMs.  Our experiments reveal the following:   To the best of our knowledge, we are the first to use a language modeling objective to do interpretable theorem proving with a Transformer. We hope that this work can shed some light on the reasoning capacity of TLMs and inspire future research to design models with greater reasoning capacity.       Systematic generalization has recently been in spotlight due to its importance in understanding the strengths and weaknesses of neural networks.  identify and evaluate the generalization capacity of visual question answering models. We however focus this study on a fully natural language domain.  There have been several recent studies on explicitly evaluating systematic generalization capabilities of natural language understanding and generation.   introduce a natural language inference  dataset which proves to be challenging for language understanding models for compositional generalization.  also evaluate systematic generalization in an NLI setting with controlled test cases to observe the failures of neural architectures.  We focus this study on the systematic generation and reasoning capabilities of Transformer-based  language model using CLUTRR , a dataset for logical question answering which provides access to the underlying logical proofs. We however focus this study on the systematic generation of logical reasoning sentences by Transformer-based  language models in a question answering setting with the CLUTRR suite . Similar datasets include SCAN  which has been instrumental to test systematic generalization  and CFQ  which measures the systematicity of language understanding via a question answering setup.  propose a series of baseline models with the CLUTRR dataset but none of them took advantage of the provided proof attached with each example. In addition, their Transformer baselines were not fine-tuned on the task. Unlike them, we focus on learning and generating proofs for studying systematic generalization.   Theorem provers are effective and interpretable solutions for systematically composing known facts into novel ones .   However these systems often rely on pre-defined sets of entities, relations, and logical rules. Identifying these building blocks requires complex NLP pipelines often requiring supervised training data . Neural proof generation  and neural theorem proving  have been explored in previous work. They tend to combine symbolic and statistical approaches to leverage the compositionality and interpretability of symbolic systems and the flexibility of statistical systems. Nevertheless, these combined systems all assume some predefined set of atoms and rules making up the environment. We instead use natural language text to define our environment and measure the limits of a purely statistical approach.  Similarly to us,  leverage logical rules expressed in natural language to answer compositional questions.  The authors show that Transformer encoders have great generalization performances, even on novel domains never seen during training.  However their system is not generative, rather they predict a true/false binary label on candidate answers. We instead focus on the systematic generalization capacity of generating proofs and using them to generate the final answer.     ========================================================   This work is a first step at exploring the robustness of NLP models used for automatic ICD-9 code classification. Clinical documents are different from regular documents as they are typically generated in a fast-paced environment with higher than average typos and non-standard acronyms. As a result, clinical NLP models are more susceptible to adversarial samples compared to a regular NLP model trained on a standard English dataset. A key extension of the work would be to consider a dictionary learnt from clinical documents and biomedical literature as a defense against these character-level perturbations. Although this might mitigate the decrease in performance, it wouldn't completely solve it. A more rigorous way to deal with this would be to account for this in the tokenization strategy. It is easy to push a word out of vocabulary when using tokenization strategies like word2vec and GloVe. Other strategies that model words unseen in training dataset such as word-piece and byte-pair encoding will also break when typos are introduced because these models learn sub words from a standard dictionary. Therefore, any defense must account for these typos in the fundamental tokenization strategy. An interesting direction would be to learn a word similarity metric and map an unknown word to a closer word in the vocabulary given the input word and the context in which it appears. Building a robust tokenization strategy would be the first step towards a robust NLP model against character-level adversarial attacks.     \medskip  \small    
","   Systematic generalization has recently been in spotlight due to its importance in understanding the strengths and weaknesses of neural networks.  identify and evaluate the generalization capacity of visual question answering models. We however focus this study on a fully natural language domain.  There have been several recent studies on explicitly evaluating systematic generalization capabilities of natural language understanding and generation.   introduce a natural language inference  dataset which proves to be challenging for language understanding models for compositional generalization.  also evaluate systematic generalization in an NLI setting with controlled test cases to observe the failures of neural architectures.  We focus this study on the systematic generation and reasoning capabilities of Transformer-based  language model using CLUTRR , a dataset for logical question answering which provides access to the underlying logical proofs. We however focus this study on the systematic generation of logical reasoning sentences by Transformer-based  language models in a question answering setting with the CLUTRR suite . Similar datasets include SCAN  which has been instrumental to test systematic generalization  and CFQ  which measures the systematicity of language understanding via a question answering setup.  propose a series of baseline models with the CLUTRR dataset but none of them took advantage of the provided proof attached with each example. In addition, their Transformer baselines were not fine-tuned on the task. Unlike them, we focus on learning and generating proofs for studying systematic generalization.   Theorem provers are effective and interpretable solutions for systematically composing known facts into novel ones .   However these systems often rely on pre-defined sets of entities, relations, and logical rules. Identifying these building blocks requires complex NLP pipelines often requiring supervised training data . Neural proof generation  and neural theorem proving  have been explored in previous work. They tend to combine symbolic and statistical approaches to leverage the compositionality and interpretability of symbolic systems and the flexibility of statistical systems. Nevertheless, these combined systems all assume some predefined set of atoms and rules making up the environment. We instead use natural language text to define our environment and measure the limits of a purely statistical approach.  Similarly to us,  leverage logical rules expressed in natural language to answer compositional questions.  The authors show that Transformer encoders have great generalization performances, even on novel domains never seen during training.  However their system is not generative, rather they predict a true/false binary label on candidate answers. We instead focus on the systematic generalization capacity of generating proofs and using them to generate the final answer.     ========================================================",23
" Singing voice synthesis  aims to synthesize high-quality and expressive singing voices based on musical score information, and attracts a lot of attention in both industry and academia ~. Singing voice synthesis shares similar pipeline with text to speech synthesis, and has achieved rapid progress~ with the techniques developed in text to speech synthesis~.   Most previous works on SVS~ adopt the same sampling rate  as used in text to speech, where the frequency bands or sampling data points are not enough to convey expression and emotion as in high-fidelity singing voices. However, simply increasing the sampling rate will cause several challenges in singing modeling. First, the audio with higher sampling rate contains wider and higher frequency bands\footnote{According to Nyquist-Shannon sampling theorem~, a sampling rate  can cover the frequency band up to . Therefore, the frequency band for the audio with 48kHz sampling rate spans from 024kHz while 012kHz for 24kHz sampling rate. The additional high frequency band 1224kHz increases the difficulty of modeling since high-frequency signals are more complicated and less predictive.}, which throws challenges when predicting these frequency spectrums in acoustic model. Second, the audio with higher sampling rate contains longer waveform points and much fine-grained fluctuations in a fixed period of time\footnote{For example, a 1 second audio waveform contains 48,000 sampling points when sampling rate is 48kHz.}, which also increases the difficulty of vocoder modeling in time domain. As a consequence, even if some previous works~ adopt higher sampling rate , they either leverage coarse-grained MFCC~ as acoustic features in slow autoregressive neural vocoder~, or use non-neural vocoder such as Griffin-Lim~ and WORLD~ to generate waveform, which do not fully exploit the potential of high sampling rate and thus cannot yield good voice quality.  In this paper, we develop HiFiSinger, an SVS system towards high-fidelity singing voices. HiFiSinger adopts FastSpeech~ as the acoustic model and Parallel WaveGAN~ as the vocoder since they are popular in speech synthesis~ to ensure fast training and inference speed and also high quality. %. Instead of using Griffin-Lim, WORLD or autoregressive neural model such as WaveRNN and WaveNet as the vocoder, HiFiSinger leverages  To address the challenges of high sampling rate in singing modeling , we design multi-scale adversarial training on both acoustic model and vocoder, and introduce several additional systematic designs and findings that are crucial to improve singing modeling:   We conduct experiments on our internal singing voice synthesis datasets that contain 11 hours high-fidelity singing recordings with 48kHz sampling rate. Experiment results demonstrate the advantages of our developed HiFiSinger over previous singing voice synthesis system. Further ablation studies verify the effectiveness of each design in HiFiSinger to generate high-fidelity voices.    In this section, we brie閾垮 introduce the background of this work, including the comparison between singing voice synthesis  and text to speech , the challenges of high fidelity singing voice synthesis.  \paragraph{SVS vs. TTS} Text to speech  aims to synthesize speech voice from a given text, which has evolved quickly from early concatenative synthesis , statistical parametric synthesis~, to neural network based parametric synthesis~, and to currently end-to-end neural models. The end-to-end models directly map input text or phonetic characters to output speech, which greatly simplifies the training pipeline and reduces the requirements for linguistic and acoustic knowledge. Popular end-to-end TTS systems include FastSpeech~, Tacotron 2~, etc. With the rapid development, TTS has been applied to various scenarios and has been the basic technology in singing voice synthesis ~. However, SVS has distinct features compared with TTS, since SVS needs more information  in addition to the given lyric  to synthesize singing voices with wide range of pitches, long vowel durations. Furthermore, singing voices focus more on expression and emotion rather than content as in speaking voices, which requires higher sampling rate than speaking voices to ensure high-fidelity voices and thus throws great challenges for singing modeling.   \paragraph{High-Fidelity SVS} Singing voices usually leverage high sampling rate to convey high-fidelity expression. For example, popular music websites such as Spotify, Apple Music, SoundCloud, QQ Music and NetEase Music all use high sampling rate . However, high sampling rate increases the difficulty of singing modeling: 1) high sampling rate causes wider spectrum band in frequency domain, where different frequency bands with distinctive characteristics make it hard for acoustic model; 2) high sampling rate causes longer waveform in a fixed period of time, where more sampling points and finer-grained fluctuations make it difficult for vocoder. Most previous works on SVS usually adopt 16kHz or 24kHz sampling rate as used in TTS. There indeed exist some works using 44.1kHz or 48kHz sampling rate~. However, they either leverage coarse-grained MFCC~ as acoustic features in slow autoregressive neural vocoder~, or use non-neural vocoder such as Griffin-Lim~ and WORLD~ to generate waveform, which cannot fully exploit the potential of high sampling rate and thus cannot yield good voice quality.   \paragraph{Vocoder} Popular pipeline for singing voice synthesis usually contains an acoustic model that maps lyrics and music score input to acoustic features such as mel-spectrogram, and a vocoder that generates singing waveform given generated acoustic features. High-fidelity singing voice not only requires high-quality acoustic features generated by the acoustic model, but also largely depends on the quality of vocoder. Previous SVS systems mostly utilize Griffin-Lim~ and WORLD~ as vocoder. Although these vocoders are simple, their synthesis quality is usually poor. Recently, some SVS systems leverage neural vocoders, but most of them~ are based on autoregressive generation which suffer from slow inference speed especially for audio with high sampling rate. In this work, we adopt a non-autoregressive vocoder called Parallel WaveGAN~ for the fast inference, with specific designs to ensure the voice quality while handle the high sampling rate challenge.       ========================================================   we are interested in understanding the current limitations of Transformers  In this work, we carefully crafted a series of experiments to understand the systematic generalization capacity of Transformer language models in a symbolic reasoning question answering dataset.  While being powerful language modelers, we believe that if Transformers are to be part of our future personal assistants, they should be able to capture logical statements expressed in natural language and to extrapolate them to unseen proofs. TLMs are state of the art models for a wide variety of natural language processing tasks. Given their widespread use, it is important to understand the limits of their ability to reason on knowledge expressed in natural language and to extrapolate learned inference procedures to unseen problem instances. Our explorations reveal multiple insights. Firstly, TLMs suffer from length-generalization issues in generating proofs. Secondly, TLMs get better at reasoning when trained with longer, exhaustive proofs.  TLMs also generalize better by leveraging backward-chaining proofs than the forward-chaining proofs.  of these properties of Transformers provides a first important evaluation in this setting.   and has led us to insights allowing us to dramatically increase their ability to systematically generalize through a simple named entity transformation. In addition, the fact that backward-chaining proof models perform better than forward-chaining ones makes us believe that backward-chaining strategies are easier to use albeit being harder to generate. Moreover, we find that no-proof models perform better than those trained to produce proofs. We conjecture that benefiting from naturally stated logical proof statements requires more complex internal representations.   At the same time, we believe that in some cases, people would prefer an interpretable system at the cost of slightly lower accuracy.   We will explore both of these directions in future research projects. Recent work on developing position-agnostic attention mechanisms for Transformers  can be useful as a future direction to develop generalizable models. Furthermore, our results motivates the use of neuro-symbolic methods such as Neural Theorem Provers  as an alternative avenue to achieving systems that systematically generalize on logical and compositional reasoning tasks. Combining these approaches with large pre-trained language models is left as future research. We hope that this work will inspire research on the systematic generalization capacity of language models and motivate further study and the creation of neural models with greater reasoning capacity.   rather than with greater number of parameters and training data.   shed some light on their symbolic reasoning capacity of Transformers and inspire future research directions   
"," In this section, we brie闁惧灝 introduce the background of this work, including the comparison between singing voice synthesis  and text to speech , the challenges of high fidelity singing voice synthesis.  \paragraph{SVS vs. TTS} Text to speech  aims to synthesize speech voice from a given text, which has evolved quickly from early concatenative synthesis , statistical parametric synthesis~, to neural network based parametric synthesis~, and to currently end-to-end neural models. The end-to-end models directly map input text or phonetic characters to output speech, which greatly simplifies the training pipeline and reduces the requirements for linguistic and acoustic knowledge. Popular end-to-end TTS systems include FastSpeech~, Tacotron 2~, etc. With the rapid development, TTS has been applied to various scenarios and has been the basic technology in singing voice synthesis ~. However, SVS has distinct features compared with TTS, since SVS needs more information  in addition to the given lyric  to synthesize singing voices with wide range of pitches, long vowel durations. Furthermore, singing voices focus more on expression and emotion rather than content as in speaking voices, which requires higher sampling rate than speaking voices to ensure high-fidelity voices and thus throws great challenges for singing modeling.   \paragraph{High-Fidelity SVS} Singing voices usually leverage high sampling rate to convey high-fidelity expression. For example, popular music websites such as Spotify, Apple Music, SoundCloud, QQ Music and NetEase Music all use high sampling rate . However, high sampling rate increases the difficulty of singing modeling: 1) high sampling rate causes wider spectrum band in frequency domain, where different frequency bands with distinctive characteristics make it hard for acoustic model; 2) high sampling rate causes longer waveform in a fixed period of time, where more sampling points and finer-grained fluctuations make it difficult for vocoder. Most previous works on SVS usually adopt 16kHz or 24kHz sampling rate as used in TTS. There indeed exist some works using 44.1kHz or 48kHz sampling rate~. However, they either leverage coarse-grained MFCC~ as acoustic features in slow autoregressive neural vocoder~, or use non-neural vocoder such as Griffin-Lim~ and WORLD~ to generate waveform, which cannot fully exploit the potential of high sampling rate and thus cannot yield good voice quality.   \paragraph{Vocoder} Popular pipeline for singing voice synthesis usually contains an acoustic model that maps lyrics and music score input to acoustic features such as mel-spectrogram, and a vocoder that generates singing waveform given generated acoustic features. High-fidelity singing voice not only requires high-quality acoustic features generated by the acoustic model, but also largely depends on the quality of vocoder. Previous SVS systems mostly utilize Griffin-Lim~ and WORLD~ as vocoder. Although these vocoders are simple, their synthesis quality is usually poor. Recently, some SVS systems leverage neural vocoders, but most of them~ are based on autoregressive generation which suffer from slow inference speed especially for audio with high sampling rate. In this work, we adopt a non-autoregressive vocoder called Parallel WaveGAN~ for the fast inference, with specific designs to ensure the voice quality while handle the high sampling rate challenge.",24
" Deep speech representation learning has been the subject of a large number of past works. Many techniques have been developed and employed for extracting representations from speech for related tasks such as speaker recognition  and speech emotion recognition  using deep learning. A significant number of these deep learning models have been based on Convolutional Neural Networks  for SR  and SER . The most common approach to training CNN models for speech-related tasks is to use time-frequency inputs such as spectrograms derived from raw audio signals. Given sufficient data, such deep learning models enable the extraction of better speech representations compared to other methods such as i-Vectors .   Attention mechanisms have been shown to have a positive impact on extracting effective deep representations from input data, for instance speech signals. Considerable improvements in accuracy of emotion recognition models  and speaker recognition models  are some of the examples that demonstrate the potential benefits of using attention mechanisms for representation learning.   Attention models uphold a memory-query paradigm, where the memory is a set of information items such as CNN embeddings of a region of the spectral representation in speech-related tasks , or a part of the utterance embedded by a recurrent cell in a recurrent neural network  . The query is derived from a hidden state of the model from either the same modality or a different one . The majority of attention models used in speech-related tasks, use features extracted from utterances using a deep neural network as the information items or memory, and the last hidden layer of the model as the query . The general purpose of an attention model in generating deep representations of speech signals is to focus on each information item individually.   The information items considered in an attention model define the granularity of what the model can focus on. The spectral representation of an utterance enables deep learning models to consider fine-grained features such as frequency bins in very short time-frames. However, typical attention models used on audio signals utilize an embedding obtained from a CNN model as the memory and the final embedding of the model as query. Using embeddings obtained from CNNs, limits the granularity of the attention models to large regions of the spectral representation. On the other hand, improving the granularity of CNN embeddings of an utterance leads to very large attention models which are harder to train and prone to over-fitting. While there have been a number of studies investigating various attention models using CNN embeddings utterances , very limited number of studies aim to use more fine-grained attention models on spectral representation of the utterance.   In this paper, we address the challenge of improving granularity of attention models by introducing a fine-grained attention mechanism for audio signals. This mechanism enables deep learning models to focus on individual frequency bins of a spectrogram without the drawbacks of having very complex models that typically involve large number of parameters. The aim of this model is to attend to each frequency bin in the spectrogram representation in order to boost the contribution of most salient bins. This mechanism also helps reduce the importance of bins with no useful information leading to more accurate representations, which can also lead to more robustness with respect to existing noise in the input audio. The performance of the proposed attention mechanism has been tested using a select set of most prominent CNN architectures on two tasks of SR and SER. The experimental results show that deploying the fine-grained frequency attention mechanism improves the performance of all the benchmark networks substantially while being less impacted by added noise.   Our contributions in this paper are as follows:   The rest of this paper is organized as follows. First, we discuss the related work in the area of speech representation learning followed by particular approaches that have used attention mechanisms for this purpose. Next, we present the proposed attention mechanism. In the following section, we discuss the experiments along with implementation details. Next, we provide the results of our work. And finally, we summarize and conclude the paper.      Speech representation, or utterance embedding, has been an area of research for decades. Classical signal processing techniques such as Gaussian Mixture Models, Hidden Markov Models, and Universal Background Models, were used in many speech related tasks to obtain a proper representation of utterances. Comprehensive reviews of prior work that have used such conventional methods for SR and SER can be found in .   Solutions based on artificial neural networks  have been widely used in speech-related tasks. In some of the earlier work in this area, speech representations extracted from audio signals using ANNs were fed to conventional classifiers for SR  and SER .   More recently, deep neural networks  have been used for learning effective representations of utterances . Most recent works on extracting deep speech representations for SR have explored the impacts of different deep learning architectures on the quality of these representations. Most prominent works include using CNN architectures such as ResNets for speech representation learning prior to identification . Among other speech-related tasks such as SER, DNN models have also been very successful for speech representation learning. Most recent studies of SER focus on improving the accuracy of the deep learning models by modifying and combining different architectures. Some of the considerable attempts include using the combination of CNN and RNNs such as long short-term memory  networks .   The performance of deep learning models has improved significantly by attention models in many cases . A number of studies using attention mechanisms for SR and SER have shown substantial improvements compared to baseline models. Attention mechanisms in SR and SER have been utilized to focus on features extracted from utterances using various deep learning models including CNN , RNN , and time-delay neural networks  . Through the following paragraphs we briefly describe some examples.  The model proposed in  utilized self-attention to focus on features obtained from a CNN model inspired by VGGNet . The study done in  used CNN-based self-attention models to attend to features extracted from a deep learning model with an architecture similar to ResNet . A novel gated attention model was proposed in  to attend to features extracted by a modified version of CNN, namely gated-CNN. The proposed models in , utilized attention models to focus on differences between two sets of features extracted from the enrollment utterance and the questioned utterance using RNN. In the common approach taken in these studies, the attention models were added to the end of deep learning pipelines. The addition of attention models in this way has shown to improve the accuracy of baseline models against in-the-wild datasets in each of these studies.    A different approach was taken in  and . The attention models used in these studies replaced the statistical pooling layer of an X-Vector model. The proposed models utilized TDNN to extract frame-level features from utterances. Attention models were then used to aggregate the features into an utterance-level embedding. The model proposed in  was evaluated against the NistSRE16 evaluation set  and the proposed model in  was evaluated against the VoxCeleb 1 test set . Both models showed substantial improvements compared to their baseline models.   The majority of the aforementioned studies have used the features obtained from DNNs as the memory component of the attention model. The queries of the attention models were also originated from the last hidden layer of the model from which the utterance-level embeddings are retrieved. Generally, DNNs learn to extract a low-dimensional latent representation from the input data without necessarily preserving localization with respect to the input information items. Thus, while the use of the last hidden layer of a DNN for extracting the query of an attention mechanism can be advantageous due to its reduced number of parameters, high levels of granularity and a localized relationship with respect to the input may not be achieved.  Compared to the methods proposed in previous studies, the fine-grained attention model proposed in this paper does not require embeddings obtained from DNN models, and can operate on spectrograms extracted from raw audio signals. Hence, the granularity of the attention model can be improved to attend to frequency-level features. While different attention mechanisms depend on the specific architectures and models, our proposed fine-grained frequency attention mechanism can be used along with various models and architectures. As proven in the experiment section, by adding the frequency attention to multiple CNN-based architectures, a substantial improvement is achieved on both tasks of SER and SR.        In this paper, we have developed HiFiSinger, an SVS system to synthesize high-fidelity singing voice. To address the challenges caused by high sampling rate, we designed a SF-GAN on acoustic model to better model the wider frequency band, a ML-GAN on vocoder to better model longer waveform sequences, and introduced several systematic designs and findings that are important to improve singing modeling. Experiment results show that HFiSinger synthesizes singing voices with much higher quality than previous systems. For future work, we will continue to close the quality gap between the synthesized voices and recordings, and also apply our fidelity solution in HiFiSinger to text to speech synthesis.         
","  Speech representation, or utterance embedding, has been an area of research for decades. Classical signal processing techniques such as Gaussian Mixture Models, Hidden Markov Models, and Universal Background Models, were used in many speech related tasks to obtain a proper representation of utterances. Comprehensive reviews of prior work that have used such conventional methods for SR and SER can be found in .   Solutions based on artificial neural networks  have been widely used in speech-related tasks. In some of the earlier work in this area, speech representations extracted from audio signals using ANNs were fed to conventional classifiers for SR  and SER .   More recently, deep neural networks  have been used for learning effective representations of utterances . Most recent works on extracting deep speech representations for SR have explored the impacts of different deep learning architectures on the quality of these representations. Most prominent works include using CNN architectures such as ResNets for speech representation learning prior to identification . Among other speech-related tasks such as SER, DNN models have also been very successful for speech representation learning. Most recent studies of SER focus on improving the accuracy of the deep learning models by modifying and combining different architectures. Some of the considerable attempts include using the combination of CNN and RNNs such as long short-term memory  networks .   The performance of deep learning models has improved significantly by attention models in many cases . A number of studies using attention mechanisms for SR and SER have shown substantial improvements compared to baseline models. Attention mechanisms in SR and SER have been utilized to focus on features extracted from utterances using various deep learning models including CNN , RNN , and time-delay neural networks  . Through the following paragraphs we briefly describe some examples.  The model proposed in  utilized self-attention to focus on features obtained from a CNN model inspired by VGGNet . The study done in  used CNN-based self-attention models to attend to features extracted from a deep learning model with an architecture similar to ResNet . A novel gated attention model was proposed in  to attend to features extracted by a modified version of CNN, namely gated-CNN. The proposed models in , utilized attention models to focus on differences between two sets of features extracted from the enrollment utterance and the questioned utterance using RNN. In the common approach taken in these studies, the attention models were added to the end of deep learning pipelines. The addition of attention models in this way has shown to improve the accuracy of baseline models against in-the-wild datasets in each of these studies.    A different approach was taken in  and . The attention models used in these studies replaced the statistical pooling layer of an X-Vector model. The proposed models utilized TDNN to extract frame-level features from utterances. Attention models were then used to aggregate the features into an utterance-level embedding. The model proposed in  was evaluated against the NistSRE16 evaluation set  and the proposed model in  was evaluated against the VoxCeleb 1 test set . Both models showed substantial improvements compared to their baseline models.   The majority of the aforementioned studies have used the features obtained from DNNs as the memory component of the attention model. The queries of the attention models were also originated from the last hidden layer of the model from which the utterance-level embeddings are retrieved. Generally, DNNs learn to extract a low-dimensional latent representation from the input data without necessarily preserving localization with respect to the input information items. Thus, while the use of the last hidden layer of a DNN for extracting the query of an attention mechanism can be advantageous due to its reduced number of parameters, high levels of granularity and a localized relationship with respect to the input may not be achieved.  Compared to the methods proposed in previous studies, the fine-grained attention model proposed in this paper does not require embeddings obtained from DNN models, and can operate on spectrograms extracted from raw audio signals. Hence, the granularity of the attention model can be improved to attend to frequency-level features. While different attention mechanisms depend on the specific architectures and models, our proposed fine-grained frequency attention mechanism can be used along with various models and architectures. As proven in the experiment section, by adding the frequency attention to multiple CNN-based architectures, a substantial improvement is achieved on both tasks of SER and SR.",25
" Text summarization aims to produce condensed summaries covering salient and non-redundant information in the source documents. Recent studies on single-document summarization  benefit from the advances in neural sequence learning  as well as pre-trained language models  and make great progress.  However, in multi-document summarization  tasks, neural models are still facing challenges and often underperform classical statistical methods built upon handcrafted features.    We observe two major challenges when adapting advanced neural SDS methods to MDS:   Large search space.  MDS aims at producing summaries from multiple source documents, which exceeds the capacity of neural SDS models  and sets learning obstacles for adequate representations, especially considering that MDS labeled data is more limited. For example, there are 287K training samples  on the CNN/Daily Mail SDS dataset and only 30 on the DUC 2003 MDS dataset .  High redundancy. In MDS, the same statement or even sentence can spread across different documents. Although SDS models adopt attention mechanisms as implicit measures to reduce redundancy, they fail to handle the much higher redundancy of MDS effectively .       There have been attempts to solve the aforementioned challenges in MDS. Regarding the large search space, prior studies  perform sentence filtering using a sentence ranker and only take top-ranked  sentences. However, such a hard cutoff of the search space makes these approaches insufficient in the exploration of the  labeled data and limited by the ranker since most sentences are discarded,\footnote{ is set to 7 in~\citet{lebanoff-etal-2018-adapting} and 15 in~\citet{zhang-etal-2018-adapting}. One document set in DUC 2004, for example, averages 265.4 sentences.} albeit the discarded sentences are important and could have been favored. As a result, although these studies perform better than directly applying their base SDS models  to MDS,  they do not outperform state-of-the-art MDS methods.  Regarding the high redundancy,  various redundancy measures have been proposed, including heuristic post-processing such as counting new bi-grams  and cosine similarity, or dynamic scoring that compares each source sentence with the current summary like Maximal Marginal Relevance .   Nevertheless, these methods still use lexical features without semantic representation learning. One extension of these studies uses capsule networks to improve redundancy measures. However, its capsule networks are pre-trained on SDS and fixed as feature inputs of classical methods  without end-to-end representation learning.  In this paper, we present a deep RL framework, MMR-guided Reinforcement Learning  for MDS, which unifies advances in SDS and one classical MDS approach, MMR through end-to-end learning. \ours addresses the MDS challenges as follows:  \ours overcomes the large search space through soft attention. Compared to hard cutoff, our soft attention favors top-ranked candidates of the sentence ranker . However, it does not discard low-ranked ones, as the ranker is imperfect, and those sentences ranked low may also contribute to a high-quality summary. Soft attention restrains the search space while allowing more exploration of the limited labeled data, leading to better representation learning. Specifically, \ours infuses the entire prediction of MMR into  its neural module by attending  to important sentences and downplaying the rest instead of completely discarding them.  \ours resolves the high redundancy of MDS in a unified way: the explicit redundancy measure in MMR is incorporated into the neural representation of the current state, and the two modules are coordinated by RL reward optimization, which encourages non-redundant summaries.  We conduct extensive experiments and ablation studies to examine the effectiveness of \ours. Experimental results show that \ours achieves state-of-the-art performance on the DUC 2004 and TAC 2011 datasets . A comparison between various combination mechanisms demonstrates the benefits of soft attention in the large search space of MDS . In addition, ablation and manual studies confirm that \ours is superior to applying either RL or MMR to MDS alone, and MMR guidance is effective for redundancy avoidance .  \start{Contributions}  We present an RL-based MDS framework that combines the advances of classical MDS and neural SDS methods via end-to-end learning.   We show that our proposed soft attention is better than the hard cutoff of previous methods for learning adequate neural representations. Also, infusing the neural representation of the current summary with explicit MMR measures significantly reduces summary redundancy.  We demonstrate that \ours achieves new state-of-the-art results on benchmark MDS datasets.        \start{Multi-document Summarization} Classical MDS explore both extractive  and abstractive methods. Many neural MDS methods  are merely comparable or even worse than classical methods due to the challenges of large search space and limited training data. Unlike DPP-Caps-Comb that incorporates neural measures into classical MDS as features, \ours opts for the opposite by endowing SDS methods with the capability to conduct MDS, enabling the potential of further improvement with advances in SDS.      \start{Bridging SDS and MDS} Initial trials adapting SDS models to MDS directly reuse SDS models. To deal with the large search space, a sentence ranker is used in the adapted models for candidate pruning.  Specifically, \citet{lebanoff-etal-2018-adapting} leverages  MMR to rank sentences, allowing only the words in the top-ranked sentences to appear in the generated summary. Similarly, \citet{zhang-etal-2018-adapting} uses topic-sensitive PageRank and computes attention only for the top-ranked sentences. Unlike \ours, these adapted models use hard cutoff and  lack end-to-end training, failing to outperform state-of-the-art methods designed specifically for MDS .   \clearpage           \section{Conclusions}              
","   \start{Multi-document Summarization} Classical MDS explore both extractive  and abstractive methods. Many neural MDS methods  are merely comparable or even worse than classical methods due to the challenges of large search space and limited training data. Unlike DPP-Caps-Comb that incorporates neural measures into classical MDS as features, \ours opts for the opposite by endowing SDS methods with the capability to conduct MDS, enabling the potential of further improvement with advances in SDS.      \start{Bridging SDS and MDS} Initial trials adapting SDS models to MDS directly reuse SDS models. To deal with the large search space, a sentence ranker is used in the adapted models for candidate pruning.  Specifically, \citet{lebanoff-etal-2018-adapting} leverages  MMR to rank sentences, allowing only the words in the top-ranked sentences to appear in the generated summary. Similarly, \citet{zhang-etal-2018-adapting} uses topic-sensitive PageRank and computes attention only for the top-ranked sentences. Unlike \ours, these adapted models use hard cutoff and  lack end-to-end training, failing to outperform state-of-the-art methods designed specifically for MDS .   \clearpage",26
" In recent years, neural LMs  have shown profound abilities to generate texts that could be almost indistinguishable from human writings . Neural LMs could be used to generate concise summaries , coherent stories , and complete documents given prompts . It is natural to question their source and extent of rhetorical knowledge: What makes neural LMs articulate, and how?  While some recent works query the linguistic knowledge , this open question remain unanswered. We hypothesize that contextualized neural LMs encode rhetorical knowledge in their intermediate representations, and would like to quantify the extent they encode rhetorical knowledge.  To verify our hypothesis, we hand-craft a set of 24 rhetorical features including those used to examine rhetorical capacities of students , and evaluate how well neural LMs encode these rhetorical features in the representations while encoding texts.  Recent work has started to evaluate encoded features from hidden representations. Among them, probing  has been a popular choice. Previous work probed morphological , agreement , and syntactic features . Probing involves optimizing a simple projection model from representations to features. The loss of this optimization measures the difficulty to decode features from the representations.   In this work, we use a probe containing self attention mechanism. We first project the variable-length embeddings to a fixed-length latent representation per document. Then, we apply a simple diagnostic classifier to detect rhetorical features from this latent representation. This design of probe reduces the total number of parameters, and enable us to better understand each model's ability to encode rhetorical knowledge. We find that:   These observations allow us to investigate the mechanisms of neural LMs to better understand the degree to which they encode linguistic knowledge. We demonstrate how discourse-level features can be queried and analyzed from neural LMs. All of our code and parsed tree data will be available at github.     Recent work has considered the interpretability of contextualized representations. For example, \citet{Jain2019AttentionIN} found attention to be uncorrelated to gradient-based feature importance, while \citet{wiegreffe-2019-attention} suggested such approaches allowed too much flexibility to give convincing results. Similarly, \citet{Serrano2019} considered attention representations to be noisy indicators of feature importance.  Many tasks in argument mining, similar to our task of examining neural LMs, require understanding the rhetorical aspects of discourse . This allows RST to be applied in relevant work. For example, RST enables understanding and analyzing argument structures of monologues  and, when used with other discourse features, RST can improve role-labelling in online arguments .  Probing neural LMs is an emergent diagnostic task on those models.  Previous work probed morphological , agreement , and syntactic features .  \citet{hewitt-liang-2019-designing} compared different probes, and recommended linear probes with as few parameters as possible, for the purpose of reducing overfitting. Recently, \citet{pimentel2020information} argued against this choice from an information-theoretic point of view. \citet{voita2020information} presents an optimization goal for probes based on minimum description length.  \citet{Liu2019} proposed 16 diverse probing tasks on top of contextualized LMs including token labeling , segmentation  and pairwise relations. While LMs augmented with a probing layer could reach state-of-the-art performance on many tasks, they found that LMs still lacked fine-grained linguistic knowledge. DiscoEval  showed that BERT outperformed traditional pretrained sentence encoders in encoding discourse coherence features, which our results echo.      \nocite{KedzieMcKeown19,shah2018bootstrapping} \nocite{budzianowski2018multiwoz,eric2019multiwoz,gavsic2015policy,hakkani2016multi,Cervoneetal19,shah2018bootstrapping,ultes2017pydial,chen2017deep}  This paper presents the first experiments on training an NLG for an extended domain ontology by re-using existing within-domain training data.  We show that we can combine two training datasets for the restaurant domain, that have different ontologies,  relying on distinct sets of dialogue acts and attributes, and generate output that combines attributes from both sources, by applying a combination of neural supervision and a novel self-training method.  While it is common practice to construct test sets with unseen attribute combinations, we know of no prior work based on constructing a new combined ontology. Our experiments show that the task is surprisingly adversarial, consistent with recent work suggesting that neural models often fail to generalize .  Work on  domain transfer shares similar goals to the experiments presented here  , but these methods do not produce NLG outputs that integrate attributes from two different sources into the same sentence. Our final results show that the ability of our self-training method to automatically construct new training instances  results in high quality natural, coherent and grammatical outputs with high semantic accuracy.    In future, we hope to generalize our novel self-training method to build an NLG that can combine two distinct domains, e.g.  hotels or movies combined with restaurants in multi-domain dialogue . Ideally systems that cover multiple domains should be able to produce utterances that seamlessly integrate both domains, if data exists for each domain independently.  However, there may be additional  challenges in such combinations. Our results require the initial neural models to generate {\bf some} combined outputs. It is not clear whether there are some aspects of our experimental setup that facilitate this, e.g. it may require  some attributes to be shared across the two initial ontologies, or  some shared vocabulary. Thus it is possible that initial models for two more distinct domains may not produce any combined outputs, and it may be necessary to seed the self-training experiments with a small number of combined training instances. We leave these issues to future work.      In future work we plan to investigate the use of our novel    self-training method for building an NLG by combining two distinct domains such    as hotel and restaurant information, where training data exists for    hotels alone and restaurants alone, e.g. to generate The Ritz is      a great place to stay because its rooms are lovely and its      restaurant serves excellent nouvelle cuisine, an utterance that    combines attributes from both domains.    This may be     {\color{red} say what assumptions we make as per the one review and our rebuttal,    say that method relies on that at least SOME output make the combination. worst    case is that you'd have to collect a small amount. May also be other training methods    can try to force more combinations.} In    We also plan to investigate whether stylistic attributes from  one source can be injected into utterances from another  source.      
"," Recent work has considered the interpretability of contextualized representations. For example, \citet{Jain2019AttentionIN} found attention to be uncorrelated to gradient-based feature importance, while \citet{wiegreffe-2019-attention} suggested such approaches allowed too much flexibility to give convincing results. Similarly, \citet{Serrano2019} considered attention representations to be noisy indicators of feature importance.  Many tasks in argument mining, similar to our task of examining neural LMs, require understanding the rhetorical aspects of discourse . This allows RST to be applied in relevant work. For example, RST enables understanding and analyzing argument structures of monologues  and, when used with other discourse features, RST can improve role-labelling in online arguments .  Probing neural LMs is an emergent diagnostic task on those models.  Previous work probed morphological , agreement , and syntactic features .  \citet{hewitt-liang-2019-designing} compared different probes, and recommended linear probes with as few parameters as possible, for the purpose of reducing overfitting. Recently, \citet{pimentel2020information} argued against this choice from an information-theoretic point of view. \citet{voita2020information} presents an optimization goal for probes based on minimum description length.  \citet{Liu2019} proposed 16 diverse probing tasks on top of contextualized LMs including token labeling , segmentation  and pairwise relations. While LMs augmented with a probing layer could reach state-of-the-art performance on many tasks, they found that LMs still lacked fine-grained linguistic knowledge. DiscoEval  showed that BERT outperformed traditional pretrained sentence encoders in encoding discourse coherence features, which our results echo.",27
"  Social media has become an essential element of our society by which people communicate and exchange information on a daily basis. The strong influence of social media on internet users has been of great benefit to many individuals, businesses, and organizations. Many companies and organizations nowadays use social media to reach customers, promote products, and ensure customer satisfaction. Despite the benefits associated with the widespread use of social media, they remain vulnerable to ill-intentioned activities, as the openness, anonymity, and informal structure of these platforms have contributed to the spread of harmful and violent content. \par  Although social media service providers have policies to control these ill-intentioned behaviors, these rules are rarely followed by users. Social media providers also allow their users to report any inappropriate content, but unreported content may not be discovered due to the huge volume of data on these platforms. Some countries have restricted the use of social media, and others have taken legal action regarding violent or harmful content that might target particular individuals or communities. However, these violations might end up unpunished due to the anonymous nature of these platforms, allowing ill-intentioned users to fearlessly share harmful content by using nicknames or fake identities. One of the most-shared harmful content on social media is hate content, which might take different forms such as text, photos, and/or video. Hate speech is any expression that encourages, promotes, or justifies violence, hatred, or discrimination against a person or group of individuals based on characteristics such as color, gender, race, sexual orientation, nationality, religion, or other attributes. Online hate speech is rapidly increasing over the entire world, as nearly \% of the world閳ユ獨 population  communicates on social media. Studies have shown that nearly \% of Americans have experienced online hate and harassment. This result is \% higher than the results of a comparable questionnaire conducted in  . For younger people, the results show that \% of teenagers frequently encounter hate speech on social media.  \par   One of the most dangerous and influential forms of online hate speech is led and spread by supporters of extreme ideologies who target other racial groups or minorities. White supremacists are one of the ideological groups who believe that people of the white race are superior and should be dominant over people of other races; this is also referred to as white nationalism in more radical ideologies. White supremacists claim that they are undermined by dark skin people, Jews, and multicultural Muslims, and they want to restore white people閳ユ獨 power, violently if necessary. They have also claimed responsibility for many violent incidents that happened in the s, including bank robberies, bombings, and murders. The white supremacist ideology has been adopted by both right-wing and left-wing extremists who combine white supremacy with political movements. \par   White supremacist hate speech has become a significant threat to the community, either by influencing young people with hateful ideas or by creating movements to implement their goals in the real world. A study has also suggested links between hate speech and hate crimes against others . Several recent brutal attacks have also been committed by supporters of radical white supremacists who were very active members on social media. The mass shootings in New Zealand, Texas, and Norway were committed by white supremacists who had shared their opinions and ideologies on social media. The attacker of two mosques in Christchurch, New Zealand, was a 28 year old man who identified himself as a white nationalist hero, and posted a manifesto that discussed his intent to kill people as a way to reinforce the sovereignty of white extremists. From a psychological point of view, any violent attack must be preceded by warning behaviors, which includes any behavior that shows before a violent attack that is associated with it, and can in certain situations predict it. Warning behaviors can be either real-world markers  or linguistic markers or signs  which can happen in real life and/or online.  \par   Automatic detection of white supremacist content on social media can be used to predict hate crimes and violent events. Perpetrators can be caught before attacks happen by examining online posts that give strong indications of an intent to make an attack. Predicting violent attacks based on monitoring online behavior would be helpful in crime prevention, and detecting hateful speech on social media will also help to reduce hatred and incivility among social media users, especially younger generations. \par  Studies have investigated the detection of different kinds of hate speech such as detecting cyberbullying , offensive language  , or targeted hate speech in general by distinguishing between types of hate speech and neutral expressions. Others have dealt with the problem by detecting a specific types of hate speech, such as anti-religion, jihadist, sexist, and racist. However, less attention has been given to detecting white supremacism in particular, with limited studies.   \par  White supremacist extremists tend to use rhetoric   in their language. They also use specific vocabulary, abbreviations, and coded words to express their beliefs and intent to promote hatred or encourage violence to avoid being detected by traditional detection methods. They mostly use hate speech against other races and religions, or claim that other races are undermining them. Figure shows an example of a white supremacist tweet.  \par  \subsection{Research goal and contributions}  In this paper, we aim to detect white supremacist tweets based on textual features by using deep learning techniques. We collected about  tweets from white supremacist accounts and hashtags to extract word embeddings, and then we labeled about  subsets of the data corpus to build a white supremacist dataset. We applied two approaches: the first uses domain-specific word embedding learned from the corpus and then classifies  tweets using a Bidirectional LSTM-based deep model. This approach is evaluated on multiple dataset and achieved different results depending on the datasets that ranged from a \% to a \% F1-score. The second approach uses a pre-trained language model that is fine-tune on the white supremacist dataset using Neural Network dense layer. The BERT language model F1-scores ranged from \% to \%. Thus, the research contribution can be summarized as follow:   \par  The rest of the paper proceeds with the Background Section , which provides information on the methodology used, related studies in the Literature Review section , a detailed description of methods in the Methodology section , details of the used datasets in the Dataset section , specifications of the methodologies and the results of each approach in the Experiments and Results section , observations and analysis of the performance of each approach in the Discussion section , and finally, the Conclusion and Future Work section .         This section provides background information on the state-of-the-art methodologies used for natural language processing  tasks, includes the current commonly used pre-trained embedding  and language models. For pre-trained word embeddings, different organizations and institutions  continuously seek to find the best methods for word representations . Here, we describe the most commonly used word embedding  model according to recent studies. Pre-trained language models have recently received massive attention in the  NLP field. They can be defined as a black box that understands natural language and can be applied and fine-tuned to solve NLP tasks. The pre-training process uses inexpensive unlabeled data to learn the initial parameters of a neural network model. Bidirectional Encoder Representations from Transformers  is one of these language models and is state of the art for many NLP problems.  \subsection {Pre-trained Word Embedding}    Word embedding is one of the most popular recent Natural Language Processing  trends. It refers to any technique aiming to map words to a dense vector representation that captures words semantic meanings that can be used to estimate similarities between words for classification. The primary purpose of this mapping is to represent linguistic terms in dense vectors to be utilized by machine learning algorithms. A word is mapped to an N-dimensional vector appropriate for representing the meaning of a specific language. Different Neural Network  models have been used to construct word vectors, as word vectors provide meaningful numerical descriptions of words based on their context.   \par  Word embedding has proven to be a powerful technique for extracting the most meaningful representations of words. The evolution of word embedding has resulted in tremendous success in various NLP tasks like text classification, document clustering, part of speech tagging, named entity recognition, sentiment analysis, and so on. Many researchers have built models to reach the best meaningful word vector representations by using word embedding, and the most common models are Google Word2Vec and Stanford GloVe.   \subsubsection {Word2Vec}    Word2Vec developed by Google research team to overcome traditional word representation  techniques by representing words in a more dense and meaningful representation given a corpus context. The word vector representation is computed from a large corpus fed into the model to produce vectors representing word meanings. The meaning of words is obtained from surrounding words within a specified window size. Word2Vec representation can be obtained from different model architectures, e.g., continuous skip-gram and continuous bag-of-words . Google released a pre-trained Word2Vec model representing word meanings that have been successfully utilized in many NLP tasks in recent years. The model is trained on a vast corpus of  billion words and is publicly available. A word vector is more meaningful if the model is trained on a progressively larger corpus size. \subsubsection {GloVe}    GloVe  is another word embedding model developed by. It is an unsupervised learning algorithm that obtains a vector representing a word閳ユ獨 semantic meaning by using corpus-based distributional features. The algorithm performs several operations on a constructed word-to-word co-occurrence statistics-based matrix. This is a costly process for a huge corpus, even though it only requires a single pass through the corpus. This matrix is used to construct word vectors instead of using a prediction-based approach like in Google Word2Vec. Thus, the main difference between Google閳ユ獨 Word2Vec and GloVe is that Word2Vec is a prediction-based model in which a loss function is used to evaluate the prediction performance, while GloVe is a count-based model. GloVe has been trained on many platforms such as Wikipedia, web crawl data, and Twitter, and provides a model for each one with different dimensions.      Bidirectional Encoder Representations from Transformers , is the latest revolution in NLP pre-trained language model trends. BERT is a deeply bidirectional language model trained on very large datasets  based on contextual representations. Other previous language models are unidirectional, which means they consider context only from left-to-right or right-to-left, whereas BERT adds a Neural Network Dense layer for classification to construct a fully pre-trained language model ready for fine-tuning. The fine-tuning advantage incorporates the contextual or the problem-specific meaning with the pre-trained generic meaning and trains it for a specific classification problem. BERT provides high performance for NLP tasks and improves on the results from traditional models.  \par  NLP tasks seek to find the best contextual word representations. Word2Vec and GloVe generate an embedding representation for each token, regardless of its contextual differences, and a word's meaning is changed according to its associated context. If the word has different meanings based on context, GloVe and Word2Vec represent the word as a single embedding, such as the word 'bank' in the phrases bank deposit and river bank. Here, the word bank would have a single representation in the whole corpus, which ignores other meanings of the word. Therefore, Word2Vec and GloVe are described as context-free models. Contextual representation has two types: unidirectional, in which the representation is learned in one direction, from left to right, or bidirectional, in which the representation of the word in learned from both directions, i.e., left to right and right to left. BERT is deeply bidirectional by jointly conditioning both left and right contexts in all layers. BERT models have different releases that differ according to model size, cased or uncased alphabet, languages, and the number of layers, and they are all available online.    Deep learning  is a subfield of machine learning which uses successive layers for accurate representations of meaning. The learning process is performed by exposing training data to the model to give representations. If the learning model consists of only one or two layers, then it is called shallow learning. Deep learning usually uses a neural network in order to learn these representations, and the neural networks are structured in layers. The learning process aims to find the best-weight values of the neural network that map an input example to its correct target, and a loss function is used, which measures the distance between the predicted and actual targets. Different constructions of layers give different deep learning models. Neural networks form the basis for deep learning, and one of the most common neural network architectures used for deep learning construction is LSTM. \subsubsection{Long Short-Term Memory }   LSTM is a recurrent neural network  developed as a solution for solving the problem of vanishing gradient in RNNs. An RNN is a specific type of neural network which considers the history or context in the computation of the output. RNN includes a memory to preserve the previous computational result and feeds back the previous set of hidden unit activation to the network with the current input. This particular architecture is beneficial for problems that require the history to be involved in the decision-making process, such as speech recognition and stock forecasting. RNNs suffer from the vanishing gradient problem, in which the weights are lost in a deeper layer of the network, thereby failing to capture very long dependencies. To avoid this problem, LSTM replaces each node by a memory cell, which consists of an input gate, forget gate, output gate, and a node connected back to itself. The memory cell in a specific layer uses the hidden state in the previous layer during the current time and the hidden state of the current layer from the previous time. The forget gate decides which information should be ignored in the cell state, and the input gate and tanh layer decide which information is stored in the cell state, then using the sigmoid function to decide the final output.       In this paper, we introduce the system WeChat submitted for the WMT 2020 shared task on ChineseEnglish news translation. Our system is based on the Transformer with different variants and the DTMT architecture. Data selection, several effective synthetic data generation approaches , advanced finetuning approaches  and self-bleu based model ensemble are employed and proven effective in our experiments. Our constrained ChineseEnglish system achieved 36.9 case-sensitive BLEU score which is the highest among all submissions.   
","    This section provides background information on the state-of-the-art methodologies used for natural language processing  tasks, includes the current commonly used pre-trained embedding  and language models. For pre-trained word embeddings, different organizations and institutions  continuously seek to find the best methods for word representations . Here, we describe the most commonly used word embedding  model according to recent studies. Pre-trained language models have recently received massive attention in the  NLP field. They can be defined as a black box that understands natural language and can be applied and fine-tuned to solve NLP tasks. The pre-training process uses inexpensive unlabeled data to learn the initial parameters of a neural network model. Bidirectional Encoder Representations from Transformers  is one of these language models and is state of the art for many NLP problems.  \subsection {Pre-trained Word Embedding}    Word embedding is one of the most popular recent Natural Language Processing  trends. It refers to any technique aiming to map words to a dense vector representation that captures words semantic meanings that can be used to estimate similarities between words for classification. The primary purpose of this mapping is to represent linguistic terms in dense vectors to be utilized by machine learning algorithms. A word is mapped to an N-dimensional vector appropriate for representing the meaning of a specific language. Different Neural Network  models have been used to construct word vectors, as word vectors provide meaningful numerical descriptions of words based on their context.   \par  Word embedding has proven to be a powerful technique for extracting the most meaningful representations of words. The evolution of word embedding has resulted in tremendous success in various NLP tasks like text classification, document clustering, part of speech tagging, named entity recognition, sentiment analysis, and so on. Many researchers have built models to reach the best meaningful word vector representations by using word embedding, and the most common models are Google Word2Vec and Stanford GloVe.   \subsubsection {Word2Vec}    Word2Vec developed by Google research team to overcome traditional word representation  techniques by representing words in a more dense and meaningful representation given a corpus context. The word vector representation is computed from a large corpus fed into the model to produce vectors representing word meanings. The meaning of words is obtained from surrounding words within a specified window size. Word2Vec representation can be obtained from different model architectures, e.g., continuous skip-gram and continuous bag-of-words . Google released a pre-trained Word2Vec model representing word meanings that have been successfully utilized in many NLP tasks in recent years. The model is trained on a vast corpus of  billion words and is publicly available. A word vector is more meaningful if the model is trained on a progressively larger corpus size. \subsubsection {GloVe}    GloVe  is another word embedding model developed by. It is an unsupervised learning algorithm that obtains a vector representing a word闁炽儲鐛 semantic meaning by using corpus-based distributional features. The algorithm performs several operations on a constructed word-to-word co-occurrence statistics-based matrix. This is a costly process for a huge corpus, even though it only requires a single pass through the corpus. This matrix is used to construct word vectors instead of using a prediction-based approach like in Google Word2Vec. Thus, the main difference between Google闁炽儲鐛 Word2Vec and GloVe is that Word2Vec is a prediction-based model in which a loss function is used to evaluate the prediction performance, while GloVe is a count-based model. GloVe has been trained on many platforms such as Wikipedia, web crawl data, and Twitter, and provides a model for each one with different dimensions.      Bidirectional Encoder Representations from Transformers , is the latest revolution in NLP pre-trained language model trends. BERT is a deeply bidirectional language model trained on very large datasets  based on contextual representations. Other previous language models are unidirectional, which means they consider context only from left-to-right or right-to-left, whereas BERT adds a Neural Network Dense layer for classification to construct a fully pre-trained language model ready for fine-tuning. The fine-tuning advantage incorporates the contextual or the problem-specific meaning with the pre-trained generic meaning and trains it for a specific classification problem. BERT provides high performance for NLP tasks and improves on the results from traditional models.  \par  NLP tasks seek to find the best contextual word representations. Word2Vec and GloVe generate an embedding representation for each token, regardless of its contextual differences, and a word's meaning is changed according to its associated context. If the word has different meanings based on context, GloVe and Word2Vec represent the word as a single embedding, such as the word 'bank' in the phrases bank deposit and river bank. Here, the word bank would have a single representation in the whole corpus, which ignores other meanings of the word. Therefore, Word2Vec and GloVe are described as context-free models. Contextual representation has two types: unidirectional, in which the representation is learned in one direction, from left to right, or bidirectional, in which the representation of the word in learned from both directions, i.e., left to right and right to left. BERT is deeply bidirectional by jointly conditioning both left and right contexts in all layers. BERT models have different releases that differ according to model size, cased or uncased alphabet, languages, and the number of layers, and they are all available online.    Deep learning  is a subfield of machine learning which uses successive layers for accurate representations of meaning. The learning process is performed by exposing training data to the model to give representations. If the learning model consists of only one or two layers, then it is called shallow learning. Deep learning usually uses a neural network in order to learn these representations, and the neural networks are structured in layers. The learning process aims to find the best-weight values of the neural network that map an input example to its correct target, and a loss function is used, which measures the distance between the predicted and actual targets. Different constructions of layers give different deep learning models. Neural networks form the basis for deep learning, and one of the most common neural network architectures used for deep learning construction is LSTM. \subsubsection{Long Short-Term Memory }   LSTM is a recurrent neural network  developed as a solution for solving the problem of vanishing gradient in RNNs. An RNN is a specific type of neural network which considers the history or context in the computation of the output. RNN includes a memory to preserve the previous computational result and feeds back the previous set of hidden unit activation to the network with the current input. This particular architecture is beneficial for problems that require the history to be involved in the decision-making process, such as speech recognition and stock forecasting. RNNs suffer from the vanishing gradient problem, in which the weights are lost in a deeper layer of the network, thereby failing to capture very long dependencies. To avoid this problem, LSTM replaces each node by a memory cell, which consists of an input gate, forget gate, output gate, and a node connected back to itself. The memory cell in a specific layer uses the hidden state in the previous layer during the current time and the hidden state of the current layer from the previous time. The forget gate decides which information should be ignored in the cell state, and the input gate and tanh layer decide which information is stored in the cell state, then using the sigmoid function to decide the final output.",28
"   Graph Neural Networks  have in recent years been shown to provide a scalable and highly performant means of incorporating linguistic information and other structural biases into NLP models. They have been applied to various kinds of representations  and shown effective on a range of tasks, including relation extraction~, question answering~, syntactic and semantic parsing tasks~, summarization ~, machine translation~ and abusive language detection in social networks~.     While GNNs often yield strong performance, % such models are % complex, and it can be difficult to understand the `reasoning' behind their predictions. For NLP practitioners, it is highly desirable to know which linguistic information a given model encodes and how that encoding happens~. The difficulty in interpreting GNNs represents a barrier to such analysis. %  Furthermore,  this opaqueness decreases user trust% , impedes the discovery of harmful biases, and complicates error analysis% ~,   an issue for GNNs where seemingly small implementation differences can make or break models~.  In this work, we focus on post-hoc analysis of GNNs and formulate some desiderata for an interpretation method:      A simple way to perform interpretation is to use  erasure search~, an approach wherein attribution happens by searching for a maximal subset of features which can be entirely removed without affecting model predictions. % The removal guarantees that all information about the discarded features is ignored by the model. This  contrasts with approaches which use heuristics to define feature importance, for example attention-based methods~ or back-propagation techniques~. They do not guarantee that the model ignores low-scoring features, attracting criticism in recent years . % The trust in erasure search is reflected in the literature through other methods % motivated as approximations of erasure~, or through new attribution techniques % evaluated using erasure search as ground truth~.  Applied to GNNs, erasure search would involve a search for the largest subgraph which can be completely discarded. Besides faithfulness considerations and conceptual simplicity, discrete attributions would also simplify the comparison of relevance between paths; this is in contrast to continuous attribution to edges, where it is not straightforward to extract and visualize important paths. Furthermore, in contrast to techniques based on artificial gradients~, erasure search would provide implementation invariance~. This is important in NLP, as models commonly use highly parametrized decoders on top of GNNs, e.g.~\citet{koncel-kedziorski-etal-2019-text}.   While arguably satisfying criteria  and  in our desiderata, erasure search unfortunately fails on tractability. In practical scenarios, it is infeasible, and even approximations, which remove one feature at a time~ and underestimate their contribution due to saturation~,  remain prohibitively expensive.   Our GraphMask aims at meeting the above desiderata by achieving the same benefits as erasure search in a scalable manner. That is, our method makes easily interpretable hard choices on whether to retain or discard edges such that discarded edges have no relevance to model predictions, while remaining tractable and model-agnostic~. GraphMask  can be understood as a differentiable form of subset erasure, where, instead of finding an optimal subset to erase for every given example, we learn an erasure function which predicts for every edge  at every layer  whether that connection should be retained. Given an example graph , our method returns for each layer  a subgraph  such that we can faithfully claim that no edges outside  influence the predictions of the model. To enable gradient-based optimization for our erasure function, we rely on sparse stochastic gates~.  In erasure search, optimization happens individually for each example. This can result in a form of overfitting where even non-superfluous edges are aggressively pruned, because a similar prediction could be made using an alternative smaller subgraph; we refer to this problem as hindsight bias. % Because our model relies on a parametrized erasure function rather than an individual per-edge choice, we can address this issue by amortizing parameter learning over a training dataset through a process similar to the readout bottleneck introduced in~\citet{schulz2020restricting}. As we demonstrate in Section, this strategy avoids hindsight bias.  \paragraph{Contributions} Our contributions are as follows:        Several recent papers have focused on developing interpretability techniques for GNNs. The closest to ours is GNNExplainer~, wherein a soft erasure function for edges is learned individually for each example. Unlike our method , GNNExplainer cannot as such guarantee that gated edges do not affect predictions. Furthermore, as we show in our experiments , separate optimization for each example results in overfitting through hindsight bias which compromises faithfulness. \citet{pope2019explainability, xie2019interpreting} explore gradient-based methods, including gradient heatmaps, Grad-CAM, and Excitation Backpropagation. Similarly, apply Layerwise Relevance Propagation~ to the GNN setting. These methods represent an alternative to , but as we have noted their faithfulness is questionable~, and the lack of implementation invariance~ is problematic . Furthermore, significant engineering is still required to develop these techniques for certain GNNs, e.g. networks with attention as the aggregation function~.  Another popular approach is to treat attention or gate scores as a measure of importance~. However, even leaving questionable faithfulness~ aside, many GNNs use neither gates nor attention, and, for those that do~, the gates  -- as we demonstrate in Section -- are not necessarily informative.       Outside of graph-specific methods, one line of research involves decomposing the output into a part attributed to a specific subset of features and a part attributed to the remaining features~. For GNNs, the computational cost for realistic use cases  is prohibitive. LIME~ like us relies on a trained erasure model, but interprets local models in place of global models. Local models cannot trivially identify useful paths or long-distance dependent pairs of edges and, as also pointed out in, LIME cannot be easily applied for large general graphs. Similarly, it is unclear how to apply integrated gradients~ to retrieve relevant paths, especially for deep GNNs operating in large graphs.   Masking messages in  can be equivalently thought of as adding a certain type of noise to these messages. Therefore,  can be categorized as belonging to the recently introduced class of perturbation-based methods~ which equate feature importance with sensitivity of the prediction to the perturbations of that feature. The closest to our model is, wherein the authors like us apply a secondary, trained model to predict relevancy, but with soft gates and to detect important features for CNNs on an image processing task.  \ificlrfinal In our very recent work we have introduced   a similar differentiable masking approach to post-hoc analysis for transformers networks. We used sparse stochastic gates and  regularization to determine which input tokens can be dropped conditioning on various hidden layers. \else Concurrently with our work, have introduced   a differentiable masking approach to post-hoc analysis for transformers networks. They use sparse stochastic gates and  regularization to determine which input tokens can be dropped conditioning on various hidden layers. \fi            The first approach of domain-specific experiments in  , the results show that domain-specific embedding with Bidirectional LSTM model outperforms the results of  who used randomly initialized word embedding with LSTM. Their accuracy was \ , while our accuracy is \ . Although our model exceeds their accuracy, but we expected much higher accuracy than only 2 points, which means that random initialization does not perform very badly. It is important to mention that white supremacist corpus for the pretrained word embedding was about 1 million tweets, increasing the corpus size would provide better performance, but we were limited by Twitter閳ユ獨 policies. This experiment shows that the Bidirectional LSTM based deep model gave good performance for the white supremacy detection, which contradicts, who said that LSTM did not give a good performance because the length of tweets was limited to 180 characters; however, now it is 280 characters. \par From the feature perspective comparison, Table shows how WSW2V performs in comparison with other domain-agnostic models using the same classifier and datasets; the WSW2V outperforms other models on both the Stormfront and Balanced datasets, but GloVe Twitter outperforms WSW2V, and this is because the big size difference of the data trained on, i.e,  for  GloVe Twitter and  for WSW2V. From the classifier perspective comparison, the Bidirectional LSTM-based deep model outperforms LR on two datasets , but LR outperforms the Bidirectional LSTM-based deep model on the Twitter dataset.  \par The second experiment involved using the BERT model on the dataset to assess its performance on the white supremacist hate speech classification task. As shown in Table, BERT outperforms all the distributional-based embeddings  with the Bidirectional LSTM-based deep model in Table. This means that the BERT model gives a closer meaningful vector of the words due to its training strategy  and the large corpus trained on. The BERT language model combines the advantages of domain-agnostic and domain-specific embeddings in its training strategy, it is petrained on a large corpus and add extra layer for training your specific task.  \par Finally, narcissists often use first-person singular pronouns and profane and aggressive language in their social media communications ,  while individuals with an argumentative personality often comment on other people閳ユ獨 posts or frequently post on similar topics to prove their point. White supremacists usually associate themselves with radical groups by either identifying themselves as a member in their profiles or by encouraging or promoting their ideological perspectives. This study focuses on tweets or textual features to detect white supremacy, and not account for profile features. Thus, we only focus on tweet features that help to identify white supremacists閳 characteristics. Further account analysis will be included in future work. 	  \section {Conclusion and Future work}    From the experiments, we have shown that a combination of word embedding, and deep learning perform well for the problem of white supremacist hate speech. Some of the datasets are imbalanced to simulate real-world data, and others are balanced to assess the model閳ユ獨 performance under an ideal situation. The BERT model has also proved that it provides the state of art for this problem. For future work, the corpus size will be maximized in order to generate more meaningful embeddings, and experiments will be done on multiclass problems instead of binary class problems and by combining Google Word2Vec and domain-specific Word2Vec.  \section {Acknowledgement} I would like to thank all the researchers who have made their resources available to the research community.    
","    Several recent papers have focused on developing interpretability techniques for GNNs. The closest to ours is GNNExplainer~, wherein a soft erasure function for edges is learned individually for each example. Unlike our method , GNNExplainer cannot as such guarantee that gated edges do not affect predictions. Furthermore, as we show in our experiments , separate optimization for each example results in overfitting through hindsight bias which compromises faithfulness. \citet{pope2019explainability, xie2019interpreting} explore gradient-based methods, including gradient heatmaps, Grad-CAM, and Excitation Backpropagation. Similarly, apply Layerwise Relevance Propagation~ to the GNN setting. These methods represent an alternative to , but as we have noted their faithfulness is questionable~, and the lack of implementation invariance~ is problematic . Furthermore, significant engineering is still required to develop these techniques for certain GNNs, e.g. networks with attention as the aggregation function~.  Another popular approach is to treat attention or gate scores as a measure of importance~. However, even leaving questionable faithfulness~ aside, many GNNs use neither gates nor attention, and, for those that do~, the gates  -- as we demonstrate in Section -- are not necessarily informative.       Outside of graph-specific methods, one line of research involves decomposing the output into a part attributed to a specific subset of features and a part attributed to the remaining features~. For GNNs, the computational cost for realistic use cases  is prohibitive. LIME~ like us relies on a trained erasure model, but interprets local models in place of global models. Local models cannot trivially identify useful paths or long-distance dependent pairs of edges and, as also pointed out in, LIME cannot be easily applied for large general graphs. Similarly, it is unclear how to apply integrated gradients~ to retrieve relevant paths, especially for deep GNNs operating in large graphs.   Masking messages in  can be equivalently thought of as adding a certain type of noise to these messages. Therefore,  can be categorized as belonging to the recently introduced class of perturbation-based methods~ which equate feature importance with sensitivity of the prediction to the perturbations of that feature. The closest to our model is, wherein the authors like us apply a secondary, trained model to predict relevancy, but with soft gates and to detect important features for CNNs on an image processing task.  \ificlrfinal In our very recent work we have introduced   a similar differentiable masking approach to post-hoc analysis for transformers networks. We used sparse stochastic gates and  regularization to determine which input tokens can be dropped conditioning on various hidden layers. \else Concurrently with our work, have introduced   a differentiable masking approach to post-hoc analysis for transformers networks. They use sparse stochastic gates and  regularization to determine which input tokens can be dropped conditioning on various hidden layers. \fi",29
" Aspect based sentiment analysis   is a fine-grained sentiment analysis task. ABSA contains several subtasks, four of which are aspect category detection  detecting aspect categories mentioned in sentences, aspect category sentiment analysis  predicting the sentiments of the detected aspect categories, aspect term extraction  identifying aspect terms presenting in sentences and aspect term sentiment analysis  classifying the sentiments toward the identified aspect terms. While aspect categories mentioned in a sentence are from a few predefined categories and may not occur in the sentence, aspect terms  explicitly appear in sentences. Fig.  shows an example. ACD detects the two aspect categories food and service and ACSA predicts the positive and negative sentiments toward them. ATE identifies the two aspect terms ``taste'' and ``service'' and ATSA classifies the positive and negative sentiments toward them. In this paper, we concentrate on the ACSA task. The ACD task as a auxiliary is used to find aspect category-related nodes from sentence constituency parse trees for the ACSA task.    Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate appropriate sentiment words for given aspect categories. Wang et al.  were the first to explore attention mechanism on the ACSA task and proposed an attention based LSTM . For a given sentence and an aspect category mentioned in the sentence, AT-LSTM first models the sentence via a LSTM model,  then combines the hidden states from the LSTM with the representation of the aspect category to generate aspect category-specific word representations, finally applies an attention mechanism over the word representations to find the aspect category-related sentiment words, that are used to predict the sentiment of the aspect category. The constrained attention networks   handles multiple aspect categories of a sentence simultaneously and introduces orthogonal and sparse regularizations to constrain the attention weight allocation. The aspect-level sentiment capsules model  performs ACD and ACSA simultaneously, which also uses an attention mechanism to find aspect category related sentiment words and achieves state-of-the-art performances on the ACSA task.  However, these models directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. For the example in Fig., ``Great'' and ``bad'' can be used interchangeably. It is hard for attention-based methods to distinguish which word is associated with aspect category food or service among ``good'' and ``bad''. To solve the problem, The HiErarchical ATtention network  first finds the aspect terms indicating the given aspect cagegory, then finds the aspect category-related sentiment words  depending on the position information and semantics of the aspect terms. Although HEAT obtains good results, to train HEAT, we additionally need to annotate the aspect terms indicating the given aspect category, which can be time-consuming and expensive.  To mitigate the mismatch problem, we propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis which does not require any additional annotation. SCAN contains two graph attention networks   and an interactive loss function. Given a sentence, we first use the Berkeley Neural Parser  to generate the constituency parse tree. The two GATs generate representations of the nodes in the sentence constituency parse tree for the ACD task and the ACSA task, respectively. The GAT for ACD mainly attends to the words indicating aspect categories, while the GAT for ACSA mainly attends to sentiment words. For a given aspect category, the interactive loss function helps the ACD task to find the nodes that can predict the aspect category but can閳ユ獩 predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. Fig.  shows the constituency parse tree of the sentence ``Greate taste bad service.''. For the aspect category food, SCAN first finds the yellow nodes ``Greate taste'' and ``taste'', then predict the sentiment of food based on the sentiment word ``Great'' in the node ``Great taste''. SCAN excludes the blue node ``Great taste bad service.'' for food, because it can predict not only food but also service.  The main contributions of our work can be summarized as follows:       Aspect-category sentiment analysis  aims to predict the sentiments of a sentence toward the given aspect categories. We summarize previous approaches for this task into two classes: non-attention based models and attention-based models. Given a sentence and an aspect category mentioned in the sentence, non-attention based models  directly generate the aspect category-specific sentence representation, then predict the sentiment of the aspect category based on the representation. Although some non-attention based models  achieve state-of-the-art results, they don't provide reasons why they make a prediction, so, they lack interpretability.  Compared with non-attention based models, attention based models  are more interpretable. They first find aspect category-related sentiment words, then generate aspect category-specific representations based on the sentiment words. Attention mechanism was first used by Wang et al.  to find aspect category-related sentiment words. Jiang et al.  proposed new capsule networks  to model the complicated relationship between aspect categories and context, where the normalization weights and routing weights can be viewed as attention weights. CAN  and AS-Capsules  perform the ACD task and the ACSA task jointly and achieves state-of-the-art performances. However, these attention based models directly use the given aspect category to find the aspect category-related sentiment words, which may cause the mismatch problem mentioned above.   Since SCAN is, to the best of our knowledge, the first syntax-aware model for ACSA, we review some syntax-aware methods for the Aspect-Term Sentiment Analysis  task. ATSA predicts the sentiments of the given aspect terms that occur in a sentence. Early methods for ATSC try to incorporate syntax knowledge using recursive neural networks. Dong et al.  first converted the dependency tree of a sentence to a binary tree. They proposed an Adaptive Recursive Neural Network  that is applied on the binary tree to propagate the sentiments of words to the target node. The representation of the target node is used to predict the sentiment label of the target. Similar to aspect term, the target is a sequence of words that occur in the sentence. Phrase Recursive Neural Network   makes the representation of the aspect term richer by using syntactic information from both the dependency and constituent trees of sentences. These methods that have to convert the original dependency tree into a binary tree may move modifying sentiment words farther away from the aspect term. Recently, A few researches use graph neural network  to incorporate syntax knowledge and achieve the state-of-the-art performance. Huang et al.  applied graph attention network   over the dependency tree of a sentence and the representations of the nodes corresponding to aspect terms is used to predict the sentiment of the aspect terms. Zhang et al.  applied graph convolutional network  over the dependency tree of a sentence, then the representation of the node corresponding to the given aspect term is used to retrive aspect-related sentiment words that are used to predict the sentiment of the aspect term. Since aspect categories may not occur in sentences, these methods can't be used for the ACSA task directly.    Complex Word Indentification is a challenging task, even when using state-of-the-art Transformer-based solutions. In this work, we introduce an approach that improves the previous results on the cross-lingual and monolingual CWI shared task 2018 by using multilingual and language-specific Transformer models, multilingual word embeddings , and different fine-tuning techniques. Fine-tuning a model on data from two different languages creates the opportunity of grasping features that empower it to better recognize complex words in certain contexts, even in a different language. In addition, zero-shot, one-shot, and few-shot learning strategies provide good results, surpassing strong baselines  and proposing an alternative to help non-native speakers to properly understand the difficult aspects of a certain language.  For future work, we intend to improve our results on the monolingual tasks by integrating additional models, such as XLNet  and techniques like adversarial training and multi-task learning. Furthermore, we intend to experiment with other pretraining techniques specific to Transformer models, such that the results for French can benefit from cross-lingual transfer learning.  
","  Aspect-category sentiment analysis  aims to predict the sentiments of a sentence toward the given aspect categories. We summarize previous approaches for this task into two classes: non-attention based models and attention-based models. Given a sentence and an aspect category mentioned in the sentence, non-attention based models  directly generate the aspect category-specific sentence representation, then predict the sentiment of the aspect category based on the representation. Although some non-attention based models  achieve state-of-the-art results, they don't provide reasons why they make a prediction, so, they lack interpretability.  Compared with non-attention based models, attention based models  are more interpretable. They first find aspect category-related sentiment words, then generate aspect category-specific representations based on the sentiment words. Attention mechanism was first used by Wang et al.  to find aspect category-related sentiment words. Jiang et al.  proposed new capsule networks  to model the complicated relationship between aspect categories and context, where the normalization weights and routing weights can be viewed as attention weights. CAN  and AS-Capsules  perform the ACD task and the ACSA task jointly and achieves state-of-the-art performances. However, these attention based models directly use the given aspect category to find the aspect category-related sentiment words, which may cause the mismatch problem mentioned above.   Since SCAN is, to the best of our knowledge, the first syntax-aware model for ACSA, we review some syntax-aware methods for the Aspect-Term Sentiment Analysis  task. ATSA predicts the sentiments of the given aspect terms that occur in a sentence. Early methods for ATSC try to incorporate syntax knowledge using recursive neural networks. Dong et al.  first converted the dependency tree of a sentence to a binary tree. They proposed an Adaptive Recursive Neural Network  that is applied on the binary tree to propagate the sentiments of words to the target node. The representation of the target node is used to predict the sentiment label of the target. Similar to aspect term, the target is a sequence of words that occur in the sentence. Phrase Recursive Neural Network   makes the representation of the aspect term richer by using syntactic information from both the dependency and constituent trees of sentences. These methods that have to convert the original dependency tree into a binary tree may move modifying sentiment words farther away from the aspect term. Recently, A few researches use graph neural network  to incorporate syntax knowledge and achieve the state-of-the-art performance. Huang et al.  applied graph attention network   over the dependency tree of a sentence and the representations of the nodes corresponding to aspect terms is used to predict the sentiment of the aspect terms. Zhang et al.  applied graph convolutional network  over the dependency tree of a sentence, then the representation of the node corresponding to the given aspect term is used to retrive aspect-related sentiment words that are used to predict the sentiment of the aspect term. Since aspect categories may not occur in sentences, these methods can't be used for the ACSA task directly.",30
"  With the rapid development of e-commerce, online reviews written by  users  have become increasingly important for reflecting real customer experiences. To ease the process of review writing, the task of personalized review generation~ has been proposed to automatically produce review text conditioned on necessary context data, \eg users, items, and ratings.  As a mainstream solution, RNN-based models  have been widely applied to the PRG task. Standard RNN models mainly model sequential dependency among tokens,  which cannot effectively generate high-quality review text. Many efforts have been devoted to improving this kind of architecture for the PRG task, including context utilization,  long text generation, and  writing style enrichment. These studies have improved the performance of the PRG task to some extent. However, two major issues still remain to be solved. First, the generated text is likely  to be uninformative, lacking factual description on product information. Although several studies try to incorporate structural or semantic features ,  they mainly extract such features from the review text.   Using review data alone, it is difficult to fully capture diverse and comprehensive facts from unstructured text. Second, most of these studies focus on word-level generation, which makes it difficult to directly model  user preference at a higher level. For example, given a product, a user may focus on the price, while another user may emphasize the look.  To address these issues, we propose to improve the PRG task with external knowledge graph . By associating online items with KG entities, we are able to obtain rich attribute or feature information for items, which is potentially useful for the PRG task. Although the idea is intuitive, it is not easy to fully utilize the knowledge information for generating review text in our task. KG typically organizes facts as triples, describing the relation between two involved entities. It may not be suitable to simply integrate KG information to enhance text representations or capture user preference due to varying intrinsic characteristics of different data signals.  In order to bridge the semantic gap, we augment the original KG with user and word nodes, and construct a heterogeneous knowledge graph  by adding user-item links and entity-word links. User-item links are formed according to user-item interactions, and entity-word links are formed according to their co-occurrence in review sentences. We seek to learn a unified semantic space that is able to encode different kinds of nodes. Figure presents an illustrative example for the HKG. Given such a graph, we focus on two kinds of useful information for the PRG task. First, the associated facts regarding to an item  can be incorporated to enrich the review content. Second, considering users as target nodes, we can utilize this graph to infer users' preference  on some specific relation or aspect . The two kinds of information reflect word- and aspect-level enrichment, respectively. To utilize the semantics at the two levels, we decompose  the review generation process into two stages, namely aspect sequence generation and sentence generation.  We aim to inject multi-granularity KG information in different generation stages for improving the PRG task.     To this end, in this paper, we propose a KG-enhanced personalized review generation model based on capsule graph neural networks~. Compared with most of existing GNN-based methods representing graphs as individual scalar features, Caps-GNN can extract underlying characteristics of graphs as capsules at the graph level through the dynamic routing mechanism and each capsule reflects the graph properties in different aspects. Based on the constructed HKG, we utilize Caps-GNN to extract graph properties in different aspects as graph capsules, which may be helpful to infer aspect- and word-level user preference. For aspect sequence generation, we propose a novel adaptive learning algorithm that is able to capture personalized user preference at the aspect level, called aspect capsules, from the graph capsules.  We associate an aspect capsule with a unique aspect from unsupervised topic models.   Furthermore, for the generation of sentences, we utilize the learned aspect capsules to capture personalized user preference at the word level. Specially, we design a graph-based copy mechanism to generate related entities or words by copying them from the HKG, which can enrich the review contents.  In this way, KG information has been effectively utilized  at both aspect and word levels in our model.   %To our knowledge, we are the first to utilize knowledge graph to generate personalized review text, which is able to capture both aspect- and word-level KG semantics for learning user preference.  To our knowledge, we are the first to utilize KG to capture both aspect- and word-level user preference for generating personalized review text. For evaluation, we constructed three review datasets by associating items with KG entities. Extensive experiments  demonstrate the effectiveness of KG information and our model. %%Our code and dataset will be released after the review period.         Recently, many researchers have made great efforts on the natural language generation  task. Automatic review generation is a specific task of NLG, which focuses on helping online users to generate product reviews.  Typical methods adopted RNNs to model the generation process and utilize available context information, such as user, item and rating.  In order to avoid repetition issue caused by the RNN models and generate long and diverse texts, Generative Adversarial Nets  based approaches have been applied to text generation. However, the generation process is unaware of  the underlying semantic structure of text.  To make the generated text more informative, several studies utilized side information with a more instructive  generation process. These works utilize context features, \eg aspect words and history corpus, to enrich the generated content.   Among them, Li et al.  propose a model to automatically generate controllable and personalized user reviews based on the arbitrarily given sentimental and stylistic information.}  While, their side information was mainly mined from the review itself, which cannot fully cover diverse and rich semantic information. We are also aware of the works that utilize structural  knowledge data to enrich the diversity of generated texts.  {Among them, Dong et al.  present an attention-enhanced attribute-to-sequence model to generate product reviews. They also propose an attention mechanism to jointly generate reviews and align words with input attributes.}  However, these studies do not utilize knowledge information to learn the writing preference of users.  Furthermore, closely related to the recommendation task, several studies attempted to model the interactions between user and product with review as explanation. They mainly capture the adoption preference over items, while, we focus on the writing preference for review generation. They still rely on the review text itself for learning useful explanation for users' adoption behaviors.   The focus of this work is to explore external KG data for extracting effective information for the PRG task.   The two lines of work can complement each other. We leave the integration and comparison as future work.   Our work is inspired by the work of capsule graph neural network, especially its application on aspect extraction. These works mainly focus on capsule networks for aspect-level sentiment classification.   For example, Du et al.  propose to use capsule network to construct vector-based feature representation and model the relationship between aspect terms and context by the capsule routing procedure. While, our work focuses on inferring aspect information using KG data for review generation.        In this paper, We propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis. The two graph attention modules and the interactive loss function in SCAN form a complete solution to alleviate the mismatch problem. The experimental results on five public datasets demonstrate the effectiveness of SCAN. Future work could consider making the representations of the leaf nodes richer by using syntactic information from the dependency tree of the sentence and modelling the inter-aspect category dependencies.     ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.   
"," Recently, many researchers have made great efforts on the natural language generation  task. Automatic review generation is a specific task of NLG, which focuses on helping online users to generate product reviews.  Typical methods adopted RNNs to model the generation process and utilize available context information, such as user, item and rating.  In order to avoid repetition issue caused by the RNN models and generate long and diverse texts, Generative Adversarial Nets  based approaches have been applied to text generation. However, the generation process is unaware of  the underlying semantic structure of text.  To make the generated text more informative, several studies utilized side information with a more instructive  generation process. These works utilize context features, \eg aspect words and history corpus, to enrich the generated content.   Among them, Li et al.  propose a model to automatically generate controllable and personalized user reviews based on the arbitrarily given sentimental and stylistic information.}  While, their side information was mainly mined from the review itself, which cannot fully cover diverse and rich semantic information. We are also aware of the works that utilize structural  knowledge data to enrich the diversity of generated texts.  {Among them, Dong et al.  present an attention-enhanced attribute-to-sequence model to generate product reviews. They also propose an attention mechanism to jointly generate reviews and align words with input attributes.}  However, these studies do not utilize knowledge information to learn the writing preference of users.  Furthermore, closely related to the recommendation task, several studies attempted to model the interactions between user and product with review as explanation. They mainly capture the adoption preference over items, while, we focus on the writing preference for review generation. They still rely on the review text itself for learning useful explanation for users' adoption behaviors.   The focus of this work is to explore external KG data for extracting effective information for the PRG task.   The two lines of work can complement each other. We leave the integration and comparison as future work.   Our work is inspired by the work of capsule graph neural network, especially its application on aspect extraction. These works mainly focus on capsule networks for aspect-level sentiment classification.   For example, Du et al.  propose to use capsule network to construct vector-based feature representation and model the relationship between aspect terms and context by the capsule routing procedure. While, our work focuses on inferring aspect information using KG data for review generation.",31
" As mentioned in Chapter , models trained simply to obtain a high accuracy on held-out sets can often learn to rely on shallow input statistics, resulting in brittle models. % susceptible to adversarial attacks. For example, \citet{lime} present a document classifier that distinguishes between Christianity and Atheism with a test accuracy of . However, on close inspection, the model spuriously separates classes based on words contained in the headers, such as ``Posting'', ``Host'', and ``Re''.  Spurious correlations in both training and test sets allow for such undesired models to obtain high accuracies. Much more complex hidden correlations may be present in any arbitrarily large and human-annotated dataset . Such correlations may be difficult to spot, and even when one identifies them, it is an open question how to mitigate them .   In this chapter, I investigate a direction that has the potential to both steer neural models away from relying on spurious correlations and provide explanations for the predictions of these models. This direction is that of enhancing neural models with the capability to learn from natural language explanations during training time and to generate such explanations at test time. For humans, it has been shown that explanations play a key role in structuring conceptual representations for categorisation and generalisation . Humans also benefit tremendously from reading explanations before acting in an environment for the first time . Thus, explanations may also be used to set a model in a better initial position to further learn the correct functionality. Meanwhile, at test time, generating correct argumentation in addition to obtaining a high accuracy has the potential to endow a model with a higher level of transparency and trust.     %In this work, we introduce a new dataset and models for exploiting and generating explanations for the task of recognizing textual entailment.  Incorporating external knowledge into a neural model was shown to result in more robust models . % show that models achieving high accuracies on SNLI, such as , show dramatically reduced performance on this simpler dataset, while the model of \citet{kim} is more robust due to incorporating external knowledge.  Free-form natural language explanations are a form of external knowledge that has the following advantages over formal language. First, it is easy for humans to provide free-form language, eliminating the additional effort of learning to produce formal language, thus making it simpler to collect such datasets. Secondly, natural language explanations might potentially be mined from existing large-scale free-form text. Finally, natural language is readily comprehensible to an end-user who needs to assert the reliability of a model.  %Thirdly, the formal languages chosen by researchers may differ from work to work and therefore models constructed over one formal language might not be trivially transferred to another. Meanwhile free-form explanations are generic and applicable to diverse areas of research, such as natural language processing, computer vision, or policy learning.   Despite the potential for natural language explanations to improve both learning and transparency, there is a scarcity of such datasets in the community, as discussed in Section .  To address this deficiency, I collected a large corpus of K human-annotated explanations for the SNLI dataset~. I chose SNLI because it constitutes an influential corpus for natural language understanding that requires deep assimilation of fine-grained nuances of commonsense knowledge. %A plethora of models have been developed on this dataset, including previous state-of-the-art in universal sentence representations , which demonstrates the power of this task and dataset. I call this explanation-augmented dataset e-SNLI, which I release publicly\footnote{The dataset can be found at \url{https://github.com/OanaMariaCamburu/e-SNLI}.} to advance research in the direction of training with and generation of free-form natural language explanations.    %To demonstrate the efficacy of the e-SNLI dataset,  %I show that it is much more difficult for neural models to produce correct natural language explanations based on spurious correlations than it is to produce correct labels. Further, I develop models that predict a label and generate an explanation for their prediction. I also investigate how the presence of natural language explanations at training time can guide neural models into learning better universal sentence representations  and into having better capabilities to solve out-of-domain instances.  Secondly, I show that it is much more difficult for a neural model to produce correct natural language explanations based on spurious correlations than it is for it to produce correct labels based on such correlations.   Thirdly, I develop models that predict a label and generate an explanation for their prediction, and I investigate the correctness of the generated explanations.   Finally, I investigate whether training a neural model with natural language explanations can result in better universal sentence representations produced by this model and in better performance on out-of-domain datasets.   \paragraph{Remark.} In this chapter, I use the concept of correct explanation to refer to the correct argumentation for the ground-truth label on an instance.  This should not be confused with the concept of faithful explanation, which refers to the accuracy with which an explanation describes the decision-making process of a model, as described in Section .  The capability of a neural model to generate correct explanations is an important aspect of the development of such models.  For example, correct argumentation may sometimes be needed in practice, alongside the correct final answer. Hence, in this chapter, I inspect the correctness of the explanations generated by the introduced neural models. In the next chapter, I will take a step towards verifying the faithfulness of these explanations.% is given in Chapter .      \paragraph{Explanatory Methods.}    Explaining predictions made by complex machine learning systems has been of increasing concern. These explanations can be divided into two categories: feature importance explanations and full-sentence natural language explanations. The methods that provide feature importance explanations aim to provide the user with the subset of input tokens that contributed the most to the prediction of the model. As pointed out by \citet{esnli}, these explanations are not comprehensive, as one would need to infer the missing links between the words in order to form a complete argument. For example, in the natural language inference task, if the explanation is formed by the words ``dog'' and ``animal'', one would not know if the model learned that ``A dog is an animal'' or ``An animal is a dog'' or maybe even that ``Dog and animal implies entailment''. It is also arguably more user-friendly to get a full sentence explanation rather than a set of tokens. Therefore, an increasing amount of works focus on providing full sentence explanations . However, generating fluent argumentation, while more appealing, it is also arguably a harder and more risky task. For example, similar in spirit to our work, \citet{grounding} identified the risk of mentioning attributes from a strong class prior without any evidence being present in the input. In our work, we bring awareness to the risk of generating inconsistent explanations.     An increasing amount of work focuses on providing natural language, free-form explanations  as a    more comprehensive and user-friendly alternative to other forms of explainability, such as feature-based explanations~.    However, generating free-form explanations is a challenging task and,  \paragraph{Verifying models that generate natural language explanations.} Works on verifying models that generate natural language explanations are very scarce. \citet{grounding} identify the risk of generating natural language explanations that mention attributes from a strong class prior without any evidence being present in the input.  In this chapter, I bring awareness to another risk, which is of generating inconsistent explanations.    Similarly, \citet{grounding} identify the risk of mentioning attributes from a strong class prior without any evidence being present in the input.     In NLP, the problem of explaining the behaviour of a model is either via black-box analysis, or via modifications of the model .    Black-box methods generate explanations by analysing the model behaviour in different regions of the input space, or via token sensitivity analysis.    White-box analysis methods augment models to generate explanations jointly with predictions, the magnitude of inner activations, gradients, and attention weights.    However, at the time of this writing, no method in the literature focuses on the problem of assessing the quality of produced explanations, and on identifying inputs yielding to inconsistent natural language explanations.     \paragraph{Generating adversarial examples.} Generating adversarial examples is an active research area in natural language processing~. For instance, \citet{DBLP:conf/emnlp/JiaL17} analyse the robustness of extractive question answering models on examples obtained by adding adversarially generated distracting text.    Other works analyse sensitivity to small random character perturbations, paraphrasing, and simple transformations requiring lexical and world knowledge.   However, most works build on the requirement that the adversarial input should be a small perturbation of an original input~, or should be preserving the semantics of the original input~.  , but leading to a different prediction.  While this is necessary for testing the robustness of a model, Our setup does not have this requirement, and any pair of task-realistic inputs that causes the model to produce inconsistent explanations suffices.    Also, existing adversarial models do not always require the adversarial input to be grammatically correct, and often they can change words or characters to completely random ones~.    This is acceptable for certain cases, such as summarisation of long pieces of text, where changing a few words would likely not change the main flow of the text.        In cases like ours, where the inputs are short sentences, and the model is tested for fine-grained reasoning, it is desirable that the adversarial examples are gra mmatically correct.   Most importantly, to my knowledge, no previous adversarial attack for sequence-to-sequence models generates exact target sequences, i.e., given a sequence, find an input that causes the model to generate the exact given sequence. Closest to this goal, \citet{DBLP:conf/iclr/ZhaoDS18} propose an adversarial framework for removing or adding tokens in the target sequence for the task of machine translation.  Similarly, \citet{keywords} require the presence of pre-defined tokens anywhere in the target sequence. They only test with up to three required tokens, and their success rate dramatically drops from  for one token to  for three tokens for the task of automatic summarisation. Hence, their method would likely not generalise to exact target sequences.    Finally, \citet{DBLP:conf/conll/Minervini018} attempted to find inputs where a model trained on SNLI  violates a set of logical constraints. This scenario may, in theory, lead to finding inputs that cause the generation of inconsistent explanations.  However, their method needs to enumerate and evaluate a potentially very large set of perturbations of the inputs, \eg{} removing sub-trees or replacing tokens with their synonyms, thus being computational expensive.  Besides the computational overhead, it also may easily generate ungrammatical inputs.  Moreover, their scenario does not address the question of automatically producing undesired  sequences.     In this paper, we propose a structured meta-learning algorithm for open domain dialogue generation on infrequent sentence functions. To tackle the low-resource issue, our proposed model, based on the recently proposed model-agnostic meta-learning, can find both transferable internal representations and sensible parameters which can produce large improvement under a few adaptation steps. Moreover, we further explore the structure across fine-grained sentence functions and such that the model can balance knowledge generalization and knowledge customization. Extensive experiments show that our structured meta-learning  algorithm outperforms existing approaches under the low-resource setting.   
","  \paragraph{Explanatory Methods.}    Explaining predictions made by complex machine learning systems has been of increasing concern. These explanations can be divided into two categories: feature importance explanations and full-sentence natural language explanations. The methods that provide feature importance explanations aim to provide the user with the subset of input tokens that contributed the most to the prediction of the model. As pointed out by \citet{esnli}, these explanations are not comprehensive, as one would need to infer the missing links between the words in order to form a complete argument. For example, in the natural language inference task, if the explanation is formed by the words ``dog'' and ``animal'', one would not know if the model learned that ``A dog is an animal'' or ``An animal is a dog'' or maybe even that ``Dog and animal implies entailment''. It is also arguably more user-friendly to get a full sentence explanation rather than a set of tokens. Therefore, an increasing amount of works focus on providing full sentence explanations . However, generating fluent argumentation, while more appealing, it is also arguably a harder and more risky task. For example, similar in spirit to our work, \citet{grounding} identified the risk of mentioning attributes from a strong class prior without any evidence being present in the input. In our work, we bring awareness to the risk of generating inconsistent explanations.     An increasing amount of work focuses on providing natural language, free-form explanations  as a    more comprehensive and user-friendly alternative to other forms of explainability, such as feature-based explanations~.    However, generating free-form explanations is a challenging task and,  \paragraph{Verifying models that generate natural language explanations.} Works on verifying models that generate natural language explanations are very scarce. \citet{grounding} identify the risk of generating natural language explanations that mention attributes from a strong class prior without any evidence being present in the input.  In this chapter, I bring awareness to another risk, which is of generating inconsistent explanations.    Similarly, \citet{grounding} identify the risk of mentioning attributes from a strong class prior without any evidence being present in the input.     In NLP, the problem of explaining the behaviour of a model is either via black-box analysis, or via modifications of the model .    Black-box methods generate explanations by analysing the model behaviour in different regions of the input space, or via token sensitivity analysis.    White-box analysis methods augment models to generate explanations jointly with predictions, the magnitude of inner activations, gradients, and attention weights.    However, at the time of this writing, no method in the literature focuses on the problem of assessing the quality of produced explanations, and on identifying inputs yielding to inconsistent natural language explanations.     \paragraph{Generating adversarial examples.} Generating adversarial examples is an active research area in natural language processing~. For instance, \citet{DBLP:conf/emnlp/JiaL17} analyse the robustness of extractive question answering models on examples obtained by adding adversarially generated distracting text.    Other works analyse sensitivity to small random character perturbations, paraphrasing, and simple transformations requiring lexical and world knowledge.   However, most works build on the requirement that the adversarial input should be a small perturbation of an original input~, or should be preserving the semantics of the original input~.  , but leading to a different prediction.  While this is necessary for testing the robustness of a model, Our setup does not have this requirement, and any pair of task-realistic inputs that causes the model to produce inconsistent explanations suffices.    Also, existing adversarial models do not always require the adversarial input to be grammatically correct, and often they can change words or characters to completely random ones~.    This is acceptable for certain cases, such as summarisation of long pieces of text, where changing a few words would likely not change the main flow of the text.        In cases like ours, where the inputs are short sentences, and the model is tested for fine-grained reasoning, it is desirable that the adversarial examples are gra mmatically correct.   Most importantly, to my knowledge, no previous adversarial attack for sequence-to-sequence models generates exact target sequences, i.e., given a sequence, find an input that causes the model to generate the exact given sequence. Closest to this goal, \citet{DBLP:conf/iclr/ZhaoDS18} propose an adversarial framework for removing or adding tokens in the target sequence for the task of machine translation.  Similarly, \citet{keywords} require the presence of pre-defined tokens anywhere in the target sequence. They only test with up to three required tokens, and their success rate dramatically drops from  for one token to  for three tokens for the task of automatic summarisation. Hence, their method would likely not generalise to exact target sequences.    Finally, \citet{DBLP:conf/conll/Minervini018} attempted to find inputs where a model trained on SNLI  violates a set of logical constraints. This scenario may, in theory, lead to finding inputs that cause the generation of inconsistent explanations.  However, their method needs to enumerate and evaluate a potentially very large set of perturbations of the inputs, \eg{} removing sub-trees or replacing tokens with their synonyms, thus being computational expensive.  Besides the computational overhead, it also may easily generate ungrammatical inputs.  Moreover, their scenario does not address the question of automatically producing undesired  sequences.",32
"  We use a sequence of vectors to represent a sentence, where each vector consists of  a semantic-role  tag, a part-of-speech  tag, and other syntactic and semantic tags,  and we refer to such a sequence as a \textsl{meta sequence}.  We present an application using meta-sequence learning to generate, on a given article,  adequate QAPs to form multiple-choice questions.  In particular, we develop a scheme called MetaQA to learn meta sequences  of declarative sentences and the corresponding interrogative sentences from a training dataset. % consisting of such sentences. Combining and removing redundant meta sequences yields a set called MSDIP  , with each element being a pair of an MD and corresponding MI, where MD and MI stand for, respectively, a meta sequence for a declarative sentence and for an interrogative sentence. A trained MetaQA model generates QAPs for a given declarative sentence  as follows: Generate a meta sequence for , find a best-matched MD from MSDIP, generates meta sequences for interrogative sentences according to the corresponding MIs and the meta sequence of , identifies the meta-sequence answer to each MI, and coverts them back to text to form a QAP.    \begin{comment}     Automatic question generation , first studied by Wolfe  as a means to aid independent study, has since attracted increasing attentions in two lines of methodologies: transformative and generative.     \paragraph{Transformative methods.} Transformative methods transform key phrases from a given single declarative sentence into  factual questions. Existing methods are rule-based on syntax, semantics, or templates.  Syntactic-based methods follow the same basic strategy: Parse sentences using a syntactic parser to identify key phrases and transform a sentence to a question based on syntactic rules.  These include methods to identify key phrases from input sentences and use syntactic rules for  different types of questions  ,  generate questions and answers using a syntactic parser, a POS tagger, and an NE analyzer , transform a sentence into a set of questions using a series of domain-independent rules , and generate questions using relative pronouns and adverbs from complex English sentences .   Semantic-based methods create questions using predicate-argument structures and semantic roles , semantic pattern recognition  ,  subtopics based on Latent Dirichlet Allocation , or  generate factual questions using  semantic-role labeling  as the main form of text analysis  .  These methods are similar. The only difference is that semantic-based methods use semantic parsing while syntactic-based methods ues syntactic parsing to determine which specific words or phrases should be asked. In a language with many syntactic and semantic exceptions, such as English, these methods would require substantial manual labor to construct rules.  Template-based methods are for special-purpose applications with built-in templates. Research in this line devises a Natural Language Generation Markup Language  ; uses a phrase structure parser to parse text and construct questions using enhanced XML  ; devise a self-questioning strategy to help children generate questions from narrative fiction ; use informational text to enhance  the self-questioning strategy ; apply pattern matching, variables, and templates to transform source sentences into questions similar to NLGML ; defines a question template as pre-defined text with placeholder variables to be replaced with content from the source text ; or incorporates semantic-based methods into a template-based method to support online learning .     \paragraph{Generative methods.}  Generative methods are neural networks trained  on large datasets, which tend to just learn how to generate questions  without concerning what the correct answers are.  or just learn how to provide an answer to a question.  Recent advancements of neural-network methodologies  have shed new light on generative methods. For example, the attention mechanism   is  used to determine what content in a sentence should be asked, and the sequence-to-sequence   and the long short-term memory    mechanisms are used to generate each word in a question . These models, however, only deal with question generations without generating correct answers.   to a randomly generated question.  Moreover, training these models require a dataset comprising over 100K questions.  To address the problem of generating questions without answers, researchers have explored ways to encode a passage  and an answer word  as input, and determine what questions are to be generated for a given answer . Kim et al.  pointed out that these models could generate a number of answer-revealing questions . They then devised a new method by encoding answers separately, at the expsense of having substantially more parameters. Their experiments show that the BLEU-4 ,  METEOR , and ROUGE-L  scores on the questions generated are, respectively, 16.2, 19.92, 43.96, which are 3 to 4 points higher than the earlier results on the same dataset  of 12.98, 16.22, 39.75  .  Such low accuracies are still a long way to go to meet the requirement for our application, and  On top of low accuracy, it is also unknown whether the questions generated are grammatically correct  because these measures do not measure grammatical correctness.      Our work put forwards an opinion triplet extraction perspective for aspect-based sentiment analysis. Existing works that are applicable to opinion triplet extraction have been shown insufficient, owing to the use of unified aspect-sentiment tagging scheme and ignorance of the interaction between elements in the triplet. Thus, we propose a multi-task learning framework to address the limitations by highlighting the uses of joint training, decoupled aspect and sentiment prediction, and regularization among correlated tasks during learning. Experimental results verify the effectiveness of our framework in comparison with a wide range of strong baselines. Comparison results with different variants of the proposed framework signify the necessity of the core components in the framework.  Based on the observations from a case study and error analysis, we plan to carry out further research in the following aspects:  more robust taggers for aspect and opinion extraction,  more flexible evaluation metric for triplet extraction, and  more mighty triplet interaction mechanism .     File emnlp2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{booktabs} \usepackage{color} \usepackage{tikz-dependency} \usepackage{amsfonts} \usepackage{amsmath} \usepackage{multirow} \usepackage{makecell} \usepackage{pifont} \newcommand{\cmark}{\ding{51}} \newcommand{\xmark}{\ding{55}} \usepackage[noend]{algpseudocode} \usepackage{algorithmicx,algorithm}    This is not strictly necessary, and may be commented out,   but it will improve the layout of the manuscript,   and will typically save some space. \usepackage{microtype}  \aclfinalcopy   Uncomment this line for the final submission \def\aclpaperid{704}    Enter the acl Paper ID here   \setlength\titlebox{5cm}   You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.  \newcommand\BibTeX{B\TeX}  \title{A Multi-task Learning Framework for Opinion Triplet Extraction}  \author{Chen Zhang\textsuperscript{1}, Qiuchi Li\textsuperscript{2}, Dawei Song\textsuperscript{1}\Thanks{ Dawei Song is the corresponding author.}, Benyou Wang\textsuperscript{2} \\   \textsuperscript{1} Beijing Institute of Technology, Beijing, China. \\   \textsuperscript{2} University of Padova, Padova, Italy. \\    \texttt{\{czhang,dwsong\}@bit.edu.cn}, \texttt{\{qiuchili,wang\}@dei.unipd.it} \\}  \date{}           
","   Automatic question generation , first studied by Wolfe  as a means to aid independent study, has since attracted increasing attentions in two lines of methodologies: transformative and generative.     \paragraph{Transformative methods.} Transformative methods transform key phrases from a given single declarative sentence into  factual questions. Existing methods are rule-based on syntax, semantics, or templates.  Syntactic-based methods follow the same basic strategy: Parse sentences using a syntactic parser to identify key phrases and transform a sentence to a question based on syntactic rules.  These include methods to identify key phrases from input sentences and use syntactic rules for  different types of questions  ,  generate questions and answers using a syntactic parser, a POS tagger, and an NE analyzer , transform a sentence into a set of questions using a series of domain-independent rules , and generate questions using relative pronouns and adverbs from complex English sentences .   Semantic-based methods create questions using predicate-argument structures and semantic roles , semantic pattern recognition  ,  subtopics based on Latent Dirichlet Allocation , or  generate factual questions using  semantic-role labeling  as the main form of text analysis  .  These methods are similar. The only difference is that semantic-based methods use semantic parsing while syntactic-based methods ues syntactic parsing to determine which specific words or phrases should be asked. In a language with many syntactic and semantic exceptions, such as English, these methods would require substantial manual labor to construct rules.  Template-based methods are for special-purpose applications with built-in templates. Research in this line devises a Natural Language Generation Markup Language  ; uses a phrase structure parser to parse text and construct questions using enhanced XML  ; devise a self-questioning strategy to help children generate questions from narrative fiction ; use informational text to enhance  the self-questioning strategy ; apply pattern matching, variables, and templates to transform source sentences into questions similar to NLGML ; defines a question template as pre-defined text with placeholder variables to be replaced with content from the source text ; or incorporates semantic-based methods into a template-based method to support online learning .     \paragraph{Generative methods.}  Generative methods are neural networks trained  on large datasets, which tend to just learn how to generate questions  without concerning what the correct answers are.  or just learn how to provide an answer to a question.  Recent advancements of neural-network methodologies  have shed new light on generative methods. For example, the attention mechanism   is  used to determine what content in a sentence should be asked, and the sequence-to-sequence   and the long short-term memory    mechanisms are used to generate each word in a question . These models, however, only deal with question generations without generating correct answers.   to a randomly generated question.  Moreover, training these models require a dataset comprising over 100K questions.  To address the problem of generating questions without answers, researchers have explored ways to encode a passage  and an answer word  as input, and determine what questions are to be generated for a given answer . Kim et al.  pointed out that these models could generate a number of answer-revealing questions . They then devised a new method by encoding answers separately, at the expsense of having substantially more parameters. Their experiments show that the BLEU-4 ,  METEOR , and ROUGE-L  scores on the questions generated are, respectively, 16.2, 19.92, 43.96, which are 3 to 4 points higher than the earlier results on the same dataset  of 12.98, 16.22, 39.75  .  Such low accuracies are still a long way to go to meet the requirement for our application, and  On top of low accuracy, it is also unknown whether the questions generated are grammatically correct  because these measures do not measure grammatical correctness.",33
"  Generating text that conforms to syntactic or semantic constraints benefits many NLP applications. To name a few, when paired data are limited, \citet{yang-etal-2019-low} build templates from large-scale unpaired data to aid the training of the dialog generation model; \citet{Niu2017ASO} and \citet{liu-etal-2019-rhetorically} apply style constraints to adjust the formality or rhetoric of the utterances; \citet{iyyer2018adversarial} and \citet{li-etal-2019-Insufficient} augment dataset using controlled generation to improve the model performance.  We study the problem of syntactically controlled text generation, which aims to generate target text with pre-defined syntactic guidance. Most recent studies on this topic  use sentences as exemplars to specify syntactic guidance. However, the guidance specified by a sentence can be vague, because its syntactic and semantic factors are tangled. Different from them, we use constituency parse trees as explicit syntactic constraints. As providing full-fledged parse trees of the target text is impractical, we require only a template parse tree that sketches a few top levels of a full tree . Figure shows our pipeline.    \citet{iyyer2018adversarial} adopt the same setting as ours. Their proposed SCPN model uses two LSTM  encoders to respectively encode source text and parse tree, and connects them to one decoder with additional attention  and pointer  structures. Nonetheless, recurrent encoders not only suffer from information loss by compressing a whole sequence into one vector but also are incapable of properly modeling the tree structure of constituency parse as well. Consequently, their network tends to ``translate'' the parse tree, instead of learning the real syntactic structures from it. % \zc{this sentence is still unclear.}  We propose a Transformer-based syntax-guided text generation method, named \ours. It first expands a template constituency parse tree to a full-fledged parse tree tailored for the input source text, and then uses the full tree to guide text generation. To capture the tree structure of the syntax, we apply a path attention mechanism  to our text generation model. It forces one node to attend to only other nodes located in its path  instead of all the nodes in the tree. Such a mechanism limits the information flow among the nodes in the constituency tree that do not have the direct ancestor-descendant relationship, forcing the parent nodes to carry more information than their children. In cooperation with path attention, we linearize the constituency trees to a more compact node-level format . Moreover, to address the challenge of properly integrating the semantic and syntactic information, we design a multi-encoder attention mechanism . It enables the Transformer decoder to accept outputs from multiple encoders simultaneously.  We evaluated our model on the controlled paraphrasing task. The experiment results show that \ours outperforms the state-of-the-art SCPN method by  in syntactic quality and  in semantic quality. % \zc{ use absolute improvements instead of relative ones} Human evaluations prove our method generates  semantically and syntactically superior sentences, with  semantic and  syntactic score improvements. % \zc{also give concrete numbers here, how much improvements?} Further, we find that the multi-encoder attention mechanism enhances the Transformer's ability to deal with multiple inputs, and the path attention mechanism significantly contributes to the model's semantic performance .   Our contributions include: 1) a multi-encoder attention mechanism that allows a Transformer decoder to attend to multiple encoders; 2) a path attention mechanism designed to better incorporate tree-structured syntax guidance with a special tree linearization format; and 3) a syntax-guided text generation method \ours that achieves new state-of-the-art semantic and syntactic performance.    Constrained text generation has attracted much attention in recent years. Categorized by the object to be controlled, there are two tracks of works: one seeks to manipulate the semantic attributes . For example, \citet{hu2017toward} generate text with specified sentiments, whereas \citet{li2018delete} and \citet{Wang-etal-2019-controllable} try to transfer the sentiments or styles of the source sentences. The other track, to which our research belongs, focuses on making generated text follow a particular style or structure . For instance, \citet{Niu2017ASO} constrain the output styles in neural machine translation task and \citet{liu2018controlling} impose length limitation to the summarization.   Based on the constraint source, syntactically controlled text generation models can be further divided into three groups. The first group  takes sentences as syntactic exemplars. They attempt to disentangle the semantic and syntactic representations into different VAE  latent spaces during training, and then use the exemplar to assign a prior distribution to the syntactic latent space at the inference stage. The second group  directly employs the constituency tree as an auxiliary input, controlling the syntax of generated text with the structure specified by it. Instead of importing externally, the third group  learns the syntax guidance from the training data and apply it in the generation phrase in return.   Considering that the fully specified exemplar sentences are hard to be effectively retrieved , we follow \citet{iyyer2018adversarial} and use constituency trees as the syntax guidance. We further take advantage of the parallel attribute of Transformer  to accommodate the tree structure in the encoding process. There are works  that adapt the recurrent encoder to the trees, but the transition matrix that RNNs depend on is less effective than our attention mechanism, especially when the tree is large.   We propose a new approach of multi-chain multi-hop rule learning for knowledge graph completion tasks. First, we formalize the concept of multi-hop rule sets with multiple relation chains from knowledge graphs. Second, we propose a game-theoretical learning approach to efficiently select predictive relation chains for a query relation. Our formulation and learning method demonstrate advantages on two benchmark datasets over existing single-chain based approaches. For future work, we plan to investigate rules beyond chains, as well as integrate KG embeddings into our framework.   
","  Constrained text generation has attracted much attention in recent years. Categorized by the object to be controlled, there are two tracks of works: one seeks to manipulate the semantic attributes . For example, \citet{hu2017toward} generate text with specified sentiments, whereas \citet{li2018delete} and \citet{Wang-etal-2019-controllable} try to transfer the sentiments or styles of the source sentences. The other track, to which our research belongs, focuses on making generated text follow a particular style or structure . For instance, \citet{Niu2017ASO} constrain the output styles in neural machine translation task and \citet{liu2018controlling} impose length limitation to the summarization.   Based on the constraint source, syntactically controlled text generation models can be further divided into three groups. The first group  takes sentences as syntactic exemplars. They attempt to disentangle the semantic and syntactic representations into different VAE  latent spaces during training, and then use the exemplar to assign a prior distribution to the syntactic latent space at the inference stage. The second group  directly employs the constituency tree as an auxiliary input, controlling the syntax of generated text with the structure specified by it. Instead of importing externally, the third group  learns the syntax guidance from the training data and apply it in the generation phrase in return.   Considering that the fully specified exemplar sentences are hard to be effectively retrieved , we follow \citet{iyyer2018adversarial} and use constituency trees as the syntax guidance. We further take advantage of the parallel attribute of Transformer  to accommodate the tree structure in the encoding process. There are works  that adapt the recurrent encoder to the trees, but the transition matrix that RNNs depend on is less effective than our attention mechanism, especially when the tree is large.",34
" Recently, there has been great success in automatic text summarization and generation. To better compare and improve the performance of models, evaluation for such systems has been a problem of interest. The selection of evaluation metrics will greatly affect the assessed quality of a generated summary and thus affect the evaluation of summarization models.   The most ideal metric is definitely human judgement, which is often treated as the gold standard. But human evaluation is time-consuming and labor-intensive, an automatic evaluation metric that cannot only save human resources but also simulate the ability of human judgement is of crucial importance.   Most of the existing automatic evaluation methods assess a summary by comparing it with reference texts written by humans. Some of them are model-free and simply use hand-crafted matching functions to calculate the similarity between the candidate summary and the reference  . These methods consider both the reference and the candidate as a sequence of tokens or n-gram blocks. For instance, as the de facto standard evaluation metric, ROUGE  calculates the n-gram overlap between the machine-generated summaries and reference summaries. Although these methods have the advantage of interpretability and efficiency, they are found to correlate poorly with human evaluation.   To reduce the requirement of exact word matching, some recent work tried to match the reference and the candidate summary in the embedding space of words or sentences . For instance, BERTScore  uses contextual word embeddings generated by BERT and performs a greedy matching to obtain the maximum cosine similarity between two texts. %\citeauthor{clarketal2019sentence}  designed a metric that combines sentence-level embeddings with the word mover閳ユ獨 distance   to calculate the distance of moving the candidate sequence into the reference and transforms the distance into a similarity score, while MoverScore  combines n-gram embeddings with WMD.   These methods are proved to correlate better with human judgement than ROUGE on many datasets, which demonstrates the effectiveness of using contextual embeddings.   }  , all the three dimensions focus on evaluating the linguistic quality of summaries.}  \end{table*}  However, the aforementioned methods all have some intrinsic drawbacks: these methods always need at least one human-generated reference to assess a candidate summary. References written by humans are costly to obtain. In addition, most of them only consider the semantic similarities with references, i.e. semantic qualities of the summaries, which ignores the linguistic qualities and other important aspects. In this paper, we propose a new unsupervised contrastive learning framework for automatically evaluating the summary qualities without comparing with reference summaries or training with human ratings. Specifically, we design an evaluator to consider both linguistic and semantic aspects of a summary. Then for each of the aspect we create a set of negative samples by perturbing the training samples. We compare the scores of original training samples and the negative samples to obtain the contrastive loss function and learn the evaluator. The experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method has much higher correlation with human judgement.  We summarize our contributions as follows:           \subsubsection{Reference-based Metrics} Most of the existing automatic metrics for summarization evaluation assess a model-generated summary  by comparing it with a human-authored summary .    Some metrics are model-free and their scoring basis are often easy to interpret. For instance, as the most widely used metric for summarization evaluation, ROUGE measures the co-occurrence of n-grams or substrings between the reference and the candidate.   Most of the model-based methods compare the embeddings of the reference and the candidate. BERTSCore uses pretrained BERT contextual embeddings and performs a greedy matching to obtain the maximum cosine similarity between embeddings of tokens in the two texts. \citeauthor{clarketal2019sentence}  proposed metrics based on sentence mover's similarity  by leveraging sentence-level embeddings for evaluating multi-sentence texts. MoverScore combines  n-gram contextual embeddings and Earth Mover's Distance. BERTScore can be viewed as a special case of MoverScore. NUBIA  considers three aspects of features of the reference-candidate pairs and aggregates the extracted features using a neural network regressor.   These metrics have a common drawback that the evaluation is based on costly human-authored references.  To assess the quality of a generated text summary, we need to obtain a corresponding ground-truth reference.    \subsubsection{Reference-free Metrics} Some work discussed how to evaluate the quality of generated text in the reference-free setting. \citeauthor{Louis2013Automatically} , \citeauthor{peyrard2017learning}  and \citeauthor{peyrardgurevych2018objective}  leveraged regression models to fit human judgement. RUSE  use sentence embeddings generated by three different models and aggregate them using a MLP regressor. \citeauthor{Xenouleas2019SumQE}  proposed a method that also uses a regression model to predict the scores, while the predictions are based on hidden representations generated using BERT as the encoder. However, these methods require ratings assigned by human annotators as training data which are also costly to obtain. In contrast, our method is unsupervised and requires no human ratings for training.  \citeauthor{sun2019feasibility}  discussed both reference-based and reference-free settings for summarization evaluation. Their method basically converts both the generated text and the text for comparison  into hidden representations using encoders like ELMo and calculates the cosine similarity between them, T in the reference-based setting and the reference-free setting stands for the human-authored reference text and the source document text, respectively. However, the experiment results show that their method's correlation with human ratings is lower than ROUGE, especially in the reference-free setting.  designed a  Question-Answering based method to compare the content difference of two texts. Although this method provides a novel perspective and the evaluation basis is easy to interpret, the results show that it has not achieved better performance than ROUGE considering the lower correlation with human ratings.   SUPERT generates pseudo references and evaluates the quality of the test summaries by calculating word mover's distance between the pseudo reference summaries and the test summaries. It is similar to MoverScore which uses the human-authored references instead of pseudo references. However, SUPERT mainly focuses on multi-document summarization evaluation, and its performance is inevitably worse than MoverScore.   The work closest to our model is an evaluation method for natural language generation  systems proposed by \citeauthor{zhou2020learning} . They implemented the sample-level evaluation by comparing a pair of texts. However, their method requires a set of different NLG systems and they need to generate weak supervision sample pairs from different checkpoints of a system. For testing, they also need to compare different samples to obtain a comparison score. In contrast, our model focuses on summarization evaluation; we do not need generated texts from many systems and different checkpoints of a system: all our negative samples are created by modifying the existing summaries; and in the test phase no comparison between different summaries is needed.       We investigated a few summarization datasets. As shown in Table, different datasets consider different evaluation dimensions. We observed that these dimensions can be roughly divided into three classes: the semantic quality , the linguistic quality , and other dimensions that can be hardly classified . In this paper, we design our method to cover both dimensions of semantic quality and linguistic quality.         We have proposed a novel syntactically guided text generation method \ours.~ \newcommand{\x}{\bm{x}}   \newcommand{\y}{\mathbf{y}} \newcommand{\y}{\bm{y}} \newcommand{\s}{\bm{s}} \newcommand{\bt}{\bm{t}} \newcommand{\z}{\bm{z}} \newcommand{\Z}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\prob}{\mathbb{P}} \newcommand{\D}{\mathcal{D}} \newcommand{\E}{\mathbb{E}} \newcommand{\N}{\mathcal{N}} \newcommand{\h}{\mathbf{h}} \newcommand{\Qp}{Q_\phi} \newcommand{\Pt}{P_\theta} \newcommand{\bmu}{\bm{\mu}} \newcommand{\ba}{\bm{\alpha}} \newcommand{\V}{\mathcal{V}}  \newcommand{\tsrc}{\bm{s}_{\rm src}} \newcommand{\ttgt}{\bm{s}_{\rm tgt}} \newcommand{\spred}{\hat{\bm{x}}_{\rm tgt}} \newcommand{\stgt}{\bm{x}_{\rm tgt}} \newcommand{\stmpl}{\bm{x}_{\rm tmpl}} \newcommand{\ssrc}{\bm{x}_{\rm src}} \newcommand{\hsrc}{\bm{h}_{\rm src}} \newcommand{\htmpl}{\bm{h}_{\rm tmpl}} \newcommand{\hscr}[1]{\h^{}} \newcommand{\uscr}[2]{#1^{}}  \newcommand{\ours}{{GuiG}\xspace} \newcommand{\synexpan}{{\ours.SE}\xspace} \newcommand{\guigen}{{\ours.TG}\xspace}  \newcommand{\zc}[1]{{[#1]}}   \aclfinalcopy   Uncomment this line for the final submission   \def\aclpaperid{***}    Enter the acl Paper ID here    \setlength\titlebox{5cm}   You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.  \newcommand\BibTeX{B\TeX}  \title{Transformer-Based Neural Text Generation with Syntactic Guidance}  \author{Yinghao Li \\   Georgia Institute of Technology \\   \texttt{yinghaoli@gatech.edu} \\\And   Rui Feng \\   Georgia Institute of Technology \\   \texttt{rfeng@gatech.edu} \\\AND   Isaac Rehg \\   Georgia Institute of Technology \\   \texttt{isaacrehg@gatech.edu} \\\And   Chao Zhang \\   Georgia Institute of Technology \\   \texttt{chaozhang@gatech.edu} \\}  \date{}  \begin{document} \maketitle                 
","      \subsubsection{Reference-based Metrics} Most of the existing automatic metrics for summarization evaluation assess a model-generated summary  by comparing it with a human-authored summary .    Some metrics are model-free and their scoring basis are often easy to interpret. For instance, as the most widely used metric for summarization evaluation, ROUGE measures the co-occurrence of n-grams or substrings between the reference and the candidate.   Most of the model-based methods compare the embeddings of the reference and the candidate. BERTSCore uses pretrained BERT contextual embeddings and performs a greedy matching to obtain the maximum cosine similarity between embeddings of tokens in the two texts. \citeauthor{clarketal2019sentence}  proposed metrics based on sentence mover's similarity  by leveraging sentence-level embeddings for evaluating multi-sentence texts. MoverScore combines  n-gram contextual embeddings and Earth Mover's Distance. BERTScore can be viewed as a special case of MoverScore. NUBIA  considers three aspects of features of the reference-candidate pairs and aggregates the extracted features using a neural network regressor.   These metrics have a common drawback that the evaluation is based on costly human-authored references.  To assess the quality of a generated text summary, we need to obtain a corresponding ground-truth reference.    \subsubsection{Reference-free Metrics} Some work discussed how to evaluate the quality of generated text in the reference-free setting. \citeauthor{Louis2013Automatically} , \citeauthor{peyrard2017learning}  and \citeauthor{peyrardgurevych2018objective}  leveraged regression models to fit human judgement. RUSE  use sentence embeddings generated by three different models and aggregate them using a MLP regressor. \citeauthor{Xenouleas2019SumQE}  proposed a method that also uses a regression model to predict the scores, while the predictions are based on hidden representations generated using BERT as the encoder. However, these methods require ratings assigned by human annotators as training data which are also costly to obtain. In contrast, our method is unsupervised and requires no human ratings for training.  \citeauthor{sun2019feasibility}  discussed both reference-based and reference-free settings for summarization evaluation. Their method basically converts both the generated text and the text for comparison  into hidden representations using encoders like ELMo and calculates the cosine similarity between them, T in the reference-based setting and the reference-free setting stands for the human-authored reference text and the source document text, respectively. However, the experiment results show that their method's correlation with human ratings is lower than ROUGE, especially in the reference-free setting.  designed a  Question-Answering based method to compare the content difference of two texts. Although this method provides a novel perspective and the evaluation basis is easy to interpret, the results show that it has not achieved better performance than ROUGE considering the lower correlation with human ratings.   SUPERT generates pseudo references and evaluates the quality of the test summaries by calculating word mover's distance between the pseudo reference summaries and the test summaries. It is similar to MoverScore which uses the human-authored references instead of pseudo references. However, SUPERT mainly focuses on multi-document summarization evaluation, and its performance is inevitably worse than MoverScore.   The work closest to our model is an evaluation method for natural language generation  systems proposed by \citeauthor{zhou2020learning} . They implemented the sample-level evaluation by comparing a pair of texts. However, their method requires a set of different NLG systems and they need to generate weak supervision sample pairs from different checkpoints of a system. For testing, they also need to compare different samples to obtain a comparison score. In contrast, our model focuses on summarization evaluation; we do not need generated texts from many systems and different checkpoints of a system: all our negative samples are created by modifying the existing summaries; and in the test phase no comparison between different summaries is needed.       We investigated a few summarization datasets. As shown in Table, different datasets consider different evaluation dimensions. We observed that these dimensions can be roughly divided into three classes: the semantic quality , the linguistic quality , and other dimensions that can be hardly classified . In this paper, we design our method to cover both dimensions of semantic quality and linguistic quality.",35
"  Conversational Machine Reading  is challenging because the rule text may not contain the literal answer, but provide a procedure to derive it through interactions . In this case, the machine needs to read the rule text, interpret the user scenario, clarify the unknown user's background by asking questions, and derive the final answer. Taking Figure  as an example, to answer the user whether he is suitable for the loan program, the machine needs to interpret the rule text to know what are the requirements, understand he meets ``American small business'' from the user scenario, ask follow-up clarification questions about ``for-profit business'' and ``not get financing from other resources'', and finally it concludes the answer ``Yes'' to the user's initial question.    Existing approaches  decompose this problem into two sub-tasks.  Given the rule text, user question, user scenario, and dialog history , the first sub-task is to make a decision among ``Yes'', ``No'', ``Inquire'' and ``Irrelevant''. The ``Yes/No'' directly answers the user question and ``Irrelevant'' means the user question is unanswerable by the rule text. If the user-provided information  are not enough to determine his fulfillment or eligibility, an ``Inquire'' decision is made and the second sub-task is activated. The second sub-task is to capture the underspecified condition from the rule text and generate a follow-up question to clarify it. \citet{zhong-zettlemoyer-2019-e3} adopt BERT  to reason out the decision, and propose an entailment-driven extracting and editing framework to extract a span from the rule text and edit it into the follow-up question.  The current \sota model EMT  uses a Recurrent Entity Network  with explicit memory to track the fulfillment of rules at each dialog turn for decision making and question generation.   In this problem, document interpretation requires identification of conditions and determination of logical structures because rules can appear in the format of bullet points, in-line conditions, conjunctions, disjunctions, etc. Hence, correctly interpreting rules is the first step towards decision making. Another challenge is dialog understanding. The model needs to evaluate the user's fulfillment over the conditions, and jointly consider the fulfillment states and the logical structure of rules for decision making. For example, disjunctions and conjunctions of conditions have completely different requirements over the user's fulfillment states. However, existing methods have not considered condition-level understanding and reasoning.   In this work, we propose \modelnameshortnsp: \modelnamecap. To better understand the logical structure of a rule text and to extract conditions from it, we first segment the rule text into clause-like elementary discourse units  using a pre-trained discourse segmentation model. Each EDU is treated as a condition of the rule text, and our model estimates its entailment confidence scores over three states: Entailment, Contradiction or Neutral by reading the user scenario description and existing dialog. Then we map the scores to an entailment vector for each condition, and reason out the decision based on the entailment vectors and the logical structure of rules. Compared to previous methods that do little entailment reasoning  or use it as multi-task learning , \modelnameshort is the first method to explicitly build the dependency between entailment states and decisions at each dialog turn.   \modelnameshort achieves new \sota results on the blind, held out test set of ShARC. In particular, \modelnameshort outperforms the previous best model EMT  by 3.8\% in micro-averaged decision accuracy and 3.5\% in macro-averaged decision accuracy. Specifically, \modelnameshort performs well on simple in-line conditions and conjunctions of rules while still needing improvements on understanding disjunctions. Finally, we conduct comprehensive analyses to unveil the limitation of \modelnameshort and current challenges for the ShARC benchmark. We find one of the biggest bottlenecks is the user scenario interpretation, in which various types of reasoning are required. % Code and models will be released to facilitate research along this line.     \paragraph{Entailment Reasoning in Reading Comprehension.}  Understanding entailments  of text is essential in dialog and question answering systems. ROPES  requires reading descriptions of causes and effects and applying them to situated questions, while ShARC , the focus of \modelnameshortnsp, requires to understand rules and apply them to questions asked by users in a conversational manner. Most existing methods simply use BERT to classify the answer without considering the structures of rule texts .  \citet{gao-etal-2020-explicit} propose Explicit Memory Tracker , which firstly addresses entailment-oriented reasoning. At each dialog turn, EMT recurrently tracks whether conditions listed in the rule text have already been satisfied to make a decision.   In this paper, we also explicitly model entailment reasoning for decision making, but there are three key differences between our \modelnameshort and EMT:  we apply discourse segmentation to parse the rule text, which is extremely helpful because there are many in-line conditions in rules;  Our stacked inter-sentence transformer layers extract better features for entailment prediction, which could be seen as a generalization of their recurrent explicit memory tracker.  Different from their utilization of entailment prediction which is treated as multi-task learning for decision making, we directly build the dependency between entailment prediction states and the predicted decisions.  \paragraph{Discourse Applications.}  Discourse analysis uncovers text-level linguistic structures , which can be useful for many downstream applications, such as coherent text generation  and text summarization . Recently, discourse information has also been introduced in neural reading comprehension. \citet{mihaylov-frank-2019-discourse} design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser  to segment surface-level in-line conditions for entailment reasoning.    We have evaluated the impact POS tag accuracy has on parsing performance for leading graph- and transition-based parsers across a diverse range of UD treebanks, highlighting the stark difference between using predicted POS tags  and gold POS tags at runtime. We observed a non-linear increase in performance when using gold tags, suggesting they are somehow exceptional\carlos{, i.e., precisely the tag patterns that not even the most accurate taggers can correctly predict  seem to be the most important for parsing}. \carlostwo{This could be due to the parsers implicitly learning POS tag information, in such a way that the taggers learn nothing new to contribute or not enough to avoid a loss in performance due to the errors disrupting what the parsers have learnt.}  at runtime and using gold POS tags with a non-linear increase in performance when using gold tags suggesting gold tagged annotation are somehow exceptional.   This was further corroborated by our experiment using treebanks for which we could obtain very high scoring taggers.  Our analysis also shows that practitioners should evaluate the efficacy of using predicted tags for a given system or language.   rather than assuming they won't have a negative impact.   Beyond the global conclusions drawn from our analysis,  We have also analysed what aspects of erroneous tagging predictions have the greatest impact and correlation to parsing performance. We observed some global trends, \carlos{like the importance of \texttt{CCONJ},} but also language-specific issues which highlight the need to evaluate the usefulness of POS tags per language. The results also suggest that using a subset of POS tags might be effective.  and potentially even per treebank.    
"," \paragraph{Entailment Reasoning in Reading Comprehension.}  Understanding entailments  of text is essential in dialog and question answering systems. ROPES  requires reading descriptions of causes and effects and applying them to situated questions, while ShARC , the focus of \modelnameshortnsp, requires to understand rules and apply them to questions asked by users in a conversational manner. Most existing methods simply use BERT to classify the answer without considering the structures of rule texts .  \citet{gao-etal-2020-explicit} propose Explicit Memory Tracker , which firstly addresses entailment-oriented reasoning. At each dialog turn, EMT recurrently tracks whether conditions listed in the rule text have already been satisfied to make a decision.   In this paper, we also explicitly model entailment reasoning for decision making, but there are three key differences between our \modelnameshort and EMT:  we apply discourse segmentation to parse the rule text, which is extremely helpful because there are many in-line conditions in rules;  Our stacked inter-sentence transformer layers extract better features for entailment prediction, which could be seen as a generalization of their recurrent explicit memory tracker.  Different from their utilization of entailment prediction which is treated as multi-task learning for decision making, we directly build the dependency between entailment prediction states and the predicted decisions.  \paragraph{Discourse Applications.}  Discourse analysis uncovers text-level linguistic structures , which can be useful for many downstream applications, such as coherent text generation  and text summarization . Recently, discourse information has also been introduced in neural reading comprehension. \citet{mihaylov-frank-2019-discourse} design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser  to segment surface-level in-line conditions for entailment reasoning.",36
"   .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Neural Language Models  have become a central component in NLP systems over the last few years, showing outstanding performance and improving the state-of-the-art on many tasks . However, the introduction of such systems has come at the cost of interpretability %and explainability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. % and, specifically, of understanding how linguistic predictors - that were common as features in earlier systems - are encoded in such models.  Recent work has begun to study these models in order to understand whether they encode %are able to learn  linguistic phenomena even without being explicitly designed %forse meglio trained?  to learn such properties . Much of this work focused on the analysis and interpretation of attention mechanisms  and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations.   Probing models trained  on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena  and even to organize this information in a hierarchical manner . However, the way in which this knowledge affects the decisions they make when solving specific downstream tasks has been less studied.  In this paper, we extended prior work by studying the linguistic properties encoded by one of the most prominent NLM, BERT , and how these properties affect its predictions when solving a specific downstream task. %,  using a suite of more than 80 probing tasks.  % qui vedere se tenere 'several' perch鑼 abbiamo 10 task di classificazione o dire che 鐚 uno solo diviso in 10 ""sotto-task"". We defined three research questions aimed at understanding:  what kind of linguistic properties are already encoded in a pre-trained version of BERT and where across its 12 layers;  how the knowledge of these properties is modified after a fine-tuning process;  whether this implicit knowledge %of these properties  affects the ability of the model to solve a specific downstream task, i.e. Native Language Identification . %With this aim, we firstly perform a very large suite of probing tasks using %on %DOMI: SPOSTIAMO QUESTA PARTE %To answer the first two questions, we firstly perform a very large suite of probing tasks using %on %the sentence representations extracted from the internal layers of BERT. Each of these tasks makes explicit a particular property of the sentence, from very shallow features  to more complex aspects of morpho--syntactic and syntactic structure , thus making them as particularly suitable to assess the implicit linguistic knowledge encoded in a NLM at a deep level of granularity. %with respect to a wide spectrum of phenomena overing lexical, morpho-syntactic and syntactic structure.  To tackle the first two questions, we adopted an approach inspired to the `linguistic profiling' methodology put forth by , which assumes that wide counts of linguistic features automatically extracted from parsed corpora allow modeling a specific language variety and detecting how it changes with respect to other varieties, e.g. complex vs simple language, female vs male--authored texts, texts written in the same L2 language by authors with different L1 languages.  Particularly relevant for our study, is that multi-level linguistic features have been shown to have a highly predictive role in tracking the evolution of learners' linguistic competence across time and developmental levels, both in first and second language acquisition scenarios .  %when leveraged by traditional learning models on a variety of text classification problems, all of which can be successfully tackled using formal, rather than content based aspects of a text: from the assessment of sentence complexity and text readability , to the identification of personal and sociodemographics traits of an author, such as his/her native language, gender, age etc.  and to the prediction of the evolution of learners' linguistic competence across time . %From this perspective, our approach can be considered as a particular implementation of the `linguistic profiling' methodology put forth by , which assumes that wide counts of linguistic features automatically extracted from parsed corpora allow modeling a specific language variety and detecting in what way it changes with respect to other varieties, e.g. complex vs simple language, female vs male--authored texts, texts written in the same L2 language by authors with different L1 languages. Given the strong informative power of these features to encode a variety of language phenomena across stages of acquisition, we assume that they can be also helpful to dig into the issues of interpretability of NLMs. In particular, we would like to investigate whether features successfully exploited to model the evolution of language competence can be similarly helpful in profiling how the implicit linguistic knowledge of a NLM changes across layers and before and after tuning on a specific downstream task. We chose the NLI task, i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language .  %Secondly, we investigate the type and degree of variations of linguistic information before and after fine-tuning the pre-trained model on 10 distinct  datasets used to solve Native Language Identification , i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language .   As shown by , linguistic features play a very important role when NLI is tackled as a sentence--classification task rather than as a traditional document--classification task.  %NLI can be addressed by exploiting only linguistic features extracted at sentence--level reaching comparable performance to those obtained by state--of--the--art models based on word embeddings .  This is the reason why we considered the sentence-level NLI classification as a task particularly suitable for probing the NLM linguistic knowledge. %perch鑼 鐚 un task che per essere risolto 鐚 necessario che il modello codifichi un'ampia gamma di informazioni linguistiche e anche perch鑼 鐚 un task basato sull'info estratta dalla sentence -come dimostrato da Cimino et al  nonostante lo stato dell'arte 鐚 stato definito soltanto usando word embeddings  %vecchia versione: a fine-tuning process based on a Native Language Identification  downstream task.  %vecchia versione: -base and 10 fine-tuned models obtained training BERT on as many Native Language Identification  tasks.  Finally, we investigated whether and which linguistic information encoded by BERT is involved in discriminating the sentences correctly or incorrectly classified by the fine-tuned models. To this end, we tried to understand if the linguistic knowledge that the model has of a sentence affects the ability to solve a specific downstream task involving that sentence.   %vecchia versione: Adopting a suite of more than 80 probing tasks, we firstly perform % We perform our experiments using a suite of more than 80 probing tasks, each of which corresponds to a specific/distinct sentence-level feature. We find that / We show that  %The remainder of the paper is organized as follows. We start by presenting some related works which are more closely related to our study  and in Section  we highlight the main novelties of our approach. We then describe in more details the data , the probing tasks  and the models  we used. Experiments and results are described in Section ,  and . To conclude, in Section  we summarize the main findings of the study.  \paragraph{Contributions} In this paper:  we carried out an in-depth linguistic profiling of BERT's internal representations %deep analysis of the implicit linguistic knowledge stored in BERT's internal representations and how it changes across layers using a wide suite of sentence-level probing tasks, corresponding to a wide spectrum of linguistic phenomena at different level of complexity; % we verify the implicit linguistic knowledge stored in BERT's internal representations using a suite of more than 80 probing tasks corresponding to a wide range of linguistic phenomena at different level of complexity;   we showed that contextualized representations tend to lose their precision in encoding a wide range of linguistic properties %general-purpose linguistic properties  after a fine-tuning process; % RIVEDERE 'GENERAL-PURPOSE' COME TERMINE PER DESCRIVERE LE NOSTRE FEATURES  we showed that the linguistic knowledge stored in the contextualized representations of BERT positively affects its ability to solve NLI downstream tasks: the more BERT stores information about these features% in its embeddings/internal representations , the higher will be its capacity of predicting the correct label.         citare ""Correlating neural and symbolic representations of language"" In the last few years, several methods have been devised to obtain meaningful explanations regarding the linguistic information encoded in NLMs . They range from techniques to examine the activations of individual neurons  to more domain specific approaches, such as interpreting attention mechanisms , studying correlations between representations  or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features . These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner  and even to support the extraction of dependency parse trees .  investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks,  found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline: POS tagging, parsing, NER, semantic roles and then coreference. , instead, quantified differences in the transferability of individual layers between different models, showing that higher layers of RNNs  are more task-specific , while transformer layers  do not exhibit this increase in task-specificity.   Alternative approaches have been proposed to study the linguistic information stored in NLMs and how this information evolve under different learning objectives or during a pre--training process.  proposed a method based on canonical correlation analysis  and mutual information to analyze how several learning objectives, such as machine translation, standard left-to-right language modelling  and masked language modeling  determine the evolution of the individual tokens' representations between layers in the transformer model. Using the same training data, model architecture and parameter initialization they found that the evolution of a token representation across layers changes according to the three different learning objectives.  used SVCCA to investigate how representations of linguistic structure are learned over time in a two-layer LSTM-based LM, discovering that different aspects of linguistic structure are learned at different rates within a single recurrent layer, acquiring POS tags early but continuing to learn topic information later in training.  Compared with the aforementioned studies, our work provides a more in depth analysis of the linguistic knowledge encoded by BERT before and after a fine-tuning process through a wide range of probing features, each of which corresponds to a specific sentence-level phenomena, and how this knowledge affects the ability of the model to solve a specific downstream task.   To our knowledge, this work is the first  Despite previous work have shown that pre--trained NLMs implicitly encode specific linguistic properties, there has been few studies that have attempted to understand how these encodings evolve after a fine-tuning process and how this implicit knowledge affects the decision they make when solving specific task. In our work, we    ricontrollare se non ci sono studi su evoluzione features linguistiche pre-training vs. fine-tuning   RICORDARE DI AGGIORNARE ARTICOLI DI EMNLP CON LA REF A EMNLP E NON AD ARXIV    inserire articolo Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs    In this paper, we present \modelnameshortnsp, a system that does discourse-aware entailment reasoning for conversational machine reading. \modelnameshort explicitly builds the connection between entailment states of conditions and the final decisions. Results on the ShARC benchmark shows that \modelnameshort outperforms existing methods by a large margin.  We also conduct comprehensive analyses to unveil the limitations of \modelnameshort and challenges for ShARC. In future, we plan to explore how to incorporate discourse parsing into the current decision making model for end-to-end learning. One possibility would be to frame them as multi-task learning with a common  encoder.  Another direction is leveraging current methods in question generation  to improve the follow-up question generation sub-task since \modelnameshort\ is on par with the previous best model EMT.  
","     citare ""Correlating neural and symbolic representations of language"" In the last few years, several methods have been devised to obtain meaningful explanations regarding the linguistic information encoded in NLMs . They range from techniques to examine the activations of individual neurons  to more domain specific approaches, such as interpreting attention mechanisms , studying correlations between representations  or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features . These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner  and even to support the extraction of dependency parse trees .  investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks,  found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline: POS tagging, parsing, NER, semantic roles and then coreference. , instead, quantified differences in the transferability of individual layers between different models, showing that higher layers of RNNs  are more task-specific , while transformer layers  do not exhibit this increase in task-specificity.   Alternative approaches have been proposed to study the linguistic information stored in NLMs and how this information evolve under different learning objectives or during a pre--training process.  proposed a method based on canonical correlation analysis  and mutual information to analyze how several learning objectives, such as machine translation, standard left-to-right language modelling  and masked language modeling  determine the evolution of the individual tokens' representations between layers in the transformer model. Using the same training data, model architecture and parameter initialization they found that the evolution of a token representation across layers changes according to the three different learning objectives.  used SVCCA to investigate how representations of linguistic structure are learned over time in a two-layer LSTM-based LM, discovering that different aspects of linguistic structure are learned at different rates within a single recurrent layer, acquiring POS tags early but continuing to learn topic information later in training.  Compared with the aforementioned studies, our work provides a more in depth analysis of the linguistic knowledge encoded by BERT before and after a fine-tuning process through a wide range of probing features, each of which corresponds to a specific sentence-level phenomena, and how this knowledge affects the ability of the model to solve a specific downstream task.   To our knowledge, this work is the first  Despite previous work have shown that pre--trained NLMs implicitly encode specific linguistic properties, there has been few studies that have attempted to understand how these encodings evolve after a fine-tuning process and how this implicit knowledge affects the decision they make when solving specific task. In our work, we    ricontrollare se non ci sono studi su evoluzione features linguistiche pre-training vs. fine-tuning   RICORDARE DI AGGIORNARE ARTICOLI DI EMNLP CON LA REF A EMNLP E NON AD ARXIV    inserire articolo Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs",37
" % 1 - What problem are you solving? Entity typing classifies textual mentions of entities, according to their semantic class, within a set of labels  organized in an inventory. %Multi-label text classification is the task of assigning to a sample all the relevant labels from a label  inventory . The task has progressed from recognizing a few coarse classes , to extremely large inventories, with hundreds  or thousands of labels . Therefore, exploiting inter-label correlations has become critical to improve performance.   % 2 - Why is it an interesting/important problem? % es interesante porque son buenos para modelar redes y estructuras jer璋﹔quicas. % Problema: su adopcion en nlp ha sido baja dado que no hay una forma muy intuitiva de modelar texto en ellos. Distintos papers muestran como agregar un peque甯給 cambio pero no una aplicacion real y completa Large inventories tend to exhibit a hierarchical structure, either by an explicit tree-like arrangement of the labels , or implicitly through the label distribution in the dataset . %A natural solution for dealing with large inventories is to organize them in hierarchy ranging from general, coarse labels near the top, to more specific, fine classes at the bottom. Prior work has integrated only explicit hierarchical information by formulating a hierarchy-aware loss  or by representing instances and labels in a joint Euclidean embedding space .  However, the resulting space is hard to interpret, and these methods fail to capture implicit relations in the label inventory. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures . Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance from the root  . %Its tree-like properties make it efficient to learn hierarchical representations with low distortion .     % Embeddings  that  are  close  to  the  origin  of  the  disk  will have a relatively small distance to all other points, rep-resenting the root of the hierarchy.  On the other hand,embeddings that are close to the boundary of the disk will have a relatively large distance to all other points and are well suited to represent leaf nodes   % 3 - How are you going to solve it? In this work, we propose a fully hyperbolic neural model for fine-grained entity typing. Noticing a perfect match between hierarchical label inventories in the linguistic task and the benefits of hyperbolic spaces, we endow a classification model with a suitable geometry to capture this fundamental property of the data distribution. By virtue of the hyperbolic representations, the proposed approach automatically infers the latent hierarchy arising from the class distribution and achieves a meaningful and interpretable organization of the label space. This arrangement captures implicit hyponymic relations  in the inventory and enables the model to excel at fine-grained classification. To the best of our knowledge, this work is the first to apply hyperbolic geometry from beginning to end to perform multi-label classification on real NLP datasets.  %NICE PHRASE FROM GULCEHRE: The focus of this work is to endow neural network representations with suitable geometry to capture fundamental properties of data... given the perfect fit between the label distribution in the linguistic task of entity typing and the mathematical properties of hyperbolic spaces.   % esto deberia ser ""hay componentes ya hechos"". Y lo conecto al toque con el parrafo sig.  Recent work has proposed hyperbolic neural components, such as word embeddings , recurrent neural networks  and attention layers . %Advantages of hyperbolic representations are well-established for discrete data such as networks  and graphs . In the realm of Natural Language Processing  components that exploit hyperbolic geometry have been developed as well, such as word embeddings , recurrent neural networks  and attention layers . %or classifiers  Me encanta este paper pero no hace NLP :. We address these issues. Our model encodes textual inputs, applies a novel attention mechanism, and performs multi-class multi-label classification, executing all operations in the Poincar\'e model of hyperbolic space . %By employing the leveraging the geometric properties of hyperbolic space through    %The lack of systems that utilize hyperbolic space from beginning to end is due to three main difficulties: %First, there are different analytic models of hyperbolic space, and not all previous work operates in the same one, which hinders their combination.  %Second, it is not clear how to integrate these components into conventional Euclidean neural models since a mapping of the data from one space onto the other is required. Third, optimization of hyperbolic models is non-trivial.   %We bridge the gaps among previous work by developing the missing connections and adapting different components to employ the Poincar\'e model of hyperbolic space in all layers of the network.  % We bridge the gaps among previous work by developing the missing connections and adapting different components, in order to accomplish a full hyperbolic neural network. This is, a network that extracts features from text, applies attention layers and performs \todo{I am the only one doing this}{multi-class classification}, executing all operations in hyperbolic geometry.   % able to perform multi-label multi-class classification with text as input    %The model is proposed in a generic manner such that it can be applied to classify sequential data . Since hyperbolic geometry is naturally equipped to model hierarchical structures, we hypothesize that the model will excel at tasks that profit from the incorporation of hierarchical information. % \todo{awful}{systems} that operate under this metric space result in superior performance when incorporating hierarchical information.   %We evaluate our model on the task of fine-grained entity type classification , which we consider a suitable testbed due to its connection with textual inputs and hierarchical type inventories.  % Introduce main results % HNN's phrase: ""On a series of experiments and datasets we showcase the effectiveness of our hyperbolic neural network layers compared to their ""classic"" Euclidean variants on"" % \todo[inline]{Forwarding a bit of the results is a good idea . %\todo[inline]{Cambiar esta frase a la idea de que ""imponer the right metric es como imponer the right bias""}  %We impose an inductive bias on the model by means of the geometry of its internal representation. This allows us to operate on very low-dimensional spaces thus substantially reducing the parameter cost. Instead of relying on large pre-trained models, we impose a suitable inductive bias by choosing an adequate metric space to embed the data, which does not introduce extra burden on the parameter footprint. %Phrase from xiong2019inductiveBias: ""Instead of using an explicit graphical model, we enforce a relational bias on model parameters, which does not introduce extra burden on label decoding."" % Misma idea pero yo meto el bias en la representacion, lo cual no introduce un costo adicional y permite operar con MUCHOS menos par璋﹎etros.   %Our components are developed in a modular way which allows them to be seamlessly integrated into NLP architectures.    %\todo{Remove!}{While there now exist several hyperbolic components, a practitioner faced with these options has a simple question: How to integrate them with conventional layers? In this work, we answer this question.}  By means of the exponential and logarithmic maps  we are able to mix hyperbolic and Euclidean components into one model, aiming to exploit their strengths at different levels of the representation. We perform a thorough ablation that allows us to understand the impact of each hyperbolic component in the final performance of the system , and showcases its ease of integration with Euclidean layers.  %In summary, we make the following contributions: %%%%% %      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       Podemos hablar de:   - Hyperbolic spaces que entran mas bien al final y no a lo largo de la representacion: Lopez o Tay .   - Their perfomance has been showcased over synthetic datasets    - Se hace una aggregation operation pero la clasificaci璐竛 no suele ser en el espacio hyperbolico  Type inventories for the task of fine-grained entity typing  have grown in size and complexity . Researchers have tried to incorporate hierarchical information on the type distribution in different manners .   encode the hierarchy through a sparse matrix.   model the relations through a hierarchy-aware loss function.  derive a graph from type co-occurrence statistics in the dataset. Experimental evidence suggests that our model encodes similar hierarchical information without the need to provide it explicitly.  Hyperbolic representations have been employed for Question Answering , in Machine Translation , and modeling language .  We build upon the hyperbolic neural layers introduced in , and develop the missing components to perform, not binary, but multi-class multi-label text classification. We test the proposed model not with a synthetic dataset, but on a concrete downstream tasks, such as entity typing.  Our work resembles  and , though they separately learn embeddings for type labels and text representations in hyperbolic space,  in order to facilitate information sharing among them,  whereas we do it in an integrated fashion.     \todo{The philosophical  framing I mean}{Overall,} we advocate for alternative representation methods, with a well-established mathematical foundation, for efficient and effective neural NLP components, pursuing a similar goal to .    From ""An Attentive Fine-Grained Entity Typing Model with Latent Type Representation"": ""Fine-grained entity typing is usually formulated as a multi-label classification problem. Previous approaches  typically address it with binary relevance that decomposes the problem into isolated binary classification subproblems and independently predicts each type. However, this method is commonly criticized for its label independence assumption, which is not valid for fine- grained entity typing.""                                                                     We demonstrated that a standard communication system, where standard Speaker and Listener LSTMs are trained to solve a simple reconstruction game, leads to long messages, close to the maximal threshold. Surprisingly, if these messages are long, LSTM agents rely only on a small number of informative message symbols, located at the end. We then introduce LazImpa, a constrained system that consists of Lazy Speaker and Impatient Listener. On the one hand, Lazy Speaker is obtained by introducing a cost on messages length once the communication is successful. We found that early exploration of potentially long messages is crucial for successful convergence . On the other hand, Impatient Listener aims to succeed at the game as soon as possible, by predicting Speaker's input at each message's symbol.   We show that both constraints are necessary for the emergence of a ZLA-like protocol, as efficient as natural languages. Specifically, Lazy Speaker alone would fail to shorten the messages. We connect this to the importance of the Impatience mechanism to locate useful information at the beginning of the messages. If the function of this mechanism is subject to a standing debate \cite[e.g.,][]{jackendoff2007,anderson2013}, many prior works had pointed to its necessity to human language understanding \citep[e.g.,][]{friston:2010,clark:2013}. We augment this line of works and suggest that impatience could be at play in the emergence of ZLA-obeying languages. However, if impatience leads to ZLA, it is not sufficient for human-level efficiency. In other words, efficiency needs constraints both on Speaker and Listener sides.   Our work highlights the importance of introducing the right pressures in the communication system. Indeed, to construct automated agents that would eventually interact with humans, we need to introduce task-agnostic constraints, allowing the emergence of more human-like communication. Moreover, while being general, LazImpa provides a more stable optimization compared to the unconstrained system. Finally, this study opens several lines of research. One would be to investigate further the gap from optimality. Indeed, while LazImpa emergent languages show human-level efficiency, they do not reach optimal coding. Specifically, emergent languages still have non-informative symbols at the end of the messages. If these additional non-useful symbols drift the protocol from optimality, we encounter similar trend in human  and animal communication  . We leave the understanding of the role of these non-informative symbols and how we can reach optimal coding for future works. A second line of research would be to apply this system to other games or NLP problems and study how it affects other properties of the language such as regularity or compositionality.   
","     Podemos hablar de:   - Hyperbolic spaces que entran mas bien al final y no a lo largo de la representacion: Lopez o Tay .   - Their perfomance has been showcased over synthetic datasets    - Se hace una aggregation operation pero la clasificaci鐠愮珱 no suele ser en el espacio hyperbolico  Type inventories for the task of fine-grained entity typing  have grown in size and complexity . Researchers have tried to incorporate hierarchical information on the type distribution in different manners .   encode the hierarchy through a sparse matrix.   model the relations through a hierarchy-aware loss function.  derive a graph from type co-occurrence statistics in the dataset. Experimental evidence suggests that our model encodes similar hierarchical information without the need to provide it explicitly.  Hyperbolic representations have been employed for Question Answering , in Machine Translation , and modeling language .  We build upon the hyperbolic neural layers introduced in , and develop the missing components to perform, not binary, but multi-class multi-label text classification. We test the proposed model not with a synthetic dataset, but on a concrete downstream tasks, such as entity typing.  Our work resembles  and , though they separately learn embeddings for type labels and text representations in hyperbolic space,  in order to facilitate information sharing among them,  whereas we do it in an integrated fashion.     \todo{The philosophical  framing I mean}{Overall,} we advocate for alternative representation methods, with a well-established mathematical foundation, for efficient and effective neural NLP components, pursuing a similar goal to .    From ""An Attentive Fine-Grained Entity Typing Model with Latent Type Representation"": ""Fine-grained entity typing is usually formulated as a multi-label classification problem. Previous approaches  typically address it with binary relevance that decomposes the problem into isolated binary classification subproblems and independently predicts each type. However, this method is commonly criticized for its label independence assumption, which is not valid for fine- grained entity typing.""",38
"  We make many decisions as we interact with the world. When we are rewarded , we learn to modify not only the proximal cause of the stimulus but the chain of decisions leading up to it, to encourage  future similar results. This process naturally is the paradigm of Reinforcement Learning . Policy-based learning seeks to find good estimates for , a function that returns the expected cumulative reward  if action  is chosen at state . A desirable property of methodologies to learn  is their ability to generalize such that an appropriate action can be taken when encountering a previously unseen state.   Recent advances have shown strong evidence of generalization in spatiotemporal modalities such as robotic manipulation , video games , and autonomous navigation . However, in the modality of language, there is less work applying generalization approaches to decision making.  Useful applications of sequential decision making language models are personal assistants that proactively anticipate client needs; anti-phishing mediation agents that waste a would-be thief's time with relevant but non-helpful responses; and investigative journalist assistants that determine what to read, whom to contact, and what questions to ask to create a revelatory news report.  Neural reinforcement learning  training approaches, such as those used to play action video games , have potential applicability in language-based decision making due to their ability to learn to navigate adversarial or exploratory scenarios. Naturally, the generalization and background knowledge capability afforded by large contextualized language models such as \bert  may be applicable as well. A useful virtual world proxy in which to explore these approaches' applicability is that of text adventure game playing. In a text adventure game, a player is immersed in an environment by reading textual descriptions of a scene and issuing natural language commands to navigate inside the scene. The player discovers and interacts with entities and accomplishes goals, while receiving explicit rewards for doing so.   Learning to play text games is a useful pursuit because it is a convenient proxy for the real world cases cited above. Unlike these, plentiful data for numerous games exist, an endless supply of games can be constructed, and text games have built-in reward functions,  making them suitable for RL. This class of problems is also useful because it is challenging: after exposure to a family of games that explore the same topic and have similar gameplay , human players perform nearly perfectly on additional games, but computer models struggle.   Why is this? Humans quickly understand the situation they are placed in and can make rational decisions based on trial-and-error and life experience, which we can call commonsense knowledge. Knowing a priori that, e.g.,  a  door should be  or that it is helpful to  in a  allows  players to learn  faster. Even though these games have the complexity of finite-state machines, computer models cannot learn to play them well. The problem appears to be due to a lack of generalization caused by a lack of commonsense. To a computer model, considering whether to  using a  is no more ludicrous than considering whether to  using a  . Both actions can be discouraged by negative reinforcement, but a human only needs to learn not to do the latter.   Furthermore, a computer player learning that one can  with a  may not generalize that one can  the same way, but a human surely will.  There is existing work in learning to play text games with RL   but the standard pattern of incorporating large language models such as \bert  has not yet been seen in current literature. It turns out that this integration is not trivial. Most models that use \bert and its ilk predominantly apply their results to supervised learning tasks that have training data with ground truth  or at least, in the case of generation-based tasks like dialogue and translation, a corpus of desirable output to mimic . For tasks suited to RL such as the exploration of and interaction with a world, there is no true target or even, initially, a corpus, and thus learning can only proceed iteratively via, e.g., exploration-exploitation , which requires millions of training iterations to converge . Integrating this process with the additional overhead of fine-tuning a large model like \bert leads to an impractical slowdown: for the experiments considered in this work, the baseline models that use \cnn require a little more than three weeks to train on an Nvidia P100 GPU-equipped machine. Using the same models on the same tasks run for the same number of iterations on the same hardware while fine-tuning a 12-layer \bert model would take more than two years.      In this work, we compare different previously used representation models for deep RL through an imitation learning method that first trains a light-weight teacher using exploration-exploitation, and then uses that trained model to train a more heavy-weight student model. This dramatically decreases the amount of training time needed to learn.  Moreover, we devise a means of casting an RL problem into a supervised learning paradigm, allowing better exploitation of large contextualized language models. In so doing, we show that agents can benefit from both the imitation learning and the reformulation, converging faster than other models, and exceeding teacher performance by 7\% and 24\% on both in- and out-of-domain problems, despite the limited search space.  The novel contributions of this work are:       Many recent works  on building agents to play text-based games apply DQNs  or their variants. Different aspects of DQN have been presented, such as action reduction with language correlation , a bounding method , the introduction of a knowledge graph , text understanding with dependency parsing , and the bandit feedback method for agent evaluation .  However, previous work uses different games to evaluate, making it difficult to compare results comprehensively. With the TextWorld framework's availability, there is more and more work concentrating on the generalization ability of agents, which seldom appears in the video game playing domain.  work on generalization of agents on variants of a very simple coin-collecting game. The simplicity of their games enables them to use an LSTM-DQN method with a counting-based reward.  use a knowledge graph as a persistent memory to encode states, while we use a knowledge graph later on to make actions more informative.  The TextWorld competition has yielded a variety of works that use different approaches and methods:  co-train a DQN with a question answering system for building new interactive machine reading comprehension tasks while creating agents to solve games.  describe a non-RL method to learn agents, by first randomly playing on training games, then collecting all winning trajectories. By using these trajectories as training data, they manage to transform an RL problem into supervised learning.   use an actor-critic framework and prune the action-space by using hierarchical RL and a specialized module trained on a recipe database to build better agents.  apply the action elimination method proposed by  on Zork to the cooking games.  For teacher-student training,  design a policy distillation method that trains different agents as teacher agents. Each of these teacher agents learns to play a single and separate game. Then they build one student learner that can be trained with a supervised learning method to distill the policy knowledge for multi-game playing.  also use teacher-student training for text-based games. However, our teacher-student training method is different: we use one teacher that can play multiple games to guide multiple student agents' learning processes.              File main.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym}  \renewcommand{\UrlFont}{\ttfamily\small}    This is not strictly necessary, and may be commented out,   but it will improve the layout of the manuscript,   and will typically save some space. \usepackage{microtype} \usepackage[utf8]{inputenc} \usepackage{dirtytalk} \usepackage{natbib} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{bm} \usepackage{adjustbox} \renewcommand{\UrlFont}{\ttfamily\small}  \usepackage{xcolor} \usepackage{xspace} \usepackage{comment} \usepackage{graphicx} \usepackage{url} \usepackage{array} \usepackage{multirow} \usepackage{booktabs} \usepackage{caption} \usepackage{subcaption} \usepackage{etoolbox}   \AtBeginEnvironment{quote}{\singlespacing \small}   \aclfinalcopy   Uncomment this line for the final submission  \def\aclpaperid{***}    Enter the acl Paper ID here   \setlength\titlebox{5cm}   You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.  \newcommand{\joy}[1]{{\color{blue}{\bf #1 -joy}}} \newcommand{\cy}[1]{{\color{orange}{\bf #1 -cy}}} \newcommand{\lwku}[1]{{\small\color{red}{\bf\xspace#1 -ku}}}  \newcommand{\kenneth}[1]{{\color{blue}{\bf #1 -Kenneth}}}  \newcommand{\fitb}{context modeling \xspace} \newcommand{\ent}{entailment modeling \xspace}  \newcommand{\entshort}{EMLA\xspace} \newcommand{\fitbshort}{CMLA\xspace}  \newcommand\BibTeX{B\TeX}  \title{Assessing the Helpfulness of Learning Materials \\with Inference-Based Learner-Like Agent}  \author{Yun-Hsuan Jen, Chieh-Yang Huang, Mei-Hua Chen, \\ Ting-Hao  Huang, and Lun-Wei Ku \\   Academia Sinica, Taipei, Taiwan. \\ \texttt{yhjen2@gmail.com}, \texttt{lwku@iis.sinica.edu.tw}\\   Pennsylvania State University, University Park, PA, USA. \\ \texttt{\{chiehyang,txh710\}@psu.edu}\\   Tunghai University, Taichung, Taiwan. \texttt{mhchen@thu.edu.tw} }  \date{}     
"," Many recent works  on building agents to play text-based games apply DQNs  or their variants. Different aspects of DQN have been presented, such as action reduction with language correlation , a bounding method , the introduction of a knowledge graph , text understanding with dependency parsing , and the bandit feedback method for agent evaluation .  However, previous work uses different games to evaluate, making it difficult to compare results comprehensively. With the TextWorld framework's availability, there is more and more work concentrating on the generalization ability of agents, which seldom appears in the video game playing domain.  work on generalization of agents on variants of a very simple coin-collecting game. The simplicity of their games enables them to use an LSTM-DQN method with a counting-based reward.  use a knowledge graph as a persistent memory to encode states, while we use a knowledge graph later on to make actions more informative.  The TextWorld competition has yielded a variety of works that use different approaches and methods:  co-train a DQN with a question answering system for building new interactive machine reading comprehension tasks while creating agents to solve games.  describe a non-RL method to learn agents, by first randomly playing on training games, then collecting all winning trajectories. By using these trajectories as training data, they manage to transform an RL problem into supervised learning.   use an actor-critic framework and prune the action-space by using hierarchical RL and a specialized module trained on a recipe database to build better agents.  apply the action elimination method proposed by  on Zork to the cooking games.  For teacher-student training,  design a policy distillation method that trains different agents as teacher agents. Each of these teacher agents learns to play a single and separate game. Then they build one student learner that can be trained with a supervised learning method to distill the policy knowledge for multi-game playing.  also use teacher-student training for text-based games. However, our teacher-student training method is different: we use one teacher that can play multiple games to guide multiple student agents' learning processes.",39
"  %   Reinforcement learning has shown great success in environments with large state spaces. Using neural networks to capture state representations has allowed end-to-end training of agents on domains like Atari  and Go . It is natural to emulate this success in text domains, especially given that the state space in language-based tasks is combinatorially large. A sentence of length  with allowed vocabulary  has  possible states, and tabular methods like learning  will fail unless coupled with powerful function approximators like neural networks.\\  While the current state of RL has multiple challenges, sparse rewards are one that leads to slow, and sometimes no convergence. Consider an agent learning in an environment with a large state space, with only a few states leading to a reward . An agent starting on the far left must take a large number of actions before encountering a reward. In turn, this sparse feedback results in a very noisy gradient for training the neural network. In an extreme scenario, as in Figure , an agent might have to take an exponential number of actions to reach a single leaf that has a reward.      Some early work, such as reward shaping , attempted to solve the sparse reward problem by introducing dense rewards based on heuristics, e.g., how close the agent is to the goal. However, these require complex design choices that might result in unexpected behavior from the agents.\\  Sparse rewards are common because they are the most straightforward way to specify how a task needs to be solved. If a robot is expected to pour water from a jug into a glass, the simplest way is to give a reward of  if it fills the glass, and  otherwise. This type of reward design is common in text-based games, in which the agent is rewarded upon reaching the goal state, and task-oriented dialogue, in which the agent is rewarded based on the successful completion of the task.\\  For this study, we examine text-based games and find that providing dense rewards with the help of sentiment analysis improves performance under some conditions.      Recent work has begun incorporating information from NLP into the rewards used in reinforcement learning tasks, particularly dialogue generation and text-based games. \citet{DBLP:journals/corr/LiMRGGJ16} trained two reinforcement learning agents to produce less repetitive, more coherent responses using rewards based on seq2seq models of the responses. \citet{DBLP:journals/corr/abs-1708-00133} mapped text descriptions to transitions and rewards in an environment by creating a Q-function for reinforcement learning conditioned on the descriptions. Focusing on the problem of finding intermediate rewards, \citet{DBLP:journals/corr/abs-1903-02020} incorporated GloVe vectors into their reward function, significantly improving performance on text-based games. However, these reinforcement learning approaches have not yet incorporated the ability to fine-tune BERT  to set the rewards given to the user output on a given turn  or the appearance of a new text description .\\  Some studies have used natural language as a decoy for reward functions , but the text used in them must explicitly indicate what the goal state is, and an object-oriented MDP  must then be used to extract the goal state. These restrictions severely limit the use cases of the method. Others have examined using text descriptions from a game manual to extract information relevant to determining rewards and promising states during gameplay, though without analyzing text during gameplay itself . \citet{krening2017learning} acknowledge the dearth of methods that use text explanations and advice, but instead propose a separate policy for each object, which hinders scaling up this method.    Alleviating the sparse reward problem has received much attention since the inception of RL. We note some research that is orthogonal to our proposed method and can thus be used in tandem. Reward shaping gives auxiliary rewards to an agent based on how ``far"" in the state space the agent is from the goal . For example, for an agent that must navigate to a goal state on a 2-dimensional grid, the auxiliary reward can be inversely proportional to the distance from the goal state on the grid. This method requires the additional restriction that the reward function be a potential-based energy function to avoid positive reward loops.\\  Hindsight Experience Replay uses the novel idea that both successful and unsuccessful trajectories can be used for training if the policy learned is conditional on the goal . For example, if an agent tries to kick a ball straight ahead and ends up kicking it to the left, it can use that trajectory by assuming it actually wanted to kick it to the left, hence increasing the number of trajectories on which it receives a positive reward.\\  Other work, such as \citet{agarwal2019learning}, makes use of a meta-framework to provide auxiliary rewards that supplement the environment's rewards. Our method resembles this in a limited setting.        More recent approaches to text-based games have examined other methods in which natural language can inform RL agents. \citet{ammanabrolu2018playing} represented the game state as a knowledge graph learned during game exploration. This method allowed the agent's action space to be reduced to a question-answering task, pruning options to allow for more efficient exploration. Work on affordance extraction--determining the set of behaviors enabled by a particular game state--has advanced by using word embeddings to create a common knowledge database. The database can then be queried by the RL agent; affordance-based action improved performance in most situations . Although some of these efforts have goals that eventually diverge from our research, they share the need for denser rewards, an overarching issue in the success of linguistically informed approaches to text-based games.     We provide a recipe for integrating large contextualized language models and deep reinforcement learning, applying to sequential decision making and a demonstration on the proxy task of text games, showing dramatic improvements over the standard practice, particularly in out-of-domain held-out tests. We expect to apply this approach to various challenging real-world sequential decision scenarios, such as goal-directed dialogue and active information-gathering.  
","   Recent work has begun incorporating information from NLP into the rewards used in reinforcement learning tasks, particularly dialogue generation and text-based games. \citet{DBLP:journals/corr/LiMRGGJ16} trained two reinforcement learning agents to produce less repetitive, more coherent responses using rewards based on seq2seq models of the responses. \citet{DBLP:journals/corr/abs-1708-00133} mapped text descriptions to transitions and rewards in an environment by creating a Q-function for reinforcement learning conditioned on the descriptions. Focusing on the problem of finding intermediate rewards, \citet{DBLP:journals/corr/abs-1903-02020} incorporated GloVe vectors into their reward function, significantly improving performance on text-based games. However, these reinforcement learning approaches have not yet incorporated the ability to fine-tune BERT  to set the rewards given to the user output on a given turn  or the appearance of a new text description .\\  Some studies have used natural language as a decoy for reward functions , but the text used in them must explicitly indicate what the goal state is, and an object-oriented MDP  must then be used to extract the goal state. These restrictions severely limit the use cases of the method. Others have examined using text descriptions from a game manual to extract information relevant to determining rewards and promising states during gameplay, though without analyzing text during gameplay itself . \citet{krening2017learning} acknowledge the dearth of methods that use text explanations and advice, but instead propose a separate policy for each object, which hinders scaling up this method.    Alleviating the sparse reward problem has received much attention since the inception of RL. We note some research that is orthogonal to our proposed method and can thus be used in tandem. Reward shaping gives auxiliary rewards to an agent based on how ``far"" in the state space the agent is from the goal . For example, for an agent that must navigate to a goal state on a 2-dimensional grid, the auxiliary reward can be inversely proportional to the distance from the goal state on the grid. This method requires the additional restriction that the reward function be a potential-based energy function to avoid positive reward loops.\\  Hindsight Experience Replay uses the novel idea that both successful and unsuccessful trajectories can be used for training if the policy learned is conditional on the goal . For example, if an agent tries to kick a ball straight ahead and ends up kicking it to the left, it can use that trajectory by assuming it actually wanted to kick it to the left, hence increasing the number of trajectories on which it receives a positive reward.\\  Other work, such as \citet{agarwal2019learning}, makes use of a meta-framework to provide auxiliary rewards that supplement the environment's rewards. Our method resembles this in a limited setting.        More recent approaches to text-based games have examined other methods in which natural language can inform RL agents. \citet{ammanabrolu2018playing} represented the game state as a knowledge graph learned during game exploration. This method allowed the agent's action space to be reduced to a question-answering task, pruning options to allow for more efficient exploration. Work on affordance extraction--determining the set of behaviors enabled by a particular game state--has advanced by using word embeddings to create a common knowledge database. The database can then be queried by the RL agent; affordance-based action improved performance in most situations . Although some of these efforts have goals that eventually diverge from our research, they share the need for denser rewards, an overarching issue in the success of linguistically informed approaches to text-based games.",40
"  Natural language data is rich in structure, but most of the structure is not visible at the surface.  Machine learning models tackling high-level language tasks would benefit from uncovering underlying structures such as trees, sequence tags, or segmentations.  Traditionally, practitioners turn to pipeline approaches where an external, pretrained model is used to predict, \eg, syntactic structure.  The benefit of this approach is that the predicted tree is readily available for inspection, but the downside is that the errors  can easily propagate throughout the pipeline and require further attention . In contrast, deep neural architectures tend to eschew such preprocessing, and instead learn soft hidden representations, not easily amenable to visualization and analysis.  The best of both worlds would be to model structure as a latent variable, combining the transparency of the pipeline approach with the end-to-end unsupervised representation learning that makes deep models appealing. Moreover, large-capacity model tend to rediscover structure from scratch , so structured latent variables may reduce the required capacity.  Learning with discrete, combinatorial latent variables is, however, challenging, due to the intersection of large cardinality and null gradient issues. For example, when learning a latent dependency tree, the latent parser must choose among an exponentially large set of possible trees; what's more, the parser may only learn from gradient information from the downstream task. If the highest-scoring tree is selected using an argmax operation, the gradients will be zero, preventing learning.  One strategy for dealing with the null gradient issue is to use a surrogate gradient, explicitly overriding the zero gradient from the chain rule, as if a different computation had been performed. The most commonly known example is the straight-through estimator \citep[STE;][]{bengio2013estimating}, which pretends that the argmax node was instead an identity operator. Such methods lead to a fundamental mismatch between the objective and the learning algorithm. The effect of this mismatch  is still insufficiently understood, and the design of successful new variants is therefore challenging. For example, the recently-proposed SPIGOT method  found it beneficial to use a projection as part of the surrogate gradient.  In this paper, we study surrogate gradient methods for deterministic learning with discrete structured latent variables. Our contributions are:    While the discrete methods do not outperform the relaxed alternatives using the same building \linebreak blocks, we hope that our interpretation and insights would trigger future latent structure research.  The code for the paper is available on \url{https://github.com/deep-spin/understanding-spigot}.         Discrete latent variable learning is often tackled in stochastic computation graphs, by estimating the gradient of an expected loss. An established method is the score function estimator  . SFE is widely used in NLP, for tasks including minimum risk training in NMT  and latent linguistic structure learning . In this paper, we focus on the alternative strategy of surrogate gradients, which allows learning in deterministic graphs with  discrete, argmax-like nodes, rather than in stochastic graphs. Examples are the straight-through estimator   and the structured projection of intermediate gradients optimization technique .   Applications of STE - from the reviewer comments: Recent work focuses on studying and explaining STE. \citet{yin2019understanding} obtained a convergence result in shallow networks for the unstructured case. \citet{cheng2018straight} show that STE can be interpreted as the simulation of the projected Wasserstein gradient flow. STE has also been studied in binary neural networks  and in other applications . Other methods based on the surrogate gradients have been recently explored .  A popular alternative is to relax an argmax into a continuous transform such as softmax or sparsemax , as seen for instance in soft attention mechanisms , or structured attention networks . In-between surrogate gradients and relaxation is Gumbel softmax, which uses the Gumbel-max reparametrization to sample from a categorical distribution, applying softmax either to relax the mapping or to induce surrogate gradients . Gumbel-softmax has been successfully applied to latent linguistic structure as well . For sampling from a structured variable is required, the Perturb-and-MAP technique  has been successfully applied to sampling latent structures in NLP applications .     We find that adding auxiliary rewards using sentiment analysis can help improve RL agents' performance in text domains. Our methods take a step in the direction of creating agents that infers rewards by themselves. We expect that these improvements are applicable to similar text-based domains, such as task-oriented dialogue. Given the rapid improvements in NLP methods, we believe that better pre-training and sentiment analysis models will translate to better RL agents in the future.  
","  Discrete latent variable learning is often tackled in stochastic computation graphs, by estimating the gradient of an expected loss. An established method is the score function estimator  . SFE is widely used in NLP, for tasks including minimum risk training in NMT  and latent linguistic structure learning . In this paper, we focus on the alternative strategy of surrogate gradients, which allows learning in deterministic graphs with  discrete, argmax-like nodes, rather than in stochastic graphs. Examples are the straight-through estimator   and the structured projection of intermediate gradients optimization technique .   Applications of STE - from the reviewer comments: Recent work focuses on studying and explaining STE. \citet{yin2019understanding} obtained a convergence result in shallow networks for the unstructured case. \citet{cheng2018straight} show that STE can be interpreted as the simulation of the projected Wasserstein gradient flow. STE has also been studied in binary neural networks  and in other applications . Other methods based on the surrogate gradients have been recently explored .  A popular alternative is to relax an argmax into a continuous transform such as softmax or sparsemax , as seen for instance in soft attention mechanisms , or structured attention networks . In-between surrogate gradients and relaxation is Gumbel softmax, which uses the Gumbel-max reparametrization to sample from a categorical distribution, applying softmax either to relax the mapping or to induce surrogate gradients . Gumbel-softmax has been successfully applied to latent linguistic structure as well . For sampling from a structured variable is required, the Perturb-and-MAP technique  has been successfully applied to sampling latent structures in NLP applications .",41
"   %% Paragraph 1:  %% * introduce the constructions of interest  %% * give broad impression of the subtlety of grammatical phenomena, %% * emphasize the verb bias problem, since this is one of our unique contributions When we use language, we are often faced with a choice between several possible ways of expressing the same message. For example, in English, to express an event of intended or actual transfer between two animate entities, one option is the double-object  construction, in which two noun phrases follow the verb.  Alternatively, the same content can be expressed using the prepositional dative  construction.  \ex.  \a. Ava gave him something. \hfill DO \b. Ava gave something to him. \hfill PO  Speakers' preferences for one or the other construction depend on multiple factors, including the length and definiteness of the arguments  . % could also cite: Davidse 1996; Givo 铏俷 1984a; Polinsky 1996; Ransom 1979; Snyder 2003; Thompson 1990, 1995;  One particularly subtle factor is the lexical verb bias. While some verbs readily occur in either construction, others have strong preferences for one over the other :  \ex.  \a. ?Ava said him something. \hfill DO \b. Ava said something to him. \hfill PO  %% Paragraph 2:  %% * transition to motivation for why this problem is interesting for NLP %% * briefly mention major previous work on this problem and its gaps   Decades of work in linguistics and psychology has investigated how humans learn these distinctions . Yet, as deep neural networks have achieved state-of-the-art performance across many tasks in natural language processing, little is known about the extent to which they have acquired similarly fine-grained preferences. Although neural language models robustly capture certain types of grammatical constraints, e.g., subject-verb agreement and long distance dependencies , they continue to struggle with other aspects of syntax, including argument structure \cite[e.g.][]{warstadt2019neural}. Verb biases provide a particularly interesting testbed.  Successfully predicting these psycholinguistic phenomena requires the integration of specific lexical information with representations of higher-level grammatical structures, with implications for understanding differential performance between models on other tasks.    %% Paragraph 3: our contribution In the current work, we take an analytic and comparative approach. First, we introduce the DAIS  dataset, containing 50K human preference judgments for 5K sentence pairs, using 200 unique verbs. These empirical judgments indicate that verb bias preferences are highly gradient in practice , rather than belonging to binary ``alternating'' and ``non-alternating'' classes, as commonly assumed. Second, we evaluate the predictions of a variety of neural models, including both recurrent architectures and transformers, and analyze their internal states to understand what drives differences in performance.  \change{Finally, we evaluate our models on natural production data from the Switchboard corpus, finding that transformers achieve similar classification accuracy as prior work using hand-annotated features \cite[;][]{bresnan2007predicting}.}      Several recent studies have investigated how neural language models represent the dative alternation. \citet{kann2018verb} constructed a corpus of verbs in common alternations, including the dative, and showed that a degree of information about acceptability is decodable directly from embeddings of the verb.  However, acceptability was not based on empirical data and verb bias was treated as a binary variable, preventing an analysis of gradient effects.  \change{\citet{kelly2020sentence} found that DO constructions are separable from non-DO constructions in high-dimensional sentence embeddings , but did not investigate verb bias.} \citet{futrell-levy-2019-rnns} confirmed that recurrent neural networks  show human-like sensitivity to several other important aspects of gradience in dative alternations, including the length and definiteness of arguments. However, they included only 16 verbs, all considered ``alternating.'' Additionally, in these studies, a limited range of neural models were considered, leaving it unclear exactly how predictions may depend on architectural choices, model size, and training regime.      In this work, we provide a novel motivation for straight-through estimator  and SPIGOT, based on pulling back the downstream loss. We derive promising new algorithms, and novel insight into existing ones. Unstructured controlled experiments suggest that our new algorithms, which use the cross-entropy loss instead of the perceptron loss, can be more stable than SPIGOT while accurately disentangling the latent variable. Differentiable relaxation models  are the easiest to optimize to high downstream accuracy, but they fail to correctly identify the latent clusters. On structured NLP experiments, relaxations  tend to overall perform better and be more stable than straight-through variants in terms of classification accuracy. However, the lack of gold-truth latent structures makes it impossible to assess recovery performance. We hope that our insights, including some of our negative results,  may encourage future research on learning with latent structures.   
","  Several recent studies have investigated how neural language models represent the dative alternation. \citet{kann2018verb} constructed a corpus of verbs in common alternations, including the dative, and showed that a degree of information about acceptability is decodable directly from embeddings of the verb.  However, acceptability was not based on empirical data and verb bias was treated as a binary variable, preventing an analysis of gradient effects.  \change{\citet{kelly2020sentence} found that DO constructions are separable from non-DO constructions in high-dimensional sentence embeddings , but did not investigate verb bias.} \citet{futrell-levy-2019-rnns} confirmed that recurrent neural networks  show human-like sensitivity to several other important aspects of gradience in dative alternations, including the length and definiteness of arguments. However, they included only 16 verbs, all considered ``alternating.'' Additionally, in these studies, a limited range of neural models were considered, leaving it unclear exactly how predictions may depend on architectural choices, model size, and training regime.",42
"     The core idea behind the predominant pretrain and fine-tune paradigm for transfer learning in NLP is that general language knowledge, gleaned from large quantities of data using unsupervised objectives, can serve as a foundation for more specialized endeavors. Current practice involves taking the full model that has amassed such general knowledge and fine-tuning it with a second objective appropriate to the new task \citep[see][for an overview]{raffelExploringLimitsTransfer2019}. Using these methods, pre-trained transformer-based language models \citep[e.g., BERT, ][]{devlin-etal-2019-bert} have been employed to great effect on a wide variety of NLP problems, thanks, in part, to a fine-grained ability to capture aspects of linguistic context .      However, this paradigm introduces a subtle but insidious limitation that becomes evident when the downstream application is a topic model. A topic model may be cast as a  autoencoder , and we could fine-tune a pretrained transformer with an identical document reconstruction objective. But in replacing the original topic model, we lose the property that makes it desirable: its interpretability. The transformer gains its contextual power from its ability to exploit a huge number of parameters, while the interpretability of a topic model comes from a dramatic dimensionality reduction.      We combine the advantages of these two approaches---the rich contextual language knowledge in pretrained transformers and the intelligibility of topic models---using knowledge distillation . In the original formulation, knowledge distillation involves training a parameter-rich teacher classifier on large swaths of data, then using its high-quality probability estimates over outputs to guide a smaller student model. Since the information contained in these estimates is useful---a picture of an ox will yield higher label probabilities for buffalo than apricot---the student needs less data to train and can generalize better.  We show how this principle can apply equally well to improve unsupervised topic modeling, which to our knowledge has not previously been attempted.  While distillation usually involves two models of the same type, it can also apply to models of differing architectures. Our method is conceptually quite straightforward: we fine-tune a pretrained transformer  on a document reconstruction objective, where it acts in the capacity of an autoencoder. When a document is passed through this BERT autoencoder, it generates a distribution over words that includes unobserved but related terms. We then incorporate this distilled document representation into the loss function for topic model estimation.    To connect this method to the more standard supervised knowledge distillation, observe that the unsupervised ``task'' for both an autoencoder and a topic model is the reconstruction of the original document, i.e. prediction of a distribution over the vocabulary. The BERT autoencoder, as ``teacher'', provides a dense prediction that is richly informed by training on a large corpus. The topic model, as ``student'', is generating its own prediction of that distribution. We use the former to guide the latter, essentially as if predicting word distributions were a multi-class labeling problem. \newcommand{\reffig}[1]{\hl{[FIG: #1]}} \newcommand{\reftable}[1]{\hl{[TABLE: #1]}} \newcommand{\refsec}[1]{\hl{[SECTION: #1]}} \newcommand{\ho}[1]{\textcolor{blue}{}} \newcommand{\pg}[1]{\textcolor{red}{}} \newcommand{\psrcomment}[1]{}  \newcommand{\ignore}[1]{} \newcommand{\ourmodel}{BAT }   \newcommand{\e}[2]{\mathbb{E}_{#1}\left[ #2 \right] } \newcommand{\B}{B} \DeclareMathOperator*{\argmin}{arg\,min}  \aclfinalcopy %   \newcommand\BibTeX{Bib\TeX}  \title{Improving Neural Topic Models using Knowledge Distillation}  \author{Alexander Hoyle\thanks{\, Equal contribution.} \\   Computer Science \\   University of Maryland \\   College Park, MD \\    \\\And   Pranav Goel\footnotemark[1] \\   Computer Science \\   University of Maryland \\   College Park, MD \\    \\\And   Philip Resnik \\   Linguistics / UMIACS \\   University of Maryland \\   College Park, MD \\    \\}  \date{}  \begin{document}                         \bibliography{anthology,refs,zotero} \bibliographystyle{acl_natbib}  \clearpage \appendix        \paragraph{Integrating embeddings into topic models.} A key goal in our use of knowledge distillation is to incorporate relationships between words  that may not be well supported by the topic model's input documents alone. Some previous topic models have sought to address this issue by incorporating external word information, including word senses  and pretrained word embeddings . More recently, \citet{bianchiPretrainingHotTopic2020} have incorporated BERT embeddings into the encoder to improve topic coherence.  We refer the reader to  for an extensive and up-to-date overview.\looseness=-1  A limitation of these approaches is that they simply import general, non-corpus-specific word-level information. In contrast, representations from a pretrained transformer can benefit from both general language knowledge and corpus-dependent information, by way of the pretraining and fine-tuning regime. By regularizing toward representations conditioned on the document, we remain coherent relative to the topic model data.  An additional key advantage for our method is that it involves only a slight change to the underlying topic model, rather than the specialized designs by the above methods.   \paragraph{Knowledge distillation.} While the focus was originally on single-label image classification, KD has also been extended to the multi-label setting . In NLP, KD has usually been applied in supervised settings , but also in some unsupervised tasks  . \citet{xuDistilledWassersteinLearning2018} use word embeddings jointly learned with a topic model in a procedure they term distillation, but do not follow the method from \citet{hintonDistillingKnowledgeNeural2015} that we employ . Recently, pretrained models like BERT have offered an attractive choice of teacher model, used successfully for a variety of tasks such as sentiment classification and paraphrasing . Work in distillation often cites a reduction in computational cost as a goal \cite[e.g.,][]{sanhDistilBERTDistilledVersion2019}, although we are aware of at least one effort that is focused specifically on interpretability .\looseness=-1      \paragraph{Topic diversity.} Coherence, commonly quantified automatically using NPMI, is the current standard for evaluating topic model quality. Recently several authors  have proposed additional metrics focused on the diversity or uniqueness of topics . However, no one metric has yet achieved acceptance or consensus in the literature. Moreover, such measures fail to distinguish between the case where two topics share the same set of top  words, therefore coming across as essentially identical, versus when one topic's top  words are repeated individually across multiple other topics, indicating a weaker and more diffuse similarity to those topics. We discuss issues related to topic diversity in \cref{appendix:tu}.             }         \caption{The NPMI for our baselines  compared with \ourmodel  using  as our base neural architecture. We achieve better NPMI than all baselines across three datasets and  topics.                   We use 5 random restarts and report the standard deviation.         }               In natural languages, speakers routinely select one alternative over others to express their intended message.   These choices are sensitive to many interacting factors, including the choice of the main verb and the length and definiteness of arguments.  Our new dataset, DAIS, not only offers a higher-resolution window into the richness of human preferences, it also provides a newly powerful benchmark for evaluating and understanding the corresponding sensitivity of language models. We found that transformer architectures corresponded especially well with human verb bias judgments.  Further work is needed to more precisely determine the source of the architectural differences we observed. One possibility is that the transformer's self-attention mechanism and layer-wise organization improves its ability to represent lexically-specific structures.  However, it is also possible that differences are attributable to training data. Another line of future research is to compare the incremental predictions of neural models to finer-grained eye-tracking evidence during sentence processing of double-object sentences \cite[e.g.][]{filik2004processing}. As neural language models become more complex, subtler phenomena like verb bias may yield new insights into how lexical and grammatical representations are jointly learned and successfully integrated for language understanding.   
","      \paragraph{Integrating embeddings into topic models.} A key goal in our use of knowledge distillation is to incorporate relationships between words  that may not be well supported by the topic model's input documents alone. Some previous topic models have sought to address this issue by incorporating external word information, including word senses  and pretrained word embeddings . More recently, \citet{bianchiPretrainingHotTopic2020} have incorporated BERT embeddings into the encoder to improve topic coherence.  We refer the reader to  for an extensive and up-to-date overview.\looseness=-1  A limitation of these approaches is that they simply import general, non-corpus-specific word-level information. In contrast, representations from a pretrained transformer can benefit from both general language knowledge and corpus-dependent information, by way of the pretraining and fine-tuning regime. By regularizing toward representations conditioned on the document, we remain coherent relative to the topic model data.  An additional key advantage for our method is that it involves only a slight change to the underlying topic model, rather than the specialized designs by the above methods.   \paragraph{Knowledge distillation.} While the focus was originally on single-label image classification, KD has also been extended to the multi-label setting . In NLP, KD has usually been applied in supervised settings , but also in some unsupervised tasks  . \citet{xuDistilledWassersteinLearning2018} use word embeddings jointly learned with a topic model in a procedure they term distillation, but do not follow the method from \citet{hintonDistillingKnowledgeNeural2015} that we employ . Recently, pretrained models like BERT have offered an attractive choice of teacher model, used successfully for a variety of tasks such as sentiment classification and paraphrasing . Work in distillation often cites a reduction in computational cost as a goal \cite[e.g.,][]{sanhDistilBERTDistilledVersion2019}, although we are aware of at least one effort that is focused specifically on interpretability .\looseness=-1      \paragraph{Topic diversity.} Coherence, commonly quantified automatically using NPMI, is the current standard for evaluating topic model quality. Recently several authors  have proposed additional metrics focused on the diversity or uniqueness of topics . However, no one metric has yet achieved acceptance or consensus in the literature. Moreover, such measures fail to distinguish between the case where two topics share the same set of top  words, therefore coming across as essentially identical, versus when one topic's top  words are repeated individually across multiple other topics, indicating a weaker and more diffuse similarity to those topics. We discuss issues related to topic diversity in \cref{appendix:tu}.             }         \caption{The NPMI for our baselines  compared with \ourmodel  using  as our base neural architecture. We achieve better NPMI than all baselines across three datasets and  topics.                   We use 5 random restarts and report the standard deviation.         }",43
"   Recent advances in self-supervised pre-training have resulted in impressive downstream performance on several NLP tasks. However, this has led to the development of enormous models, which often require days of training on non-commodity hardware . Furthermore, studies have shown that it is quite challenging to successfully train these large Transformer models, requiring complicated learning schemes and extensive hyperparameter tuning.  Despite these expensive training regimes, recent studies have found that once trained, these bi-directional language models exhibit simple patterns of self-attention without much linguistic backing. For example, 40\% of heads in a pre-trained BERT model simply pay attention to delimiters added by the tokenizer . Since these attention patterns are independent of linguistic phenomena, a natural question arises: can Transformer models be guided towards such attention patterns without requiring extensive training?    In this paper, we propose an attention guidance  mechanism for self-attention modules in Transformer architectures to enable faster, more efficient, and robust self-supervised learning. Our approach is simple and agnostic to the training objective. Specifically, we introduce an auxiliary loss function to guide the self-attention heads in each layer towards a set of pre-determined patterns . These patterns encourage the formation of both  global  and local  structures in the model.   Through several experiments, we show that our approach enables training large Transformer models considerably faster 閳 for example, we can train a 16-layer RoBERTa model with SOTA performance on a low-resource domain in just two days using four GPUs, while excluding our loss leads to slow or no convergence. Our method also achieves competitive performance with BERT on three English natural language understanding tasks, and outperforms the baseline masked language modeling  models on eleven out of twelve settings considered.  Further, we also show that our initialization is agnostic to the training objective by demonstrating gains on the replaced token detection objective proposed by ELECTRA and on machine translation with Transformers. Finally, we provide an analysis of the attention heads learned using our method. Surprisingly, contrary to recent studies, we find that it is possible to train models that perform well on language modeling without learning a single attention head that models coreferences. % . For example, our model fails the co-reference test in  while still performing well on language modeling and downstream tasks.  To summarize, our main contributions are:      \paragraph{Transformers for language modeling}  Since OpenAI GPT  and BERT  popularized the Transformer for self-supervised learning from raw text, several studies have critically evaluated each component of these models and proposed variants like GPT-2 , RoBERTa, XLNet, and T5. In other work, \citet{sun2019ernie} proposed a lifelong multi-task learning setup  with a focus on language-understanding on downstream tasks. \citet{lan2019albert}  proposed ALBERT -- a variant of BERT with weights shared across all layers.   \paragraph{Improving efficiency of LMs} The high computational costs of BERT-style models have accelerated research on developing efficient contextual language models. \citet{clark2020electra} used a GAN-like setup to predict if each word in the input sequence is corrupted by a generator . They show that their method is more sample efficient than the standard MLM objective. Other studies have explicitly focused on making the self-attention modules more efficient. Reformer and Sparse Transformer introduce locality-sensitive hashing and sparse factorizations to reduce the quadratic complexity of dot-product attention, while Longformer uses local-windowed and task motivated global attention to scale the memory usage of self-attention modules linearly.   TinyBERT uses a two-stage distillation framework to learn from a BERT ``teacher"" model. DistilBERT uses knowledge distillation during pre-training and retains a large fraction of BERT's performance.   \paragraph{Analyzing Self-Attention} Recent papers have analyzed the attention patterns in trained Transformer-based LMs. Some studies hypothesize that multiple attention heads capture linguistic phenomena like co-reference links and dependency arcs. However, other studies show that pruning those heads leads to minimal performance degradation on downstream tasks. Others note that there are recurring patterns in attention distributions corresponding to different attention heads , which are not language or task-dependent. While our study also questions the role of heads for language modeling and downstream performance, we focus on making modifications to the LM pre-training and not on analyzing published pre-trained models.     This points to 2 things -- 1) there is a lot of redundancy in the attention patterns in Transformer-based LMs, and 2) the linguistic capabilities of self-attention heads is not a given even if the LM achieves very good perplexity scores.        a pre-trained transformer's self-attention . , , , and  note that there are recurring patterns in attention distributions corresponding to different attention heads  which are not language or task dependent. While  and  hypothesize and show that there are multiple attention heads which could be performing linguistically motivated tasks like co-reference resolution and subsets of dependency parsing,  note that pruning those heads leads to no performance degradation on downstream tasks.  use greedy algorithms to prune a large number of heads while suffering very little deterioration. This redundancy in attention is exploited in BlockBERT  and Longformer  where attention is paid mainly to a local context only. However, these implementations need significant changes to the model.  \paragraph{Constraining Self-Attention} \citet{qiu2019blockwise} enforce local constraints on the attention patterns to reduce computation and build deeper models with longer contexts.   . The studies that are perhaps most similar to ours explore fixed attention patterns for machine translation. \citet{you2020hard} replace all attention heads in the encoder with hard-coded Gaussian distributions centered around the position of each token while observing a minimal reduction in BLEU scores. \citet{raganato2020fixed} substitute all but one head with fixed attention patterns in each encoder layer and note little performance degradation. Both these studies enforce hard constraints on the self-attention and try to match baselines in terms of speed and performance. Our approach is complementary -- our attention guidance loss is a form of soft regularization and outperforms baseline models both in terms of convergence speed and quantitative metrics.    \paragraph{Papers analyzing BERT attention}     \paragraph{BERT in different languages}   - Have similar attention patterns    - Can be finetuned even on English tasks    \paragraph{Case for Hard attention}   - REFORMER   - Longformer   - BlockBERT    \paragraph{Other Attention Analysis Studies}   Make the case that attention need not correspond to respective tokens in any layer other than the first layer. Either cite other papers, or run experiments to show this. Also cite Clark's paper and ``Dark Secrets"" paper    \paragraph{Evidence for hypothesis}       \paragraph{Redundancy in BERT}      We formulate the general IF game playing as MPRC tasks, enabling an MPRC-style solution to efficiently address the key IF game challenges on the huge combinatorial action space and the partial observability in a unified framework.  Our approaches achieved significant improvement over the previous state-of-the-art on both game scores and training data efficiency. Our formulation also bridges broader NLU/RC techniques to address other critical challenges in IF games for future work, e.g., common-sense reasoning, novelty-driven exploration, and multi-hop inference.    
","    \paragraph{Transformers for language modeling}  Since OpenAI GPT  and BERT  popularized the Transformer for self-supervised learning from raw text, several studies have critically evaluated each component of these models and proposed variants like GPT-2 , RoBERTa, XLNet, and T5. In other work, \citet{sun2019ernie} proposed a lifelong multi-task learning setup  with a focus on language-understanding on downstream tasks. \citet{lan2019albert}  proposed ALBERT -- a variant of BERT with weights shared across all layers.   \paragraph{Improving efficiency of LMs} The high computational costs of BERT-style models have accelerated research on developing efficient contextual language models. \citet{clark2020electra} used a GAN-like setup to predict if each word in the input sequence is corrupted by a generator . They show that their method is more sample efficient than the standard MLM objective. Other studies have explicitly focused on making the self-attention modules more efficient. Reformer and Sparse Transformer introduce locality-sensitive hashing and sparse factorizations to reduce the quadratic complexity of dot-product attention, while Longformer uses local-windowed and task motivated global attention to scale the memory usage of self-attention modules linearly.   TinyBERT uses a two-stage distillation framework to learn from a BERT ``teacher"" model. DistilBERT uses knowledge distillation during pre-training and retains a large fraction of BERT's performance.   \paragraph{Analyzing Self-Attention} Recent papers have analyzed the attention patterns in trained Transformer-based LMs. Some studies hypothesize that multiple attention heads capture linguistic phenomena like co-reference links and dependency arcs. However, other studies show that pruning those heads leads to minimal performance degradation on downstream tasks. Others note that there are recurring patterns in attention distributions corresponding to different attention heads , which are not language or task-dependent. While our study also questions the role of heads for language modeling and downstream performance, we focus on making modifications to the LM pre-training and not on analyzing published pre-trained models.     This points to 2 things -- 1) there is a lot of redundancy in the attention patterns in Transformer-based LMs, and 2) the linguistic capabilities of self-attention heads is not a given even if the LM achieves very good perplexity scores.        a pre-trained transformer's self-attention . , , , and  note that there are recurring patterns in attention distributions corresponding to different attention heads  which are not language or task dependent. While  and  hypothesize and show that there are multiple attention heads which could be performing linguistically motivated tasks like co-reference resolution and subsets of dependency parsing,  note that pruning those heads leads to no performance degradation on downstream tasks.  use greedy algorithms to prune a large number of heads while suffering very little deterioration. This redundancy in attention is exploited in BlockBERT  and Longformer  where attention is paid mainly to a local context only. However, these implementations need significant changes to the model.  \paragraph{Constraining Self-Attention} \citet{qiu2019blockwise} enforce local constraints on the attention patterns to reduce computation and build deeper models with longer contexts.   . The studies that are perhaps most similar to ours explore fixed attention patterns for machine translation. \citet{you2020hard} replace all attention heads in the encoder with hard-coded Gaussian distributions centered around the position of each token while observing a minimal reduction in BLEU scores. \citet{raganato2020fixed} substitute all but one head with fixed attention patterns in each encoder layer and note little performance degradation. Both these studies enforce hard constraints on the self-attention and try to match baselines in terms of speed and performance. Our approach is complementary -- our attention guidance loss is a form of soft regularization and outperforms baseline models both in terms of convergence speed and quantitative metrics.    \paragraph{Papers analyzing BERT attention}     \paragraph{BERT in different languages}   - Have similar attention patterns    - Can be finetuned even on English tasks    \paragraph{Case for Hard attention}   - REFORMER   - Longformer   - BlockBERT    \paragraph{Other Attention Analysis Studies}   Make the case that attention need not correspond to respective tokens in any layer other than the first layer. Either cite other papers, or run experiments to show this. Also cite Clark's paper and ``Dark Secrets"" paper    \paragraph{Evidence for hypothesis}       \paragraph{Redundancy in BERT}",44
" %  % Transformer models  have outperformed previously used RNN based models and traditional statistical MT techniques.  This improvement, though, comes at the cost of higher computation complexity. The decoder computation is sequential and becomes the bottleneck due to the autoregressive nature, large depth and self-attention structure.   % Another recent trend has been making the models larger and ensembling multiple models to achieve the best possible translation quality . Leading solutions on common benchmark  usually use an ensemble of Transformer big models, which combined can have more than 1 billion parameters.   % In this paper, we focus on developing architectures which are faster during inference and have less number of parameters, without sacrificing translation quality.  % Recent work \citet{ludicrously:kim2019} proposed methods to replace self-attention in the decoder with simpler simple recurrent units  and used knowledge distillation to simplify training for the final architecture. \citet{deepencoder} also proposed to make the decoder lightweight by training a deep-encoder, shallow decoder architecture. Another line of effort to make NMT architectures more efficient is pruning different components of the model. \citet{prune_voita-etal-2019-analyzing} and \citet{prune_michel:NIPS2019_9551} show that most of the attention heads in the network learn redundant information and can be pruned away.  % All of the above works use the vanilla Transformer architecture as their baseline, so it is not clear if these approaches can give complimentary results when combined together. In this work, we explore and benchmark combining all of the above techniques, with the goal of maximizing inference speed without hurting in translation quality. % %We adapt the same approach and  extend it with the following ideas. First, we optimized the SSRU to make it more efficient. Second, we removed the feed-forward network in the decoder completely. Then, we kept only 1 layer in the decoder and used very deep encoder. Last we pruned all the redundant heads in the deep encoder.  % After carefully stacking the approaches, our proposed architecture is able to achieve a significant speed improvement of 84\% on GPU and 102\% on CPU architectures without any degradation of translation quality in terms of BLEU.  % %%%%%%%% original Related Work %%%%%%%%% %        There is a large body of work related to the optimization of various parts of Transformer architecture.   \citet{AAN-zhang-Etal:2018} proposed the idea of the average attention network  as an alternative to the self-attention network. Instead of computing dynamic attention weights over all previous states, AAN places equal attention on all previously decoded words via averaged weights.      places equal attention on all previous decoded words     treats each previous hidden state equally.      Two-layer structure is introduced in original paper, an average attention layer, and a gating layer.    \citet{SRU:lei-etal-2018-simple} proposed a lightweight recurrent unit, Simple Recurrent Unit , to speed up the network. \citet{ludicrously:kim2019} proposed Simpler Simple Recurrent Unit , a simpler version of SRU, which consists of only two matrix multiplications.    Because of the autoregressive property of the decoder in a standard Transformer model during the inference, reducing computation cost in the decoder is much more important than in the encoder. Recent publications  have suggested the structure of deep encoder, shallow decoder can speed up inference while maintaining a similar BLEU score.     proposed a lightweight Transformer, DeLighT, which has about less than half parameters and operations but still achieves similar translation quality as Transformer-based models. Previous works also claimed that quantization of the model can provide computational speed improvement at inference time .     Model pruning techniques have been proposed to compress the model and reduce the computation cost. There are structured pruning and unstructured pruning methods. Structured pruning methods  prune the filters or even layers. On the other hand, unstructured pruning methods  prune the redundant weights, which result in sparse matrices.      In this paper, we explore only structured pruning methods. In the Transformer model, multi-head attention is important; however, not all the heads are necessary~. \citet{prune_michel:NIPS2019_9551} proposed the idea of pruning head through head importance score. \citet{prune_voita-etal-2019-analyzing} integrates the  regularization technique to prune the attention heads.  Note that, both of these two methods require a trained model.      There is only one layer, which consists of one SSRU and one encoder-decoder attention in decoder. At last, we prune 82\  of attention heads of a simplified Transformer, while the others pruned attention heads on the original Transformer.        The Transformer-based  model has been state-of-the-art machine translation architecture. Stacking multi-head attention and feed-forward network is the key structure of the Transformer. In order to achieve a higher translation performance, it grows deep and wide. Therefore, the computation requirement also exponentially grows. However, it puts significant pressure on the efficiency of the model. Efficiency is even more important during the inference.                                                       In this section, we investigate two questions: 1) why We project token-level representations obtained from the BERT embedders onto a 2-dimensional space using t-SNE. \autoref{fig:tsne} presents the visualization results on the CoNLL and WNUT test sets .  beyond optimal visual effect)  Fine-tuning BERT on OntoNotes clearly improves the task-awareness with respect to both CoNLL and WNUT datasets, as instances of the same class are much closer compared to those obtained from the non-fine-tuned BERT model. The separation of different entity classes is more evident on CoNLL due to the greater tag set overlap with OntoNotes.   Instances labeled with \nertag{O} are spread across the space, regardless of fine-tuning.   This explains the effectiveness of \sysname. First, fine-tuning BERT in a conventional NER setting is able to learn a good entity specific metric space. Second, the nearest neighbor classifier that emphasizes more on local distance is more appropriate for assigning \nertag{O} to an instance.    , \nertag{c-work}, and \nertag{corp.} correspond \nertag{MEDICAL-RECORD}, \nertag{location}, \nertag{creative-work}, and \nertag{corporation} respectively.}    \paragraph{Per-class performance analysis} We attempt to shed some light on the second question by analyzing outputs from the best five-shot \sysname~systems on the domain transfer task. The per-class F1 scores are shown in, where we exclude I2B2 classes with less than 200 instances in the test set. \sysname~achieves reasonable performance on less ambiguous entity classes such as \nertag{DATE}, \nertag{CITE}, \nertag{person}, and \nertag{location}. However, it struggles to distinguish between highly ambiguous classes. For example, \nertag{AGE}, \nertag{MEDICAL-RECORD}, \nertag{PHONE}, and \nertag{IDNUM} are all numbers. It is still challenging for our system to differentiate different numerical types without any domain specific knowledge. Similarly, \sysname~often predicts a \nertag{PATIENT} entity as \nertag{DOCTOR} and it nearly always assigns the \nertag{corporation} label to entities of \nertag{group}. We believe that domain specific cues like `Dr.' and `MD.' can be useful in resolving these ambiguities and enable few-shot NER systems to generalize better.  beyond the few support examples.    We focus on the typical errors made on the I2B2 and WNUT test sets, as the system performance on CoNLL is very solid. \section{Experiments}   In this section, we compare \sysname~against existing methods on two few-shot NER scenarios: tag set extension and domain transfer. We adopt several benchmark NER corpora in different domains for the few-shot experiments. \let\argmin\relax \let\argmax\relax \DeclareMathOperator*{\argmin}{arg\,min} \DeclareMathOperator*{\argmax}{arg\,max} \newcommand{\example}[1]{`#1'} \newcommand{\nertag}[1]{\texttt{#1}} \newcommand{\sysname}{}  \newcommand{\arzoo}[1]{[{Arzoo: {#1}}]}    --------------------------------------------  \aclfinalcopy   Uncomment this line for the final submission \def\aclpaperid{2799}    Enter the acl Paper ID here  \setlength\titlebox{5cm}      Expanding the titlebox \title{Simple and Effective Few-Shot Named Entity Recognition\\ with Structured Nearest Neighbor Learning}  \author{Yi Yang \\ 	ASAPP Inc.\\ 	New York, NY 10007\\ 	yyang@asapp.com \\\And  	Arzoo Katiyar \thanks{ \hspace{0.15cm}Work done at ASAPP Inc.} \\ 	Pennsylvania State University \\ 	University Park, PA 16802\\ 	arzoo@psu.edu     }                             \date{}      ***********************************************************   Introduction     ***********************************************************   Problem     ***********************************************************   Model     ***********************************************************   Experiment     ***********************************************************   Discussion     ***********************************************************   Related Work     ***********************************************************   Conclusion and Future Work     ***********************************************************   Acknowledgments     ***********************************************************       ***********************************************************   Appendix   
","      There is a large body of work related to the optimization of various parts of Transformer architecture.   \citet{AAN-zhang-Etal:2018} proposed the idea of the average attention network  as an alternative to the self-attention network. Instead of computing dynamic attention weights over all previous states, AAN places equal attention on all previously decoded words via averaged weights.      places equal attention on all previous decoded words     treats each previous hidden state equally.      Two-layer structure is introduced in original paper, an average attention layer, and a gating layer.    \citet{SRU:lei-etal-2018-simple} proposed a lightweight recurrent unit, Simple Recurrent Unit , to speed up the network. \citet{ludicrously:kim2019} proposed Simpler Simple Recurrent Unit , a simpler version of SRU, which consists of only two matrix multiplications.    Because of the autoregressive property of the decoder in a standard Transformer model during the inference, reducing computation cost in the decoder is much more important than in the encoder. Recent publications  have suggested the structure of deep encoder, shallow decoder can speed up inference while maintaining a similar BLEU score.     proposed a lightweight Transformer, DeLighT, which has about less than half parameters and operations but still achieves similar translation quality as Transformer-based models. Previous works also claimed that quantization of the model can provide computational speed improvement at inference time .     Model pruning techniques have been proposed to compress the model and reduce the computation cost. There are structured pruning and unstructured pruning methods. Structured pruning methods  prune the filters or even layers. On the other hand, unstructured pruning methods  prune the redundant weights, which result in sparse matrices.      In this paper, we explore only structured pruning methods. In the Transformer model, multi-head attention is important; however, not all the heads are necessary~. \citet{prune_michel:NIPS2019_9551} proposed the idea of pruning head through head importance score. \citet{prune_voita-etal-2019-analyzing} integrates the  regularization technique to prune the attention heads.  Note that, both of these two methods require a trained model.      There is only one layer, which consists of one SSRU and one encoder-decoder attention in decoder. At last, we prune 82\  of attention heads of a simplified Transformer, while the others pruned attention heads on the original Transformer.        The Transformer-based  model has been state-of-the-art machine translation architecture. Stacking multi-head attention and feed-forward network is the key structure of the Transformer. In order to achieve a higher translation performance, it grows deep and wide. Therefore, the computation requirement also exponentially grows. However, it puts significant pressure on the efficiency of the model. Efficiency is even more important during the inference.",45
" Intent Detection  is a crucial task in natural language understanding, whose objective is to extract underlying intents behind the given utterances. The extracted intents could provide further contexts for further downstream Natural Language Processing tasks such as dialogue state tracking or question answering. Unlike traditional text classification, ID is challenging for two main reasons  Utterances are usually short and diversely expressed,  Emerging intents occur continuously, especially across different domains .  Despite recent advances, state-of-the-art ID methods  require a large amount of annotated data to achieve competitive performance. This requirement inhibits models' capability in generalizing to newly emerging intents with no or limited annotations during inference. Re-training or fine-tuning large models on few samples of emerging classes could easily lead to overfitting problems.      Motivated by human capability in correctly categorizing new classes with only a few examples , few-shot learning  paradigms are adopted to tackle the scarcity problems of emerging classes. FSL methods take advantage of a small set of labeled examples  to learn how to discriminate unlabeled samples  between classes, even those not seen during training.  Recent works in FSL  focus on learning the matching information between the labeled samples  and the unlabeled samples  to provide additional contextual information for instance-level representations, leading to effective prototype representation. However, these methods only extract similarity based on fine-grained word semantics, failing to capture the diverse expressions of users' utterances. This problem could further lead to overfitting either to seen intents or novel intents, especially in the challenging Generalized Few-shot Intent Detection  setting  where both seen and novel intents are existent in a joint label space during inference. Instead, matching support and query samples on coarser-grained semantic components could provide additional informative contexts beyond word levels. For instance, two utterances ""i need to get a table at a pub with southeastern cuisine"" and ``book a spot for six friends"" share a similar intent label ``Book Restaurant"". While word-level semantics might find similar action words as ``get"" and ``book"", these words do not necessarily contribute to the correct intent findings. Instead, coarser-grained semantics such as ``get a table"" and ``book a spot"" could provide further hints to identify ``Book Restaurant"" intent.      As semantic components  could be effectively extracted from multi-head self-attention, matching these SC between support and query can enhance both query and support representations, leading to improvements in generalization from seen training classes to unseen testing classes. To further enhance the dynamics of extracted SC across various domains and diversely expressed utterances, we introduce additional head regularizations. In addition, to overcome the insufficiency of a single similarity measure for matching sentences with diverse semantics, a more comprehensive matching method is further explored.      Our main contribution is summarized as follows:      \paragraph{Few-shot Learning} Few-shot learning refers to problems where classifiers are required to generalize to unseen classes with only a few training examples per class . To overcome challenges of potential overfitting, most FSL methods adopt meta-learning approach where knowledge is extracted and transferred across multiple tasks. There are two major approaches towards FSL:  metric-based approach whose goal is to learn feature extractor that extract and generalize to emerging classes , and  optimization-based approach that aims to optimize model parameters from few samples . In this work, we focus mostly on metric-based learning approach. Specifically, we extend Prototypical Network   in which prototypes are not only represented by support samples but also matching information between support and query samples.  Traditionally, FSL methods are evaluated in episodic procedure due to the major principle that test and train conditions must match . Each episode represents a meta-learning task in which the models explicitly ``learn to learn"" minimize the loss on an unlabeled/ query set given the support/ labeled set. However, we claim that this evaluation is lack of practicality for two main reasons. First, evaluation on random samples could not help us understand the strengths or weaknesses of the model. For instance, if the trained model overfits a subset of novel classes, it is  impossible to pinpoint the overfitting classes with episodic evaluation. Secondly, in realistic applications, there is a need to categorize unlabeled samples into one of the novel/joint classes, rather than a set of sampled classes. Episodic  testing  does not provide an end-to-end systematic evaluation. Therefore, in our work, we propose a more challenging but realistic non-episodic evaluation setting where unlabeled samples are only inferred once with a probablility distribution over a fixed set of classes in novel or joint label space.  \paragraph{Sentence Matching} Recent FSL works adopt multi-level matching and aggregation methods to improve FSL performance . Instead of constructing prototypes purely from support samples, recent works integrate matching information between support and query samples on multiple levels. \citet{gao2019hybrid} introduces feature-level and instance-level attention. \citet{sun-etal-2019-hierarchical} introduces additional word-level attention and proposes more advanced multi-cross attention on instance-level. On the other hand, \citet{ye-ling-2019-multi} adopts soft matching between support and query samples to build local context representation for both support and query samples. These methods have been proven effective in few-shot relation classification tasks. However, they rely on overly fine-grained level matching which potentially causes overfitting problems towards either seen or unseen set of classes. Our work mainly differs in two aspects:  Comprehensive multi-perspective matching for information matching and  Matching on coarser-grained semantic-component levels that are extracted dynamically for effective knowledge transfer, especially in GFSL settings.       In this paper we explored the combination of techniques aimed at improving inference speed which lead to the discovery of a very efficient architecture. The best architecture has a deep -layer encoder, and a shallow decoder with only one single lightweight recurrent unit layer and one encoder-decoder attention mechanism. \  of the encoder heads were pruned giving rise to a model with \  fewer parameters than the baseline Transformer. In terms of inference speed, the proposed architecture is \  faster on a GPU, and \  faster on a CPU.   In the future, we plan to investigate pruning the feed-forward network in the encoder, and explore application of the lottery ticket hypothesis.        In this paper,     we have investigated various approaches of simplifying Transformer model to speed up the inference and successfully combine multiple techniques. To be more specific,    we achieve the very efficient inference architecture, which consists of only one lightweight recurrent unit layer and one encoder-decoder attention mechanism in the decoder. With the head pruning method, only 18\  of attention heads are required in the deep encoder and shallow decoder architecture. This model has 13\  fewer parameters, and during the inference stage, it is 84\  and 102\  faster than baseline on GPU and CPU, respectively.    In the future, we plan to prune the feed-forward network in the encoder and explore the combination with the lottery ticket hypothesis.      In the future, we plan to investigate more different approaches and build a mroe efficient inference architecture for machine translation.     we plan to prune the feed-forward neurons and apply the unstructured pruning techniques to remove weights in the whole model.  
","  \paragraph{Few-shot Learning} Few-shot learning refers to problems where classifiers are required to generalize to unseen classes with only a few training examples per class . To overcome challenges of potential overfitting, most FSL methods adopt meta-learning approach where knowledge is extracted and transferred across multiple tasks. There are two major approaches towards FSL:  metric-based approach whose goal is to learn feature extractor that extract and generalize to emerging classes , and  optimization-based approach that aims to optimize model parameters from few samples . In this work, we focus mostly on metric-based learning approach. Specifically, we extend Prototypical Network   in which prototypes are not only represented by support samples but also matching information between support and query samples.  Traditionally, FSL methods are evaluated in episodic procedure due to the major principle that test and train conditions must match . Each episode represents a meta-learning task in which the models explicitly ``learn to learn"" minimize the loss on an unlabeled/ query set given the support/ labeled set. However, we claim that this evaluation is lack of practicality for two main reasons. First, evaluation on random samples could not help us understand the strengths or weaknesses of the model. For instance, if the trained model overfits a subset of novel classes, it is  impossible to pinpoint the overfitting classes with episodic evaluation. Secondly, in realistic applications, there is a need to categorize unlabeled samples into one of the novel/joint classes, rather than a set of sampled classes. Episodic  testing  does not provide an end-to-end systematic evaluation. Therefore, in our work, we propose a more challenging but realistic non-episodic evaluation setting where unlabeled samples are only inferred once with a probablility distribution over a fixed set of classes in novel or joint label space.  \paragraph{Sentence Matching} Recent FSL works adopt multi-level matching and aggregation methods to improve FSL performance . Instead of constructing prototypes purely from support samples, recent works integrate matching information between support and query samples on multiple levels. \citet{gao2019hybrid} introduces feature-level and instance-level attention. \citet{sun-etal-2019-hierarchical} introduces additional word-level attention and proposes more advanced multi-cross attention on instance-level. On the other hand, \citet{ye-ling-2019-multi} adopts soft matching between support and query samples to build local context representation for both support and query samples. These methods have been proven effective in few-shot relation classification tasks. However, they rely on overly fine-grained level matching which potentially causes overfitting problems towards either seen or unseen set of classes. Our work mainly differs in two aspects:  Comprehensive multi-perspective matching for information matching and  Matching on coarser-grained semantic-component levels that are extracted dynamically for effective knowledge transfer, especially in GFSL settings.",46
"  Neural machine translation  is a data-hungry approach, which requires a large amount of data to train a well-performing NMT model. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult.  To relieve this problem, several approaches have been proposed to better exploit the training data, such as curriculum learning, data diversification, and data denoising.  In this paper, we explore an interesting alternative which is to reactivate the inactive examples in the  training data for NMT models. By definition, inactive examples are the training examples that only marginally contribute to or even inversely harm the performance of NMT models.  Concretely, we use sentence-level output probability assigned by a trained NMT model to measure the activeness level of training examples, and regard the examples with the least probabilities as inactive examples . Experimental results show that removing 10\% most inactive examples can marginally improve translation performance. In addition, we observe a high overlapping ratio  of the most inactive and active examples across random seeds, model capacity, and model architectures . These results provide empirical support for our hypothesis of the existence of inactive examples in large-scale datasets, which is invariant to specific NMT models and depends on the data distribution itself.  We further propose data rejuvenation to rejuvenate the inactive examples to improve the performance of NMT models.  Specifically, we train an NMT model on the active examples as the rejuvenation model to re-label the inactive examples, resulting in the rejuvenated examples~. The final NMT model is trained on the combination of the active examples and rejuvenated examples. Experimental results show that the data rejuvenation approach consistently and significantly improves performance on SOTA NMT models  on the benchmark WMT14 English-German and English-French datasets~. Encouragingly, our approach is also complementary to existing data manipulation methods , and combining them can further improve performance.      Finally, we conduct extensive analyses to better understand the inactive examples and the proposed data rejuvenation approach. Quantitative analyses reveal that the inactive examples are more difficult to learn than active ones, and rejuvenation can reduce the learning difficulty~. The rejuvenated examples stabilize and accelerate the training process of NMT models~, resulting in final models with better generalization capability~.  Our contributions of this work are as follows:          \paragraph{Data Manipulation.} Our work is closely related to previous studies on manipulating training data for NMT models, which focuses on exploiting the original training data without augmenting additional data. For example, the data denoising approach aims to identify and clean the noise training examples. Data diversification tries to diversify the training data by applying forward-translation to the source side of the parallel data, or back-translation to the target side of parallel data in a reverse translation direction. Our approach is complementary to theirs, and using them together can further improve translation performance .  Another distantly related direction is to simplify the source sentences so that a black-box machine translation system can better translate them, which is out of scope in this work.  \paragraph{Distinguishing Training Examples.} Our work is also related to previous work on distinguishing training examples in machine learning.   For example, self-paced learning emphasizes the learning on examples with lower loss  in early training, which is robust to outliers or noisy labels. Hard example mining aims to accelerate the training process by hard examples with higher loss. Active learning combines the advantages of both self-paced learning and hard example mining by emphasizing high variance examples. Especially, curriculum learning has been applied to the training of NMT models successfully by scheduling the order of training examples according to their difficulty.  One stream is to re-weight training examples with different choices of  preferred examples during the training stage. For example, self-paced learning prefers easy examples, hard example mining exploits hard examples, and active learning emphasizes high variance examples. Another stream is to schedule the order of training examples according to their difficulty, e.g., curriculum learning which has been applied to the training of NMT models successfully.  In contrast, we explore strategies to simplify the difficult  examples without changing the model architecture and model training strategy.  \paragraph{Inactive Examples in Computer Vision Dataset.}  reveals that data redundancy exists in large-scale image recognition datasets, e.g., CIFAR-10 and ImageNet datasets. They find that a subset can generalize on par with the full dataset and that at least 10 of training data are redundant in these large-scale image classification datasets.  Our results confirm these findings on the large-scale NLP datasets. In addition, we propose to rejuvenate the inactive examples to further improve the model performance.      In this work, we propose a multi-task learning framework that jointly trains the model with the translation task on bitext data, the masked language modeling task on the source-side monolingual data and the denoising auto-encoding task on the target-side monolingual data. We explore data and noising scheduling approaches and demonstrate their efficacy for the proposed approach. We show that the proposed MTL approach can effectively improve the performance of MNMT on both high-resource and low-resource languages with large margin, and can also significantly improve the translation quality for zero-shot language pairs without bitext training data. We showed that the proposed approach is more effective than pre-training followed by finetuning for NMT. Furthermore, we showed the effectiveness of multitask learning for cross-lingual downstream tasks outperforming  SOTA larger models trained on single task.  For future work, we are interested in investigating the proposed approach in a scaled setting with more languages and a larger amount of monolingual data. Scheduling the different tasks and different types of data would be an interesting problem. Furthermore, we would also like to explore the most sample efficient strategy to add a new language to a trained MNMT system.  
","   \paragraph{Data Manipulation.} Our work is closely related to previous studies on manipulating training data for NMT models, which focuses on exploiting the original training data without augmenting additional data. For example, the data denoising approach aims to identify and clean the noise training examples. Data diversification tries to diversify the training data by applying forward-translation to the source side of the parallel data, or back-translation to the target side of parallel data in a reverse translation direction. Our approach is complementary to theirs, and using them together can further improve translation performance .  Another distantly related direction is to simplify the source sentences so that a black-box machine translation system can better translate them, which is out of scope in this work.  \paragraph{Distinguishing Training Examples.} Our work is also related to previous work on distinguishing training examples in machine learning.   For example, self-paced learning emphasizes the learning on examples with lower loss  in early training, which is robust to outliers or noisy labels. Hard example mining aims to accelerate the training process by hard examples with higher loss. Active learning combines the advantages of both self-paced learning and hard example mining by emphasizing high variance examples. Especially, curriculum learning has been applied to the training of NMT models successfully by scheduling the order of training examples according to their difficulty.  One stream is to re-weight training examples with different choices of  preferred examples during the training stage. For example, self-paced learning prefers easy examples, hard example mining exploits hard examples, and active learning emphasizes high variance examples. Another stream is to schedule the order of training examples according to their difficulty, e.g., curriculum learning which has been applied to the training of NMT models successfully.  In contrast, we explore strategies to simplify the difficult  examples without changing the model architecture and model training strategy.  \paragraph{Inactive Examples in Computer Vision Dataset.}  reveals that data redundancy exists in large-scale image recognition datasets, e.g., CIFAR-10 and ImageNet datasets. They find that a subset can generalize on par with the full dataset and that at least 10 of training data are redundant in these large-scale image classification datasets.  Our results confirm these findings on the large-scale NLP datasets. In addition, we propose to rejuvenate the inactive examples to further improve the model performance.",47
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.      Word embedding methods typically represent words as vectors in a low-dimensional space; usually, the vector space is Euclidean, but recently   other spaces, e.g. hyperbolic, have been explored.  However, vectorial embeddings   can have undesired properties:  e.g., in dot product spaces certain words cannot be assigned high probability regardless of their context. A conceptually different approach is to model words as probability density functions. We propose a new setting: embedding words as nodes in a weighted graph.   Representing language data in the form of a graph has been a long-standing task. Graph lexicons were used to learn word embeddings specialized towards certain types of lexical knowledge. It is also possible to incorporate external linguistic information from graphs, e.g. dependency parser outputs.  To learn a weighted graph, we use the method by.  Prior approaches to learning graphs from data are eigher highly problem-specific and not scalable \citet{escolano2011points,karasuyama2017adaptive,kang2019robust} or solve a less general but important case of learning directed acyclic graphs.  The opposite to learning a graph from data is the task of embedding nodes in a given graph to reflect graph distances and/or other properties; see \citet{hamilton2017representation} for a thorough survey.    Analysis of word embeddings and the structure of the learned feature space often reveals interesting language properties and is an important research direction. We show that graph-based embeddings can be a powerful tool for language analysis.                               In this study, we propose data rejuvenation to exploit the inactive training examples for neural machine translation on large-scale datasets. The proposed data rejuvenation scheme is a general framework where one can freely define, for instance, the identification and rejuvenation models. Experimental results on different model architectures and language pairs demonstrate the effectiveness and universality of the data rejuvenation approach.  Future directions include exploring advanced identification and rejuvenation models that can better reflect the learning abilities of NMT models, as well as validating on other NLP tasks such as dialogue and summarization.    
","  Word embedding methods typically represent words as vectors in a low-dimensional space; usually, the vector space is Euclidean, but recently   other spaces, e.g. hyperbolic, have been explored.  However, vectorial embeddings   can have undesired properties:  e.g., in dot product spaces certain words cannot be assigned high probability regardless of their context. A conceptually different approach is to model words as probability density functions. We propose a new setting: embedding words as nodes in a weighted graph.   Representing language data in the form of a graph has been a long-standing task. Graph lexicons were used to learn word embeddings specialized towards certain types of lexical knowledge. It is also possible to incorporate external linguistic information from graphs, e.g. dependency parser outputs.  To learn a weighted graph, we use the method by.  Prior approaches to learning graphs from data are eigher highly problem-specific and not scalable \citet{escolano2011points,karasuyama2017adaptive,kang2019robust} or solve a less general but important case of learning directed acyclic graphs.  The opposite to learning a graph from data is the task of embedding nodes in a given graph to reflect graph distances and/or other properties; see \citet{hamilton2017representation} for a thorough survey.    Analysis of word embeddings and the structure of the learned feature space often reveals interesting language properties and is an important research direction. We show that graph-based embeddings can be a powerful tool for language analysis.",48
"  Sentiment analysis  has attracted increasing attention recently. Aspect-based sentiment analysis   is a fine-grained sentiment analysis task and includes many subtasks, two of which are aspect category detection  that detects the aspect categories mentioned in a sentence and aspect-category sentiment analysis  that predicts the sentiment polarities with respect to the detected aspect categories. Figure shows an example. ACD detects the two aspect categories, ambience and food, and ACSA predicts the negative and positive sentiment toward them respectively. In this work, we focus on ACSA, while ACD as an auxiliary task is used to find the words indicating the aspect categories in sentences for ACSA.    Since a sentence usually contains one or more aspect categories, previous studies have developed various methods for generating aspect category-specific sentence representations to detect the sentiment toward a particular aspect category in a sentence. To name a few, attention-based models  allocate the appropriate sentiment words for the given aspect category. \citet{xue2018aspect} proposed to generate aspect category-specific representations based on convolutional neural networks and gating mechanisms. Since aspect-related information may already be discarded and aspect-irrelevant information may be retained in an aspect independent encoder, some existing methods  utilized the given aspect to guide the sentence encoding from scratch. Recently, BERT based models  have obtained promising performance on the ACSA task. However, these models ignored that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category. It leads to suboptimal performance of these models. For the example in Figure, both ``drinks'' and ``food'' indicate the aspect category food. The sentiment about food is a combination of the sentiments of ``drinks'' and ``food''. Note that, words indicating aspect categories not only contain aspect terms explicitly indicating an aspect category but also contain other words implicitly indicating an aspect category . In Figure, while ``drinks'' and ``food'' are aspect terms explicitly indicating the aspect category food, ``large'' and ``noisy'' are not aspect terms implicitly indicating the aspect category ambience.  In this paper, we propose a Multi-Instance Multi-label Learning Network for Aspect-Category sentiment analysis . AC-MIMLLN explicitly models the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category. Specifically, AC-MIMLLN treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances  of the aspect category. Given a bag and the aspect categories mentioned in the bag, AC-MIMLLN first predicts the instance sentiments, then finds the key instances for the aspect categories, finally aggregates the sentiments of the key instances to get the bag-level sentiments of the aspect categories.  Our main contributions can be summarized as follows:      Aspect-Category Sentiment Analysis predicts the sentiment polarities with regard to the given aspect categories. Many methods have been developed for this task. \citet{wang2016attention} proposed an attention-based LSTM network, which can concentrate on different parts of a sentence when different aspect categories are taken as input. Some new attention-based methods  allocated more appropriate sentiment words for aspect categories and obtained bertter performance. \citet{ruder-etal-2016-hierarchical} modeled the interdependencies of sentences in a text with a hierarchical bidirectional LSTM. \citet{xue2018aspect} extracted sentiment features with convolutional neural networks and selectively outputted aspect category related features with gating mechanisms. \citet{xing2019earlier}, \citet{liang2019novel} and \citet{10.1145/3350487} incorporated aspect category information into sentence encoders in the context modeling stage. \citet{lei2019human} proposed a human-like semantic cognition network to simulate the human beings閳 reading cognitive process. \citet{sun2019utilizing} constructed an auxiliary sentence from the aspect category and converted ACSA to a sentence-pair classification task. \citet{jiang2019challenge} put forward new capsule networks to model the complicated relationship between aspect categories and contexts. The capsule networks achieved state-of-the-art results. Several joint models   were proposed to avoid error propagation, which performed ACD and ACSA jointly.   However, all these models mentioned above ignored that the sentiment of an aspect category discussed in a sentence is an aggregation of the sentiments of the words indicating the aspect category.   Multi-Instance Multi-Label Learning   deals with problems where a training example is described by multiple instances and associated with multiple class labels. MIMLL has achieved success in various applications due to its advantages on learning with complicated objects, such as image classification , text categorization , relation extraction , etc. In ACSA, a sentence contains multiple words  and expresses sentiments to multiple aspect categories , so MIMLL is suitable for ACSA. However, as far as our knowledge, MIMLL has not been explored in ACSA.   Multiple instance learning   is a special case of MIMLL, where a real-world object described by a number of instances is associated with only one class label. Some studies  have applied MIL to sentiment analysis. \citet{angelidis-lapata-2018-multiple} proposed a Multiple Instance Learning Network , where the overarching polarity of a text is an aggregation of sentence or elementary discourse unit polarities, weighted by their importance. An attention-based polarity scoring method is used to obtain the importance of segments. Similar to MILNET, our model also uses an attention mechanism to obtain the importance of instances. However, the attention in our model is learned from the ACD task, while the attention in MILNET is learned from the sentiment classification task. \citet{pappas2014explaining} applied MIL to another subtask of ABSA. They proposed a multiple instance regression  model to assign sentiment scores to specific aspects of products. However, i) their task is different from ours, and ii) their model is not a neural network.    In this paper, we prove that existing NMT systems are over-parameterized and propose to improve the utilization efficiency of parameters in NMT models by introducing a rejuvenation approach. Empirical results on a variety of language pairs and architectures demonstrate the effectiveness and universality of the presented method. We also analyze the gains from perspectives of learning dynamics and linguistic probing, which give insightful research directions for future work.   Future directions include continuing the exploration of this research topic for large sequence-to-sequence pre-training models  and multi-domain translation models . We will employ recent analysis methods to better understand the behaviors of rejuvenated models.     \clearpage  
","  Aspect-Category Sentiment Analysis predicts the sentiment polarities with regard to the given aspect categories. Many methods have been developed for this task. \citet{wang2016attention} proposed an attention-based LSTM network, which can concentrate on different parts of a sentence when different aspect categories are taken as input. Some new attention-based methods  allocated more appropriate sentiment words for aspect categories and obtained bertter performance. \citet{ruder-etal-2016-hierarchical} modeled the interdependencies of sentences in a text with a hierarchical bidirectional LSTM. \citet{xue2018aspect} extracted sentiment features with convolutional neural networks and selectively outputted aspect category related features with gating mechanisms. \citet{xing2019earlier}, \citet{liang2019novel} and \citet{10.1145/3350487} incorporated aspect category information into sentence encoders in the context modeling stage. \citet{lei2019human} proposed a human-like semantic cognition network to simulate the human beings闁 reading cognitive process. \citet{sun2019utilizing} constructed an auxiliary sentence from the aspect category and converted ACSA to a sentence-pair classification task. \citet{jiang2019challenge} put forward new capsule networks to model the complicated relationship between aspect categories and contexts. The capsule networks achieved state-of-the-art results. Several joint models   were proposed to avoid error propagation, which performed ACD and ACSA jointly.   However, all these models mentioned above ignored that the sentiment of an aspect category discussed in a sentence is an aggregation of the sentiments of the words indicating the aspect category.   Multi-Instance Multi-Label Learning   deals with problems where a training example is described by multiple instances and associated with multiple class labels. MIMLL has achieved success in various applications due to its advantages on learning with complicated objects, such as image classification , text categorization , relation extraction , etc. In ACSA, a sentence contains multiple words  and expresses sentiments to multiple aspect categories , so MIMLL is suitable for ACSA. However, as far as our knowledge, MIMLL has not been explored in ACSA.   Multiple instance learning   is a special case of MIMLL, where a real-world object described by a number of instances is associated with only one class label. Some studies  have applied MIL to sentiment analysis. \citet{angelidis-lapata-2018-multiple} proposed a Multiple Instance Learning Network , where the overarching polarity of a text is an aggregation of sentence or elementary discourse unit polarities, weighted by their importance. An attention-based polarity scoring method is used to obtain the importance of segments. Similar to MILNET, our model also uses an attention mechanism to obtain the importance of instances. However, the attention in our model is learned from the ACD task, while the attention in MILNET is learned from the sentiment classification task. \citet{pappas2014explaining} applied MIL to another subtask of ABSA. They proposed a multiple instance regression  model to assign sentiment scores to specific aspects of products. However, i) their task is different from ours, and ii) their model is not a neural network.",49
" The recent success of the language model pre-training approaches~, which train language models on diverse text corpora with self-supervised or multi-task learning, have brought up huge performance improvements on several natural language understanding  tasks~. The key to this success is their ability to learn generalizable text embeddings that achieve near optimal performance on diverse tasks with only a few additional steps of fine-tuning on each downstream task.    Most of the existing works on language model aim to obtain a universal language model that can address nearly the entire set of available natural language tasks on heterogeneous domains. Although this train-once and use-anywhere approach has been shown to be helpful for various natural language tasks~, there have been considerable needs on adapting the learned language models to domain-specific corpora . Such domains may contain new entities that are not included in the common text corpora, and may contain only a small amount of labeled data as obtaining annotation on them may require expert knowledge.  Some recent works~ suggest to further pre-train the language model with self-supervised tasks on the domain-specific text corpus for adaptation, and show that it yields improved performance on tasks from the target domain.  Masked Language Models  objective in BERT~ has shown to be effective for the language model to learn the knowledge of the language in a bi-directional manner~. In general, masks in MLMs are sampled at random~, which seems reasonable for learning a generic language model pre-trained from scratch, since it needs to learn about as many words in the vocabulary as possible in diverse contexts.  However, in the case of further pre-training of the already pre-trained language model, such a conventional selection method may lead a domain adaptation in an inefficient way, since not all words will be equally important for the target task. Repeatedly learning for uninformative instances thus will be wasteful. Instead, as done with instance selection, it will be more effective if the masks focus on the most important words for the target domain, and for the specific NLU task at hands. How can we then obtain such a masking strategy to train the MLMs?   Several works~ propose rule-based masking strategies which work better than random masking ~ when applied to language model pre-training from scratch. Based on those works, we assume that adaptation of the pre-trained language model can be improved via a learned masking policy which selects the words to mask. Yet, existing models are inevitably suboptimal since they do not consider the target domain and the task. To overcome this limitation, in this work, we propose to adaptively generate mask by learning the optimal masking policy for the given task, for the task-adaptive pre-training~ of the language model.  As described in Figure , we want to further pre-train the language model on a specific task with a task-dependent masking policy, such that it directs the solution to the set of parameters that can better adapt to the target domain, while task-agnostic random policy leads the model to an arbitrary solution.  To tackle this problem, we pose the given learning problem as a meta-learning problem where we learn the task-adaptive mask-generating policy, such that the model learned with the masking strategy obtains high accuracy on the target task.  We refer to this meta-learner as the Neural Mask Generator . Specifically, we formulate mask learning as a bi-level problem where we pre-train and fine-tune a target language model in the inner loop, and learn the NMG at the outer loop, and solve it using renforcement learning. We validate our method on diverse NLU tasks, including question answering and text classification. The results show that the models trained using our NMG outperforms the models pre-trained using rule-based masking strategies, as well as finds a proper adaptive masking strategy for each domain and task.  Our contribution is threefold:      \paragraph{Language Model Pre-training}  Ever since \citet{UniLM} suggested language model pre-training with multi-task learning, inspired by the success of fine-tuning on ImageNet pre-trained models on computer vision tasks~, research on the representation learning for natural language understanding tasks have focused on obtaining a global language model that can generalize to any NLU tasks. A popular approach is to use self-supervised pre-training tasks for learning the contextualized embedding from large unannotated text corpora using auto-regressive~ or auto-encoding~ language modeling. Following the success of the Masked Language Model  from~, several works have proposed different model architecture~ and pre-training objectives~, to improve upon its performance. Some works have also proposed alternative masking policies for the MLM pre-training over random sampling, such as SpanBERT and ERNIE. Yet, none of the existing approaches have tried to learn the task-adaptive mask in a context-dependent manner which is the problem we target in this work.   \paragraph{Language Model Adaptation} Pre-training the language model on the target domain, then fine-tuning on downstream tasks, is the most simple yet successful approach for adapting the language model to a specific task. Some studies~ have shown the advantage of further pre-training the language model on a large unlabeled text corpus collected from a specific domain. Moreover, \citet{HowtoFT} and \citet{ unsuperivsed-domain-adaptation} investigate the effectiveness of further pre-training of the language model on small domain-related text corpora. Recently, \citet{DontStopPT} integrates prior works and defines domain-adaptive pre-training and task-adaptive pre-training, showing that domain adaptation of the language model can be done with additional pre-training with the MLM objective on a domain-related text corpus, as well as a smaller but directly task-relevant text corpus.  \paragraph{Meta-Learning} Meta-learning aims to train the model to generalize over a distribution of tasks, such that it can generalize to an unseen task. There exist large number of different approaches to train the meta-learner~. However, existing meta-learning approaches do not scale well to the training of large models such as masked language models. Thus, instead of the existing meta-learning method such a gradient based approach, we formulate the problem as a bi-level problem of learning the language model in the inner loop and the mask at the outer loop, and solve it using reinforcement learning. Such optimization of the outer objective using RL is similar to the formulation used in previous works on neural architecture search.  In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis . AC-MIMLLN predicts the sentiment of an aspect category mentioned in a sentence by aggregating the sentiments of the words indicating the aspect category in the sentence. Experimental results demonstrate the effectiveness of AC-MIMLLN. Since AC-MIMLLN finds the key instances for the given aspect category and predicts the sentiments of the key instances, it is more interpretable. In some sentences, phrases or clauses rather than words indicate the given aspect category, future work could consider multi-grained instances, including  words, phrases and clauses. Since directly finding the key instances for some aspect categories is ineffective, we will try to first recognize all opinion snippets in a sentence, then assign these snippets to the aspect categories mentioned in the sentence.   
","  \paragraph{Language Model Pre-training}  Ever since \citet{UniLM} suggested language model pre-training with multi-task learning, inspired by the success of fine-tuning on ImageNet pre-trained models on computer vision tasks~, research on the representation learning for natural language understanding tasks have focused on obtaining a global language model that can generalize to any NLU tasks. A popular approach is to use self-supervised pre-training tasks for learning the contextualized embedding from large unannotated text corpora using auto-regressive~ or auto-encoding~ language modeling. Following the success of the Masked Language Model  from~, several works have proposed different model architecture~ and pre-training objectives~, to improve upon its performance. Some works have also proposed alternative masking policies for the MLM pre-training over random sampling, such as SpanBERT and ERNIE. Yet, none of the existing approaches have tried to learn the task-adaptive mask in a context-dependent manner which is the problem we target in this work.   \paragraph{Language Model Adaptation} Pre-training the language model on the target domain, then fine-tuning on downstream tasks, is the most simple yet successful approach for adapting the language model to a specific task. Some studies~ have shown the advantage of further pre-training the language model on a large unlabeled text corpus collected from a specific domain. Moreover, \citet{HowtoFT} and \citet{ unsuperivsed-domain-adaptation} investigate the effectiveness of further pre-training of the language model on small domain-related text corpora. Recently, \citet{DontStopPT} integrates prior works and defines domain-adaptive pre-training and task-adaptive pre-training, showing that domain adaptation of the language model can be done with additional pre-training with the MLM objective on a domain-related text corpus, as well as a smaller but directly task-relevant text corpus.  \paragraph{Meta-Learning} Meta-learning aims to train the model to generalize over a distribution of tasks, such that it can generalize to an unseen task. There exist large number of different approaches to train the meta-learner~. However, existing meta-learning approaches do not scale well to the training of large models such as masked language models. Thus, instead of the existing meta-learning method such a gradient based approach, we formulate the problem as a bi-level problem of learning the language model in the inner loop and the mask at the outer loop, and solve it using reinforcement learning. Such optimization of the outer objective using RL is similar to the formulation used in previous works on neural architecture search.",50
"   Sentiment analysis has become an increasingly popular natural language  processing  task in academia and industry.  It provides real-time  feedback on consumer experience and their needs, which helps  producers to offer better services.  To deal with the presence of  multiple categories in one document,  ACSA tasks, including aspect-category  sentiment analysis  and targeted aspect-category sentiment analysis , were introduced.   The main purpose for ACSA task  is to identify sentiment polarity  of an input sentence upon specific predefined categories . For  example, as shown in Table , giving an input sentence ``Food is  always fresh and hot-ready to eat, but it is too expensive."" and predefined categories \{food, service, price,  ambience and anecdotes/miscellaneous\},  the sentiment of category food is positive, the polarity  regarding to category price is negative, while is none for others.  In this task, the models should  capture both explicit expressions and implicit expressions. For example, the phrase ``too expensive"" indicates the  negative polarity  in the price category, without a direct indication of ``price"".    In order to  deal with ACSA with both multiple categories and multiple targets, TACSA task was introduced  to analyze sentiment polarity on a set of predefined target-category pairs. An example is shown in Table , given targets ``restaurant-1"" and ``restaurant-2"", in the case ``I like  restaurant-1 because it's cheap, but restaurant-2 is too  expansive"", the category price for target ``restaurant-1"" is positive, but is  negative for target ``restaurant-2"", while is none for other target-category pairs. A mathematical definition for ACSA is given  as follows: giving a  sentence  as input, a predefined set of targets  and a predefined set of  aspect categories , a model predicts the sentiment polarity  for  each target-category pair . For ACSA  task, there is only one target  in all  categories. In this paper, in order to simplify the expression in TACSA, we use predefined categories, which is short for predefined target-category pairs.   	} 	 \end{table*}  Multi-task learning, with shared encoders but individual decoders for each category, is an approach to analyze all the categories in one sample simultaneously for ACSA . Compared with single-task ways , multi-task approaches utilize category-specific knowledge in training signals from each task and get better performance. However, current multi-task models still suffer from a lack of  features such as category name . Models with category name features encoded in the model may further improve the performance.  On the other hand, the predefined categories in ACSA task make the application  in new categories inflexible, as for ACSA applications, the number of categories maybe  varied over time.  For example, fuel consumption, price level, engine power, space and so  on are source categories to be analyzed in the gasoline automotive domain. For  electromotive domain, source categories in the automotive domain will still be used, while new target category such as battery duration should also be analyzed.  Incremental learning is a way to solve this problem. Therefore, it is necessary to propose an  incremental learning task and an incremental learning model concerned with new  category for ACSA tasks.  Unfortunately, in the current multi-task learning ACSA models, the encoder is shared but the decoders for each category are individual. This parameter sharing mechanism results in only the shared encoder  and target-category-related decoders are finetuned during the finetuning process, while the decoder of source categories remains unchanged. The finetuned encoder and original decoder of source categories may cause catastrophic forgetting problem in the origin  categories. For real applications, high accuracy is excepted in source  categories and target  categories.  Based on the previous researches that decoders between different tasks are usually modeled by mean regularization   , an idea comes up to further make the decoders the same by sharing the decoders in all categories to decrease the catastrophic forgetting problem. But here raises another question, how to identify each category in the encoder and decoder shared network? In our approach, we  solve the category discrimination problem by the input category name feature.   In this paper,  we proposed a multi-task category name embedding network  .  The multi-task learning  framework makes full use of training signals from all categories. To make it feasible for incremental learning, both encoder and decoders for each category are shared. The category names were applied as another input feature for task discrimination. We also present a new task for ACSA incremental learning. In particular,  our contribution is three-folded:    We proposed a multi-task CNE-net framework with both encoder and decoder shared to weaken catastrophic forgetting problem in multi-task learning ACSA model.     We achieved  state-of-the-art on the two ACSA datasets, SemEval14-Task4  and Sentihood.   We proposed a new task for incremental learning in ACSA. By sharing both encoder layers and decoder layers of all the tasks, we   achieved better results compared with other baselines both in source  categories and in the target category.        ACSA task is to  predict sentiment polarity on a set of predefined categories.  It is able to  analyze sentiment in an end-to-end way with explicit expressions or implicit expressions .  The earliest works most concerned on feature engineering  . Subsequently,   applied neural network models to achieve higher accuracy.   then involved commonsense knowledge as additional features.  The  current approaches consist of multi-task models  ,  which analyze all the categories simultaneously in one sample to make  full use of all the features and labels in the training sample, and single-task  models that treat one category in one sample  .      Multi-task learning utilizes all the related tasks by  sharing the commonalities while learning individual features for each sub-task.  MTL has been proven to be effective in many NLP tasks, such as  information retrieval , machine translation  , and semantic role labeling  .  For ACSA task,   applied MTL framework with a shared LSTM encoder and individual decoder classifiers for each category. The multiple aspects in MTL were handled by constrained attention networks with orthogonal and sparse  regularization .   Incremental learning was inspired by adding new abilities to a model without having to retrain the entire model. For  example,  presented several random forest models to  perform sentiment analysis on customers' reviews. Many domain  adaptation approaches utilizing transfer learning suffer from ``catastrophic  forgetting"" problem . To solve this problem,   proposed an incremental learning Deep-Adaption-Network that  constrains newly learned filters to be linear combinations of existing ones.  To the best of our knowledge, for ACSA task, few researches concerned with  incremental learning in new categories. In this paper,  we proposed a ACSA incremental learning task and the CNE-net model to solve this problem in a multi-task learning approach with a shared encoder and shared decoders. We also apply category name for task discrimination.      We proposed a novel framework which automatically generates an adaptive masking for masked language models based on the given context, for language model adaptation to low-resource domains. To this end, we proposed the Neural Mask Generator , which is trained with reinforcement learning to mask out words that are helpful for domain adaptation. We performed an empirical study of various rule-based masking strategies on multiple datasets for question answering and text classification tasks, which shows that the optimal masking strategy depends on both the language model and the domain. We then validated NMG against rule-based masking strategies, and the results show that it either outperforms, or obtains comparable performance to the best heuristic. Further qualitative analysis suggests that such good performance comes from its ability to adaptively mask meaningful words for the given task.  
","    ACSA task is to  predict sentiment polarity on a set of predefined categories.  It is able to  analyze sentiment in an end-to-end way with explicit expressions or implicit expressions .  The earliest works most concerned on feature engineering  . Subsequently,   applied neural network models to achieve higher accuracy.   then involved commonsense knowledge as additional features.  The  current approaches consist of multi-task models  ,  which analyze all the categories simultaneously in one sample to make  full use of all the features and labels in the training sample, and single-task  models that treat one category in one sample  .      Multi-task learning utilizes all the related tasks by  sharing the commonalities while learning individual features for each sub-task.  MTL has been proven to be effective in many NLP tasks, such as  information retrieval , machine translation  , and semantic role labeling  .  For ACSA task,   applied MTL framework with a shared LSTM encoder and individual decoder classifiers for each category. The multiple aspects in MTL were handled by constrained attention networks with orthogonal and sparse  regularization .   Incremental learning was inspired by adding new abilities to a model without having to retrain the entire model. For  example,  presented several random forest models to  perform sentiment analysis on customers' reviews. Many domain  adaptation approaches utilizing transfer learning suffer from ``catastrophic  forgetting"" problem . To solve this problem,   proposed an incremental learning Deep-Adaption-Network that  constrains newly learned filters to be linear combinations of existing ones.  To the best of our knowledge, for ACSA task, few researches concerned with  incremental learning in new categories. In this paper,  we proposed a ACSA incremental learning task and the CNE-net model to solve this problem in a multi-task learning approach with a shared encoder and shared decoders. We also apply category name for task discrimination.",51
"   Conditional random fields  have been shown to perform well in various sequence labeling tasks. Recent work uses rich neural network architectures to define the ``unary'' potentials, i.e., terms that only consider a single position's label at a time~. However, ``binary'' potentials, which consider pairs of adjacent labels, are usually quite simple and may consist solely of a parameter or parameter vector for each unique label transition. Models with unary and binary potentials are generally referred to as ``first order'' models.   A major challenge with CRFs is the complexity of training and inference, which are quadratic in the number of output labels for first order models and grow exponentially when higher order dependencies are considered. This explains why the most common type of CRF used in practice is a first order model, also referred to as a ``linear chain'' CRF.   One promising alternative to CRFs is structured prediction energy networks , which use deep neural networks to parameterize arbitrary potential functions for structured prediction. While SPENs also pose challenges for learning and inference, \citet{tu-18} proposed a way to train SPENs jointly with ``inference networks'', neural networks trained to approximate structured  inference.   In this paper, we leverage the frameworks of SPENs and inference networks to explore high-order energy functions for sequence labeling. Naively instantiating high-order energy terms can lead to a very large number of parameters to learn, so we instead develop concise neural parameterizations for high-order terms. In particular, we draw from vectorized Kronecker products, convolutional networks, recurrent networks, and self-attention.  We also consider ``skip-chain'' connections~ with various skip distances and ways of reducing their total parameter count for increased learnability.   Our experimental results on four sequence labeling tasks show that a range of high-order energy functions can yield performance improvements. While the optimal energy function varies by task, we find strong performance from skip-chain terms with short skip distances, convolutional networks with filters that consider label trigrams, and recurrent networks and self-attention networks that consider large subsequences of labels.     We also demonstrate that modeling high-order dependencies can lead to significant performance improvements in the setting of noisy training and test sets.  Visualizations of the high-order energies show various methods capture intuitive structured dependencies among output labels.   Throughout, we use inference networks that share the same architecture as unstructured classifiers for sequence labeling, so test time inference speeds are unchanged between local models and our method.  Enlarging the inference network architecture by adding one layer leads consistently to better results, rivaling or improving over a BiLSTM-CRF baseline,  suggesting that training efficient inference networks with high-order energy terms can make up for errors arising from approximate inference. While we focus on sequence labeling in this paper, our results show the potential of developing high-order structured models for other NLP tasks in the future.          We denote the input space by . For an input , we denote the structured  output space by . The entire space of structured outputs is denoted .  We define an energy function~   parameterized by  that  computes a scalar energy for an input/output pair: .  At test time, for a given input , prediction is done by choosing the output with lowest energy:     \paragraph{Inference.}  Solving \eqref{eq:inf} requires combinatorial algorithms because  is a structured, discrete space. This becomes intractable when  does not decompose into a sum over small ``parts'' of .  \citet{belanger2016structured} relax this problem by allowing the discrete vector  to be continuous.  Let  denote the relaxed output space.  They solve the relaxed problem by using gradient descent to iteratively minimize the energy with respect to .   \citet{tu-18} propose an alternative that replaces gradient descent   with a neural network trained to do inference, i.e., to mimic the function performed in \eqref{eq:inf}. This ``inference network''  is parameterized by  and trained with the goal that  \citet{tu-gimpel-2019-benchmarking} show that inference networks achieve a better speed/accuracy/search error trade-off than gradient descent given pretrained energy functions.   \paragraph{Joint training of energy functions and inference networks.}  \citet{belanger2016structured} proposed a structured hinge loss for learning the energy function parameters , using gradient descent for the ``cost-augmented'' inference step required during learning. \citet{tu-18} replaced the cost-augmented inference step in the structured hinge loss with training of a ``cost-augmented inference network''  trained with the following goal:    where  is a structured cost function that computes the distance between its two arguments.  The new optimization objective becomes:    where  is the set of training pairs and .  \citet{tu-18}  alternatively optimized  and , which is similar to training in generative adversarial networks~.      One challenge with the optimization problem above is that it still requires training an inference network  for test-time prediction. \citet{tu2019improving} proposed a ``compound'' objective that avoids this by training two inference networks jointly ,  for cost-augmented inference and  for test-time inference:  As indicated, this loss can be viewed as the sum of the margin-rescaled and perceptron losses.  , , and  are alternatively optimized. The objective for the energy function parameters  is:  The objective for the other parameters is:   where  is a supervised token-level loss which is added to aid in training inference networks. In this paper, we use the standard cross entropy summed over all positions. Like \citet{tu2019improving}, we drop the zero truncation  when updating the inference network parameters to improve stability during training, which also lets us remove the terms that do not have inference networks. We use two independent networks but with the same architecture for the two inference networks.     In this paper, in order to make multi-task learning feasible for incremental learning,  we proposed CNE-net with different attention mechanisms. The category  name features and the multi-task learning structure help the model  achieve state-of-the-art on ACSA and TACSA tasks. Furthermore,  the shared encoder and decoder layers weaken catastrophic forgetting in the incremental learning task.  We proposed a task for ACSA incremental learning and achieved the best  performance with CNE-net compared with other strong baselines.  Further research may be concerned with zero-shot learning on new categories.   
","      We denote the input space by . For an input , we denote the structured  output space by . The entire space of structured outputs is denoted .  We define an energy function~   parameterized by  that  computes a scalar energy for an input/output pair: .  At test time, for a given input , prediction is done by choosing the output with lowest energy:     \paragraph{Inference.}  Solving \eqref{eq:inf} requires combinatorial algorithms because  is a structured, discrete space. This becomes intractable when  does not decompose into a sum over small ``parts'' of .  \citet{belanger2016structured} relax this problem by allowing the discrete vector  to be continuous.  Let  denote the relaxed output space.  They solve the relaxed problem by using gradient descent to iteratively minimize the energy with respect to .   \citet{tu-18} propose an alternative that replaces gradient descent   with a neural network trained to do inference, i.e., to mimic the function performed in \eqref{eq:inf}. This ``inference network''  is parameterized by  and trained with the goal that  \citet{tu-gimpel-2019-benchmarking} show that inference networks achieve a better speed/accuracy/search error trade-off than gradient descent given pretrained energy functions.   \paragraph{Joint training of energy functions and inference networks.}  \citet{belanger2016structured} proposed a structured hinge loss for learning the energy function parameters , using gradient descent for the ``cost-augmented'' inference step required during learning. \citet{tu-18} replaced the cost-augmented inference step in the structured hinge loss with training of a ``cost-augmented inference network''  trained with the following goal:    where  is a structured cost function that computes the distance between its two arguments.  The new optimization objective becomes:    where  is the set of training pairs and .  \citet{tu-18}  alternatively optimized  and , which is similar to training in generative adversarial networks~.      One challenge with the optimization problem above is that it still requires training an inference network  for test-time prediction. \citet{tu2019improving} proposed a ``compound'' objective that avoids this by training two inference networks jointly ,  for cost-augmented inference and  for test-time inference:  As indicated, this loss can be viewed as the sum of the margin-rescaled and perceptron losses.  , , and  are alternatively optimized. The objective for the energy function parameters  is:  The objective for the other parameters is:   where  is a supervised token-level loss which is added to aid in training inference networks. In this paper, we use the standard cross entropy summed over all positions. Like \citet{tu2019improving}, we drop the zero truncation  when updating the inference network parameters to improve stability during training, which also lets us remove the terms that do not have inference networks. We use two independent networks but with the same architecture for the two inference networks.",52
"  Event argument extraction  aims to identify the entities that serve as arguments of an event and to classify the specific roles they play. As in Fig., ``two soldiers'' and ``yesterday'' are arguments, where the event triggers are ``attacked''   and ``injured'' . For the trigger ``attacked'', ``two soldiers'' plays the argument role Target while ``yesterday'' plays the argument role Attack\_Time. For the event trigger ``injured'', ``two soldiers'' and ``yesterday'' play the role Victim and INJURY\_Time, respectively. There has been significant work on event extraction  , but the EAE task remains a challenge and has become the bottleneck for improving the overall performance of EE.\footnote{EAE has similarities with semantic role labeling. Event triggers are comparable to predicates in SRL and the roles in most SRL datasets have a standard convention of interpreting who did what to whom. EAE has a custom taxonomy of roles by domain. We also use inspiration from the SRL body of work .}     Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that,  We use  BERT as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike%previous studies ~ who added a final/prediction layer to BERT for argument extraction, we use BERT as token embedder and build a sequence of EAE task-specific components .  We use  in-domain data to adapt the BERT model parameters in a subsequent pretraining step as in . This makes the encoder domain-aware.  We perform self-training to construct auto-labeled data .  A crucial aspect for EAE is to integrate event trigger information into the learned representations. This is important because arguments are dependent on triggers, i.e., the same argument span plays completely different roles toward different triggers. An example is shown in Fig., where ``two soldiers'' plays the role Target for the event ATTACK and the role Victim for INJURY. Different from existing work that relies on regular sequence encoders, we design a novel trigger-aware encoder which simultaneously learns four different types of trigger-informed sequence representations. %for candidate arguments.   Capturing the long-range dependency is another important factor, e.g., the connection between an event trigger and a distant argument. Syntactic information could be useful in this case, as it could help bridge the gap from a word to another distant but highly related word. We modify a Transformer  by explicitly incorporating syntax via an attention layer driven by the dependency parse of the sequence. % .  %Since arguments of an event are entities, entity mentions are very effective hints.  We design our role-specific argument decoder to seamlessly accommodate both settings . We also tackle the role overlap problem  using a set of classifiers or taggers in our decoder.   Our model achieves the new state-of-the-art on ACE2005 Events data.% for EAE.  % % Motivation 1: data scarcity. Proposed and used solutions:  pretrained model BERT  External embedding   Self-training   BERT MLM  MLM encoder and decoder joint pre-training.  Teacher-Student    %   Event Argument Exaction  is an important task in Event Extraction . Early studies designed lexical, contextual or syntactical features to tackle the EE problem. Later on, neural networks demonstrated their effectiveness in representation learning without manual feature engineering. Our proposed model belongs to the latter category.   Here we present and discuss the most related studies to our work. ~ used a pre-trained model with a state-machine based span boundary detector. They used heuristics to resolve final span boundaries. ~  used a pre-trained model in a state-machine based span boundary learning fashion, with the involvement of some heuristic designs. \rishitacomment{I would shorten and split it in summary with mention of what we think are the 'heuristic designs': ""used a pre-trained model with a state-machine based span boundary detector. They used heuristics to resolve to final span boundaries.""}   ~  \rishitacomment{Why is this not in the list of neural models in first paragraph?} also used a pre-trained model together with a hand-crafted conceptual hierarchy.  Their solutions both need human designs \rishitacomment{Do you mean manual feature-engineering? 'Human designs' is a bit vague.} while ours do not require.  Our approach does not need the design of such heuristics or conceptual hierarchy. In terms of modeling, their approaches used regular BERT as their encoders to generate the whole event sequence and then add a prediction layer on top of it , where the argument representations are not explicitly conditioned on triggers.  The representations from BERT encoder are then past to their \rishitacomment{which decoders, sequence taggers? Maybe call that?}decoders for argument extraction.  \rishitacomment{That is only their encoders. I assume you want to say sequence taggers on top of BERT for them?}.  In contrast, our encoder is enhanced by providing more trigger-oriented information and BERT is only used as one part of it, which results in a trigger-aware sequence encoder.  Along with other sophisticated modeling units, our approach can better  This allows us to better model interactions between arguments and triggers. ~ added a GCN layer to integrate the syntactic information into a neural model. Different from their solution, we encode the syntax jointly with attention mechanism, simplifying the learning, making it more efficient, and achieving better results. Finally, no prior work has deeply studied the data scarcity issue in EAE, while we exploit several techniques to tackle it in this work.  \rishitacomment{One thing to add here is a comparison to dep parse papers and say how they use it at inference too. You use the information but only at training time, hence taking no inference cost of running a dep parser. This is quite strong I think in comparison.}    (        does not consider addressing the role overlapping problem.  used a pre-trained model in a relatively complex state-machine based span boundary learning fashion, which aims to extract start and end tokens with the involvement of heuristic designs.     A previous work JMEE also uses syntax, but in a complicated way.      We present the first systematic study of negative interference  in multilingual models and shed light on its causes. We further propose a method and show it can improve cross-lingual transferability  by mitigating negative interference.  While prior efforts focus on improving sharing and cross-lingual alignment,  we provide new insights and a different perspective  on unsharing and resolving language conflicts.   
","  Event Argument Exaction  is an important task in Event Extraction . Early studies designed lexical, contextual or syntactical features to tackle the EE problem. Later on, neural networks demonstrated their effectiveness in representation learning without manual feature engineering. Our proposed model belongs to the latter category.   Here we present and discuss the most related studies to our work. ~ used a pre-trained model with a state-machine based span boundary detector. They used heuristics to resolve final span boundaries. ~  used a pre-trained model in a state-machine based span boundary learning fashion, with the involvement of some heuristic designs. \rishitacomment{I would shorten and split it in summary with mention of what we think are the 'heuristic designs': ""used a pre-trained model with a state-machine based span boundary detector. They used heuristics to resolve to final span boundaries.""}   ~  \rishitacomment{Why is this not in the list of neural models in first paragraph?} also used a pre-trained model together with a hand-crafted conceptual hierarchy.  Their solutions both need human designs \rishitacomment{Do you mean manual feature-engineering? 'Human designs' is a bit vague.} while ours do not require.  Our approach does not need the design of such heuristics or conceptual hierarchy. In terms of modeling, their approaches used regular BERT as their encoders to generate the whole event sequence and then add a prediction layer on top of it , where the argument representations are not explicitly conditioned on triggers.  The representations from BERT encoder are then past to their \rishitacomment{which decoders, sequence taggers? Maybe call that?}decoders for argument extraction.  \rishitacomment{That is only their encoders. I assume you want to say sequence taggers on top of BERT for them?}.  In contrast, our encoder is enhanced by providing more trigger-oriented information and BERT is only used as one part of it, which results in a trigger-aware sequence encoder.  Along with other sophisticated modeling units, our approach can better  This allows us to better model interactions between arguments and triggers. ~ added a GCN layer to integrate the syntactic information into a neural model. Different from their solution, we encode the syntax jointly with attention mechanism, simplifying the learning, making it more efficient, and achieving better results. Finally, no prior work has deeply studied the data scarcity issue in EAE, while we exploit several techniques to tackle it in this work.  \rishitacomment{One thing to add here is a comparison to dep parse papers and say how they use it at inference too. You use the information but only at training time, hence taking no inference cost of running a dep parser. This is quite strong I think in comparison.}    (        does not consider addressing the role overlapping problem.  used a pre-trained model in a relatively complex state-machine based span boundary learning fashion, which aims to extract start and end tokens with the involvement of heuristic designs.     A previous work JMEE also uses syntax, but in a complicated way.",53
" In most current NLP tasks, fixed-length vector representations of words, word embeddings, are used to represent some form of the meaning of the word. In the case of humans, however, oftentimes we will use a sequence of words known as a definition ---a statement of the meaning for a term--- to express meanings of terms . It is with this in mind that the question of ``Can machines define?'' is aimed to be answered with the task of definition modeling .  Definition modeling can be framed as a task of conditional generation, in which the definition  of the word or phrase is generated given a conditioning variable  such as a word's associated word embedding or other representations of context. Current approaches for this task  are mainly encoder-decoder based, in which one encodes a contextual representation for a word/phrase  using a variety of features such as context or character composition, and uses the contextual representation to generate the definition .   % here discuss issues of these approaches including Despite the relative success of existing approaches for definition modelling, their discriminative nature ---where distributional-derived information is at one end of the model and lexical information is at the other--- limits their power as the underlying semantic representations of the distributional and lexical information are learned in an implicit rather than direct way. For example, although \citet{ishiwatari-etal-2019-learning} successfully showed that both local and global contexts are useful to disambiguate meanings of phrases in certain cases, their approach heavily relies on an attention mechanism to identify semantic alignments between the input phrase and the output definition, which may introduce noise and ultimately be insufficient to capture the entire meaning of each phrase-definition pair.  % latent definition space with  To tackle this issue, we propose to explicitly model the underlying semantics of  phrase-definition pairs by introducing a continuous latent variable  over a definition space, which is used in conjunction with  to guide the generation of definition . The introduction of this latent representation enables us to treat it as a global defining signal during the generation process, complementing existing alignment mechanisms such as the attention.   % We specifically incorporate the latent variable directly into the decoder cell, showing that the addition of the latent variable in this way leads to increased performance on our task.   Although the latent definition variable enables us to explicitly model underlying semantics of context-definition pairs, the incorporation of it into the task renders the posterior intractable. In this paper we recur to variational inference to estimate this intractable posterior, effectively making our model a Conditional Variational Autoencoder and evolving the generation process from  to .  %to serve as a global decoding signal allows for the decoder to rely on both the attention, but if the attention is misleading you can rely on the latent variable % and this issue of misleading attentions is exacerbated in noisy datasets, so we can see improvements there as well when the generator learns misleading attention representations.    % EDISON % enables us not only to generate definitions of previously unknown words or phrases, by menas of an example , but also to obtain semantically meaningful vectors of new words by means of providing their definition alongside with an example of their usage. Effectively, our mode is able to mapping both inputs  to the same smooth space/manifold?.  We also note that existing approaches for definition modelling heavily rely on word embeddings, which due to their fixed nature can only capture so much of the semantics, being known to offer limited capabilities when dealing with polysemy. Considering the success of pretrained deep contextualized word representations which by specifically addressing these limitations have been shown to improve performance on a variety of downstream NLP tasks , in this paper we propose a mechanism to integrate deep contextualized word representations in the definition modelling task. Specifically, we successfully leverage BERT  as our contextual encoder and our definition encoder to produce representations for  and  respectively.   %While the inclusion of deep contextual word representations is important to our approach, our resuts show that it is not essential . %As a result, our model is able to  allowing for  a more meaningful continuous latent space, and  . [2-3 more sentences]  Finally, we develop two new datasets for this task, one derived from the Cambridge Dictionary , and the other derived from Le Petit Robert. In summary, our contributions are:  Datasets and pre-trained models will be publicly released to the greater NLP community to help facilitate further advances on this task upon acceptance of this paper.    Our work is related to the seminal paper by \citet{hill-dictionary-2016}, who proposed using the definitions found in everyday dictionaries as a means of bridging existing gaps between lexical and phrasal semantics. Effectively, they train a language model to map dictionary definitions to lexical representations of words, presenting the task of reverse dictionaries, where the goal is to return the name of a concept given a definition.   get embedding space. For each of these words, we   extracted dictionary-style definitions from five elec-   tronic resources: Wordnet, The American Heritage   Dictionary, The Collaborative International Dictio-   nary of English, Wiktionary and Webster閳ユ獨.  limitations    HILL   Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal se mantics. Neural language embedding models can be effectively trained to map dictionary definitions  to  representations of the words defined by those definitions. We present two applications of these architectures: reverse dictionaries that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing mode  \citet{noraset} later introduced the task of definition modeling, in which a model is tasked with generating a definition for a given word, given its respective embedding. The authors argued that, compared to other related tasks such as word similarity or analogical relatedness, definition generation can be considered a more transparent view of the information captured by an embedding. However, this method does not incorporate contextual information, preventing it from generating appropriate definitions for polysemic words. Addresing this, \citet{gadetsky} studied the problem of polysemy in definition modeling, introducing an attention-based model which uses contextual information determine components in the embedding which may refer to a relevant word meaning.     The authors also released adefinition dataset extracted from the Oxford Dictionaries, which was larger in terms of number of unique words, to the data used by \citet{noraset}, while also supplementing each example with context sentences in which each example word is used.    GADETSKY   We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words ambiguity and polysemy leads to performance improvement.   - we introduce two models based on recurrent neural network  language models,     we collect new dataset of definitions  , which is larger in number of unique words than proposed in Noraset et al.  and also supplement it with examples of the word usage    -  finally, in the experiment section we show that our models outperform previously proposed models and have the ability to genearte definitions depending on the meaning of words.    NI \citet{niLearningExplainNonStandard2017a} explore a different but related problem, proposing an approach for automatically explaining non-standard English expressions  in a given sentence. They present a hybrid word-character sequence-to-sequence model that directly explains unseen non-standard expression, garnering reasonable definitions of expressions given their context.   . Unlike prior studies that focus on matching keywords from a slang dictionary, we investigate the possibility of learning a neural sequence-to-sequence model that generates explanations of unseen non-standard English expressions given context. We propose a dual encoder approach閳ユ攣 word-level encoder learns the representation of context, and a second character-level encoder to learn the hidden representation of the target non-standard expression. Our model can produce reasonable definitions of new non-standard English expressions given their context with certain confidence   - We present a large, publicly available corpus of non-standard English words and phrases, including 15 years of definitions and exam ples for each entry via crowdsourcing;    - We present a hybrid word-character sequence-to-sequence model that directly explains unseen non-standard expressions from social media;   - We present a hybrid word-character sequence-to-sequence model that directly explains unseen non-standard expressions from social media;  More recently, \citet{ishiwatari-etal-2019-learning} have tackled some of the limitations of previous works on definition modelling and non-standard English expression explanation. Concretely, they note that whenever it is not possible to figure out the meaning of a given expression from its immediate local context, it is common to consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. In light of this, they introduce the task of describing a given phrase in natural language, based on its local and global contexts. To tackle this the authors introduce a model which consists of two context encoders  as well as a description decoder. Our proposed model, uses a more practical variational encoder-decoder framework, allowing us to take advantage of explicitly modeling the phrase-definition relationship, while also leveraging deep contextualized word representations for more informative context representations.    ISHIWATARI   When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation  and definition generation , our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets  and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.  Finally, our model is also related to \citet{sohnLearningStructuredOutput2015}, in which Conditional Variational Autoencoders  ---an extension of the original variational autoencoder  --- were proposed for generating diverse structured output, mainly in the context of image generation, and visual object segmentation and labeling. Our work is also related to CVAE models that have been developed for the domain of natural language processing, specifically \citet{zhang-etal-2016-variational-neural} who proposed a CVAE in the context of Neural Machine Translation . As the usage of VAEs has become relatively common, we will omit a detailed explanation of these models, referring readers to \citet{kingmaAutoEncodingVariationalBayes2014}.    differences with original and other potential CVAE here   We present a new model which provides the best results in the EAE task. The model can generate trigger-aware argument representations, incorporate syntactic information , and handle the role overlapping problem with role-specific argument decoder. We also experiment with some methods to address the data scarcity issue. Experimental results show the effectiveness of our proposed approaches.    Experimental results demonstrate the effectiveness of our approach.   We also addressd the learning gap/discrepancy between pre-trained and newly-trained components.  
","  Our work is related to the seminal paper by \citet{hill-dictionary-2016}, who proposed using the definitions found in everyday dictionaries as a means of bridging existing gaps between lexical and phrasal semantics. Effectively, they train a language model to map dictionary definitions to lexical representations of words, presenting the task of reverse dictionaries, where the goal is to return the name of a concept given a definition.   get embedding space. For each of these words, we   extracted dictionary-style definitions from five elec-   tronic resources: Wordnet, The American Heritage   Dictionary, The Collaborative International Dictio-   nary of English, Wiktionary and Webster闁炽儲鐛.  limitations    HILL   Distributional models that learn rich semantic word representations are a success story of recent NLP research. However, developing models that learn useful representations of phrases and sentences has proved far harder. We propose using the definitions found in everyday dictionaries as a means of bridging this gap between lexical and phrasal se mantics. Neural language embedding models can be effectively trained to map dictionary definitions  to  representations of the words defined by those definitions. We present two applications of these architectures: reverse dictionaries that return the name of a concept given a definition or description and general-knowledge crossword question answerers. On both tasks, neural language embedding models trained on definitions from a handful of freely-available lexical resources perform as well or better than existing commercial systems that rely on significant task-specific engineering. The results highlight the effectiveness of both neural embedding architectures and definition-based training for developing mode  \citet{noraset} later introduced the task of definition modeling, in which a model is tasked with generating a definition for a given word, given its respective embedding. The authors argued that, compared to other related tasks such as word similarity or analogical relatedness, definition generation can be considered a more transparent view of the information captured by an embedding. However, this method does not incorporate contextual information, preventing it from generating appropriate definitions for polysemic words. Addresing this, \citet{gadetsky} studied the problem of polysemy in definition modeling, introducing an attention-based model which uses contextual information determine components in the embedding which may refer to a relevant word meaning.     The authors also released adefinition dataset extracted from the Oxford Dictionaries, which was larger in terms of number of unique words, to the data used by \citet{noraset}, while also supplementing each example with context sentences in which each example word is used.    GADETSKY   We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words ambiguity and polysemy leads to performance improvement.   - we introduce two models based on recurrent neural network  language models,     we collect new dataset of definitions  , which is larger in number of unique words than proposed in Noraset et al.  and also supplement it with examples of the word usage    -  finally, in the experiment section we show that our models outperform previously proposed models and have the ability to genearte definitions depending on the meaning of words.    NI \citet{niLearningExplainNonStandard2017a} explore a different but related problem, proposing an approach for automatically explaining non-standard English expressions  in a given sentence. They present a hybrid word-character sequence-to-sequence model that directly explains unseen non-standard expression, garnering reasonable definitions of expressions given their context.   . Unlike prior studies that focus on matching keywords from a slang dictionary, we investigate the possibility of learning a neural sequence-to-sequence model that generates explanations of unseen non-standard English expressions given context. We propose a dual encoder approach闁炽儲鏀 word-level encoder learns the representation of context, and a second character-level encoder to learn the hidden representation of the target non-standard expression. Our model can produce reasonable definitions of new non-standard English expressions given their context with certain confidence   - We present a large, publicly available corpus of non-standard English words and phrases, including 15 years of definitions and exam ples for each entry via crowdsourcing;    - We present a hybrid word-character sequence-to-sequence model that directly explains unseen non-standard expressions from social media;   - We present a hybrid word-character sequence-to-sequence model that directly explains unseen non-standard expressions from social media;  More recently, \citet{ishiwatari-etal-2019-learning} have tackled some of the limitations of previous works on definition modelling and non-standard English expression explanation. Concretely, they note that whenever it is not possible to figure out the meaning of a given expression from its immediate local context, it is common to consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. In light of this, they introduce the task of describing a given phrase in natural language, based on its local and global contexts. To tackle this the authors introduce a model which consists of two context encoders  as well as a description decoder. Our proposed model, uses a more practical variational encoder-decoder framework, allowing us to take advantage of explicitly modeling the phrase-definition relationship, while also leveraging deep contextualized word representations for more informative context representations.    ISHIWATARI   When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation  and definition generation , our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets  and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.  Finally, our model is also related to \citet{sohnLearningStructuredOutput2015}, in which Conditional Variational Autoencoders  ---an extension of the original variational autoencoder  --- were proposed for generating diverse structured output, mainly in the context of image generation, and visual object segmentation and labeling. Our work is also related to CVAE models that have been developed for the domain of natural language processing, specifically \citet{zhang-etal-2016-variational-neural} who proposed a CVAE in the context of Neural Machine Translation . As the usage of VAEs has become relatively common, we will omit a detailed explanation of these models, referring readers to \citet{kingmaAutoEncodingVariationalBayes2014}.    differences with original and other potential CVAE here",54
"  Topic segmentation is a fundamental NLP task that has received considerable attention in recent years .  It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units ,  and . The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization , question answering , machine reading  and dialogue modeling .   }  A Wikipedia sample article about City Marcus covering three topics: ,  and } \end{table}  A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps , Bayesian contexts  or   %the  semantic relatedness graphs  to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them. More recently, several works have framed topic segmentation as neural supervised learning, because of the remarkable success achieved by such models in most NLP tasks .  %While one line of research forms topic segmentation as a sequence labeling problem and builds neural models to predict segment boundaries directly ;  %another line of works first trains neural models for other tasks , and then uses these models' outputs to predict boundaries .  Despite %the  minor architectural differences, most of these neural solutions adopt Recurrent Neural Network  and its variants  as their main framework.  On the one hand, RNNs are appropriate because topic segmentation can be modelled as a sequence labeling task where each sentence is either the end of a segment or not. On the other hand, this choice makes these neural models limited in how to model the context. Because some sophisticated RNNs  are able to preserve long-distance information , which can largely help language models. But for topic segmentation, it is critical to supervise the model to focus more on the local context.    %In fact, RNNs are superior on many NLP tasks due to their capability of preserving long-distance information . %However, for topic segmentation, it is also critical to supervise the model to learn the right information from the local context.   As illustrated in Table, the prediction of the segment boundary between  and  hardly depends on the content in . Bringing in excessive long-distance signals may cause unnecessary noise and %further  hurt %model's  performance. Moreover, text coherence has strong relation with topic segmentation .  For instance, in Table, sentence pairs from the same segment  %should be  are more coherent %to put together than sentence pairs across segments .  Arguably, with a proper way of modeling the coherence between adjacent sentences, a topic segmenter can be further enhanced.   %\textcolor{red}{We hypothesize that topic segment prediction should rely on local contextual information in a way that cannot be effectively captured by RNNs.} %\textcolor{red}{In essence, RNNs are able to model long and short-distance dependencies only implicitly.}  %However, with restricted self-attention, our model can pay attention to the local context from the neighboring sentences in a more explicitly constrained way . %In essence, local contextual information is critical in predicting topical boundaries, but simple Recurrent Neural Network  and its variants are arguably not sufficiently powerful to represent the necessary information.  %However, both approaches still face the challenge of insufficient context modeling. Topic segment boundary prediction usually heavily relies on local contextual information. Hence, how to effectively select local contexts and model the relations between contexts becomes important. Neural models like RNN and its variants can represent the state of each timestep by memorizing or forgetting the information from its previous and later contexts. But how these learned contextual information contribute to model's decision is not straightforward and sufficiently transparent.  In this paper, we propose to enhance a state-of-the-art  topic segmenter  based on hierarchical attention BiLSTM network to better model the local context of a sentence in two complementary ways. First, we add a coherence-related auxiliary task to make our model learn more informative hidden states for all the sentences in a document.  %More specifically, we refine the objective of our model to encourage that the coherence of the sentences from different segments is smaller than the coherence of the sentences from the same segment.  More specifically, we refine the objective of our model to encourage smaller coherence for the sentences from different segments and larger coherence for the sentences from the same segment.  Secondly, we enhance context modeling by utilizing restricted self-attention , which enables our model to pay attention to the local context and make better use of the information from the closer neighbors of each sentence .  Our empirical results show  that our proposed context modeling strategy significantly improves the performance of the SOTA neural segmenter on three datasets,  that the enhanced segmenter is more robust in domain transfer setting when applied to four challenging real-world test sets, sampled differently from the training data,  that our context modeling strategy is also effective for the segmenters trained on other challenging languages , rather than just English.       \paragraph{Topic Segmentation}  In the literature,  Early unsupervised models exploit the lexical overlaps of sentences to measure the lexical cohesion between sentences or paragraphs . Then, by moving two sliding windows over the text, the cohesion between successive text units could be measured and a cohesion drop would signal a segment boundary. Even if these models do not require any training data, they only show limited performance in practice and are not general enough to handle the temporal change of the languages .  More recently, neural-based supervised methods have been devised for topic segmentation because of their more accurate predictions and greater efficiency. One line of research frames topic segmentation as a sequence labeling problem and builds neural models to predict segment boundaries directly. \citet{wang-etal-2016-learning} proposed a simple BiLSTM model to label if a sentence is a segment boundary or not. They demonstrated that along with engineered features based on cue phrases , their model can achieve marginally better performance than early unsupervised methods.  Later, \citet{koshorek-etal-2018-text} proposed a hierarchical neural sequence labeling model for topic segmentation and showed its superiority compared with their selected supervised and unsupervised baselines.  Around the same time, \citet{Badjatiya-2018} proposed an attention-based BiLSTM model to classify whether a sentence was a segment boundary or not, by considering the context around it. The work we present in this paper can be seen as pushing this line of research even further by encouraging the model to more explicitly consider contextual coherence,  relation by optimizing a coherence-related objective,  as well as to prefer more information from the neighbor context through restricted self-attention.  with a certain window size.  Another rather different line of works first trains neural models for other tasks, and then uses these models' outputs to predict boundaries. \citet{wang-etal-2017-learning} trained a Convolutional Neural Network  network to predict the coherence scores for text pairs. Sentences in a pair with large cohesion are supposed to belong to the same segment. However, their ``learning to rank"" framework asks for the pre-defined number of segments, which limits their model's applicability in practice.  on real-world issues.  Our selected framework overcomes this constraint by tuning a confidence threshold during  over model's  the training stage. A sentence with the output probability above this threshold will be predicted as the end of a segment. Following a very different approach, \citet{arnold2019sector} introduced a topic embedding layer into a BiLSTM model. After training their model to predict the sentence topics, the learned topic embeddings can be utilized for topic segmentation. However, one critical flaw of their method is that it requires a complicated pre-processing pipeline, which includes topic extraction and synset clustering, whose errors can propagate to the main topic segmentation task. In contrast, our proposal only requires the plain content of the training data without any complex pre-processing. but no other additional features, which also ensures its applicability to more real-world data.   \paragraph{Coherence Modeling} Early works on coherence modeling merely predict the coherence score for documents by tracking the patterns of entities' grammatical role transition . More recently, researchers started modeling the coherence for sentence pairs by their semantic similarities and used them for higher level coherence prediction or even other tasks, including topic segmentation.   { \citet{mesgar-strube-2018-neural} proposed a neural coherence model which first identified the salient semantics from adjacent sentence pairs to represent sentence relations. The document coherence score is then predicted based on the consecutive sentence relation shift. \citet{xu-etal-2019-cross} found that a document coherence was highly correlated with the summation of the coherence scores of all consecutive sentence pairs in this document. Therefore, they devised a discriminative coherence model only for sentence pairs. They applied this model to all the sentence pairs in a document and added them together to represent the overall coherence of this document.} \citet{wang-etal-2017-learning} demonstrated the strong relation between text-pair coherence modeling and topic segmentation. They assumed that  a pair of texts from the same document should be ranked more coherent than a pair of texts from different documents;  a pair of texts from the same segment should be ranked more coherent than a pair of texts from different segments of a document. With these assumptions, they created a ``quasi"" training corpus for text-pair coherence prediction by assigning different coherence scores to the texts from the same segment, different segments but the same document, and different documents. Then they proposed the corresponding model, and further use this model to directly conduct topic segmentation. Following their second assumption, we propose a neural solution in which by injecting a coherence-related auxiliary task, topic segmentation and sentence level coherence modeling can mutually benefit each other.   We agree with their hypothesis and believe that with a proper way of modeling, topic segmentation and sentence level coherence modeling can mutually benefit each other.     In this paper we have introduced a generative model that directly combines distributional and lexical semantics via a continuous latent variable for the task of definition modeling. Empirical results on multiple corpora, including two new datasets released, show that our model is able to outperform previous work by a consistent margin, also successfully being able to leveraging contextualized word representations. For future work we are interested in exploring how definition modeling could be adapted to a multilingual or cross-lingual setting.     where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets,  and the first non-English corpus , which we release to complement our empirical study. Our Variational Contextual Definition Modeler  achieves state-of-the-art performance in terms automatic and human evaluation metrics, demonstrating the effectiveness of our approach.     Conclude something here.  
","   \paragraph{Topic Segmentation}  In the literature,  Early unsupervised models exploit the lexical overlaps of sentences to measure the lexical cohesion between sentences or paragraphs . Then, by moving two sliding windows over the text, the cohesion between successive text units could be measured and a cohesion drop would signal a segment boundary. Even if these models do not require any training data, they only show limited performance in practice and are not general enough to handle the temporal change of the languages .  More recently, neural-based supervised methods have been devised for topic segmentation because of their more accurate predictions and greater efficiency. One line of research frames topic segmentation as a sequence labeling problem and builds neural models to predict segment boundaries directly. \citet{wang-etal-2016-learning} proposed a simple BiLSTM model to label if a sentence is a segment boundary or not. They demonstrated that along with engineered features based on cue phrases , their model can achieve marginally better performance than early unsupervised methods.  Later, \citet{koshorek-etal-2018-text} proposed a hierarchical neural sequence labeling model for topic segmentation and showed its superiority compared with their selected supervised and unsupervised baselines.  Around the same time, \citet{Badjatiya-2018} proposed an attention-based BiLSTM model to classify whether a sentence was a segment boundary or not, by considering the context around it. The work we present in this paper can be seen as pushing this line of research even further by encouraging the model to more explicitly consider contextual coherence,  relation by optimizing a coherence-related objective,  as well as to prefer more information from the neighbor context through restricted self-attention.  with a certain window size.  Another rather different line of works first trains neural models for other tasks, and then uses these models' outputs to predict boundaries. \citet{wang-etal-2017-learning} trained a Convolutional Neural Network  network to predict the coherence scores for text pairs. Sentences in a pair with large cohesion are supposed to belong to the same segment. However, their ``learning to rank"" framework asks for the pre-defined number of segments, which limits their model's applicability in practice.  on real-world issues.  Our selected framework overcomes this constraint by tuning a confidence threshold during  over model's  the training stage. A sentence with the output probability above this threshold will be predicted as the end of a segment. Following a very different approach, \citet{arnold2019sector} introduced a topic embedding layer into a BiLSTM model. After training their model to predict the sentence topics, the learned topic embeddings can be utilized for topic segmentation. However, one critical flaw of their method is that it requires a complicated pre-processing pipeline, which includes topic extraction and synset clustering, whose errors can propagate to the main topic segmentation task. In contrast, our proposal only requires the plain content of the training data without any complex pre-processing. but no other additional features, which also ensures its applicability to more real-world data.   \paragraph{Coherence Modeling} Early works on coherence modeling merely predict the coherence score for documents by tracking the patterns of entities' grammatical role transition . More recently, researchers started modeling the coherence for sentence pairs by their semantic similarities and used them for higher level coherence prediction or even other tasks, including topic segmentation.   { \citet{mesgar-strube-2018-neural} proposed a neural coherence model which first identified the salient semantics from adjacent sentence pairs to represent sentence relations. The document coherence score is then predicted based on the consecutive sentence relation shift. \citet{xu-etal-2019-cross} found that a document coherence was highly correlated with the summation of the coherence scores of all consecutive sentence pairs in this document. Therefore, they devised a discriminative coherence model only for sentence pairs. They applied this model to all the sentence pairs in a document and added them together to represent the overall coherence of this document.} \citet{wang-etal-2017-learning} demonstrated the strong relation between text-pair coherence modeling and topic segmentation. They assumed that  a pair of texts from the same document should be ranked more coherent than a pair of texts from different documents;  a pair of texts from the same segment should be ranked more coherent than a pair of texts from different segments of a document. With these assumptions, they created a ``quasi"" training corpus for text-pair coherence prediction by assigning different coherence scores to the texts from the same segment, different segments but the same document, and different documents. Then they proposed the corresponding model, and further use this model to directly conduct topic segmentation. Following their second assumption, we propose a neural solution in which by injecting a coherence-related auxiliary task, topic segmentation and sentence level coherence modeling can mutually benefit each other.   We agree with their hypothesis and believe that with a proper way of modeling, topic segmentation and sentence level coherence modeling can mutually benefit each other.",55
" Natural Language Understanding  evaluation plays a key role in benchmarking progress in natural language processing  research. With the recent advance in language representative learning, results on previous benchmarks have rapidly saturated. This leads to an explosion of difficult, diverse proposals of tasks/datasets for NLU evaluation, including Natural Language Inference , Grounded Commonsense Inference, Commonsense QA, Social Interactions Reasoning, Abductive Commonsense Reasoning , etc.   One common practice followed by most of these recent works is to simplify the evaluation of various reasoning abilities as a classification task. This is analogous to asking objective questions to a human in educational testing. This simplification not only facilitates the data annotation but also gives interpretable evaluation results, based on which behaviors of the models are studied and then weaknesses are diagnosed.   Despite the straightforwardness of this formalization, one assumption behind most prior benchmark data sourcing is that there exists a single prescriptive ground truth label for each example. The assumption might be true in human educational settings where prescriptivism is preferred over descriptivism because the goal is to test humans with well-defined knowledge or norms. However, it is not true for many NLP tasks due to their pragmatic nature where the meaning of the same sentence might differ depending on the context or background knowledge.   Specifically for the NLI task, \citet{manning2006local} advocate that annotation tasks should be ``natural'' for untrained annotators, and the role of NLP should be to model the inferences that humans make in practical settings. Previous work that uses a graded labeling schema on NLI, showed that there are inherent disagreements in inference tasks. All these discussions challenge the commonly used majority ``gold-label'' practice in most prior data collections and evaluations.  Intuitively, such disagreements among humans should be allowed because different annotators might have different subjective views of the world and might think differently when they encounter the same reasoning task. Thus, from a descriptive perspective, evaluating the capacity of NLP models in predicting not only individual human opinions or the majority human opinion, but also the overall distribution over human judgments provides a more representative comparison between model capabilities and `collective' human intelligence.  Therefore, we collect ChaosNLI, a large set of Collective HumAn OpinionS for examples in several existing  NLI datasets, and comprehensively examine the factor of human agreement  on the state-of-the-art model performances. Specifically, our contributions are:  The ChaosNLI dataset and experimental scripts are available at \url{https://github.com/easonnie/ChaosNLI}      \paragraph{Uncertainty of Annotations.} Past discussions of human disagreement on semantic annotation tasks were mostly focused on the uncertainty of individual annotators and the noisiness of the data collection process. These tasks include word sense disambiguation, coreference, frame corpus collection, anaphora resolution, entity linking, tagging and parsing, and veridicality. These works focused on studying the ambiguity of annotations, how the design of the annotation setup might affect the inter-annotator-agreement, and how to make the annotations reliable. However, we consider the disagreements and subjectivity to be an intrinsic property of the populations. Our work discusses the disagreements among a large group of individuals, and further examines the relation between the annotation disagreement and the model performance.   \paragraph{Disagreements in NLI Annotations.} Our work is significantly inspired by previous work that reveals the ``inherent disagreements in human textual inference"". It employed 50 independent annotators for a ``graded"" textual inference task, yielding a total of roughly 19,840 annotations, and validates that disagreements among the annotations are reproducible signals.  In particular, in their work, the labeling schema is modified from 3-way categorical NLI to a graded one, whereas our study keeps the original 3-way labeling schema to facilitate a direct comparison between old labels and new labels, and focuses more on giving an in-depth analysis regarding the relation between the level of disagreements among humans and the state-of-the-art model performance.   \paragraph{Graded Labeling Schema.} Some previous work attempts to address the issues with human disagreements by modifying or re-defining the evaluation task with a more fine-grained ordinal or even real-value labeling schema rather than categorical labeling schema  to reduce the issues of ambiguity. Our work is independent and complementary to those by providing analysis on general language understanding from a collective distribution perspective.       This work aims to establish a better way to represent the language modality in text-based ZSL for image classification. Our approach only relies on semantic information about visual features, and not on the visual features themselves. Specifically, our two orthogonal text-processing methods, employing textual similarity and visually-relevant summaries, lead to significant improvements across models, splits, and datasets, and illustrate that adequate text-processing is essential in text-based ZSL tasks.  We   conjecture that text-processing methods will be essential in a range of vision and language-based tasks, and hope this work will assist future research in better representing the language modality in various multi-modal tasks.    First, unsupervised clustering algorithms are used to construct textual similarity vectors between seen and unseen image-text pairs.   Second, visually relevant summaries  from texts. Each sentence from the image description is assigned a VRS-score, which determines the sentence's level of groundedness in images.   
","  \paragraph{Uncertainty of Annotations.} Past discussions of human disagreement on semantic annotation tasks were mostly focused on the uncertainty of individual annotators and the noisiness of the data collection process. These tasks include word sense disambiguation, coreference, frame corpus collection, anaphora resolution, entity linking, tagging and parsing, and veridicality. These works focused on studying the ambiguity of annotations, how the design of the annotation setup might affect the inter-annotator-agreement, and how to make the annotations reliable. However, we consider the disagreements and subjectivity to be an intrinsic property of the populations. Our work discusses the disagreements among a large group of individuals, and further examines the relation between the annotation disagreement and the model performance.   \paragraph{Disagreements in NLI Annotations.} Our work is significantly inspired by previous work that reveals the ``inherent disagreements in human textual inference"". It employed 50 independent annotators for a ``graded"" textual inference task, yielding a total of roughly 19,840 annotations, and validates that disagreements among the annotations are reproducible signals.  In particular, in their work, the labeling schema is modified from 3-way categorical NLI to a graded one, whereas our study keeps the original 3-way labeling schema to facilitate a direct comparison between old labels and new labels, and focuses more on giving an in-depth analysis regarding the relation between the level of disagreements among humans and the state-of-the-art model performance.   \paragraph{Graded Labeling Schema.} Some previous work attempts to address the issues with human disagreements by modifying or re-defining the evaluation task with a more fine-grained ordinal or even real-value labeling schema rather than categorical labeling schema  to reduce the issues of ambiguity. Our work is independent and complementary to those by providing analysis on general language understanding from a collective distribution perspective.",56
" Understanding and reasoning over natural language plays a significant role in artificial intelligence tasks such as Machine Reading Comprehension  and Question Answering . Several QA tasks have been proposed in recent years to evaluate the language understanding capabilities of machines . These tasks are single-hop QA tasks and consider answering a question given only one single paragraph. % The drawback of single-hop QA tasks is the lack of evaluating deep reasoning capability.  % We observe that many existing neural models achieve promising performance without reasoning.  Many existing neural models rely on learning context and type-matching heuristics. Those rarely build reasoning modules but achieve promising performance on single-hop QA tasks. The main reason is that these single-hop QA tasks are lacking a realistic evaluation of reasoning capabilities because they do not require complex reasoning.   Recently multi-hop QA tasks, such as HotpotQA  and WikiHop, have been proposed to assess multi-hop reasoning ability. HotpotQA task provides annotations to evaluate document level question answering and finding supporting facts. Providing supervision for supporting facts improves explainabilty of the predicted answer because they clarify the cross paragraph reasoning path.   Due to the requirement of multi-hop reasoning over multiple documents with strong distraction, multi-hop QA tasks are challenging.  Figure shows an example of HotpotQA. Given a question and 10 paragraphs, only paragraph  and paragraph  are relevant. The second sentence in paragraph  and the first sentence in paragraph  are the supporting facts. The answer is ``Geelong Football Club''.   Primary studies in HotpotQA task prefer to use a reading comprehension neural model. First, they use a neural retriever model to find the relevant paragraphs to the question. After that, a neural reader model is applied to the selected paragraphs for answer prediction. Although these approaches obtain promising results, the performance of evaluating multi-hop reasoning capability is unsatisfactory.   To solve the multi-hop reasoning problem, some models tried to construct an entity graph using Spacy or Stanford CoreNLP and then applied a graph model to infer the entity path from question to the answer. However, these models ignore the importance of the semantic structure of the sentences and the edge information and entity types in the entity graph. To take the in-depth semantic roles and semantic edges between words into account here we use semantic role labeling  graph as the backbone of a graph convolutional network. Semantic role labeling provides the semantic structure of the sentence in terms of argument-predicate relationships.  % such as ``who did what to whom.'' The argument-predicate relationship graph can significantly improve the multi-hop reasoning results. Our experiments show that SRL is effective in finding the cross paragraph reasoning path and answering the question.  Our proposed semantic role labeling graph reasoning network  jointly learns to find cross paragraph reasoning paths and answers questions on multi-hop QA. In SRLGRN model, firstly, we train a paragraph selection module to retrieve gold documents and minimize distractor. Second, we build a heterogeneous document-level graph that contains sentences as nodes ,  % and the sentence nodes include  and SRL sub-graphs including semantic role labeling arguments as nodes and predicates as edges. Third, we train a graph encoder to obtain the graph node representations that incorporate the argument types and the semantics of the predicate edges in the learned representations. Finally, we jointly train a multi-hop supporting fact prediction module that finds the cross paragraph reasoning path, and answer prediction module that obtains the final answer. Notice that both supporting fact prediction and answer prediction are based on contextual semantics graph representations as well as token-level BERT pre-trained representations. The contributions of this work are as follows:   {\bf 1)} We propose the SRLGRN framework that considers the semantic structure of the sentences in building a reasoning graph network. Not only the semantics roles of nodes but also the semantics of edges are exploited in the model.  {\bf 2)} We evaluate and analyse the reasoning capabilities of the semantic role labeling graph compared to usual entity graphs. %We analyze the multi-hop reasoning capacity on HotpotQA task.  The fine-grained semantics of SRL graph help in both finding the answer and the explainability of the reasoning path.  {\bf 3)} Our proposed model obtains competitive results on both HotpotQA  and the SQuAD benchmarks.       Previous QA datasets, such as TriviaQA  and SearchQA , and MRC datasets, like SQuAD , rarely require sophisticated reasoning  to answer the question and fail to provide ground-truth explanations for answers.  Recently, WikiHop and HotpotQA are two published multi-hop QA datasets that provide multiple paragraphs. Those QA datasets require a multi-hop reasoning model to learn the cross paragraph reasoning paths and predict the correct answer.  Most of the existing multi-hop QA models  utilize graph based neural networks, such as graph attention network , graph recurrent network , and graph convolutional network .  Moreover, multi-hop QA models use different ways to construct entity graphs. Coref-GRN  utilize co-reference resolution to build the entity graph.  MHQA-GRN  is an updated version of Coref-GRN that adds sliding windows. Entity-GCN  builds the graph using entities and different types of edges called match edges and complement edges. DFGN  and SAE  construct entity graph through named entity recognition .  In contrast to the above mentioned models, our SRLGRN builds a heterogeneous graph that contains a document-level graph of various sentences and replaces the entity-based graphs with argument-predicate based sub-graphs using SRL.        Learning representations has been widely deployed in numerous natural language processing tasks.   Pre-training embeddings can be divided into two main categories: standard word embedding, such as Word2Vec  and Glove , and contextualized embedding, including GPT  and BERT , etc.    However, the obstacle of BERT  or RoBERTa  is the memory limitation because of millions or billions of parameters. To address this issue, ALBERT  utilizes two technologies, factorized embedding parameterization and cross-layer parameter sharing, to lower memory consumption and increase the training speed of BERT.    The goal of semantic role labeling is to capture argument and predicate relationships given a sentence, such as 閳ユ辅ho did what to whom.閳  Several deep SRL models achieve highly accurate results in finding argument spans . However, those models are evaluated based on given gold predicates. Therefore, some deep models are proposed to recognize all argument-predicate pairs. Recently, \citeauthor{shi2019simple} proposed a BERT Model for SRL and Relation Extraction.    In this paper, we presented our approach on detecting and categorizing offensive language in social media. We proposed a multi-lingual learning method to detect offensive language and a knowledge distillation method to categorize offensive language. We will further our exploration of multilingual offensive language identification in future, e.g. validating the zero-shot performance of our model in more languages.       include your own bib file like this: 
","    Previous QA datasets, such as TriviaQA  and SearchQA , and MRC datasets, like SQuAD , rarely require sophisticated reasoning  to answer the question and fail to provide ground-truth explanations for answers.  Recently, WikiHop and HotpotQA are two published multi-hop QA datasets that provide multiple paragraphs. Those QA datasets require a multi-hop reasoning model to learn the cross paragraph reasoning paths and predict the correct answer.  Most of the existing multi-hop QA models  utilize graph based neural networks, such as graph attention network , graph recurrent network , and graph convolutional network .  Moreover, multi-hop QA models use different ways to construct entity graphs. Coref-GRN  utilize co-reference resolution to build the entity graph.  MHQA-GRN  is an updated version of Coref-GRN that adds sliding windows. Entity-GCN  builds the graph using entities and different types of edges called match edges and complement edges. DFGN  and SAE  construct entity graph through named entity recognition .  In contrast to the above mentioned models, our SRLGRN builds a heterogeneous graph that contains a document-level graph of various sentences and replaces the entity-based graphs with argument-predicate based sub-graphs using SRL.        Learning representations has been widely deployed in numerous natural language processing tasks.   Pre-training embeddings can be divided into two main categories: standard word embedding, such as Word2Vec  and Glove , and contextualized embedding, including GPT  and BERT , etc.    However, the obstacle of BERT  or RoBERTa  is the memory limitation because of millions or billions of parameters. To address this issue, ALBERT  utilizes two technologies, factorized embedding parameterization and cross-layer parameter sharing, to lower memory consumption and increase the training speed of BERT.    The goal of semantic role labeling is to capture argument and predicate relationships given a sentence, such as 闁炽儲杈卙o did what to whom.闁  Several deep SRL models achieve highly accurate results in finding argument spans . However, those models are evaluated based on given gold predicates. Therefore, some deep models are proposed to recognize all argument-predicate pairs. Recently, \citeauthor{shi2019simple} proposed a BERT Model for SRL and Relation Extraction.",57
"   The organizers of the 2020 VarDial Evaluation Campaign  proposed a shared task targeted towards the geolocation of short texts, e.g.~tweets, namely the Social Media Variety Geolocation  task. Typically formulated as a double regression problem, the task is about predicting the location, expressed in latitude and longitude, from where the text received as input was posted on a certain social media platform. Twitter and Jodel are the platforms used for data collection, divided by the language area in three subtasks, namely:   In this paper, we focus only on the second subtask, SMG-CH, proposing a variety of handcrafted and deep learning models, as well as an ensemble model that combines all our previous models through meta-learning. Our first model is a Support Vector Regression  classifier  based on string kernels, which are known to perform well in other dialect identification tasks . Our second model is a character-level convolutional neural network  , which is also known to provide good results in dialect identification . Due to the high popularity and the outstanding results of Bidirectional Encoder Representations from Transformers   in solving mainstream NLP tasks, we decided to try out a Long Short-Term Memory  network  based on German BERT embeddings as our third model. Lastly, we combine our three models into an ensemble that employs Extreme Gradient Boosting   as meta-learner. We conducted experiments on the development set provided by the organizers, in order to decide which models to choose for our three submissions for the SMG-CH subtask. Our results indicate that the ensemble model attains the best results. Perhaps surprisingly, our shallow approach based on string kernels outperforms both deep learning models. Our observations are consistent across the development and the test sets provided by the organizers.  % We experimented with a few Machine Learning algorithms for the second subtask, namely CH,  % Geolocation can be framed as a double regression task, but more sophisticated model architectures have been proposed .  % Jodel is a mobile chat application that lets people anonymously talk to other users within a 10km-radius around them.   % All three subtasks will use the same data format and evaluation methodology, and participants are encouraged to submit their systems for all subtasks.  The rest of this paper is organized as follows. We present related work on dialect identification and geolocation of short texts in Section. Our approaches are described in more detail in Section. We present the experiments and empirical results in Section. Finally, our conclusions are drawn in Section.      One of the initial works on text-based geotagging  aims at automatically finding the geographic scope of web pages, in a classification setup relying on named location entities such as cities and states. The authors used gazetteers as the source of the location mappings, proposing a rather heuristic approach. Gazetteers, constitute a tool used in one of the three general approaches taken so far in text-based geolocation, this tool being adopted in a number of works . In this line of research, some researchers employed rule-based methods , while others plugged named entity recognition into various machine learning techniques . The main disadvantage of these methods is that they rely on the existence of specific mentions of locations in text, rather than inferring them in a not so straightforward manner. These direct mentions of places do not represent a safe assumption, especially when it comes to social media platforms such as Twitter, which is used as the data source in some of these studies .  The other two main categories of approaches for text-based geolocation rely on either unsupervised learning  or supervised classification . The unsupervised methods can be described in large part as clustering techniques based on topic models.  There are some studies on user geolocation in social media, that look at this task from a supervised learning perspective  and can be included in the second set of approaches for geotagging. However, in such works, other details  in the users profile have been considered rather than their written content. Although these works cover geolocation prediction in social media, they do not use text as input. Our current interest in studying language variation for the geolocation of users in social media has been covered in the literature in a series of works , employing various machine learning techniques, that range from probabilistic graphical models  and adaptive grid search  to Bayesian methods  and neural networks .   The related work to date covers a wide range of languages and dialects, including Dutch , British , American  and even African American Vernacular English . Most related to our work is the study of , which targets the German language and its variations and, in addition to the previously mentioned endeavours, performs a quantitative analysis against a dialect map. Moreover,  collected 16.8 million online posts from the German-speaking area with the aim of learning document representations of cities. Among these posts, some were from the German speaking side of Switzerland, being part of the SMG shared task, more specifically the SMG-CH subtask that we are addressing. The authors aimed at capturing enough regional variations in the written language, serving as input in automatically distinguishing the geographical region of speakers. The focus was on larger regions covering a given dialect, the proposed approach being based on clustering. Given the shared task formulation, we take a different approach and use the provided data in a double regression setup, addressing the problem both from a shallow perspective and a deep learning perspective, respectively.    We proposed a novel semantic role labeling graph reasoning network  to deal with multi-hop QA.    The model jointly trains to detect the supporting facts and to find the final answer. The backbone graph of our proposed graph convolutional network  is created based on the semantic structure of the sentences. In creating the  edges and nodes of the graph, we exploit a semantic role labeling sub-graph for each sentence and connect the candidate supporting facts. The cross paragraph argument-predicate structure of the sentences expressed in the graph provides an explicit representation of the reasoning path and helps in both finding and explaining the multiple hops of reasoning that lead to the final answer.   We analyze the multi-hop reasoning ability of our model.  SRLGRN exceeds most of the SOTA results on the HotpotQA benchmark. Moreover, we evaluate the model  on other reading comprehension benchmarks. Our approach achieves competitive performance on SQuAD v and v.  
","   One of the initial works on text-based geotagging  aims at automatically finding the geographic scope of web pages, in a classification setup relying on named location entities such as cities and states. The authors used gazetteers as the source of the location mappings, proposing a rather heuristic approach. Gazetteers, constitute a tool used in one of the three general approaches taken so far in text-based geolocation, this tool being adopted in a number of works . In this line of research, some researchers employed rule-based methods , while others plugged named entity recognition into various machine learning techniques . The main disadvantage of these methods is that they rely on the existence of specific mentions of locations in text, rather than inferring them in a not so straightforward manner. These direct mentions of places do not represent a safe assumption, especially when it comes to social media platforms such as Twitter, which is used as the data source in some of these studies .  The other two main categories of approaches for text-based geolocation rely on either unsupervised learning  or supervised classification . The unsupervised methods can be described in large part as clustering techniques based on topic models.  There are some studies on user geolocation in social media, that look at this task from a supervised learning perspective  and can be included in the second set of approaches for geotagging. However, in such works, other details  in the users profile have been considered rather than their written content. Although these works cover geolocation prediction in social media, they do not use text as input. Our current interest in studying language variation for the geolocation of users in social media has been covered in the literature in a series of works , employing various machine learning techniques, that range from probabilistic graphical models  and adaptive grid search  to Bayesian methods  and neural networks .   The related work to date covers a wide range of languages and dialects, including Dutch , British , American  and even African American Vernacular English . Most related to our work is the study of , which targets the German language and its variations and, in addition to the previously mentioned endeavours, performs a quantitative analysis against a dialect map. Moreover,  collected 16.8 million online posts from the German-speaking area with the aim of learning document representations of cities. Among these posts, some were from the German speaking side of Switzerland, being part of the SMG shared task, more specifically the SMG-CH subtask that we are addressing. The authors aimed at capturing enough regional variations in the written language, serving as input in automatically distinguishing the geographical region of speakers. The focus was on larger regions covering a given dialect, the proposed approach being based on clustering. Given the shared task formulation, we take a different approach and use the provided data in a double regression setup, addressing the problem both from a shallow perspective and a deep learning perspective, respectively.",58
"  Comparing and contrasting the meaning of text conveyed in different languages is a fundamental nlp task. It can be used to curate clean parallel corpora for downstream tasks such as machine translation~, cross-lingual transfer learning, or semantic modeling~, and it is also useful to directly analyze multilingual corpora. For instance, detecting the commonalities and divergences between sentences drawn from English and French Wikipedia articles about the same topic would help analyze language bias~, or mitigate differences in coverage and usage across languages~. This requires not only detecting coarse content mismatches, but also fine-grained differences in sentences that overlap in content.  Consider the following English and French sentences, sampled from the WikiMatrix parallel corpus. While they share important content, highlighted words convey meaning missing from the other language:     We show that explicitly considering diverse types of semantic divergences in bilingual text benefits both the annotation and prediction of cross-lingual semantic divergences. We create and release the Rationalized English-French Semantic Divergences corpus , based on a novel divergence annotation protocol that exploits rationales to improve annotator agreement. We introduce \modelname, a  bert-based model that detects fine-grained semantic divergences without supervision by learning to rank synthetic divergences of varying granularity. Experiments on \dataset show that our model distinguishes semantically equivalent from divergent examples much better than a strong sentence similarity baseline and that unsupervised token-level divergence tagging offers promise to refine distinctions among divergent instances. We make our code and data publicly available.\footnote{Implementations of \modelname can be found at: \url{https://github.com/Elbria/xling-SemDiv}; the \dataset dataset is hosted at:   \url{https://github.com/Elbria/xling-SemDiv/tree/master/REFreSD}.}          Our work is closely related to but distinct from the Semantic Textual Similarity  task that measures the degree of equivalence in the underlying semantics of paired snippets of text. Most commonly, state-of-the-art models address the  task via interaction models that use alignment mechanisms to integrate word-level interactions in their final  predictions or via learning vector representations of sentences that are then compared using distance-based measures .      In the current work, we tackled the SMG-CH shared subtask of the 2020 VarDial Evaluation Campaign. We addressed this challenge from a shallow perspective, with handcrafted models such as a -SVR based on string kernels, as well as from a deep learning perspective, with neural models such as an LSTM based on BERT embeddings and a character-level CNN, respectively. Additionally, we combined the proposed models into an ensemble, employing the XGBoost meta-learner. We obtained our best results with the XGBoost ensemble, which benefits from complementary information from the handcrafted and deep models. We therefore brought one more proof regarding the effectiveness of ensemble learning in general, and of XGBoost, in particular. Another important conclusion is that our shallow model based on string kernels outperforms the two deep neural networks. We consider this as yet another indicator of the high discriminative power that string kernels can bring to a fairly standard learning model, i.e.~the -SVR.   In future work, we aim to explore ways to improve our performance with respect to the metrics proposed by the shared task organizers. Currently, it seems that training the models to simply minimize the MSE or the MAE values is not effective, as our best model was significantly outperformed by the model proposed by the shared task organizers themselves.  
","  Our work is closely related to but distinct from the Semantic Textual Similarity  task that measures the degree of equivalence in the underlying semantics of paired snippets of text. Most commonly, state-of-the-art models address the  task via interaction models that use alignment mechanisms to integrate word-level interactions in their final  predictions or via learning vector representations of sentences that are then compared using distance-based measures .",59
"   The recent advances in neural machine translation   have provided the research community and the commercial landscape with effective translation models that can at times achieve near-human performance. However, this usually holds at phrase or sentence level. When using these models in larger units of text, such as paragraphs or documents, the quality of the translation may drop considerably in terms of discourse attributes such as lexical and stylistic consistency.  In fact, document-level translation is still a very open and challenging problem. The sentences that make up a document are not unrelated pieces of text that can be predicted independently; rather, a set of sequences linked together by complex underlying linguistics aspects, also known as the discourse . The discourse of a document includes several properties such as grammatical cohesion , lexical cohesion , document coherence  and the use of discourse connectives . Ensuring that the translation retain such linguistic properties is expected to significantly improve its overall readability and flow.  However, due to the limitations of current decoder technology, NMT models are still bound to translate at sentence level. In order to capture the discourse properties of the source document in the translation, researchers have attempted to incorporate more contextual information from surrounding sentences. Most document-level NMT approaches augment the model with multiple encoders, extra attention layers and memory caches to encode the surrounding sentences, and leave the model to implicitly learn the discourse attributes by simply minimizing a conventional NLL objective. The hope is that the model will spontaneously identify and retain the discourse patterns within the source document. Conversely, very little work has attempted to model the discourse attributes explicitly. Even the evaluation metrics typically used in translation such as BLEU  are not designed to assess the discourse quality of the translated documents.  For these reasons, in this paper we propose training an NMT model by directly targeting two specific discourse metrics: lexical cohesion  and coherence . LC is a measure of the frequency of semantically-similar words co-occurring in a document  . For example, car, vehicle, engine or wheels are all semantically-related terms. There is significant empirical evidence that ensuring lexical cohesion in a text eases its understanding . At its turn, COH measures how well adjacent sentences in a text are linked to each other. In the following example from Hobbs \shortcite{hobbs1979coherence}:       the two sentences make little `sense' one after another. An incoherent text, even if grammatically and syntactically perfect, is anecdotally very difficult to understand and therefore coherence should be actively pursued. Relevant to translation, Vasconcellos \shortcite{vasconcellos1989cohesion} has found that a high percentage of the human post-editing changes over machine-generated translations involves the improvement of cohesion and coherence.  Several LC and COH metrics that well correlate with the human judgement have been proposed in the literature. However, like BLEU and most other evaluation metrics, they are discrete, non-differentiable functions of the model's parameters. Hereafter, we propose to overcome this limitation by using the well-established policy gradient approach from reinforcement learning  which allows using any evaluation metric as a reward without having to differentiate it. By combining different types of rewards, the model can be trained to simultaneously achieve more lexically-cohesive and more coherent document translations, while at the same time retaining faithfulness to the reference translation. %the information contained in the source document.  %The rest of the paper is organized as follows. Section  discusses related work. Section  describes the baseline NMT architectures used for the experiments. Section  presents the proposed training approach and the discourse rewards used with it. Section  presents the experiments and, finally, Section  concludes the paper.        NMT models have usually followed the encoder-decoder architecture based on recurrent neural networks  . An important characterestic, known as the attention mechanism, has later been added to improve the alignment of long sentences . Vaswani et al. \shortcite{vaswani2017attention} have proposed to replace the RNNs with self-attention networks which has improved the training speed and the accuracy of NMT models.     Many document-level NMT models have proposed taking the context into account by concatenating surrounding sentences or extra features to the current input sentence, with otherwise no modifications to the model. For example, Rios et al. \shortcite{rios2017improving} have trained an NMT model that learns to disambiguate words given the context semantic landscape by simply extracting lexical chains from the source document, and using them as additional features. Other researchers have proposed concatenating previous source and target sentences to the current source sentence, so that the decoder can observe a proper amount of context . Their work has shown that concatenating even just one or two previous sentences can result in a noticeable improvement. Mac{\'e} and Servan \shortcite{mace2019using} have added an embedding of the entire document to the input, and shown promising results in English-French.  Conversely, other document-level NMT approaches have proposed modifications to the standard encoder-decoder architecture to more effectively account for the context from surrounding sentences. Jean et al. \shortcite{jean2017does} have introduced a dedicated attention mechanism for the previous source sentences. Multi-encoder approaches with hierarchical attention networks have been proposed to separately encode each of the context sentences before they are merged back into a single context vector in the decoder . These models have shown significant improvements over sentence-level NMT baselines on many different language pairs. Kuang et al. \shortcite{kuang2017modeling} and Tu et al. \shortcite{tu2018learning} have proposed using an external cache to store, respectively, a set of topical words or a set of previous hidden vectors. This information has proved to benefit the decoding step at limited additional computational cost. In turn, Maruf and Haffari \shortcite{maruf2017document} have presented a model that incorporates two memory networks, one for the source and one for the target, to capture document-level interdependencies. For the inference stage, they have proposed an iterative decoding algorithm that incrementally refines the predicted translation.  However, all the aforementioned models assume that the model can implicitly learn the occurring discourse patterns. Moreover, the training objective is the standard negative log-likelihood  loss, which simply maximizes the probability of the reference target words in the sentence. Only one work these authors are aware of  has attempted to train the model by explicitly learning discourse attributes. Inspired by recent work in text generation , Xiong et al. \shortcite{xiong2019modeling} have proposed automatically learning neural rewards that can encourage translation coherence at document level. However, it is not clear whether the learned rewards would be in good correspondence with human judgment. For this reason, in our work we prefer to rely on established discourse metrics as rewards.     As a matter of fact, several metrics have been proposed in the literature to measure discourse properties. For LC, Wong and Kit \shortcite{wong2012extending} have proposed a metric that looks for repetitions of words and their related terms  by using WordNet . Gong et al. \shortcite{gong2015document} have proposed a similar metric that uses lexical chains. For COH, mainly two types of metrics have been proposed: entity-based and topic-based. The former follow the Centering Theory  which states that documents with a high frequency of the same salient entities are more coherent. An entity-based coherence metric was proposed by Barzilay and Lapata \shortcite{barzilay2008modeling}. At their turn, topic-based metrics assume that a document is coherent when adjacent sentences are similar in topic and vocabulary. Accordingly, Hearst \shortcite{hearst1997texttiling} has proposed the Texttiling algorithm which computes the cosine distance between the bag-of-word  vectors of adjacent sentences. Foltz et al. \shortcite{foltz1998measurement} have proposed to replace the BoW vectors with topic vectors. Li et al. \shortcite{li2016neural} have learned topic embeddings with a self-supervised neural network. There is also a third group of COH metrics that are based solely in syntactic regularities  that have also shown to be effective at modelling textual coherence. Other metrics have been proposed to measure different discourse properties such as grammatical cohesion  and discourse connectives .     Researchers in NMT and other natural language generation tasks have used reinforcement learning  techniques to train the models to maximize discrete sentence-level and document-level metrics as an alternative or a complement to the NLL. For example, Ranzato et al. \shortcite{ranzato2015sequence} have proposed training NMT systems targeting the BLEU score, showing consistent improvements with respect to strong baselines. In addition to training the model directly with the evaluation function, they claim that this approach mollifies the exposure bias problem . Expected risk minimization has been proposed as an alternative reinforcement learning-style training to maximize the sentence-level  and the document-level  BLEU scores. Paulus et al. \shortcite{paulus2017deep} have proposed a similar approach for summarization using ROUGE as the training loss . Tebbifakhr et al. \shortcite{tebbifakhr2019machine} have used a similar objective function to improve the sentiment classification of translated sentences. Finally, Edunov et al. \shortcite{edunov2017classical} have presented a comprehensive comparison of reinforcement learning and structured prediction losses for NMT model training.      We address the challenge of information fusion in the context of neural abstractive summarization by making crucial use of points of correspondence between sentences. We enrich Transformers with PoC information and report model performance on a new test bed for information fusion. Our findings suggest that modeling points of correspondence is crucial for effective sentence fusion, and sentence fusion remains a challenging direction of research. Future work may explore the use of points of correspondence and sentence fusion in the standard setting of document summarization. Performing sentence fusion accurately and succinctly is especially important for summarizing long documents and book chapters. These domains may contain more entities and events to potentially confuse a summarizer, making our method of explicitly marking these entities beneficial.   
","    NMT models have usually followed the encoder-decoder architecture based on recurrent neural networks  . An important characterestic, known as the attention mechanism, has later been added to improve the alignment of long sentences . Vaswani et al. \shortcite{vaswani2017attention} have proposed to replace the RNNs with self-attention networks which has improved the training speed and the accuracy of NMT models.     Many document-level NMT models have proposed taking the context into account by concatenating surrounding sentences or extra features to the current input sentence, with otherwise no modifications to the model. For example, Rios et al. \shortcite{rios2017improving} have trained an NMT model that learns to disambiguate words given the context semantic landscape by simply extracting lexical chains from the source document, and using them as additional features. Other researchers have proposed concatenating previous source and target sentences to the current source sentence, so that the decoder can observe a proper amount of context . Their work has shown that concatenating even just one or two previous sentences can result in a noticeable improvement. Mac{\'e} and Servan \shortcite{mace2019using} have added an embedding of the entire document to the input, and shown promising results in English-French.  Conversely, other document-level NMT approaches have proposed modifications to the standard encoder-decoder architecture to more effectively account for the context from surrounding sentences. Jean et al. \shortcite{jean2017does} have introduced a dedicated attention mechanism for the previous source sentences. Multi-encoder approaches with hierarchical attention networks have been proposed to separately encode each of the context sentences before they are merged back into a single context vector in the decoder . These models have shown significant improvements over sentence-level NMT baselines on many different language pairs. Kuang et al. \shortcite{kuang2017modeling} and Tu et al. \shortcite{tu2018learning} have proposed using an external cache to store, respectively, a set of topical words or a set of previous hidden vectors. This information has proved to benefit the decoding step at limited additional computational cost. In turn, Maruf and Haffari \shortcite{maruf2017document} have presented a model that incorporates two memory networks, one for the source and one for the target, to capture document-level interdependencies. For the inference stage, they have proposed an iterative decoding algorithm that incrementally refines the predicted translation.  However, all the aforementioned models assume that the model can implicitly learn the occurring discourse patterns. Moreover, the training objective is the standard negative log-likelihood  loss, which simply maximizes the probability of the reference target words in the sentence. Only one work these authors are aware of  has attempted to train the model by explicitly learning discourse attributes. Inspired by recent work in text generation , Xiong et al. \shortcite{xiong2019modeling} have proposed automatically learning neural rewards that can encourage translation coherence at document level. However, it is not clear whether the learned rewards would be in good correspondence with human judgment. For this reason, in our work we prefer to rely on established discourse metrics as rewards.     As a matter of fact, several metrics have been proposed in the literature to measure discourse properties. For LC, Wong and Kit \shortcite{wong2012extending} have proposed a metric that looks for repetitions of words and their related terms  by using WordNet . Gong et al. \shortcite{gong2015document} have proposed a similar metric that uses lexical chains. For COH, mainly two types of metrics have been proposed: entity-based and topic-based. The former follow the Centering Theory  which states that documents with a high frequency of the same salient entities are more coherent. An entity-based coherence metric was proposed by Barzilay and Lapata \shortcite{barzilay2008modeling}. At their turn, topic-based metrics assume that a document is coherent when adjacent sentences are similar in topic and vocabulary. Accordingly, Hearst \shortcite{hearst1997texttiling} has proposed the Texttiling algorithm which computes the cosine distance between the bag-of-word  vectors of adjacent sentences. Foltz et al. \shortcite{foltz1998measurement} have proposed to replace the BoW vectors with topic vectors. Li et al. \shortcite{li2016neural} have learned topic embeddings with a self-supervised neural network. There is also a third group of COH metrics that are based solely in syntactic regularities  that have also shown to be effective at modelling textual coherence. Other metrics have been proposed to measure different discourse properties such as grammatical cohesion  and discourse connectives .     Researchers in NMT and other natural language generation tasks have used reinforcement learning  techniques to train the models to maximize discrete sentence-level and document-level metrics as an alternative or a complement to the NLL. For example, Ranzato et al. \shortcite{ranzato2015sequence} have proposed training NMT systems targeting the BLEU score, showing consistent improvements with respect to strong baselines. In addition to training the model directly with the evaluation function, they claim that this approach mollifies the exposure bias problem . Expected risk minimization has been proposed as an alternative reinforcement learning-style training to maximize the sentence-level  and the document-level  BLEU scores. Paulus et al. \shortcite{paulus2017deep} have proposed a similar approach for summarization using ROUGE as the training loss . Tebbifakhr et al. \shortcite{tebbifakhr2019machine} have used a similar objective function to improve the sentiment classification of translated sentences. Finally, Edunov et al. \shortcite{edunov2017classical} have presented a comprehensive comparison of reinforcement learning and structured prediction losses for NMT model training.",60
" In recent years, neural models have led to state-of-the-art results in machine translation  . Many of these systems can broadly be characterized as following a multi-layer encoder-decoder neural network design: both the encoder and decoder learn representations of word sequences by a stack of layers , building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model , deep systems have shown promising BLEU improvements by either easing the information flow through the network  or constraining the gradient norm across layers . An improved system can even learn a 35-layer encoder, which is  deeper than that of vanilla Transformer .  Although these methods have enabled training deep neural MT  models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner . It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely deep model is expensive although a narrow-and-deep network can speed up training . For example, it takes us  longer time to train the model when we deepen the network from 6 layers to 48 layers. This might prevent us from exploiting deeper models in large-scale systems.  In this paper, we explore why deep architectures work to render learning NMT models more effectively. By investigating the change of the hidden states in different layers, we find that new representations are learned by continually stacking layers on top of the base model. More stacked layers lead to a stronger model of representing the sentence. This particularly makes sense in the deep NMT scenario because it has been proven that deep models can benefit from an enriched representation .  In addition, the finding here inspires us to develop a simple yet efficient method to train a deep NMT encoder: we train model parameters from shallow to deep, rather than training the entire model from scratch. To stabilize training, we design a sparse linear combination method of connecting lower-level layers to the top. It makes efficient pass of information through the deep network but does not require large memory footprint as in dense networks. We experiment with the method in a state-of-the-art deep Transformer system. Our encoder consists of 48-54 layers, which is almost the deepest Transformer model used in NMT. On WMT En-De and En-Fr tasks, it yields a  speedup of training, matching the state-of-the-art on the WMT'16 En-De task.      In this section, we discuss the related work from two aspects as follows:   In recent years, researchers gradually concentrate on building deep networks for Transformer . \citet{pham2019very} developed a 48-layer Transformer for speech recognition and adopted the stochastic residual connection to alleviate gradient vanishing/exploding problem.  \citet{bapna-etal-2018-training} demonstrated the challenge when training deep encoder models with vanilla Transformer on NMT task, due to the gradient vanishing or exploding. They also proposed a transparent attention mechanism to alleviate the problem. \citet{wang-etal-2019-learning} demonstrated the essential of layer-normalization in each layer and proposed the dynamic linear combination method to ease the information flow. Homochronously,  trained a 8-layer Transformer-Big with three specially designed components. More recently, \citet{wei2004multiscale} further enhanced the Transformer-Big up to 18 layers through a multiscale collaborative framework. In general, shortening the path from bottom to top can obtain consistent improvements in the aforementioned studies. On the other hand, researchers observed that proper initialization strategies without any structure adjustment can also ease the optimization of Post-Norm Transformer, which highlighted the importance of careful parameter-initialization .    When the model goes deeper, a challenge is the long training time for model convergence and the huge GPU cost. To alleviate this issue, several attempts have been made. \citet{ChangMHTB18} proposed a multi-level training method by interpolating a residual block right after each existing block to accelerate the training of ResNets in computer version. Similarly, \citet{GongHLQWL19} adopted a progressive stacking strategy to transfer the knowledge from a shallow model to a deep model, thus successfully trained a large-scale pre-training model BERT  at a faster rate with comparable performance on downstream tasks. Unlike previous work, we only copy parameters of the  top-most layers and employ sparse connections across each stacking block in our shallow to deep training method, which has not been discussed yet in learning deep MT models.        In this paper, we have presented a novel training method for document-level NMT models that uses discourse rewards to encourage the models to generate more lexically cohesive and coherent translations at document level. As training objective we have used a reinforcement learning-style function, named Risk, that permits using discrete, non-differentiable terms in the objective. Our results on four different language pairs and three translation domains have shown that our models have achieved a consistent improvement in discourse metrics such as LC and COH, while retaining comparable values of accuracy metrics such as BLEU and . In fact, on certain datasets, the models have even improved on those metrics. While the approach has proved effective in most cases, the best combination of discourse rewards, accuracy rewards and NLL has had to be selected by validation for each dataset. In the near future we plan to investigate how to automate this selection, and also explore the applicability of the proposed approach to other natural language generation tasks.  
","   In this section, we discuss the related work from two aspects as follows:   In recent years, researchers gradually concentrate on building deep networks for Transformer . \citet{pham2019very} developed a 48-layer Transformer for speech recognition and adopted the stochastic residual connection to alleviate gradient vanishing/exploding problem.  \citet{bapna-etal-2018-training} demonstrated the challenge when training deep encoder models with vanilla Transformer on NMT task, due to the gradient vanishing or exploding. They also proposed a transparent attention mechanism to alleviate the problem. \citet{wang-etal-2019-learning} demonstrated the essential of layer-normalization in each layer and proposed the dynamic linear combination method to ease the information flow. Homochronously,  trained a 8-layer Transformer-Big with three specially designed components. More recently, \citet{wei2004multiscale} further enhanced the Transformer-Big up to 18 layers through a multiscale collaborative framework. In general, shortening the path from bottom to top can obtain consistent improvements in the aforementioned studies. On the other hand, researchers observed that proper initialization strategies without any structure adjustment can also ease the optimization of Post-Norm Transformer, which highlighted the importance of careful parameter-initialization .    When the model goes deeper, a challenge is the long training time for model convergence and the huge GPU cost. To alleviate this issue, several attempts have been made. \citet{ChangMHTB18} proposed a multi-level training method by interpolating a residual block right after each existing block to accelerate the training of ResNets in computer version. Similarly, \citet{GongHLQWL19} adopted a progressive stacking strategy to transfer the knowledge from a shallow model to a deep model, thus successfully trained a large-scale pre-training model BERT  at a faster rate with comparable performance on downstream tasks. Unlike previous work, we only copy parameters of the  top-most layers and employ sparse connections across each stacking block in our shallow to deep training method, which has not been discussed yet in learning deep MT models.",61
"   Task-oriented dialogue systems complete tasks for users, such as making a hotel reservation or finding train routes, in a multi-turn conversation . The generated system utterances should not only be naturally sound, but more importantly be informative, i.e., to proceed the dialogue towards task completion. To fulfill this requirement, conditioned response generation is widely adopted based on system actions .  The response generation process is decoupled into two consecutive steps, where an action is first selected and then an utterance is generated conditioned on this action. One can optimize each step towards its goal, i.e., informative and naturally sound, without impinging the other . However, such approaches rely on action annotations , which require domain knowledge and extensive efforts to obtain.     %  \end{threeparttable} \end{table}    To deal with the absence of action annotations, latent action learning has been introduced .  System utterances are represented as low-dimensional latent variables by an auto-encoding task , and utterances with the same representations are considered to convey similar meanings.  Such action representations might be prone to over-dependence on the training data, which restricts the model generalization capability, especially when multiple domains are considered. % This is because the implicit nature of latent variables makes it unable to enforce the desired properties of the latent space, i.e., to capture the intentions of system utterances, without explicit supervision .  This is because, without explicit supervision, the desired property of capturing the intentions of system utterances in the latent space cannot be enforced , which in turn is due to the implicit nature of latent variables. For example, variational auto-encoder , which is often used for latent action learning, tends to produce a balanced distribution over the latent variables , while the true distribution of system actions is highly imbalanced . The resulting misaligned action representations would confuse the model of both steps and degenerate the sample efficiency in training.      % This is because without explicit supervision the desired property of capturing the intentions of system utterances in the latent space cannot be enforced , which in turn is due to the implicit nature of latent variables.   To address the above issues, we propose to learn natural language actions that represent system utterances as a span of words, which explicitly reveal the underlying intentions. % benefits of natural language actions Natural language provides unique compositional structure while retaining the representation flexibility. These properties promote model generalization and thus make natural language a 閾夸骏xible representation for capturing characteristics with minimal assumptions . % the main rationale to obtain such actions % In our scenarios, we aim to use language as the interface by  Motivated by these advantages, we learn natural language actions by identifying salient words of system utterances.  Salient refers to indicative for a prediction task  that takes as input the original utterance. % for the characteristics of utterances. The main rationale is that the principal information that the task concerns can be preserved by just the salient words. For example, the sentiment of sentence ``The movie starts out as competent but turn bland'' can be revealed by the word ``bland'' when it is identified salient by considering the complete context.   In our scenarios, we consider measuring word saliency in terms of state transitions. This is because state transitions reflect how the intentions of a system utterance influence the dialogue progress, and action representations that capture such influences can well reveal the intentions . By considering salient words for state tracking tasks as actions, we obtain action representations that enjoy the merits of natural language and indeed capture the characteristics of interest, i.e., intentions of system utterances. % explainable     % technical contributions Obtaining salient words by applying existing saliency identification approaches  is, however, unable to produce unified action representations. Specifically, system utterances with the same intention might not share similar wordings, and existing attribution approaches can only identify salient words within utterances. We tackle this challenge by proposing a memory-augmented saliency approach that identifies salient words from a broader vocabulary. The vocabulary consists of all the words that could compose natural language actions,~\footnote{We consider content words from state annotations and task descriptions, which will be specified in Sec. } and each word is stored as a slot in the memory component.  By incorporating the memory component into a dialogue state tracking model, we use each system utterance as a query to perform memory retrieval, and the retrieval results are considered as salient words. The retrieval results might contain words that are redundant since we do not have direct supervision for the retrieval operations. For example, the resulting salient words might be ``but turn bland'' in the example shown earlier, which include unnecessary words and may lead to degenerated action results.  To obtain compact action representations, we propose an auxiliary task based on pseudo parallel corpus, i.e., dialogue context and state annotation pairs.  We observe that dialogue states serve as good examples of how compact representation should be. Therefore, we use the encoded dialogue context as query and ask the memory component to reconstruct its text-based dialogue states. In this way, the obtained concise actions generalize better and can be easily interpreted.     Our contributions are summarized as follows:           Conditioned response generation aims to generate meaningful and fluent responses via intermediate meaning representations .  Early studies of conditioned response generation focus on enriching the meaning representations in task-oriented dialogues, e.g., utilizing graph structures and hierarchies among actions , decomposing into fine-grained actions , or encoding syntax attributes .  Since these approaches often assume expensive action annotations, recent years have seen a growing interest in learning latent actions in an unsupervised way . These approaches build on either adversarial learning  or variational inference  and encode all system utterances via a self-reconstruction task or distant supervision .  Due to their implicit nature, latent actions are difficult to generalize, and we aim to overcome this limitation by learning explicit action representations.    Our study is also related to attribution approaches, which aims to find features or regions of input that are important for tasks.  Different types of techniques, including gradient-based  and post-hoc , are applied for reinforcement learning , computer vision , and text classification . While these works focus on interpreting model behaviors, we aim to find salient words beyond input and utilize them as action representations.           We have investigated the behaviour of the well-trained deep Transformer models and found that stacking more layers could improve the representation ability of NMT systems. Higher layers share more global information over different positions and adjacent layers behave similarly. Also, we have developed a shallow-to-deep training strategy and employ sparse connections across blocks to ease the optimization. With the help of learning rate restart and appropriate initialization we successfully train a 48-layer RPR model by progressive stacking and achieve a  speedup on both WMT'16 English-German and WMT'14 English-French tasks. Furthermore, our -RPR-24L  achieves a BLEU score of  on WMT'16 English-German task, and speeds up the training by .  
","   Conditioned response generation aims to generate meaningful and fluent responses via intermediate meaning representations .  Early studies of conditioned response generation focus on enriching the meaning representations in task-oriented dialogues, e.g., utilizing graph structures and hierarchies among actions , decomposing into fine-grained actions , or encoding syntax attributes .  Since these approaches often assume expensive action annotations, recent years have seen a growing interest in learning latent actions in an unsupervised way . These approaches build on either adversarial learning  or variational inference  and encode all system utterances via a self-reconstruction task or distant supervision .  Due to their implicit nature, latent actions are difficult to generalize, and we aim to overcome this limitation by learning explicit action representations.    Our study is also related to attribution approaches, which aims to find features or regions of input that are important for tasks.  Different types of techniques, including gradient-based  and post-hoc , are applied for reinforcement learning , computer vision , and text classification . While these works focus on interpreting model behaviors, we aim to find salient words beyond input and utilize them as action representations.",62
"      Consider helping a friend prepare dinner in an unfamiliar house: when your friend asks you to clean and slice an apple for an appetizer, how would you approach the task? Intuitively, one could reason abstractly:  find an apple  wash the apple in the sink  put the clean apple on the cutting board  find a knife  use the knife to slice the apple  put the slices in a bowl. Even in an unfamiliar setting, abstract reasoning can help accomplish the goal by leveraging semantic priors. Priors like locations of objects --~apples are commonly found in the kitchen along with implements for cleaning and slicing, object affordances --~a sink is useful for washing an apple unlike a refrigerator, pre-conditions --~better to wash an apple before slicing it, rather than the converse. We hypothesize that, learning to solve tasks using abstract language,  unconstrained by the particulars of the physical world, enables agents to complete embodied tasks in novel environments by leveraging the kinds of semantic priors that are exposed by abstraction and interaction.      To test this hypothesis, we have created the novel \env framework, the first interactive, parallel environment that aligns text descriptions and commands with physically embodied robotic simulation. We build \env by extending two prior works: \tw~ - an engine for interactive text-based games, and \alfred~ - a large scale dataset for vision-language instruction following in embodied environments.  \env provides two views of the same underlying world and two modes by which to interact with it: \tw, an abstract, text-based environment, generates textual observations of the world and responds to high-level text actions;  \alfred, the embodied simulator, renders the world in high-dimensional images and responds to low-level physical actions as from a robot .\footnote{Note: Throughout this work, for clarity of exposition, we use \alfred{} to refer to both tasks and the grounded simulation environment, but rendering and physics are provided by \thor{}~.} Unlike prior work on instruction following , which typically uses a static corpus of cross-modal expert demonstrations, we argue that aligned parallel environments like \env offer a distinct advantage: they allow agents to explore, interact, and learn in the abstract environment of language before encountering the complexities of the embodied environment.  While fields such as robotic control use % simulators like MuJoCo~ to provide infinite data through interaction, there has been no analogous mechanism  -- short of hiring a human around the clock --  for providing linguistic feedback and annotations to an embodied agent.  \tw{} addresses this discrepancy by providing programmatic and aligned linguistic signals during agent exploration. This facilitates the first work, to our knowledge, in which an embodied agent learns the meaning of complex multi-step policies, expressed in language, directly through interaction.    Empowered by the \env framework, we introduce \model , an agent that first learns to perform abstract tasks in \tw using Imitation Learning  and then transfers the learned policies to embodied tasks in \alfred.  When operating in the embodied world, \model leverages the abstract understanding gained from \tw to generate text-based actions; these serve as high-level subgoals that facilitate physical action generation by a low-level controller. Broadly, we find that \model is capable of generalizing in a zero-shot manner from \tw to unseen embodied tasks and settings. Our results show that training first in the abstract text-based environment is not only  faster, but also yields better performance than training from scratch in the embodied world. These results lend credibility to the hypothesis that solving abstract language-based tasks can help build priors that enable agents to generalize to unfamiliar embodied environments.   Our contributions are as follows:\\[-15pt]                  The longstanding goal of grounding language learning in embodied settings~ has lead to substantial work on interactive environments. \env extends that work with fully-interactive aligned environments that parallel textual interactions with photo-realistic renderings and physical interactions.      Interactive Text-Only Environments: We build on the work of text-based environments like \tw~ and Jericho~. While these environment allow for textual interactions, they are not grounded in visual or physical modalities.  Vision and language: While substantial work exists on vision-language representation learning \eg~MAttNet~, CMN~, VQA~, CLEVR~, ViLBERT~, they lack embodied or sequential decision making.  Embodied Language Learning: To address language learning in embodied domains, a number of interactive environments have been proposed: BabyAI , Room2Room , \alfred , InteractiveQA , EmbodiedQA , and NetHack . These environments use language to communicate instructions, goals, or queries to the agent, but not as a fully-interactive textual modality.  Language for State and Action Representation:  Others have used language for more than just goal-specification.  use language as an intermediate state to learn policies in VizDoom.  Similarly,  and  use language as an intermediate representation to transfer policies across different environments.  use a natural language instructor to command a low-level executor, and   use language as an abstraction for hierarchical RL. However these works do not feature an interactive text environment for pre-training the agent in an abstract textual space.   use high-level commands similar to \env to solve tasks in THOR with IL and RL-finetuning methods, but the policy only generalizes to a small set of tasks due to the vision-based state representation.  Using symbolic representations for state and action is also an inherent characteristic of works in task-and-motion-planning~ and symbolic planning~.  World Models: The concept of using \tw as a ``game engine'' to represent the world is broadly related to inverse graphics~ and inverse dynamics~ where abstract visual or physical models are used for reasoning and future predictions. Similarly, some results in cognitive science suggest that humans use language as a cheaper alternative to sensorimotor simulation~.              We propose explicit action learning to achieve generalizable and interpretable dialogue generation.  Our proposed model MASP learns unified and compact action representations.     We propose a memory component that summarizes system utterances into natural language actions, i.e., spans of words from a unified vocabulary.  We further introduce an auxiliary task to encourage natural language actions to only preserve task-relevant information. Experimental results confirm that MASP achieves better performance compared with the state-of-the-art in different settings, especially when supervision is limited.  We plan to consider structural action representation learning that could convey more information as future work.  
","    The longstanding goal of grounding language learning in embodied settings~ has lead to substantial work on interactive environments. \env extends that work with fully-interactive aligned environments that parallel textual interactions with photo-realistic renderings and physical interactions.      Interactive Text-Only Environments: We build on the work of text-based environments like \tw~ and Jericho~. While these environment allow for textual interactions, they are not grounded in visual or physical modalities.  Vision and language: While substantial work exists on vision-language representation learning \eg~MAttNet~, CMN~, VQA~, CLEVR~, ViLBERT~, they lack embodied or sequential decision making.  Embodied Language Learning: To address language learning in embodied domains, a number of interactive environments have been proposed: BabyAI , Room2Room , \alfred , InteractiveQA , EmbodiedQA , and NetHack . These environments use language to communicate instructions, goals, or queries to the agent, but not as a fully-interactive textual modality.  Language for State and Action Representation:  Others have used language for more than just goal-specification.  use language as an intermediate state to learn policies in VizDoom.  Similarly,  and  use language as an intermediate representation to transfer policies across different environments.  use a natural language instructor to command a low-level executor, and   use language as an abstraction for hierarchical RL. However these works do not feature an interactive text environment for pre-training the agent in an abstract textual space.   use high-level commands similar to \env to solve tasks in THOR with IL and RL-finetuning methods, but the policy only generalizes to a small set of tasks due to the vision-based state representation.  Using symbolic representations for state and action is also an inherent characteristic of works in task-and-motion-planning~ and symbolic planning~.  World Models: The concept of using \tw as a ``game engine'' to represent the world is broadly related to inverse graphics~ and inverse dynamics~ where abstract visual or physical models are used for reasoning and future predictions. Similarly, some results in cognitive science suggest that humans use language as a cheaper alternative to sensorimotor simulation~.",63
"  Annual Reports may extend up to 250 pages long as stated above, which contains different sections General Corporate Information, financial and operating cost, CEOs message, Narrative texts, accounting policies, Financial statement including balance sheet and summary of financial data documents. In the Financial narrative summarisation task, only the narrative section is summarised, which is not explicitly marked in the dataset, making it challenging and interesting.  In recent years, previous manual small-scale research in the Accounting and Finance literature has been scaled up with the aid of NLP and ML methods, for example, to examine approaches to retrieving structured content from financial reports, and to study the causes and consequences of corporate disclosure and financial reporting outcomes . \par Companies produce glossy brochures of annual reports with a much looser structure, and this makes automatic summarisation of narratives in UK annual reports a challenging task . Hence we summarize the narrative section of annual reports, particular narrative sentences that are spread loosely across the document need to be first identified and summarise those sentences. The summarisation limit is set to 1000 words, where the actual length of the report may go up to 250 pages long. Hence to summarize these long annual reports using a combination of extractive and abstractive summarisation.\par The text summary method can be classified into two paradigms: extractive and abstractive. The extractive summarisation method extracts the meaningful sentences or a section of text from the original text and combines them  to form a summary . Whereas abstractive summarisation generates words and sentences that are similar in meaning to the given text to form a summary that may not be in actual text . When summarizing long documents such as in our case up to 250 pages long, extractive summarisation may not produce a coherent and readable summary, and abstractive summarisation cannot cover complete information using encoder-decoder architecture. One problem is that typical seq2seq frameworks often generate unnatural summaries consisting of repeated words or phrases . Hence, we come up with a combination of extractive and abstractive summarisation to first select important narrative sentences and concisely convey them. \par Pointer Networks  is used in various combinatorial optimization problems, such as Travelling Salesman Problem , Convex hull optimization. We used pointer networks in our task of financial narrative summarization to extract relevant narrative sentences in a particular order to have a logical flow in summary. These extracted sentences are paraphrased to summarise these sentences in an abstractive way using the T-5 sequence-to-sequence model. We train the complete model by optimizing the ROUGE-LCS evaluation metric through a reinforcement learning objective.   % % The following footnote without marker is nebe fireded for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version             % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. }    In this section and Appendix , we discuss related works in the fields of abstractive summarisation, extractive summarisation, combinations of these two methods, reinforcement learning applications, and summarisation of Financial Narratives and their methodology. Studies of human summarizers show that it is common to apply various operations while condensing, such as paraphrasing, generalization, sentence-level summarisation and reordering . We continue related work in Appendix .\par    We introduced \env, the first interactive text environment with aligned embodied worlds. \env allows agents to explore, interact, and learn abstract polices in a textual environment. Pre-training our novel \model agent in \tw, we show zero-shot  generalization to embodied tasks in the \alfred dataset. The results indicate that reasoning in textual space allows for better generalization to unseen tasks and also faster training, compared to other modalities like vision.   \model is designed with modular components which can be upgraded in future work. Examples include the template-based state-estimator and the A* navigator which could be replaced with learned modules, enabling end-to-end training of the full pipeline. Another avenue of future work is to learn ``textual dynamics models'' through environment interactions, akin to vision-based world models~. Such models would facilitate construction of text-engines for new domains, without requiring access to symbolic state descriptions like PDDL. Overall, we are excited by the challenges posed by aligned text and embodied environments for better cross-modal learning.    
","  In this section and Appendix , we discuss related works in the fields of abstractive summarisation, extractive summarisation, combinations of these two methods, reinforcement learning applications, and summarisation of Financial Narratives and their methodology. Studies of human summarizers show that it is common to apply various operations while condensing, such as paraphrasing, generalization, sentence-level summarisation and reordering . We continue related work in Appendix .\par",64
"   Neural Architecture Search  methods aim to automatically discover neural architectures that perform well on a given task and dataset. These methods search over a space of possible model architectures, looking for ones that perform well on the task and will generalize to unseen data. There has been substantial prior work on how to define the architecture search space, search over that space, and estimate model performance .    Recent works, however, cast doubt on the quality and performance of NAS-optimized architectures , showing that current methods fail to find the best performing architectures for a given task and perform similarly to random architecture search.  In this work, we explore applications of a SOTA NAS algorithm, ENAS , to two sentence-pair tasks, paraphrase detection  and semantic textual similarity . We conduct a large set of experiments testing the effectiveness of ENAS-optimized RNN architectures across multiple models , embeddings  and datasets . We are the first, to our knowledge, to apply ENAS to PD and STS, to explore applications across multiple embeddings and traditionally LSTM-based NLP models, and to conduct extensive SOTA HPT across multiple ENAS-RNN architecture candidates.   Our experiments suggest that baseline LSTM models, with appropriate hyperparameter tuning , can sometimes match or exceed the performance of models with ENAS-RNNs. We also observe that random architectures sampled from the ENAS search space offer a strong baseline, and can sometimes outperform ENAS-RNNs. Given these observations, we recommend that researchers  conduct extensive HPT  across various candidate architectures for the fairest comparisons;  compare the performances of ENAS-RNNs against both standard architectures like LSTMs and RNN cells randomly sampled from the ENAS search space;  examine the computational  requirements of ENAS methods alongside the gains observed.       NAS methods have shown strong performance on many NLP and CV tasks, such as language modeling and image classification . Applications in NLP, such as NER , translation , text classification , and natural language inference   have also been explored.   Current SOTA approaches focus on learning new cell architectures as replacements for LSTM or convolutional cells  or entire model architectures to replace hand-designed models such as the transformer or DenseNet .  Recently, the superiority of NAS to random architecture search and traditional architectures with SOTA HPT methods has been called into question. \citet{Li2019RandomSA} discuss reproducibility issues with current NAS methods and find that, on language modeling and image classification tasks, NAS algorithms perform similarly to random architecture search. Similarly, \citet{Sciuto2020EvaluatingTS} find minimal differences in performance between NAS and random search and that the popular weight-sharing strategy  decreases performance.  With this in perspective, we conduct a study to investigate the value added by ENAS to two NLP tasks, PD and STS, which, to our knowledge, have not been been explored in previous NAS literature.    In this work we present our solution on Financial Narrative Summarisation dataset using PoinT-5 method explained in . It is combination of both extractive and abstractive methods using Pointer Network and T-5. With these methods we are able to achieve highest precision score in every evaluation metric and achieve highest F-1 scores in ROUGE-LCS and ROUGE-1.\par In our future work we would like to address several limitation of our method such as factual correctness in summaries which is very important in financial domain as done in  in summarizing radiology reports. To improve precision of our generated summaries under 1000 words we would formulate a penalty if system generates more than 1000 words during training of RL algorithm rather than restricting algorithm to fixed number of sentences.     include your own bib file like this: 
","   NAS methods have shown strong performance on many NLP and CV tasks, such as language modeling and image classification . Applications in NLP, such as NER , translation , text classification , and natural language inference   have also been explored.   Current SOTA approaches focus on learning new cell architectures as replacements for LSTM or convolutional cells  or entire model architectures to replace hand-designed models such as the transformer or DenseNet .  Recently, the superiority of NAS to random architecture search and traditional architectures with SOTA HPT methods has been called into question. \citet{Li2019RandomSA} discuss reproducibility issues with current NAS methods and find that, on language modeling and image classification tasks, NAS algorithms perform similarly to random architecture search. Similarly, \citet{Sciuto2020EvaluatingTS} find minimal differences in performance between NAS and random search and that the popular weight-sharing strategy  decreases performance.  With this in perspective, we conduct a study to investigate the value added by ENAS to two NLP tasks, PD and STS, which, to our knowledge, have not been been explored in previous NAS literature.",65
" 	 	Although neural machine translation  has achieved great progress in recent years , when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation  methods are proposed to utilize source-side or target-side inter-sentence contextual information to improve translation quality over sentences in a document . 	 	More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena . However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? 	 	 			 			 		\end{center} 	\end{table} 	 	 	We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table , we train two DocNMT models and test them using various context settings\footnote{We apply a typical DocNMT method  to train models on ZhEn TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06.}. During the test, we obtain dynamic context sentences that achieve the best BLEU scores by traversing all the context combinations for each source sentence. Compared with the fixed size context , dynamic context  can significantly improve translation quality. Although row 2 uses more context, redundant information may hurt the results. Experiments indicate that only the limited context sentences are really useful, and they change with source sentences. 	 	Majority of existing DocNMT models set the context size or scope to be fixed. They utilize all of the 	previous  context sentences , or the full context in the entire document . As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, \citet{maruf2019selective} propose a selective attention approach that uses the sparsemax function  instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the model can focus on the sentences with high probability. However, the learning of attention weights lacks guidance, and they cannot handle the situation where the source sentences achieve the best translation results without relying on any context, which happens in about 39.4\% of sentences in the experiment. 	 	To address the problem, we propose an effective approach to select contextual sentences {\bf dynamically} for each source sentence in the document-level translation. Specifically, we propose a Context Scorer to score each candidate context sentence according to the currently translated source sentence. Then, we utilize two selection strategies to select useful context sentences for the translation module. The size of selected context is variable for different sentences. A core challenge of our approach is that the selection process is non-differentiable. Therefore, we leverage the reinforcement learning  method to train the selection and DocNMT modules together. We design a novel reward to encourage the model to be aware of different context sentences and select more appropriate context to improve translation quality. 	 	In this paper, we make the following contributions: 	 	 	  	Standard neural machine translation methods usually focus on the sentence-level translation . As a contrast, document-level neural machine translation methods mainly pay attention to how to utilize the cross-sentence context. Researchers propose various context-aware networks to utilize contextual information to improve the performance of DocNMT models on the translation quality  or discourse phenomena . However, most methods roughly leverage all context sentences in a fixed size that is tuned on development sets   , or full context in the entire document . They ignore the individualized needs for context when translating different source sentences. 	 	Some works have noticed that not all context is useful . \citet{kimura-etal-2019-selecting} explore the context selection in the single-encoder framework , and select context sentences that yield highest forced back-translation probability. However, the method cannot optimize DocNMT model at training phase, and requires back-translation model at inference phrase. \citet{maruf2019selective} sharpen the attention weights between the source and context sentences through the sparsemax function, and implicitly select context with high attention weights. Nevertheless, the method lacks direct supervision over context selection, and it cannot cover the situation where context is not needed. Inspired by the extractive-abstractive summarization , our approach is different from above DocNMT methods. Our approach can explicitly select dynamic size  of context sentences for the translation of different source sentences. 	 	    We provide empirical evidence on the ability of self-attention  networks to learn generalized  languages. We compare the performance of two SA networks, SA and SA, which differ only in the inclusion of a starting symbol in their vocabulary.  We demonstrate that a simple addition of the starting symbol helps SA generalize to sequences that are longer and have higher depths.  The competitive performance of SA  against LSTMs might seem surprising, considering that the recognition of  languages is an inherently hierarchical task. From our experiments, we conclude that recognizing Dyck languages is not tied to recursion, but rather learning the right representations to look up the head token. Further, we find that the representations learned by SA are highly interpretable and the network performs computations similar to a stack automaton. Our results suggest formal languages could be an interesting avenue to explore the interplay between performance and interpretability for SA. Comparisons between SA and LSTM reveal interesting contrast between the two architectures which calls for further investigation. Recent work  shows how to express the Transformer as an RNN through linearization of the attention mechanism, which could lay grounds for more theoretical analysis of these neural architectures           \setcounter{section}{0} \import{}{supp_arxiv}     
"," 	Standard neural machine translation methods usually focus on the sentence-level translation . As a contrast, document-level neural machine translation methods mainly pay attention to how to utilize the cross-sentence context. Researchers propose various context-aware networks to utilize contextual information to improve the performance of DocNMT models on the translation quality  or discourse phenomena . However, most methods roughly leverage all context sentences in a fixed size that is tuned on development sets   , or full context in the entire document . They ignore the individualized needs for context when translating different source sentences. 	 	Some works have noticed that not all context is useful . \citet{kimura-etal-2019-selecting} explore the context selection in the single-encoder framework , and select context sentences that yield highest forced back-translation probability. However, the method cannot optimize DocNMT model at training phase, and requires back-translation model at inference phrase. \citet{maruf2019selective} sharpen the attention weights between the source and context sentences through the sparsemax function, and implicitly select context with high attention weights. Nevertheless, the method lacks direct supervision over context selection, and it cannot cover the situation where context is not needed. Inspired by the extractive-abstractive summarization , our approach is different from above DocNMT methods. Our approach can explicitly select dynamic size  of context sentences for the translation of different source sentences.",66
"  \vsec Automatic text summarization\footnote{We refer to abstractive summarization in this paper.} is an attractive technique for helping humans to grasp the content of documents effortlessly. While supervised neural methods have shown good performances, the unsupervised approach is starting to attract interest due to its advantage of not requiring costly parallel corpora. However, the empirical performance of unsupervised methods is currently behind that of state-of-the-art supervised models. Unsupervised text summarization is still developing and is now at the stage where various solutions should be actively explored.     One previous unsupervised approach extends neural encoder-decoder modeling to the zero paired data scenario, where a model is trained with a paradigm called compression-reconstruction  learning. The mechanism is similar to that of the back-translation: the model consists of a compressor  and a reconstructor, and they are co-trained so that the reconstructor can recover the original sentence from the summary generated by the compressor~. Experimental results showed that such an unsupervised encoder-decoder-based summarizer is able to learn the mapping from a sentence to a summary without paired data. % Also, \citealp{zhou-rush-2019-simple} proposes a more straightforward method that mimics the reconstruction part by means of contextual similarity between an original input sentence and a top of a generating summary. % However, the performance of any unsupervised methods is still deficient compared to the latest supervised models.   Reinforcement learning  is also a potential solution for the no paired data situation. In related fields, for example, there are unsupervised methods for text simplification and text compression with policy-gradient learning. Recent RL techniques take a value-based approach  such as DQN or the combination of policy and value-based approaches such as Asynchronous Advantage Actor-Critic. A critical requirement to leverage a value-based method is a value function that represents the goodness of an action on a given state. We can naturally define the value function by utilizing the CR-learning paradigm, and it makes the latest value-based approaches available for unsupervised text summarization. % , and they require to define value-function. % We can leverage the values-based approach  % A crucial requirement for RL is a value function that represents a goodness of action on a given state. % We can satisfy the requirement by leveraging the definition in CR learning paradigm. % One concern is, however, that RL with large action space   generally has difficulty in the training. % In addition, the latest techniques to improve RL are from a value-based approach  such as DQN or the combination of policy-based and value-based approaches such as Asynchronous Advantage Actor-Critic.   In this paper, we propose a new method based on Q-learning and an edit-based summarization~. The edit-based summarization generates a summary by operating an edit action  for each word in the input sentence. Our method implements the editing process with two modules: 1) an {\bf E}ditorial {\bf A}gent that predicts edit actions, and 2) a {\bf L}anguage {\bf M}model  converter that deterministically decodes a sentence on the basis of action signals, which we call \ealm. The CR learning is defined on the Q-learning framework to train the agent to predict edit actions that instruct the LM converter to produce a good summary. Although a vast action space causing sparsity in reward, such as the word generation of an encoder-decoder model, is generally difficult to be learned in RL, our method mitigates this issue thanks to its fewer edit actions and the deterministic decoding of a language model. Moreover, the formulation by Q-learning enables us to incorporate the latest techniques in RL.  The main contribution of this paper is that we provide a new solution in the form of an unsupervised edit-based summarization leveraging Q-learning and a language model. Experimental results show that our method achieved a competitive performance with encoder-decoder-based methods even with truly no paired data , and qualitative analysis brings insights as to what current unsupervised models are missing. Also, the problem formulation on Q-learning enables us to import the latest techniques in RL, which leads to potential improvements in future research.  % 2) We propose the first Q-learning-based method that uses a pre-trained language model. % , which mitigates the issues prevalent among the previous methods. % Empirically, our method shows a competitive performance in the news corpus benchmarks with truly no paired data . % Also, our method requires no parallel data even for validation; therefore, it can be instantly applicable to any situation if there is a language model.  % Our proposed approach brings new insights to the growing field of unsupervised text summarization, and will pave the way to future development.  % This paper is organized as follows: Section defines the problem statement of unsupervised text summarization with the \algoname\ paradigm. % After reviewing the previous methods in Section, we introduce our approach in Section . % Then, we report experimental results in Section . % Discussing insights from the experiment in Section , we conclude the contribution of this paper for future unsupervised text summarization in Section .    % Text Summarization is a task to transform an input sentence into an informative summary . % Although supervised summarization models like encoder-decoder have shown success for these years , it still has an issue to demand us to create massive parallel data. % The question ``how we can model the transformation only from the input sentences?"" attracts research interests, and known as unsupervised text summarization .  % In unsupervised text summarization, only the input sentences are available for training a model. % Instead, it holds a hypothesis: a summary should contain information about its input sentence to some extent that we can guess the original contents. % And, lgoname is an approach to leverage this hypothesis .  % In \algoname, we prepare two modules, the one for compression that produces the summary  from the input sentence, and the other one for reconstruction that re-produces the input sentence from the generated summary. % These two modules are optimized based on the hypothesis, more specifically, minimizing the difference between the input sentence and the reconstructed sentence while the compressed sentence  is satisfying essential properties such as shortness or readability . % In previous studies, they use generative models such as encoder-decoder for compression and reconstruction, and directly train them to output desired sentences . % We illustrate the flow in the left-hand side of Figure .  % Our proposed method is also on top of the same paradigm but uses different modules, {\bf Q-learning agent} and {\bf fixed-language model} .\footnote{A pretrained language model that is not fine-tuned, i.e., fixed, during training.} % As illustrated in the right-hand side of Figure , the agent determines action, whether to remove, keep, or replace each word in the input sentence. % Receiving the action signals, the fixed-LM deterministically produces compressed and reconstructed sentences. % In short, we train the agent to properly control the fixed-LM so that we obtain desired sentences as the results of compression and reconstruction.  % The primary contribution of this paper is to provide a new option leveraging Q-learning with a language model to the growing field of unsupervised text summarization. % Introducing Q-learning, we open the problem to sophisticated techniques on value-based Reinforcement Learning  algorithms , which is not covered only with policy-based RL algorithms employed so far.\footnote{RL algorithms are classified into value-based  and policy-based . To the best of our knowledge, most of the text summarization methods with RL, both in supervised and unsupervised settings, leverages policy-based RL algorithms . Combining such a previous policy-based and our value-based methods for sentence compression will lead to the applicability of more advanced RL algorithms such as Actor-Critic  and Asynchronous Advantage Actor-Critic .} % Also, proposing an approach to fixedly utilize the pre-trained language model, we benefit from its powerful performance capturing sentence semantics along with mitigating issues generative models inherently hold such as complexity in co-training of multiple generators or repetition in decoding. % Experimentally, our approach shows promising results; it achieves competitive performance in standard datasets and outperforms the previous generator models in out-of-domain circumstances. % This paper brings novel insights for unsupervised text summarization and contributes to be flourishing in the future.  % This paper is organized as follows: Section defines the problem statement of unsupervised text summarization with the \algoname\ paradigm. % After reviewing the previous methods in Section, we introduce our approach in Section . % Then, we report experimental results in Section . % Discussing insights from the experiment in Section , we conclude the contribution of this paper for future unsupervised text summarization in Section .  \vsecu    \vsec  Our proposed method, which we call \ealm, consists of two essential modules: the editorial agent and the \lmconv. The agent sends action signals  to the conveter, which then deterministically transforms the input sentence according to the signals. We train the agent to find action signals so that the \lmconv~produces sentences demanded by the \algoname. In the following sections, we first share the background of Q-learning  and then present how to put the task and our approach on the Q-learning framework . We next explain the core algorithmic details  and finish with explanations about training and inference .    \vsubsec Q-learning is a popular approach in RL as represented by Deep Q-Networks . Q-learning leverages an action-value function to estimate the value of a pair of state and action with respect to a policy . The action-value function  is represented as the expected reward for the state-action pair:     \vskip -16pt \[     Q^\pi = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t} r \mid s_0=s, a_0=a \right], \]   \vskip -3pt   where  is a state,  is an action,  is a reward function for the state-action pair, and  is the discount factor. Hence, to solve a text summarization task via Q-learning, we first need to appropriately define the state, action, and reward function.   Hence, to solve a text summarization task via Q-learning, we first need to define the state , action , and reward , appropriately.   We formally represent our approach as the Q-learning task in the next section.     In our approach, given an input sentence , we define a state  in regard to each word . An action  for the state  is chosen from among the three options, .    Q, \]   \vskip -5pt   where  is defined as . The agent then reiterates the predictions until it finishes determining an action on all words. By defining the state in regard to a single word instead of a whole sentence and asking the agent to determine the prediction order, we can handle variable sentence lengths in natural language. Note that this is not a left-to-right process; the agent conducts the prediction in the order of ``confidence"".   i.e., .    Subsequently, let us describe how we create the state .    In terms of creating the state , we dynamically obtain it by  two-layer encoding that we call local encoding  and global encoding .   We represent a state  via the concatenation of two types of word embeddings that we call local encoding  and global encoding .   In terms of creating the state , we dynamically obtain it by the concatenation of two type encodings that we call local encoding  and global encoding . Next we explain how to encode . To send the agent contextual information, such as the previous decisions, the prediction statuses, and the whole sentence, we dynamically create a state  with a concatenation of two encodings; local encoding  and global encoding      \vskip 0pt \[     s_{i} = [\localenc_{i}; \globalenc_{i}]\text{.} \]   To create the two encodings, first, we map  to a fixed-sized vector  with an arbitrary encoder , and  is repeatedly used throughout the process regardless of the steps. Then, we define the local encoding as \[ \localenc_{i} = \bm{e}_i + \bm{b}^{a_{i}} + \bm{b}^{u_{i}}\text{, } \] where  and  are learnable bias vectors for the action and prediction status of the -th word, respectively.     \vskip 0pt   \[       \localenc_{i} = \bm{e}_i + \bm{b}^{a_{i}} + \bm{b}^{u_{i}}   \]   \vskip 0pt   The all elements in the bias vector have a same trainable value corresponded to the action  or the status of prediction .   All elements of the bias vector are filled with same value corresponded to the superscription .    has same size with  and the all elements have same value corresponding to .    has a predicted action if  is already operated, otherwise, . Next, we create the global encoding in a self-attention fashion as \[ \globalenc_{i} = \sum_{j} w_{j} \localenc_{j}\text{, } \] where  is computed with ReLU:\footnote{We used  instead of the conventional  because  caused the exploding gradient in our case.} \[ w_{j} = \frac     {         \text{ReLU}} \cdot \localenc_{j})     }     {         \sum_{k}\text{ReLU}} \cdot \localenc_{k})\text{.}     } \] Thanks to the bias terms in  and the self-attention in ,  is aware of the previous decisions for each word and the interactions between those decisions. In addition, BERT encoding  enables us to take a whole sentence into account.                                              \subsubsection{Deterministic Decoding by Language Model with Action Signals}  In this section, we explain how to compress and reconstruct sentences in a deterministic manner with the \lmconv.   This transformations occur after the agent finishes the action prediction on all words in the given sentence. For the \lmconv, we use BERT which is a masked language model  trained to predict ``masked"" portions in a sentence. MLM can estimate the probability distribution of -th word  in a sentence as    MLM can estimate the probability distribution of -th word  in a sentence  as  ,   \footnote{We borrow the notations of the input sentence here.}     \vskip -8pt   \[       P, \forall x \in V,   \]   where  is the same as  except that it has a mask at the -th position .  denotes a function to return a word with the highest probability for the -th position.   If there are multiple masks, we apply the prediction in an autoregressive fashion .     The procedure to obtain  and  by using  and MLM is shown in Figure . First, we convert  to a skeleton sequence  consisting of  tokens  where  is  if  is , otherwise a null token .   First, we convert each word in  with a rule that returns  if  is , otherwise a null token :     \vskip -5pt   \[       z_i =           \]   \vskip -pt     \awcomment{What is ? epsilon is defined in two meanings .}   where  is each token in  obtained after the conversion. We then define our compression and reconstruction functions  and  as   \\ \nonumber     \hat{x}_i &= \tilde{R} \nonumber \\         &=             A word is predicted only for  given by  in compression, but it does so for all  in reconstruction. Also, we set the original sentence as a prefixed context, which comes from  in compression and  in reconstruction, to make MLM aware of a former meaning. An example is shown in Figure , where MLM receives ``Machine learning is not perfect . \mask ~is \mask~."" as the compression input and predicts words for the \mask s.     If there are multiple masks, we conduct the prediction in an autoregressive fashion . Note that while any language model can be used for the \lmconv, MLM is advantageous because it utilizes before and after contexts, and there is no restriction on looking ahead at upcoming words.                                                          \subsubsection{Stepwise Reward Computation}  In this section, we explain the reward computation of the chosen action by referring to  and .  As stated in  , we have an action sequence  for every step .   Each element of  has the predicted action if already operated,  otherwise. When we apply  and  to all the , we can obtain a list of tuples . A tuple --- let us say, experience --- enables us to evaluate a state-action pair with respect to a single transition. In this section, we propose three techniques --- {\bf step reward}, {\bf violation penalty}, and {\bf summary assessment} --- to evaluate the agent's behavior with the stepwise experiences. Refer to Table  to see how these work in reward computation with an actual example.  Before moving on to the details, let us define two important notions throughout this section, compression rate  and reconstruction rate :          The CR learning assumes that the higher values of  and  are better. We use these for calculating rewards and pruning experiences.  \paragraph{Step Reward.} The task of the agent is to produce an action sequence with which the \lmconv~generates an appropriately compressed sentence while keeping the reconstruction successful. As such, we define the reward function  as     \vskip -10pt \[     \freward = r_{SR} + \breward, \]   \vskip -3pt   where  is the step reward that are designed to encourage the agent to improve the compression and reconstruction rate, respectively.   where the scores of  and  are designed to encourage the agent to improve the compression and reconstruction rate, respectively.   We call their multiplication a step reward.  is an additional score from the qualitative assessment of , which we explain later.  Returning to the step reward , it is a multiplication of  and  defined as     , \nonumber      \vskip -1pt \[     r_{SR} = \creward \times \rreward \text{,} \] \[     \creward = 1 - \frac{|\comp_{}|}{|\comp_{}|}\text{, }     \rreward =      , \nonumber \]   \vskip -5pt   where  is a minimum requirement for the reconstruction rate at the -th step and is defined as  with the hyperparameter .   \awcomment{Needs intuition for }   \rkcomment{How about now?} If we set  that requests perfect reconstruction, then  regardless of . However, we need to forgive reconstruction failure to some extent because of the information loss in compression, and  adjusts the allowed number of failures. For example,  requests the model to recover at least half of the original sentence correctly.    determines to what extent the agent is forgiven to fail the reconstruction, which is necessary because the reconstruction failure is inevitable due to the information loss by compression.   This constraint is stricter in earlier steps than later because reconstruction becomes harder as compression progresses.  Let us describe the behavior of the step reward . First, the reward is 0 when the agent chooses  or  because  due to there being no change in the length of . Second, the reward gets a positive value when the agent chooses  and satisfies the requirement for the reconstruction rate .   \footnote{ is larger when more words are already removed because  becomes difficult  in such circumstances.} Third, the reward gets a negative value when the agent chooses , but the reconstruction rate is less than the requirement. In short, the step reward recommends  as long as the agent can recover the original word, and otherwise,  or .    is for evaluating  and .   \footnote{ and  are evaluated through bonus reward explained later.}   However, it allows reconstruction failure lower than the threshold because information loss by compression is inevitable.                                                  \paragraph{Violation Penalty.}    Sequential modeling, including that performed by our agent, essentially suffers from error propagation caused by incorrect predictions at an earlier stage. The violation penalty mitigates this issue by giving a negative reward to the latest problematic action and excluding experiences after the mistake.  Here, in addition to , we introduce the hyperparameter , which represents a minimum requirement for the compression rate.  denotes its threshold at the -step defined as , and the agent must satisfy the condition . As the penalty, we forcibly assign  reward for the state-action pair at the -th step when the agent breaks either constraint of  or . In addition, we ignore experiences from step  and onward. If the agent keeps predicting until the end, we define . Figure  shows how these constraints work for the experience sequence.                                                   \paragraph{Summary Assessment.} Although the step reward considers the compression and reconstruction ratios, it ignores the critical aspects of the generated summary such as replacement with a shorter synonym and fluency as a sentence. Here, we explain the  mentioned in the previous paragraph and describe how to reflect such qualitative assessments to the reward given to the agent.  As the essential properties for , we take three perspectives into account: informativeness, shortness, and fluency. The informativeness refers to how much  retains the original meaning of , and the shortness and fluency are self-explanatory. To reflect these perspectives onto the agent's decision, we define  as       where  computes a similarity score of  and , and  computes a log-likelihood of .  and  are hyperparameters to adjust the importance of  and . In addition to , we give  to the experiences from the beginning to -th steps as defined in the step reward paragraph.   }           Let us explain the terms inside the square brackets first.  The first term, which is the multiplication of  and , aims for shortness and informativeness. It gets a higher value when the agent achieves the right balance of compression and reconstruction. The second term  aims to evaluate informativeness brought about by . Concretely,  returns a semantic similarity score in the range of  through the sentence vectors of  and  rather than just checking exact matches of words. The last term  represents fluency via the log-likelihood of  given by a pre-trained language model . We use BERT for the computation of  and  . Finally,  is the ratio of the number of operated words. It becomes closer to 1 when the agent is reaching a termination, i.e., finishing the prediction on all words by avoiding the violation penalty, which makes  larger. In contrast, the agent who fails at an earlier stage gets a small value of .                                          \vsubsec \paragraph{Training.} Leveraging the experiences  in the replay buffer , the agent learns the policy for summarizing a sentence  within the Q-learning framework. Specifically, we utilize DQN  to learn the Q-function  corresponding to the optimal policy by minimizing the loss,  \[ \mathcal{L} = \mathbb{E}_{s, a, r, s'} [-\psi)^2], \] where  and  is a target Q-function whose parameters are periodically updated in accordance with the latest network parameters. During the collection of experiences, RL requires the agent to explore an action on a given state for finding a better policy. As a unique point in this work, the agent must explore not only the action but also the order to predict.  For both explorations, we use the -greedy algorithm  that stochastically forces the agent to ignore Q-values and to behave randomly .                                        \paragraph{Inference.} Our modeling that provides  and  for each step has another advantage in terms of the inference. For the final output, we use  at the -th step that achieves the best balance of the compression and reconstruction ratios, where . This is based on the trade-off relationship of compression and reconstruction as seen in the precision-recall curve.                                        \vsecu   	 	We propose a dynamic selection method to choose variable sizes of context sentences for document-level translation. The candidate context sentences are scored and selected by two proposed strategies. We train the whole model via reinforcement learning, and design a novel reward to encourage the selection of useful context sentences. When applied to existing DocNMT models, our approach can improve translation quality significantly. In the future, we will select context sentences in larger candidate space, and explore more effective ways to extend our approach to select target-side context sentences. 	 	
","  \vsec  Our proposed method, which we call \ealm, consists of two essential modules: the editorial agent and the \lmconv. The agent sends action signals  to the conveter, which then deterministically transforms the input sentence according to the signals. We train the agent to find action signals so that the \lmconv~produces sentences demanded by the \algoname. In the following sections, we first share the background of Q-learning  and then present how to put the task and our approach on the Q-learning framework . We next explain the core algorithmic details  and finish with explanations about training and inference .    \vsubsec Q-learning is a popular approach in RL as represented by Deep Q-Networks . Q-learning leverages an action-value function to estimate the value of a pair of state and action with respect to a policy . The action-value function  is represented as the expected reward for the state-action pair:     \vskip -16pt \[     Q^\pi = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t} r \mid s_0=s, a_0=a \right], \]   \vskip -3pt   where  is a state,  is an action,  is a reward function for the state-action pair, and  is the discount factor. Hence, to solve a text summarization task via Q-learning, we first need to appropriately define the state, action, and reward function.   Hence, to solve a text summarization task via Q-learning, we first need to define the state , action , and reward , appropriately.   We formally represent our approach as the Q-learning task in the next section.     In our approach, given an input sentence , we define a state  in regard to each word . An action  for the state  is chosen from among the three options, .    Q, \]   \vskip -5pt   where  is defined as . The agent then reiterates the predictions until it finishes determining an action on all words. By defining the state in regard to a single word instead of a whole sentence and asking the agent to determine the prediction order, we can handle variable sentence lengths in natural language. Note that this is not a left-to-right process; the agent conducts the prediction in the order of ``confidence"".   i.e., .    Subsequently, let us describe how we create the state .    In terms of creating the state , we dynamically obtain it by  two-layer encoding that we call local encoding  and global encoding .   We represent a state  via the concatenation of two types of word embeddings that we call local encoding  and global encoding .   In terms of creating the state , we dynamically obtain it by the concatenation of two type encodings that we call local encoding  and global encoding . Next we explain how to encode . To send the agent contextual information, such as the previous decisions, the prediction statuses, and the whole sentence, we dynamically create a state  with a concatenation of two encodings; local encoding  and global encoding      \vskip 0pt \[     s_{i} = [\localenc_{i}; \globalenc_{i}]\text{.} \]   To create the two encodings, first, we map  to a fixed-sized vector  with an arbitrary encoder , and  is repeatedly used throughout the process regardless of the steps. Then, we define the local encoding as \[ \localenc_{i} = \bm{e}_i + \bm{b}^{a_{i}} + \bm{b}^{u_{i}}\text{, } \] where  and  are learnable bias vectors for the action and prediction status of the -th word, respectively.     \vskip 0pt   \[       \localenc_{i} = \bm{e}_i + \bm{b}^{a_{i}} + \bm{b}^{u_{i}}   \]   \vskip 0pt   The all elements in the bias vector have a same trainable value corresponded to the action  or the status of prediction .   All elements of the bias vector are filled with same value corresponded to the superscription .    has same size with  and the all elements have same value corresponding to .    has a predicted action if  is already operated, otherwise, . Next, we create the global encoding in a self-attention fashion as \[ \globalenc_{i} = \sum_{j} w_{j} \localenc_{j}\text{, } \] where  is computed with ReLU:\footnote{We used  instead of the conventional  because  caused the exploding gradient in our case.} \[ w_{j} = \frac     {         \text{ReLU}} \cdot \localenc_{j})     }     {         \sum_{k}\text{ReLU}} \cdot \localenc_{k})\text{.}     } \] Thanks to the bias terms in  and the self-attention in ,  is aware of the previous decisions for each word and the interactions between those decisions. In addition, BERT encoding  enables us to take a whole sentence into account.                                              \subsubsection{Deterministic Decoding by Language Model with Action Signals}  In this section, we explain how to compress and reconstruct sentences in a deterministic manner with the \lmconv.   This transformations occur after the agent finishes the action prediction on all words in the given sentence. For the \lmconv, we use BERT which is a masked language model  trained to predict ``masked"" portions in a sentence. MLM can estimate the probability distribution of -th word  in a sentence as    MLM can estimate the probability distribution of -th word  in a sentence  as  ,   \footnote{We borrow the notations of the input sentence here.}     \vskip -8pt   \[       P, \forall x \in V,   \]   where  is the same as  except that it has a mask at the -th position .  denotes a function to return a word with the highest probability for the -th position.   If there are multiple masks, we apply the prediction in an autoregressive fashion .     The procedure to obtain  and  by using  and MLM is shown in Figure . First, we convert  to a skeleton sequence  consisting of  tokens  where  is  if  is , otherwise a null token .   First, we convert each word in  with a rule that returns  if  is , otherwise a null token :     \vskip -5pt   \[       z_i =           \]   \vskip -pt     \awcomment{What is ? epsilon is defined in two meanings .}   where  is each token in  obtained after the conversion. We then define our compression and reconstruction functions  and  as   \\ \nonumber     \hat{x}_i &= \tilde{R} \nonumber \\         &=             A word is predicted only for  given by  in compression, but it does so for all  in reconstruction. Also, we set the original sentence as a prefixed context, which comes from  in compression and  in reconstruction, to make MLM aware of a former meaning. An example is shown in Figure , where MLM receives ``Machine learning is not perfect . \mask ~is \mask~."" as the compression input and predicts words for the \mask s.     If there are multiple masks, we conduct the prediction in an autoregressive fashion . Note that while any language model can be used for the \lmconv, MLM is advantageous because it utilizes before and after contexts, and there is no restriction on looking ahead at upcoming words.                                                          \subsubsection{Stepwise Reward Computation}  In this section, we explain the reward computation of the chosen action by referring to  and .  As stated in  , we have an action sequence  for every step .   Each element of  has the predicted action if already operated,  otherwise. When we apply  and  to all the , we can obtain a list of tuples . A tuple --- let us say, experience --- enables us to evaluate a state-action pair with respect to a single transition. In this section, we propose three techniques --- {\bf step reward}, {\bf violation penalty}, and {\bf summary assessment} --- to evaluate the agent's behavior with the stepwise experiences. Refer to Table  to see how these work in reward computation with an actual example.  Before moving on to the details, let us define two important notions throughout this section, compression rate  and reconstruction rate :          The CR learning assumes that the higher values of  and  are better. We use these for calculating rewards and pruning experiences.  \paragraph{Step Reward.} The task of the agent is to produce an action sequence with which the \lmconv~generates an appropriately compressed sentence while keeping the reconstruction successful. As such, we define the reward function  as     \vskip -10pt \[     \freward = r_{SR} + \breward, \]   \vskip -3pt   where  is the step reward that are designed to encourage the agent to improve the compression and reconstruction rate, respectively.   where the scores of  and  are designed to encourage the agent to improve the compression and reconstruction rate, respectively.   We call their multiplication a step reward.  is an additional score from the qualitative assessment of , which we explain later.  Returning to the step reward , it is a multiplication of  and  defined as     , \nonumber      \vskip -1pt \[     r_{SR} = \creward \times \rreward \text{,} \] \[     \creward = 1 - \frac{|\comp_{}|}{|\comp_{}|}\text{, }     \rreward =      , \nonumber \]   \vskip -5pt   where  is a minimum requirement for the reconstruction rate at the -th step and is defined as  with the hyperparameter .   \awcomment{Needs intuition for }   \rkcomment{How about now?} If we set  that requests perfect reconstruction, then  regardless of . However, we need to forgive reconstruction failure to some extent because of the information loss in compression, and  adjusts the allowed number of failures. For example,  requests the model to recover at least half of the original sentence correctly.    determines to what extent the agent is forgiven to fail the reconstruction, which is necessary because the reconstruction failure is inevitable due to the information loss by compression.   This constraint is stricter in earlier steps than later because reconstruction becomes harder as compression progresses.  Let us describe the behavior of the step reward . First, the reward is 0 when the agent chooses  or  because  due to there being no change in the length of . Second, the reward gets a positive value when the agent chooses  and satisfies the requirement for the reconstruction rate .   \footnote{ is larger when more words are already removed because  becomes difficult  in such circumstances.} Third, the reward gets a negative value when the agent chooses , but the reconstruction rate is less than the requirement. In short, the step reward recommends  as long as the agent can recover the original word, and otherwise,  or .    is for evaluating  and .   \footnote{ and  are evaluated through bonus reward explained later.}   However, it allows reconstruction failure lower than the threshold because information loss by compression is inevitable.                                                  \paragraph{Violation Penalty.}    Sequential modeling, including that performed by our agent, essentially suffers from error propagation caused by incorrect predictions at an earlier stage. The violation penalty mitigates this issue by giving a negative reward to the latest problematic action and excluding experiences after the mistake.  Here, in addition to , we introduce the hyperparameter , which represents a minimum requirement for the compression rate.  denotes its threshold at the -step defined as , and the agent must satisfy the condition . As the penalty, we forcibly assign  reward for the state-action pair at the -th step when the agent breaks either constraint of  or . In addition, we ignore experiences from step  and onward. If the agent keeps predicting until the end, we define . Figure  shows how these constraints work for the experience sequence.                                                   \paragraph{Summary Assessment.} Although the step reward considers the compression and reconstruction ratios, it ignores the critical aspects of the generated summary such as replacement with a shorter synonym and fluency as a sentence. Here, we explain the  mentioned in the previous paragraph and describe how to reflect such qualitative assessments to the reward given to the agent.  As the essential properties for , we take three perspectives into account: informativeness, shortness, and fluency. The informativeness refers to how much  retains the original meaning of , and the shortness and fluency are self-explanatory. To reflect these perspectives onto the agent's decision, we define  as       where  computes a similarity score of  and , and  computes a log-likelihood of .  and  are hyperparameters to adjust the importance of  and . In addition to , we give  to the experiences from the beginning to -th steps as defined in the step reward paragraph.   }           Let us explain the terms inside the square brackets first.  The first term, which is the multiplication of  and , aims for shortness and informativeness. It gets a higher value when the agent achieves the right balance of compression and reconstruction. The second term  aims to evaluate informativeness brought about by . Concretely,  returns a semantic similarity score in the range of  through the sentence vectors of  and  rather than just checking exact matches of words. The last term  represents fluency via the log-likelihood of  given by a pre-trained language model . We use BERT for the computation of  and  . Finally,  is the ratio of the number of operated words. It becomes closer to 1 when the agent is reaching a termination, i.e., finishing the prediction on all words by avoiding the violation penalty, which makes  larger. In contrast, the agent who fails at an earlier stage gets a small value of .                                          \vsubsec \paragraph{Training.} Leveraging the experiences  in the replay buffer , the agent learns the policy for summarizing a sentence  within the Q-learning framework. Specifically, we utilize DQN  to learn the Q-function  corresponding to the optimal policy by minimizing the loss,  \[ \mathcal{L} = \mathbb{E}_{s, a, r, s'} [-\psi)^2], \] where  and  is a target Q-function whose parameters are periodically updated in accordance with the latest network parameters. During the collection of experiences, RL requires the agent to explore an action on a given state for finding a better policy. As a unique point in this work, the agent must explore not only the action but also the order to predict.  For both explorations, we use the -greedy algorithm  that stochastically forces the agent to ignore Q-values and to behave randomly .                                        \paragraph{Inference.} Our modeling that provides  and  for each step has another advantage in terms of the inference. For the final output, we use  at the -th step that achieves the best balance of the compression and reconstruction ratios, where . This is based on the trade-off relationship of compression and reconstruction as seen in the precision-recall curve.                                        \vsecu",67
" Neural machine translation  systems are data driven models, which highly depend on the training corpus.  NMT models have a tendency towards over-fitting to frequent observations  while neglecting those low-frequency observations.  Unfortunately, there exists a token imbalance phenomenon in natural languages as different tokens appear with different frequencies, which roughly obey the Zipf's Law.  Table shows that there is a serious imbalance between high-frequency tokens and low-frequency tokens.  NMT models rarely have the opportunity to learn and generate those ground-truth low-frequency tokens in the training process. %It is harder for the NMT model to generate ground-truth low-frequency tokens even in the training process.  %Compared to the reference, the NMT model tends to generate more high-frequency tokens and less low-frequency tokens, which hurts the translation quality.  Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary or adding extra components, which bring in extra training complexity and computing expense.  Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model, BPE-based model and word-piece-based model. %For example, the sub-word model adapted byte pair encoding  technique to the task of word segmentation.  These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models.  Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table shows.  %It is obvious that there are always low-frequency tokens no matter what the number of merge operations of BPE is. %As shown in Table, the rare word 'slower' is split into two tokens as 'slow' and 'er', there still exist obvious token-level imbalance between 'slow' and other tokens.   \iffalse            \end{table} \fi    \iffalse  \fi   \iffalse            \end{table} \fi Furthermore, current NMT models generally assign equal training weights to target tokens without considering their frequencies.  It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the training sets. The parameters related to them can not be adequately trained, which will, in turn, make NMT models tend to prioritize output fluency over translation adequacy, and ignore the generation of low-frequency tokens during decoding, which is illustrated in Table. It shows that the vanilla NMT model tends to generate more high-frequency tokens and less low-frequency tokens. %This will, in turn, make the model %tend to generate too many high-frequency tokens and too less low-frequency tokens during decoding. However, low-frequency tokens may carry critical semantic information which may affect translation quality once they are neglected.   %It is very likely for NMT models to ignore the loss produced by rare words so that the patterns learned by the encoder, decoder, or attention modules from them can't be adequately updated. What's more, NMT models tend to prioritize output fluency over translation adequacy and ignore the translation of rare words during generation.  %In our experiments, we observed that vanilla NMT models usually produce more frequent words and less rare words than real references. Therefore, some techniques should be adopted to improve the translation of rare words. %distribution.   %It is obvious that there are always rare tokens no matter what the number of merge operations of BPE is and the problem of token distribution imbalance still exists.  %One of the advantages of this technique is that it reduces the number of rare words by splitting them into more frequent subword tokens , which in fact  %relieve the imbalance of word   %The strength is that NMT models can make use of large amounts of parallel training sentences and learn the knowledge and features embodied in the training data. However, one of the weaknesses is that NMT models have a tendency towards over-fitting to frequent observations , but neglecting those rare cases which are not frequently observed. Unfortunately, there is a natural word distribution imbalance in the corpus. According to the Zipf's Law, the frequency of any word is inversely proportional to its ranking in the frequency table, which indicates that the occurrences of some words are far more than others naturally.     %For word-level NMT models, NMT has its limitation in handling a larger vocabulary because of the training complexity and computing expense.   % %In their work, they first represent each word as a sequence of characters and then iteratively combine the most frequent pair as a new symbol. %which achieved better accuracy for the translation of rare words %, we seek to further alleviate the token imbalance problem based on the above analysis. For this purpose,  To address the above issue, we proposed token-level adaptive training objectives based on target token frequencies.  We aimed that those meaningful but relatively low-frequency tokens could be assigned with larger loss weights during training so that the model will learn more about them. %In our objectives, those relatively low-frequency but valuable tokens will be assigned with larger loss weights during training to encourage the model to learn more about them. To explore suitable adaptive objectives for NMT, we first applied existing adaptive objectives from other tasks to NMT and analyzed their performance. We found that though they could bring modest improvement on the translation of low-frequency tokens, they did much damage to the translation of high-frequency tokens, which led to an obvious degradation on the overall performance. This implies that the objective should ensure the training of high-frequency tokens first. %training of high-frequency tokens should be ensured first. %We should ensure the training of high-frequency tokens and enlarge the weights of low-frequency tokens at the same time. %We firstly tried the focal loss, which was proposed for solving the token imbalance problem in the CV task, and analyzed the performance.  Then, based on our observations, we proposed two heuristic criteria for designing the token-level adaptive objectives based on the target token frequencies. Last, we presented two specific forms for different application scenarios according to the criteria. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation. %We carried out experiments on ZHEN, ENRO, and ENDE translation tasks to validate our methods. The experimental results show that our methods achieve significant improvement in translation quality, especially in sentences that contain more low-frequency tokens.  %Besides, the token distribution of our translations becomes closer to references for test sets.  %Besides, our method also improves the diversity of the translations.   Our contributions can be summarized as follows:   %More specifically, NMT models are first trained with equal weights and then fine-tuned with well-defined weights introduced by the scoring functions. In this way, it won't hurt the translation of frequent tokens, but also can improve the translation of rare tokens to a certain degree. To the best of our knowledge, this is the first work trying to concern about the training weights at the token level to solve the distribution imbalance problem in NMT. The experiments on multiple translation tasks show that our method can improve the overall translation performance without almost any additional computing or storage expense. And the analysis experiments indicate that our method can improve the rare tokens translation significantly and the tokens distribution of our translation are much closer to the references than the baseline translations.    Rare Word Translation. Rare word translation is one of the key challenges for NMT. For word-level NMT models, NMT has its limitation in handling a larger vocabulary because of the training complexity and computing expense.  Some work tries to solve this problem by maintaining phrase tables or back-off vocabulary. The subword-based NMT reduces the size of vocabulary greatly and become the mainstream technology gradually. \citet{abs-2004-02334} gave a detailed analysis about the effects of the BPE size on the data distribution and translation quality. Some recent work tried to further improve the translation of the rare words with the help of the memory network or the pointer network.  In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques.   Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks.  In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem, the inability of MT system to handle morphologically richer language correctly, or the exposure bias problem. The methods of trying to solve this can be divided into two types. The data-based methods make use of over- and under-sampling to reduce the imbalance. The algorithm-based methods give extra reward to different classes.   Most commonly, algorithms are modified to consider an extra reward.  Our method is algorithm-based which brings no extra cost.  To the best of our knowledge, we are the first to concern about the class imbalance problem of NMT.  Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation and term extraction. In NMT, word frequency information is used for curriculum learning and domain adaptation data selection. \citet{WangTSL20} analyzed the miscalibration problem on the low-frequency tokens. \citet{JiangRMR19} proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT.    some work use word frequency as one of the difficulty metrics for curriculum learning, or one of the criteria for domain adaptation data selection. These work all try to utilize frequency information in the sentence level, while our work uses it at the token level, which is more flexible in practice.       \vsec We brought the Q-learning framework into unsupervised text summarization and proposed a new method \ealm~that is an edit-based unsupervised summarizer leveraging a Q-learning agent and a language model. The experments showed that \ealm~performed competitively with the previous encoder-decoder-based methods. However, in qualitative analysis, we found that the quality of the generated summaries of any unsupervised model was not sufficient, and there are individual limitations for each model. These issue must be overcome as the step forward to generating practically available summaries without paired data. In particular for \ealm, there is room for improvement by importing the latest techniques in RL. Our work paves the way for further research on bridging Q-learning and unsupervised text summarization.                                   
"," Rare Word Translation. Rare word translation is one of the key challenges for NMT. For word-level NMT models, NMT has its limitation in handling a larger vocabulary because of the training complexity and computing expense.  Some work tries to solve this problem by maintaining phrase tables or back-off vocabulary. The subword-based NMT reduces the size of vocabulary greatly and become the mainstream technology gradually. \citet{abs-2004-02334} gave a detailed analysis about the effects of the BPE size on the data distribution and translation quality. Some recent work tried to further improve the translation of the rare words with the help of the memory network or the pointer network.  In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques.   Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks.  In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem, the inability of MT system to handle morphologically richer language correctly, or the exposure bias problem. The methods of trying to solve this can be divided into two types. The data-based methods make use of over- and under-sampling to reduce the imbalance. The algorithm-based methods give extra reward to different classes.   Most commonly, algorithms are modified to consider an extra reward.  Our method is algorithm-based which brings no extra cost.  To the best of our knowledge, we are the first to concern about the class imbalance problem of NMT.  Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation and term extraction. In NMT, word frequency information is used for curriculum learning and domain adaptation data selection. \citet{WangTSL20} analyzed the miscalibration problem on the low-frequency tokens. \citet{JiangRMR19} proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT.    some work use word frequency as one of the difficulty metrics for curriculum learning, or one of the criteria for domain adaptation data selection. These work all try to utilize frequency information in the sentence level, while our work uses it at the token level, which is more flexible in practice.",68
"   Graph structures play a pivotal role in NLP because they are able to capture particularly rich structural information. For example, Figure shows a directed, labeled Abstract Meaning Representation  graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts  neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks  have been explored to better encode structural information for this task .   % \tzy{papers before 2018??? Gated Graph Neural networks??? Do not miss an important paper.}     One type of such GNNs is Graph Convolutional Networks .  GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate  neighbors.  Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions .  However, prior efforts  have shown that the locality property of existing GCNs precludes efficient non-local information propagation. \citet{AbuElHaija2019MixHopHG} further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks  have been explored as an alternative to capture global dependencies. As shown in Figure , SANs associate each node with other nodes such that we model interactions between any two nodes in the graph. Still, this approach ignores the structure of the original graph. \citet{Zhu2019ModelingGS} and \citet{Cai2019GraphTF} propose structured SANs that incorporate additional neural components to encode the structural information of the input graph.   Convolutional operations, however, are more computationally efficient than self-attention operations because the computation of attention weights scales quadratically while convolutions scale linearly with respect to the input length . Therefore, it is worthwhile to explore the possibility of models based on graph convolutions. One potential approach that has been considered is to incorporate information from higher order neighbors, which helps to facilitate non-local information aggregation for node classification . However, simple concatenation of different order representations may not be able to model complex interactions in semantics for text generation .    We propose to better integrate high-order information, by introducing a novel dynamic fusion mechanism and propose the Lightweight, Dynamic Graph Convolutional Networks . As shown in Figure  , nodes in the LDGCN model are able to integrate information from first to third-order neighbors. With the help of the dynamic mechanism, LDGCNs can effectively synthesize information from different orders to model complex interactions in the AMR graph for text generation. Also, LDGCNs require no additional computational overhead, in contrast to vanilla GCN models. We further develop two novel weight sharing strategies based on the group graph convolutions and weight tied convolutions. These strategies allow the LDGCN model to reduce memory usage and model complexity.  Experiments on AMR-to-text generation show that LDGCNs outperform best reported GCNs and SANs trained on LDC2015E86 and LDC2017T10 with significantly fewer parameters. On the large-scale semi-supervised setting, our model is also consistently better than others, showing the effectiveness of the model on a large training set. We release our code and pretrained models at \url{https://github.com/yanzhang92/LDGCNs}.\footnote{Our implementation is based on  MXNET  and the Sockeye toolkit .}      Graph  convolutional  networks   have  been  widely used as the structural encoder in various NLP applications including question answering , semantic parsing  and relation extraction .    \paragraph{Graph-to-sequence.}   Early efforts for AMR-to-text generation mainly include grammar-based models  and sequence-based models , discarding crucial structural information when linearising the input AMR graph. To solve that, various GNNs including graph recurrent networks  and graph convolutional networks  have been used to encode the AMR structure. Though GNNs are able to operate directly on graphs, the locality nature of them precludes efficient information propagation . Larger and deeper models are required to model the complex non-local interactions . More recently, SAN-based models  outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model.        is used to model the global dependencies as the locality nature of GNNs preclude efficient non-local information propagation.      \citet{Cao2018FactorisingAG} factor the generation process by leveraging syntactic information to further improve the performance.     Recently, GNNs have been widely used for representation learning of graphs as it directly operates on graphs and maintains structural information. GNNs-based models can be classified into two categories: models based on graph recurrent neural networks and models based on GCNs. For recurrent models, \citet{Beck2018GraphtoSequenceLU} and \citet{Ribeiro2019EnhancingAG} employs gated graph neural networks  on their transformed graph structures for better relation encoding. \citet{Song2018AGM} propose a graph state LSTM to directly encode graph-level semantics. For GCN-based models,  \citet{Bastings2017GraphCE} and  apply a two-layer GCN model upon RNNs while \citet{guo2019densely} introduce the dense connection into GCNs and encode the graph with a deeper model. More recently, \citet{Zhu2019ModelingGS, Cai2019GraphTF} propose the graph transformer based on self attention to model the global dependencies of the input graph. Our work is mainly motivated by these advances with the goal of capturing non-local interactions by integrating information from different orders.   Unlike previous GCN-based model that follows the local information aggregation scheme, our HGCN model is able to capture non-local interactions by integrating information from different orders.     Our graph encoder is also built based on the graph convolutional networks but can be deeper and more lightweight compared to.      \paragraph{Mixhop Graph Convolutional Networks}      Graph Convolutional Networks compute the representation of each node iteratively based on those of its adjacent nodes.  This makes it difficult for GCNs to learn a general class of neighborhood mixing relationships. Recent work generalized graph convolution networks   to higher order structure by repeatedly mixing feature representations of neighbors at various distances through column-wise concatenation operations, which result in more computational complexity. In our model, we integrate different order neighbors into one  adjacency matrix and can save memory efficiently.    \paragraph{Group Convolution} The concept of group convolution, was first used in the AlexNet architecture and has more recently been popluarized by their successful application in  a series of lightweight architecture, for example, ResNeXt,  ShuffleNet and CondenseNet. Our work generalizes group convolution to graph convolutional networks.  To the best of our knowledge, we are the first to verify the effectivity of group convolution in GCNs.      In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity.  the vanilla NMT model tends to generate more high-frequency words than the true distribution due to.  there is a token imbalance phenomenon in the natural language and the vanilla NMT model tends to generate more high-frequency words than the true distribution.  and less low-frequency words    This output bias will affect the translation quality since the low-frequency tokens may carry critical semantic information.  To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the observations. Next, we gave two simple but effective forms based on the criteria, which can assign appropriate training weights to target tokens.  we propose token-level adaptive objectives based on token frequencies, aiming to assign appropriate training weights to target tokens. To achieve this, we propose three heuristic criteria and then put forward two simple but effective forms based on the criteria.  The final results show that our methods can achieve significant improvement in performance, especially on sentences that contain more low-frequency tokens. Further analyses show that our method can also improve the lexical diversity.   
","  Graph  convolutional  networks   have  been  widely used as the structural encoder in various NLP applications including question answering , semantic parsing  and relation extraction .    \paragraph{Graph-to-sequence.}   Early efforts for AMR-to-text generation mainly include grammar-based models  and sequence-based models , discarding crucial structural information when linearising the input AMR graph. To solve that, various GNNs including graph recurrent networks  and graph convolutional networks  have been used to encode the AMR structure. Though GNNs are able to operate directly on graphs, the locality nature of them precludes efficient information propagation . Larger and deeper models are required to model the complex non-local interactions . More recently, SAN-based models  outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model.        is used to model the global dependencies as the locality nature of GNNs preclude efficient non-local information propagation.      \citet{Cao2018FactorisingAG} factor the generation process by leveraging syntactic information to further improve the performance.     Recently, GNNs have been widely used for representation learning of graphs as it directly operates on graphs and maintains structural information. GNNs-based models can be classified into two categories: models based on graph recurrent neural networks and models based on GCNs. For recurrent models, \citet{Beck2018GraphtoSequenceLU} and \citet{Ribeiro2019EnhancingAG} employs gated graph neural networks  on their transformed graph structures for better relation encoding. \citet{Song2018AGM} propose a graph state LSTM to directly encode graph-level semantics. For GCN-based models,  \citet{Bastings2017GraphCE} and  apply a two-layer GCN model upon RNNs while \citet{guo2019densely} introduce the dense connection into GCNs and encode the graph with a deeper model. More recently, \citet{Zhu2019ModelingGS, Cai2019GraphTF} propose the graph transformer based on self attention to model the global dependencies of the input graph. Our work is mainly motivated by these advances with the goal of capturing non-local interactions by integrating information from different orders.   Unlike previous GCN-based model that follows the local information aggregation scheme, our HGCN model is able to capture non-local interactions by integrating information from different orders.     Our graph encoder is also built based on the graph convolutional networks but can be deeper and more lightweight compared to.      \paragraph{Mixhop Graph Convolutional Networks}      Graph Convolutional Networks compute the representation of each node iteratively based on those of its adjacent nodes.  This makes it difficult for GCNs to learn a general class of neighborhood mixing relationships. Recent work generalized graph convolution networks   to higher order structure by repeatedly mixing feature representations of neighbors at various distances through column-wise concatenation operations, which result in more computational complexity. In our model, we integrate different order neighbors into one  adjacency matrix and can save memory efficiently.    \paragraph{Group Convolution} The concept of group convolution, was first used in the AlexNet architecture and has more recently been popluarized by their successful application in  a series of lightweight architecture, for example, ResNeXt,  ShuffleNet and CondenseNet. Our work generalizes group convolution to graph convolutional networks.  To the best of our knowledge, we are the first to verify the effectivity of group convolution in GCNs.",69
" In recent years, cyberbullying has become one of the most pressing online risks among youth and raised serious concerns in society. Cyberbullying is commonly defined as the electronic transmission of insulting or embarrassing comments, photos or videos, as illustrated in Figure~ . Harmful bullying behavior can include posting rumors, threats, pejorative labels, and sexual remarks. Research from the American Psychological Association and the White House has revealed more than  of young people in the US indicate that they have been bullied on social media platforms~. Such a growing prevalence of cyberbullying on social media has detrimental societal effects, such as victims may experience lower self-esteem, increased suicidal ideation, and a variety of negative emotional responses~. Therefore, it has become critically important to be able to detect and prevent cyberbullying on social media. Research in computer science aimed at identifying, predicting, and ultimately preventing cyberbullying through better understanding the nature and key characteristics of online cyberbullying.     In the literature, existing efforts toward automatically detecting cyberbullying have primarily focused on textual analysis of user comments, including keywords~ and sentiments analysis ~. These studies attempt to build a generic binary classifier by taking high-dimensional text features as the input and make predictions accordingly. Despite their satisfactory detection performance in practice, these models largely overlooked temporal information of cyberbullying behaviors. They also ignore user interactions in social networks. Furthermore, the majority of these methods focus on detecting cyberbullying sessions effectively but cannot explain ``why'' a media session was detected as cyberbullying. Given a sequence of comments with user attributes, we think sequential learning can allow us to better exploit and model the evolution and correlations among individual comments. Besides, graph-based learning can enable us to represent and learn how users interact with each other in a session.   This work aims to detect cyberbullying by jointly exploring explainable information from user comments on social media. To this end, we build an explainable cyberbullying detection framework, \underline{HE}terogeneous \underline{N}eural \underline{I}nteraction \underline{N}etworks , through a coherent process. HENIN consists of three main components that learn various interactions among heterogeneous information displayed in social media sessions. A comment encoder is created to learn the representations of user comments through a hierarchical self-attention neural network so that the semantic and syntactic cues on cyberbullying can be captured. We create a post-comment co-attention mechanism to learn the interactions between a posted text and its comments. Moreover, two graph convolutional networks are leveraged to learn the latent representations depicting how sessions interact with one another in terms of users, and how posts are correlated with each other in terms of words.  Specifically, we address several challenges in this work:  how to perform explainable cyberbullying detection that can boost detection performance,  how to highlight explainable comments without the ground truth,  how to model the correlation between posted text and user comments, and  how to model the interactions between sessions in terms of users, and the interactions between textual posts in terms of words. Our solutions to these challenges result in a novel framework HENIN.   Our contributions are summarized as follows. %       In this section, we briefly summarize prior and related works on cyberbullying detection.  Relevant studies can be categories into social contexts-based and user comment-based approaches. Social contexts-based approaches utilize three categories of features, user-based, post-based, and network-based.   Post-based features rely on text analysis to identify cyberbullying evidences  on social media~.    concatenated profane words as content features to detect explicit cyberbullying behaviors in negative text comments.  \citet{xu2012learning} point out Latent Semantic Analysis and Latent Dirichlet Allocation  can be used to learn latent representations of posts. In addition, SICD~ further models post sentiments for cyberbullying detection.   User-based features are extracted from user profiles to measure their characteristics. Gender-specific features, user's past posts, account registration time, and frequently-used words are useful user-based features~.   facilitate cyberbullying detection by modeling genders-specific features and contextual features such as users' previous posted text and vulgar words in comments.   Existing studies~ also prove that network-based features are effective in detecting cyberbullying. These features are learned by constructing propagation networks or interaction networks that depict how posts are spread and how users interact with each other.  User comment-based approaches utilize the sequence of user comments to detect cyberbullying of the source post.   Meanwhile, there have some methods different from above three category, for example,  CONcISE~ is a sequential hypothesis testing method conducted on the comment sequence to select the significant comment features.   which search drastically reduce the number of features used in classifying each user comments   the model also called CONcISE.  \citet{raisi2018weakly} detect harassment-based cyberbullying by identifying expert-provided key phrases from user comments.    In this paper, we propose a novel self-paced learning model for NMT in which the learning schedule is determined by model itself rather than being intuitively predefined by humans. Experimental results on three translation tasks verify the universal effectiveness of our approach. Quantitative analyses confirm that exploiting self-paced strategy presents a more flexible way to facilitate the model convergence than its CL counterparts. It is interesting to combine with other techniques to further improve NMT. Besides, as this idea is not limited to machine translation, it is also interesting to validate our model in other NLP tasks, such as low-resource NMT model training and neural architecture search.  
","   In this section, we briefly summarize prior and related works on cyberbullying detection.  Relevant studies can be categories into social contexts-based and user comment-based approaches. Social contexts-based approaches utilize three categories of features, user-based, post-based, and network-based.   Post-based features rely on text analysis to identify cyberbullying evidences  on social media~.    concatenated profane words as content features to detect explicit cyberbullying behaviors in negative text comments.  \citet{xu2012learning} point out Latent Semantic Analysis and Latent Dirichlet Allocation  can be used to learn latent representations of posts. In addition, SICD~ further models post sentiments for cyberbullying detection.   User-based features are extracted from user profiles to measure their characteristics. Gender-specific features, user's past posts, account registration time, and frequently-used words are useful user-based features~.   facilitate cyberbullying detection by modeling genders-specific features and contextual features such as users' previous posted text and vulgar words in comments.   Existing studies~ also prove that network-based features are effective in detecting cyberbullying. These features are learned by constructing propagation networks or interaction networks that depict how posts are spread and how users interact with each other.  User comment-based approaches utilize the sequence of user comments to detect cyberbullying of the source post.   Meanwhile, there have some methods different from above three category, for example,  CONcISE~ is a sequential hypothesis testing method conducted on the comment sequence to select the significant comment features.   which search drastically reduce the number of features used in classifying each user comments   the model also called CONcISE.  \citet{raisi2018weakly} detect harassment-based cyberbullying by identifying expert-provided key phrases from user comments.",70
"  \zc{ Title: need to be more concrete, something like ""Denoising Multi-Source Weak Supervision for Neural Text Classification"" will probably be better  Introduction:  Paragraph 1: many NLP tasks can be formulated as text classification  dnns are successful  but they require labeled data, which are expensive to obtain  recently, pre-trained language models can alleviate this problem, but still suffers degraded performance when labeled data is limited. \wendi{BERT still need labeled data}  Paragraph 2: weak supervision is promising, but also challenging to apply because weak labels are inaccurate and incomplete.  Paragraph 3: we study using multiple weak supervision sources to learn text classifiers; the intuition is multiple weak supervision sources can provide complementary information to eliminate noise; and combined with unlabeled data, they can address label incompleteness as well. \wendi{key: complementary information; bootstrapping on D_U}  Paragraph 4: there are a large body of works on weakly-supervised learning, most are dealing with only single-source weak supervision  they may suffer from the unreliability of single sources and error propagation; \wendi{sensitive to single source} several works deal with multiple sources, but they XXX , need to make sure we cite and discuss them).  Paragraph 5: introduce our method, the key idea, the uniqueness compared with existing methods. I feel the current method description is a bit plain, need to distill the main ideas. I think the main ideas are: - source reliability estimation and neural classification benefit each other  the co-training framework  \wendi{regularization} - conditional source reliability - self-training to leverage unmatched samples to obtain more labeled instances. - maybe also mention we rely on pre-trained language models to get good representations, which helps denoising  \wendi{high level: denoise, and how to enhance}  Other Sections: Section 2: make it at most half a page Section 3: 2.5 pages Section 4: 3 pages others: 1 page  Something we had better show in the experiments: - multi-source weak supervision can be powerful   for this, we already have a lot of results - majority voting does not work - our method works better than existing weak supervision methods  - what happens if we use some subsets of the multiple weak supervision sources - are there any interpretations about the source reliability we learned - how the different designs in our method work  - would labeled data help further }  Text classification, relation extraction, question answering are the fundamental natural language tasks with numerous applications such as document classification or knowledge extraction.  \zc{ Many NLP tasks can be formulated as text classification   problems, such as sentiment analysis, topic classification, relation   extraction, and XXX .} Recently, deep neural nets  have demonstrated superior performance for this problem \zc{briefly mention earlier dnns , to the recent trend of BERT-based ones}, largely due to their capabilities of automatically learning distributed features and fitting complex functions based on large-scale training data.   However, in many real world scenarios, large-scale labeled data are unavailable and manually annotating data at a large scale is prohibitively expensive. \zc{merge paragraph 1 and 2}  To address the label scarcity bottleneck, we study the problem of  using heuristic rules to train neural text classifiers.  While domain experts \zc{not   necessarily domain experts, can be also KBs.} often cannot afford to annotate millions of documents carefully, they can easily provide a set of heuristic rules as weak supervision signals.  Using such rules can automatically induce labeled data for model training , but meanwhile it introduces two major challenges: label noise and low label coverage. %The first challenge is label noise.   The label noise issue arises because heuristic rules are often too simple to capture the rich contexts and complex patterns for text classification. For instance, while a rule `expensive \ negative' for restaurant ranking is correct for most times, but sometimes it wrong because the delicious food deserves the high price. Seed rules have limited coverage because real-life text corpora often have long-tail distributions, many heuristic rules are defined over the most frequent keywords, so the instances containing only long-tail keywords cannot be covered by any given rules. \zc{can merge the previous paragraph and shorten it.}  There have been studies  that attempt to use weak supervision for deep text classification. Unfortunately, their performance is limited by the above two challenges. Ratner \etal  proposed a data programming method, which uses heuristic rules as labeling functions and then trains discriminative models using the automatically created labeled data. However, the training data annotated by data programming come from instances that can be directly matched by the rules, making the model have limited performance on the unmatched data.  Meng \etal  proposed a deep self-training method, which uses weak supervision to learn an initial model and then updates the model by using the model's own confident predictions. However, the self-training procedure can overfit the label noise and suffer from the error propagation.  \sep Our contributions. We propose a new method that uses weak supervision to train deep text classifiers in a label-efficient way, while addressing the label noise and label coverage issues. We assume multiple weak supervision sources  provide complementary sets of heuristic rules. \zc{the previous two sentences can be merged.} Our idea is that the complementary information in the multiple sources can not only reduce label noise, but also effectively bootstrap on unlabeled data to improve label coverage, making it possible to learn an accurate deep text classifier with weak supervision.   Motivated by the above, we propose a model with two carefully designed components. The first component is a rule-based classifier \zc{ rule reliability estimators} using the conditional soft attention mechanism. Given weak labels from annotators and document representations, we learn reliability scores for labeling sources, which emphasize the weak annotators' opinions that are most informative for our particular corpus. We then use the reliability scores to aggregate our disparate weak labels into a denoised pseudo label. \zc{need to highlight that our rule reliability is conditional on input text features}  The second component is a neural classifier that learns labels and distributed feature representations for all samples, matched and unmatched. This neural classifier is supervised by both the denoised labels and its own confident predictions on the unmatched data, enabling it to solve the rule coverage problem while simultaneously enhancing the rule denoiser via patterns present in the unmatched data.  The two components are integrated into a end-to-end training framework.  \zc{maybe we should also say we use pre-trained BERT as our feature extractor:   its representation power can help our denoiser work better.}  We evaluate our model on four text classification tasks, including sentiment analysis, topic classification, spam classification, and information extraction. The results on five benchmarks show that:  the soft-attention module can indeed effectively denoise the noisy training data induced from weak supervision sources, achieving \textasciitilde{}\% accuracy for denoising; and  the co-training design can improve prediction accuracy for unmatched samples, achieving at least \% accuracy increase on them. In terms of the overall performance, our model consistently outperforms state-of-the-art weakly supervised methods , semi-supervised methods , and fine-tuning methods   by 9.2\% on average. Further, we show that the denoised labels can be fed into fully supervised models and fine-tune the models to improve their performance.   % Our contributions are summarized as follows: %      %  =============================================== % Chao: I outline a structure for the intro, fill and extend these paragraphs!  % % Paragraph 1: Text classification is one of the most fundamental problems in text mining, information retrieval, and natural language processing. While deep neural nets % % have achieved dominant performance for text classification, they are highly label-hungry, often requiring hundreds of thousands of labeled samples to achieve strong performance.  This has become a key bottleneck of applying deep % % text classifiers to many real-life applications, where large-scale labeled data are too expensive to obtain.  % % Paragraph 2: An overview of existing methods for handling label sparsity. Including:  % % self-training methods, % % fine-tuning methods,  % % weakly supervised methods. Think hard about their drawbacks.  % % Paragraph 3: An overview of our model: we propose a deep neural text classifier, which is learned not from excessive labeled data, but only unlabeled data plus a set of easy-to-provide heuristic rules.  % % Paragraph 4: Two challenges of learning from rules: Learning the model from heuristic rules is difficult, because the rules can only induce noisy training data and can have limited coverage.  % % Paragraph 5: How we address the two challenges: % % First, it has a label denoising module, which estimates source reliability and denoises rule-induced supervision with a soft attention mechanism. Second, it has a self-learning module for improving the label coverage issue, which iteratively predicts soft labels for unmatched samples by aggregating the denoised multi-source classifiers. The two modules are integrated into a neural co-training model, which can be learned in an end-to-end manner.  % % Paragraph 6: The results we obtain on real data  % % A bullet list summarizing our contributions:      \zc{related work in the appendix? That may not be a good choice.}   We review existing relevant work in three areas:  learning from noisy   supervision;  learning from multiple sources;  self-training and   co-training.  \sep Learning from Noisy Supervision. Our work is closely related to existing work on learning from noisy supervision. To deal with label noise, several studies  adopt a data cleaning approach that detects and removes mislabeled instances. This is achieved by outlier detection , a-priori heuristics , self-training , or reinforcement learning . One drawback of this data cleaning approach is that it can discard many samples and incur information loss.  Different from data cleaning, some works adopt a data correction approach. The most prominent idea in this line is to estimate the noise transition matrix among labels  and then use the transition matrices to re-label the instances or adapt the loss functions. Specifically, \citet{DBLP:conf/emnlp/WangLLYL19} and \citet{DBLP:journals/corr/abs-1911-00068} generate label noise by flipping clean labels based on such noise transition matrices. They are thus not applicable to our weak supervision setting where no clean labels are given. Meanwhile, re-weighting strategies have been explored to adjust the input training data. These techniques weigh training samples according to the predictions confidence , one-sided noise assumption , a clean set  or the similarity of their descent directions . Recently, a few studies  have also explored designing denoising modules for neural networks. However, our method differs from them in that:  our method learns conditional reliability scores for multiple sources; and  these methods still require clean data for denoising, while ours does not.  \sep Learning from Multi-Source Supervision  The crowdsourcing area also faces the problem of learning from multiple sources . Different strategies have been proposed to integrate the annotations for the same instance, such as estimating the confidence intervals for workers  or leveraging approval voting . Compared with crowdsourcing, our problem is different in that the multiple sources provide only feature-level noisy supervision instead of instance-level supervision.  More related to our work are data programming methods  that learn from multiple weak supervision sources. One seminal work in this line is Snorkel , which treats true labels as latent variables in a generative model and weak labels as noisy observations. The generative model is learned to estimate the latent variables, and the denoised training data are used to learn classifiers. Our approach differs from data programming methods where we use a soft attention mechanism to estimate source reliability, which is integrated into neural text classifiers to improve the performance on unmatched samples.    in two ways. First, instead of using generative models to estimate the latent clean labels, we use a soft attention mechanism to estimate source reliability, which can be integrated into neural text classifiers and trained end-to-end. Second, data programming methods suffer from limited performance on unmatched samples, while our approach addresses this issue by jointly training a label denoiser and neural classifier.  \sep Self-training  Self-training is a classic technique for learning from limited supervision . The key idea is to use a model's confident predictions to update the model itself iteratively. However, one major drawback of self-training is that it is sensitive to noise, \ie, the model can be mis-guided by its own wrong predictions and suffer from error propagation .  Although self-training is a common technique in semi-supervised learning, only a few works like WeSTClass  have applied it to weakly-supervised learning. Our self-training differs from WeSTClass in two aspects: 1) it performs weighted aggregation of the predictions from multiple sources, which generates higher-quality pseudo labels and makes the model less sensitive to the error in one single source; 2) it uses temporal ensembling, which aggregates historical pseudo labels and alleviates noise propagation.   Co-training  extends self-training by allowing two  classifiers to exchange their expertise until they reach a consensus.  This is  typically be achieved by letting the two classifiers annotate unlabeled samples  with their confident predictions and update each other. Recently, the  co-training idea has been extended to deep neural networks. Qiao \etal   used co-training to train two networks that have consistent predictions over all the samples, and meanwhile make each network resistant to   the adversarial examples from its peer network to prevent them from collapsing   into each other.   used different regularizations and input   augmentation conditions to improve deep co-training.     proposed the Tri-Net model, which uses output smearing to initialize modules   and then fine-tunes on labeled data to augment model diversity. Compared with   these self-training and co-training methods, our method alleviates the error   propagation issue by estimating conditional source reliability and performing   weighted majority voting. Moreover, these methods are designed for   semi-supervised settings with clean supervision, whereas our method learns from   noisy supervision.  Cyberbullying detection on social media attracts growing attention in recent years. It is also crucial to understand why a media session is detected as cyberbullying. Thus we study the novel problem of explainable cyberbullying detection that aims at improving detection performance and highlighting explainable comments. We propose a novel deep learning-based model, HEterogeneous Neural Interaction Networks , to learn various feature representations from comment encodings, post-comment co-attention, and graph-based interactions between sessions and posts. Experimental results exhibit both promising performance and evidential explanation of HENIN. We also find that the learning of graph-based session-session and post-post interactions contributes most to the performance. Such results can encourage future studies to develop advanced graph neural networks in better representing the interactions between heterogeneous information. In addition, it is worthwhile to further model information propagation and temporal correlation of comments in the future.   
","     \zc{related work in the appendix? That may not be a good choice.}   We review existing relevant work in three areas:  learning from noisy   supervision;  learning from multiple sources;  self-training and   co-training.  \sep Learning from Noisy Supervision. Our work is closely related to existing work on learning from noisy supervision. To deal with label noise, several studies  adopt a data cleaning approach that detects and removes mislabeled instances. This is achieved by outlier detection , a-priori heuristics , self-training , or reinforcement learning . One drawback of this data cleaning approach is that it can discard many samples and incur information loss.  Different from data cleaning, some works adopt a data correction approach. The most prominent idea in this line is to estimate the noise transition matrix among labels  and then use the transition matrices to re-label the instances or adapt the loss functions. Specifically, \citet{DBLP:conf/emnlp/WangLLYL19} and \citet{DBLP:journals/corr/abs-1911-00068} generate label noise by flipping clean labels based on such noise transition matrices. They are thus not applicable to our weak supervision setting where no clean labels are given. Meanwhile, re-weighting strategies have been explored to adjust the input training data. These techniques weigh training samples according to the predictions confidence , one-sided noise assumption , a clean set  or the similarity of their descent directions . Recently, a few studies  have also explored designing denoising modules for neural networks. However, our method differs from them in that:  our method learns conditional reliability scores for multiple sources; and  these methods still require clean data for denoising, while ours does not.  \sep Learning from Multi-Source Supervision  The crowdsourcing area also faces the problem of learning from multiple sources . Different strategies have been proposed to integrate the annotations for the same instance, such as estimating the confidence intervals for workers  or leveraging approval voting . Compared with crowdsourcing, our problem is different in that the multiple sources provide only feature-level noisy supervision instead of instance-level supervision.  More related to our work are data programming methods  that learn from multiple weak supervision sources. One seminal work in this line is Snorkel , which treats true labels as latent variables in a generative model and weak labels as noisy observations. The generative model is learned to estimate the latent variables, and the denoised training data are used to learn classifiers. Our approach differs from data programming methods where we use a soft attention mechanism to estimate source reliability, which is integrated into neural text classifiers to improve the performance on unmatched samples.    in two ways. First, instead of using generative models to estimate the latent clean labels, we use a soft attention mechanism to estimate source reliability, which can be integrated into neural text classifiers and trained end-to-end. Second, data programming methods suffer from limited performance on unmatched samples, while our approach addresses this issue by jointly training a label denoiser and neural classifier.  \sep Self-training  Self-training is a classic technique for learning from limited supervision . The key idea is to use a model's confident predictions to update the model itself iteratively. However, one major drawback of self-training is that it is sensitive to noise, \ie, the model can be mis-guided by its own wrong predictions and suffer from error propagation .  Although self-training is a common technique in semi-supervised learning, only a few works like WeSTClass  have applied it to weakly-supervised learning. Our self-training differs from WeSTClass in two aspects: 1) it performs weighted aggregation of the predictions from multiple sources, which generates higher-quality pseudo labels and makes the model less sensitive to the error in one single source; 2) it uses temporal ensembling, which aggregates historical pseudo labels and alleviates noise propagation.   Co-training  extends self-training by allowing two  classifiers to exchange their expertise until they reach a consensus.  This is  typically be achieved by letting the two classifiers annotate unlabeled samples  with their confident predictions and update each other. Recently, the  co-training idea has been extended to deep neural networks. Qiao \etal   used co-training to train two networks that have consistent predictions over all the samples, and meanwhile make each network resistant to   the adversarial examples from its peer network to prevent them from collapsing   into each other.   used different regularizations and input   augmentation conditions to improve deep co-training.     proposed the Tri-Net model, which uses output smearing to initialize modules   and then fine-tunes on labeled data to augment model diversity. Compared with   these self-training and co-training methods, our method alleviates the error   propagation issue by estimating conditional source reliability and performing   weighted majority voting. Moreover, these methods are designed for   semi-supervised settings with clean supervision, whereas our method learns from   noisy supervision.",71
"  Systematic reviews are part of the field of evidence-based analysis, and are a methodology for conducting literature surveys, where the focus is on comprehensively summarising and synthesising existing research for the purpose of answering research questions . The aim of this process is to be very broad coverage to avoid unknown bias creeping into results via the alternative of cherry-picking scientific results . %As many relevant documents as possible should be included, and the process should also be thoroughly documented to aid replicability.  Conducting systematic reviews requires trained researchers with domain knowledge. The stages of the process are time-consuming, but vary in how much physical and mental labour they require . As a result, systematic reviews suffer from three primary challenges :  So though systematic reviews have been shown to be very effective and less prone to human biases , these issues often prove prohibitive. \\  However, these challenges are well suited to Machine Learning solutions, and there has recently been an increase in interest in applying NLP to this process . In this paper, we investigate the feasibility of implementing the multi-stage human process of a systematic review as a Machine Learning pipeline. We construct a systematic review pipeline which aims to assist researchers and organisations focusing on livestock health in various African countries who previously performed reviews manually . The pipeline begins with scraping for articles, then classifies them into whether or not to include in the review, then identifies data to extract and outputs a spreadsheet. We discuss the technical options we evaluated at each steps. Pipeline components are evaluated with intrinsic metrics as well as more pragmatic, extrinsic, considerations such as time and effort saved.  While previous work exists surveying the applicability of various Machine Learning methods and toolkits to the systematic review process  and a few apply them, there are no extant studies that implement a full system and analyse the trade-offs between different methods of training data creation, different annotation schemas, human expert hours needed to build a system, and final accuracy. We experiment with all of these factors, as well as with a few different architectures, with the aim of informing the planning and implementation of systematic review automation more broadly.    To further this goal, we particularly experiment with low resource scenarios and with generalisability. We investigate different thresholds for training data for the document classifier and different annotation schemas for the data extraction. We additionally test the ability of the system to generalise to documents from new countries.   % also talk about not needing deep learning resources  Key research questions are as follows: \paragraph{Extraction} Which techniques are best for identifying and extracting the desired information? \paragraph{Data Requirements} How much labelled training data is needed? Can existing resources be leveraged? \paragraph{Re-usability} How generalisable is a pipeline to new diseases and countries? \paragraph{Performance} What is the trade-off between pipeline accuracy and human time savings? \paragraph{Architecture \& Pre-training} How important is model architecture as applied to extraction tasks? How important is embedding pre-training, and how important is pre-training on scientific literature vs. general content ?\\  We find that surprisingly little training data  are necessary to get an accurate document classifier, and that it generalises well to unseen African countries , which enables systematic reviews to be expanded to new areas with essentially constant time. In our text extraction experiments, we find that both sentence and phrase level extraction models can each play a role in such a pipeline,  %given their complementary strengths and weaknesses on this kind of data,  but that phrase extraction, which has not previously been done for this task, performed better than expected both with baseline CNN models  and with BERT-based Transformers , with Transformers based on scientific pre-training  performing best. We demonstrate how the creation of labelled training data can be sped up through annotation tools, and that consideration should be given to the balance of training examples present within this data, since doing so may require less data overall while still maintaining good performance. Furthermore, besides automatic information extraction, much labour in constructing systematic reviews can be saved through simply automating the process of searching and downloading documents.   We empirically demonstrate that most of the three month pipeline of a systematic review can be automated to require very little human intervention, with acceptable accuracy of results. We release our code, annotation schema, and labelled data to assist in the expansion of systematic reviews via automation.  While we demonstrate this system on one domain, the framework is domain independent and could be applied to other kinds of systematic reviews. New training data and annotation schemes would be necessary to switch to medical or other domains, but our findings on time saving processes for annotation  would apply, and confidence thresholds that we implement are adjustable to customise to different levels of accuracy to human time trade-offs that are appropriate to different fields. Our exploration into necessary amounts of training data for accuracy and generalisability are broadly applicable.     The application of NLP to systematic reviews is relatively new, but has been recently receiving more attention. There is a growing body of work that assesses the potential for automation in systematic reviews, but little that builds systems for the purpose and tests them empirically.   review available tools that can be used to automate each element of the systematic review pipeline.  further review opportunities for semi-automation and assess opportunities and risks.  conduct systematic reviews of automation for systematic reviews.  analyse the systematic review pipeline to find ways that human-machine collaboration can be applied and improve the speed.     create a PDF viewer that humans can use to make the systematic review process easier and faster, by training a CNN to assess risk of bias in a document  and identifies and displays sentences to the user that contain a subset of the information necessary for a systematic review.  create an extraction system that identifies sentences and then post-processes them to extract data, but operate only on structured HTML \& XML.  apply fine-tuned BERT-based Transformers to the task of to sentence classification for semi-automated systematic review.  build a PDF retrieval system for systematic reviews for psychology and use a random forest classifier to identify sentences for extraction.  As far as we are aware, no other work builds a phrase-based system, tests data volume and generalisability, or applies a diverse set of modern architectures to the task.           \item Compositionality provides an explanation for why LSTMs learn long-range connections slowly and how LSTMs take advantage of linguistic structure.       \item Long-range connections build on predictable short range connections during training.       \item Familiar patterns attract new significance by encouraging interdependence, even at the cost of more general predictors.       \item Syntactically associated words have higher interdependence in English.  Using our proposed tool of Decompositional Interdependence, we illustrate how information exchanged between words aligns roughly with syntactic structure, indicating LSTMs compose meaning bottom-up. Synthetic experiments then illustrate that a memorized span intervening between a long distance dependency promotes early learning of the dependency rule, but fails to generalize to new domains, implying that these memorized spans are used as scaffolding in a bottom-up learning process.   This combination of behaviors is similar to a syntactic language model, suggesting that the LSTM's demonstrated inductive bias towards hierarchical structures is implicitly aligned with our understanding of language and emerges from its natural learning process.  
","  The application of NLP to systematic reviews is relatively new, but has been recently receiving more attention. There is a growing body of work that assesses the potential for automation in systematic reviews, but little that builds systems for the purpose and tests them empirically.   review available tools that can be used to automate each element of the systematic review pipeline.  further review opportunities for semi-automation and assess opportunities and risks.  conduct systematic reviews of automation for systematic reviews.  analyse the systematic review pipeline to find ways that human-machine collaboration can be applied and improve the speed.     create a PDF viewer that humans can use to make the systematic review process easier and faster, by training a CNN to assess risk of bias in a document  and identifies and displays sentences to the user that contain a subset of the information necessary for a systematic review.  create an extraction system that identifies sentences and then post-processes them to extract data, but operate only on structured HTML \& XML.  apply fine-tuned BERT-based Transformers to the task of to sentence classification for semi-automated systematic review.  build a PDF retrieval system for systematic reviews for psychology and use a random forest classifier to identify sentences for extraction.  As far as we are aware, no other work builds a phrase-based system, tests data volume and generalisability, or applies a diverse set of modern architectures to the task.",72
"   Cryptography has been used since antiquity to encode important secrets.  There are many unsolved ciphers of historical interest, residing in national libraries, private archives, and recent corpora collection projects .  Solving classical ciphers with automatic methods is a needed step in analyzing these materials.  In this work, we are concerned with automatic algorithms for solving a historically-common type of book code, in which word tokens are systematically replaced with numerical codes. Encoding and decoding are done with reference to a dictionary possessed by both sender and recipient.  While this type of code is common, automatic decipherment algorithms do not yet exist.  The contributions of our work are:         Figure gives a simplified typology of classical, substitution-based cryptosystems.\footnote{Our typology is geared toward explaining our contribution in the context of related systems.  For a fuller picture of classical cryptology, the reader is directly to  and .  For example, we do not discuss here systems in which a substitution key evolves during the encoding process, such as the Vigen\`ere cipher or the German Enigma machine.}  {\bf Table-based Ciphers} involve character-based substitutions.  The substitution may take the form of a simple offset, as in the Caesar substitution system, e.g., , , , etc.  The Caesar cipher can be easily solved by algorithm, since there are only 26 offsets to check.  The algorithm need only be able to recognize which of the 26 candidate plaintexts form good English.  Since 25 of the candidates will be gibberish, even the simplest language model will suffice.  A simple substitution cipher uses a substitution table built by randomly permuting the alphabet.  Since there are 26!  4~~10 possible tables, algorithmic decipherment is more difficult.  However, there are many successful algorithms, e.g., .  Many of these systems search for substitution tables that result in candidate plaintexts that score well according to a character n-gram language model , and they use search techniques like hill-climbing, expectation-maximization, beam search, and exact search.  The main practical challenge is to decipher short messages.  In a very long ciphertext, it is easy to ``spot the q'' because it is always followed by the same cipher character, which we can immediately guess stands for plaintext u, and so forth.    More sophisticated ciphers use homophonic substitution, in which plaintext characters are replaced non-deterministically.  By applying high nondeterminism to frequent characters, the cryptographer can flatten out ciphertext character frequencies. Homophonic ciphers occur frequently in historical collections.  The Copiale cipher  is a well-known example from a German secret society in the 1700s.  These ciphers can also be attacked successfully by algorithm.  For example, the homophonic Zodiac 408 cipher can be solved with EM with restarts , Bayesian sampling , or beam search  .   employ a more powerful character-based neural language model to break short ciphers more accurately.  In the present work, we use a word-based neural language model.  {\bf Book-based ciphers} increase homophony, and also avoid physical substitution tables that can be stolen or prove incriminating.  In a book-based cipher, sender and recipient verbally agree up front on an innocuous-looking shared document , such as the US~Declaration of Independence, or a specific edition of the novel Moby Dick.  When enciphering a plaintext letter token like f, the sender selects a random letter f from the shared document---if it is the 712th character in the document, the plaintext f might be enciphered as 712.  The next plaintext f might be enciphered differently.    solve one of the most well-known book ciphers, part two of the Beale Cipher .  Surprisingly, they treat the cipher as a regular homophonic cipher, using the same beam-search algorithm as for the table-based Zodiac 408 cipher, together with an 8-gram character language model.  One might imagine exploiting the fact that the book is itself written in English, so that if ciphertext unit 712 is known to be f, then ciphertext unit 713 is probably not h, as fh is unlikely to appear in the book.  's simple, effective algorithm ignores such constraints.  Other methods have been proposed for attacking book ciphers, such as crib dragging .   {\bf Codes}, in contrast to ciphers, make substitutions at the whole-word level.\footnote{Commonly, a single historical system will mix letter substitutions and word substitutions. Such a system is called a nomenclator.}  A large proportion of the encrypted material in  consists of {\bf table-based codes}. A famous example is Antoine Rossignol's Grand Chiffre, used during the reign of Louis~XIV.  The sender and receiver each own copies of huge specially-prepared tables that map words onto numbers .  If the enciphering tables are kept secret, this type of code is very hard to break.  One might guess that the most frequent cipher token stands for the word the, but it quickly becomes challenging to decide which number means practice and which means paragon.  Even so,  take on the task of automatically deciphering newswire encrypted with an arbitrary word-based substitution code, employing a slice-sampling Bayesian technique.  Given a huge ciphertext of 50,000 words, they can decipher 50\  of those tokens correctly.  From one billion ciphertext tokens, they recover over 90\  of the word tokens.  However, this method is clearly inapplicable in the world of short-cipher correspondence.  In the present work, we consider {\bf book-based codes}. Instead of using specially-prepared tables, the sender and receiver verbally agree to use an already-existing book as a key.  Because it may be difficult to find a word like paragon in a novel like Moby Dick, the sender and receiver often agree on a shared pocket dictionary, which has nearly all the words.  If paragon were the 10,439th word in the dictionary, the sender might encode it as 10439.    Such codes have been popular throughout history, employed for example by George Scovell during the Napoleonic Wars , and by John Jay during the US~Revolutionary War .  They were used as late as World War~II, when German diplomats employed the Langenscheidt's Spanish-German Pocket Dictionary as a key to communicate between the cities of Chapultepec, Mexico and Nauen, Germany .  In that case, the US~Coast Guard intercepted messages and was able to make a bit of headway in deciphering them, but the real breakthrough came only when they obtained the applicable dictionary .  Unfortunately, there appear to be no automatic algorithms for solving book-based codes without the key.\footnote{ suggests that national security services have long ago digitized all published books and applied brute-force to find the book that renders a given code into natural plaintext.}  According to :    In this paper, we develop an algorithm for automatically attacking book-based codes, and we apply it to a corpus of historically-important codes from the late 1700s.     We implement and evaluate techniques to pronounce Chinese text in Mandarin, without the use of a pronunciation dictionary or parallel resource.  The EM method achieves a test-set accuracy of 71\ , while the vector-based method achieves 81\ .  By combining the two methods, we obtain 89\  accuracy, which significantly exceeds that of prior work.  We also demonstrate that current methods for unsupervised matching of vector spaces are sensitive to the structure of the spaces.  In the presence of one-to-many mappings between pinyin and characters,  the mapping accuracy is severely downgraded, leaving open an opportunity to design more robust unsupervised vector mapping systems.   
","    Figure gives a simplified typology of classical, substitution-based cryptosystems.\footnote{Our typology is geared toward explaining our contribution in the context of related systems.  For a fuller picture of classical cryptology, the reader is directly to  and .  For example, we do not discuss here systems in which a substitution key evolves during the encoding process, such as the Vigen\`ere cipher or the German Enigma machine.}  {\bf Table-based Ciphers} involve character-based substitutions.  The substitution may take the form of a simple offset, as in the Caesar substitution system, e.g., , , , etc.  The Caesar cipher can be easily solved by algorithm, since there are only 26 offsets to check.  The algorithm need only be able to recognize which of the 26 candidate plaintexts form good English.  Since 25 of the candidates will be gibberish, even the simplest language model will suffice.  A simple substitution cipher uses a substitution table built by randomly permuting the alphabet.  Since there are 26!  4~~10 possible tables, algorithmic decipherment is more difficult.  However, there are many successful algorithms, e.g., .  Many of these systems search for substitution tables that result in candidate plaintexts that score well according to a character n-gram language model , and they use search techniques like hill-climbing, expectation-maximization, beam search, and exact search.  The main practical challenge is to decipher short messages.  In a very long ciphertext, it is easy to ``spot the q'' because it is always followed by the same cipher character, which we can immediately guess stands for plaintext u, and so forth.    More sophisticated ciphers use homophonic substitution, in which plaintext characters are replaced non-deterministically.  By applying high nondeterminism to frequent characters, the cryptographer can flatten out ciphertext character frequencies. Homophonic ciphers occur frequently in historical collections.  The Copiale cipher  is a well-known example from a German secret society in the 1700s.  These ciphers can also be attacked successfully by algorithm.  For example, the homophonic Zodiac 408 cipher can be solved with EM with restarts , Bayesian sampling , or beam search  .   employ a more powerful character-based neural language model to break short ciphers more accurately.  In the present work, we use a word-based neural language model.  {\bf Book-based ciphers} increase homophony, and also avoid physical substitution tables that can be stolen or prove incriminating.  In a book-based cipher, sender and recipient verbally agree up front on an innocuous-looking shared document , such as the US~Declaration of Independence, or a specific edition of the novel Moby Dick.  When enciphering a plaintext letter token like f, the sender selects a random letter f from the shared document---if it is the 712th character in the document, the plaintext f might be enciphered as 712.  The next plaintext f might be enciphered differently.    solve one of the most well-known book ciphers, part two of the Beale Cipher .  Surprisingly, they treat the cipher as a regular homophonic cipher, using the same beam-search algorithm as for the table-based Zodiac 408 cipher, together with an 8-gram character language model.  One might imagine exploiting the fact that the book is itself written in English, so that if ciphertext unit 712 is known to be f, then ciphertext unit 713 is probably not h, as fh is unlikely to appear in the book.  's simple, effective algorithm ignores such constraints.  Other methods have been proposed for attacking book ciphers, such as crib dragging .   {\bf Codes}, in contrast to ciphers, make substitutions at the whole-word level.\footnote{Commonly, a single historical system will mix letter substitutions and word substitutions. Such a system is called a nomenclator.}  A large proportion of the encrypted material in  consists of {\bf table-based codes}. A famous example is Antoine Rossignol's Grand Chiffre, used during the reign of Louis~XIV.  The sender and receiver each own copies of huge specially-prepared tables that map words onto numbers .  If the enciphering tables are kept secret, this type of code is very hard to break.  One might guess that the most frequent cipher token stands for the word the, but it quickly becomes challenging to decide which number means practice and which means paragon.  Even so,  take on the task of automatically deciphering newswire encrypted with an arbitrary word-based substitution code, employing a slice-sampling Bayesian technique.  Given a huge ciphertext of 50,000 words, they can decipher 50\  of those tokens correctly.  From one billion ciphertext tokens, they recover over 90\  of the word tokens.  However, this method is clearly inapplicable in the world of short-cipher correspondence.  In the present work, we consider {\bf book-based codes}. Instead of using specially-prepared tables, the sender and receiver verbally agree to use an already-existing book as a key.  Because it may be difficult to find a word like paragon in a novel like Moby Dick, the sender and receiver often agree on a shared pocket dictionary, which has nearly all the words.  If paragon were the 10,439th word in the dictionary, the sender might encode it as 10439.    Such codes have been popular throughout history, employed for example by George Scovell during the Napoleonic Wars , and by John Jay during the US~Revolutionary War .  They were used as late as World War~II, when German diplomats employed the Langenscheidt's Spanish-German Pocket Dictionary as a key to communicate between the cities of Chapultepec, Mexico and Nauen, Germany .  In that case, the US~Coast Guard intercepted messages and was able to make a bit of headway in deciphering them, but the real breakthrough came only when they obtained the applicable dictionary .  Unfortunately, there appear to be no automatic algorithms for solving book-based codes without the key.\footnote{ suggests that national security services have long ago digitized all published books and applied brute-force to find the book that renders a given code into natural plaintext.}  According to :    In this paper, we develop an algorithm for automatically attacking book-based codes, and we apply it to a corpus of historically-important codes from the late 1700s.",73
"   Neural network language models , pretrained on vast amounts of raw text, have become  the dominant input to downstream tasks . Commonly, these tasks involve aspects of language  comprehension . One explicit example is coreference resolution, wherein anaphora  are linked to antecedents  requiring knowledge of syntax, semantics,  and world-knowledge to match human-like comprehension.   Recent work has suggested that LMs acquire abstract, often human-like, knowledge of syntax  \cite[e.g.,][]{gulordavaetal18, futrelletal2018, huetal2020-systematic}. Additionally, knowledge of grammatical and referential aspects linking a pronoun to its antecedent noun   have been demonstrated for both  transformer and long short-term memory architectures . Humans are able  to modulate both referential and syntactic comprehension  given abstract linguistic knowledge . Contrary to humans, we find that discourse structure  only influences LM behavior  for reference, not syntax, despite model representations that encode the necessary discourse information.  The particular discourse structure we examined is governed by implicit causality  verbs . Such verbs influence pronoun comprehension:  \ex.      \a. Sally frightened Mary because she was so terrifying.      \b. Sally feared Mary because she was so terrifying.   In , she agrees in gender with both Sally and Mary, so  both are possible antecedents. However, English speakers overwhelmingly  interpret she as referring to Sally in  and Mary  in , despite the semantic overlap between the verbs. Verbs that  have a subject preference  are called subject-biased IC verbs, and verbs with a object preference  are called object-biased IC verbs.   In addition to pronoun resolution, IC verbs also interact with relative clause  attachment:   \ex.      \a.  John babysits the children of the musician who...         \a.  ...lives in La Jolla.         \b.  ...are students at a private school.         \z.     \b.  John detests the children of the musician who...         \a.  ...lives in La Jolla.         \b.  ...are arrogant and rude.          \z.     \z.     \citep[from][]{rohdeetal2011}  In ,  and  are sentence fragments with possible  continuations modifying the musician in  and  and  continuations modifying the children in  and . We might expect  human continuation preferences to be the same in  and . However, the use  of an object-biased IC verb  in  increases the proportion of continuations given by human participants  that refer to the children . Without  an object-biased IC verb the majority of continuations refer to the more recent noun  .  Effects  of IC have received renewed interest in the field of psycholinguistics in recent years \cite[e.g.,][]{kehler2008coherence, ferstl2011implicit, hartshorne2013verb, hartshorne2014, williams_IC_2020}. Current accounts of IC claim that the phenomenon is inherently a linguistic process, which  does not rely on additional pragmatic inferences by comprehenders \cite[e.g.,][]{rohdeetal2011, hartshorne2013verb}. Thus, IC is argued to be contained within the linguistic signal, analogous to  evidence of syntactic agreement and verb argument structure within corpora. We  hypothesize that if these claims are correct, then current LMs will be able to  condition reference and syntactic attachment by  IC verbs with just language data .   We tested this hypothesis using unidirectional transformer and long short-term memory network \citep[LSTM;][]{hochreiterschmidhuber97} language models. We find that LSTM  LMs fail to acquire a subject/object-biased IC distinction that influences reference or RC attachment.   In contrast, transformers learned a representational  distinction between subject-biased and object-biased IC verbs that interacts  with both reference and RC attachment,  but the distinction only influenced model output for reference. The apparent failure of model  syntactic behavior to exhibit an IC  contrast that is present in model representations raises questions  about the broader capacity of LMs to display  human-like linguistic knowledge.    The ability of LMs to encode referential knowledge has largely  been explored in the domain of coreference resolution. Prior  work has suggested that LMs can learn coreference resolution to  some extent \cite[e.g.,][]{petersetal18, sorodoc-etal-2020-probing}. In the present study, we focus on within-sentence resolution  rather than the ability of LMs to track entities over larger  spans of text \cite[cf.][]{sorodoc-etal-2020-probing}. Previous  work at this granularity of coreference resolution has shown LSTM LMs strongly  favor reference to male entities ,  for which the present study finds additional support. Rather  than utilizing a more limited modeling objective such as coreference resolution \cite[cf.][]{cheng-erk-2020-attending},  we followed \citet{sorodoc-etal-2020-probing} in focusing on the  representation of referential knowledge by models trained with  a general language modeling objective.   With regards to linguistic representations, a growing body of literature  suggests that LSTM LMs are able to acquire syntactic  knowledge. In particular, subject-verb agreement has been explored extensively \cite[e.g.,][]{linzenetal16, bernardyetal2017, Enguehard17} with results at human level performance in some cases . Additionally, work has shown human-like behavior when processing reflexive pronouns,  negative polarity items , center embedding, and syntactic islands . This literature generally suggests that LMs encode some type of abstract  syntactic representation \cite[e.g.,][]{prasadetal2019}. Additionally, recent work has shown  LMs learn linguistic representations beyond syntax, such  as pragmatics and discourse structure  .  The robustness of these abstract linguistic representations, however,  have been questioned in recent work, suggesting that learned  abstractions are weaker than standardly assumed \cite[e.g.,][]{trasketal2018, vanSchijndeletal2019, kodner-gupta-2020-overestimation, davis-van-schijndel-2020-recurrent}. The present study builds on these recent developments by demonstrating  the inability of LMs to utilize discourse structure in syntactic  processing.      In this work, we show that it is possible to decipher a book-based cipher, using a known-plaintext attack and a neural English language model. We apply our method to letters written to and from US General James Wilkinson, and we recover 75.1\  of the word tokens correctly.    We believe word-based neural language models are a  powerful tool for decrypting classical codes and ciphers.  Because they have much lower perplexities than widely-used n-gram models, they can distinguish between candidate plaintexts that resemble English at a distance, versus candidate plaintexts that are grammatical, sensible, and relevant to the historical context.  
"," The ability of LMs to encode referential knowledge has largely  been explored in the domain of coreference resolution. Prior  work has suggested that LMs can learn coreference resolution to  some extent \cite[e.g.,][]{petersetal18, sorodoc-etal-2020-probing}. In the present study, we focus on within-sentence resolution  rather than the ability of LMs to track entities over larger  spans of text \cite[cf.][]{sorodoc-etal-2020-probing}. Previous  work at this granularity of coreference resolution has shown LSTM LMs strongly  favor reference to male entities ,  for which the present study finds additional support. Rather  than utilizing a more limited modeling objective such as coreference resolution \cite[cf.][]{cheng-erk-2020-attending},  we followed \citet{sorodoc-etal-2020-probing} in focusing on the  representation of referential knowledge by models trained with  a general language modeling objective.   With regards to linguistic representations, a growing body of literature  suggests that LSTM LMs are able to acquire syntactic  knowledge. In particular, subject-verb agreement has been explored extensively \cite[e.g.,][]{linzenetal16, bernardyetal2017, Enguehard17} with results at human level performance in some cases . Additionally, work has shown human-like behavior when processing reflexive pronouns,  negative polarity items , center embedding, and syntactic islands . This literature generally suggests that LMs encode some type of abstract  syntactic representation \cite[e.g.,][]{prasadetal2019}. Additionally, recent work has shown  LMs learn linguistic representations beyond syntax, such  as pragmatics and discourse structure  .  The robustness of these abstract linguistic representations, however,  have been questioned in recent work, suggesting that learned  abstractions are weaker than standardly assumed \cite[e.g.,][]{trasketal2018, vanSchijndeletal2019, kodner-gupta-2020-overestimation, davis-van-schijndel-2020-recurrent}. The present study builds on these recent developments by demonstrating  the inability of LMs to utilize discourse structure in syntactic  processing.",74
" Word ordering often determines the meaning of a sentence; therefore how to utilize the position information of a word sequence has been an important topic in NLP and widely investigated recently. A common approach for modeling word ordering is to use recurrent neural networks , such as long short-term memory   or gated  recurrent unit  , which use a hidden state to represent the information of an ordered sequence and update model weights by backpropagation through time  ; thus the ordering information can be modeled by this structure.  However, RNN and BPTT are very inefficient in modern GPU computation due to the difficulty of parallelization with the time dependency. To solve this problem, recent work, such as convolutional seq2seq  and Transformers  which apply convolutional neural network   and self-attention respectively, succeed to eliminate the time dependency to take the computational advantage of GPU.  Instead of storing the information of ordered sequences, these models utilize the position information by using a feature-level positional encoding. For example, convolutional seq2seq proposed learnable position embeddings to represent the positions in a sequence.  Recently, various pre-trained Transformer language models keep breaking state-of-the-art results in numerous NLP tasks.  There are many different ways to pre-train a Transformer language model. For example, using an encoder, decoder, or the whole part of the Transformer, adapting the self-attention masks, or training with different objectives .  However, in terms of positional encoding, most work only used a learned position embedding which is originally proposed in convolutional seq2seq  without any analysis, even different objectives may learn completely different position information.  Motivated by the above observations, our goal is to investigate what position information the pre-trained Transformers could learn under different settings. We conduct a deep analysis of the learned position embeddings among three iconic pre-trained Transformer language models: BERT , RoBERTa  and GPT-2 . To examine the performance of different NLP types, we conduct the experiments on text classification, language modeling, and machine translation, and empirically analyze and explain the meaning and influence of position embeddings from different aspects.  The contributions of this paper are 3-fold:     The concept of using position embedding on position-insensitive models was first proposed by convolutional seq2seq , which built an encoder-decoder architecture on convolutional neural networks. \citet{vaswani2017attention} proposed Transformers that used the self-attention mechanism in the basic blocks.  Because the attention mechanism is position-insensitive, it proposed a pre-defined sinusoidal function as positional encoding.  Pre-trained language models became a trend among many NLP tasks after  introduced ELMo. Affected by ELMo, OpenAI GPT  is the first pre-trained language model using a Transformer architecture, then many different variant of pre-trained Transformer including BERT , RoBERTa  and GPT-2  started evolving the researches of NLP tremendously. In Transformers, the attention values are the same in each input position. Thus, \citet{shaw2018self} proposed a relative position representation in the attention level to address this issue. \citet{dai2019transformer} used a segment-level recurrence mechanism on Transformers and also utilized an adaptive version of relative position embeddings inspired by \citet{shaw2018self}. Furthermore, \citet{wang2019encoding} extended the embedding space from real numbers to complex values  , which has been verified helpful in word embeddings~ , and also proposed a new learnable positional encoding function instead of a simple position embedding mapping.     The present study examined the extent to which  discourse structure, determined by implicit causality verbs,  could be acquired by  transformer and LSTM language models . Specifically, we evaluated, via comparison to human experiments, whether IC verb biases could influence reference and syntactic attachment in LMs. Analyses were conducted at two levels of granularity: model  behavior  and model representation . Given  the claims in recent literature that implicit causality arises without extra pragmatic inference on the part  of human comprehenders, we hypothesized that LMs  would be able to acquire such contrasts .   We found that LSTM LMs were unable to demonstrate  knowledge of IC either in influencing reference or syntax.  However, a  transformer  trained on the exact  same data as the LSTM LMs was able to partially represent an IC distinction, but model output was only influenced by IC bias when resolving reference, not syntactic attachment. In evaluating a transformer model trained on  vastly more data , we found a more robust, human-like sensitivity to IC bias when  resolving reference: subject-biased IC verbs  increased model preference for subject pronouns and object-biased IC verbs increased model preferences for object pronouns. However, the same mismatch as TransformerXL between model representation and model behavior  arose in processing syntactic attachment.   In contrast to our results,  \citet{davis-van-schijndel-2020b} showed syntactic predictions for LSTM LMs are influenced by some aspects of discourse structure. A simple explanation for these conflicting results may  be that the LMs we examined here are unable to learn the syntactic operation of attachment, and thus no influence of discourse can  surface. The erasure  of number agreement in the final layers of the transformer LMs  provides  compelling evidence towards this conclusion.\footnote{Further cross-linguistic evidence bearing on the inability of LSTM LMs, specifically, to learn relative clause attachment is given in \citet{davis-van-schijndel-2020-recurrent}.}   From a theoretical perspective, the present study provides additional support for the centering of  implicit causality within the linguistic signal proper.  That is, IC bias is learnable, to some degree,  without pragmatic inference as hypothesized in Section  \cite[see also][]{hartshorne2014}.  The mismatches  in syntactic representations and behavior suggest, however, that models ignore the abstract categories that are learned,  contrary to human findings \cite[cf.][]{rohdeetal2011}.   We believe a solution may lie in changing model  training objectives .  Psycholinguistic studies focusing on the interaction  of discourse and syntax have suggested that  coherence relations may be the unit of linguistic  prediction, in contrast to the next-word prediction  used in most language modeling work \cite[see][]{rohdeetal2011}. We leave to  future work an  investigation of this suggestion as well as  teasing apart the exact role that training data and  model architecture play in the interaction  between types of linguistic representation.    \section*{Acknowledgments} Thank you to members of the C.Psyd lab at Cornell, who gave feedback on an earlier form of this work. We would also like to thank the three anonymous reviewers for their comments and suggestions.       \section{Stereotypically gendered nouns used in referential experiments}                          
"," The concept of using position embedding on position-insensitive models was first proposed by convolutional seq2seq , which built an encoder-decoder architecture on convolutional neural networks. \citet{vaswani2017attention} proposed Transformers that used the self-attention mechanism in the basic blocks.  Because the attention mechanism is position-insensitive, it proposed a pre-defined sinusoidal function as positional encoding.  Pre-trained language models became a trend among many NLP tasks after  introduced ELMo. Affected by ELMo, OpenAI GPT  is the first pre-trained language model using a Transformer architecture, then many different variant of pre-trained Transformer including BERT , RoBERTa  and GPT-2  started evolving the researches of NLP tremendously. In Transformers, the attention values are the same in each input position. Thus, \citet{shaw2018self} proposed a relative position representation in the attention level to address this issue. \citet{dai2019transformer} used a segment-level recurrence mechanism on Transformers and also utilized an adaptive version of relative position embeddings inspired by \citet{shaw2018self}. Furthermore, \citet{wang2019encoding} extended the embedding space from real numbers to complex values  , which has been verified helpful in word embeddings~ , and also proposed a new learnable positional encoding function instead of a simple position embedding mapping.",75
"  Autoregressive sequence to sequence  models such as Transformers  are trained to maximize the log-likelihood of the target sequence, conditioned on the input sequence. Furthermore, approximate inference  is typically done using the beam search algorithm , which allows for a controlled exploration of the exponential search space. However, seq2seq models  suffer from a discrepancy between token level classification during learning and sequence level inference during search. This discrepancy also manifests itself in the form of the curse of sentence length i.e. the models' proclivity to generate shorter sentences during inference, which has received considerable attention in the literature .  In this work, we focus on how to better model long-tailed phenomena, i.e. predicting the long-tail of low-frequency words/tokens , in seq2seq models, on the task of Neural Machine Translation . Essentially, there are two mechanisms by which tokens with low frequency receive lower probabilities during prediction: firstly, the norms of the embeddings of low frequency tokens are smaller, which means that during the dot-product based softmax operation to generate a probability distribution over the vocabulary, they receive less probability. This has been well known in Image Classification  and Neural Language Models . Since NMT shares the same dot-product softmax operation, we observe that the same phenomenon holds true for NMT as well. For example, we observe a Spearman閳ユ獨 Rank Correlation of 0.43 between the norms of the token embeddings and their frequency, when a standard transformer model is trained on the IWSLT-14 De-En dataset . Secondly, for transformer based NMT, the embeddings for low frequency tokens lie in a different subregion of space than semantically similar high frequency tokens, due to the different rates of updates , thereby, making rare words token embeddings ineffective. Since these token embeddings have to match to the context vector for getting next-token probabilities, the dot-product similarity score is lower for low frequency tokens, even when they are semantically similar to the high frequency tokens.   Further, better modeling long-tailed phenomena has significant implications for several text generation tasks, as well as for compositional generalization . To this end, we primarily ask and seek answers to the following two fundamental questions in the context of NMT: By exploring these questions, we arrive at the conclusion that the widely used cross-entropy  loss limits NMT models' expressivity during inference and propose a new loss function to better incorporate the inductive biases of beam search.     At a high level, we categorize the solutions to better model long-tailed phenomena into three groups, namely, learning better representations, improving  classification and improvements in sequence inference algorithms. In this work, we will be mainly concerned with the interaction between  classification and sequence inference.  \paragraph{Better Representations} Many recent works  propose to either learn better representations for low-frequency tokens or to integrate pre-trained representations into NMT models. To better capture long range semantic structure, \citet{optimal_transport} argue for sequence level supervision during learning.  \paragraph{Long-Tailed Classification} A number of works, , have focused on designing algorithms that improve classification of low-frequency classes. Below, we list two such algorithms, used as baselines in section :     \paragraph{Focal Loss} Proposed in , Focal loss  increases the relative loss of low-confidence predictions vis-鑴-vis high confidence predictions, when compared to cross-entropy. It is described in equation , where  and  refers to the probability/confidence of the prediction.  \paragraph{-Normalization} \citet{iclr2020} link the norms of the penultimate  layer to the frequency of the class in image classification , and show that normalizing their weights  i.e. leads to improved classification:    Here,  is a hyperparameter. The intuition behind -Normalization is based on the simple observation that the norms of the penultimate layer dictate the feature span of the corresponding class during prediction.  At the sequence level, a parallel line of work has explored penalizing overconfident predictions , e.g., Label smoothing has been shown to yield consistent gains in seq2seq tasks .   \paragraph{Sequence Inference} \citet{diverse_beam_search, optimal_beam} try to modify beam search to allow for better exploring the output state space.      This paper investigates the implicit meaning of pre-trained Transformer position embeddings. Transformer encoders learn the local position information that can only be effective in masked language modeling. On the other hand, the Transformer decoders for autoregressive language modeling actually learn about absolute positions.  The empirical experiments on the pre-trained position embeddings validate our hypothesis. We also show that different NLP tasks with different model architectures and different training objectives may utilize the position information in different ways. As a result, it is believed that this study will benefit future work about choosing suitable positional encoding functions or designing other modeling methods for position information in the target NLP tasks based on their properties.  
","  At a high level, we categorize the solutions to better model long-tailed phenomena into three groups, namely, learning better representations, improving  classification and improvements in sequence inference algorithms. In this work, we will be mainly concerned with the interaction between  classification and sequence inference.  \paragraph{Better Representations} Many recent works  propose to either learn better representations for low-frequency tokens or to integrate pre-trained representations into NMT models. To better capture long range semantic structure, \citet{optimal_transport} argue for sequence level supervision during learning.  \paragraph{Long-Tailed Classification} A number of works, , have focused on designing algorithms that improve classification of low-frequency classes. Below, we list two such algorithms, used as baselines in section :     \paragraph{Focal Loss} Proposed in , Focal loss  increases the relative loss of low-confidence predictions vis-閼-vis high confidence predictions, when compared to cross-entropy. It is described in equation , where  and  refers to the probability/confidence of the prediction.  \paragraph{-Normalization} \citet{iclr2020} link the norms of the penultimate  layer to the frequency of the class in image classification , and show that normalizing their weights  i.e. leads to improved classification:    Here,  is a hyperparameter. The intuition behind -Normalization is based on the simple observation that the norms of the penultimate layer dictate the feature span of the corresponding class during prediction.  At the sequence level, a parallel line of work has explored penalizing overconfident predictions , e.g., Label smoothing has been shown to yield consistent gains in seq2seq tasks .   \paragraph{Sequence Inference} \citet{diverse_beam_search, optimal_beam} try to modify beam search to allow for better exploring the output state space.",76
" Grammar induction is the task of learning the grammar of a target corpus without exposure to the parsing ground truth or any expert-labeled tree structures . Recently emerging latent tree learning models provide a new approach to this problem . They learn syntactic parsing under only indirect supervision from their main training tasks such as language modelling and natural language inference.  In this study, we analyze ON-LSTM , a new latent tree learning model that set the state of the art on unsupervised constituency parsing on WSJ test  when it was published at ICLR 2019. The model is trained on language modelling and can generate binary constituency parsing trees of input sentences like the one in Figure .     As far as we know, though there is an excellent theoretical analysis paper  of the ON-LSTM model that focuses on the model's architecture and its parsing algorithm, there is no systematic analysis of the parses the model generates. There are no in-depth investigations of  whether the model's parsing behavior is consistent among different restarts or  how the parses it produces are different from PTB gold standards. Answering these questions is crucial for a better understanding of the capability of the model and may bring insights into how to build more advanced latent tree learning models in the future.  Therefore, we replicate the model with 5 random restarts and look into the parses it generates. We find that  ON-LSTM has fairly consistent parsing behaviors across different restarts, achieving a self F1 of 65.7 on WSJ test.  The model struggles to correctly parse the internal structures of complex noun phrases.  The model has a consistent tendency to overestimate the height of the split points right before verbs or auxiliary verbs, leading to a major difference between its parses and the Penn Treebank gold-standard parses. We speculate that both problems can be explained by the training task, unidirectional language modelling, and thus we hypothesize that training a bidirectional model on a more syntax-related task like acceptability judgement might be a good choice for future latent tree learning models.          ST-Gumbel  and RL-SPINN  are two earlier latent tree learning models. These models are designed to learn to parse input sentences in order to help solve a downstream sentence understanding task such as natural language inference. Since they are not designed to approximate PTB grammar , their unsupervised parsing F1's on WSJ test are relatively low .  PRPN  and URNNG  are two of the stronger latent tree learning models that have comparable unsupervised parsing performance  with ON-LSTM . URNNG is based on Recurrent Neural Network Grammar , a probablitic generative model; PRPN is a neural language model that implicitly models syntax using a structured attention mechanism.    ST-Gumbel  constructs task-specific trees using a novel Tree-LSTM-based architecture and a composition query vector. RL-SPINN  is based on SPINN , trained through reinforcement learning, and is able to match the performance of supervised Tree-LSTM models on Natural Language Inference.  \citet{williams2018} analyze ST-Gumbel and RL-SPINN. They find that though the two models perform well on sentence understanding, neither of the models induces consistent and non-trivial grammars.    ON-LSTM is the model analyzed in this work. It is a standard LSTM  equipped with a master forget gate  and a master input gate . The model is forced to erase and update lower dimensions when it needs to update a higher dimension of the cell state at a timestep. In this way, the model is encouraged to store long-term information in higher dimensions. When parsing a sentence, the model recursively chooses the token with the highest  as the split point until there are only unary sentence pieces left, where  is the estimate height of the split point in the forget gate.   Prior to this work, \citet{chris-dyer} also analyze ON-LSTM. They raise doubts on the necessity of the model's novel gates and mathematically prove that it is impossible for the parsing algorithm used by \citet{ONLSTMShen} to correctly parse a certain class of structures. In comparison, this study takes a more empirical approach that is similar to that of \citet{williams2018}.      In this work, we characterized the long-tailed phenomena in NMT and demonstrated that NMT models aren't able to effectively generate low-frequency tokens in the output. We proposed a new loss function, the Anti-Focal loss, to incorporate the inductive biases of beam search into the NMT training process. We conducted comprehensive evaluations on 9 language pairs with different amounts of training data from the IWSLT and TED corpora. Our proposed technique leads to gains across a range of metrics, improving long-tailed NMT at both the token as well as at the sequence level. In future, we wish to explore its connections to entropy regularization and model calibration and whether we can fully encode the inductive biases of label smoothing in the loss function itself.  
","      ST-Gumbel  and RL-SPINN  are two earlier latent tree learning models. These models are designed to learn to parse input sentences in order to help solve a downstream sentence understanding task such as natural language inference. Since they are not designed to approximate PTB grammar , their unsupervised parsing F1's on WSJ test are relatively low .  PRPN  and URNNG  are two of the stronger latent tree learning models that have comparable unsupervised parsing performance  with ON-LSTM . URNNG is based on Recurrent Neural Network Grammar , a probablitic generative model; PRPN is a neural language model that implicitly models syntax using a structured attention mechanism.    ST-Gumbel  constructs task-specific trees using a novel Tree-LSTM-based architecture and a composition query vector. RL-SPINN  is based on SPINN , trained through reinforcement learning, and is able to match the performance of supervised Tree-LSTM models on Natural Language Inference.  \citet{williams2018} analyze ST-Gumbel and RL-SPINN. They find that though the two models perform well on sentence understanding, neither of the models induces consistent and non-trivial grammars.    ON-LSTM is the model analyzed in this work. It is a standard LSTM  equipped with a master forget gate  and a master input gate . The model is forced to erase and update lower dimensions when it needs to update a higher dimension of the cell state at a timestep. In this way, the model is encouraged to store long-term information in higher dimensions. When parsing a sentence, the model recursively chooses the token with the highest  as the split point until there are only unary sentence pieces left, where  is the estimate height of the split point in the forget gate.   Prior to this work, \citet{chris-dyer} also analyze ON-LSTM. They raise doubts on the necessity of the model's novel gates and mathematically prove that it is impossible for the parsing algorithm used by \citet{ONLSTMShen} to correctly parse a certain class of structures. In comparison, this study takes a more empirical approach that is similar to that of \citet{williams2018}.",77
" Deep learning has become the dominant approach to address most Natural Language Processing  tasks, including text classification. With sufficient and high-quality training data, deep learning models can perform incredibly well . However, in real-world cases, such ideal datasets are scarce. Often times, the available datasets are small,  full of regular but irrelevant words, and contain unintended biases . These can lead to suboptimal models with undesirable properties. For example, the models may have biases against some sub-populations or may not work effectively in the wild as they overfit the imperfect training data.  To improve the models, previous work has looked into different techniques beyond standard model fitting. If the weaknesses of the training datasets or the models are anticipated, strategies can be tailored to mitigate such weaknesses. For example, augmenting the training data with gender-swapped input texts helps reduce gender bias in the models . Adversarial training can prevent the models from exploiting irrelevant and/or protected features . With a limited number of training examples, using human rationales or prior knowledge together with training labels can help the models perform better .  Nonetheless, there are side-effects of sub-optimal datasets that cannot be predicted and are only found after training thanks to post-hoc error analysis. To rectify such problems, there have been attempts to enable humans to fix the trained models  . Since the models are usually too complex to understand, manually modifying the model parameters is not possible. Existing techniques, therefore, allow humans to provide feedback on individual predictions instead. Then, additional training examples are created based on the feedback to retrain the models.  However, such local improvements for individual predictions could add up to inferior overall performance . Furthermore, these existing techniques allow us to rectify only errors related to examples at hand but provide no way to fix problems kept hidden in the model parameters.   In this paper, we propose a framework which allows humans to debug and improve deep text classifiers by disabling hidden features which are irrelevant to the classification task. We name this framework FIND . FIND exploits an explanation method, namely layer-wise relevance propagation  , to understand the behavior of a classifier when it predicts each training instance. Then it aggregates all the information using word clouds to create a global visual picture of the model. This enables humans to comprehend the features automatically learned by the deep classifier and then decide to disable some features that could undermine the prediction accuracy during testing. The main differences between our work and existing work are:   first, FIND leverages human feedback on the model components, not the individual predictions, to perform debugging;   second, FIND targets deep text classifiers which are more convoluted than traditional classifiers used in existing work .   We conducted three human experiments  to demonstrate the usefulness of FIND.  For all the experiments, we used as classifiers convolutional neural networks  ,  which are a popular, well-performing architecture for many text classification tasks including the tasks we experimented with . The overall results show that FIND with human-in-the-loop can improve the text classifiers and mitigate the said problems in the datasets.  After the experiments, we discuss the generalization of the proposed framework to other tasks and models. Overall, the {\bf main contributions} of this paper are:   The rest of this paper is organized as follows.  Section  explains related work about analyzing, explaining, and human-debugging text classifiers.  Section  proposes FIND, our debugging framework.  Section  explains the experimental setup followed by the three human experiments in Section  to . Finally, Section  discusses generalization of the framework and concludes the paper. Code and datasets of this paper are available at \url{https://github.com/plkumjorn/FIND}.     Analyzing deep NLP models --  There has been substantial work in gaining better understanding of complex, deep neural NLP models.  By visualizing dense hidden vectors, \citet{li-etal-2016-visualizing} found that some dimensions of the final representation learned by recurrent neural networks capture the effect of intensification and negation in the input text.  \citet{karpathy2015visualizing} revealed the existence of interpretable cells in a character-level LSTM model for language modelling. For example, they found a cell acting as a line length counter and cells checking if the current letter is inside a parenthesis or a quote.  \citet{jacovi-etal-2018-understanding} presented interesting findings about CNNs for text classification including the fact that one convolutional filter may detect more than one n-gram pattern and may also suppress negative n-grams. Many recent papers studied several types of knowledge in BERT , a deep transformer-based model for language understanding, and found that syntactic information is mostly captured in the middle BERT layers while the final BERT layers are the most task-specific . Inspired by many findings, we make the assumption that each dimension of the final representation  captures patterns or qualities in the input which are useful for classification. Therefore, understanding the roles of these dimensions  is a prerequisite for effective human-in-the-loop model debugging, and we exploit an explanation method to gain such an understanding.   Explaining predictions from text classifiers --  Several methods have been devised to generate explanations supporting  classifications in many forms, such as natural language texts , rules , extracted rationales , and attribution scores .  Some explanation methods, such as LIME  and SHAP , are model-agnostic and do not require access to model parameters.  Other methods access the model architectures and parameters to generate the explanations, such as DeepLIFT  and LRP  . In this work, we use LRP to explain not the predictions but the learned features so as to expose the model behavior to humans and enable informed model debugging.   Debugging text classifiers using human feedback -- Early work in this area comes from the human-computer interaction community. \citet{stumpf2009interacting} studied the types of feedback humans usually give in response to machine-generated predictions and explanations. Also, some of the feedback collected  was used to improve the classifier via a user co-training approach.  \citet{kulesza2015principles} presented an explanatory debugging approach in which the system explains to users how it made each prediction, and the users then rectify the model by adding/removing words from the explanation and adjusting important weights.  Even without explanations shown, an active learning framework proposed by \citet{settles-2011-closing} asks humans to iteratively label some chosen features  and adjusts the model parameters that correspond to the features.  However, these early works target simpler machine learning classifiers  and it is not clear how to apply the proposed approaches to deep text classifiers.   Recently, there have been new attempts to use explanations and human feedback to debug classifiers in general. Some of them were tested on traditional text classifiers.  For instance, \citet{ribeiro2016lime} showed a set of LIME explanations for individual SVM predictions to humans and asked them to remove irrelevant words from the training data in subsequent training. The process was run for three rounds to iteratively improve the classifiers. \citet{teso2019explanatory} proposed CAIPI, which is an explanatory interactive learning framework. At each iteration, it selects an unlabelled example to predict and explain to users using LIME, and the users respond by removing irrelevant features from the explanation. CAIPI then uses this feedback to generate augmented data and retrain the model.  While these recent works use feedback on low-level features  and individual predictions, our framework  uses feedback on the learned features with respect to the big picture of the model. This helps us avoid local decision pitfalls which usually occur in interactive machine learning . Overall, what makes our contribution different from existing work is that  we collect the feedback on the model, not the individual predictions, and  we target deep text classifiers which are more complex than the models used in previous work.      In summary, the model shows basic self-consistency on the task of constituency parsing, and it is consistently able to correctly identify certain constituents . All these results show that the unique design of the model brings us closer to developing consistently powerful unsupervised parsing models. However, the experiments show that it  struggles with the internal structures of complex NPs, and  often overestimates the height of the split points right before verbs. Based on our analysis, we hypothesize that both of the failures can be at least partially attributed to the use of unidirectional language modelling as the training task.   There are two potential problems with this training task. First, the motivation of language modelling generally does not perfectly match the target task constituency parsing, since cross-constituent hints are sometimes helpful, as revealed by . Second, it is very hard for a unidirectional model to correctly identify some high-level constituents, as revealed by . Therefore, we believe a promising research direction is to build latent tree learning models based on bidirectional model architectures like transformer  and the task of acceptability judgement with a dataset like CoLA , which is a more syntax-related sentence-level task that requires the model to predict whether an input sentence is grammatically acceptable. Another option to consider is masked language modelling because it is also a bidirectional task and is much easier to scale up compared to acceptability judgement since it is a self-supervised task.    
","  Analyzing deep NLP models --  There has been substantial work in gaining better understanding of complex, deep neural NLP models.  By visualizing dense hidden vectors, \citet{li-etal-2016-visualizing} found that some dimensions of the final representation learned by recurrent neural networks capture the effect of intensification and negation in the input text.  \citet{karpathy2015visualizing} revealed the existence of interpretable cells in a character-level LSTM model for language modelling. For example, they found a cell acting as a line length counter and cells checking if the current letter is inside a parenthesis or a quote.  \citet{jacovi-etal-2018-understanding} presented interesting findings about CNNs for text classification including the fact that one convolutional filter may detect more than one n-gram pattern and may also suppress negative n-grams. Many recent papers studied several types of knowledge in BERT , a deep transformer-based model for language understanding, and found that syntactic information is mostly captured in the middle BERT layers while the final BERT layers are the most task-specific . Inspired by many findings, we make the assumption that each dimension of the final representation  captures patterns or qualities in the input which are useful for classification. Therefore, understanding the roles of these dimensions  is a prerequisite for effective human-in-the-loop model debugging, and we exploit an explanation method to gain such an understanding.   Explaining predictions from text classifiers --  Several methods have been devised to generate explanations supporting  classifications in many forms, such as natural language texts , rules , extracted rationales , and attribution scores .  Some explanation methods, such as LIME  and SHAP , are model-agnostic and do not require access to model parameters.  Other methods access the model architectures and parameters to generate the explanations, such as DeepLIFT  and LRP  . In this work, we use LRP to explain not the predictions but the learned features so as to expose the model behavior to humans and enable informed model debugging.   Debugging text classifiers using human feedback -- Early work in this area comes from the human-computer interaction community. \citet{stumpf2009interacting} studied the types of feedback humans usually give in response to machine-generated predictions and explanations. Also, some of the feedback collected  was used to improve the classifier via a user co-training approach.  \citet{kulesza2015principles} presented an explanatory debugging approach in which the system explains to users how it made each prediction, and the users then rectify the model by adding/removing words from the explanation and adjusting important weights.  Even without explanations shown, an active learning framework proposed by \citet{settles-2011-closing} asks humans to iteratively label some chosen features  and adjusts the model parameters that correspond to the features.  However, these early works target simpler machine learning classifiers  and it is not clear how to apply the proposed approaches to deep text classifiers.   Recently, there have been new attempts to use explanations and human feedback to debug classifiers in general. Some of them were tested on traditional text classifiers.  For instance, \citet{ribeiro2016lime} showed a set of LIME explanations for individual SVM predictions to humans and asked them to remove irrelevant words from the training data in subsequent training. The process was run for three rounds to iteratively improve the classifiers. \citet{teso2019explanatory} proposed CAIPI, which is an explanatory interactive learning framework. At each iteration, it selects an unlabelled example to predict and explain to users using LIME, and the users respond by removing irrelevant features from the explanation. CAIPI then uses this feedback to generate augmented data and retrain the model.  While these recent works use feedback on low-level features  and individual predictions, our framework  uses feedback on the learned features with respect to the big picture of the model. This helps us avoid local decision pitfalls which usually occur in interactive machine learning . Overall, what makes our contribution different from existing work is that  we collect the feedback on the model, not the individual predictions, and  we target deep text classifiers which are more complex than the models used in previous work.",78
"     Neural summarizers have achieved impressive performance when evaluated by ROUGE ~ on in-domain setting, and the recent success of pre-trained models drives the state-of-the-art results on benchmarks to a new level ~. However, the superior performance is not a guarantee of a perfect system since exsiting models tend to show defects when evaluated from other aspects. For example, \citet{zhang-etal-2018-abstractiveness} observes that many abstractive systems tend to be near-extractive in practice. \citet{cao2018faithful,wang2020asking,kryscinski2019evaluating,maynez2020faithfulness,durmus2020feqa} reveal that most generated summaries are factually incorrect. These non-mainstream evaluation methods make it easier to identify the model's weaknesses.  Orthogonal to above two evaluation aspects, we aim to diagnose the limitation of existing systems under cross-dataset evaluation, in which a summarization system trained on  one corpus would be evaluated on a range of out-of-dataset corpora. Instead of evaluating the quality of summarizers solely based on one dataset or multiple datasets individually, cross-dataset evaluation enables us to evaluate model performance from a  different angle. For example, Fig. shows the ranking of  summarization systems studied in this paper under different  evaluation metrics, in which the ranking list `` in-dataset R2'' is obtained by traditional ranking criteria while other two are based on our designed cross-dataset measures. Intuitively, we observe that 1) there are different definitions of a ``good'' system in various evaluation aspects; 2) abstractive and extractive systems exhibit diverse behaviors when evaluated under the cross-dataset setting.    The above example recaps the general motivation of this work, encouraging us to rethink the generalization ability of current top-scoring summarization systems from the perspective of cross-dataset evaluation. Specifically, we ask two questions as follows:   Q1: {How do different neural architectures of summarizers influence the cross-dataset generalization performances?} When designing summarization systems, a plethora of neural components can be adopted ~. For example, will copy  and coverage   mechanisms improve the cross-dataset generalization ability of summarizers? Is there a risk that BERT-based summarizers will perform worse when adapted to new areas compared with the ones without BERT? So far, the generalization ability of current summarization systems when transferring to new datasets still remains unclear, which poses a significant challenge to design a reliable system in realistic scenarios. Thus, in this work, we take a closer look at the effect of model architectures on cross-dataset generalization setting.    Q2: {Do different generation ways  of summarizers influence the cross-dataset generalization ability?} Extractive and abstractive models, as two typical ways to summarize texts, usually follow diverse learning frameworks and favor different datasets.  It would be absorbing to know their discrepancy from the perspective of cross-dataset generalization.      To answer the questions above, we have conducted a comprehensive experimental analysis, which involves eleven summarization systems , five benchmark datasets from different domains, and two evaluation aspects. Tab. illustrates the overall analysis framework. We explore the effect of different architectures and generation ways on model generalization ability in order to answer Q1 and Q2. Semantic equivalency  and factuality are adopted to characterize the different aspects of cross-dataset generalization ability. Additionally, we strengthen our analysis by presenting two views of evaluation: holistic and fine-grained views .   }%        % \end{table}%  Our contributions can be summarized as: 1) Cross-dataset evaluation is orthogonal to other evaluation aspects , which can be used to re-evaluate current summarization systems, accelerating the creation of more robust summarization systems. 2) We have design two measures Stiffness and Stableness, which could help us to characterize generalization ability in different views, encouraging us to diagnose the weaknesses of state-of-the-art systems.  3) We conduct dataset bias-aided analysis  and suggest that a better understanding of datasets will be helpful for us to interpret systems'  behaviours.       Our work is connected to the following threads of topics of NLP research.  \paragraph{Cross-Dataset Generalization in NLP} Recently, more researchers shift their focus from individual dataset to cross-dataset evaluation, aiming to get a comprehensive understanding of system's generalization ability.  \citet{fried-etal-2019-cross} explores the generalization ability of different constituency parsers.  \citet{talmor-berant-2019-multiqa}, on the other hand, shows the generalization ability of reading comprehension models can be improved by pre-training on one or two other reading comprehension datasets. \citet{fu2020rethinking} studies the model generalization in the field of NER. They point out the bottleneck of the existing NER systems through in-depth analyses and provide suggestions for further improvement. Different from the above works, we attempt to explore generalization ability for summarization systems.    \paragraph{Diagnosing Limitations of Existing Summarization Systems}  Beyond ROUGE, some recent works try to explore the weaknesses of existing systems from divese aspects. \citet{zhang-etal-2018-abstractiveness} tries to figure out to what extent the neural abstractive summarization systems are abstractive and discovers many of abstractive systems tend to perform near-extractive. On the other hand, \citet{cao2018faithful} and \citet{kryscinski2019evaluating} study the factuality problem in modern neural summarization systems. The former puts forward one model that combining source document and preliminary extracted fact description and prove the effectiveness of this model in terms of factuality correctness. While the latter contributes to design a model-based automatic factuality evaluation metric. Abstractiveness and factuality error the above works studied are  orthogonal to this work and can be easily combined with cross-dataset evaluation framework in this paper as Sec. shows.  Moreover, \citet{wang2019exploring,hua2017pilot} attempt to investigate the domain shift problem on text summarization while they focus on a single generation way . We also investigate the generalization of summarizers when transferring to different datasets, but include more datasets and models.        This paper describes SJTU-NICT's submission to the WMT20 news translation task. For three typical scenarios, we adopt different strategies. In this work, we not only study the pre-trained language model to enhance MT, but also consider the impact of document information on translation. We considered both the way of converting document alignment into sentence alignment and the use of BERT's NSP to recover the structure of documents. In addition, transfer learning from supervision is taken into account in unsupervised translation, and various means are used to enhance low-resource translation. Our systems performed strongly among all the constrained submissions: we ranked 1st in PLEN, ENZH, and DEHSB respectively, and stayed Top-3 for the HSBDE.  
"," Our work is connected to the following threads of topics of NLP research.  \paragraph{Cross-Dataset Generalization in NLP} Recently, more researchers shift their focus from individual dataset to cross-dataset evaluation, aiming to get a comprehensive understanding of system's generalization ability.  \citet{fried-etal-2019-cross} explores the generalization ability of different constituency parsers.  \citet{talmor-berant-2019-multiqa}, on the other hand, shows the generalization ability of reading comprehension models can be improved by pre-training on one or two other reading comprehension datasets. \citet{fu2020rethinking} studies the model generalization in the field of NER. They point out the bottleneck of the existing NER systems through in-depth analyses and provide suggestions for further improvement. Different from the above works, we attempt to explore generalization ability for summarization systems.    \paragraph{Diagnosing Limitations of Existing Summarization Systems}  Beyond ROUGE, some recent works try to explore the weaknesses of existing systems from divese aspects. \citet{zhang-etal-2018-abstractiveness} tries to figure out to what extent the neural abstractive summarization systems are abstractive and discovers many of abstractive systems tend to perform near-extractive. On the other hand, \citet{cao2018faithful} and \citet{kryscinski2019evaluating} study the factuality problem in modern neural summarization systems. The former puts forward one model that combining source document and preliminary extracted fact description and prove the effectiveness of this model in terms of factuality correctness. While the latter contributes to design a model-based automatic factuality evaluation metric. Abstractiveness and factuality error the above works studied are  orthogonal to this work and can be easily combined with cross-dataset evaluation framework in this paper as Sec. shows.  Moreover, \citet{wang2019exploring,hua2017pilot} attempt to investigate the domain shift problem on text summarization while they focus on a single generation way . We also investigate the generalization of summarizers when transferring to different datasets, but include more datasets and models.",79
"    As robots are deployed in collaborative applications like healthcare and household assistance , there is a growing need for reliable human-robot communication. One such communication modality that is both user-friendly and versatile is natural language; to this end, we focus on robust natural language interfaces  that can map utterances to executable behavior .  Most existing work on NLIs  falls into a static train-then-deploy paradigm: models are first trained on large datasets of  pairs and then deployed, with the hope they will reliably generalize to new utterances. Yet, what happens when such models make mistakes or are faced with types of utterances unseen at training --- for example, providing a household robot with a novel utterance like ``wash the coffee mug?'' Such static systems will fail with no way to recover, burdening the user to find alternate utterances to accomplish the task . Instead, we argue that NLIs need to be dynamic and adaptive, learning interactively from user feedback to index and perform more complicated behaviors.   In this work, we explore building NLIs for simulated robotics that learn from real humans. Inspired by \citet{wang2017naturalizing}, we leverage the idea of learning from decomposition to learn new abstractions. Just like how a human interactively teaches a new task to a friend by breaking it down, users interactively teach our system by simplifying utterances that the system cannot understand  into lower-level utterances that it can .  To map language to executable behavior, \citet{wang2017naturalizing} and \citet{thomason2019improving} built adaptive NLIs that leverage grammar-based parsers that allow reliable one-shot generalization but lack lexical flexibility. For example, a grammar-based system that understands how to ``wash the coffee mug'' may not generalize to ``clean the mug.'' Meanwhile, recent semantic parsers are based primarily on neural sequence-to-sequence models . While these models excel from a lexical flexibility perspective, they lack the ability to perform reliable one-shot generalization: it is difficult to train them to generalize from individual examples .    In this paper we propose a new interactive NLI that is lexically flexible and can reliably and efficiently perform one-shot generalization. We introduce a novel exemplar-based neural network semantic parser that first abstracts away entities , allowing for generalization to previously taught utterances with novel object combinations. Our parser then retrieves the corresponding ``lifted'' utterance and respective program  from the training examples based on a learned metric , giving us the lexical flexibility of sequence-to-sequence models.  We demonstrate the efficacy of our learning from decomposition framework through a set of human-in-the-loop experiments where crowdworkers use our NLI to solve a suite of simulated robotics tasks in household environments. Crucially, after completing a task, we update the semantic parser so that users can immediately reuse what they taught. We show that over time, users are able to complete complex tasks  more efficiently with our exemplar-based method compared to a neural sequence-to-sequence baseline. However, for more straightforward tasks that can be completed in fewer steps, we see similar performance to the baseline. We end with an error analysis and discussion of user trust and incentives in the context of building interactive semantic parsing systems, paving the way for future work that better realizes the potential of the interactive paradigm.     We build on a long tradition of learning semantic parsers for mapping language to executable programs , with a focus on using context and learning from interaction.  \paragraph{Contextual Semantic Parsing.}  In many settings, successfully parsing an utterance requires reasoning about both linguistic and environment context. \citet{artzi2013weakly} developed a model for parsing instructions in the SAIL Navigation dataset~ that leverages the environment context. Later, \citet{long2016projections} introduced the SCONE Dataset, requiring building models that can reason over both types of context. More recently, \citet{yu2019cosql} introduced the large-scale Conversational Text-to-SQL  dataset that requires jointly reasoning over dialogue history and databases to parse user queries to SQL. We handle both linguistic context and environment context in our work, by decoupling semantic parsing from grounding; our lifted semantic parser handles linguistic context, while our entity resolver and reranker handle environment context.  \paragraph{Learning from Interaction.}  Closest to our work is Voxelurn , and its close predecessor SHRDLURN . Voxelurn defined an open-ended environment where the goal was to build arbitrary voxel structures using language instructions. We take inspiration from its teaching procedure where users decompose high-level utterances into low-level actions in the context of a grammar-based parser. Other work uses alternative modes of interaction to teach new behaviors. \citet{srivastava2017joint} used natural language explanations to teach new concepts. Relatedly, \citet{labutov2018lia} introduced LIA, a programmable personal assistant that learned from user-provided condition-action rules. Furthermore, \citet{weigelt2020programmingfuse} introduce an approach for teaching systems new programmatic functions from language that explicitly reasons about whether utterances contain ``teaching intents,'' a mechanism that is similar to our procedure for returning \notsure{}. Once these ``teaching intents'' have been identified, they are parsed into corresponding code blocks that can then be executed. Other work leverages conversations to learn new concepts, generating queries for users to respond to . Notably, \citet{thomason2019improving} used this conversational structure in a robotics setting similar to ours, but focused on learning new percepts, rather than structural abstractions. \citet{yao2019model} defined a similar conversational system for Text-to-SQL models that decides when intervention is needed, and generates a clarification question accordingly.   \paragraph{General Instruction Following.} Other work looks at instruction following for robotics tasks outside the semantic parsing paradigm, for example by mapping language directly to sequences of actions , mapping language to representations of reward functions , or learning language-conditioned policies via reinforcement learning .    By performing a comprehensive evaluation on eleven summarization systems and five mainstream datasets, we summarize our observations below:  1) Abstractive summarizers are extremely brittle compared with extractive approaches, and the maximum gap between them reaches 37 in terms of the measure stableness  defined in this paper.  2) BART  is superior over other abstractive models and even comparable with extractive models in terms of stiffness . On the other hand, it is robust when transferring between datasets as it possesses high stableness . 3) BERT  performs excellently in terms of stiffness, while still lacks stableness when transferred to \texttt{Bigpatent B} from other datasets.   4) The robustness of models can be improved through either equipped the model with ability to copy span from source document  or make use of well trained sequence to sequence pre-trained model . 5) Simply adding BERT on encoder could improve the stiffness  of model but will cause larger cross-dataset and in-dataset performance gap, a better way should be found to merge BERT into abstractive model, or a better training strategy should be applied to offset the negative influence it brings. 6) Existing factuality checker  is limited in predictive power of positive samples  . 7) Out-of-domain systems can even surpass in-domain systems in terms of factuality.      
","  We build on a long tradition of learning semantic parsers for mapping language to executable programs , with a focus on using context and learning from interaction.  \paragraph{Contextual Semantic Parsing.}  In many settings, successfully parsing an utterance requires reasoning about both linguistic and environment context. \citet{artzi2013weakly} developed a model for parsing instructions in the SAIL Navigation dataset~ that leverages the environment context. Later, \citet{long2016projections} introduced the SCONE Dataset, requiring building models that can reason over both types of context. More recently, \citet{yu2019cosql} introduced the large-scale Conversational Text-to-SQL  dataset that requires jointly reasoning over dialogue history and databases to parse user queries to SQL. We handle both linguistic context and environment context in our work, by decoupling semantic parsing from grounding; our lifted semantic parser handles linguistic context, while our entity resolver and reranker handle environment context.  \paragraph{Learning from Interaction.}  Closest to our work is Voxelurn , and its close predecessor SHRDLURN . Voxelurn defined an open-ended environment where the goal was to build arbitrary voxel structures using language instructions. We take inspiration from its teaching procedure where users decompose high-level utterances into low-level actions in the context of a grammar-based parser. Other work uses alternative modes of interaction to teach new behaviors. \citet{srivastava2017joint} used natural language explanations to teach new concepts. Relatedly, \citet{labutov2018lia} introduced LIA, a programmable personal assistant that learned from user-provided condition-action rules. Furthermore, \citet{weigelt2020programmingfuse} introduce an approach for teaching systems new programmatic functions from language that explicitly reasons about whether utterances contain ``teaching intents,'' a mechanism that is similar to our procedure for returning \notsure{}. Once these ``teaching intents'' have been identified, they are parsed into corresponding code blocks that can then be executed. Other work leverages conversations to learn new concepts, generating queries for users to respond to . Notably, \citet{thomason2019improving} used this conversational structure in a robotics setting similar to ours, but focused on learning new percepts, rather than structural abstractions. \citet{yao2019model} defined a similar conversational system for Text-to-SQL models that decides when intervention is needed, and generates a clarification question accordingly.   \paragraph{General Instruction Following.} Other work looks at instruction following for robotics tasks outside the semantic parsing paradigm, for example by mapping language directly to sequences of actions , mapping language to representations of reward functions , or learning language-conditioned policies via reinforcement learning .",80
"  % ============== version 5.0 ================= Intent detection, a fundamental component of task-oriented dialogue system , is increasingly raising attention as a Multi-Label Classification  problem , since a single utterance often carries multiple user intents .  In real-world scenarios, intent detection often suffers from lack of training data, because dialogue tasks/domains change rapidly and new domains usually contain only a few data examples.  Recent success of Few-Shot Learning  presents a promising solution for such data scarcity challenges.  It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience. % from old domains.   %For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy to convert the multi-class classification into binary-class classifications .  State-of-the-art works for multi-label intent detection focus on threshold-based strategy, where a common practice is estimating label-instance relevance scores and picking the intent labels with score higher than a threshold value .  Usually, the coordination and respective quality of the two modules, i.e. thresholding and relevance scoring, are crucial to the performance of MLC models.  However, in few-shot scenarios, such multi-label setting poses unique challenges for both threshold estimation and label-instance relevance scoring.  For thresholding, previous works explore to tune a fixed threshold  or to learn thresholds from data . But, these thresholds work well only when learning examples are sufficient.  In few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %In few-shot scenarios, it is pretty hard to determine appropriate thresholds  %with only a few examples. %without overfitting to the limited examples. % to the limited examples. %For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. Besides, it is also difficult to directly transfer the pre-learned thresholds due to the domain differences, such as differences in label number per instance, score density and scale.    Estimation of the label-instance relevance scores is also challenging. %It is also challenging to compute the label-instance relevance scores.  Few-shot learning has achieved impressive progress with similarity-based methods  , where the relevance scores can be modeled as label-instance similarities.  And the label representations can be obtained from corresponding support examples.  Unfortunately, despite huge success in previous single-label tasks, these similarity-based methods become impractical for multi-label problems.  When instances have multiple labels, representations of different labels may be obtained from the same support examples and become confused with each other. For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation,  %Such confused label representations  which makes it impossible to predict correct labels with similarity scores.  %In such situations, vanilla similarities will assign query x equal score to query\_time and query\_loc  In this paper, we study the few-shot learning problem of multi-label intent detection and propose a novel framework to tackle the challenges from both thresholding and label-instance relevance scoring.  To solve the thresholding difficulties of prior-knowledge transferring and domain adaption with limited examples, we propose a Meta Calibrated Threshold  mechanism that first learns universal thresholding experience on data-rich domains, then adapts the thresholds to certain few-shot domains with a Kernel Regression based calibration.  Such combination of universal training and domain-specific calibration allows to estimate threshold using both prior domain experience and new domain knowledge.  %Here, as a non-parametric learning method, Kernel Regression allows to alleviate overfitting by calibrating the thresholds without finetuning.  To tackle the challenge of confused label representation in relevance scoring, we propose the Anchored Label Representation  to obtain well-separated label representations. Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represents each label with both support examples and corresponding anchors.  Different from the previous single-label intent detection that uses label embedding as additional features , our label embeddings here have unique effects of separating different labels in metric space.  Finally, to encourage better coordination between thresholding and label-instance relevance scoring, we introduce the Logit-adapting mechanism to MCT that automatically adapts thresholds to different score densities.   Experiments on two datasets show that our methods significantly outperform strong baselines.  Our contributions are summarized as follows:   We explore the few-shot multi-label problem in intent detection of task-oriented dialogue, which is also an early attempt for the few-shot multi-label classification.   We propose a Meta Calibrated Threshold mechanism with Kernel Regression and Logits Adapting that estimates threshold using both prior domain experience and new domain knowledge.  We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance scoring.    %% ============== version 4.0 ================= %Intent detection, a fundamental component of task-oriented dialogue system , is increasingly raising attention as a Multi-Label Classification  problem , since a single utterance often carries multiple user intents .  %In real-world scenarios, intent detection often suffers from lack of training data, because dialogue tasks/domains change rapidly and new domains usually contain only a few data examples.  %Recent success of Few-Shot Learning  presents a promising solution for such data scarcity challenges.  %It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience. %% from old domains.  % %%For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy to convert the multi-class classification into binary-class classifications .  %State-of-the-art works for multi-label intent detection focus on threshold-based strategy, where a common practice is estimating label-instance relevance scores and picking the intent labels with score higher than a threshold value .  %Usually, the coordination and respective quality of the two modules, i.e. thresholding and relevance scoring, are crucial to the performance of MLC models.  %However, in few-shot scenarios, such multi-label setting poses unique challenges for both threshold estimation and label-instance relevance scoring. % %For thresholding, previous works explore to tune a fixed threshold  or to learn thresholds from data . %But, these thresholds work well only when learning examples are sufficient.  %In few-shot scenarios, it is pretty hard to determine appropriate thresholds without overfitting. %% to the limited examples. %%For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %Besides, it is also difficult to directly transfer the pre-learned thresholds due to the domain differences, such as differences in label number per instance, score density and scale. % % % %It is also challenging to compute the label-instance relevance scores.  %Few-shot learning has achieved impressive progress with similarity-based methods  , where the relevance scores can be modeled as label-instance similarities.  %And the label representations can be obtained from corresponding support examples.  %Unfortunately, despite huge success in previous single-label tasks, these similarity-based methods become impractical for multi-label problems.  %When instances have multiple labels, representations of different labels may be obtained from the same support examples and become confused with each other. %For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation,  %%Such confused label representations  %which makes it impossible to predict correct labels with similarity scores.  %%In such situations, vanilla similarities will assign query x equal score to query\_time and query\_loc % %In this paper, we study the few-shot learning problem of multi-label intent detection and propose a novel framework to tackle the challenges from both thresholding and label-instance relevance scoring. % %To solve the thresholding difficulties of prior-knowledge transferring and overfitting, we propose a Meta Calibrated Threshold  mechanism that first learns universal thresholding experience on data-rich domains, then adapts the thresholds to certain few-shot domains with a Kernel Regression based calibration.  %Here, as a non-parametric learning method, Kernel Regression allows to avoid overfitting by calibrating the thresholds without finetuning. % %To tackle the challenge of confused label representation in relevance scoring, we propose the Anchored Label Representation  to obtain well-separated label representations. %Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represents each label with both support examples and corresponding anchors.  %Different from the previous single-label intent detection that uses label embedding as additional features , our label embeddings here have unique effects of separating different labels in metric space. % %Finally, to encourage better coordination between thresholding and label-instance relevance scoring, we introduce the logit-adapting mechanism to MCT that automatically adapts thresholds to different score densities.  % %Experiments on two datasets show that our methods significantly outperform strong baselines.  %Our contributions are summarized as follows:  % We explore the few-shot multi-label problem in intent detection of task-oriented dialogue, which is also an early attempt for the few-shot multi-label classification.  % We propose a Meta Calibrated Threshold mechanism with Kernel Regression and Logits Adapting that estimates threshold using both prior domain experience and new domain knowledge. % We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance scoring.    %% ============== version 3.0 EMNLP version ================= % %Intent detection  is a fundamental component for task-oriented dialogue system . %In real-word scenarios, intent detection often suffers from rapid changing of domains, because the new domains are usually lacking in data and may contain only a few data examples.  %Few-Shot Learning  is a promising solution to this problem.  %It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience from old domains.  % %In addition to data scarcity problem, intent detection also faces the problem of multi-label prediction. %As shown in Fig , a single utterance may carry multiple user intents.  %For this consideration, intent detection needs to be formulated as a Multi-Label Classification  problem , where a common practice is estimating label-instance relevance scores and picking the labels with score higher than a threshold value . % %Usually, the threshold is crucial to the performance of MLC models. %For multi-label intent detection, previous works explore to tune a fixed threshold  or to learn thresholds from data .  %However, these thresholds work well only when learning examples are sufficient.  %For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %Also, it is difficult to directly transfer the threshold learned in data-rich domains due to the domain differences, such as differences in label number per instance, score density and scale. % % % %It is also challenging to compute the label-instance relevance scores for few-shot MLC.  %Previous few-shot research mainly focuses on single label classification and has achieved impressive progress with similarity-based methods  .  %Generally, these methods first obtain per class representations from a few examples , and then classify an  instance according to its similarity with the representation of each class. %However, such similarity scores rely on well-separated class  representations, which poses unique challenges in multi-label settings. %When instances have multiple labels, representations of different labels may be obtained from same support examples and become confused with each other. %For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation.  % %In this paper, we study the few-shot learning problem of multi-label intent detection . %As mentioned above, it is difficult to estimate and transfer thresholds for few-shot MLC. %To solve this, we first learn universal thresholding experience on data-rich domains, and exploit the experience to estimate appropriate thresholds for unseen few-shot domains. %Specifically, we propose Meta Calibrated Threshold , which first learns a domain-general meta threshold, and then learns to calibrate it to fit specific domains with Kernel-Regression.  %To further encourage threshold generalization, we introduce the logit-adapting mechanism that automatically adapts meta thresholds to different score densities.  % %For computing label-instance score of few-shot MLC, we propose the Anchored Label Representation  to obtain well-separated label representations. %Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represent each label with both support examples and corresponding anchors.  % %Experiments on two datasets show that our methods significantly outperform strong baselines.  %Our contributions are summarized as follows:  % We explore the few-shot multi-label problem in intent detection of task-oriented dialogue,  %which is also an early attempt for few-shot multi-label classification.  % We propose a Meta Calibrated Threshold mechanism that estimate threshold using both prior domain experience and new domain knowledge. % We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance score calculation.       Usually, multi-label classification  methods rely on thresholds to predict multiple labels.  For MLC problem in NLP, \citet{wu2019learning} leverage meta learning to estimate thresholds for data-rich setting.  For thresholding of intent detection, \citet{gangadharaiah2019joint} leverage a fixed threshold over intent possibility.  \citet{xu2017convolutional} learn threshold with linear regression.   Without threshold, one solution to MLC is Label Powerset  , which regards combination of multiple labels as a single label. \citet{xu2013exploiting} explore idea of LP in multi-label intent detection. However, LP often suffers from data sparseness from label combination even in data-rich settings. In addition to LP, \citet{kim2017two} propose to learn multi-intent detection from single intent data.  They first detect sub-sentence, and then predict single intents on sub-sentences.  But, their method is limited by the explicit conjunctions in data, and it is hard to learn to detect sub-sentence in few-shot setting.   Few-shot learning in NLP has been widely explored for single-label classification tasks, including text classification , relation classification , sequence labeling . However, few-shot multi-label problem is less investigated.  Previous works focus on computer vision  and signal processing .  For few-shot MLC in NLP,  \citet{rios2018few} investigate few-shot MLC for medical texts.  But, their method requires descriptions and EMR structure of labels, which are often hard to obtain and not available in our task.  For the use of label name semantics, it has been proven to be effective for data scarcity problem of both slot filling  and intent detection .  Our method shares the similar idea but introduces it to tackle the special challenges of multi-label setting.     We employed a copy mechanism to address the lexical cohesion problem in document-level NMT.  Our model computes a copy probability and weights of words to copy referring to preceding source sentences and their translation outputs.  Experiments on Japanese to English translation indicated that our model is effective to improve lexical cohesion, compared to strong context-aware NMT models.  As future work, we intend to evaluate the effectiveness of our model on various language pairs and domains, such as English-French and English-Russian; news and novels.  Also, we will improve the weighting method to copy words to avoid copying inappropriate words.     
","  Usually, multi-label classification  methods rely on thresholds to predict multiple labels.  For MLC problem in NLP, \citet{wu2019learning} leverage meta learning to estimate thresholds for data-rich setting.  For thresholding of intent detection, \citet{gangadharaiah2019joint} leverage a fixed threshold over intent possibility.  \citet{xu2017convolutional} learn threshold with linear regression.   Without threshold, one solution to MLC is Label Powerset  , which regards combination of multiple labels as a single label. \citet{xu2013exploiting} explore idea of LP in multi-label intent detection. However, LP often suffers from data sparseness from label combination even in data-rich settings. In addition to LP, \citet{kim2017two} propose to learn multi-intent detection from single intent data.  They first detect sub-sentence, and then predict single intents on sub-sentences.  But, their method is limited by the explicit conjunctions in data, and it is hard to learn to detect sub-sentence in few-shot setting.   Few-shot learning in NLP has been widely explored for single-label classification tasks, including text classification , relation classification , sequence labeling . However, few-shot multi-label problem is less investigated.  Previous works focus on computer vision  and signal processing .  For few-shot MLC in NLP,  \citet{rios2018few} investigate few-shot MLC for medical texts.  But, their method requires descriptions and EMR structure of labels, which are often hard to obtain and not available in our task.  For the use of label name semantics, it has been proven to be effective for data scarcity problem of both slot filling  and intent detection .  Our method shares the similar idea but introduces it to tackle the special challenges of multi-label setting.",81
"    Self-supervised pretraining through language modeling on massive datasets has revolutionized NLP. One reason this method works is that pretraining shapes a model's hypothesis space, giving it inductive biases that help it learn linguistic tasks . Numerous probing studies have provided support for this idea by showing that language models learn representations that encode linguistic features .   However, feature learning is just the first step to acquiring helpful inductive biases. Models must also be able to learn which features matter. The NLU datasets these models are often fine-tuned on are ambiguous and contain artifacts, and often support multiple possible generalizations. Neural networks are not mind readers: Models that have been shown to represent linguistic features sometimes fail to use them during fine-tuning on NLU tasks, instead adopting shallow surface generalizations . To this end, recent work in probing pretrained models advocates for shifting the focus of study away from whether they represent linguistic features and in favor of whether they learn useful representations of those features .  % }           \end{table*}  We investigate how RoBERTa  acquires language-specific inductive biases during self-supervised pretraining. We track separately how RoBERTa's representation of linguistic features and its preferences for linguistic generalizations over surface generalizations change as the amount of pretraining data increases. We pretrain RoBERTa from scratch on datasets ranging from 1M to 1B words and evaluate these models alongside RoBERTa in a series of experiments to probe the inductive biases of a pretrained model at the time of fine-tuning on a downstream task.   We probe these models in three kinds of experiments: First, we conduct control experiments where we fine-tune models on unambiguous binary classification tasks to test whether they learn to represent simple linguistic and surface features. Second, we conduct ambiguous experiments following the poverty of the stimulus design , as illustrated in Figure . In these experiments, we fine-tune a pretrained model on an ambiguous binary classification task in which the training set is consistent with both a linguistic generalization and a surface one. We then test the classifier on disambiguating data to reveal which generalization the model adopted, and by extension its preference among the two features. Third, we conduct inoculation experiments \citep[following][]{liu2019inoculation} to test how hard it is to sway a model with a surface bias to adopt a linguistic generalization. We do this by introducing small amounts of disambiguating data into an otherwise ambiguous training set. We automatically generate data for all these tasks, and call the resulting dataset \dataset\ , pronounced ``messages''.   The results show that RoBERTa acquires a stronger linguistic bias as pretraining increases. RoBERTa has the strongest linguistic bias, and requires little to no inoculating data to reliably make the linguistic generalization. In general, models with more pretraining data can generally be induced to adopt linguistic generalizations with less inoculating data. We also find a large gap between the amount of pretraining data that RoBERTa needs to learn the linguistic features necessary to generalize out-of-domain and the amount it needs to learns that it should prefer those features when generalizing. The control experiments on unambiguous data reveal that models with little pretraining do actually represent the linguistic features, but nonetheless show a strong surface bias. In other words, the main contribution of pretraining to linguistic bias learning is devoted not to extracting features, but to learning which features matter.   We conclude that helpful inductive biases can be learned through pretraining, but current models require abundant data to do so. The implications of this conclusion point in two directions: First, we can probably continue to pretrain on increasingly massive training sets to improve on the generalization and few-shot learning abilities of models like T5  and GPT-3 . Second, since models learn useful features early, there is hope that future advances could accelerate by reducing the amount of data needed to learn which features matter. To aid in this effort, we release the MSGS dataset, our pretrained RoBERTas, and all our code: \href{https://github.com/nyu-mll/msgs}{\url{https://github.com/nyu-mll/msgs}}.       There is increasing interest in studying the inductive biases of neural networks. Much of this work has grown out of numerous findings that these models often fail to generalize in ways that task designers intend. For example, \citet{jia2017adversarial} and \citet{mccoy2019right} demonstrate that ambiguity in widely used NLU datasets like SQuAD  and MultiNLI  leads models like BERT to adopt some surface generalizations, despite the fact that they represent linguistic features. This continues to be a problem for models like RoBERTa which show an overall linguistic bias in our experiments. However, for tasks like NLI, the underlying linguistic feature depends on a combination of significant syntactic knowledge, semantic knowledge, and world knowledge. It stands to reason that representations and preferences for such high level features require more data to learn than the features we probe.  Other work has used the poverty of stimulus design to study inductive biases associated with particular neural architectures during syntactic generalization. \citet{ravfogel2019studying} train RNNs on a morphological prediction task using artificial languages derived from naturally occurring English text, finding that RNNs show a recency bias in acquiring agreement rules. \citet{mccoy2018revisiting,mccoy2020does} train a seq2seq models on generated data ambiguous between a surface and a structural generalization to learn the subject-auxiliary inversion rule in English question formation. They find that, while tree-structured models show a structural bias, sequence models do not. \citet{warstadt2020can} conduct related experiments on subject-auxiliary inversion and other English structural rules, and find that BERT likely acquires a structural bias from pretraining.   More abstract inductive biases have also been studied. Using zero-shot learning in an artificial language, \citet{lake2018generalization} show that RNNs lack a bias in favor of learning compositional meanings for new symbols. \citet{gandhi2019mutual} and \citet{gulordava2020one} explore conditions under which neural networks exhibit a bias towards learning mutually exclusive meanings for new symbols.  Data augmentation and inoculation have also been explored previously as a way to influence how models generalize. \citet{mccoy2019right} and \citet{min2020syntactic} show that small amounts of inoculating data during training on textual entailment help BERT overlook certain surface generalizations. \citet{jha2020does} study inoculation using a constructed language of numerical sequences. Like us, they generate ambiguous datasets, though they only compare features that resemble our surface features. They find that it is relatively easy to nudge models away from shallow generalizations, but harder to nudge them towards deeper ones.  Finally, several earlier studies explored how increasing training data impacts linguistic knowledge in LMs. Unlike the present study, these studies evaluate LMs using an unsupervised acceptability judgment task on minimal pairs , and do not attempt to separate feature learning from feature preferences. \citet{vanschijndel2019quantity} find the greatest increase in sensitivity to acceptability contrasts occurs between training on 2M and 10M words. \citet{warstadt2020blimp} find that while LMs learn agreement phenomena at a similarly early stage, other phenomena require more data to learn. Finally, \citet{hu2020systematic} find that adopting architectures that build in linguistic bias, such as RNNGs , has a bigger effect on the acceptability task than increasing training data from 1M to 40M words.      Tagging words with target language gender inflection is a powerful way to improve accuracy of translated inflections. This could be applied in cases where the correct grammatical gender to use for a given referent is known, or as monolingual coreference resolution tools improve sufficiently to be used for automatic tagging.  It also has potential application to new inflections defined for gender-neutral language.   However, there is a risk that gender features will be used in an over-general way. Providing a strong gender signal for one entity has the potential to harm users and referents by erasing other entities in the same sentence, unless a model is specifically trained to translate sentences with multiple entities. In particular we find that our V3 system, which is trained on multiple-entity translation examples, allows good performance while minimizing peripheral effects.   We conclude by emphasising that work on gender coreference in translation requires care to ensure that the effects of interventions are as intended, as well as testing scenarios that capture the full complexity of the problem, if the work is to have an impact on gender bias.   
","    There is increasing interest in studying the inductive biases of neural networks. Much of this work has grown out of numerous findings that these models often fail to generalize in ways that task designers intend. For example, \citet{jia2017adversarial} and \citet{mccoy2019right} demonstrate that ambiguity in widely used NLU datasets like SQuAD  and MultiNLI  leads models like BERT to adopt some surface generalizations, despite the fact that they represent linguistic features. This continues to be a problem for models like RoBERTa which show an overall linguistic bias in our experiments. However, for tasks like NLI, the underlying linguistic feature depends on a combination of significant syntactic knowledge, semantic knowledge, and world knowledge. It stands to reason that representations and preferences for such high level features require more data to learn than the features we probe.  Other work has used the poverty of stimulus design to study inductive biases associated with particular neural architectures during syntactic generalization. \citet{ravfogel2019studying} train RNNs on a morphological prediction task using artificial languages derived from naturally occurring English text, finding that RNNs show a recency bias in acquiring agreement rules. \citet{mccoy2018revisiting,mccoy2020does} train a seq2seq models on generated data ambiguous between a surface and a structural generalization to learn the subject-auxiliary inversion rule in English question formation. They find that, while tree-structured models show a structural bias, sequence models do not. \citet{warstadt2020can} conduct related experiments on subject-auxiliary inversion and other English structural rules, and find that BERT likely acquires a structural bias from pretraining.   More abstract inductive biases have also been studied. Using zero-shot learning in an artificial language, \citet{lake2018generalization} show that RNNs lack a bias in favor of learning compositional meanings for new symbols. \citet{gandhi2019mutual} and \citet{gulordava2020one} explore conditions under which neural networks exhibit a bias towards learning mutually exclusive meanings for new symbols.  Data augmentation and inoculation have also been explored previously as a way to influence how models generalize. \citet{mccoy2019right} and \citet{min2020syntactic} show that small amounts of inoculating data during training on textual entailment help BERT overlook certain surface generalizations. \citet{jha2020does} study inoculation using a constructed language of numerical sequences. Like us, they generate ambiguous datasets, though they only compare features that resemble our surface features. They find that it is relatively easy to nudge models away from shallow generalizations, but harder to nudge them towards deeper ones.  Finally, several earlier studies explored how increasing training data impacts linguistic knowledge in LMs. Unlike the present study, these studies evaluate LMs using an unsupervised acceptability judgment task on minimal pairs , and do not attempt to separate feature learning from feature preferences. \citet{vanschijndel2019quantity} find the greatest increase in sensitivity to acceptability contrasts occurs between training on 2M and 10M words. \citet{warstadt2020blimp} find that while LMs learn agreement phenomena at a similarly early stage, other phenomena require more data to learn. Finally, \citet{hu2020systematic} find that adopting architectures that build in linguistic bias, such as RNNGs , has a bigger effect on the acceptability task than increasing training data from 1M to 40M words.",82
" .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  \reza{ Neural models have been revolutionising machine translation , and have achieved state-of-the-art for many high-resource language pairs . However, the scarcity of bilingual parallel corpora is still a major challenge for training high-quality NMT models  % especially for a broad range of languages for which the available translation training resources are too small to be used with existing NMT systems  . % Transfer learning by fine-tuning, from a model trained for a high-resource language-pair, % \wray{Wray: Having trouble with this:  isn't ""high-resource language-pair"" mean both source and target are high resource so how does this relate to what we do?} is a standard approach to tackle the scarcity of the data in the target low-resource language-pair . % However, this is a one-to-one approach, which is not able to exploit models trained for multiple high-resource language-pairs for the target language-pair of interest.  % Furthermore, models transferred from different high-resource language-pairs may have complementary syntactic and/or semantic strengths, hence using a single model may be sub-optimal.  }   %Transfer learning is one of the widely used solutions for addressing the data scarcity problem in low-resource scenarios .  % However, applying the original transfer learning to LR models is neither able to make full use of highly related multiple high-resource languages nor to receive different parameters from all effective high-resource NMT models simultaneously.  %However, transfer learning from high-resource to low-resource NMT models is generally a one-to-many approach which is not able to exploit multiple high-resource languages and high-resource NMT models' parameters simultaneously. Contrariwise,   \reza{ Another appealing approach is multilingual NMT, whereby a single NMT model is trained  by combining data from multiple high-resource and low-resource language-pairs . % %is an appealing approach for low-resource languages by utilizing the training examples of multiple languages .  %In practice, for training a multilingual NMT, a multilingual vocabulary set from all language pairs are used for training a single NMT model among all languages that enable sharing resources between high-resource and low-resource languages. % and improves the regularization of the model by avoiding over-fitting to the limited data of the low-resource languages.  However, the performance of a multilingual NMT model is highly dependent on the types of languages used to train the model.  Indeed, if languages are from very distant language families, they lead to negative transfer, causing low translation quality in the multilingual system compared to the counterparts trained on the individual language-pairs  .  % To address this problem,  has proposed a knowledge distillation approach to effectively train a multilingual model,  % by selectively distilling the knowledge from individual teacher models to the multilingual student model. However, still all the language pairs are trained in a single model with a blind contribution during training. %during the training process when the accuracy of the individual models surpasses the multilingual one.  % by distilling knowledge from individual NMT models. To avoid distilling knowledge from the not effective teachers, they selectively apply distillation during the training process when the accuracy of the individual models surpasses the multilingual one.  }  \reza{ In this paper, we propose a many-to-one transfer learning approach which can effectively transfer models from multiple high-resource language-pairs to a target low-resource language-pair of interest.  % As the fine-tuned models from different high-resource language pairs can have complementary syntactic and/or semantic strengths in the target language-pair, our idea is to distill their knowledge into a single student model to make the best use of these teacher models.  % We further propose an effective adaptive knowledge distillation  approach to dynamically adjust the contribution of the teacher models during the distillation process, enabling making the best use of teachers in the ensemble.  % Each teacher model provides dense supervision to the student via dark knowledge  using a mechanism similar to label smoothing , where the amount of smoothing is regulated by the teacher.  % In our AKD approach, the label smoothing coming from different teachers is  combined and regulated, based on the loss incurred by the teacher models during the distillation process.   % %\wray{Wray:  This next sentence could be deleted if you need space.} %Although we focus on the application of this method for NMT, it can be applied more generally to other NLP tasks suffering from the scarcity of training data, e.g. summarisation {CITE}  and question answering \todo{CITE}. } %Experimental results on various teacher-student language pairs show up to 0.9 BLEU score improvement compare to the strong baselines. Experiments on transferring from a collection of six language pairs from IWSLT to five low-resource language-pairs from TED Talks demonstrate the effectiveness of our approach, achieving up to +0.9 BLEU score improvements compared to strong baselines.  %\todo{talk more about the experiments?}  %In this paper, we introduce a new distil-based approach to make full use of all high-resource languages % and NMT models simultaneously and effectively. To do so, we firstly apply transfer learning from high-resource to low-resource languages to generate strong teachers. Then, we adaptively distil knowledge from multiple teachers based on their effectiveness  %to improve the accuracy of low-resource NMT model.  % What distinguishes our approach from the previous distil-based method  is choosing the best teachers statistically rather than deterministically. Our approach weights teachers based on the context of each mini-batch and the ability of each teacher to improve the prediction of student for that specific mini-batch during training. % Our experiments show that the proposed approach outperforms the vanilla transformer, original transfer learning, multilingual NMT, and selective knowledge distillation for translation of five low-resource languages to English.  %Our main contributions are as follows: %    a) We      % propose a new approach to  %    transfer knowledge from high-resource to low-resource language pairs which only assumes the availability of translation models in high-resource and the bilingual data for low-resource languages which leads to best usage of computational resources via exploiting the computational work already done on the high-resource side.     % , which is particularly interesting when there is limitation in the available computational resources. %    b) We      % propose a new method to %    dynamically distil knowledge from existing teacher models to a student model. What distinguishes our approach from the previous distillation-based methods  is choosing the best teachers statistically based on the data and knowledge gap of the student model, rather than deterministically as done in the previous work . %    c) Experimental results on various teacher-student language pairs show up to 0.9 BLEU score improvement compare to the strong baselines.  %      \paragraph{Low-Resource NMT.}   \todo{cover mltlingual NMT, multitask learning . }    \paragraph{Knowledge Distillation.} \todo{ hinton's paper , label smoothing, the theoretical work on bias-variance, the NIPS paper .  }      \hspace{0.4cm}         In this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output  which chooses a proper video cover and generates an appropriate textual summary for a video-attached article. We propose a model named Dual-Interaction-based Multimodal Summarizer  including a local conditional self-attention mechanism and a global-attention mechanism to jointly model and summarize multimodal input. Our model achieves state-of-the-art results in terms of autometrics and outperforms human evaluations by a large margin. In near future, we aim to incorporate the video script information in the multimodal summarization process.  
","    \paragraph{Low-Resource NMT.}   \todo{cover mltlingual NMT, multitask learning . }    \paragraph{Knowledge Distillation.} \todo{ hinton's paper , label smoothing, the theoretical work on bias-variance, the NIPS paper .  }      \hspace{0.4cm}",83
" %A common situation for language learners is to encounter unrecognized words. %In this case, looking up the dictionary may be the preferred solution for many people. %However, the capacity of dictionaries is limited, and they may not contain new words or new meanings of words. %What's more, not all language pairs have dictionaries, especially those with low resources. %Therefore, it may be a good idea to directly generate definitions for words.  The definition modeling task proposed by \citet{Noraset2017DefinitionML} is to generate a dictionary definition of a specific word. This task can prove useful for language learners, such as provide reading help by giving definitions for words in the text. However, definition modeling can only work for a specific language, which puts high demands on users because it requires them to read definitions written in this language. Besides, many low-resource languages lack large-scale dictionary data, making it difficult to train definition generation models for these languages. %This task can prove useful for language learners, such as provide reading help by giving definitions for words in the text. %However, definition modeling can only work for a specific language, which puts high demands on users because it requires them to read definitions written in this language.  Therefore, we emphasize the necessity of generating definitions cross-lingually, which can generate definitions for various language inputs, as illustrated in figure . Since English is widely used around the world, and English dictionary resources are relatively easy to obtain, we choose to generate definitions in English. In this way, a cross-lingual model trained on English can be directly applied to other languages.  The challenging issue is how to effectively transfer the knowledge of definition generation learned in English to other languages. To solve this problem, we propose to employ cross-lingual pretrained language models  as encoders. These models have shown to be able to encode sequences of various languages, which enables the ability of cross-lingual transfer . %In this work, we emphasize the necessity of generating definitions cross-lingually, which requires the model to generate definitions with just one language for words in various languages as illustrated in figure . %Considering English is widely used around the world, and English dictionary resources are relatively easy to obtain, we choose to use English to generate definitions for other languages in this work.  %Recently, cross-lingual pretrained language models  have shown to be capable of encoding sequences of different languages into the same vector space, which enables the ability of cross-lingual transfer. %Therefore, we propose to employ them as encoders for cross-lingual definition generation. %After training and fine-tuning the model on English dataset, we directly apply the obtained model to generate definitions for other languages.    To verify our proposed method, we build an English dataset for model training and a Chinese dataset for zero-shot cross-lingual evaluation. %We collected English words, example sentences and definitions in the OALD as the English dataset, and collected Chinese words, example sentences and English definitions in the Chinese WordNet   as the Chinese dataset. Experiments and manual analyses on the constructed datasets show that our proposed models have good cross-lingual transfer ability. Compared with the reference definitions in the CWN dataset, although the generated definitions are still insufficient on the accuracy, their fluency is already good enough.  Furthermore, considering the generated definitions are provided for language learners, and many of them are non-English native speakers, we argue that the difficulty of definitions should be under control. We control the lexical complexity of generated definitions by limiting definitions in the training set to the Oxford 3000 vocabulary, which is a list of important and useful words that are carefully selected by language experts and experienced teachers . %These words have been used to write definitions in the Oxford Advanced Learner's Dictionary  , in order to make them easy to understand. %We compute the Type/Token Ratio  as a measure of lexical complexity. %The TTR of generated definitions  is much lower than that of reference definitions , which indicates a lower lexical complexity. We compute four different metrics to measure the lexical complexity. Definitions generated by our models outperform the reference definitions on all four metrics by a large margin. The result shows that our method can generate simpler definitions, which is suitable for language learners.    This paper mainly related to two aspects of work, namely definition modeling and cross-lingual pretrained language models.  \citet{Noraset2017DefinitionML} first proposed the use of language models to generate definitions for given words. Since their work can only generate one definition for one word, it cannot serve polysemies well. \citet{Gadetsky2018ConditionalGO} introduced the context of words as input and computed the AdaGram vector  for the given words to distinguish different meanings of it. To make the model more interpretable, \citet{Chang2018xSenseLS} proposed to project the given words to high-dimensional sparse vectors, and picked different dimensions for different meanings. While all previous work studied English, \citet{Yang2019IncorporatingSI} specifically explored definition modeling for Chinese words. They incorporated sememes, minimal semantic units, as part of the representation of given words to generate definitions. \citet{Mickus2019MarkMW} implemented a transformer-based sequence-to-sequence model to train the definition generation model in an end-to-end fashion. \citet{Ishiwatari2019LearningTD} extended this task to describe unknown phrases by using both local and global contexts.  \citet{Devlin2019BERTPO} released the multilingual BERT pretrained on corpora of 104 languages, which is capable of generating high quality cross-lingual representations. \citet{Lample2019CrosslingualLM} then introduced the TLM task into cross-lingual language model pretraining, and received SOTA results on cross-lingual classification and machine translation tasks.  \citet{Wang2019CrossLingualBT} employed a linear transformation mechanism to generate cross-lingual contextualized word embeddings based on BERT models, and then used these embeddings for zero-shot dependency parsing. \citet{Chi2019CrossLingualNL} proposed a novel pretrained model named XNLG for many-to-many corss-lingual NLG tasks.     We investigated text matching, a core task in information retrieval and semantic analysis. We introduced the notation and definition of metric learning, and how it can be applied to text matching. Then, we explored \textsf{FILM} , which aim to reduces the time cost and memory usage, also save energy consumption. In order to solve this task efficiently, \textsf{FILM} combined with a fast approximate k nearest neighbour search index. Compare to neural models, our method also has advantage in time and memory usage on large-scale and high-dimensional datasets.   
"," This paper mainly related to two aspects of work, namely definition modeling and cross-lingual pretrained language models.  \citet{Noraset2017DefinitionML} first proposed the use of language models to generate definitions for given words. Since their work can only generate one definition for one word, it cannot serve polysemies well. \citet{Gadetsky2018ConditionalGO} introduced the context of words as input and computed the AdaGram vector  for the given words to distinguish different meanings of it. To make the model more interpretable, \citet{Chang2018xSenseLS} proposed to project the given words to high-dimensional sparse vectors, and picked different dimensions for different meanings. While all previous work studied English, \citet{Yang2019IncorporatingSI} specifically explored definition modeling for Chinese words. They incorporated sememes, minimal semantic units, as part of the representation of given words to generate definitions. \citet{Mickus2019MarkMW} implemented a transformer-based sequence-to-sequence model to train the definition generation model in an end-to-end fashion. \citet{Ishiwatari2019LearningTD} extended this task to describe unknown phrases by using both local and global contexts.  \citet{Devlin2019BERTPO} released the multilingual BERT pretrained on corpora of 104 languages, which is capable of generating high quality cross-lingual representations. \citet{Lample2019CrosslingualLM} then introduced the TLM task into cross-lingual language model pretraining, and received SOTA results on cross-lingual classification and machine translation tasks.  \citet{Wang2019CrossLingualBT} employed a linear transformation mechanism to generate cross-lingual contextualized word embeddings based on BERT models, and then used these embeddings for zero-shot dependency parsing. \citet{Chi2019CrossLingualNL} proposed a novel pretrained model named XNLG for many-to-many corss-lingual NLG tasks.",84
"  Despite \bert{'s}  popularity and  effectiveness, little is known about its inner workings. Several attempts have been made to demystify certain aspects of \bert , often leading to contradicting conclusions. For instance, \citet{clark-etal-2019-bert} argue that attention measures the importance of a particular word when computing the next level representation for this word. However, \citet{kovaleva-etal-2019-revealing} showed that most attention heads contain trivial linguistic information and follow a vertical pattern , which could be related to under-utilization or over-parameterization issues. Other studies attempted to link specific \bert heads with linguistically interpretable functions ,  agreeing that no single head densely encodes enough relevant information but instead different linguistic features are learnt by different attention heads. We hypothesize that the aforementioned largely contributes to the lack of attention-based explainability of \bert. Another open topic is how the knowledge is distributed across \bert layers. Most studies agree that syntactic knowledge is gathered in the middle layers , while the final layers are more task-specific. Most importantly, it seems that any semantic knowledge is spread across the model, explaining why non-trivial tasks are better solved at the higher layers .  Driven by the above discussion, we propose a novel fine-tuning approach where different parts of \bert are guided to directly solve increasingly challenging classification tasks following an underlying label hierarchy. Specifically, we focus on Large Scale Multilabel Text Classification  where documents are assigned with one or more labels from a large predefined set. The labels are organized in a hierarchy from general to specific concepts. Our approach attempts to tie specific \bert layers with specific hierarchy levels. In effect, each of these layers is responsible for predicting the labels of the corresponding level. We experiment with two \lmtc datasets  and several variations of structured \bert training. Our contributions are:  We propose a novel structured approach to fine-tune \bert where specific layers are tied to specific hierarchy levels;  We show that structured training yields better results than the baseline across all levels of the hierarchy, while also leading to better parameter utilization.       Our approach is similar to \citet{wehrmann18a} but they experiment with fully connected networks, which are not well suited for text classification, contrary to stacked transformers . Similarly, \citet{Yan2015} used Convolutional Neural Networks, albeit with shallow hierarchies . Although our approach leverages the label hierarchy it should not be confused with hierarchical classification methods , which typically employ one classifier per node and cannot scale-up to large hierarchies when considering neural classifiers. A notable exception is the work of \citet{You2019} who employed one bidirectional \lstm with label-wise attention  per hierarchy node. However, for their method to scale-up, they use probabilistic label trees  to organize the labels in their own shallow hierarchy which does not follow the abstraction level of the original hierarchy. To the best of our knowledge we are the first to apply this approach to pre-trained language models.      In this paper, we have tested the few-shot learning capabilities of neural language models, as well as whether these models can learn grammatical representations that are invariant to syntactic transformation. First, we addressed neural models' ability to learn nominal number, introducing a novel testing paradigm that leveraged polar questions to assess subject/verb number agreement learning in syntactically transformed settings. Second, we turned to neural models' ability to represent verbal argument structure, developing two novel suites of tests that assessed preference for themes---either realized as direct objects or passive subjects---in both active contexts and passive contexts. In each experiment we assessed the effect of syntactic supervision on learning outcomes by comparing two supervised models to one purely sequence model.  A summary of our results can be seen in Table, with few-shot learning outcomes in colored cells on the left, and the effect of structural supervision on the right. The results from experiments that assess syntactic invariance are on the bottom, below the line break. This table makes it clear that all neural models are capable of making syntactic generalizations about a token from minimal exposure during training. Although model accuracy is reduced for tests that assess syntactic invariance, all neural models show at least a moderate ability to generalize across syntactic transformations. Furthermore, Table shows that syntactic invariance is enhanced in structurally supervised models. Interestingly, both ActionLSTM and RNNG have access to syntactic information, but the comparison in Table indicates that RNNG can leverage that information more effectively to produce syntactic invariance. Therefore we suggest that RNNG's improved performance does not come from the mere presence of syntactic information in the training and test data, but rather from the fact that it uses syntactic information to structure its computation in a non-sequential way.   Models performed better on singular nouns and transitive verbs, especially when the token occurred minimally during training. This behavioral pattern is consistent with the hypothesis outlined by \citet{jumelet2019analysing}, who suggest that models acquire default syntactic categories, and require supporting evidence before they make non-default predictions.  Because these experiments require careful and robust syntactic analysis of the training data, we evaluated models trained on a relatively small, human-annotated corpus. While the small training data poses some limitations when interpreting the results, it makes them more relevant to low-resource NLP applications and suggests that using structurally supervised models can lead to better generalization in a sparse data environment. While sub-word tokenization schemes such as Byte-Pair Encoding  have helped reduce the number of individual lexical items that need to learned, they do not completely eliminate the long tail of sub-word units. Thus, robust few-shot generalization is still an important problem in these environments. It may be that larger amounts of training data support even better few-shot learning and syntactic invariance outcomes. Scaling these carefully-controlled methods to the larger data setting will be an important next step. However, even with the relatively small models tested here, the results support a growing body of evidence that incremental statistical models of language are able to induce many key features of human linguistic competence.   \section*{Acknowledgements}  The authors thank the anonymous reviewers for their feedback. This work was supported by the MIT-IBM Watson AI Lab.       \section{Effect of Exposure on Model Accuracy}  In this section we report the result for statistical tests assessing the effect of a token's frequency in training on model accuracy for that token. We derive significance from a general linear model with \# of exposures as a sole predictor, with random by-item intercepts ))})  \paragraph{Nominal Number} For the base context, in the no modifier condition we find a positive effect of increased exposure for all models . For the PP modifier test we find an effect of exposure for the ActionLSTM and the RNNG , and a negative, but insignificant effect for the -gram and the LSTM. For the RC Modifier experiment we find an effect of increased exposure for all three neural models , but no effect for the -gram. For the inverted contexts: in the no modifier tests we find no effect of increased exposure, except for the LSTM, where the effect is negative . For the modifier tests, we find a significant effect for the ActionLSTM and the RNNG .  \paragraph{Argument Structure} For the base context : In the infinitival tests, we find a significant effect of exposure on accuracy for the ActionLSTM and the RNNG  and a negative effect for the -gram model . In the past-tense, we find no significant effect for the RNNG or the ActionLSTM, and a negative effect for the -gram and LSTM models . In the transformed contexts , for the no-modifier tests we find a significant effect of exposure for all models . For the short-modifier tests we find an effect for the ActionLSTM  and the RNNG . And in the long-modifier test we find a marginally significant effect for the three neural models .  \section{Learning Outcomes by Grammatical Condition}       In this section, for each test reported in the paper, we break down model performance by grammatical category, either singular vs plural nouns  or transitive vs. intransitive verbs . Charts follow the same presentational paradigm: -axis shows accuracy and -axis the number of times each word appears during training, on a log-10 scale. Smooth lines are results of logistic regression model fits on the raw data, with shaded regions indicating standard error. Dark blue lines show model performance averaged between the two conditions .   The data presented here are consistent with the hypothesis from . When models receive scant evidence of a token's syntactic properties in training, they assume that it belongs to a ``base"" category, which is singular for nouns and transitive for verbs. Thus, models are more accurate for singular nouns and transitive verbs seen rarely in training. As the model receives more evidence that a token is not in the base category, its predictions flip. Hence, gains in overall-accuracy tend to come from models learning the proper agreement for non-base tokens . Generally, these effects are stronger for nominal number learning, and stronger for structurally supervised models than for the LSTM, which is consistent with the findings presented in the main body of the text.     The nominal number breakdown for base contexts can be seen in Figure , with accuracy scores for singular nouns  in red and plural nouns \texttt{NNS}) in teal. Over all, models tended to show higher accuracy scores for singular nouns, which indicates the presence of a singular bias. Interestingly, the ActionLSTM and the RNNG are capable of overcoming the singular bias when presented with sufficient data, however the LSTM remains equally biased for tokens seen 2 and 100 times in training.        The nominal number breakdown for transformed can be seen in Figure . The empirical picture is more complicated here, however if anything models show higher performance for plural nouns. This behavior suggests that is sets up weaker expectations for singular nouns than are does for plural nouns. Such a pattern is consistent with the hypothesis that models learn the singular as a base form, in which case it would set up weaker expectations for singular nouns. These results compliment those from \citet{an2019representation} , who also test in inverted settings and find that models tend not to be surprised at coordinated NPs following a singular verb, as in the ungrammatical sentence *What is the pig and the cat eating?     The breakdown for argument structure learning base contexts can be seen in Figure , with accuracy scores for intransitive verbs in red and transitive verbs in teal. Here, we see a strong transitive bias for the two structurally supervised models, with no obvious bias for the LSTM and an intransitive bias for the -gram.      The breakdown for argument structure learning in the transformed contexts can be seen in Figure  with transformation tests on the top and invariance tests on the bottom. In this case, where performance is different between the two conditions models display higher accuracy scores for transitive verbs.  
"," Our approach is similar to \citet{wehrmann18a} but they experiment with fully connected networks, which are not well suited for text classification, contrary to stacked transformers . Similarly, \citet{Yan2015} used Convolutional Neural Networks, albeit with shallow hierarchies . Although our approach leverages the label hierarchy it should not be confused with hierarchical classification methods , which typically employ one classifier per node and cannot scale-up to large hierarchies when considering neural classifiers. A notable exception is the work of \citet{You2019} who employed one bidirectional \lstm with label-wise attention  per hierarchy node. However, for their method to scale-up, they use probabilistic label trees  to organize the labels in their own shallow hierarchy which does not follow the abstraction level of the original hierarchy. To the best of our knowledge we are the first to apply this approach to pre-trained language models.",85
"   %   %Added value of \atomicTT{}: 1) diversity in terms of vocab, style, concepts, 2) higher quality   %\ronan{Cite publications that used ATOMIC in a downstream application}  Commonsense understanding % knowledge modeling and reasoning remain long-standing challenges in general artificial intelligence.  % However, in the subfield of natural language processing, the last few years have brought tremendous progress in AI applications.  However, large-scale language models have brought tremendous progress in the sub-field of natural language processing.  Such large-scale language models   trained on extreme-scale data have been shown to effectively adapt to diverse downstream tasks, achieving significant performance gains across natural language benchmarks .  %%%%%%%OLD %%%%%% Despite these successes, these models have been shown to learn brittle representations, often from only simple surface word associations , which routinely lead them to make nonsensical predictions detached from common sense . Interestingly, as these models have grown larger , their benchmark performance has continued to improve  despite limited conceptual improvements,  %leading many researchers to conjecture as to  leaving open questions regarding  the source of these remarkable generalization properties.   Recent work has hypothesized that many of these performance gains could be a result of language models being able to memorize facts in their parameters during training  that can be leveraged at evaluation time. As a result, a new paradigm of language models as knowledge bases has emerged . In this setting, language models are prompted with natural language prefixes or questions, and they express knowledge through language generation. The initial success of this paradigm for representing commonsense knowledge  %, combined with limited examples of LMs being successfully integrated with structured commonsense knowledge resources for downstream application,  has led to the optimistic claim that language models comprehensively encode commonsense knowledge, and remove the need for structured knowledge resources. %\antoine{run-on sentence, need to shorten}  We take a more skeptical view of this capacity of language models -- Does scaling up language models actually endow them with commonsense knowledge? While language models can successfully express certain types of knowledge, their best results are observed in narrowly specific conditions -- we show  that they perform better when evaluated on knowledge bases that prioritize ontological relations and whose examples resemble language-like assertions .\footnote{An observation supported by \citet{brown2020language}'s \gpttt{} model, whose best few-shot performance on commonsense knowledge benchmarks comes on the PhysicalIQA  and HellaSwag  datasets.} Consequently, the types of knowledge that can be directly accessed through the language model's interface remains limited.  %Consequently, while these methods are encouraging, they also demonstrate that the limited interface of language models precludes them from expressing the diversity of commonsense knowledge that must be accessible for robust commonsense reasoning.  %\chandra{i am not sure the last line in the paragraph flows logically from the rest of the paragraph. maybe i am missing something?}  However, prior work has also shown that training language models on knowledge graph tuples leads them to learn to express their implicit knowledge directly , allowing them to provide commonsense knowledge on-demand. These adapted knowledge models have exhibited promising results on commonsense benchmarks compared with methods that require linking entities to knowledge graphs . Inspired by these successes, we propose a dual use for commonsense knowledge bases going forward: as static graphs that can be linked to for discrete knowledge access, and as resources for adapting language models to hypothesize commonsense knowledge about un-annotated entities and events.   %%%%%%% OLD %%%%%%%% As a result, recent work has investigated augmenting language models with retrieval mechanisms that query commonsense knowledge graphs  for related facts to the entities mentioned in text. The idea behind these approaches is that access to these facts and the potential to compose them with learned reasoning functions would allow models to more robustly leverage commonsense knowledge to make predictions. Despite the premise of these approaches, they are unfortunately limited by the coverage of the resources used to provide commonsense knowledge facts , motivating the need for new, high coverage resources in the short-term.   % Option 1 % With this second purpose in mind, we shift the design goals of commonsense knowledge resources toward prioritizing pieces of knowledge that are not readily accessible in pretrained language models.  % Option 2 With this second purpose in mind, we propose evaluating commonsense knowledge resources based on the complementary information they can bring to pretrained language models. We construct \atomicTT{}, a new, high-quality knowledge graph with M commonsense knowledge tuples across  commonsense relations. We compare \atomicTT{} with respect to its coverage and accuracy in competition with other highly used CSKGs, such as \conceptnet~. Our results show that \atomicTT{} is able to cover more correct facts about more diverse types of commonsense knowledge than any existing, publicly-available commonsense knowledge resource. However, our results also indicate that there remains a large amount of exclusivity between these KGs, highlighting the challenge of creating resources that cover the scale and diversity of general commonsense knowledge.   %%%%%%% OLD %%%%%%Meanwhile, a new paradigm has emerged that proposes that large-scale language models implicitly learn to represent large amounts of factual and commonsense knowledge . While these methods are promising, they also show that the limited interface of language models precludes them from producing commonsense knowledge robustly. However, using knowledge graph tuples as additional training signal allows these model to be better adapted to representing knowledge . Furthermore, the use of these knowledge models to provide commonsense knowledge on-demand has shown promising results over static knowledge graphs . Consequently, in this work, we propose evaluating commonsense knowledge resources on a new, second purpose: whether they can be used to repurpose language models for commonsense modeling.   Furthermore, we formalize the \comet framework of \citet{Bosselut2019COMETCT} across different seed language models and training knowledge graphs, and evaluate the commonsense knowledge hypothesized by these adapted knowledge models. %Our results indicate that this purpose is a promising evaluation for commonsense resources, as \comet models can successfully hypothesize plausible knowledge for new, unseen entities.  Our empirical study yields two promising conclusions. First, it confirms that KG-adapted language models learn to express knowledge more precisely than naive language models trained only on language. And second, we show that \atomicTT{} as a transfer resource leads to \comet models that achieve the largest increase over their seed language model  for the commonsense knowledge types it covers, validating the importance of constructing knowledge resources with examples of knowledge not readily found in language models. %allows language models to learn representations of commonsense knowledge types that are less covered in naive language models. % Furthermore, a comparison of these \comet models across different commonsense knowledge graphs shows that \atomicTT{} as a transfer resource allows language models to learn richer commonsense knowledge representation than training with other resources.   %   Key Contributions:  In summary, we make three key contributions in this paper. We present \atomicTT{}---a new commonsense knowledge graph covering social, physical, and eventive aspects of everyday inferential knowledge . Next, we compare \atomicTT{} with other prominent CSKBs head-to-head and show that our new symbolic knowledge graph is more accurate than any current CSKB  . Finally, we show that our new neural knowledge model \comet{}-\atomicTT{} successfully transfers \atomicTT{}'s declarative knowledge to beat \gpttt{}, the largest pre-trained language model, in spite of using ~400x fewer parameters  . This demonstrates the utility and importance of high-quality symbolic knowledge provided by \atomicTT{} to generalize on commonsense information that LMs cannot expressively capture on their own .  % * Our new symbolic knowledge graph ATOMICTT is superior in accuracy and coverage to the currently existing large-scale knowledge graphs .  % * our neural knowledge model COMET-ATOMICTT successfully transfers the ATOMICTT's declarative knowledge to beat even the most impressively large pretrained model, GPT-3 . This demonstrates LMs, no matter its size, can benefit from the symbolic knowledge provided by high quality KB like ATOMICTT.        \antoine{Initial thoughts from related work... needs to be rewritten  and may be too many citations to start with}    Previous work has looked at constructing knowledge bases as relational schemas using expert knowledge ,   semi-structured text extraction  and unstructured text extraction . In our work, we focus on construction of commonsense knowledge bases which require the use of open-text events rather than a well-defined relational schema structure.  Other work in information extraction can also be applied to knowledge base construction with open-text entities , but these methods typically extract explicitly stated text relations.          Remove comments to put back fig in main paper:         Comparing the two methods of estimating the amount of hallucinations in a target, for applications where the input and the output use the same vocabulary with a comparable term distribution the overlap method may be better as it has a clear foundation.    The LM-based method that we proposed has an important advantage that it makes no assumptions about the data. In our WikiBio experiment it also produced better results in the human evaluation, presumably because it allowed for paraphrasing and straightforward inferences. For example, the target ozren nedoklan was a yugoslav footballer and manager. has a high  score because the source table has no occupation field and does not mention yugoslav. The  score of that example is zero because footballer and manager can be inferred from the names of the clubs and the manageryears fields in the source.  \paragraph{Possible extensions} It should be emphasized that alternative methods of detecting noise can be explored and may perform better in the controlled-hallucination framework. For example, it is possible to measuring target-source similarity in an embedded space or use word alignment tools to find unsupported information.   While here we have focused on eliminating hallucinations, one can think of applications where one is interested in generating adversarial sentences which sound fluent but are guaranteed to include unsupported information. Figure  shows how the amount of hallucinations in the output increases following the value of the hallucination knob.  \paragraph{Why is BLUE so different?} It is striking that while all the models tested outperform \citet{liu-2018-structure-aware} in terms of PARENT and human evaluation scores, none could approach its BLEU performance. We do not have an explanation of why this is so but note that our results are in line with the review by \citet{reiter-2018-structured} who concludes that BLEU is an inappropriate metric for generation tasks other than MT.   \paragraph{Can we measure length instead of noise?} One may wonder whether an even simpler approach of controlling for length would deliver a similar reduction in hallucinations. Indeed, hallucinations and length are expected to correlate, and shorter length should result in fewer hallucinations. However, as pointed out in Sec.\ , drastically reducing hallucinations may be possible without any control mechanism and can be achieved, at least on WikiBio, with templates. The main challenge lies in doing so without a big drop in informativeness, that is, in coverage of input fields. Comparing the outputs of  with those of , and both with those of \citet{tian-sticking}, we note that the ranking in terms of average sentence length  coincides with the ranking in terms of coverage : 17.2, 17.8, 18.7. While  may associate the special hal\_0 token with the shortest 20\  of the training data, for  this token is apparently associated with a different selection of 20\  of the data points.   \section{Conclusions}  We presented a simple but powerful idea of controlling hallucinations which are caused by the noise in the training data and proposed two ways of detecting such noise.    We demonstrated that it is possible to reduce the amount of hallucinations at no coverage cost by informing the model about how noisy every source-target example is and without changing the model architecture. Importantly, this was done without making any assumptions about the data.   In an evaluation with humans we showed that the faithfulness of generated sentences can be significantly improved at no loss in fluency or coverage. The results we reported on the noisy WikiBio dataset improve upon the prior work.     
","    \antoine{Initial thoughts from related work... needs to be rewritten  and may be too many citations to start with}    Previous work has looked at constructing knowledge bases as relational schemas using expert knowledge ,   semi-structured text extraction  and unstructured text extraction . In our work, we focus on construction of commonsense knowledge bases which require the use of open-text events rather than a well-defined relational schema structure.  Other work in information extraction can also be applied to knowledge base construction with open-text entities , but these methods typically extract explicitly stated text relations.          Remove comments to put back fig in main paper:",86
" Pre-trained Transformers  have lead to state-of-the-art results on a wide range of NLP tasks, for example, named entity recognition, relation extraction and question answering, often approaching human inter-rater agreement .  These models have also been demonstrated to learn effective cross-lingual representations, even without access to parallel text or bilingual lexicons .  Multilingual pre-trained Transformers, such as mBERT and XLM-RoBERTa , support surprisingly effective zero-shot cross-lingual transfer, where training and development data are only assumed in a high resource source language , and performance is evaluated on another target language. 	 Because no target language annotations are assumed in this setting, source language data is typically used to select among models that are fine-tuned with different hyperparameters and random seeds.  However, recent work has shown that English dev accuracy does not always correlate well with target language performance .  In this paper, we propose an alternative strategy for model selection in a zero-shot setting.  Our approach, dubbed Learned Model Selection , learns a function that scores the compatibility between a fine-tuned multilingual transformer, and a target language. The compatibility score is calculated based on features of the multilingual model's learned representations and the target language.  A model's features are based on its own internal representations; this is done by aggregating representations over an unlabeled target language text corpus.  These model-specific features capture information about how the cross-lingual representations transfer to the target language after fine-tuning on source language data.  In addition to model-specific representations, we also make use of learned language embeddings from the lang2vec package , which have been shown to encode typological information, for example, whether a language has prepositions or postpositions.  To measure compatibility between a multilingual model's fine-tuned representations and a target language, the model- and language- specific representations are combined in a bilinear layer.  Parameters of the scoring function are optimized to minimize a pairwise ranking loss on a set of held-out models, where the gold ranking is calculated using standard performance metrics, such as accuracy or F, on a set of pivot languages .  LMS does not rely on any annotated data in the target language for meta-learning or hyperparameter tuning, yet it is effective in learning to predict whether a multilingual model's representations are a good match for a specific target language.    In experiments on five well-studied NLP tasks , we find LMS consistently selects models with better target-language performance than those chosen using English dev data.  Appendix  demonstrates that our framework supports multi-task learning, which can be helpful in settings where some target-language annotations are available, but not for the desired task.  Finally, we show that LMS generalizes to both mBERT and XLM-RoBERTa in Appendix .     Zero-shot cross-lingual transfer using       {\bf Meta-learning and Model Selection} Recent work has explored hyper-parameter optimization , and model selection for a new task.  task2vec~ presents a meta-learning approach to selecting a pre-trained feature extractor from a library for a new visual task. More concretely, task2vec represents tasks in a vector space and is capable of predicting task similarities and taxonomic relations. It encodes a new task and selects the best feature extractor trained on the most similar task. Unlike task2vec, we select a trained model for a specific task, and we represent a trained model with model-specific features on a target language.   MAML  is another approach to meta-learning, pre-training a single model with a meta-loss to initialize a set of parameters that can be quickly fine-tuned for related tasks.  \citet{nooralahzadeh2020zero} explore the use of MAML in the cross-lingual transfer setting. MAML is designed to support few-shot learning through better initialization of model parameters and does not address the problem of model selection.  In contrast, our approach improves model selection in the zero-shot cross-lingual transfer setting.  Aside from meta-learning, and use collaborate filtering techniques to select a model for a target visual task from a pool of models trained on other visual tasks.  Most relevant to our work, use regression methods to predict a model's performance on an NLP task.  They formulate this as a regression problem based on features of the task , incorporating a discrete feature to represent the choice of model. In contrast, LMS inspects a model's internal representations, thus it is suitable for predicting which out of a set of fine-tuned models will best transfer to a target language.             In this paper, we propose A-GCN for CCG supertagging, with its graph built from chunks extracted from a lexicon.    We use two types of edges for the graph, namely, in-chunk and cross-chunk edges for word pairs within and across chunks, respectively, and propose an attention mechanism   where an attention mechanism is used to enhance the model.     Specifically,  we construct the graph based on word groups suggested by high confident n-grams where in-group and cross-group edges are used and A-GCN is able to learn from the word groups through those edges.   an attention mechanism is proposed  to distinguish the important word pairs according to their contribution to CCG supertagging.   婵傝棄顦╅敍宀顑囨稉閺勵垰灏崚顐＄啊闁插秷顩﹂惃鍕嫲娑撳秹鍣哥憰浣烘畱 n-gram閿涙稓顑囨禍灞炬Ц閼宠棄顧勬禒搴ㄥ亝娴滄盯鍣哥憰浣烘畱闂 n-gram 娑擃厼顒熼崚鐗堟纯鏉╂粏绐涚粋鑽ゆ畱 context information   Therefore, not only the important n-grams are distinguished, but also can our approach discriminatively learn from n-grams in different length, especially the long and infrequent ones that carry important long distance contextual information and could be influenced by the majority voting effect.   Therefore, context features are appropriately modeled and the GCN can discriminatively learn from them.     The effectiveness of our approach to CCG supertagging as well as to parsing is demonstrated by the experimental results and the ablation study on the English CCGbank, where state-of-the-art performance is obtained. Experimental results and the ablation study on the English CCGbank demonstrate the effectiveness of our approach to CCG supertagging, where state-of-the-art performance is obtained on both CCG supertagging and parsing.   Further analysis is performed to investigate using different types of edges, which reveals their quality and confirms the necessity of introducing attention to GCN for CCG supertagging.  For future studies,  we plan to explore other approaches to building the graph as well as performing end-to-end   analyze the effect of them on  CCG supertagging and parsing.  
","  Zero-shot cross-lingual transfer using       {\bf Meta-learning and Model Selection} Recent work has explored hyper-parameter optimization , and model selection for a new task.  task2vec~ presents a meta-learning approach to selecting a pre-trained feature extractor from a library for a new visual task. More concretely, task2vec represents tasks in a vector space and is capable of predicting task similarities and taxonomic relations. It encodes a new task and selects the best feature extractor trained on the most similar task. Unlike task2vec, we select a trained model for a specific task, and we represent a trained model with model-specific features on a target language.   MAML  is another approach to meta-learning, pre-training a single model with a meta-loss to initialize a set of parameters that can be quickly fine-tuned for related tasks.  \citet{nooralahzadeh2020zero} explore the use of MAML in the cross-lingual transfer setting. MAML is designed to support few-shot learning through better initialization of model parameters and does not address the problem of model selection.  In contrast, our approach improves model selection in the zero-shot cross-lingual transfer setting.  Aside from meta-learning, and use collaborate filtering techniques to select a model for a target visual task from a pool of models trained on other visual tasks.  Most relevant to our work, use regression methods to predict a model's performance on an NLP task.  They formulate this as a regression problem based on features of the task , incorporating a discrete feature to represent the choice of model. In contrast, LMS inspects a model's internal representations, thus it is suitable for predicting which out of a set of fine-tuned models will best transfer to a target language.",87
" %   Summarization is the process of identifying the most important information pieces in a document. For humans, this process is heavily guided by background knowledge, which encompasses preconceptions about the task and priors about what kind of information is important .    %  %   % Understanding background knowledge would yield insights about what, on average, humans consider as known, interesting and important.  % Furthermore, accurate models of human background knowledge would be greatly valuable to improve the selection methods of information selection systems.  %  Despite its fundamental role, background knowledge has received little attention from the summarization community. Existing approaches largely focus on the relevance aspect, which enforces similarity between the generated summaries and the source documents . % , without consideration for background knowledge.   In previous work, background knowledge has usually been modeled by simple aggregation of large background corpora. % A prominent example is \cpt{TFIDF} , a practical solution to the problem of identifying content words based on document frequencies within background corpora. For instance, using \cpt{TFIDF} , one may operationalize background knowledge as the set of words with a large document frequency in background corpora.  %While this approach was useful for the stopword problem significant to the development of summarization systems, it is cannot easily be extended to model background knowledge.  However, the assumption that frequently discussed topics reflect what is, on average, known does not necessarily hold. For example, common-sense information is often not even discussed . Also, information present in background texts has already gone through the importance filter of humans, e.g., writers and publishers. In general, a particular difficulty preventing the development of proper background knowledge models is its latent nature. We can only hope to infer it from proxy signals. Besides, there is, at present, no principled way to compare and evaluate background knowledge models.   %  In this work, we put the background knowledge in the foreground and propose to infer it from summarization data. Indeed, choices made by human summarizers and human annotators provide implicit information about their background knowledge. We build upon a recent theoretical model of information selection , which postulates that information selected in the summary results from 3 desiderata: low redundancy , high relevance , and high informativeness . The tension between these 3 elements is encoded in a summary scoring function  that explicitly depends on the background knowledge . % that explicitly depends on the background knowledge .  As illustrated by \Figref{fig:overall}, the latent  can then be inferred from the residual differences in information selection that are not explained by relevance and redundancy. For example, the black information unit in \Figref{fig:overall} is not selected in the summary despite being very prominent in the source document. Intuitively, this is explained if this unit is already known by the receiver.  % and the human summarizer regarded it as not important. To leverage this implicit signal, we view  as a latent parameter learned to best fit the observed summarization data.  %  \xhdr{Contributions} We develop algorithms for inferring  in two settings:  when only pairs of documents and reference summaries pairs are observed  and  when pairs of document and summaries are enriched with human judgments . % The framework also provides an evaluation methodology for , by measuring how well the resulting  correlates with human judgments.  In \Secref{sec:comparison} we evaluate our inferred s with respect to how well the induced scoring function  correlates with human judgments. Our proposed algorithms significantly surpass previous baselines by large margins.   In \Secref{sec:geometry}, we give a geometrical perpespective on the framework and show that a clear geometrical structure emerges from real summarization data.  % The framework is simple, constrained and interpretable but this does not hinder its ability to fit the data. In fact, our proposed algorithms significantly and largely surpass previous baselines in terms of correlation with human judgments.   % The framework is general and inferring human prior on information importance can be of broad use. We explore several applications and briefly discuss potential for future work. The ability to infer interpretable importance priors in a data-driven way has many applications, some of which we explore in \Secref{sec:applications}.  % We explore some of them and later discuss possibilities for future work. \Secref{sec:qualitative_analysis} qualitatively reveals which topics emerge as known and unkown in the fitted priors. % First, it is possible to investigate qualitatively the fitted priors to understand which topics emerge as known and unkown.  % We do so both at the word level and at the topic-model level.  Moreover, we can infer  based on different subsets of the data. By training on the data of one annotator, we get a prior specific to this annotator. Similarly, one can find domain-specific 's by training on different datasets. This is explored in \Secref{sec:annotator_specific}, where we analyze  annotators and  different summarization datasets, yielding interesting insights, e.g., averaging several, potentially biased, annotator-specific or domain-specific 's results in systematic generalization gains. % Adding the inferred 's to summarization systems can produce improvements in the quality of extracted summaries .   Finally, we discuss future work and potential applications beyond summarization in \Secref{sec:ccl}. Our code is available at \url{https://github.com/epfl-dlab/KLearn}     %that averaging various annotator specific 's gives large generalization improvements over single annotators and compared to previous baselines. Furthermore, the average of all annotators performs almost as good as the optimal . Similarly, averaging many domain-specific 's gives significant improvements over baselines in TAC datasets.  %Finally, a more qualitative analysis of the best 's reveals that they capture stopwords and some properties of IDFs even without being exposed to any background corpora.       %Background knowledge is important in summarization and often left out.  %When not left out, it requires design choices and collection of large background corpora.  %Previous work has defined simple models of summarization which involves background knowledge from first principles  %We show that such formulation allows us to infer background knowledge simply from observing human preferences.   %In fact, a probabilistic model is developed that can infer background knowledge only from pairs of document summaries.    This work builds upon the abstract model introduced by , whose relevant aspects we briefly present here.  Let  be a text and a function mapping a text to its semantic representation of the following form:  The semantic representation is a probability distribution  over so-called semantic units . Many different text representation techniques can be chosen, e.g., topic models with topics as semantic units, or a properly renormalized semantic vector space with the dimensions as semantic units.  In the summarization setting, the source document  and the summary  are represented by probability distributions over the semantic units,  and . Similarly, , the background knowledge, is represented as a distribution  over semantic units.\footnote{We use  and  interchangeably when there is no ambiguity.} Intuitively,  is high whenever  is known. A summary scoring   can be derived from simple requirements:   where \cpt{Red} captures the redundancy in the summary via the entropy \cpt{H}. \cpt{Rel} reflects the relevance of the summary via the Kullback-Leibler  divergence between the summary and the document. A good summary is expected to be similar to the original document, i.e., the KL divergence  should be low. Finally, \cpt{Inf} models the informativeness of the summary via the KL divergence between the summary and the latent background knowledge . The summary should bring new information, i.e., the KL divergence  should be high.    Since the dependency on the document  is never ambiguous, we drop it from the notation and simply use  instead of the explicit  notation.  In this work, we fix .   and focus on word distributions as representations for texts as in previous works .         In this paper, we presented a meta-learning approach to model selection for zero-shot cross-lingual transfer.  We showed that our approach improves over the standard practice of model selection using source language development data.  Experiments on five well-studied NLP tasks show that by inspecting internal representations, our method consistently selects better models. LMS also achieves comparable results to the slower and more expensive alternative of annotating small amounts of target-language development data.       \subsubsection*{Acknowledgments}    We thank Wei Xu for helpful feedback.    Use unnumbered third level headings for the acknowledgments. All   acknowledgments, including those to funding agencies, go at the end of the paper.        
"," This work builds upon the abstract model introduced by , whose relevant aspects we briefly present here.  Let  be a text and a function mapping a text to its semantic representation of the following form:  The semantic representation is a probability distribution  over so-called semantic units . Many different text representation techniques can be chosen, e.g., topic models with topics as semantic units, or a properly renormalized semantic vector space with the dimensions as semantic units.  In the summarization setting, the source document  and the summary  are represented by probability distributions over the semantic units,  and . Similarly, , the background knowledge, is represented as a distribution  over semantic units.\footnote{We use  and  interchangeably when there is no ambiguity.} Intuitively,  is high whenever  is known. A summary scoring   can be derived from simple requirements:   where \cpt{Red} captures the redundancy in the summary via the entropy \cpt{H}. \cpt{Rel} reflects the relevance of the summary via the Kullback-Leibler  divergence between the summary and the document. A good summary is expected to be similar to the original document, i.e., the KL divergence  should be low. Finally, \cpt{Inf} models the informativeness of the summary via the KL divergence between the summary and the latent background knowledge . The summary should bring new information, i.e., the KL divergence  should be high.    Since the dependency on the document  is never ambiguous, we drop it from the notation and simply use  instead of the explicit  notation.  In this work, we fix .   and focus on word distributions as representations for texts as in previous works .",88
"  . }  Definition Extraction refers to the task in Natural Language Processing  of detecting and extracting a term and its definition in different types of text. A common use of automatic definition extraction is to help building dictionaries , but it can be employed for many other applications. For example, ontology building can benefit from methods that extract definitions , whilst the fields of definition extraction and information extraction can employ similar methodologies. It is therefore normal that there is growing interest in the task of definition extraction.  This paper describes our system that participated in two of the three subtasks of Task 6 at SemEval 2020 , a shared task focused on definition extraction from a specialised corpus. Our method employs state-of-the-art neural architectures in combination with automatic methods which extend and clean the provided dataset.  %Task 6 at SemEval 2020  is a shared task for definition extraction from a specialised corpus, tailoured specifically to the needs of definition extraction. This paper describes the RGCL team system that works on all three subtasks of the shared task. We employ state-of-the-art neural architectures and combine them with simple automatic methods to extend and clean the provided dataset where appropriate.  The remaining parts of this paper are structured as follows. First, we present related work in the area of definition extraction and the related field of relation extraction . The three subtasks and the dataset provided by the task organisers are described in Section . Next, we describe our system , followed by the results of the evaluation  and a final conclusion .      The first efforts related to definition extraction happened in the field of hypernym extraction, where relations that usually indicate a definition were also dealt with. This includes the X is a type of Y relation, such as  salmon is a type of fish, where salmon is a hyponym of fish.  , which is the hypernym in this case.  Notable work includes , who automatically extracts hyponyms from large amounts of unstructured text using lexico-syntactic patterns. Inspired by this approach,  describe a similar method to mine definitions in French, which are then classified in terms of their semantic relations, limited to the hypernymy - synonymy relation. The approach is also used for building ontologies .  The importance of the semantic relations between words for pattern-based approaches to definition extraction is highlighted in . Here, the authors describe and explain definitional verbal patterns in Spanish, which they also propose to use for mining definitions. The proposed system is further presented in  and is aimed at Spanish technical texts. The system uses the aforementioned verbal patterns, as well as corresponding tense and distance restrictions, in order to extract a set of candidate terms and their definitions. Once extracted, the system applies some filtering rules and a decision tree to further analyse the candidates. Finally, the results are ranked using heuristic rules. All aspects of the system were developed by analysing the Institut Universitari de Ling\""{u}\'{i}stica Aplicada technical corpus in Spanish, which is also used for evaluation.   Machine learning algorithms have also been used for definition extraction.  describe an approach that is said to be language independent and test it with decision trees and Random Forest, as well as Na\""{i}ve Bayes, k-Nearest Neighbour and Support Vector Machines using different sampling techniques to varying degrees of success.  process Polish texts and use Balanced Random Forests, which bootstrap equal sets of positive and negative training examples to the classifier, as opposed to a larger group of unequal sets of training examples. Overall, while the approach is said to increase run time, it does bring minor increases in performance with some fine-tuning.  Most recently,  have created DEFT, a corpus for definition extraction from unstructured and semi-structured texts. Citing some of the pattern-based approaches also mentioned here, the authors argue that definitions have been well-defined and not necessarily representative of natural language. Therefore, a new corpus is presented that is said to more accurately represent natural language, and includes more messy examples of definitions. Parts of the DEFT corpus make up the dataset for this shared task, which is described in more detail in the following section.      We focus on the often-ignored background knowledge for summarization and infer it from implicit signals from human summarizers and annotators. We introduced and evaluated different approaches, observing strong abilities to fit the data.   We also provide geometrical insights for the framework and the inferred background knowledge.  The newly-gained ability to infer interpretable priors on importance in a data-driven way has many potential applications. For example, we can describe which topics should be extracted more frequently by systems to improve their agreement with humans. Using pretrained priors also helps systems to reduce overfitting on the frequency signal within source documents as illustrated by initial results in \Appref{sec:summarization}.   An important application made possible by this framework is to infer  on any meaningful subset of the data. In particular, we learned annotator-specific 's, which yielded interesting insights: some annotators exhibit large differences from the others, and averaging several, potentially biased 's results in generalization improvements. We also inferred 's from different summarization datasets and also found increased performance on the news domain when averaging 's from diverse domains.  For future work, different choices of semantic units can be explored, e.g., learning  directly in the embedding space. Also, we fixed  to get comparable results across methods, but including them as learnable parameters could provide further performance boosts. Investigating how to infuse the fitted priors into summarization systems is another promising direction.   More generally, inferring  from a common-sense task like summarization can provide insights about general human importance priors. Inferring such priors has applications beyond summarization, as the framework can model any information selection task.   Finally, inferring unobserved importance priors is a general problem with applications beyond summarization. The proposed framework can benefit any information selection task.    and the method proposed here can bene information selection task.    Put the focus on the background knowledge   Data-driven way to infer it from implicit signal in summarization data   Proposed several approaches that work with different kind of data    They work well    The general framework for inferring priors has several potential applications.   Some of which we investigated in this work. For example, we found some which topics should be extracted more/less by summarization systems to improve their agreement with human judgments. Also, the use such priors can help systems to not overfit on the frequency signal in original documents     An interesting application is to aggregate on different subsets of data. In particular, we obtained annotator specific and domain specific priors and could compare quantitatively annotators and domains. Interestingly, we find consistent improvements resulting from averaging several, potentially biased, priors.    The framework also has application beyond summarization has the methodology can be easily extended to general information selection tasks .    Within summarization, one can also explore the use of different semantic units, in particular learning  directly in semantic sapce of embeddings could be interesting.   Also, we fix here the parameters  and  to 1, learning this parameters alongside  would give better ability to fit the data. Finally, investigating how to infuse the fitted priors into summarization systems is promising direction for improviment systems.      In this work, we leveraged summarization data to infer background knowledge. We inferred annotator and domain-specific priors and found large benefits resulting from averaging different background knowledge.     For future work, such human priors can be used to improve summarization systems but also automatic evaluation metrics. Another promising direction could study different semantic unit representations, e.g., distributional representations.     In general, a better understanding of human priors and background knowledge can benefit a wide range of applications like information retrieval or dialog systems.                                                                                  Introduction                                                                                    Include this file in all LaTeX papers that you write at dlab by adding a line   """" right after the ""\documentclass"" command.                                                                                    Some standard packages                                                                                  \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc} \usepackage{hyphenat} \usepackage{xspace} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{hyperref} \usepackage{url} \usepackage{booktabs} \usepackage{multirow}  \usepackage{subfig} \usepackage{makecell} \usepackage{caption} \usepackage{minibox} \usepackage{bbm} \usepackage{graphicx} \usepackage{balance} \usepackage{mathtools} \usepackage{color} \usepackage{marvosym} \usepackage{ifthen} \usepackage{textcomp} \usepackage{enumitem} \usepackage{verbatim} \usepackage{algorithm} \usepackage{algorithmic} \usepackage{numprint} \usepackage{balance}  \usepackage{amsthm} \theoremstyle{plain} \newtheorem{theorem}{Theorem} \newtheorem{definition}[theorem]{Definition} \newtheorem{lemma}[theorem]{Lemma} \newtheorem{proposition}[theorem]{Proposition} \newtheorem{example}[theorem]{Example}                                                                                    How to include TODOs and notes                                                                                    Adapted from the widely circulating chato-notes.sty -- thanks, ChaTo!  \newcommand{\chatoDisplayMode}[1]{#1}    If you quickly want to hide all notes, e.g., to check how long your paper   would be without them, add the following line to your preamble or uncomment   it here.   \renewcommand{\chatoDisplayMode}[1]{}    Usage:   \todo[Your name]{What needs to be done}   \note[Your name]{A note to include in a box}   \inote{An inline note}   \citemissing{}   \definecolor{MyRed}{rgb}{0.6,0.0,0.0}  \definecolor{MyBlack}{rgb}{0.1,0.1,0.1}  \newcommand{\inred}[1]{{\color{MyRed}\sf}} \newcommand{\frameit}[2]{      }\\   }    }  \newcommand{\note}[2][]{\chatoDisplayMode{\def\@tmpsig{#1}\frameit{{\Pointinghand} Note}{#2\ifx \@tmpsig \@empty \else \mbox{ --\em #1}\fi}}} \newcommand{\todo}[2][]{\chatoDisplayMode{\def\@tmpsig{#1}\frameit{{\Writinghand} To-do}{#2\ifx \@tmpsig \@empty \else \mbox{ --\em #1}\fi}}} \newcommand{\inote}[1]{\chatoDisplayMode{\inred{{{\Pointinghand} }} {\sf #1} \inred{}}} \newcommand{\citemissing}[0]{\chatoDisplayMode{\inred{[citation]}}}                                                                                    How to make your edits conspicuous                                                                                    In the final stages of editing, it is often useful to mark edits in color, so   everyone can easily see what was changed. To do so, define a command that has   the same name as you and use your favorite color. \newcommand{\bob}[1]{{#1}} \newcommand{\yourname}[1]{{#1}}                                                                                    Latin abbreviations                                                                                    Don't use plain text for Latin abbreviations such as ""e.g."", ""i.e."", etc.   Use these macros instead. Advantage: you can consistently change their style,   e.g., if you want to typeset them in italics at some point.    Latin abbreviations in normal font. \newcommand{\abbrevStyle}[1]{#1}   Latin abbreviations in italics.   \newcommand{\abbrevStyle}[1]{#1}  \newcommand{\ie}{\abbrevStyle{i.e.}\xspace} \newcommand{\eg}{\abbrevStyle{e.g.}\xspace} \newcommand{\cf}{\abbrevStyle{cf.}\xspace} \newcommand{\etal}{\abbrevStyle{et al.}\xspace} \newcommand{\vs}{\abbrevStyle{vs.}\xspace} \newcommand{\etc}{\abbrevStyle{etc.}\xspace} \newcommand{\viz}{\abbrevStyle{viz.}\xspace}                                                                                    Referring to sections, figures, tables, etc.                                                                                    To refer to sections, figures, tables, etc., use the following macros.   Don't type ""Section~1"", ""Fig.~1"", etc., manually. This way, you can easily   and consistently switch between styles, e.g., if you want to use ""Sec.""   instead of ""Section"" at some point.  \newcommand{\Secref}[1]{Sec.} \newcommand{\Eqnref}[1]{Eq.} \newcommand{\Dashsecref}[2]{Sec.--} \newcommand{\Dblsecref}[2]{Sec. and } \newcommand{\Tabref}[1]{Table} \newcommand{\Figref}[1]{Fig.} \newcommand{\Dashfigref}[2]{Fig.--} \newcommand{\Appref}[1]{Appendix} \newcommand{\Thmref}[1]{Thm.} \newcommand{\Lemmaref}[1]{Lemma} \newcommand{\Defref}[1]{Def.}                                                                                    Paragraph headings                                                                                    Academic text is often much more legible if you give important paragraphs a   concise name that describes what the paragraph is about. Use the \xhdr   command for this. \newcommand{\xhdr}[1]{{{\bf #1.}}}    Same as \xhdr, but without a period after the heading. Use this version if   the heading is directly integrated into the first sentence of the paragraph;   e.g., ""\xhdrNoPeriod{Results} are shown in \Figref{fig}."" \newcommand{\xhdrNoPeriod}[1]{{{\bf #1}}}                                                                                    More compact lists                                                                                    In some styles, list items are widely spaced. To condense them and save some   space, you may use this command. \newcommand{\denselist}{ \itemsep -2pt\topsep-10pt\partopsep-10pt }    Same, but with slightly different spacing. \newcommand{\denselistRefs}{ \itemsep -2pt\topsep-5pt\partopsep-7pt }                                                                                    Miscellaneous useful macros                                                                                    Some bibliography styles make it hard to typeset references like   ""Einstein et al. "". This command provides a convenient way to do so. \newcommand{\textcite}[1]{\citeauthor{#1} \shortcite{#1}}    When you frequently refer to Wikipedia articles, Wikidata entities, etc., it   may be useful to typeset those in a particular font. Use the \cpt  command for this purpose. \newcommand{\cpt}[1]{}}    To exclude a large portion of text from the PDF, wrap it in \hide. \newcommand{\hide}[1]{}    Wrap matrix variables in \mtx. Don't make them bold etc. manually. By using   a macro, you can consistently change the rendering style at any point. \newcommand{\mtx}[1]{\mathbf{#1}}    Transpose of a matrix, e.g., . \newcommand{\trans}{^\top}    \argmin and \argmax. \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}                                                                                    Hyphenation                                                                                    Some words are ill-hyphenated by default. Here you can define the correct   hyphenation once, and it is then used consistently.  \hyphenation{ Wi-ki-pe-dia Wi-ki-me-dia Wi-ki-da-ta De-ter-mine Page-Rank web-page web-pages da-ta-set }                                                                                    Avoid widows!                                                                                    The term ""widow"" refers to the first line of a paragraph if it is the last   line on a page, or to the last line of a paragraph if it is the first line on   a page. Widows are considered a cardinal typesetting sin, so avoid them at   all cost, via the following commands.  \widowpenalty=10000 \clubpenalty=10000                                                                                    Enable section numbering in the AAAI style                                                                                     In the AAAI style, this enables section numbering. \setcounter{secnumdepth}{2}                                                                                    Listing authors in a space-economic way in the ACM style                                                                                    By default, using ""\documentclass[sigconf]{acmart}"" will list authors in rows   of 2, which can take up a lot of space. To get more authors in one row, use   something like this:   \author{     \authorbox{Author 1}{Affiliation 1}{Email 1}     \authorbox{Author 2}{Affiliation 2}{Email 2}     ...   }    If you use \authorbox, you will also have to suppress the standard reference   block, by pasting the following row somewhere before ""     BEFORE BOB'S EDITS:   Despite being an essential aspect of any information selection process, background knowledge has received little attention from the summarization field.      In contrast, this work puts the focus on this neglected component. We emphasize that choices made by human summarizers and annotators contain implicit information about their priors. Thus, we develop and compare several approaches leveraging these signals.    This produces data-driven and interpretable information importance priors that fit human judgment data significantly better than baselines.   We then illustrate some of the many potential applications. First, we investigate which topics received low or high weight in the inferred priors. By using different aggregation of data, we obtain annotator\hyp specific and domain\hyp specific priors. A simple analysis yields interesting insights, e.g., averaging many, potentially biased, priors systematically and greatly improves performance. Finally, the resulting priors can be used to guide summarization systems.              
","  The first efforts related to definition extraction happened in the field of hypernym extraction, where relations that usually indicate a definition were also dealt with. This includes the X is a type of Y relation, such as  salmon is a type of fish, where salmon is a hyponym of fish.  , which is the hypernym in this case.  Notable work includes , who automatically extracts hyponyms from large amounts of unstructured text using lexico-syntactic patterns. Inspired by this approach,  describe a similar method to mine definitions in French, which are then classified in terms of their semantic relations, limited to the hypernymy - synonymy relation. The approach is also used for building ontologies .  The importance of the semantic relations between words for pattern-based approaches to definition extraction is highlighted in . Here, the authors describe and explain definitional verbal patterns in Spanish, which they also propose to use for mining definitions. The proposed system is further presented in  and is aimed at Spanish technical texts. The system uses the aforementioned verbal patterns, as well as corresponding tense and distance restrictions, in order to extract a set of candidate terms and their definitions. Once extracted, the system applies some filtering rules and a decision tree to further analyse the candidates. Finally, the results are ranked using heuristic rules. All aspects of the system were developed by analysing the Institut Universitari de Ling\""{u}\'{i}stica Aplicada technical corpus in Spanish, which is also used for evaluation.   Machine learning algorithms have also been used for definition extraction.  describe an approach that is said to be language independent and test it with decision trees and Random Forest, as well as Na\""{i}ve Bayes, k-Nearest Neighbour and Support Vector Machines using different sampling techniques to varying degrees of success.  process Polish texts and use Balanced Random Forests, which bootstrap equal sets of positive and negative training examples to the classifier, as opposed to a larger group of unequal sets of training examples. Overall, while the approach is said to increase run time, it does bring minor increases in performance with some fine-tuning.  Most recently,  have created DEFT, a corpus for definition extraction from unstructured and semi-structured texts. Citing some of the pattern-based approaches also mentioned here, the authors argue that definitions have been well-defined and not necessarily representative of natural language. Therefore, a new corpus is presented that is said to more accurately represent natural language, and includes more messy examples of definitions. Parts of the DEFT corpus make up the dataset for this shared task, which is described in more detail in the following section.",89
"  Recently, pre-trained self-supervised models such as BERT have attracted an increasing amount of attention in natural language processing and vision-language processing.  Benefiting from common knowledge contained in massive unlabeled data, the pretraining-finetuning framework has become a representative paradigm for advancing various language-related downstream tasks.   Most endeavors on pre-trained representation models rely on elaborately designed self-supervised tasks, which typically corrupt the given sequence with certain types of noise , and then train the model to recover the original sequence.  As a consequence, the learned representations tend to be covariant with the input noise of pre-training in this paradigm.  However, when transferred to downstream tasks, the pre-trained model is responsible for encoding the original sequence without noise, and is expected to obtain noise invariant representations.  Such pretrain-finetune discrepancy not only impedes fast fine-tuning, but also may result in suboptimal sequence representations, thus affecting the performance in downstream tasks.   %%%%%%%%%%%% %  % 	\vskip -0.1in % \end{table} %%%%%%%%%%%%  %%%%%%%%%%%% %  %%%%%%%%%%%%  To remedy this, we present ContrAstive Pre-Training  to learn noise invariant  sequence representations. %, inspired by the Noise Contrastive Estimation. The core idea of CAPT is to enhance the consistency between semantic representations of the original sequence and that of corresponding corrupted version  via unsupervised instance-wise training signals. %can be fully utilized via elaborately designed semantic contrastive loss. %As shown in Figure, our approach  In more detail, it strives to pull the representation of the corrupted sequence towards that of the original instance in the semantic space, while pushing it away from representations of other instances. % Such training objectives are formulated as a multi-class classification task, which aims at classifying the original sequence to the class of its corrupted version and vice versa, while classifying different instances into different classes. % For implementation feasibility, two effective model extension are proposed to further enhance the capability of the model to extract noise-concentrated and instance-diffused features. Moreover, in order to enable the model to learn from more ``difficult'' and ``diverse'' instances, two effective methods are proposed to further enhance the capability of the model to extract noise-concentrated and instance-diffused features. With such training objective, the pre-trained model is encouraged to learn noise invariant representations, thereby alleviating the pretrain-finetune discrepancy to some extent.  As an additional benefit, CAPT also assists the pre-trained model to more effectively capture the global semantics of the input.  Most prior work only focuses on token-level pre-training tasks , which lacks the modeling of global semantics of the input.  Some other efforts alleviate this problem by introducing sentence-level pre-training tasks  that rely on the relative position of segments in the document. However, the semantic connection between these segments tends to be excessively loose, which may result in confusing gradient signals.  By contrast, our CAPT offers incentives for representations of inputs sharing the same semantics  to be similar, while the representations of inputs expressing different semantics  are penalized to be distinguished from each other. Such more reasonable sentence-level supervision enables our approach to look beyond the local structures of input sequences and become more aware of the global semantics. %With such more reasonable sentence-level supervision, our approach achieves better modeling of global semantics of the input.   We perform the evaluation on a comprehensive suite of benchmark, covering 8 natural language understanding and 3 cross-modal tasks.  Extensive empirical evidence demonstrates that our approach can achieve consistent improvements over the baselines in both language and vision-language domains. To be more specific, our CAPT raises the performance of RoBERTa from 88.9\% to 89.5\% on the GLUE dev set, and also surpasses LXMERT by 0.5\%, 0.6\% and 0.8\% on VQA, GQA and , respectively.          In summary, this work is mainly related to the following three lines of research.  \paragraph{Pre-trained language representations.}  This task strives to build linguistic representations benefiting various downstream tasks.   In terms of model architecture,  One line of research focuses on autoregressive  pre-training, while the other centers on denoising autoencoding .  as the core foundation.  Representative work of AR pre-training includes ELMo and GPT, which aim to predict the next word based on previous tokens   in a unidirectional pattern  but lack the modeling of bidirectional context. Furthermore, XLNet remedies this with  generalized AR pre-training based on  permutation language modeling, but it enlarges the training cost.  The other research line is built upon DAE, which strives to reconstruct the original sequence based on the corrupted input by jointly attending to both the left and right context.   The related endeavors share the same model architecture , with the core difference being pre-training tasks. Main efforts focus on token-level pre-training tasks. For instance, both BERT and RoBERTa adopt MLM to recover the masked words.  or spans.  StructBERT   attempts to  incorporates word structure by restoring each shuffled token to its correct position.   ELECTRA presents a more efficient approach by partially replacing the original input by the sequence from the generator. However, DAE introduces the noise discarded on downstream tasks during pre-training, which is prone to learn representations covariant with the input noise, leading to the pretrain-finetune discrepancy.   However, these approaches neglect the modeling of global semantics of the input.    Others address this problem by incorporating supervision signals regarding the representation of entire segments through sentence-level tasks .   Besides, most AR and DAE based pre-training tasks neglect the modeling of global semantics of the input. Some DAE based approaches address this problem by incorporating supervisions regarding the entire segment through sentence-level tasks .  or adjacent sentence prediction).  However, such training relies heavily on the relative position of segments, which suffers from excessively loose semantic connections. Thus, it tends to result in confusing gradient signals.  In addition, denoising autoencoding is prone to learn representations that are covariant with the input noise of pre-training,  In comparison, our CAPT encourages the semantic consistency of the original sequence and its corrupted version  via unsupervised contrastive loss. This not only alleviates the pretrain-finetune discrepancy, but also better captures the global semantics of the input.   \paragraph{Pre-trained vision-language representations.}   This research direction aims to build generic representation models for vision-language tasks.     Recent work has led to significant improvement on cross-modal tasks by following the pretraining-finetuning paradigm.     Recent work typically involves extracting RoI features and bounding box information with off-the-shelf systems , using self-supervised learning ) to train a transformer-based model on large-scale cross-modal datasets, and then transferring it to downstream tasks.     For example, ViLBERT, LXMERT, VL-BERT, VisualBERT, B2T2, UNITER and Unicoder-VL are targeted at learning image-text representations. All of them use RoI features and bounding box information as visual inputs, which are obtained through an object detection system such as Faster RCNN.    In terms of model architecture, one representative research line focuses on two-stream architectures.   For instance, both ViLBERT and LXMERT first encode visual and textual features by two separate transformers, respectively.    Then, the co-attentional transformer layers are introduced to allow visual representations to attend to the texts and vice versa.    ViLBERT and LXMERT proposed a two-stream architecture where image features and texts are first encoded by two transformers separately. Co-attentional transformer layers are then introduced to allow the image representations to attend to the texts and vice versa.    By contrast, the other line such as   VL-BERT, VisualBERT, B2T2, UNITER and Unicoder-VL, strives to learn generic image-text representations with a unified single-stream model.    They usually combine visual and textual information early, for example, by projecting them into a common embedding space. The fused representations are then fed into a single transformer like BERT to produce the cross-modal features.    This direction attempts to build generic representation models for vision-language tasks.  In terms of model architecture, one research line focuses on one-stream BERT-based architecture, which strives to learn generic image-text representations with a unified model. The corresponding representative work includes VideoBERT, VisualBERT,  B2T2,  UNITER, Unicoder-VL, etc. They usually first combine visual and textual information by projecting them into a common embedding space. The fused representations are then fed into a single transformer to produce the cross-modal features.  In contrast, the other line such as ViLBERT and LXMERT focuses on the two-stream architecture.  They first encode visual and textual features by two separate transformers, respectively.  Then, the co-attentional transformer layers are introduced to allow visual representations to attend to the textual representations and vice versa.    Thus they can adapt to the needs of different input processing for each modality and better capture the interactions between multiple modalities. As for pre-training tasks, different work exhibits commonalities, all focusing on MRM, MLM, and several specific tasks . However, most of these tasks are prone to learning noise covariant representations in the pre-training stage. Compared with these endeavors, our CAPT benefits the pre-trained model to learn noise invariant vision-language representations via elaborately-designed semantic contrastive loss, thereby bringing better model performance.  \paragraph{Contrastive learning.}  Contrastive learning is a branch of unsupervised representation learning, which has been widely used in learning graph representations, word embeddings, image/video representations and structured world models. The main idea is to construct pairs of related  data as positive samples  and pairs of unrelated data as negative samples, and then learn to classify them via the contrastive loss.  The contrastive loss can come in several forms, including noise contrastive estimation, instance-wise classification, and etc. It serves as an unsupervised objective to learn feature embeddings where representations of positive samples are concentrated and negative representations are as distant as possible.  Inspired by these works, we adapt contrastive learning to the natural language and vision-language domains to learn noise invariant sequence representations and demonstrate its effectiveness in improving massive pre-trained models.   Contrastive learning is a branch of unsupervised representation learning, the goal of which is to learn generic embedding features preserving the high-level signals of the input while stripping off the noise. It serves the purpose of reducing the dimensionality of the input, alleviating the scarcity of labeled data and producing representations that can be transferred to downstream tasks. The idea is to learn a feature embedding where representations of positive examples  are concentrated and representations of negative examples are as distant as possible. Various pretext tasks and optimization objectives have been designed to achieve this goal. Instance discrimination task treats each instance as a distinctive class and connects the objective with instance-wise classification loss or mutual information between input data and learned representations. This task is subject to high computational cost due to the large number of instances in the training data. Therefore, in practice, many techniques have been introduced to tackle this problem, such as approximating the full softmax distribution ) and improving efficiency using a memory bank or momentum contrast. Another commonly used pretext task aims at learning features that are invariant to data augmentation methods or views. Related efforts usually construct positive pairs by transforming the input data using different rules and feed the pairs into a Siamese network. They then train this network by minimizing the contrastive loss, maximizing mutual information or using NCE. Other tasks include autoregressive future prediction which is employed in contrastive predictive coding  along with a loss based on NCE called InfoNCE. Contrastive learning has been widely used in learning word embeddings, image/video representations and structured world models without supervision. Some work also uses contrastive learning in a supervised setting. For example,  proposed to adapt contrastive learning to the task of image captioning to encourage distinctiveness between input data, where true image-caption pairs are regarded as positive examples and mismatched pairs are considered negative examples.     In this paper, we have proposed a novel cross-supervised mechanism which allows models to extract entities and triggers jointly. Our mechanism alternately supervises the extraction process for either the triggers or the entities, based on the information in the type distribution of each other. In this way, we incorporate the co-occurrence relationships between entities and triggers into the joint-event-extraction process of our model. Moreover, to further address the problem caused by the sparse co-occurrence relationships, our method also resorts to the heterogeneous information network technology to collect indirect co-occurrence relationships. The empirical results show that our method improves the extraction performances for entities and triggers simultaneously. This verifies that the incorporated co-occurrence relationships are useful for the joint-event-extraction task and our method is more effective than existing methods in utilizing training samples. Our future works include:  investigating the impact of length of sampled meta-paths, as in this paper we have limited the meta-path into a fixed length;  connecting the extracted entities and triggers from a corpus to facilitate the automatic knowledge graph construction.  
","     In summary, this work is mainly related to the following three lines of research.  \paragraph{Pre-trained language representations.}  This task strives to build linguistic representations benefiting various downstream tasks.   In terms of model architecture,  One line of research focuses on autoregressive  pre-training, while the other centers on denoising autoencoding .  as the core foundation.  Representative work of AR pre-training includes ELMo and GPT, which aim to predict the next word based on previous tokens   in a unidirectional pattern  but lack the modeling of bidirectional context. Furthermore, XLNet remedies this with  generalized AR pre-training based on  permutation language modeling, but it enlarges the training cost.  The other research line is built upon DAE, which strives to reconstruct the original sequence based on the corrupted input by jointly attending to both the left and right context.   The related endeavors share the same model architecture , with the core difference being pre-training tasks. Main efforts focus on token-level pre-training tasks. For instance, both BERT and RoBERTa adopt MLM to recover the masked words.  or spans.  StructBERT   attempts to  incorporates word structure by restoring each shuffled token to its correct position.   ELECTRA presents a more efficient approach by partially replacing the original input by the sequence from the generator. However, DAE introduces the noise discarded on downstream tasks during pre-training, which is prone to learn representations covariant with the input noise, leading to the pretrain-finetune discrepancy.   However, these approaches neglect the modeling of global semantics of the input.    Others address this problem by incorporating supervision signals regarding the representation of entire segments through sentence-level tasks .   Besides, most AR and DAE based pre-training tasks neglect the modeling of global semantics of the input. Some DAE based approaches address this problem by incorporating supervisions regarding the entire segment through sentence-level tasks .  or adjacent sentence prediction).  However, such training relies heavily on the relative position of segments, which suffers from excessively loose semantic connections. Thus, it tends to result in confusing gradient signals.  In addition, denoising autoencoding is prone to learn representations that are covariant with the input noise of pre-training,  In comparison, our CAPT encourages the semantic consistency of the original sequence and its corrupted version  via unsupervised contrastive loss. This not only alleviates the pretrain-finetune discrepancy, but also better captures the global semantics of the input.   \paragraph{Pre-trained vision-language representations.}   This research direction aims to build generic representation models for vision-language tasks.     Recent work has led to significant improvement on cross-modal tasks by following the pretraining-finetuning paradigm.     Recent work typically involves extracting RoI features and bounding box information with off-the-shelf systems , using self-supervised learning ) to train a transformer-based model on large-scale cross-modal datasets, and then transferring it to downstream tasks.     For example, ViLBERT, LXMERT, VL-BERT, VisualBERT, B2T2, UNITER and Unicoder-VL are targeted at learning image-text representations. All of them use RoI features and bounding box information as visual inputs, which are obtained through an object detection system such as Faster RCNN.    In terms of model architecture, one representative research line focuses on two-stream architectures.   For instance, both ViLBERT and LXMERT first encode visual and textual features by two separate transformers, respectively.    Then, the co-attentional transformer layers are introduced to allow visual representations to attend to the texts and vice versa.    ViLBERT and LXMERT proposed a two-stream architecture where image features and texts are first encoded by two transformers separately. Co-attentional transformer layers are then introduced to allow the image representations to attend to the texts and vice versa.    By contrast, the other line such as   VL-BERT, VisualBERT, B2T2, UNITER and Unicoder-VL, strives to learn generic image-text representations with a unified single-stream model.    They usually combine visual and textual information early, for example, by projecting them into a common embedding space. The fused representations are then fed into a single transformer like BERT to produce the cross-modal features.    This direction attempts to build generic representation models for vision-language tasks.  In terms of model architecture, one research line focuses on one-stream BERT-based architecture, which strives to learn generic image-text representations with a unified model. The corresponding representative work includes VideoBERT, VisualBERT,  B2T2,  UNITER, Unicoder-VL, etc. They usually first combine visual and textual information by projecting them into a common embedding space. The fused representations are then fed into a single transformer to produce the cross-modal features.  In contrast, the other line such as ViLBERT and LXMERT focuses on the two-stream architecture.  They first encode visual and textual features by two separate transformers, respectively.  Then, the co-attentional transformer layers are introduced to allow visual representations to attend to the textual representations and vice versa.    Thus they can adapt to the needs of different input processing for each modality and better capture the interactions between multiple modalities. As for pre-training tasks, different work exhibits commonalities, all focusing on MRM, MLM, and several specific tasks . However, most of these tasks are prone to learning noise covariant representations in the pre-training stage. Compared with these endeavors, our CAPT benefits the pre-trained model to learn noise invariant vision-language representations via elaborately-designed semantic contrastive loss, thereby bringing better model performance.  \paragraph{Contrastive learning.}  Contrastive learning is a branch of unsupervised representation learning, which has been widely used in learning graph representations, word embeddings, image/video representations and structured world models. The main idea is to construct pairs of related  data as positive samples  and pairs of unrelated data as negative samples, and then learn to classify them via the contrastive loss.  The contrastive loss can come in several forms, including noise contrastive estimation, instance-wise classification, and etc. It serves as an unsupervised objective to learn feature embeddings where representations of positive samples are concentrated and negative representations are as distant as possible.  Inspired by these works, we adapt contrastive learning to the natural language and vision-language domains to learn noise invariant sequence representations and demonstrate its effectiveness in improving massive pre-trained models.   Contrastive learning is a branch of unsupervised representation learning, the goal of which is to learn generic embedding features preserving the high-level signals of the input while stripping off the noise. It serves the purpose of reducing the dimensionality of the input, alleviating the scarcity of labeled data and producing representations that can be transferred to downstream tasks. The idea is to learn a feature embedding where representations of positive examples  are concentrated and representations of negative examples are as distant as possible. Various pretext tasks and optimization objectives have been designed to achieve this goal. Instance discrimination task treats each instance as a distinctive class and connects the objective with instance-wise classification loss or mutual information between input data and learned representations. This task is subject to high computational cost due to the large number of instances in the training data. Therefore, in practice, many techniques have been introduced to tackle this problem, such as approximating the full softmax distribution ) and improving efficiency using a memory bank or momentum contrast. Another commonly used pretext task aims at learning features that are invariant to data augmentation methods or views. Related efforts usually construct positive pairs by transforming the input data using different rules and feed the pairs into a Siamese network. They then train this network by minimizing the contrastive loss, maximizing mutual information or using NCE. Other tasks include autoregressive future prediction which is employed in contrastive predictive coding  along with a loss based on NCE called InfoNCE. Contrastive learning has been widely used in learning word embeddings, image/video representations and structured world models without supervision. Some work also uses contrastive learning in a supervised setting. For example,  proposed to adapt contrastive learning to the task of image captioning to encourage distinctiveness between input data, where true image-caption pairs are regarded as positive examples and mismatched pairs are considered negative examples.",90
" \subsection{Natural Language Processing} Ang Natural Language Processing  ay isang subfield ng linguistics, computer science, at artificial intelligence na nauukol sa pag proseso at pag-unawa ng natural na wika . Ang ilan sa mga aplikasyon ng NLP ay ang email spam filters , pag-unawa ng nais sabihin tulad ng mga smart assistants , pagsasalin ng isang wika sa iba pang wika , mag predict ng susunod na salita base sa mga naunang salita , at marami pang iba. Dahil sa kaunlaran sa kasaganahan sa datos at pagiging accessible ng malakas na compute power, nabuhay muli ang machine learning approach. Sa maikling salita, ang machine learning approach ay gumagamit ng malaking datos na ginagamit ng isang computer algorithm upang matutunan ang mga patterns ng datos na ito. Dahil dito, naging epektibo siyang approach sa mga komplikadong problema dahil hindi na kailangan direktang i-program ang mga rules para malutas ang isang problema.  \subsection{Transfer Learning} Notorious ang machine learning approach sa pangangailangan nito ng sobrang laking datos para mapakinabangan. Ang Transfer Learning  ay isang area ng research na concerned sa problemang ito . Sa maikling salita, ang TL ay ang pag retain o pagpapanatili ng mga natutunan ng isang model sa isang gawain at paggamit o ""transfer"" ng mga natutunan nito sa iba pero may kaugnayan na gawain. Halimbawa, ang mga natutunan ng isang model sa pag detect ng muka ng tao ay maaring gamitin bilang tuntungan para sa pag-aaral ng model na matutunan kung ang muka ng tao ay galit, masaya, at iba pang facial expressions .      Ang Natural Language Processing  ay isang subfield ng linguistics, computer science, at artificial intelligence na nauukol sa pag proseso at pag-unawa ng natural na wika . Ang ilan sa mga aplikasyon ng NLP ay ang email spam filters , pag-unawa ng nais sabihin tulad ng mga smart assistants , pagsasalin ng isang wika sa iba pang wika , mag predict ng susunod na salita base sa mga naunang salita , at marami pang iba. Dahil sa kaunlaran sa kasaganahan sa datos at pagiging accessible ng malakas na compute power, nabuhay muli ang machine learning approach. Sa maikling salita, ang machine learning approach ay gumagamit ng malaking datos na ginagamit ng isang computer algorithm upang matutunan ang mga patterns ng datos na ito. Dahil dito, naging epektibo siyang approach sa mga komplikadong problema dahil hindi na kailangan direktang i-program ang mga rules para malutas ang isang problema.   Notorious ang machine learning approach sa pangangailangan nito ng sobrang laking datos para mapakinabangan. Ang Transfer Learning  ay isang area ng research na concerned sa problemang ito . Sa maikling salita, ang TL ay ang pag retain o pagpapanatili ng mga natutunan ng isang model sa isang gawain at paggamit o ""transfer"" ng mga natutunan nito sa iba pero may kaugnayan na gawain. Halimbawa, ang mga natutunan ng isang model sa pag detect ng muka ng tao ay maaring gamitin bilang tuntungan para sa pag-aaral ng model na matutunan kung ang muka ng tao ay galit, masaya, at iba pang facial expressions .     This work presents contrastive pre-training for learning denoised sequence representations in a self-supervised manner. By enhancing the consistency between representations of the original sequence and the corresponding corrupted version, the pre-trained model is encouraged to learn noise invariant sequence representations.  On this account, the proposed approach not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also better captures the global semantics of the input via more effective sentence-level supervision. Extensive experiments demonstrate the effectiveness and versatility of our approach, which can achieve consistent improvements over baselines in both language and vision-language domains.  
","  Ang Natural Language Processing  ay isang subfield ng linguistics, computer science, at artificial intelligence na nauukol sa pag proseso at pag-unawa ng natural na wika . Ang ilan sa mga aplikasyon ng NLP ay ang email spam filters , pag-unawa ng nais sabihin tulad ng mga smart assistants , pagsasalin ng isang wika sa iba pang wika , mag predict ng susunod na salita base sa mga naunang salita , at marami pang iba. Dahil sa kaunlaran sa kasaganahan sa datos at pagiging accessible ng malakas na compute power, nabuhay muli ang machine learning approach. Sa maikling salita, ang machine learning approach ay gumagamit ng malaking datos na ginagamit ng isang computer algorithm upang matutunan ang mga patterns ng datos na ito. Dahil dito, naging epektibo siyang approach sa mga komplikadong problema dahil hindi na kailangan direktang i-program ang mga rules para malutas ang isang problema.   Notorious ang machine learning approach sa pangangailangan nito ng sobrang laking datos para mapakinabangan. Ang Transfer Learning  ay isang area ng research na concerned sa problemang ito . Sa maikling salita, ang TL ay ang pag retain o pagpapanatili ng mga natutunan ng isang model sa isang gawain at paggamit o ""transfer"" ng mga natutunan nito sa iba pero may kaugnayan na gawain. Halimbawa, ang mga natutunan ng isang model sa pag detect ng muka ng tao ay maaring gamitin bilang tuntungan para sa pag-aaral ng model na matutunan kung ang muka ng tao ay galit, masaya, at iba pang facial expressions .",91
"  \iffalse \dr{%If we want to reposition it as in the abstract, we should start by considering the event in Fig. 1:  Natural language text is typically written to tell the reader about events. But events are not expressed as single predicate mentions, but rather as structures over multiple such predicates and their arguments. Consider the description the impact of the Typhoon in Fig..... It is mentioned that the typhoon killed people , flights canceled and affected many people. It is also clear that there is temporal order among some of the predicates, and recognizing this is important to understanding the composite event. Then you can continue saying that this is our goal.}  \fi  % typically, a single predicate mention  does not constitute what we typically think about as events; we typically think of an event as something that consists of multiple such primitive structures %{\fontsize{10.5}{11} \selectfont Text}           %\fontsize{11pt}{13pt}\selectfont Human languages evolve to communicate about %always involve the description of  real-world events. Therefore, understanding events plays a critical role in natural language understanding . A key challenge to this mission lies in the fact that events are not just simple, standalone predicates. Rather, they are often described at different granularities and may form complex structures. %topologies. Consider the example in Figure, where the description of a storm  involves more fine-grained event mentions about people killed , flights canceled  and passengers affected . Some of those mentions also follow strict temporal order . Our goal is to induce such an event complex that recognizes %organizes  the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering , narrative prediction , timeline construction  and summarization . %\dr{The choice of references is good but revealing; I suggest to replace the summarization with a ``classical"" summarization paper .  %such as question answering , narrative prediction , coreference resolution , and summarization . Since events are not standalone objects, understanding event essentially involves comprehending their relations, %cite{wities-etal-2017-consolidated, wadden-etal-2019-entity}, relations , as well as their internal structures and processes .  inasmuch as they necessarily provide actionable knowledge to support question answering , narrative prediction , timeline construction  and summarization .      \muhao{TODO: forming what we call a ``event complex''} Human languages always involve the description of real-world events. Therefore, understanding events plays a critical role in natural language understanding , and supports tasks such as question answering , narrative prediction , timeline construction  and summarization . Typically, events are not just standalone predicate mentions, but rather as structures over multiple such predicates. Consider the example in Figure.  The description to the impact of the storm  also involves mentions about killed people , canceled flights  and affected passengers . Some of mentions thereof also follow temporal order. To support the comprehension of complex events, it is important to recognize the multifaceted relations for the predicate mentions in the text. \fi        % second paragraph \iffalse Recently, much research effort has been put into extracting specific aspects of relations for events. \citet{ning-etal-2018-improving} studied event temporal relation  extraction with a statistical common sense resource \citet{ning-etal-2019-improved} and \citet{han-etal-2019-joint} adopted data-driven methods for TempRel extraction; parent-child relations among events are studied in \citealp[]{liu-etal-2018-graph} and \citealp[]{aldawsari-finlayson-2019-detecting}. Though some of the previous work has ensured consistency via adding constraints in the inference phase, essentially they are not improving local predictions and the inconsistent results from the models might not be corrected in the inference stage. Besides, most of the approaches suffered from limited learning resources and the tasks are studied separately. \fi  Recently, significant %much research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation  extraction  and subevent relation extraction . Addressing such challenging tasks requires a model to recognize the inherent connection between event  %\dr{should it be predicate mentions, to ease the ambiguity?}  mentions as well as their contexts in the documents. Accordingly, a few previous methods apply statistical learning methods to characterize the grounded events in the documents . Such methods often require designing various features to characterize the structural, discourse and narrative aspects of the events, which are costly to produce and are often specific to a certain task or dataset. More recent works attempted to use data-driven methods based on neural relation extraction models  which refrain from feature engineering and offer competent performances.     \iffalse \dr{The next two paragraphs can be shortened, but they are the right paragrpahs to include here.} While data-driven methods provide a general and tractable way to capture specific event-event relations, it still remains challenging for those methods to precisely infer the correct relations. One challenge is that almost every task for event-event relation extraction comes with limited available annotated resources. Specifically, most tasks annotate no more than a hundred articles . Even the largest one in the literature, i.e., MATRES  for TempRel extraction, contains annotation for merely 275 articles. The lack of supervision hinders feature learning of events as well as inference of the relations, %Therefore, effectively tackling these tasks inevitably calls  therefore calling upon plausible auxiliary supervision from resources that are external to each of the tasks.    On the other hand, the event-event relations are often constrained by  %\drc{logical \dr{}change everywhere} %logic %\muhao{done.} properties, such as transitivity of TempRels Before and After , as well as that of %the relation between parent and child events subevent relations . In favor of such constraints, literature has employed global inference in the inference phase to comply with the logical properties particularly for TempRels . However, there lacks an effective way to ensure the global logical consistency in the training phase, which is key to making a data-driven machine learning model consistent on the beliefs of training data for various relation types . Moreover, the logical constraints may apply to different categories of %event-event  relations, and form complex conjunctive rules.  Consider the example in Figure : given that e2:died is Before e3:canceled and e3:canceled is a parent event of e4:affecting, the learning process should enforce e2:died Before e4:affecting. %\todo{Add an example of a conjunctive rule containing temporal and subevent relations.} Accordingly, ensuring the logical constraints across task-specific relations is another challenge being overlooked by the literature, the resolve of which provides a natural way to bridge the learning processes on multiple tasks. %\magenta{HW:TCR?} \fi  While data-driven methods provide a general and tractable way for event-event relation extraction, their performance is restricted by the limited annotated resources available. For example, the largest temporal relation extraction dataset MATRES only has 275 articles, which is far from enough for training a well-performing supervised model. The observation that relations and, in particular, event-event relations should be constrained by their logical properties , led to employing global inference to comply with transitivity and symmetry consistency, specifically on TempRel . However, in an event complex, the logical constraints may globally apply to different task-specific relations, and form more complex conjunctive constraints.  Consider the example in Figure : given that e2:died is Before e3:canceled and e3:canceled is a Parent event of e4:affecting, the learning process should enforce e2:died Before e4:affecting by considering the conjunctive constraints on both TempRel and subevent relations. While previous works focus on preserving logical consistency through  inference or structured learning , there was no %lacks an  effective way to endow neural models with the sense of global logical consistency during training.  %\dr{Notice that the previous statement was not correct; I change to limit it to neural models, since structure learning did it} %ensure the global logical consistency in the training phase.  This is key to bridging %bridge  the learning processes of %on both TempRel and subevent relations, which is a research focus of this paper.  %Event-relation extraction is a non-trivial task because of the following challenges: %1) Almost every event relation extraction task comes with limited learning resources with annotations. %2) Event relations are often volatile given different scenarios, and the determination of parent-child relation is especially difficult since there are less explicit lexical expressions compared with the cases for time and causation. %3) Event relations are often endowed with logical properties: % some temporal relations and parent-child relations comply with transitivity; % logical consistency should also be ensured across different categories of event relations.  The first contribution of this work is proposing %to propose  a joint constrained learning model for multifaceted event-event relation extraction.  The joint constrained learning framework seeks to regularize the model towards consistency with the logical constraints across both temporal and subevent relations, for which three types of consistency requirements are considered: annotation consistency, symmetry consistency and conjunction consistency. Such consistency requirements comprehensively define the interdependencies among those relations, essentially unifying the ordered nature of time and the topological nature of multi-granular subevents based on a set of declarative logic rules. Motivated by the logic-driven framework proposed by \citet{li-etal-2019-logic}, the declarative logical constraints are converted into differentiable functions that can be incorporated into the learning objective for relation extraction tasks.  Enforcing logical constraints across temporal and subevent relations is also a natural way to combine %two event-event relation extraction tasks with a shared learning objective. the supervision signals coming from two different datasets, one for each of the  relation extraction tasks with a shared learning objective. %\dr{You said what is the first contribution, but not the second; do you want now to claim this as the second contribution? Note that I modified to emphasize the two datasets} %Besides, the consistency of the final prediction is further enforced by global inference via an ILP solver.  Despite the scarce annotation for both tasks, the proposed method surpasses the SOTA TempRel extraction method on MATRES by relatively 3.27\% in ; %\dr{I don't understand -- is it relative or F1? Also, Tab. 2 shows 2.5\%}  it also offers promising performance on the HiEve dataset for subevent relation extraction, relatively surpassing previous methods by at least 3.12\% in .  %\dr{which table is this from?} %by 3.12\% and 21.4\%. %We further provide ablation studies to show the importance of each component of our framework. %This fact is further illustrated by ablation studies.   From the NLU perspective, %the acquired knowledge of our method is able to simultaneously models the internal membership structure of a complex event, as well as the temporal relations among both simple and complex events. the second contribution of this work lies in providing a general method for inducing an event complex that comprehensively represents the relational structure of several related event %\drc{predicate} % mentions. %in two directions.  This is supported by the memberships vertically identified between multi-granular events, as well as the horizontal temporal reasoning within the event complex. As far as we know, this is %essentially different from all %many  previous works that only formulated relations along a single axis. Our model further demonstrates the potent capability of inducing event complexes  %with promising performance  when evaluated  %based  on the RED dataset .          Various approaches have been proposed to extract event TempRels.  Early attempts to temporal relation  extraction include \citet{mani-etal-2006-machine, verhagen-pustejovsky-2008-temporal, mirza-tonelli-2014-classifying} which utilized machine learning methods  and hand-crafted features for each pair of events.  Early effort focused on characterizing event pairs based on various types of semantic and linguistic features, and utilizing statistical learning methods, such as logistic regression   and SVM , to capture the relations.  Whereas \citet{girju-2003-automatic, chang2004causal, blanco2008} explored causal relation extraction by discovering lexico-syntactic patterns and employing various classification models. Those methods typically require extensive feature engineering, and do not comprehensively consider the contextual information and global constraints among event-event relations.  Recently, data-driven methods have been developed for TempRel extraction, and have offered promising performance.  \citet{ning-etal-2019-improved} addressed this problem using a system combining an LSTM document encoder and a Siamese multi-layer perceptron  encoder for temporal commonsense knowledge from  . \citet{han-etal-2019-deep} proposed a bidirectional LSTM  with structured prediction to extract TempRels. Both of these works incorporated global inference to facilitate constraints on TempRels.     ??? Therefore, with contextual representations learned through neural models, we develop an integrated joint learning process instead of employing hand-crafted features.      \paragraph{Event Hierarchy Construction} Besides TempRels, a couple of efforts have focused on event hierarchy construction, a.k.a. subevent relation extraction. This task seeks to extract the hierarchy where each parent event contains child events that are described in the same document. To cope with this task, both \citet{araki-etal-2014-detecting} and \citet{glavas-snajder-2014-constructing} introduced a variety of features and employed logistic regression models for classifying event pairs into subevent relations , and no relation .   followed their work by creating a more comprehensive set of features for events including discourse and narrative features. \citet{aldawsari-finlayson-2019-detecting} further extended the characterization with more features on the discourse and narrative aspects. \citet{ZNKR20} presented a data-driven method by fine-tuning a time duration-aware BERT  on corpora of time mentions, and used the estimation of time duration to predict subevent relations.  \citet{liu-etal-2018-graph} seek to address the problem of event sequencing  \footnote{Event sequencing involves clustering events into scripts, ordering events in the same scripts, and identifying subevent links between event mentions.}   with a graph-based decoding algorithm.  Another line of work  focuses on the extraction of subevents by noticing that subevents are commonly mentioned in sentences that refer to the source of information or simply in quotation sentences.  Though it is feasible to find subevents in this way, it provides few clue for hierarchical relations \red{since these patterns cannot help find the parent of extracted subevents in the aforementioned kind of sentences} \muhao{not sure what this means}.   Though the method by \citet{aldawsari-finlayson-2019-detecting} has achieved decent results on two benchmark datasets,   using large amount of features, they did not provide detailed analysis unveiling the importance of features they used. Furthermore,   the linguistic and knowledge-based features used in the model may fail to capture long-term cues and contextualized information of an event.  in different contexts.       Though previous efforts have been devoted to preserving logical consistency through inference or structured learning, this is difficult to do in the context of neural networks. Moreover, while it is a common strategy to combine multiple training data in multi-task learning , our work is distinguished by enhancing the learning process by pushing the model towards a coherent output that satisfies logical constraints across separate tasks.     \iffalse  Note that limited efforts have been devoted to enforce the logical constraints in the training phase of event-event relation extraction models. Besides, few have investigated the feasibility of combining and mutually enhancing the constrained learning process of multiple relation extraction tasks.  These are exactly the focuses of this work.  \fi   We also incorporate common sense knowledge from TemProb and multiple constraints imposed by the nature of relations.  \dr{As I said in the introduction, this is not accurate; we need to say that structured learning has done it, but this was difficult to do in the context of NNs. In terms of combining training over multiple dataset, maybe it is worthwhile to mention mutlitask training, but highlight that our work is distinguished by enhancing the learning process by pushing the model toward a coherent output the satisfies logical constraints across the tasks.}   In this paper we explore the problem of seed-guided topical taxonomy construction. Our proposed framework \corel completes the taxonomy structure by a relation transferring module and enriches the semantics of concept nodes by a concept learning module. The relation transferring module learns the user-interested relation preserved in seed parent-child pairs, then transfers it along multiple paths to expand the taxonomy in width and depth. The concept learning module finds discriminative topical clusters for each concept in the process of jointly embedding concepts and words. Extensive experiments show that both modules work effectively in generating a high-quality topical taxonomy based on user-given seeds.  For future work, it is interesting to study how we can generate multi-faceted taxonomy automatically, so that each concept node is described by terms from different aspects . Though these terms can be captured by our concept learning module, how to recognize them and organize them into meaningful clusters remains challenging and worth exploring.\clearpage  \onecolumn 
","    Various approaches have been proposed to extract event TempRels.  Early attempts to temporal relation  extraction include \citet{mani-etal-2006-machine, verhagen-pustejovsky-2008-temporal, mirza-tonelli-2014-classifying} which utilized machine learning methods  and hand-crafted features for each pair of events.  Early effort focused on characterizing event pairs based on various types of semantic and linguistic features, and utilizing statistical learning methods, such as logistic regression   and SVM , to capture the relations.  Whereas \citet{girju-2003-automatic, chang2004causal, blanco2008} explored causal relation extraction by discovering lexico-syntactic patterns and employing various classification models. Those methods typically require extensive feature engineering, and do not comprehensively consider the contextual information and global constraints among event-event relations.  Recently, data-driven methods have been developed for TempRel extraction, and have offered promising performance.  \citet{ning-etal-2019-improved} addressed this problem using a system combining an LSTM document encoder and a Siamese multi-layer perceptron  encoder for temporal commonsense knowledge from  . \citet{han-etal-2019-deep} proposed a bidirectional LSTM  with structured prediction to extract TempRels. Both of these works incorporated global inference to facilitate constraints on TempRels.     ??? Therefore, with contextual representations learned through neural models, we develop an integrated joint learning process instead of employing hand-crafted features.      \paragraph{Event Hierarchy Construction} Besides TempRels, a couple of efforts have focused on event hierarchy construction, a.k.a. subevent relation extraction. This task seeks to extract the hierarchy where each parent event contains child events that are described in the same document. To cope with this task, both \citet{araki-etal-2014-detecting} and \citet{glavas-snajder-2014-constructing} introduced a variety of features and employed logistic regression models for classifying event pairs into subevent relations , and no relation .   followed their work by creating a more comprehensive set of features for events including discourse and narrative features. \citet{aldawsari-finlayson-2019-detecting} further extended the characterization with more features on the discourse and narrative aspects. \citet{ZNKR20} presented a data-driven method by fine-tuning a time duration-aware BERT  on corpora of time mentions, and used the estimation of time duration to predict subevent relations.  \citet{liu-etal-2018-graph} seek to address the problem of event sequencing  \footnote{Event sequencing involves clustering events into scripts, ordering events in the same scripts, and identifying subevent links between event mentions.}   with a graph-based decoding algorithm.  Another line of work  focuses on the extraction of subevents by noticing that subevents are commonly mentioned in sentences that refer to the source of information or simply in quotation sentences.  Though it is feasible to find subevents in this way, it provides few clue for hierarchical relations \red{since these patterns cannot help find the parent of extracted subevents in the aforementioned kind of sentences} \muhao{not sure what this means}.   Though the method by \citet{aldawsari-finlayson-2019-detecting} has achieved decent results on two benchmark datasets,   using large amount of features, they did not provide detailed analysis unveiling the importance of features they used. Furthermore,   the linguistic and knowledge-based features used in the model may fail to capture long-term cues and contextualized information of an event.  in different contexts.       Though previous efforts have been devoted to preserving logical consistency through inference or structured learning, this is difficult to do in the context of neural networks. Moreover, while it is a common strategy to combine multiple training data in multi-task learning , our work is distinguished by enhancing the learning process by pushing the model towards a coherent output that satisfies logical constraints across separate tasks.     \iffalse  Note that limited efforts have been devoted to enforce the logical constraints in the training phase of event-event relation extraction models. Besides, few have investigated the feasibility of combining and mutually enhancing the constrained learning process of multiple relation extraction tasks.  These are exactly the focuses of this work.  \fi   We also incorporate common sense knowledge from TemProb and multiple constraints imposed by the nature of relations.  \dr{As I said in the introduction, this is not accurate; we need to say that structured learning has done it, but this was difficult to do in the context of NNs. In terms of combining training over multiple dataset, maybe it is worthwhile to mention mutlitask training, but highlight that our work is distinguished by enhancing the learning process by pushing the model toward a coherent output the satisfies logical constraints across the tasks.}",92
"  Word embeddings which can capture semantic similarities have been extensively explored in a wide spectrum of Natural Language Processing  applications in recent years.  Word2Vec , FastText , and Glove  are some examples. Even though distributional word embeddings produce high quality representations, representing longer pieces of text such as sentences and paragraphs is still an open research problem. A sentence embedding is a contextual representation of a sentence which is often created by transformation of word embeddings through a composition function. There has been a large body of work in the literature which propose different approaches to represent sentences from word embeddings. SkipThought , InferSent , and Universal Sentence Encoder  are well-known examples.  % Other proposed methods for learning sentence representations include, but are not limited to .  There has been a growing interest in understanding what linguistic knowledge is encoded in deep contextual representation of language. For this purpose, several probing tasks are proposed to understand what these representations are capturing . One of the interesting findings is that despite the existence of explicit syntactic annotations, these learned deep representations encode syntax to some extent . Hewitt et. al. provide an evidence that the entire syntax tree is embedded implicitly in deep model's vector geometry. Kuncoro et. al.  show that LSTMs trained on language modeling objectives capture syntax-sensitive dependencies. Even though deep contextual language models implicitly capture syntactic information of sentences, explicit modeling of syntactic structure of sentences has been shown to further improve the results in different NLP tasks including neural language modeling \cite {shen2017neural, havrylov2019cooperative}, machine comprehension , summarization , text generation , machine translation , authorship attribution , etc. Furthermore, Kuncoro et. al. provide evidence that models which have explicit syntactic information result in better performance . Of particular interest, one of the areas where syntactic structure of sentences plays an important role is style-based text classification tasks, including authorship attribution. The syntactic structure of sentences captures the syntactic patterns of sentences adopted by a specific author and reveal how the author structures the sentences in a document.   Inspired by the above observations, our initial work demonstrates that explicit syntactic information of sentences improves the performance of a recurrent neural network classifier in the domain of authorship attribution . We continue this work in this paper by investigating if structural representation of sentences can be learned explicitly. In other words, similar to pre-trained word embeddings which mainly capture semantics, can we have pre-trained embeddings which mainly capture syntactic information of words. Such pre-trained word embeddings can be used in conjunction with semantics embeddings in different domains including authorship attribution. For this purpose, we propose a self-supervised framework using a Siamese  network  to explicitly learn the structural representation of sentences. The Siamese network is comprised of two identical components; a lexical sub-network and a syntactic sub-network; which take the sequence of words in the sentence and its corresponding linearized syntax parse tree as the inputs, respectively. This model is trained based on a contrastive loss objective where each pair of vectors  is close to each other in the embedding space if they belong to an identical sentence , and are far from each other if they belong to two different sentences .    As a result, each word in the sentence is embedded into a vector representation which mainly carries structural information. Due to the -to- mapping of word types to structural labels, the word representation is deduced into structural representations. In other words, semantically different words  are mapped to similar structural labels ; hence, semantically different words may have similar structural representations. These pre-trained structural word representations can be used as complimentary information to their pre-trained semantic embeddings . We use probing tasks proposed by Conneau et al.  to investigate the linguistic features learned by such a training.  The results indicate that structural embeddings show competitive results compared to the semantic embeddings, and concatenation of structural embeddings with semantic embeddings achieves further improvement.  Finally, we investigate the efficiency of the learned structural embeddings of words for the domain of authorship attribution across four datasets. Our experimental results demonstrate classification improvements when structural embeddings are concatenated with the pre-trained word embeddings.  The remainder of this paper is organized as follows: we elaborate our proposed self-supervised framework in Section .  The details of the datasets and experimental configuration are provided and the experimental results reported in Section ; We review the related work in Section . Finally, we conclude this paper in Section .        Style-based text classification is dual to topic-based text classification  since the features which capture the style of a document are mainly independent of its topic . Writing style is a combination of consistent decisions at different levels of language production including lexical, syntactic, and structural associated to a specific author  . Style-based text classification was introduced by Argamon-Engelson et al. . The authors used basic stylistic features  to classify news documents based on the corresponding publisher  as well as text genre .  Nowadays, computational stylometry has a wide range of applications in literary science , forensics , and psycholinguistics .         Syntactic n-grams are shown to achieve promising results in different stylometric tasks including author profiling  and author verification . In particular, Raghahvan et al. investigated the use of syntactic information by proposing a probabilistic context-free grammar for the authorship attribution purpose, and used it as a language model for classification . A combination of lexical and syntactic features has also been shown to enhance the model performance. Sundararajan et al. argue that, although syntax can be helpful for cross-genre authorship attribution, combining syntax and lexical information can further boost the performance for cross-topic attribution and single-domain attribution . Further studies which combine lexical and syntactic features include .       With recent advances in deep learning, there exists a large body of work in the literature which employs deep neural networks in the domain of authorship attribution. For instance,   ---RNN--- Ge et al. used a feed forward neural network language model on an authorship attribution task. The output achieves promising results compared to the n-gram baseline .  Bagnall et al. have employed a recurrent neural network with a shared recurrent state which outperforms other proposed methods in PAN 2015 task .  Shrestha et al. applied CNN based on character n-gram to identify the authors of tweets. Given that each tweet is short in nature, their approach shows that a sequence of character n-grams as the result of CNN allows the architecture to capture the character-level interactions, which can then be aggregated to learn higher-level patterns for modeling the style .  Sari et al. have proposed to use continuous representations for authorship attribution. Unlike the previous work which uses discrete representations, they represent each n-gram as a continuous vector and learn these representations in the context of the authorship attribution tasks .  Hitchler et al. propose a CNN based on pre-trained embedding word vector concatenated with one-hot encoding of POS tags; however, they have not shown any ablation study to report the contribution of POS tags on the final performance results .  Zhang et.al introduces a syntax encoding approach using convolutional neural networks which combines with a lexical models, and applies it to the domain of authorship attribution . We propose a simpler yet more effective way of encoding syntactic information of documents for the domain of authorship attribution . Moreover, we employ a hierarchical neural network to capture the structural information of documents and finally introduce a neural model which incorporates all three stylistic features including lexical, syntactic and structural .           Event-event relation extraction is a challenging task which is beneficial to understanding event complex composed of multi-granular events with temporal orders. Despite the existence of previous attempts for addressing TempRel and subevent relation extraction, this is the first work  We propose a joint constrained learning framework for extracting event complexes from documents.  that combines the two tasks and addresses them by constrained learning with shared objectives.  The proposed framework bridges TempRel and subevent relation extraction tasks with a comprehensive set of logical constraints, which are enforced during learning by  converting them into differentiable objective functions.  On two benchmark datasets, the proposed method outperforms SOTA statistical learning methods and data-driven methods for each task, without using data that is jointly annotated with the two classes of relations. It also presents promising event complex extraction results on RED that is external to training.  Thus, our work shows that the global consistency of the event complex significantly helps understanding both temporal order and event membership. For future work, we plan to extend the framework towards an end-to-end system with event extraction. We also seek to extend the conjunctive constraints along with event argument relations.  , demonstating the effectiveness of joint constrained learning framework from both machine-learning and NLU view .    
","  Style-based text classification is dual to topic-based text classification  since the features which capture the style of a document are mainly independent of its topic . Writing style is a combination of consistent decisions at different levels of language production including lexical, syntactic, and structural associated to a specific author  . Style-based text classification was introduced by Argamon-Engelson et al. . The authors used basic stylistic features  to classify news documents based on the corresponding publisher  as well as text genre .  Nowadays, computational stylometry has a wide range of applications in literary science , forensics , and psycholinguistics .         Syntactic n-grams are shown to achieve promising results in different stylometric tasks including author profiling  and author verification . In particular, Raghahvan et al. investigated the use of syntactic information by proposing a probabilistic context-free grammar for the authorship attribution purpose, and used it as a language model for classification . A combination of lexical and syntactic features has also been shown to enhance the model performance. Sundararajan et al. argue that, although syntax can be helpful for cross-genre authorship attribution, combining syntax and lexical information can further boost the performance for cross-topic attribution and single-domain attribution . Further studies which combine lexical and syntactic features include .       With recent advances in deep learning, there exists a large body of work in the literature which employs deep neural networks in the domain of authorship attribution. For instance,   ---RNN--- Ge et al. used a feed forward neural network language model on an authorship attribution task. The output achieves promising results compared to the n-gram baseline .  Bagnall et al. have employed a recurrent neural network with a shared recurrent state which outperforms other proposed methods in PAN 2015 task .  Shrestha et al. applied CNN based on character n-gram to identify the authors of tweets. Given that each tweet is short in nature, their approach shows that a sequence of character n-grams as the result of CNN allows the architecture to capture the character-level interactions, which can then be aggregated to learn higher-level patterns for modeling the style .  Sari et al. have proposed to use continuous representations for authorship attribution. Unlike the previous work which uses discrete representations, they represent each n-gram as a continuous vector and learn these representations in the context of the authorship attribution tasks .  Hitchler et al. propose a CNN based on pre-trained embedding word vector concatenated with one-hot encoding of POS tags; however, they have not shown any ablation study to report the contribution of POS tags on the final performance results .  Zhang et.al introduces a syntax encoding approach using convolutional neural networks which combines with a lexical models, and applies it to the domain of authorship attribution . We propose a simpler yet more effective way of encoding syntactic information of documents for the domain of authorship attribution . Moreover, we employ a hierarchical neural network to capture the structural information of documents and finally introduce a neural model which incorporates all three stylistic features including lexical, syntactic and structural .",93
"   In recent years, neural networks have shown impressive performance gains on long-standing AI problems, such as natural language understanding, speech recognition, and computer vision.  Based on these successes, researchers have considered the application of neural nets to data management problems, including learning indices, query optimization and entity matching.  In applying neural nets to data management, research has so far assumed that the data was modeled by a database schema.    The success of neural networks in processing unstructured data such as natural language and images   raises the question of whether their use can be extended to a point where we can relax the fundamental assumption of database management, which is that the data we process is represented as fields of a pre-defined schema.  What if, instead, data and queries can be represented as short natural language sentences, and queries can be answered from these sentences?  This paper presents a first step in answering that question.  We describe \systemname, a database system in which updates and queries are given in natural language. The query processor of a \ndb\ builds on the primitives that are offered by the state of the art Natural Language Processing~ techniques.  Figure shows example facts and queries that \ndb\ can answer. %\ms{In Figure 1, queries 4&5 are not really joins, they just need language understanding/paraphrasing}  Realizing the vision of \systemname\ will offer several benefits that database systems have struggled to support for decades.  The first, and most important benefit is that a \ndb, by definition, has no pre-defined schema. Therefore, the scope of the database does not need to be defined in advance and any data that becomes relevant as the application is used can be stored and queried. The second benefit is that updates and queries can be posed in a variety of natural language forms, as is convenient to any user.  In contrast, a traditional database query needs to be based on the database schema.  A third benefit comes from the fact that the \ndb\  is based on a pre-trained language model that already contains a lot of knowledge.   For example, the fact that London is in the UK is already encoded in the language model. Hence, a query asking who lives in the UK can retrieve people who are known to live in London without having to explicitly specify an additional join. Furthermore, using the same paradigm, we can endow the \ndb\  with more domain knowledge by extending the pre-training corpus to that domain.   By nature, a \ndb\ is not meant to provide the same correctness guarantees of a traditional database system, i.e., that the answers returned for a query satisfy the precise binary semantics of the query language.  Hence, \ndb s should not be considered as an alternative to traditional databases in applications where such guarantees are required.    Given its benefits, \neuraldatabases\ are well suited for emerging applications where the schema of the data cannot be determined in advance and data can be stated in a wide range of linguistic patterns.  A family of such applications arise in the area of storing knowledge for personal assistants that currently available for home use and in the future will accompany Augmented Reality glasses. In these applications, users store data about their habits and experiences, their friends and their preferences, and designing a schema for such an application is impractical.  Another class of applications is the modeling and querying of political claims .  Here too, claims can be about a huge variety of topics and expressed in many ways.   Our first contribution is to show that state of the art transformer models can be adapted to answer simple natural language queries. Specifically, the models can process facts that are relevant to a query independent of their specific linguistic form, and combine multiple facts to yield correct answers, effectively performing a join. However, we identify two major limitations of these models:  they do not perform well on aggregation queries , and  since the input size to the transformer is bounded and the complexity of the transformer is quadratic in the size of its input, they only work on a relatively small collection of facts.  Our second contribution is to  propose an architecture for neural databases that uses the power of transformers at its core, but puts in place several other components in order to address the scalability and aggregation issues. Our architecture runs multiple instances of a Neural SPJ operator in parallel. The results of the operator are either the answer to the query or the input to an aggregation operator, which is done in a traditional fashion. Underlying this architecture is a novel algorithm for generating the small sets of database sentences that are fed to each Neural SPJ operator.  Finally, we describe an experimental study that validates the different components of \systemname s, namely the ability of the Neural SPJ to answer queries or create results for a subsequent aggregation operator even with minimal supervision, and our ability to produce support sets that are fed into each of the Neural SPJ operators. Putting all the components together, our   final result shows that we can accurately answer queries over thousands of sentences with very high accuracy. To run the experiments we had to create an experimental dataset with training data for \ndb s, which we make available for future research.    % and capable of generating intermediate results and  accurately predicting the aggregation operation to execute over these intermediate results.      \subsubsection*{NLP and data management} Bridging the gap between unstructured natural language data and database-style querying has been a long-standing theme in database research.  The work on information extraction has developed techniques for translating segments of natural language text into triples that can be further processed by a database system.  Wikidata itself is a social experiment where additions to the knowledge graph are encouraged to use already existing relation names if possible, thereby alleviating the need for information extraction.  There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known, with extensions to semi-structured data and knowledge bases. More recently, systems such as {BREAK} and {ShARC} have trained models to translate a natural language query into a sequence of relational operators .     \ndb s do not try to map data or queries into a pre-defined schema. At the core, we use neural techniques to process the facts in the database with the query given as context in natural language. However, \ndb s do some rudimentary analysis of the query when they decide whether it requires an aggregation operator, and one can imagine that \ndb s will need more sophisticated understanding of the structure of a query as they tackle more complex queries. Similarly, the processing performed by the Neural SPJ operator is reminiscent of information extraction in the sense that it produces a structured representation of facts that can be used by subsequent operators. However, a key difference is that the extraction performed by the Neural SPJ is query dependent and is independent of any schema.   With similar goals in mind, the Information Retrieval community has developed search engines to answer SQL queries. The work most close to ours, explores the problem of answering queries from a collection of non-schematic XML documents that exhibit  heterogeneous structures, and hence are cumbersome in languages such as XPath or XQuery. Another similar research line is that of Whang et al.. Similarly to what we propose they also support natural language queries but they still exploit semi-structured data.   to answer queries such as: ``Finds papers about 閳ユ竷loud computing閳 published after 閳2005閳 in digital libraries''.  Whereas in our case,  to solve this query  the system needs to ``understand'' what are the relations and attributes that need to be used and the relative operators  .and at the same time IR techniques are used to retrieve the initial set of potentially relevant information.  \subsubsection*{Question answering from text} The NLP community has made great strides recently on the problem of answering queries from text, which includes tasks such as open-book question answering and fact verification. To efficiently scale machine comprehension to very large databases, the NLP community adopt either a pipelined or jointly trained architecture of information retrieval with neural reasoning. Like \ndb s, many of these works answer questions using an explicit memory of knowledge  in addition to the pre-trained language model.  However, these works typically require extracting a span from a single document or predicting a token or label as an answer, whereas \ndb s require combining multiple facts, performing selections and aggregation.  While in-roads have been made to perform discrete reasoning over passages , with explicit computation , these use only a single passage rather than requiring aggregation over large numbers of facts.  Multi-hop question answering is a recent setting where answering a query requires finding supporting evidence in multiple documents . In solving multi-hop questions, the works either decompose the question into simpler sub questions, or condition each hop on the previously retrieved documents. Some of these ideas inspired the design of the SSG in Section.  Transformers have been shown to perform soft-reasoning when provided with simple propositional rules. In that work, transformers were able to join a small number of facts and rules of the form .     The Squad dataset has been very popular in recent years for developing models that are capable of finding the answer in a passage. Note that this is a much harder task in comparison with the multiple choice QA as the number of choices for spans in the text is quadratic in the number of words present in the passage.     In an abstractive or generative question answering task, the model reads one or more passages and answer the question by generating one word at a time      \subsubsection{Answer Type}   Multiple-choice question answering  is a type of question answering task where the answer can be selected from a list of possible answers. MCTest is a well-known dataset that is based on multiple choice QA, where each accompanying passage is a short story.    Another very common type of QA is extractive question answering. When an extractive QA system is presented a question and a passage, it is tasked with returning a segment from the passage which answers the question. {\sf Passage: The Rankine cycle is sometimes referred to as a practical Carnot cycle. Q: What is the Rankine cycle sometimes called? A: practical Carnot cycle}.      In our setting, we have a combination of answer types, multiple choice , extractive  {\sf Fact: John works at Cusac Gold Mines Ltd. Q: For which company does John work? A: Cusac Gold Mines Ltd} and abstractive such as counting questions {\sf How many people in the database live in Europe? A: 20}.   While other works modeling the web as a knowledge bases have focused on combining multiple snippets of text together , their assumption is that the query is decomposed into a SPARQL program that is executed on pre-extracted information. Our innovation is that no latent program or structure is needed and that information extraction is dynamic and dependent on the query.                                        IR + DB perspective                           \subsubsection*{Extending neural architectures to reasoning tasks}  In the same spirit to Neural Turing Machines and Memory Networks architectures, an alternative way of building \ndb\ is to encode all the facts in the database to a neural memory and build machinery to read, write, and reason on top of this neural memory. However, such an approach would not have control and transparency: It is challenging to remove facts from the database or check whether a particular fact exists.  Also, it would not be possible to explain query results. Furthermore, these architectures perform well on bAbI  tasks where the number of facts is limited, and mainly lookup or simple reasoning is needed. But, In our experiments in \ndb\, they couldn't perform well; we hypothesize that encoding the query and facts together by a stack of self-attention in the encoder is necessary to answer database queries.  There also have been considerable efforts in mixing traditional symbolic reasoning or data management algorithms with neural network architectures. For example, Rockt\""{a}schel et al.  have developed a differentiable version of the backward chaining algorithm that drives prolog. Most closely to our work, Minervini et al. has showed how differentiable prolog interpreters can be used to support reasoning with facts in natural language. Instead of ``neuralizing'' existing symbolic reasoners, in our work we start off with a scalable neural architecture, and support it with symbolic computation only where necessary. This enables us to directly leverage the rapid progress made in retrieval augmented QA models and ensures scalability.         In this work, we have tackled the Tunisian Romanized alphabet sentiment analysis task. We have experimented two different word-level representations  and two deep neural networks , without the use of any pre-processing step. Results showed that CNN trained with M-BERT achieved the best results compared to the word2vec, frWac and Bi-LSTM. This model could improve the performance over the baselines. Experiments and promising results achieved on the TUNIZI and TSAC-TUNIZI datasets helped us to better understand the nature of the Tunisian dialect and its specificities. This will help the Tunisian NLP community in further research activities not limited to the sentiment analysis task, but also in more complex NLP tasks.  A natural future step would involve releasing TunaBERT, a Tunisian version of the Bi-directional Encoders for Transformers  that should be learned on a very large and heterogeneous Tunisia dataset. The Tunisian language model can be applied to complex NLP tasks . To demonstrate the value of building a dedicated version of BERT for Tunisian, we also plan to compare TunaBERT to the multilingual cased version of BERT.   
","  \subsubsection*{NLP and data management} Bridging the gap between unstructured natural language data and database-style querying has been a long-standing theme in database research.  The work on information extraction has developed techniques for translating segments of natural language text into triples that can be further processed by a database system.  Wikidata itself is a social experiment where additions to the knowledge graph are encouraged to use already existing relation names if possible, thereby alleviating the need for information extraction.  There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known, with extensions to semi-structured data and knowledge bases. More recently, systems such as {BREAK} and {ShARC} have trained models to translate a natural language query into a sequence of relational operators .     \ndb s do not try to map data or queries into a pre-defined schema. At the core, we use neural techniques to process the facts in the database with the query given as context in natural language. However, \ndb s do some rudimentary analysis of the query when they decide whether it requires an aggregation operator, and one can imagine that \ndb s will need more sophisticated understanding of the structure of a query as they tackle more complex queries. Similarly, the processing performed by the Neural SPJ operator is reminiscent of information extraction in the sense that it produces a structured representation of facts that can be used by subsequent operators. However, a key difference is that the extraction performed by the Neural SPJ is query dependent and is independent of any schema.   With similar goals in mind, the Information Retrieval community has developed search engines to answer SQL queries. The work most close to ours, explores the problem of answering queries from a collection of non-schematic XML documents that exhibit  heterogeneous structures, and hence are cumbersome in languages such as XPath or XQuery. Another similar research line is that of Whang et al.. Similarly to what we propose they also support natural language queries but they still exploit semi-structured data.   to answer queries such as: ``Finds papers about 闁炽儲绔穕oud computing闁 published after 闁2005闁 in digital libraries''.  Whereas in our case,  to solve this query  the system needs to ``understand'' what are the relations and attributes that need to be used and the relative operators  .and at the same time IR techniques are used to retrieve the initial set of potentially relevant information.  \subsubsection*{Question answering from text} The NLP community has made great strides recently on the problem of answering queries from text, which includes tasks such as open-book question answering and fact verification. To efficiently scale machine comprehension to very large databases, the NLP community adopt either a pipelined or jointly trained architecture of information retrieval with neural reasoning. Like \ndb s, many of these works answer questions using an explicit memory of knowledge  in addition to the pre-trained language model.  However, these works typically require extracting a span from a single document or predicting a token or label as an answer, whereas \ndb s require combining multiple facts, performing selections and aggregation.  While in-roads have been made to perform discrete reasoning over passages , with explicit computation , these use only a single passage rather than requiring aggregation over large numbers of facts.  Multi-hop question answering is a recent setting where answering a query requires finding supporting evidence in multiple documents . In solving multi-hop questions, the works either decompose the question into simpler sub questions, or condition each hop on the previously retrieved documents. Some of these ideas inspired the design of the SSG in Section.  Transformers have been shown to perform soft-reasoning when provided with simple propositional rules. In that work, transformers were able to join a small number of facts and rules of the form .     The Squad dataset has been very popular in recent years for developing models that are capable of finding the answer in a passage. Note that this is a much harder task in comparison with the multiple choice QA as the number of choices for spans in the text is quadratic in the number of words present in the passage.     In an abstractive or generative question answering task, the model reads one or more passages and answer the question by generating one word at a time      \subsubsection{Answer Type}   Multiple-choice question answering  is a type of question answering task where the answer can be selected from a list of possible answers. MCTest is a well-known dataset that is based on multiple choice QA, where each accompanying passage is a short story.    Another very common type of QA is extractive question answering. When an extractive QA system is presented a question and a passage, it is tasked with returning a segment from the passage which answers the question. {\sf Passage: The Rankine cycle is sometimes referred to as a practical Carnot cycle. Q: What is the Rankine cycle sometimes called? A: practical Carnot cycle}.      In our setting, we have a combination of answer types, multiple choice , extractive  {\sf Fact: John works at Cusac Gold Mines Ltd. Q: For which company does John work? A: Cusac Gold Mines Ltd} and abstractive such as counting questions {\sf How many people in the database live in Europe? A: 20}.   While other works modeling the web as a knowledge bases have focused on combining multiple snippets of text together , their assumption is that the query is decomposed into a SPARQL program that is executed on pre-extracted information. Our innovation is that no latent program or structure is needed and that information extraction is dynamic and dependent on the query.                                        IR + DB perspective                           \subsubsection*{Extending neural architectures to reasoning tasks}  In the same spirit to Neural Turing Machines and Memory Networks architectures, an alternative way of building \ndb\ is to encode all the facts in the database to a neural memory and build machinery to read, write, and reason on top of this neural memory. However, such an approach would not have control and transparency: It is challenging to remove facts from the database or check whether a particular fact exists.  Also, it would not be possible to explain query results. Furthermore, these architectures perform well on bAbI  tasks where the number of facts is limited, and mainly lookup or simple reasoning is needed. But, In our experiments in \ndb\, they couldn't perform well; we hypothesize that encoding the query and facts together by a stack of self-attention in the encoder is necessary to answer database queries.  There also have been considerable efforts in mixing traditional symbolic reasoning or data management algorithms with neural network architectures. For example, Rockt\""{a}schel et al.  have developed a differentiable version of the backward chaining algorithm that drives prolog. Most closely to our work, Minervini et al. has showed how differentiable prolog interpreters can be used to support reasoning with facts in natural language. Instead of ``neuralizing'' existing symbolic reasoners, in our work we start off with a scalable neural architecture, and support it with symbolic computation only where necessary. This enables us to directly leverage the rapid progress made in retrieval augmented QA models and ensures scalability.",94
"  The following instructions are directed to authors of papers submitted to EACL 2021 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.         \paragraph{Narrative generation tasks} The work on narrative generation is split between cloze tasks, open-ended generation, and guided generation. In a cloze task, a full story except for the final word or sentence is given, and the model completes it. This could be cast as a short generation problem---or, more commonly in this domain, a multiple-choice problem .          Open-ended generation is when the model generates a story conditioned on a prompt .     \citet{fan-etal-2018-hierarchical} create a paired prompt and response dataset from the subreddit r/WritingPrompts\footnote{} to train a sequence-to-sequence ``fusion model.""      \citet{see-etal-2019-massively} extends \citet{fan-etal-2018-hierarchical}, but uses GPT-2 small and performs a top- decoding parameter sweep.     We focus on this open-ended narrative generation task in our investigation, but primarily focus on GPT-2 Medium and on performing parameter sweeps over nucleus sampling and diverse decoding strengths.          Guided generation is the middle ground of cloze and open-ended generation. The model is provided more context, such as characters, plot information, and potentially other information, and then generates a story based on all of the provided structural and semantic information .                \\         \toprule          \\         \bottomrule                                          \paragraph{Decoding methods for generation} Decoding refers to the inference methods used in natural language generation; given input sequence , how should we construct the output sequence ? Since finding the exact most probable token at each time step often does not produce human-like or high-quality results , search and sampling are used to induce more stochasticity and thus generate more human-like language. One popular search method is beam search , where at each time step, the algorithm keeps track of the top  most probable partial hypotheses. When , this method reduces to the greedy decoder, which chooses the argmax over the model's token distribution at each time step.           An alternative to search are sampling-based approaches, which select a token with likelihood proportional to a  probability distribution at each time step. Such methods include top-  which restricts the sampling space to the top  most probable tokens at every time step, and ``nucleus sampling''\footnote{Also referred to as ``top-''.}  which thresholds the cumulative token probability distribution according to a hyperparameter . We focus on nucleus sampling, as it has tended to be a more effective decoding method in various response generation settings .          An approach to control sampling is temperature , which modifies the softmax estimating the token probability distribution. This has been applied widely in neural text generation , especially when using top- or random sampling. Low temperatures bias the model toward high-probability events, which tends to increase generation quality while decreasing token diversity . Temperature sampling has been investigated extensively in natural language generation over multiple sampling methods, and nucleus sampling has been found to be a more effective method of controlling the sampling distribution , so we do not investigate this here.      \paragraph{Decoding objective} In chatbot response generation, top- and nucleus sampling have been known to generate fluent, but uninteresting and simple high-probability responses which do not address the input . This issue is commonly referred to as the ``I don't know'' problem, where the response to all inputs is often the high-probability phrase ``I don't know.'' Research has been done to address response blandness, specifically by altering the decoding objective. Some recent work in this domain includes \citet{RyoNakamura18}, who use Inverse Token Frequency to reweight generated tokens. \citet{xu18} and \citet{zhang18} use adversarial loss to optimize for diversity, informativeness, and fluency.          Another approach explores variants of the standard log-likelihood loss, applying different objectives during inference. An example of this is maximum mutual information , an objective that promotes more diverse responses in the neural response generation task. This mitigates the ``I don't know'' problem in which all responses tend to converge to some high-probability sequence with no real content conveyed in response to the input sequence. Two versions are introduced in \citet{jiweili16}: bidirectional  and an anti-language model  objective. The typical decoding objective is defined as          where  is a target sequence ,  is the output narrative, and  is the input source sequence. We use a slightly modified form of the MMI-antiLM objective, defined as follows:          where  is a hyperparameter corresponding to the degree to which the language modeling objective should be subtracted from the sequential transduction objective.          This diverse decoding objective has been applied to response generation, but has not yet been applied to the narrative generation task. In this work, we evaluate the effect of the MMI-antiLM objective on narrative generation quality.      The past decade witnessed text generation dribbling from niche scenarios into several mainstream NLP applications. This urges the need for a snapshot to retrospect the progress of varied text generation tasks in unison. This paper is written with the goal of presenting a one-stop destination for task agnostic components and factors in text generation for researchers foraging to situate their work and guage their impact in this vast field. Moving forward, we envision that there are some of the crucial directions to focus for impactful innovation in text generation. These include  generation in real time  non-autoregressive decoding  consistency with situated contexts in real and virtual environments and games  consistency with personality with opinions especially for virtual agents  conditioning on multiple modalities together with text and data  investigation is still ongoing on finding better metrics to evaluate NLG with better correlated human judgements   creative text generation. We believe this is the right time to extend advancements in any particular task to other tightly coupled tasks to revamp improvements in text generation as a holistic task.  
","     \paragraph{Narrative generation tasks} The work on narrative generation is split between cloze tasks, open-ended generation, and guided generation. In a cloze task, a full story except for the final word or sentence is given, and the model completes it. This could be cast as a short generation problem---or, more commonly in this domain, a multiple-choice problem .          Open-ended generation is when the model generates a story conditioned on a prompt .     \citet{fan-etal-2018-hierarchical} create a paired prompt and response dataset from the subreddit r/WritingPrompts\footnote{} to train a sequence-to-sequence ``fusion model.""      \citet{see-etal-2019-massively} extends \citet{fan-etal-2018-hierarchical}, but uses GPT-2 small and performs a top- decoding parameter sweep.     We focus on this open-ended narrative generation task in our investigation, but primarily focus on GPT-2 Medium and on performing parameter sweeps over nucleus sampling and diverse decoding strengths.          Guided generation is the middle ground of cloze and open-ended generation. The model is provided more context, such as characters, plot information, and potentially other information, and then generates a story based on all of the provided structural and semantic information .                \\         \toprule          \\         \bottomrule                                          \paragraph{Decoding methods for generation} Decoding refers to the inference methods used in natural language generation; given input sequence , how should we construct the output sequence ? Since finding the exact most probable token at each time step often does not produce human-like or high-quality results , search and sampling are used to induce more stochasticity and thus generate more human-like language. One popular search method is beam search , where at each time step, the algorithm keeps track of the top  most probable partial hypotheses. When , this method reduces to the greedy decoder, which chooses the argmax over the model's token distribution at each time step.           An alternative to search are sampling-based approaches, which select a token with likelihood proportional to a  probability distribution at each time step. Such methods include top-  which restricts the sampling space to the top  most probable tokens at every time step, and ``nucleus sampling''\footnote{Also referred to as ``top-''.}  which thresholds the cumulative token probability distribution according to a hyperparameter . We focus on nucleus sampling, as it has tended to be a more effective decoding method in various response generation settings .          An approach to control sampling is temperature , which modifies the softmax estimating the token probability distribution. This has been applied widely in neural text generation , especially when using top- or random sampling. Low temperatures bias the model toward high-probability events, which tends to increase generation quality while decreasing token diversity . Temperature sampling has been investigated extensively in natural language generation over multiple sampling methods, and nucleus sampling has been found to be a more effective method of controlling the sampling distribution , so we do not investigate this here.      \paragraph{Decoding objective} In chatbot response generation, top- and nucleus sampling have been known to generate fluent, but uninteresting and simple high-probability responses which do not address the input . This issue is commonly referred to as the ``I don't know'' problem, where the response to all inputs is often the high-probability phrase ``I don't know.'' Research has been done to address response blandness, specifically by altering the decoding objective. Some recent work in this domain includes \citet{RyoNakamura18}, who use Inverse Token Frequency to reweight generated tokens. \citet{xu18} and \citet{zhang18} use adversarial loss to optimize for diversity, informativeness, and fluency.          Another approach explores variants of the standard log-likelihood loss, applying different objectives during inference. An example of this is maximum mutual information , an objective that promotes more diverse responses in the neural response generation task. This mitigates the ``I don't know'' problem in which all responses tend to converge to some high-probability sequence with no real content conveyed in response to the input sequence. Two versions are introduced in \citet{jiweili16}: bidirectional  and an anti-language model  objective. The typical decoding objective is defined as          where  is a target sequence ,  is the output narrative, and  is the input source sequence. We use a slightly modified form of the MMI-antiLM objective, defined as follows:          where  is a hyperparameter corresponding to the degree to which the language modeling objective should be subtracted from the sequential transduction objective.          This diverse decoding objective has been applied to response generation, but has not yet been applied to the narrative generation task. In this work, we evaluate the effect of the MMI-antiLM objective on narrative generation quality.",95
" Cross-lingual abstractive summarization is the task to generate a summary of a given document in a different target language. This task provides the overview of an article in a foreign language and thus helps readers understand a text written in an unfamiliar language quickly.   Early work on cross-lingual abstractive summarization adopted the pipeline approach: either translation of the given document into the target language followed by summarization of the translated document or summarization of the given document followed by translation of the summary into the target language. On the other hand, recent studies have applied a neural encoder-decoder model, which is widely used for natural language generation tasks including machine translation and monolingual abstractive summarization, to generate a summary in the target language from the given document directly. %Such direct generation approaches prevent the error propagation problems in pipeline methods. Such direct generation approaches prevent the error propagation in pipeline methods.  Training neural encoder-decoder models requires numerous sentence pairs. In fact,  provided 3.8M sentence-summary pairs to train their neural encoder-decoder model for English abstractive summarization, and the following studies used the same training data. However, constructing a large-scale cross-lingual abstractive summarization dataset is much more difficult than collecting monolingual summarization datasets because we require sentence-summary pairs in different languages. To address this issue, recent studies applied a machine translation model to monolingual sentence-summary pairs. They used the constructed pseudo dataset to train their neural encoder-decoder models.    Meanwhile, the possibility whether existing genuine parallel corpora such as translation pairs and monolingual abstractive summarization datasets can be utilized needs to be explored. In machine translation,  indicated that using translation pairs in multiple languages improved the performance of a neural machine translation model. Similarly, we consider that such existing genuine parallel corpora have a positive influence on the cross-lingual abstractive summarization task since the task is a combination of machine translation and summarization.   In this study, we propose a multi-task learning framework, Transum, which includes machine translation, monolingual abstractive summarization, and cross-lingual abstractive summarization, for neural encoder-decoder models. The proposed method controls the target task with a special token which is inspired by Google's multilingual neural machine translation system. For example, we attach the special token  to the beginning of the source-side input sentence in translation.   The proposed Transum is quite simple because it does not require any additional architecture in contrast to  but effective in cross-lingual abstractive summarization. Experimental results show that Transum improves the performance of cross-lingual abstractive summarization and outperforms previous methods in Chinese-English and Arabic-English summarization. In addition, Transum significantly improves machine translation performance compared to that obtained using only a genuine parallel corpus for machine translation.   Furthermore, we construct a new test set to simulate more realistic situations: cross-lingual summarization with several length constraints. In a summarization process, it is important to generate a summary of a desired length. However, existing test sets for cross-lingual abstractive summarization cannot evaluate whether each model controls output lengths because the test sets do not contain summaries with multiple lengths. Thus, we translate an existing monolingual abstractive summarization that contains summaries with multiple lengths to construct the new test set.    The contributions of this study are as follows:     Early explorations on cross-lingual summarization adopted the pipeline approach, which combines the machine translation with summarization methods.  applied the Maximal Marginal Relevance to summarize given documents and then automatically translated the summary. To prevent unreadable outputs,  proposed the method to predict the machine translation quality for sentences in a source document, and then generate a summary based on the predicted quality score before translation.  extended phrase-based machine translation models to select important phrases. However,  indicated that we should use information from both sides rather than such pipeline approaches.  Recent studies applied a neural encoder-decoder model to generate cross-lingual abstractive summaries from the given document directly. To construct pseudo cross-lingual abstractive summarization data for training,  adopted the approach consisting of two steps: machine translation of a document in the source language and then monolingual abstractive summarization of the translated document. Then, they used the pairs of summarized translations and the original documents as pseudo training data.  used genuine summaries to improve the quality of constructed training data. They applied a machine translation model to source sentences in monolingual sentence-summary pairs and used the pairs of translated sentences and genuine summaries as pseudo training data.  proposed a round-trip translation strategy to obtain high quality pseudo training data from existing monolingual summarization datasets. Their round-trip strategy translates a source sentence in monolingual sentence-summary pairs in the same manner as , and then re-translates the translated sentence into the source language. Their approach filters out based on the similarity between the source sentence and round-trip translation. These studies explored the sophisticated way to construct pseudo training data but pay little attention to the existing genuine parallel corpora. In contrast, this study utilizes such genuine parallel corpora in addition to constructed pseudo data with the multi-task learning framework.  In addition to the round-trip translation strategy,  introduced a multi-task learning approach for cross-lingual abstractive summarization. Their method prepares two decoders: one for cross-lingual abstractive summarization and the other for machine translation or monolingual summarization. The method trains the decoders to generate corresponding output for a given sentence. They indicated that their multi-task learning approach improved the performance of cross-lingual abstractive summarization but it requires additional parameters for a decoder. In contrast, our proposed Transum is more simple because it only needs to attach the special token to the source sentence. Moreover, experimental results show that Transum outperformed the multi-task approach of .     Our results suggest that GPT-2 generally outputs better narratives than the most recent non-GPT-based neural model. Additionally, we find that larger models are better. While GPT-2 Large may be infeasible for very long sequence generation, it is possible to use GPT-2 Medium for all narrative lengths generated here. Once GPT-3  is released for public use, it is very likely that this model will outperform GPT-2 based on these trends. We encourage future work to investigate similar hyperparameters to see whether the trends observed here are stable across model sizes.  We recommend keeping the  hyperparameter within the range  to . This aligns with the findings of \citet{ippolito2020}, who suggest that  values well below  are needed to generate text that more closely approximates human text.    Diverse decoding increased narrative quality on all metrics at small . This could be used to qualitatively induce more intense and vivid stories with higher  values, though this finding should be seen as preliminary and tested in other domains. Using higher values of  also seemed to induce more vivid stories, but with less consistent fluency and coherence; thus, the diverse decoding objective could be a promising way to increase narrative interestingness without significantly decreasing performance on any particular metric.  While relatively low dist- may correlate with consistently poor quality stories and relatively high dist- may correlate with more variable-quality stories, we find that this metric did not correlate well with any of our metrics in general . Sent-BERT did not correlate with any of our metrics of narrative quality. Thus, we do not recommend optimizing over either of these automatic quantities.  Surprisingly, we do not find a strong diversity-quality trade-off in narrative generation, perhaps due to the more creative and long-form nature of this task. Indeed, diversity and quality do not correlate well in general: diverse decoding and higher  values often coincide with better performance on all human metrics in this domain up to a point. This could be due to the more creative and long-form nature of narrative generation compared to tasks such as chatbot response generation. We thus encourage future work to investigate other methods of inducing more diverse output, as certain methods can increase human perceptions of narrative quality.  Our findings aim to inform future efforts in the narrative generation domain by establishing future baselines given our recommended hyperparameters, and by facilitating further investigation of decoding objectives for better narrative generation. Additionally, we hope that this investigation highlights issues to be addressed in future work when evaluating narratives automatically, since no metrics aside from perplexity seem to correlate well with human judgments of quality.     \clearpage    
"," Early explorations on cross-lingual summarization adopted the pipeline approach, which combines the machine translation with summarization methods.  applied the Maximal Marginal Relevance to summarize given documents and then automatically translated the summary. To prevent unreadable outputs,  proposed the method to predict the machine translation quality for sentences in a source document, and then generate a summary based on the predicted quality score before translation.  extended phrase-based machine translation models to select important phrases. However,  indicated that we should use information from both sides rather than such pipeline approaches.  Recent studies applied a neural encoder-decoder model to generate cross-lingual abstractive summaries from the given document directly. To construct pseudo cross-lingual abstractive summarization data for training,  adopted the approach consisting of two steps: machine translation of a document in the source language and then monolingual abstractive summarization of the translated document. Then, they used the pairs of summarized translations and the original documents as pseudo training data.  used genuine summaries to improve the quality of constructed training data. They applied a machine translation model to source sentences in monolingual sentence-summary pairs and used the pairs of translated sentences and genuine summaries as pseudo training data.  proposed a round-trip translation strategy to obtain high quality pseudo training data from existing monolingual summarization datasets. Their round-trip strategy translates a source sentence in monolingual sentence-summary pairs in the same manner as , and then re-translates the translated sentence into the source language. Their approach filters out based on the similarity between the source sentence and round-trip translation. These studies explored the sophisticated way to construct pseudo training data but pay little attention to the existing genuine parallel corpora. In contrast, this study utilizes such genuine parallel corpora in addition to constructed pseudo data with the multi-task learning framework.  In addition to the round-trip translation strategy,  introduced a multi-task learning approach for cross-lingual abstractive summarization. Their method prepares two decoders: one for cross-lingual abstractive summarization and the other for machine translation or monolingual summarization. The method trains the decoders to generate corresponding output for a given sentence. They indicated that their multi-task learning approach improved the performance of cross-lingual abstractive summarization but it requires additional parameters for a decoder. In contrast, our proposed Transum is more simple because it only needs to attach the special token to the source sentence. Moreover, experimental results show that Transum outperformed the multi-task approach of .",96
" Table-to-text generation is an important task for text generation from structured data. It aims at automatically producing descriptive natural language text that covers the salient information in table to help people to get the salient information of the tables. Practical applications can be found in domains such as weather forecasts, biography generation, NBA news generation, etc.  Over the pass several years, neural text generation methods have made significant progress on this task. \citeauthor{lebret-etal-2016-neural,wiseman-etal-2017-challenges,bao2018table} model it as a machine translation task and view the input table a record sequence. To generate text that contains more salient and well-organized facts,  \citeauthor{sha2018order,puduppully-etal-2019-data,moryossef-etal-2019-step,trisedya2020sentence,ijcai2020-522} explicitly model content selection and planning. %Some works also introduce extra knowledge  or pre-executed symbolic operations on table  to improve the result. To learning better representation for tables,  \citeauthor{liu2018table,bao2018table,nema-etal-2018-generating,jain-etal-2018-mixed,gong-etal-2019-table} explicitly model the structure of table from multiple levels or different dimensions. In addition, \citeauthor{liu2019hierarchical} propose three auxiliary supervision tasks to capture accurate semantic representation of the table.    However, some issues have been overlooked. First, many tables ) contain a large number of numerical records. For instance,  of records and almost  of column types are numeric in ROTOWIR , a benchmark of NBA basketball games. Current methods treat these records as words in natural language text and ignore the characteristics of the number itself which play an important role in table representation, such as size attribute. In addition, there are noises in human-written summaries in dataset. These noises include redundant information and records that do not exist in the input tables ). These noises may cause incorrect alignments between input tables and target text or wrong supervision signals. And they can affect the performance of models based on content selection and planning or auxiliary supervision. %In addition, when human are writing a summary to describe the given table, they may consider the most salient records. For example, when describing the table in Figure  , they may pay more attention to K. Leonard, because he is the top scorer.   To solve above problems, we explore the use of the information contained in the tables and introduce two self-supervised tasks to learn better representation for tables. We argue that the better representation of tables can help the model to capture and organize the important facts, even without explicitly modeling content selection and planning. Specially, we improve ~\citeauthor{gong-etal-2019-table}'s method and employ a hierarchical table encoder to model the table structure from record level and row level. The record-level encoder utilizes two cascaded self-attention models to encode the table from column and row dimension, respectively. And then, we introduce a row-level fusion gate to obtain the row-level representation for each row. To learn a number-aware record representation, we introduce a Number Ordering  task. This task utilizes a pointer network to generate a descending record sequence for each column in table, according to their content. Figure   shows a number ordering example for column PTS. To the best of our knowledge, this is the first work on neural table-to-text generation via focusing on learning representation for number in table. Another self-supervised task, Significance Ordering , is further proposed to learn a significance-aware representation for the record. The significance denotes the relative relation between records in same row. This is inspired by the intuition that when humans describe the performance of a player, they tend to focus on his more salient records. For example, in Figure , K. Thompson's scores  is more likely to be described than his other's records. The SO task executes a descending sort operation on each row according to the significance scores of records. We use the position index of record  to measure its importance and the smaller the significance score, the more important the record is. The position index of record is obtained by the results of Number Ordering. For example, in Figure  , K. Thompson scores  points which are the largest in PTS, so the significance score of this record is 1. The proposed two tasks are trained together with the table2text generation model and they share the same encoder parameters. Obviously, the two proposed tasks are self-supervised and the training labels are easily obtained from the input tables. Therefore, the errors caused by noises in training set are avoided. %For record in same row, it includes another size information:significance. It denotes the relative relation between records in same row. To learn a significance-aware representation for table, we propose a Significance Ordering task which executes a ascending sort operation on each row according to the significance of records. We use the position index of record  to measure its importance and the smaller the significance score, the more important the record is. The position index of record is obtained by the results of Number Ordering. For example, in Figure  , K. Leonard score 45 points which are the largest in PTS, so the significance score of this record is 1). Obviously, the two proposed tasks are self-supervised and the training labels are easily obtained from the input tables. Therefore, the errors caused by noise in training set are avoided.   We conducted experiments on ROTOWIRE to verify the effectiveness of the proposed approach. The experimental results demonstrate that, even without explicitly modeling content selection or introducing extra knowledge, our method can help to generate text that contains more salient and well-organized facts. And we achieve the state-of-the-art performance on automatic metrics. %Content Selection , Content Ordering  and BLEU.     Recently, neural models have been the mainstream for table-to-text generation and obtained impressive results.  Early works on table-to-text generation regard it as a distinct machine translation task and view a structured table as a sequence of records . Most recent works inspired by the traditional methods for data-to-text generation and introduce explicit content selection and planning to improve the results , and they obtain training labels by aligning the input tables with related summaries. However, this alignment may introduce additional errors. Some works attempt to use additional knowledge to improve the quality of generated text. \citeauthor{nie-etal-2018-operation} utilize pre-executed symbolic operations on input table in a sequence-to-sequence model to improve the fidelity of neural table-to-text generation.  \citeauthor{chen2019enhancing} introduce the background knowledge of entity in table to improve results.   Various studies have been conducted to  In addition to introducing external knowledge, some works learn better representation for table by explicitly modeling the structure of table. \citeauthor{liu2018table} propose a structure-aware learning which incorporates the filed information as the additional inputs to the table encoder. Some works  model the representation of table from row level and column level and utilize the dual attention decoder to generate. \citeauthor{gong-etal-2019-table} introduce the historical data for each table and utilize a self-attention based hierarchical encoder on three dimensions  to enrich table's representation. Furthermore, \citeauthor{liu2019hierarchical} propose three auxiliary supervision tasks  to capture accurate semantic representation of the tables and the supervised signals of text auto-encoding task are from summary which there may be noises in.   \citeauthor{gong-etal-2019-table} introduce the historical data for each table  and utilize a self-attention based hierarchical encoder on three dimensions  to enrich table's representation. Our basic method also is a hierarchical encoder and is similar to , the most differences between our approach and theirs are that our method acquire the information from row dimension and column dimension simultaneously and introduce a attention based method to obtain the row-level representation dynamically.   Existing works on table-to-text generation neglect the representation of numeric records and the relative relation between records on different dimensions . In this paper, we propose two simple but effective self-supervised tasks to help to learn better table representation and the two tasks are trained with our generation task by end to end.     Self-supervised learning aims to train a network on an auxiliary task where ground-truth is obtained automatically. It has made great successful in image processing domain and natural language processing domain.    This paper presents a multi-task learning framework for cross-lingual abstractive summarization to augment training data. The proposed method, Transum, attaches the special token to the beginning of the input sentence to indicate the target task. The special token enables us to use genuine translation pairs and the monolingual abstractive summarization dataset in addition to the pseudo cross-lingual abstractive summarization data for training. The experimental results show that Transum achieved better performance than the pipeline approach and model trained with pseudo data only. We achieved the top ROUGE scores in Chinese-English and Arabic-English abstractive summarization. Moreover, Transum also improved the performance of machine translation and outperformed the previous top score in the JIJI English-Japanese translation.         
"," Recently, neural models have been the mainstream for table-to-text generation and obtained impressive results.  Early works on table-to-text generation regard it as a distinct machine translation task and view a structured table as a sequence of records . Most recent works inspired by the traditional methods for data-to-text generation and introduce explicit content selection and planning to improve the results , and they obtain training labels by aligning the input tables with related summaries. However, this alignment may introduce additional errors. Some works attempt to use additional knowledge to improve the quality of generated text. \citeauthor{nie-etal-2018-operation} utilize pre-executed symbolic operations on input table in a sequence-to-sequence model to improve the fidelity of neural table-to-text generation.  \citeauthor{chen2019enhancing} introduce the background knowledge of entity in table to improve results.   Various studies have been conducted to  In addition to introducing external knowledge, some works learn better representation for table by explicitly modeling the structure of table. \citeauthor{liu2018table} propose a structure-aware learning which incorporates the filed information as the additional inputs to the table encoder. Some works  model the representation of table from row level and column level and utilize the dual attention decoder to generate. \citeauthor{gong-etal-2019-table} introduce the historical data for each table and utilize a self-attention based hierarchical encoder on three dimensions  to enrich table's representation. Furthermore, \citeauthor{liu2019hierarchical} propose three auxiliary supervision tasks  to capture accurate semantic representation of the tables and the supervised signals of text auto-encoding task are from summary which there may be noises in.   \citeauthor{gong-etal-2019-table} introduce the historical data for each table  and utilize a self-attention based hierarchical encoder on three dimensions  to enrich table's representation. Our basic method also is a hierarchical encoder and is similar to , the most differences between our approach and theirs are that our method acquire the information from row dimension and column dimension simultaneously and introduce a attention based method to obtain the row-level representation dynamically.   Existing works on table-to-text generation neglect the representation of numeric records and the relative relation between records on different dimensions . In this paper, we propose two simple but effective self-supervised tasks to help to learn better table representation and the two tasks are trained with our generation task by end to end.     Self-supervised learning aims to train a network on an auxiliary task where ground-truth is obtained automatically. It has made great successful in image processing domain and natural language processing domain.",97
" In healthcare, real-world data  refers to patient data routinely collected during clinic visits, hospitalization, as well as patient-reported results. In recent years, RWD's volume has become enormous, and invaluable insights and real-world evidence can be generated from these datasets using the latest data processing and analytical techniques. However, RWD's quality remains one of the main challenges that prevent novel machine learning methods from being readily adopted in healthcare.  Therefore, creating data quality tools is of great importance in health care and health data sciences.  Erroneous data in healthcare systems could jeopardize a patient's clinical outcomes and affect the care provider's ability to optimize its performance.     Common data quality issues include missing critical information about medical history, wrong coding of a condition, and inconsistency in documentation across different care sites. Manual review by domain experts is the gold standard for achieving the highest data quality but is unattainable in regular care practices. Recent developments in the field of Natural Language Processing  has attracted great interest in the healthcare community since algorithms for identifying variables of interest and classification algorithm for diseases  have been recently developed .  In this paper, we presented a novel model for the extraction of queries  in a corpus of dialogue between data entry clinicians and expert reviewers in a multi-site dialysis environment.   %The work's ultimate goal is to identify the data elements that caused most uncertainty or errors during the documentation process.  The main contributions of this work are:  Finally, in addition to evaluating our model's performance in a medical context, we also experimented in section  with a general-domain dataset  to show our model's generalizability.  The rest of the paper is organized as follows. Related work is presented in section . The different question detection methods that will be examined,   are described in section . Section  details the characteristics of the proposed multi-channel CNN model. Finally, the results of the experiments are reported in section  and a conclusion and a plan for future work are given in section .     Different question-detection methods have  mainly been focused on the extraction of questions in social online settings  . These methods can be classified into two categories:   Recently, different deep-neural networks have achieved a state-of-the-art results in text classification.  In the KIM-CNN model  filters are applied to the concatenated word embeddings of each document in order to produce  feature maps, which are  fed to a max pooling layer, in order to create a -dimensional representation of the document. In addition, in  the XML-CNN network  was introduced, where a dynamic max-pooling scheme and a hidden bottleneck layer were used to achieve a better representation of documents.  Another state-of-the art deep model is Seq-CNN  where   each word is represented as a -dimensional one-hot vector where  is the vocabulary of the dataset and the concatenation of the word vectors are passed through a convolutional layer, followed by a special dynamic pooling layer. Furthermore, in FastText  the embedding of the words that  appear in a document were averaged to create a document representation. Finally, a comprehensive analysis of clinical-domain embedding methods is presented in .       In this work, we first point out the shortcomings of MLE based training for keyphrase generation. We specifically address the lack of output diversity issue via the use of unlikelihood training objective. We adopt a target level unlikelihood loss and propose a novel copy token unlikelihood loss, the combination of which provides large diversity gains. In addition, a -step ahead MLE and UL objective is incorporated into the training. Through extensive experiments on datasets from three different domains, we demonstrate the effectiveness of our model for diverse keyphrase generation. For future work, we plan to explore directions that would enable us to simultaneously optimize for quality and diversity metrics.  
","  Different question-detection methods have  mainly been focused on the extraction of questions in social online settings  . These methods can be classified into two categories:   Recently, different deep-neural networks have achieved a state-of-the-art results in text classification.  In the KIM-CNN model  filters are applied to the concatenated word embeddings of each document in order to produce  feature maps, which are  fed to a max pooling layer, in order to create a -dimensional representation of the document. In addition, in  the XML-CNN network  was introduced, where a dynamic max-pooling scheme and a hidden bottleneck layer were used to achieve a better representation of documents.  Another state-of-the art deep model is Seq-CNN  where   each word is represented as a -dimensional one-hot vector where  is the vocabulary of the dataset and the concatenation of the word vectors are passed through a convolutional layer, followed by a special dynamic pooling layer. Furthermore, in FastText  the embedding of the words that  appear in a document were averaged to create a document representation. Finally, a comprehensive analysis of clinical-domain embedding methods is presented in .",98
"  Semantic parsing is the task of mapping a natural language query into a formal language, that is extensively used in goal-oriented dialogue systems. For a given query, such model should identify the requested action  and the associated values specifying parameters of the action . For example, if the query is Call Mary the action is call and the value of slot contact is Mary.  The number of different intents and slots in publicly available datasets  can be close to a hundred and it may be the orders of magnitude larger in real-world systems. Such a big number of classes usually causes a long tail in the class frequency distribution . These tail classes can be significantly improved with small quantities of additional labeled data.    However, training a neural semantic parsing model from scratch can take hours even on a relatively small public dataset . The real-world datasets can contain millions of examples  which can change the time scale to weeks. % Need to describe the problem and motivation to production settings more.  In this work, we propose to fine-tune a model that has already been trained on the old dataset  instead of training a new model to significantly speed up the incorporation of a new portion of data. We call this setting Incremental training, as the new portions of data can be added incrementally.  We focus on semantic parsing % and seq2seq networks for our case studies for the following reasons. Semantic parsing is a more complex NLP task compared to classification or NER and we hope that the lessons learned here would be more widely applicable. Task-oriented semantic parsing tend to have a large output vocabulary that can be frequently updated, and thus, benefit most from the Incremental setting. % We choose seq2seq networks for this work due to two reasons: first, seq2seq networks are very % general and can be easily adapted to simpler tasks like NER; % second, seq2seq models perform really well on popular natural language understanding datasets like TOP and SNIPS.  % Exploring this space of possible solutions, we compare the effectiveness of these approaches with each other and come up with a set of guidelines that are useful for incremental training tasks as well.  % To emulate the ""data-patch"" scenario, we split these datasets by focusing on a few classes. We show that naive fine-tuning leads to catastrophic forgetting and come up with approaches to remedy this. We observe that it is possible to fine-tune models to new classes in a few minutes compared to hours when retraining from scratch. We also compare the effect of pre-trained representations like BERT on fine-tuning. Using these observations we come up with fine-tuning guidelines in scenarios where the label space does not change. We verify that our approaches work on 2 popular semantic parsing datasets: TOP and SNIPS under different data splits.  The main contributions of this work are:    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Related work %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%       Continual and lifelong learning   Fine-tuning of pre-trained deep learning models is an active area of research, but most recent work  focuses on new tasks or new labels.  introduce adapters which modify the pre-trained representations for new tasks by adding per-task parameters.  compare two common fine-tuning strategies, freezing the encoder  and fine-tuning the whole model on variety of downstream tasks.  propose techniques for effective fine-tuning of language models for classification tasks including gradual unfreezing while training. All of these works focus on using a pre-trained model and fine-tuning to a different downstream task.    In contrast, there is not much recent work in the NLP community on the ""data-patch"" use-case. In this setting, we fine-tune on the same task but under a different data distribution. This has been commonly referred to as continual learning  in the broader ML community. Continual Learning has been a long-studied  problem. One of the main challenges of continual learning is the catastrophic forgetting -- the network forgets existing knowledge when learning from novel observations -- is a common problem in this setting. Due to the interleaving of data, training from scratch usually does not suffer from catastrophic forgetting as the network is jointly optimized for all classes.   Usually when training from scratch, since we interleave or shuffle data we do not suffer from this problem as the network is jointly optimized for all classes.  We draw inspiration from the work on lifelong learning . We also survey popular fine-tuning strategies including gradual unfreezing and discriminative fine-tuning .  introduce Elastic Weight Consolidation  as a regularization approach which adds a penalty between weights of original and the fine-tuned model.  showed that interleaving information about new experiences with previous experiences can help overcome catastrophic forgetting.  propose sparse experience replay for continual language learning.  provides a comprehensive review of Continual lifelong learning techniques in neural networks.   Our proposed approaches are a combination of interleaving old and new information, selective layer freezing, and simple regularization methods between the pre-trained and fine-tuned models.     In this usecase we get additional labeled data for one of the minority classes but the label space does not change. An effective solution in such a case could be to retrain the network from scratch but doing so requires time and computation.    In this work we explore different Incremental training regimes given a limited computational budget.    Recently in the NLP community there has been a lot of work on fine-tuning and   adapting pre-trained representations .    These works use techniques like gradual unfreezing and adapters    to adapt the pre-trained network to a new task.   \citet{Peters2019} propose recipes for this setting where we have to adapt to a new task.   Our work aims to build similar recipes but for the continual learning setting.                                              Incremental training                                             In this paper, we have provided an analysis of the performance of existing methods for question extraction with real-world misclassification examples that showed the weak point of each method. Furthermore, we have proposed a novel approach for the automatic identification of real questions   and c-questions. We have also shown empirically that the proposed architecture of unifying syntactic, semantic and statistical features achieved a state-of-the-art F1 score for this particular task. Finally, we have presented the relevance of exploiting domain knowledge in the overall performance of a model.  We are in the process of obtaining access to datasets from different application contexts in order to examine the generalizability of our model. As for future work, we plan to extend our work by calculating the similarity of questions in order to create groups of questions that represent the most impactful ``problems'' of a given application environment. Finally, we plan to compare our model with recent  language   representation  models like the BERT model  in  both for the task of question identification and for the task of creating the above mentioned ``problem'' groups.      
","    Continual and lifelong learning   Fine-tuning of pre-trained deep learning models is an active area of research, but most recent work  focuses on new tasks or new labels.  introduce adapters which modify the pre-trained representations for new tasks by adding per-task parameters.  compare two common fine-tuning strategies, freezing the encoder  and fine-tuning the whole model on variety of downstream tasks.  propose techniques for effective fine-tuning of language models for classification tasks including gradual unfreezing while training. All of these works focus on using a pre-trained model and fine-tuning to a different downstream task.    In contrast, there is not much recent work in the NLP community on the ""data-patch"" use-case. In this setting, we fine-tune on the same task but under a different data distribution. This has been commonly referred to as continual learning  in the broader ML community. Continual Learning has been a long-studied  problem. One of the main challenges of continual learning is the catastrophic forgetting -- the network forgets existing knowledge when learning from novel observations -- is a common problem in this setting. Due to the interleaving of data, training from scratch usually does not suffer from catastrophic forgetting as the network is jointly optimized for all classes.   Usually when training from scratch, since we interleave or shuffle data we do not suffer from this problem as the network is jointly optimized for all classes.  We draw inspiration from the work on lifelong learning . We also survey popular fine-tuning strategies including gradual unfreezing and discriminative fine-tuning .  introduce Elastic Weight Consolidation  as a regularization approach which adds a penalty between weights of original and the fine-tuned model.  showed that interleaving information about new experiences with previous experiences can help overcome catastrophic forgetting.  propose sparse experience replay for continual language learning.  provides a comprehensive review of Continual lifelong learning techniques in neural networks.   Our proposed approaches are a combination of interleaving old and new information, selective layer freezing, and simple regularization methods between the pre-trained and fine-tuned models.     In this usecase we get additional labeled data for one of the minority classes but the label space does not change. An effective solution in such a case could be to retrain the network from scratch but doing so requires time and computation.    In this work we explore different Incremental training regimes given a limited computational budget.    Recently in the NLP community there has been a lot of work on fine-tuning and   adapting pre-trained representations .    These works use techniques like gradual unfreezing and adapters    to adapt the pre-trained network to a new task.   \citet{Peters2019} propose recipes for this setting where we have to adapt to a new task.   Our work aims to build similar recipes but for the continual learning setting.                                              Incremental training",99
"  Neural attention mechanisms have been widely applied in  computer vision and have been shown to enable neural networks to only focus on those aspects of their input that are important for a given task. While neural networks are able to learn meaningful attention mechanisms using only supervision received for the target task, the addition of human gaze information has been shown to be beneficial in many cases. An especially interesting way of leveraging gaze information was demonstrated by works incorporating human gaze into neural attention mechanisms, for example for image and video captioning or visual question answering.  While attention is at least as important for reading text as it is for viewing images, integration of human gaze into neural attention mechanisms for natural language processing  tasks remains under-explored. A major obstacle to studying such integration is data scarcity: Existing corpora of human gaze during reading consist of too few samples to provide effective supervision for modern data-intensive architectures and human gaze data is only available for a small number of NLP tasks. For paraphrase generation and sentence compression, which play an important role for tasks such as reading comprehension systems, no human gaze data is available.  We address this data scarcity in two novel ways: First, to overcome the low number of human gaze samples for reading, we propose a novel hybrid text saliency model  in which we combine a cognitive model of reading behavior with human gaze supervision in a single machine learning framework. More specifically, we use the E-Z Reader model of attention allocation during reading to obtain a large number of synthetic training examples. We use these examples to pre-train a BiLSTM network with a Transformer whose weights we subsequently refine by training on only a small amount of human gaze data. We demonstrate that our model yields predictions that are well-correlated with human gaze on out-of-domain data. Second, we propose a novel joint modeling approach of attention and comprehension that allows human gaze predictions to be flexibly adapted to different NLP tasks by integrating TSM predictions into an attention layer. By jointly training the TSM with a task-specific network, the saliency predictions are adapted to this upstream task without the need for explicit supervision using real gaze data. Using this approach, we outperform the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10\% in BLEU-4 and achieve state of the art performance on the Google Sentence Compression corpus. As such, our work demonstrates the significant potential of combining cognitive and data-driven models and establishes a general principle for flexible gaze integration into NLP that has the potential to also benefit tasks beyond paraphrase generation and sentence compression.     Our work is related to previous works on 1) NLP tasks for text comprehension, 2) human attention modeling, as well as 3) gaze integration in neural network architectures.    Two key tasks in machine text comprehension are paraphrasing and summarization. While paraphrasing is the task of ``conveying the same meaning, but with different expressions'', summarization deals with extracting or abstracting the key points of a larger input sequence. Though advances have helped bring machine comprehension closer to human performance, humans are still superior for most tasks. While attention mechanisms can improve performance by helping models to focus on relevant parts of the input, the benefit of explicit supervision through human attention remains under-explored.    Predicting what people visually attend to in images  is a long-standing challenge in neuroscience and computer vision. In contrast to images, most attention models for eye movement behaviors during reading are cognitive process models, i.e. models that do not involve machine learning but implement cognitive theories. Key challenges for such models are a limited number of parameters, hand-crafted rules and thus a difficulty to adapt them to different tasks and domains, as well as the difficulty to use them as part of an end-to-end trained machine learning architectures. One of the most influential cognitive models of gaze during reading is the E-Z Reader model It assumes attention shifts to be strictly serial in nature and that saccade production depends on different stages of lexical processing. that has been successful in explaining different effects seen in attention allocation during reading.  In contrast, learning-based attention models for text remain under-explored. \citet{nilsson2009learning} trained person-specific models on features including length and frequency of words to predict fixations and later extended their approach to also predict fixation durations. The first work to present a person-independent model for fixation prediction on text used a linear CRF model. A separate line of work has instead tried to incorporate assumptions about the human reading process into the model design. For example, the Neural Attention Trade-off  language model  was trained with hard attention and assigned a cost to each fixation \citet{hahn2016modeling}. Subsequent work applied the NEAT model to question answering tasks, showing task-specific effects on learned attention patterns that reflect human behavior. Further approaches include sentence representation learning using surprisal and part of speech tags as proxies to human attention, attention as a way to improve time complexity for NLP tasks, and learning saliency scores by training for sentence comparison. Our work is fundamentally different from all of these works in that we, for the first time, combine cognitive theory and data-driven approaches.    Integration of human gaze data into neural network architectures has been explored for a range of computer vision tasks. \citet{sugano2016seeing} were the first to use gaze as an additional input to the attention layer for image captioning, while \citet{qiao2018exploring} used human-like attention maps as an additional supervision for the attention layer for a visual question answering task. Most previous work in gaze-supported NLP has used gaze as an input feature, e.g. for syntactic sequence labeling, classifying referential versus non-referential use of pronouns, reference resolution, key phrase extraction, or prediction of multi-word expressions. Recently, \citet{hollenstein2019advancing} proposed to build a lexicon of gaze features given word types, overcoming the need for gaze data at test time. Two recent works proposed methods inspired by multi-task learning to integrate gaze into NLP classification tasks. did not integrate gaze into the attention layers but demonstrated performance improvements by adding a gaze prediction task to regularize a sentence compression model. did not predict human gaze for the target task but used ground-truth gaze from another eye tracking corpus to regularize their neural attention function. In stark contrast, our work is the first to combine a cognitive model of reading and a data-driven approach to predict human gaze, to directly integrate these predictions into the neural attention layers, and to jointly train for two different tasks -- generative  and classification .      This work analyzes pre-trained summarization models via uncertainty, or the entropy of decoding decisions. We pursue several lines of inquiry: uncertainty can help us understand copying document spans vs.~generating novel text, the behavior of models in different syntactic environments, and coarse properties of the model's attention distribution. All of these give insight into what conditions most heavily restrict the model's generation: generating an observed bigram , low syntactic distance, and attention which can easily identify decoder context in the source document. We believe this approach can power future analyses of pre-trained text generation systems.  
","  Our work is related to previous works on 1) NLP tasks for text comprehension, 2) human attention modeling, as well as 3) gaze integration in neural network architectures.    Two key tasks in machine text comprehension are paraphrasing and summarization. While paraphrasing is the task of ``conveying the same meaning, but with different expressions'', summarization deals with extracting or abstracting the key points of a larger input sequence. Though advances have helped bring machine comprehension closer to human performance, humans are still superior for most tasks. While attention mechanisms can improve performance by helping models to focus on relevant parts of the input, the benefit of explicit supervision through human attention remains under-explored.    Predicting what people visually attend to in images  is a long-standing challenge in neuroscience and computer vision. In contrast to images, most attention models for eye movement behaviors during reading are cognitive process models, i.e. models that do not involve machine learning but implement cognitive theories. Key challenges for such models are a limited number of parameters, hand-crafted rules and thus a difficulty to adapt them to different tasks and domains, as well as the difficulty to use them as part of an end-to-end trained machine learning architectures. One of the most influential cognitive models of gaze during reading is the E-Z Reader model It assumes attention shifts to be strictly serial in nature and that saccade production depends on different stages of lexical processing. that has been successful in explaining different effects seen in attention allocation during reading.  In contrast, learning-based attention models for text remain under-explored. \citet{nilsson2009learning} trained person-specific models on features including length and frequency of words to predict fixations and later extended their approach to also predict fixation durations. The first work to present a person-independent model for fixation prediction on text used a linear CRF model. A separate line of work has instead tried to incorporate assumptions about the human reading process into the model design. For example, the Neural Attention Trade-off  language model  was trained with hard attention and assigned a cost to each fixation \citet{hahn2016modeling}. Subsequent work applied the NEAT model to question answering tasks, showing task-specific effects on learned attention patterns that reflect human behavior. Further approaches include sentence representation learning using surprisal and part of speech tags as proxies to human attention, attention as a way to improve time complexity for NLP tasks, and learning saliency scores by training for sentence comparison. Our work is fundamentally different from all of these works in that we, for the first time, combine cognitive theory and data-driven approaches.    Integration of human gaze data into neural network architectures has been explored for a range of computer vision tasks. \citet{sugano2016seeing} were the first to use gaze as an additional input to the attention layer for image captioning, while \citet{qiao2018exploring} used human-like attention maps as an additional supervision for the attention layer for a visual question answering task. Most previous work in gaze-supported NLP has used gaze as an input feature, e.g. for syntactic sequence labeling, classifying referential versus non-referential use of pronouns, reference resolution, key phrase extraction, or prediction of multi-word expressions. Recently, \citet{hollenstein2019advancing} proposed to build a lexicon of gaze features given word types, overcoming the need for gaze data at test time. Two recent works proposed methods inspired by multi-task learning to integrate gaze into NLP classification tasks. did not integrate gaze into the attention layers but demonstrated performance improvements by adding a gaze prediction task to regularize a sentence compression model. did not predict human gaze for the target task but used ground-truth gaze from another eye tracking corpus to regularize their neural attention function. In stark contrast, our work is the first to combine a cognitive model of reading and a data-driven approach to predict human gaze, to directly integrate these predictions into the neural attention layers, and to jointly train for two different tasks -- generative  and classification .",100
" %\hh{check the fuzziness: pre-trained or pretrained and decide which one to use .} Modern techniques for text summarization generally can be categorized as either extractive methods, which identify the most suitable %\pfliu{How about ``which identify the most suitable semantic units ''}  words or sentences from the input document and concatenate them to form a summary, or abstractive methods, which generate summaries freely and are able to produce novel words and sentences. Compared with extractive algorithms, abstractive algorithms are more flexible, making them more likely to produce fluent and coherent summaries. %\pfliu{better if adding some references here}  %and the generation process is more human-like \gn{Re ``more human-like''. First, I'm not sure if this is actually true: humans copy-paste text as well. Second, it doesn't seem really important here. Maybe you could just expand on the ``more flexible'' part and mention the practical advantages of this.}. However, the unconstrained nature of abstractive summarization can also result in problems. First, it can result in unfaithful summaries, containing factual errors as well as hallucinated content. Second, it can be difficult to control the content of summaries; it is hard to pick in advance which aspects of the original content an abstractive system may touch upon. %\pfliu{I'm thinking about if it's suitable to place the following paragraph here .  Will it be better if we exchange it with ``There have been some ...'' this paragraph and make corresponding modification.} To address the issues, we propose methods for guided neural abstractive summarization: methods that provide various types of guidance signals that 1) constrain the summary so that the output content will deviate less from the source document; 2) allow for controllability through provision of user-specified inputs.             % Table generated by Excel2LaTeX from sheet 'Sheet1' \iffalse %   %      \end{table*}%  \fi      \iffalse  %   '' and ``{cover.}'' represent the copy and coverage mechanism respectively. Guidance represents different guided information while Guiding Method denotes how to introduce the guided information. ``ourGuidance'' contains sentences, relations keywords and retrieved summaries. ``Marker Embedding'' suggests that the guided information is introduced by embedding it as a feature vector.}% \gn{add  for completeness. Make sure it's in chronological order. I don't think BART needs to be included, but you might also include other methods that provide guidance on, for example, the style of the output .}} %\zj{Is there any particular reason to make ``copy'' and ``cover.'' italic?}.}   % \end{table*}%  \fi  %      %'' and ``{cover.}'' represent the copy and coverage mechanism respectively. Guidance represents different guided information while Guiding Method denotes how to introduce the guided information. ``ourGuidance'' contains sentences, relations keywords and retrieved summaries. ``Marker Embedding'' suggests that the guided information is introduced by embedding it as a feature vector.}% \gn{add  for completeness. Make sure it's in chronological order. I don't think BART needs to be included, but you might also include other methods that provide guidance on, for example, the style of the output .}} %\zj{Is there any particular reason to make ``copy'' and ``cover.'' italic?}.}   % \end{table*}%  %\gn{The term ``hybrid summarization models'' is sudden, and it doesn't follow clearly from the last sentence in the previous paragraph. I think the point of this paragraph is ``we are not the first to propose guided neural summarization models, but previous methods were limited to only a particular type of guidance''. If so, then you can say the ``we are not the first'' part at the beginning of this paragraph, and the ``limited'' part at the final part of the paragraph.} There have been some previous methods for guiding neural abstractive summarization models. For example,~\citet{kikuchi-etal-2016-controlling} specify the length of abstractive summaries,~\citet{li2018guiding} provide models with keywords to prevent the model from missing key information, and ~\citet{cao2018retrieve} propose models that retrieve and reference relevant summaries from the training set. %, and~\citet{gehrmann2018bottom} propose to train a model to identify salient words and encourage the final model to faithfully copy them from the source. While these methods have demonstrated improvements in summarization quality and controllability, each focuses on one particular type of guidance -- it remains unclear which is better and whether they are complementary to each other. %In addition, most of the previous work whether they are compatible with pre-trained language models such as BERT. %Previously, in order to address the issues of abstractive summarization models, researchers have proposed hybrid summarization models that combine the merits of extractive and abstractive methods. %\gn{In the following three sentences, it is not explicitly stated or clear how these methods address the issues of abstractive summarization models.} %For example,~\citet{gu2016incorporating} propose methods to copy words from the source document.~\citet{gehrmann2018bottom} utilize bottom-up attention to constrain the decoder to attend to salient parts of the inputs. %Similarly, %While these approaches can achieve good performance in terms of ROUGE, we cannot guarantee the models learn to identify the salient segments correctly or control the summaries due to the lack of explicit supervision signals  %\gn{can your model guarantee this? if you're putting it as a downside here it seems that it should be something that does not apply to your model.} \zd{I think our model does not try to learn to identify the salient part. Instead, we explicitly provide the salient part to the model so that the model learns to rely on this input.} \gn{But the extractive summarization model may fail at test time, right?} \zd{right, but i think that's the problem of extractive summarization, and the goal of our model is to learn to depend on the input, no matter whether the input signal is correct or not. } \gn{See my comment below. I think that there's a problem of a disconnect between how you're presenting the method , and what we're actually doing in experiments. It'd be best if you can write the story in the way that encompasses the things in experiments . Could you think of a way to reframe the intro a little bit in this direction? I think one thing you can definitely say about your method is that it can use a wide variety of different types of guidance, including that from automatic up-stream systems, or perhaps user-specified keywords etc. You are using a method to encourage the model to pay close attention to this guidance . This is very empirically effective. I'll take a look once you've thought about this a bit and modified the intro accordingly. Additionally, you might want to add a sentence to the end of the first paragraph describing what you attempt to achieve in this paper before jumping into the previous work. This will help make the contrasts more clear in this paragraph.} \zd{Thanks a lot! I'll think more about this and change the paper accordingly!}. %To improve the controllability of summarization models, previous works have attempted to provide models with keywords or length information, but the choices of guidance are limited and thus the controllability of the output summaries is hindered \gn{Again, here it's not super-clear how or why your proposed method is better in these aspects}.   %\gn{I think this is OK, but could really benefit from a figure at the top-right of page 1 demonstrating the behavior.} %To obtain abstractive summarization models with good performance as well as flexible controllability, In this paper, we propose a general and extensible guided summarization framework that can take different kinds of external guidance as input. %\gn{Maybe one more sentence on how the framework works.} Like most recent summarization models, our model is based on neural encoder-decoders, instantiated with contextualized pretrained language models, including BERT and BART. With this as strong starting point, we make modifications allowing the model to attend to both the source documents and the guidance signals when generating outputs. %\gn{A little more concreteness here could help, even just saying ``attends to sequences representing both the source document and the guidance signal''.} %\gn{I would put the next two sentences in the method description above, before we discuss the specific types of guidance we provide.} As shown in Figure, we can provide automatically extracted or user-specified guidance to the model during test time to constrain the model output. At training time, to encourage the model to pay close attention to the guidance, %\pfliu{Since oracle-based training method is a  contribution of this work, it would be better if we can express this more explicitly. For example: ``we propose to use ...instead of ..''} we propose to use an oracle to select informative guidance signals -- a simple modification that nonetheless proved essential in effective learning of our guided summarization models.  %\gn{How is this different than ? This sentence seems to say the same thing as the second-to-last sentence of the previous paragraph. I understand that ``extensible'' may be attempting to make a contrast, but it's not very clear.}. Using this framework, we investigate four types of guidance signals:  highlighted sentences in the source document,  keywords,  salient relational triples in the form of , and  retrieved summaries. %\zj{Just a minor point. Maybe better to make the orders here consistent with the experiment section .}   We evaluate our methods on 6 popular summarization benchmarks. Our best model, using highlighted sentences as guidance, can achieve state-of-the-art performance on 4 out of the 6 datasets, including 1.28/0.79/1.13 ROUGE-1/2/L improvements over previous state-of-the-art model on the widely-used CNN/DM dataset. In addition, we perform in-depth analyses of different guidance signals and demonstrate that they are complementary to each other in that we can aggregate their outputs together and obtain further improvements. An analysis of the results also reveals that our guided models can generate more faithful summaries and more novel words. Finally, we demonstrate that we can control the output by providing user-specified guidance signals, with different provided signals resulting in qualitatively different summaries.  %\pfliu{Do we need to highlight our contributions?} %We first evaluate our methods on the widely-used CNN/DailyMail benchmark and perform in-depth analysis of different guidance signals. Experimental results demonstrate that our best method can achieve 1.13 ROUGE-L improvements over the state-of-the-art model. We then pick the best guidance signal and evaluate our models on the other five popular summarization benchmarks. Extensive experiments demonstrate the effectiveness of our model on extractive datasets and analyses reveal that our methods can generate more novel words and more faithful summaries. In addition, we can control the output by providing user-specified guidance signals.      In this part, we will introduce both the extractive and abstractive neural summarization models.  \paragraph{Extractive Summarization.} Extractive summarization models generate a summary by selecting and concatenating the most important sentences in a document. To this end, the summarization task is typically formalized as a sentence-level classification problem for neural approaches. Specifically, each sentence will be assigned with a probability of whether it should be included in the output and  sentences with the highest probabilities will be selected, where  is a hyper-parameter and is dataset-dependent.   Because most summarization corpora only contain human-generated summaries, heuristics are often used to obtain an oracle summary for each summary. For example, and employ a greedy approach that incrementally add one sentence to the summary, and the sentence that can increase the ROUGE score with the entire human-written summary the most will be selected at each step.   It should be noted that there are also neural approaches that do not adopt this paradigm. For example, directly maximize the likelihood of human summaries given selected sentences and formulate the extractive summarization task as a semantic text matching problem and achieve state-of-the-art performance on almost all popular benchmarks.  \paragraph{Abstractive Summarization.} Sequence-to-sequence framework~ with the attention mechanism~ is usually adopted in current neural abstractive summarization approaches, where an encoder generates a context vector for each source sentence and a decoder then outputs a summary, one target word at a time.   While being conceptually simple, neural abstractive summarization models can suffer from generating unfaithful summaries. To alleviate such a problem, copy mechanism is introduced in the field of text summarization~, which allows the model to learn to detect important words or phrases in the source side and directly copy them to the target side with certain probabilities.  Current state-of-the-art abstractive summarization model mainly utilize pre-trained language models. For example, use the pretrained BERT model~ to initialize the encoder of summarization models and then fine-tune the model on summarization datasets. Similarly, pretrain a language model with a denoising autoencoding objective and fine-tune it on summarization datasets.         \pfliu{Or, we can also organize our related work section in this way and put it here.}                    In this work we made two novel contributions towards improving natural language processing tasks using human gaze predictions as a supervisory signal. First, we introduced a novel hybrid text saliency model that, for the first time, integrates a cognitive reading model with a data-driven approach to address the scarcity of human gaze data on text. Second, we proposed a novel joint modeling approach that allows the TSM to be flexibly adapted to different NLP tasks without the need for task-specific ground truth human gaze data. We showed that both advances result in significant performance improvements over the state of the art in paraphrase generation as well as competitive performance for sentence compression but with a much less complex model than the state of the art. We further demonstrated that this approach is effective in yielding task-specific attention predictions. Taken together, our findings not only demonstrate the feasibility and significant potential of combining cognitive and data-driven models for NLP tasks -- and potentially beyond -- but also how saliency predictions can be effectively integrated into the attention layer of task-specific neural network architectures to improve performance.            
","  In this part, we will introduce both the extractive and abstractive neural summarization models.  \paragraph{Extractive Summarization.} Extractive summarization models generate a summary by selecting and concatenating the most important sentences in a document. To this end, the summarization task is typically formalized as a sentence-level classification problem for neural approaches. Specifically, each sentence will be assigned with a probability of whether it should be included in the output and  sentences with the highest probabilities will be selected, where  is a hyper-parameter and is dataset-dependent.   Because most summarization corpora only contain human-generated summaries, heuristics are often used to obtain an oracle summary for each summary. For example, and employ a greedy approach that incrementally add one sentence to the summary, and the sentence that can increase the ROUGE score with the entire human-written summary the most will be selected at each step.   It should be noted that there are also neural approaches that do not adopt this paradigm. For example, directly maximize the likelihood of human summaries given selected sentences and formulate the extractive summarization task as a semantic text matching problem and achieve state-of-the-art performance on almost all popular benchmarks.  \paragraph{Abstractive Summarization.} Sequence-to-sequence framework~ with the attention mechanism~ is usually adopted in current neural abstractive summarization approaches, where an encoder generates a context vector for each source sentence and a decoder then outputs a summary, one target word at a time.   While being conceptually simple, neural abstractive summarization models can suffer from generating unfaithful summaries. To alleviate such a problem, copy mechanism is introduced in the field of text summarization~, which allows the model to learn to detect important words or phrases in the source side and directly copy them to the target side with certain probabilities.  Current state-of-the-art abstractive summarization model mainly utilize pre-trained language models. For example, use the pretrained BERT model~ to initialize the encoder of summarization models and then fine-tune the model on summarization datasets. Similarly, pretrain a language model with a denoising autoencoding objective and fine-tune it on summarization datasets.         \pfliu{Or, we can also organize our related work section in this way and put it here.}",101
" In recent years, abstractive summarization  has made impressive progress with the development of sequence-to-sequence  framework . This framework is composed by an encoder and a decoder. The encoder processes the source text and extracts the necessary information for the decoder, which then predicts each word in the summary. Thanks to their generative nature, abstractive summaries can include novel expressions never seen in the source text, but at the same time, abstractive summaries are more difficult to produce compared with extractive summaries  which formed by directly selecting a subset of the source text. It has been also found that seq2seq-based abstractive methods usually struggle to generate out-of-vocabulary  words or rare words, even if those words can be found in the source text. Copy mechanism  can alleviate this problem and meanwhile maintain the expressive power of the seq2seq framework. The idea is to allow the decoder not only to generate a summary from scratch but also copy words from the source text.  Though effective in English text summarization, the copy mechanism remains relatively undeveloped in the summarization of some East Asian languages e.g. Chinese. Generally speaking, abstractive methods for Chinese text summarization comes in two varieties, being word-based and character-based. Since there is no explicit delimiter in Chinese sentence to indicate word boundary, the first step of word-based methods  is to perform word segmentation . Actually, in order to avoid the segmentation error and to reduce the size of vocabulary, most of the existing methods are character-based . When trying to combine the character-based methods in Chinese with copy mechanism, the original ``word copy'' degrades to ``character copy'' which does not guarantee a multi-character word to be copied verbatim from the source text . Unfortunately, copying multi-character words is quite common in Chinese summarization tasks. Take the Large Scale Chinese Social Media Text Summarization Dataset   as an example, according to Table I, about 37\% of the words in the summaries are copied from the source texts and consist of multiple characters.    		} 	\end{center}  	   	 \end{table}  Selective read  was proposed to handle this problem. It calculates the weighted sum of encoder states corresponding to the last generated character and adds this result to the input of the next decoding step. Selective read can provide location information of the source text for the decoder and help it to perform the consecutive copy. A disadvantage of this approach, however, is that it increases reliance of present computation on partial results before the current step which makes the model more vulnerable to the errors accumulation and leads to exposure bias during inference.  Another way to make copied content consecutive is through directly copying text spans. Zhou et al.  implement span copy operation by equipping the decoder with a module that predicts the start and end positions of the span. Because a longer span can be decomposed to shorter ones, there are actually many different paths to generate the same summary during inference, but their model is optimized by only the longest common span at each time step during training, which exacerbates the discrepancy between two phases. In this work, we propose a novel lexicon-constrained copying network . The decoder of LCN can copy either a single character or a text span at a time, and we constrain the text span to match a potential multi-character word. Specifically, given a text and several off-the-shell word segmentators, if a text span is included in any segmentation result of the text, we consider it as a potential word. By doing so, the number of available spans is significantly reduced, making it is viable to marginalize over all possible paths during training. Furthermore, during inference, we aggregate all partial paths on the fly that producing the same output using a word-enhanced beam search algorithm, which encourages the model to copy multi-character words and facilitates the parallel computation.  To be in line with the aforementioned decoder, the encoder should be revised to learn the representations of not only characters but also multi-character words. In the context of neural machine translation, Su et al.  first organized characters and multi-character words in a directed graph named word-lattice. Following Xiao et al. , we adopt an encoder based on the Transformer  to take the word-lattice as input and allow each character and word to have its own hidden representation. By taking into account relative positional information when calculating self-attention, our encoder can capture both global and local dependencies among tokens, providing an informative representation of source text for the decoder to make copy decisions.   Although our model is character-based , it can directly utilize word-level prior knowledge, such as keywords. In our setting, keywords refer to words in the source text that have a high probability of inclusion in the summary. Inspired by Gehrmann et al. , we adopt a separate word selector based on the large pre-trained language model, e.g. BERT  to extract keywords. When the decoder intends to copy words from the source text, those selected keywords will be treated as candidates, and other words will be masked out.  Experimental results show that our model can achieve better performance when incorporating with the word selector.     Most existing neural methods to abstractive summarization fall into the sequence to sequence framework. Among them, models based on recurrent neural networks   are more common than those built on convolutional neural network  , because the former models can more effectively handle long sequences. Attention  is easily integrated with RNNs and CNNs, as it allows the model to focus more on salient parts of the source text . Also, it can serve as a pointer to select words in the source text for copying . In particular, architectures that are constructed entirely of attention, e.g. Transformer  can be adopted to capture global dependencies between source text and summary .    Prior knowledge has proven helpful for generating informative and readable summaries. Templates that are retrieved from training data can guide summarization process at the sentence-level when encoded in conjunction with the source text . Song et al.  show that the syntactic structure can help to locate the content that is worth keeping in the summary, such as the main verb.  Keywords are commonly used in Chinese text summarization. When the decoder is querying from the source representation, Wang and Ren  use the keywords extracted by the unsupervised method to exclude noisy and redundant information. Deng et al.  propose a word-based model that not only utilizes keywords in the decoding process, but also adds the keywords produced by the generative method into the vocabulary in the hope of alleviating out of vocabulary  problem. Our model is drastically different from the above two models in terms of the way keywords being extracted and encoded.  The most related works are in the field of neural machine translation, in which many researchers resort to the assistance of multi-granularity information.   On the source side, Su et al.  use an RNN-based network to encode the word-lattice, an input graph that contains both word and character. Xiao et al.  apply the lattice-structured input to the Transformer  and generalize the lattice to construct at a subword level. To fully take advantage of multi-head attention in the Transformer, Nguyen et al. first partition input sequence to phrase fragments based on n-gram type and then allow each head to attend to either one certain n-gram type or all different n-gram types at the same time. In addition to n-gram phrases, the multi-granularity self-attention proposed by Hao et al.  also attends to syntactic phrases obtained from syntactic trees to enhance structure modeling.  On the target side, when the decoder produces an UNK symbol which denotes a rare or unknown word, Luong et al. restore it to a natural word using a character-level component.  Srinivasan et al.  adopt multiple decoders that map the same input into translations at different subword-levels, and combine all the translations into the final result, trying to improve the flexibility of the model without losing semantic information. While our model and the above models all utilize multi-granularity information, our model differs at that we impose a lexical constraint on both encoding and decoding.       We propose a general framework for guided neural summarization, using which we investigate four types of guidance signals and achieve state-of-the-art performance on various popular datasets. We demonstrate the complementarity of the four guidance signals, and find that our models can generate more novel words and more faithful summaries. We also show that we can control the output by providing user-specified guidance signals.  Given the generality of our framework, this opens the possibility for several future research directions including 1) developing strategies to ensemble models under different guidance signals; 2) incorporating sophisticated techniques such as copy or coverage over the source document, the guidance signal, or both; and 3) experimenting with other kinds of guidance signals such as salient elementary discourse units.  
"," Most existing neural methods to abstractive summarization fall into the sequence to sequence framework. Among them, models based on recurrent neural networks   are more common than those built on convolutional neural network  , because the former models can more effectively handle long sequences. Attention  is easily integrated with RNNs and CNNs, as it allows the model to focus more on salient parts of the source text . Also, it can serve as a pointer to select words in the source text for copying . In particular, architectures that are constructed entirely of attention, e.g. Transformer  can be adopted to capture global dependencies between source text and summary .    Prior knowledge has proven helpful for generating informative and readable summaries. Templates that are retrieved from training data can guide summarization process at the sentence-level when encoded in conjunction with the source text . Song et al.  show that the syntactic structure can help to locate the content that is worth keeping in the summary, such as the main verb.  Keywords are commonly used in Chinese text summarization. When the decoder is querying from the source representation, Wang and Ren  use the keywords extracted by the unsupervised method to exclude noisy and redundant information. Deng et al.  propose a word-based model that not only utilizes keywords in the decoding process, but also adds the keywords produced by the generative method into the vocabulary in the hope of alleviating out of vocabulary  problem. Our model is drastically different from the above two models in terms of the way keywords being extracted and encoded.  The most related works are in the field of neural machine translation, in which many researchers resort to the assistance of multi-granularity information.   On the source side, Su et al.  use an RNN-based network to encode the word-lattice, an input graph that contains both word and character. Xiao et al.  apply the lattice-structured input to the Transformer  and generalize the lattice to construct at a subword level. To fully take advantage of multi-head attention in the Transformer, Nguyen et al. first partition input sequence to phrase fragments based on n-gram type and then allow each head to attend to either one certain n-gram type or all different n-gram types at the same time. In addition to n-gram phrases, the multi-granularity self-attention proposed by Hao et al.  also attends to syntactic phrases obtained from syntactic trees to enhance structure modeling.  On the target side, when the decoder produces an UNK symbol which denotes a rare or unknown word, Luong et al. restore it to a natural word using a character-level component.  Srinivasan et al.  adopt multiple decoders that map the same input into translations at different subword-levels, and combine all the translations into the final result, trying to improve the flexibility of the model without losing semantic information. While our model and the above models all utilize multi-granularity information, our model differs at that we impose a lexical constraint on both encoding and decoding.",102
"  Humans are not supervised by the natural language inference . Supervision is necessary for applications in human-defined domains. For example, humans need the supervision of what is a noun before they do POS tagging, or what is a tiger in Wordnet before they classify an image of tiger in ImageNet. However, for NLI, people are able to entail that \textcircled{a} A man plays a piano contradicts \textcircled{b} A man plays the clarinet for his family without any supervision from the NLI labels. In this paper, we define such inference as a more general process of establishing associations and inferences between texts, rather than strictly classifying whether two sentences entail or contradict each other. Inspired by this, we raise the core problem in this paper: Given a pair of natural language sentences, can machines entail their relationship without any supervision from inference labels?   In his highly acclaimed paper, neuroscientist Moshe Bar claims that ``predictions rely on the existing scripts in memory, which are the result of real as well as of previously imagined experiences''. The exemplar theory argues that humans use {\bf similarity} to recognize different objects and make decisions.   Analogy helps humans understand a novel object by linking it to a similar representation existing in memory. Such linking is facilitated by the object itself and its context. Context information has been widely applied in self-supervision learning . Adapting context to NLI is even more straightforward. A simple idea of {\bf constant conjunction} is that A causes B if they are constantly conjoined. Although constant conjunction contradicts ``correlation is not causation'', modern neuroscience has confirmed that humans use it for reasoning in their mental world. For example, they found an increase in synaptic efficacy arises from a presynaptic cell's repeated and persistent stimulation of a postsynaptic cell in Hebbian theory. As to the natural language, the object and its context can be naturally used to determine the inference. For example, \textcircled{a} contradicts \textcircled{b} because they cannot happen simultaneously in the same {\bf context}.  The context representation learned by SSL  has already achieved big success in NLP. From the perspective of context, these models learn the sentence level contextual information  and the word level contextual information .  Besides linguistic contexts, humans also link other modalities  to novel inputs. Even if the goal is to reason about plain texts, other modalities still help . For example, if only textual information is used, it is difficult to entail the contradiction between \textcircled{a} and \textcircled{b}. We need the commonsense that a man only has two arms, which cannot play the piano and clarinet simultaneously. This commonsense is hard to obtain from the text. However, if we link the sentences to their visual scenes, the contradiction is much clearer because the two scenes cannot happen in the same visual context. We think it is necessary to incorporate other modalities for the unsupervised natural language inference.  The idea of adapting multimodal in SSL is not new.  According to, we briefly divide previous multimodal SSL approaches into two categories based on their encoder infrastructures. As shown in Fig., the first category uses one joint encoder to represent the multimodal inputs. Obviously, if the downstream task is only for plain text, we cannot extract the representation of text separately from the joint encoder. So the first category is infeasible for the natural language inference. The second category first encodes the text and the image separately by two encoders. Then it represents the multimodal information via a joint encoder over the lower layer encoders. This is shown in Fig.. Although the textual representation can be extracted from the text encoder in the lower layer, such representation does not go through the joint learning module and contains little visual knowledge. In summary, the encoders in previous multimodal SSL approaches are coupled. If only textual inputs are given, they cannot effectively incorporate visual knowledge in their representations. Thus their help for entailing the contradiction between \textcircled{a} and \textcircled{b} is limited.    In order to benefit from multimodal data in plain text inference, we propose the \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning  network. This is shown in Fig.. Its text encoder is decoupled, which only takes the plain text as inputs. Thus it can be directly adapted to downstream NLI tasks. Besides, we use multimodal contrastive loss between the text encoder and the image encoder, thereby forcing the text representation to align with the corresponding image. Therefore even if the text encoder in MACD only takes the plain text as input, it still represents visual knowledge. In the downstream plain text inference tasks, without taking images as input, the text encoder of MACD still implicitly incorporating the visual knowledge learned by the multimodal contrastive loss. Note that we do not need a decoupled image encoder in the SSL. So the image encoder in Fig. in MACD takes texts as inputs to provides a more precise image encoder. We will elaborate this in section.       {\bf Unsupervised Natura Language Representation Learning} Unsupervised learning has become a new paradigm for natural language representation learning. On many NLP tasks, using unsupervised learning as pre-training has achieved state-of-the-art results. Recent related researches includes the use of larger unsupervised datasets and larger network structures to improve model performance. Other studies use knowledge distillation or adversarial learning to compress model parameters or accelerate model training.  The success of these efforts has undoubtedly led to the use of self-supervised learning for multimodal learning in this paper. We emphasize that multimodal learning is essential for more accurate understanding of text by representing its visual context. Besides, the pre-trained unsupervised learning model  is used for the parameter initialization of the text encoder in this paper.  {\bf Multimodal Pre-training} Recently, some studies have extended unsupervised natural language representation learning to the multimodal settings, including image, and video. Due to the great success of BERT, these studies often follow its parameters and network architecture.  uses a co-attention transformer layer to extend the standard transformer layer. This new network architecture uses two joint transformers to simultaneously model visual and linguistic features.  further treats visual elements and text elements as one sequence, and directly uses a standard transformer to model its joint representation, and uses cloze tasks for joint representation learning.  However, these studies have two problems in solving the problems raised in this paper. First, in their models, the encoders of language features and visual features are coupled. This limits that the downstream tasks of the model must also be multimodal problems. But we want to solve the plain text inference problems. In contrast, the language feature encoder proposed in this paper is decoupled and encode the plain text along. Second, the training of these models only considers text-visual joint task training, which leads to catastrophic forgetting when reasoning over the natural language. In this paper, we propose the lifelong learning regularization to anchor the natural language. In this way we avoid the catastrophic forgetting problem when learning new visual knowledge.  {\bf Self-supervised Learning via Contrastive Learning} Self-supervised learning is widely used in tasks such as unsupervised image classification, temporal prediction, and image inpainting. Contrastive learning has become a popular toolkit for this problem. Its core idea is to learn the representation to reconstruct of identify some part of the data. Recently, this goal was formalized to maximize mutual information between different pieces of information. Because the mutual information is intractable, some approximations have been proposed, including Noise-Contrastive Estimation, Donsker-Varadhan estimator, and Jensen-Shannon estimator.  Although these studies have not been directly applied to the multi-modal data of text2image, some studies also focus on the multimodal data. However, their inference of text and other modalities are coupled, which makes it infeasible to represent text with visual knowledge when only text is used as input.    consider the structured correlations between different modalities, and often treat the data of different modalities as some orthogonal views. Since our problem is for self-supervised learning over text2image data, their matchings have a typical structured relationship. We need to propose new algorithms to carefully deal with the structural correlations.        This is file `sample-sigconf.tex',    generated with the docstrip utility.       The original source files were:       samples.dtx         IMPORTANT NOTICE:       For the copyright see the source file.       Any modified versions of this file must be renamed    with new filenames distinct from sample-sigconf.tex.       For distribution of the original source see the terms    for copying and modification in the file samples.dtx.       This generated file may be distributed as long as the    original source files, as listed above, are part of the    same distribution.        The first command in your LaTeX source must be the \documentclass command. \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small}  \usepackage{microtype}  \usepackage{amsmath,amsfonts,amssymb} \usepackage[space]{grffile} \usepackage{subcaption} \captionsetup{compatibility=false} \usepackage{courier} \usepackage{amsthm} \usepackage{xcolor} \usepackage{listings}  \usepackage{subfigure} \usepackage{epstopdf} \usepackage{epsfig} \usepackage{amsmath} \usepackage{multirow} \usepackage{bm} \usepackage{adjustbox} \usepackage{enumitem} \usepackage{amssymb} \usepackage{caption} \usepackage{framed} \usepackage{tabularx,ragged2e} \newcolumntype{C}{>{\Centering\arraybackslash}X}   centered ""X"" column  \relpenalty=10000 \binoppenalty=10000  \widowpenalty=10000 \clubpenalty=10000       \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{    \providecommand\BibTeX{{      \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}     Rights management information.  This information is sent to you    when you complete the rights form.  These commands have SAMPLE    values in them; it is your responsibility as an author to replace    the commands and values with those provided to you when you    complete the rights form.  \DeclareMathOperator*{\argmax}{\arg\!\max} \DeclareMathOperator*{\argmin}{\arg\!\min} \newcommand{\nop}[1]{} \newcommand{\cwy}[1]{{{ #1}}} \newcommand{\loss}[0]{{\mathcal{L}}}  \aclfinalcopy        Submission ID.    Use this when submitting an article to a sponsored event. You'll    receive a unique submission ID from the organizers    of the event, and this ID should be used as the parameter to this command.   \acmSubmissionID{123-A56-BU3}        The majority of ACM publications use numbered citations and    references.  The command \citestyle{authoryear} switches to the    ""author year"" style.       If you are preparing content for an event    sponsored by ACM SIGGRAPH, you must use the ""author year"" style of    citations and references.    Uncommenting    the next command will enable that style.   \citestyle{acmauthoryear}        end of the preamble, start of the body of the document source.               Keywords. The author should pick words that accurately describe    the work being presented. Separate the keywords with commas.  \keywords{self-supervised learning, multimodal, natural language inference}     A ""teaser"" image appears between the author and affiliation    information and the body of the document, and typically spans the    page.        This command processes the author and affiliation and title    information and builds the first part of the formatted document.              {\small   In this paper, we propose a novel lexicon-constrained copying network for Chinese summarization. Querying the multigranularity representation learned by our encoder, our decoder can copy either a character or a multi-character word at each time step. Experiments on the LCSTS dataset show that our model is superior to the Transformer baselines and quite competitive with the latest models. With the help of keyword information provide by the word selector, it can even achieve state-of-the-art performance. In the future, we plan to apply our model to other tasks, such as comment  generation, and to other languages, such as English.                      \hfill mds   \hfill August 26, 2015       An example of a floating figure using the graphicx package.   Note that \label must occur AFTER  \caption.   For figures, \caption should occur after the \includegraphics.   Note that IEEEtran v1.7 and later has special internal code that   is designed to preserve the operation of \label within \caption   even when the captionsoff option is in effect. However, because   of issues like this, it may be the safest practice to put all your   \label just after \caption rather than within \caption{}.     Reminder: the ""draftcls"" or ""draftclsnofoot"", not ""draft"", class   option should be used if it is desired that the figures are to be   displayed while in draft mode.        Note that the IEEE typically puts floats only at the top, even when this   results in a large percentage of a column being occupied by floats.     An example of a double column floating figure using two subfigures.      The subfigure \label commands are set within each subfloat command,   and the \label for the overall figure must come after \caption.   \hfil is used as a separator to get equal spacing.   Watch out that the combined width of all the subfigures on a   line do not exceed the text width or a line break will occur.         Note that often IEEE papers with subfigures do not employ subfigure   captions , but instead will   reference/describe all of them , , etc., within the main caption.   Be aware that for subfig.sty to generate the , , etc., subfigure   labels, the optional argument to \subfloat must be present. If a   subcaption is not desired, just leave its contents blank,   e.g., \subfloat[].     An example of a floating table. Note that, for IEEE style tables, the   \caption command should come BEFORE the table and, given that table   captions serve much like titles, are usually capitalized except for words   such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to   and up, which are usually not capitalized unless they are the first or   last word of the caption. Table text will default to \footnotesize as   the IEEE normally uses this smaller font for tables.   The \label must come after \caption as always.           Note that the IEEE does not put floats in the very first column   - or typically anywhere on the first page for that matter. Also,   in-text middle  positioning is typically not used, but it   is allowed and encouraged for Computer Society conferences . Most IEEE journals/conferences use   top floats exclusively.   Note that, LaTeX2e, unlike IEEE journals/conferences, places   footnotes above bottom floats. This can be corrected via the   \fnbelowfloat command of the stfloats package.  
","  {\bf Unsupervised Natura Language Representation Learning} Unsupervised learning has become a new paradigm for natural language representation learning. On many NLP tasks, using unsupervised learning as pre-training has achieved state-of-the-art results. Recent related researches includes the use of larger unsupervised datasets and larger network structures to improve model performance. Other studies use knowledge distillation or adversarial learning to compress model parameters or accelerate model training.  The success of these efforts has undoubtedly led to the use of self-supervised learning for multimodal learning in this paper. We emphasize that multimodal learning is essential for more accurate understanding of text by representing its visual context. Besides, the pre-trained unsupervised learning model  is used for the parameter initialization of the text encoder in this paper.  {\bf Multimodal Pre-training} Recently, some studies have extended unsupervised natural language representation learning to the multimodal settings, including image, and video. Due to the great success of BERT, these studies often follow its parameters and network architecture.  uses a co-attention transformer layer to extend the standard transformer layer. This new network architecture uses two joint transformers to simultaneously model visual and linguistic features.  further treats visual elements and text elements as one sequence, and directly uses a standard transformer to model its joint representation, and uses cloze tasks for joint representation learning.  However, these studies have two problems in solving the problems raised in this paper. First, in their models, the encoders of language features and visual features are coupled. This limits that the downstream tasks of the model must also be multimodal problems. But we want to solve the plain text inference problems. In contrast, the language feature encoder proposed in this paper is decoupled and encode the plain text along. Second, the training of these models only considers text-visual joint task training, which leads to catastrophic forgetting when reasoning over the natural language. In this paper, we propose the lifelong learning regularization to anchor the natural language. In this way we avoid the catastrophic forgetting problem when learning new visual knowledge.  {\bf Self-supervised Learning via Contrastive Learning} Self-supervised learning is widely used in tasks such as unsupervised image classification, temporal prediction, and image inpainting. Contrastive learning has become a popular toolkit for this problem. Its core idea is to learn the representation to reconstruct of identify some part of the data. Recently, this goal was formalized to maximize mutual information between different pieces of information. Because the mutual information is intractable, some approximations have been proposed, including Noise-Contrastive Estimation, Donsker-Varadhan estimator, and Jensen-Shannon estimator.  Although these studies have not been directly applied to the multi-modal data of text2image, some studies also focus on the multimodal data. However, their inference of text and other modalities are coupled, which makes it infeasible to represent text with visual knowledge when only text is used as input.    consider the structured correlations between different modalities, and often treat the data of different modalities as some orthogonal views. Since our problem is for self-supervised learning over text2image data, their matchings have a typical structured relationship. We need to propose new algorithms to carefully deal with the structural correlations.        This is file `sample-sigconf.tex',    generated with the docstrip utility.       The original source files were:       samples.dtx         IMPORTANT NOTICE:       For the copyright see the source file.       Any modified versions of this file must be renamed    with new filenames distinct from sample-sigconf.tex.       For distribution of the original source see the terms    for copying and modification in the file samples.dtx.       This generated file may be distributed as long as the    original source files, as listed above, are part of the    same distribution.        The first command in your LaTeX source must be the \documentclass command. \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small}  \usepackage{microtype}  \usepackage{amsmath,amsfonts,amssymb} \usepackage[space]{grffile} \usepackage{subcaption} \captionsetup{compatibility=false} \usepackage{courier} \usepackage{amsthm} \usepackage{xcolor} \usepackage{listings}  \usepackage{subfigure} \usepackage{epstopdf} \usepackage{epsfig} \usepackage{amsmath} \usepackage{multirow} \usepackage{bm} \usepackage{adjustbox} \usepackage{enumitem} \usepackage{amssymb} \usepackage{caption} \usepackage{framed} \usepackage{tabularx,ragged2e} \newcolumntype{C}{>{\Centering\arraybackslash}X}   centered ""X"" column  \relpenalty=10000 \binoppenalty=10000  \widowpenalty=10000 \clubpenalty=10000       \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{    \providecommand\BibTeX{{      \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}     Rights management information.  This information is sent to you    when you complete the rights form.  These commands have SAMPLE    values in them; it is your responsibility as an author to replace    the commands and values with those provided to you when you    complete the rights form.  \DeclareMathOperator*{\argmax}{\arg\!\max} \DeclareMathOperator*{\argmin}{\arg\!\min} \newcommand{\nop}[1]{} \newcommand{\cwy}[1]{{{ #1}}} \newcommand{\loss}[0]{{\mathcal{L}}}  \aclfinalcopy        Submission ID.    Use this when submitting an article to a sponsored event. You'll    receive a unique submission ID from the organizers    of the event, and this ID should be used as the parameter to this command.   \acmSubmissionID{123-A56-BU3}        The majority of ACM publications use numbered citations and    references.  The command \citestyle{authoryear} switches to the    ""author year"" style.       If you are preparing content for an event    sponsored by ACM SIGGRAPH, you must use the ""author year"" style of    citations and references.    Uncommenting    the next command will enable that style.   \citestyle{acmauthoryear}        end of the preamble, start of the body of the document source.               Keywords. The author should pick words that accurately describe    the work being presented. Separate the keywords with commas.  \keywords{self-supervised learning, multimodal, natural language inference}     A ""teaser"" image appears between the author and affiliation    information and the body of the document, and typically spans the    page.        This command processes the author and affiliation and title    information and builds the first part of the formatted document.              {\small",103
"  %閺鍙ラ嚋閸ユ拝绱濋弰顖氱秼閸撳秶娈戞径姘侀崹瀣劥缂冨弶鍎忛崘纰夌礉鐠侇厾绮屾稉娑擃亜銇囧Ο鈥崇烽敍灞藉晙閽傛悂顩撮崚鐧楁稉顏勭毈濡崇烽敍灞剧槨娑擃亜鐨Ο鈥崇烽崘宥呭礋閻欘剟鍣洪崠...  閹存垳婊戦惃鍕煙濞夋洩绱濈拋顓犵矊娑撴稉顏勩亣濡崇烽敍瀹杋netune鏉╂瑤閲滄径褎膩閸ㄥ鎮撻弮鍫曞倸绨睳娑擃亙绗夐崥灞剧箒鎼达妇娈戠亸蹇斈侀崹瀣剁礉閸欘亪娓剁电绻栨稉娑擃亝膩閸ㄥ绻樼悰宀勫櫤閸...   As neural machine translation models become heavier and heavier , we have to resort to model compress techniques  to deploy  smaller models in devices with limited resources, such as mobile phones. However, a practical challenge is that the hardware conditions of different devices vary greatly. To ensure the same calculation latency, customizing distinct model sizes  for different devices is necessary, which leads to huge model training and maintenance costs . For example, we need to distill the pre-trained large model into N individual small models.  %Then some model post-processing steps, such as model pruning  and quantization , are also performed independently for each small model.  The situation becomes worse for the industry when considering more translation directions and more frequent model iterations.  An ideal solution is to train a single model that can run in different model sizes. Such attempts have been explored in SlimNet  and LayerDrop . SlimNet allows running in four width configurations by joint training of these width networks, while LayerDrop can decode with any depth configuration by applying Dropout  on layers during training.      In this work, we take a further step along the line of flexible depth network like LayerDrop.  As shown in Figure, we first demonstrate that when there is a large gap between the predefined layer dropout during training and the actual pruning ratio during inference, LayerDrop's performance is poor.  %We attribute it to huge sub-network training space and mismatch between random sampling training and deterministic inference.  To solve this problem, we propose to use multi-task learning to train a flexible depth model by treating each supported depth configuration as a task. We reduce the supported depth space for the aggressive model compression rate and propose an effective deterministic sub-network assignment method to eliminate the mismatch between training and inference in LayerDrop. %Specifically, we design two metrics to determine which sub-network assignment is good.  Experimental results on deep Transformer  show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and LayerDrop.        鏉╂瑥娼￠惄绋垮彠閻ㄥ嫪绗夋径姘剧礉閸欘垯浜掔拋韫瑓 mnmt閿    In NMT, most previous developments on model compression focus on pruning  and distillation , which generally pre-train a large model and then fine-tune or retrain independently on different model sizes. However, this paradigm has complex training pipelines and high-cost model maintenance.   Our goal is to directly train a single model without any post-processing to fit various network capacities. Although this topic is attractive and particularly useful to the industry, we notice that less attention has been paid on it.  The two most relevant studies are SlimNet  and LayerDrop . SlimNet proposes swithable batch normalization to jointly train 4 widths convolution neural networks together . In contrast, since NMT model includes both encoder and decoder, the total task number is .  For example, there are 6x4 tasks for a 12-layer encoder and a 6-layer decoder. Therefore, multi-task learning on depth in our scenerio is more difficult. LayerDrop randomly drops layers to regularize deep networks, whose by-product is that small networks with any depth can be used directly without any finetuning. We note that similar technique has been used in Stochastic Depth , but its purpose is to accelerate very deep network training.  \fi    Our approach is also related to adaptive computation, which learns to skip unnecessary computations at runtime. ACT proposed by \citet{graves2016adaptive} adds a gating to indicate the completion of current feature extraction. When it reaches to 1, it means that there is no need to go through the subsequent layers. This idea has been successfully applied to RNN , CNN  and Transformer layers .  BranchyNet  puts some ``early-exit'' points on the middle of the network and immediately stop forward computation once the early layer gives a confident prediction.  BlockDrop  resorts to policy gradient to learn which layers should be pruned according to the features of the input image.   ``Learning strict identity mapping'' prunes a layer where all elements in its output are lower than a threshold.   However, all the above methods need dynamically select the pruned layers for each sample, which is unfriendly for parallel computations in modern graphics processing units.     In this paper, we study the multimodal self-supervised learning for unsupervised NLI.  The major flaw of previous multimodal SSL methods is that they use a joint encoder for representing the cross-modal correlations. This prevents us from integrating visual knowledge into the text encoder. We propose the multimodal aligned contrastive decoupled learning , which learns to represent visual knowledge while using only texts as inputs. In the experiments, our proposed approach steadily surpassed other methods by a large margin.     
","   閺夆晜鐟ュ锟犳儎缁嬪灝褰犻柣銊ュ缁楀寰勫鍓х闁告瑯鍨禍鎺旀媼闊厾鐟 mnmt闁    In NMT, most previous developments on model compression focus on pruning  and distillation , which generally pre-train a large model and then fine-tune or retrain independently on different model sizes. However, this paradigm has complex training pipelines and high-cost model maintenance.   Our goal is to directly train a single model without any post-processing to fit various network capacities. Although this topic is attractive and particularly useful to the industry, we notice that less attention has been paid on it.  The two most relevant studies are SlimNet  and LayerDrop . SlimNet proposes swithable batch normalization to jointly train 4 widths convolution neural networks together . In contrast, since NMT model includes both encoder and decoder, the total task number is .  For example, there are 6x4 tasks for a 12-layer encoder and a 6-layer decoder. Therefore, multi-task learning on depth in our scenerio is more difficult. LayerDrop randomly drops layers to regularize deep networks, whose by-product is that small networks with any depth can be used directly without any finetuning. We note that similar technique has been used in Stochastic Depth , but its purpose is to accelerate very deep network training.  \fi    Our approach is also related to adaptive computation, which learns to skip unnecessary computations at runtime. ACT proposed by \citet{graves2016adaptive} adds a gating to indicate the completion of current feature extraction. When it reaches to 1, it means that there is no need to go through the subsequent layers. This idea has been successfully applied to RNN , CNN  and Transformer layers .  BranchyNet  puts some ``early-exit'' points on the middle of the network and immediately stop forward computation once the early layer gives a confident prediction.  BlockDrop  resorts to policy gradient to learn which layers should be pruned according to the features of the input image.   ``Learning strict identity mapping'' prunes a layer where all elements in its output are lower than a threshold.   However, all the above methods need dynamically select the pruned layers for each sample, which is unfriendly for parallel computations in modern graphics processing units.",104
"  %\todo[inline]{Why predicting hate-speech is important in general?}  %\todo[inline]{why detecting hate-speech is important in Yahoo news and Yahoo finance?}   %  %What is the problem? %Why is it interesting and important? %Why is it hard?  %Why hasn't it been solved before?  %What are the key components of my approach and results? Also include any specific limitations.  %Hatespeech is speech that ``intended to insult, offend, or intimidate a person because of some trait "". The occurrence of hatespeech has been increasing. It has become easier than before to reach a large audience quickly via social media, causing an increase of the temptation for inappropriate behaviors such as hatespeech, and potential damage to social systems. In particular, hatespeech interferes with civil discourse and turns good people away. Furthermore, hatespeech in the virtual world can lead to physical violence against certain groups in the real world\footnote{https://www.nytimes.com/2018/10/31/opinion/caravan-hate-speech-bowers-sayoc.html}\footnote{https://www.washingtonpost.com/nation/2018/11/30/how-online-hate-speech-is-fueling-real-life-violence}, so it should not be ignored on the ground of freedom of speech.  To detect hatespeech, researchers developed human-crafted feature-based classifiers , and proposed deep neural network architectures . %Online service providers also strive to combat the hatespeech through ranking algorithms, filtering, and suspending or deactivating user accounts. \textcolor{red}{However, blah blah blah}. However, they might not explore all possible important features for hatespeech detection, ignored pre-trained language model understanding, or proposed uni-directional language models by reading from left to right or right to left.   %--> 2. Other deep model for hatespeech detection: either didn't understand fully hateful context , or  ignore pretrained language model understanding and/or uni-directionally understanding language models by reading from left to right or right to left .  Recently, the BERT  model  has achieved tremendous success in Natural Language Processing % . The key innovation of BERT is in applying the transformer to language modeling tasks. %It proposed to do language modeling through two tasks: predicting masked words and predicting the next sentence. A BERT model pre-trained on these language modeling tasks forms a good basis for further fine-tuning on supervised tasks such as machine translation and question answering, etc.  Recent work on hatespeech detection  has applied the BERT model and has shown its prominent results over previous hatespeech classifiers. However, we point out its two limitations in hatespeech detection domain. First, the previous studies  have shown that a hateful corpus owns distinguished linguistic/semantic characteristics compared to a non-hateful corpus. For instance, hatespeech sequences are often informal or even intentionally mis-spelled, so words in hateful sequences can sit in a long tail when ranking their uniqueness, and a comment can be hateful or non-hateful using the same words .  %For example, ``n1gger'' in the sentence ``i am not a `n1gger' as you have indicated'' is non-hateful, but ``n1gger'' in ``you all are such a n1gger!'' is hateful.  For example, ``dick'' in the sentence ``Nobody knew dick about what that meant'' is non-hateful, but ``d1ck'' in ``You are a weak small-d1cked keyboard warrior'' is hateful \footnote{It is important to note that this paper contains hate speech examples, which may be offensive to some readers. They do not represent the views of the authors. We tried to make a balance between showing less number of hate speech examples and illustrating the challenges in real-world applications.}.  Thus, to better understand hateful vocabularies and contexts, it is better to pre-train on a mixture of both hateful and non-hateful corpora. Doing so helps to overcome the limitation of using BERT models pre-trained on non-hateful corpora like English Wikipedia and BookCorpus. Second, even the smallest pre-trained BERT ``base'' model contains 110M parameters. It takes a lot of computational resources to pre-train, fine-tune, and serve.  %There have been recent efforts reducing  Some recent efforts aim to reduce  the complexity of BERT model with the knowledge distillation technique such as DistillBert  and TinyBert . In these methods, a pre-trained BERT-alike model is used as a teacher model, and a student  model  is trained to produce similar output to that of the teacher model. Unfortunately, while their complexity is reduced, the performance is also degraded in NLP tasks compared to BERT. Another direction is to use cross-layer parameter sharing, such as ALBERT . However, ALBERT's computational time is similar to BERT, since the number of layers remains the same as BERT; likewise, its inference is equally expensive.  Based on the above observation and analysis, we aim to investigate whether it is possible to achieve a better hatespeech prediction performance than state-of-the-art machine learning classifiers, including classifiers based on publicly available BERT model, while significantly reducing the number of parameters compared with the BERT model. By doing so, we believe that performing pre-training tasks from the ground up and on a hatespeech-related corpus would allow the model to understand hatespeech patterns better and enhance the predictive results. However, while language model pretraining tasks require a large scale corpus size, available hatespeech datasets are normally small: only 16K115K annotated comments . Thus, we introduce a large annotated hatespeech dataset with 1.4M comments extracted from Yahoo News and Yahoo Finance. To reduce the complexity, we reduce the number of layers and hidden size, and propose Quaternion-based Factorization mechanisms in BERT architecture. To further improve the model effectiveness and robustness, we introduce a multi-source ensemble-head fine-tuning architecture, as well as a target-based adversarial training.  %Internet platforms can  moderate user-generated content in the interest of the majority of their users, and the business needs. Through ranking algorithms, filtering, suspending or deactivating user accounts, many Internet companies strive to combat hatespeech. %Twitter, for example, has ""the twitter rules"", which states that ``Violence, harassment and other similar types of behavior discourage people from expressing themselves, and ultimately diminish the value of global public conversation."".  %To ensure that users have a positive experience on its properties, Verizon Media also has clear rules against hatespeech. %, which state that ``Don't use hatespeech. Hatespeech directly attacks a person or group on the basis of race, ethnicity, national origin, religion, disability, disease, age, sexual orientation, gender, or gender identity. As noted above, we're a diverse global community of many types of people, with different beliefs, opinions, sensitivities, and comfort levels. If you don't feel that you can abide by our Community Guidelines as outlined below, maybe participating in the Oath community isn't for you."" %At Verizon Media, the Standard Moderation Platform  runs a platform service to moderate text, URL, images and videos. The hatespeech classifiers in SMP are based on  a number of past research work, including.   % The purpose of the work described in this paper is to improve the performance of the current state of the art for hatespeech classifiers. In a previous study, % we used a pretrained BERT  model as a starting point for fine tuning. % %, we investigated a range of different machine learning models for text classification. % %We show that a combination of a linear model, and % We found that the BERT architecture gives better performance than most baseline models, including. %, as well as Google's Prospective API. In that study, a pretrained BERT model is used as a starting point for fine tuning.  %Recently, the BERT  model has become a state-of-the-art language model and has achieved tremendous success in Natural Language Processing . %\todo[inline]{talking about BERT and its success in variety of nlp tasks with some cited works} %BERT is a modified transformer network architecture. Traditionally, many language tasks such as translation or question answering, are handled using recurrent neural networks, combined with the attention mechanism. This %reflects the fact that we tend to read a sentence from left to right. However, human also read words within context of other words, %some of them could be quite far apart, %instead of only from left to right or right to left in a mechanical way. Furthermore, recurrent network has a memory problem and can not handle long text, due to problems with vanishing or exploding gradient. In addition, it is intrinsically sequential, making the training process slow. Transformer network was proposed to solve these problems. In its setup, each word in the input text has visibility of all other words, through the use of multi-headed attention.   %It has been used %in a variety of NLP tasks as well as in other area such as image processing.     %One of the motivation of this paper is to investigate whether it is possible to achieve performance similar to, or better than the publicly available BERT models, but with smaller models. In doing so, we want to realize considerable saving in training and serving time. Another motivation is to see if it is possible to improve the BERT model further, by introducing changes to the model architecture. The third motivation is the following. The pretrained BERT models are based on % BooksCorpus and English Wikipedia. They have very different characteristics from the dataset of interest to us, which consists of users-generated comments in Yahoo News and Yahoo Finance. Consequently we believe that retraining a language model from scratch % should give us a model that understands % the language of our dataset better.  %\todo[inline]{talking about the limitation of BERT, like it is to complicated and heavy or has too many parameters. Then question is to build a better model, but with less number of parameters?}  The major contributions of our work are: \squishlist % \squishend  % We organize the paper as followed. % We give related work in Section, and % define the problem we are solving formerly in Section. We present our approach in Section, and show experimental results in Section. We conclude our paper in Section with discussions and future work.  %     hatespeech classification has drawn a lot of attention from the research community in past few years.  Hatespeech detection approaches mostly fall into   are mostly stemmed from  two types: approaches based on the use of lexicon, and\ methods based on the use of machine learning. The machine learning based methods are more dominant, so we focus on surveying these methods in this section.  Some of the earlier works in hatespeech detection have applied a variety of classical machine learning algorithms . Their intuition is to do feature engineering , then apply classification methods such as SVM, Random Forest, and Logistic Regression. The features are mostly Term-Frequency Inverse-Document-Frequency scores or Bag-of-Words vectors, and can be combined with additional features extracted from the user account's meta information and network structure . Those methods are suboptimal as they mainly rely on the quality and quantity of the human-crafted features.  Recent works have used deep neural network architectures for hatespeech detection  such as CNN , RNN  , combining CNN with RNN , or fine tuning a pretrained language models .    Recently, deep neural networks have attracted tremendous attention from the research community as they can automatically learn the latent representations from raw input information, which can be seen as latent features. In the hatespeech detection domain, recent works have used neural networks to classify hatespeech sequences using different neural architectures such as CNNs , RNN  , or combining CNN with RNN .  Another direction  in the domain of hatespeech classification focuses on the testing generalization of the current hatespeech classifiers , where those methods are tested in other datasets and domains such as Twitter data , Wikipedia data , Formspring data , and YouTube comment data .  Unlike previous works, we pre-train a hateful language model, then build a multi-source multi-head hatespeech classifier with regularized adversarial training to enhance the model's performance.   In this paper, we aim to improve the performance of  hatespeech classification, using language modeling and a large scale dataset consists of user-generated content from Yahoo News and Yahoo Finance. To our knowledge, this is the largest dataset for hatespeech classification task. Inspired by the recent success of the  BERT model , we exploit the BERT architecture to enhance the performance of our hatespeech classifiers. However, the lowest configuration of the pretrained BERT model still has a lot of parameters , which is not only expensive in terms of CPU/GPU and memory to fine-tuning, but also expensive  during the inference phase. Hence, we construct our proposed {HABERTOR} model with a much smaller number of parameters  and perform even better than the fine-tuning version of the pre-trained BERT model.     {WHy not pretraining distil bert on ground up}   {Add information on number of parameters in the statistic table}   {Adding more experiments on the transfer learning, and the running time too.}     In this paper, we have proposed a new multimodal English-Japanese corpus with comparable sentences. Based on the baseline performance of this data, we believe that current multimodal NMT models are not well suited to this type of task, and further research is required in order to better leverage the comparable sentences and images together in order to improve translation performance. In the future, we hope to see our corpus used to encourage research into multimodal machine translation tasks with comparable sentences instead of parallel sentences.   
","   hatespeech classification has drawn a lot of attention from the research community in past few years.  Hatespeech detection approaches mostly fall into   are mostly stemmed from  two types: approaches based on the use of lexicon, and\ methods based on the use of machine learning. The machine learning based methods are more dominant, so we focus on surveying these methods in this section.  Some of the earlier works in hatespeech detection have applied a variety of classical machine learning algorithms . Their intuition is to do feature engineering , then apply classification methods such as SVM, Random Forest, and Logistic Regression. The features are mostly Term-Frequency Inverse-Document-Frequency scores or Bag-of-Words vectors, and can be combined with additional features extracted from the user account's meta information and network structure . Those methods are suboptimal as they mainly rely on the quality and quantity of the human-crafted features.  Recent works have used deep neural network architectures for hatespeech detection  such as CNN , RNN  , combining CNN with RNN , or fine tuning a pretrained language models .    Recently, deep neural networks have attracted tremendous attention from the research community as they can automatically learn the latent representations from raw input information, which can be seen as latent features. In the hatespeech detection domain, recent works have used neural networks to classify hatespeech sequences using different neural architectures such as CNNs , RNN  , or combining CNN with RNN .  Another direction  in the domain of hatespeech classification focuses on the testing generalization of the current hatespeech classifiers , where those methods are tested in other datasets and domains such as Twitter data , Wikipedia data , Formspring data , and YouTube comment data .  Unlike previous works, we pre-train a hateful language model, then build a multi-source multi-head hatespeech classifier with regularized adversarial training to enhance the model's performance.   In this paper, we aim to improve the performance of  hatespeech classification, using language modeling and a large scale dataset consists of user-generated content from Yahoo News and Yahoo Finance. To our knowledge, this is the largest dataset for hatespeech classification task. Inspired by the recent success of the  BERT model , we exploit the BERT architecture to enhance the performance of our hatespeech classifiers. However, the lowest configuration of the pretrained BERT model still has a lot of parameters , which is not only expensive in terms of CPU/GPU and memory to fine-tuning, but also expensive  during the inference phase. Hence, we construct our proposed {HABERTOR} model with a much smaller number of parameters  and perform even better than the fine-tuning version of the pre-trained BERT model.     {WHy not pretraining distil bert on ground up}   {Add information on number of parameters in the statistic table}   {Adding more experiments on the transfer learning, and the running time too.}",105
"   %------------------Previous version------------------ %Since UNMT in low-resource domains is not yet an actively explored field, one may naively approach this problem by training a model on multiple domains and expect it to generalize on the unseen, low-resource domains, e.g., training the model on news and sports domains and evaluating on the biomedical domain. %However, due to domain mismatch, studied on supervised NMT, the model can show inferior performance. %------------------------------------------------------- Unsupervised neural machine translation  leverages unpaired monolingual corpora for its training, without requiring an already labeled, parallel corpus. Recently, the state of the art in UNMT has achieved comparable performances against supervised machine translation approaches. However, in the case of the translation of domain-specific documents, the monolingual data themselves are scarce, and collecting them involves high cost, still suffering from low NMT performance. For instance, a model trained with monolingual data in such a low-resource domain, say, the medical domain, can experience degraded translation quality due to overfitting.  %------------------Previous version------------------ %Another reasonable approach is transfer learning, which has been frequently used for domain adaption in the literature of supervised NMT and often showed improvements in the target domain. The model is pretrained with multiple domains and then finetuned with the new domain. However, this approach may suffer from overfitting  and catastrophic forgetting when given a small number of training data and a large domain gap in a downstream task. %-------------------------------------------------- Yet, UNMT for low-resource domains is not an actively explored field. One naive approach is to train a model on high-resource domains  while hoping it to generalize on an unseen low-resource domain  as well. However, it has been shown from recent studies on supervised NMT that a nontrivial domain mismatch can significantly cause low translation accuracy.  Another reasonable approach is transfer learning, or in particular domain adaptation, which has shown  performance improvements in the literature of supervised NMT. In this approach, the model is first pretrained using existing domains and then finetuned using the data in a new domain. However, this approach may suffer from overfitting and catastrophic forgetting due to a small number of training data and a large domain gap.  As an effective method for handling a small number of training data, meta-learning has shown its superiority in various NLP tasks, such as dialog generation, translation, and natural language understanding. However, to the best of our knowledge, it was not applied to tackle the UNMT tasks with a small number of training data, i.e., low-resource UNMT.   In response, this paper extends meta-learning approach for low-resource UNMT, called \toolnameMeta. The objective of \toolnameMeta is to find the optimal initialization for model parameters that can quickly adapt to a new domain even with only a small amount of monolingual data. To be specific, assuming that data from multiple source domains are available, which makes meta-learning applicable, we first pretrain the UNMT model with source domains based on \toolnameMeta and then finetune the model  using a target domain.   Moreover, we propose an improved meta-learning approach called \ourtoolname for low-resource UNMT by explicitly promoting common knowledge across multiple domains as well as generalizable knowledge from a particular domain to another. In particular, our proposed approach prevents the model from overfitting due to a small amount of training data in a new domain.   In summary, our contributions include the following.    %\item \ourtoolname shows that it has domain-general knowledge and is faster in convergence than all the other methods. %\item We empirically demonstrate that our enhanced algorithm, \ourtoolname, consequently boosts up the performance of low-resource UNMT against other baseline models including \toolnameMeta.  %\item We extend the meta-learning algorithm by incorporating the domain mixing loss, and it outperforms all the other methods. % %We show zero-shot performance to evaluate generalization ability for \ourtoolname, where \ourtoolname outperforms other methods.  % To the best of our knowledge, our work is the first to apply a meta-learning approach to UNMT tasks. Our proposed algorithms can quickly adapt to in-domain with only a few iteration steps. Both \toolnameMeta and \ourtoolname consistently outperform the baseline models up to 3 BLEU scores. Especially, \ourtoolname achieves promising results among others including \toolnameMeta. Besides, we show zero-shot performance to evaluate generalization ability for \ourtoolname, where \ourtoolname outperforms other methods.  %--------------------------------- % 闋冩﹥顫呮 姘╁牗妫撮瀽鎰冲妧闆 闇冨嫴饪洪灇 闈镐緛纾ら瀽鍡㈡緤 鐡垮婀㈣嚙 闇呮﹤濮 general 闋 feature闇屻倢婢 闉涘牕瀚.  % Although each domain is very distance each others in domain adaptation, they share some linguistic features, such as the grammar and basic words.     % Azam: To alleviate the aforementioned challenge, % Azam: To overcome this issue, many %   % \item  % \item 闉栧崐螠闉 contribtuion bullet point鎼 summary : % \item 1. formulate new task  % \item 2. New frame work proposed % \item 3. evaluate various domain. show fast adaptation and quality. %On the other hand, since unsupervised machine translation has attained comparable performance against supervised machine translation, fully unsupervised domain adaptation, which uses only monolingual data for both in-domain and out-of-domain, is more suitable to handle data-scarce languages. %yet has a challenge for data-scarce domains. In other words, an unsupervised domain adaptation task can handle data-scarce languages; however, it cannot resolve a challenge for low-resource domains. % %such that it alleviates the aforementioned challenge, building a parallel corpus. %Therefore, a fully unsupervised domain adaptation task, consisted of unpaired language corpus for both in-domain and out-of-domain, is more realistic setting than a supervised domain adaptation task.%and substantial effort to collect domain specific data. %A meta-learning algorithm is superior for low-resource data. Unlike domain adaptation, a meta-learning algorithm does not require in-domain data to learn initial parameters. It only asks few training samples to meta-train  the model. Collaborating a meta-learning algorithm with unsupervised machine translation    %Since we leverage cross-lingual language model pretraining  which allows the model to learn cross-lingual representations, our gradient updates are divided into two objective functions, back-translation and language modeling. % Several approaches have been proposed to resolve the scarcity problem. For instance, a data mixing can be one approach that aggregates high-resource and low-resource data and train the model to adequately translate the low-resource target language   % To overcome the data scarcity problem, one simple approach is a data mixing that aggregates high-resource and low-resource data and train the model to adequately translate the low-resource target language. The other approach is a transfer learning that first pretrains on high-resource data and fine-tunes the low-resource data. Although these aforementioned approaches explicitly tackle the low-resource challenge, the scarcity problem still remains in NMT because building parallel corpus with specialized expertise is costly expensive.   % In this paper, we leverage recent success of unsupervised NMT  that uses only monolingual corpus. Inspired by , we propose new task called low-resource UNMT. To the best of our knowledge, this is a first attempt    % To overcome this issue, unsupervised learning in NMT has been proposed to resolve the parallel data scarcity problem. However, this approach has a constraint that abundant monolingual corpus should be always available.   % In reality, monolingual corpus can be also scarce if the domains or languages are often used. %鑶﹂浛 闉涙劤鍔爩 闉氭尗婀 闈广倠鐛  % Although various approaches have been proposed to address a low-resource challenge , none of the works consider a low-resource unsupervised task in NMT. To the best of our knowledge, this is first the attempt which explicitly tackles the low-resource UNMT task.  % In this paper,    % When we translate a word to the different language, the semantic meaning can be changed. For instance, the meaning of word ""CNN"" is different in the domain of deep learning and news     % To overcome this issue, the abundant parallel data are required which are not easy to obtain. Recently, unsupervised NMT  studies show reasonable performance comparison to supervised NMT.     % Data mixing with high-resource and low-resource is one approach to handle the following issue. The other approach is a transferring learning method that first trains on high-resource datasets and fine-tunes on a low-resource datatset. However, the problem can still remain if the parallel data is scarce in domains or languages. To overcome the parallel data scarcity problem    % To overcome this issue, unsupervised learning in NMT has been proposed to resolve the the problem of insufficient parallel data. However, this approach assumes that obtaining monolingual corpus is always easier than acquiring parallel corpus. Since the languages can vary by domains , either monolingual or parallel data can be scarce.  % utilize monolingual corpus which assume that monolingual corpus are always available. However,     % In NMT, the data scarcity problem can be divided into two different training data scenarios, insufficient training parallel data and training data itself . To overcome the scarce parallel data issue, recent studies proposed to utilize monolingual corpus.    % the parallel data is essential to train the NMT model    % several learning methods, such as unsupervised learning and transfer learning in NMT, are proposed to overcome the data scarcity problem. However, those works only consider in languages which still remains the problem in domains . Moreover, to the best of our knowledge, none of the works attempt to  %Learning is an inevitable phase when we adapt to a new task. However, various learning experiences can reduce the exertion of learning a new task.         %   %Domains can be  %To overcome this issue, one simple approach is a domain mixing that aggregates high-resource and low-resource domains and train the model to adequately translate the low-resource domain. The other approach is a transfer learning that first pretrains on high-resource domains and fine-tunes the low-resource domain.  %Despite the remarkable success on neural machine translation ~, the performance of NMT drops substantially against traditional statistical machine translation  when the training data is scarce  %To overcome the scarcity of training data in languages, variants of multilingual translation approaches have been proposed. These approaches basically exploit high-resource knowledge by aggregating both high-resource and low-resource data to train one single model. The other approach is utilizing transfer learning that the model first pretrains on high-resource data and later fine-tunes on low-resource data. The similar manner follows for the domains as well.  %Moreover, few-shot learning and meta-learning arise in machine learning where both attempt to handle the data scarcity problem. In NMT, ~\citet{gu2018meta} re-formulates the model-agnostic meta-learning  algorithm to resolve the low-resource challenge for NMT.  %Although aforementioned approaches tackle the low-resource challenge, the data scarcity problem can still remain because following approaches require parallel data, and building a low-resource language pair  with specialized expertise is costly expensive. Hence, the recent research suggests to rely only on monolingual corpus instead of using parallel corpus. The various unsupervised NMT ~ studies show the reasonable performance comparison to supervised NMT. % 闋冩﹥顫呮 monolingual corpus 姣靛牗鐖涢渻 闆垮姙婢婇煰 鐡寸粖鏅爢鍕冲剨闉 闉氭鏌庨洰鐘惧灛闉涘牗娼 -鏀磋兂鏌涢瀼鎸 鏀撮附寮版皡鏃嶆緫闉 闉濅緟娼夐澗姗冪抽浖..?  %sufficient in-domain data to train the model; however, in real-world, collecting domain specific data requires substantial effort. %building a low-resource language pair  with specialized expertise is costly expensive  % 鏂撴粔鑺 闇屻倢鏌 MT闉 姘氭粚鐖犻灇 闈奉剢鐎 姘╁牗妫撮澗姗冾槬鏃矅顫 闇冨嫴瀚 闋冩﹥姒鹃灇 . % Machine learning 闉愭劤鍔闉 Data scarcity 姘嶈兂鐗 闉濇粔璧 % Domain translation闉 闉 娆锋埄娈ч爟婊岊潊  % unsuperivsed machine translation % data mixing, transfer learning % knowledge gets partially vanished  % 闆垮姙婢婇煰 鐡寸粖鏅 % 闋冩悡鑸堕爟姗佽荡闉欏嫶鏆ｉ澒 transfer learning mixing data 鑷ф瑬娼 姘氣晣鐭 闈奉剣姣 % 闋冩﹥顫呮 parallel setting 闉愭劤鍔 闉濅緟姣勯爟 % parallel 闆垮姙婢婇煰鐡ｇ殰 闈炬﹥顫栭爟姗佽荡鑷 闋屾﹤鎽 % monolingual corpus姣 闈奉剣姣勯爟姗傚 UNMT work闇屻倢婢 闈告繉绠 % unsupervised 闈瑰姜濮ら灇 鏂撴粔鑺抽湆銈屾煄 姣靛韩婢 闆存帥鏅炴瓎 % 闋冩﹥顫呮 monolingual corpus 姣靛牗鐖涢渻 闆垮姙婢婇煰 鐡寸粖鏅爢鍕冲剨闉 闉氭鏌庨洰鐘惧灛闉涘牗娼 -鏀磋兂鏌涢瀼鎸 鏀撮附寮版皡鏃嶆緫闉 闉濅緟娼夐澗姗冪抽浖..? % 鏀撮附螠闋冩﹥妫 闉栧崐螠闆 low-resource UNMT姣 meta-learning algorithm闉欒導顢 闊块附濮 姘氣晥鏅ラ灇 闉濇粚瀚 % 鏃姙銆 meta-nmt 闆茶導顑撶摽鑼у 闆笺倠顩 闉氭尗婀㈤浕 闉栧崐螠鏃拌導濮 unsupervised闉 %  multi doamin 鑷ф洢鈥 %    In this section, we review previous approaches regarding low-resource neural machine translation models and the meta-learning.   Our work leverages two critical components from the natural language processing domain. In this section, we review previous approaches based on these two essential elements extensively used for this study: low-resource neural machine translation models and meta-learning. Our study extensively leverages two critical components from the natural language processing  field. In this section, we discuss previous studies by concentrating on these two parts: low-resource neural machine translation and meta learning.    Based on the success of attention-based models, neural machine translation models  achieve significant performances in numerous language datasets, even showing human-like performances. However, the performance of NMT models depends on a size of parallel sentences from the source and target language. To address this problem, diverse approaches have been proposed, which are categorized into two different directions:  utilizing monolingual datasets, and  transferring the knowledge from high-resource domain to the low-resource domain.    Recent studies point out the difficulty of gathering the parallel data abundant for machined translation , whereas the monolingual datasets are relatively easy to collect.    Focused on this aspect, many research attempt to handle this issue by only using monolingual datasets or adding small set of parallel dataset with the monolingual one.  To facilitate monolingual corpora, several studies apply dual learning, back-translation, and pretraining the model with the bilingual corpora.  As a more challenge scenario, NMT models only access monolingual copora.  Furthermore, as a challenging scenario, recent studies propose the unsupervised neural machine translation  methods without using any parallel corpus.    These algorithms extend back-translation algorithm and incorporate cross-lingual embedding vectors. Owing to methods for a good initialization, such as the shared byte pair encoding and the cross lingual representations, the UNMT models show the compatible performances compared with the supervised NMT models. By extending the back-translation method and incorporating the methods for good initialization, such as the shared byte pair encoding  and the cross-lingual representations, the UNMT models show comparable performances, following the ones of the supervised NMT. However, this approach assumes that the models requires plenty of monolingual data or the parallel ones.  However, these approach assume to train the models with the abundant monolingual corpora or the parallel ones.   These approaches assume that the model can be trained by the abundant monolingual datasets for a target domain or parallel corpus together.  Practically, When we intend to create a translation system into a particular language in a particular domain, there may be fewer languages in the domain you want to translate even monolingual data. In this case,    闉併倢鐗呴灊渚呮簜鎼挎粖濮 闊歌建鐖 闇冨嫴饪洪灇鑳虫剑 闊歌建鐖 闉忚兂鏌庨灇 monolingual data鑷 闉濅緟娼 闈炬﹤寮 闉涘牕瀚   parallel data 闉 monolingual data姣 闉濅緟鐖ら爟姗佺煀 闈奉剣姣勯爟姗佽荡 闉欏嫶鏅块瀽 parallel dat姣 闊辨穩鏆 supervised learning 闉 闋冩﹣纰 monolingual data姣 闋囨粚姣勯爟姗冩３    Monolingual copora 闆垮姙婢婇煰鍗娿偧 闋囨粚姣勯爟姗佽荡 闉欏嫶鏅块瀽, 闉栧崐螠闆 pseudo parallel   Secondly, other studies concentrate on transferring the knowledge of the rich resources corpora into the low-resource corpus. Initially, many researches focused on improving low-resource language pairs in multi-lingual language settings. Round trip approach makes the model to learn several language pairs into a single model, and incorporate additional parallel data as pivot language to improve the source and target languages. This approach exploits the source-to-pivot and the pivot-to-target corpus for transfer learning. Other approaches share the parameters of whole or part of the model across the languages when the model is difficult to collect pivot languages corpus.         Domain Adaptation of Neural Machine Translation by Lexicon Induction     Instead of Multi-lingual settings, recent researches reveal the problem of sensitive of domain shift. When training data of a objective corpus is a low-resource so that we exploit other domain corpora, this problem intensify. Motivated by this issue, there are two distinctions, which are model-based and data-centric approaches. Model-based approaches attempt to incorporate the regularization, the domain discriminator and domain embeddings. Data-centric approaches generate synthetic parallel corpora from monolingual in-domain corpus after training on high resources parallel corpora.     In summary, most of the previous works with regard to the transfer learning has two characteristics as follow: 1) the model is jointly trained on the rich sources domain and the low-resource domain, and 2) one of the rich-source corpora or a low-resource corpus includes paired sentences between the source and target language at least.  Secondly, a few other studies concentrate on transferring the knowledge from the rich-resources corpora into the low-resource corpus.   Such studies has two characteristics as follows 1) the model is jointly trained on the rich sources domain and the low-resource domain, and 2) one of the rich-source corpora or a low-resource corpus includes paired sentences between the source and target language at least. Owing to transferring the knowledge from rich resources, Some models show better performances than when trained with the low-resource corpora only. In spite of the improvements by the transfer learning approaches, these approaches still need the parallel corpus for the target or source domains and do not fully perform when both the in-domain and out-domain parallel datasets are scarce.   these approaches apply in constraint conditions, which are one or both of target domains or source domain corpus are the parallel corpus. For example, if we intend to create a translation system of a particular language in a particular domain, there may be fewer sentences in-domain as far as parallel out-of-domain data is scarce.  To address this constraint,    Azam: To address the above issues, we define a new task as the unsupervised domain adaptation on the low-resource dataset. Our work is a more challenging one than any other previous studies, because we apply in both the low-resource corpus and the out-domain corpora as monolingual one.  To the best of our knowledge, our work is a more challenging scenario than any other previous researches, because we apply in both the low-resource corpus and the out-of-domain corpora as monolingual one. Our work is a first attempt to solve the unsupervised domain adaptation for the low-resource unsupervised machine translation.  Azam:       Domain Adaptive Dialog Generation via Meta Learning  Meta-learning for low-resource natural language generation in task-oriented dialogue systems   Fast context adaptation via meta-learning    MAML    Meta-learning representations for continual learning    As increasing the training data, machine learning methods show significant performances in a task. In reality, we face a problem such as building the machine learning model in scare dataset settings. In this condition, the model is easily overfitted and fails to solve the task. To address this issue, recent researches make an effort to adapt to a new task with only a few training data. In this aspect, meta learning approaches attract a solution by quickly and accurately adapting to a low-resource task, and show impressive results in various domains.   In scarce dataset settings, most of machine learning models are easily overfitted to the low-resource dataset and fail to find an appropriate solution. To deal with the problem, meta-learning approaches seek how to adapt quickly and accurately to a low-resource task, and show impressive results in various domains.  The meta-learning approaches are classified into three broad categories: metric-based, model-based, and optimization-based.  The metric-based meta-learning aims to learn kernel function, which captures the relationship between samples and input data.  The model-based meta-learning updates the parameters of the model by meta-learner networks. Lastly, the optimization-based meta-learning makes the model to initialize proper parameters, easily adapting to a low-resource dataset in a few gradient steps. Owing to the success of the optimization-based meta-learning approaches, recent studies apply the meta-learning to the low-resource NLP tasks, such as multi-lingual NMT and dialog generation.   Our study attempts to address the low-resource UNMT task by exploiting the meta-learning approaches. Moreover, we present two novel losses that encourage the model to utilize knowledge learned from high-resource domains.    In the above meta-learning categories, our paper is related to the optimization-based approaches because of focusing to find good initialization parameters for the unsupervised low-resource machine translation task.     Moreover, \ourtoolname improves the domain generalization ability by utilizing domain-general loss and the cross-domain loss.       Though as an important issue, long sequence translation used to be regarded as a hard problem that sequence-to-sequence methods in neural style can not handle. However, in this paper, we prove the feasibility of end to end training in this field. Firstly, the basic experiments show that direct document-level translation with a large scale dataset has comparable performance compared with merged sentences generated by sentence unit model. One step forward, with widely available large scale sentence-level parallel data and almost infinite document-level monolingual data that can be used in back-translation, the potential of document-level translation can be activated. In other words, we propose a training criteria in document translation which can break the length bottleneck of sentence-level translation model. The observation may shed important light on extremely long sentence generation and make us rethink the routine of the long sequence machine translation. Finally, our dataset proposed in this paper contributes greatly as a boost in this field.     In this paper, we review main challenges unsolved in document-level neural machine translation, including datasets, metrics, context usage, and restricted training. After pointing out the under-explored status of this field, we attempt to refine the groundwork. A package of datasets, along with a new training paradigm for DNMT is proposed to push the limitation in this field. We hope our work can advance the research works and inspire correlative long-range sequence generation.     In this paper, we review the recent studies of document-level NMT and find them sort of misguided. Most works focus on appending more model modules as model regularization, which turns out to be oversold. Instead, we suggest heading back to the original but concise way of Doc2Doc to deal with DNMT. With our multi-resolutional training strategy, Doc2Doc yields the best results and show significant superiority over Doc2Sent. We also make a further step to reveal that the bottleneck in this field lies in datasets and metrics. Correspondingly, we propose a package of datasets along with metrics to boost the development of DNMT. We hope the analytical review and contributive datasets can be thought-provoking and inspire more works.  In this paper, we propose the literal document-to-document  translation and successfully activate it with multi-resolutional training. Different from traditional methods of modifying the model architectures, our approach introduces no extra parameters. A comprehensive set of experiments on various metrics show the advantage of MR Doc2Doc. In addition, we contribute a new document-level dataset as well as three new metrics to the community. 
","  In this section, we review previous approaches regarding low-resource neural machine translation models and the meta-learning.   Our work leverages two critical components from the natural language processing domain. In this section, we review previous approaches based on these two essential elements extensively used for this study: low-resource neural machine translation models and meta-learning. Our study extensively leverages two critical components from the natural language processing  field. In this section, we discuss previous studies by concentrating on these two parts: low-resource neural machine translation and meta learning.    Based on the success of attention-based models, neural machine translation models  achieve significant performances in numerous language datasets, even showing human-like performances. However, the performance of NMT models depends on a size of parallel sentences from the source and target language. To address this problem, diverse approaches have been proposed, which are categorized into two different directions:  utilizing monolingual datasets, and  transferring the knowledge from high-resource domain to the low-resource domain.    Recent studies point out the difficulty of gathering the parallel data abundant for machined translation , whereas the monolingual datasets are relatively easy to collect.    Focused on this aspect, many research attempt to handle this issue by only using monolingual datasets or adding small set of parallel dataset with the monolingual one.  To facilitate monolingual corpora, several studies apply dual learning, back-translation, and pretraining the model with the bilingual corpora.  As a more challenge scenario, NMT models only access monolingual copora.  Furthermore, as a challenging scenario, recent studies propose the unsupervised neural machine translation  methods without using any parallel corpus.    These algorithms extend back-translation algorithm and incorporate cross-lingual embedding vectors. Owing to methods for a good initialization, such as the shared byte pair encoding and the cross lingual representations, the UNMT models show the compatible performances compared with the supervised NMT models. By extending the back-translation method and incorporating the methods for good initialization, such as the shared byte pair encoding  and the cross-lingual representations, the UNMT models show comparable performances, following the ones of the supervised NMT. However, this approach assumes that the models requires plenty of monolingual data or the parallel ones.  However, these approach assume to train the models with the abundant monolingual corpora or the parallel ones.   These approaches assume that the model can be trained by the abundant monolingual datasets for a target domain or parallel corpus together.  Practically, When we intend to create a translation system into a particular language in a particular domain, there may be fewer languages in the domain you want to translate even monolingual data. In this case,    闂変降鍊㈤悧鍛寸亰娓氬懏绨滈幖鎸庣矕婵 闂婃瓕寤洪悥 闂囧啫瀚撮オ娲亣閼宠櫕鍓 闂婃瓕寤洪悥 闂夊繗鍏傞弻搴ㄧ亣 monolingual data閼 闂夋繀绶熷 闂堢偓锕ゅ 闂夋稑鐗曠   parallel data 闂 monolingual data濮 闂夋繀绶熼悥銈夌垷濮椾胶鐓 闂堝鍓ｅВ鍕垷濮椾浇鑽 闂夋瑥瀚堕弲鍧楃 parallel dat濮 闂婅鲸绌╅弳 supervised learning 闂 闂嬪啯锕ｇ喊 monolingual data濮 闂嬪洦绮氬В鍕垷濮楀啯锛    Monolingual copora 闂嗗灝濮欏濠囩叞閸楀ǹ鍋 闂嬪洦绮氬В鍕垷濮椾浇鑽 闂夋瑥瀚堕弲鍧楃, 闂夋牕宕愯灎闂 pseudo parallel   Secondly, other studies concentrate on transferring the knowledge of the rich resources corpora into the low-resource corpus. Initially, many researches focused on improving low-resource language pairs in multi-lingual language settings. Round trip approach makes the model to learn several language pairs into a single model, and incorporate additional parallel data as pivot language to improve the source and target languages. This approach exploits the source-to-pivot and the pivot-to-target corpus for transfer learning. Other approaches share the parameters of whole or part of the model across the languages when the model is difficult to collect pivot languages corpus.         Domain Adaptation of Neural Machine Translation by Lexicon Induction     Instead of Multi-lingual settings, recent researches reveal the problem of sensitive of domain shift. When training data of a objective corpus is a low-resource so that we exploit other domain corpora, this problem intensify. Motivated by this issue, there are two distinctions, which are model-based and data-centric approaches. Model-based approaches attempt to incorporate the regularization, the domain discriminator and domain embeddings. Data-centric approaches generate synthetic parallel corpora from monolingual in-domain corpus after training on high resources parallel corpora.     In summary, most of the previous works with regard to the transfer learning has two characteristics as follow: 1) the model is jointly trained on the rich sources domain and the low-resource domain, and 2) one of the rich-source corpora or a low-resource corpus includes paired sentences between the source and target language at least.  Secondly, a few other studies concentrate on transferring the knowledge from the rich-resources corpora into the low-resource corpus.   Such studies has two characteristics as follows 1) the model is jointly trained on the rich sources domain and the low-resource domain, and 2) one of the rich-source corpora or a low-resource corpus includes paired sentences between the source and target language at least. Owing to transferring the knowledge from rich resources, Some models show better performances than when trained with the low-resource corpora only. In spite of the improvements by the transfer learning approaches, these approaches still need the parallel corpus for the target or source domains and do not fully perform when both the in-domain and out-domain parallel datasets are scarce.   these approaches apply in constraint conditions, which are one or both of target domains or source domain corpus are the parallel corpus. For example, if we intend to create a translation system of a particular language in a particular domain, there may be fewer sentences in-domain as far as parallel out-of-domain data is scarce.  To address this constraint,    Azam: To address the above issues, we define a new task as the unsupervised domain adaptation on the low-resource dataset. Our work is a more challenging one than any other previous studies, because we apply in both the low-resource corpus and the out-domain corpora as monolingual one.  To the best of our knowledge, our work is a more challenging scenario than any other previous researches, because we apply in both the low-resource corpus and the out-of-domain corpora as monolingual one. Our work is a first attempt to solve the unsupervised domain adaptation for the low-resource unsupervised machine translation.  Azam:       Domain Adaptive Dialog Generation via Meta Learning  Meta-learning for low-resource natural language generation in task-oriented dialogue systems   Fast context adaptation via meta-learning    MAML    Meta-learning representations for continual learning    As increasing the training data, machine learning methods show significant performances in a task. In reality, we face a problem such as building the machine learning model in scare dataset settings. In this condition, the model is easily overfitted and fails to solve the task. To address this issue, recent researches make an effort to adapt to a new task with only a few training data. In this aspect, meta learning approaches attract a solution by quickly and accurately adapting to a low-resource task, and show impressive results in various domains.   In scarce dataset settings, most of machine learning models are easily overfitted to the low-resource dataset and fail to find an appropriate solution. To deal with the problem, meta-learning approaches seek how to adapt quickly and accurately to a low-resource task, and show impressive results in various domains.  The meta-learning approaches are classified into three broad categories: metric-based, model-based, and optimization-based.  The metric-based meta-learning aims to learn kernel function, which captures the relationship between samples and input data.  The model-based meta-learning updates the parameters of the model by meta-learner networks. Lastly, the optimization-based meta-learning makes the model to initialize proper parameters, easily adapting to a low-resource dataset in a few gradient steps. Owing to the success of the optimization-based meta-learning approaches, recent studies apply the meta-learning to the low-resource NLP tasks, such as multi-lingual NMT and dialog generation.   Our study attempts to address the low-resource UNMT task by exploiting the meta-learning approaches. Moreover, we present two novel losses that encourage the model to utilize knowledge learned from high-resource domains.    In the above meta-learning categories, our paper is related to the optimization-based approaches because of focusing to find good initialization parameters for the unsupervised low-resource machine translation task.     Moreover, \ourtoolname improves the domain generalization ability by utilizing domain-general loss and the cross-domain loss.",106
" Numerous entities are emerging everyday. The attributes of the entities are often noisy or incomplete, even missing.  In the field of electronic commerce, target attributes  of new products are often missing .  In medical analysis, attributes like transmission, genetics and origins of a novel virus are often unknown to people.  Even in DBpedia, a well-constructed and large-scale knowledge base extracted from Wikipedia, half of the entities contain less than 5 relationships .  %In KG construction area, KGs often suffer from incompleteness.  %For example, in DBpedia, a well-constructed and large-scale knowledge base extracted from Wikipedia, half of the entities contain less than 5 relationships . %Therefore,  A method that is capable of supplementing reliable attribute values for emerging entities can be highly useful in many applications.  %With the method to automatically extract attribute values for emerging entities, the eCommerce retailers are able to better serve the customers with updated information; the extracted medical attribute information of a novel virus can be organized to assist the understanding of the virus; the KG will be able to provide more complete information for users.      Although information extraction methods have been extensively studied, the task of open attribute value extraction remains challenging. First, the emerging entities may have new attribute values that are absent in the existing KG. Under such circumstances, the prediction methods under the closed-world assumption and the methods that cannot utilize external information are not well suited due to their limited recalls. Second,  while web corpus can be used as a good resource to provide relatively updated and relevant articles for large varieties of emerging entities, %that are relatively complete and updated in a timely manner,  %considering the large variety of the emerging entities, the web corpus, which is relatively complete and updated in a timely manner, is able to provide a rich collection of relevant articles.  %However,  the articles retrieved from web corpus can be noisy and/or irrelevant, which in turn leads to a limited precision.  Finally, even when articles are relevant, the extracted answers might still be inaccurate due to the error-prone information extraction model.    To effectively filter out noisy answers that are obtained either due to the irreverent articles or the errors incurred by the information extraction system, we  %need to answer  pose the following two questions: First, how many articles should we collect from the enormous web corpus? Second,  how to select the most reliable value out of the pool of all the possible answers extracted from the articles?  There is no common answer to the first question that works for all triplets because of the inconsistent degrees of difficulties in finding the correct attribute values. The decision of when to stop querying more external articles needs to be made after successive evaluations of the candidate answers. Thus the decision making process is inherently sequential. %Thus, it is inherently a sequential decision making problem.   Reinforcement learning  is a commonly adopted method to deal with sequential decision problems and has been widely studied in the field of robotic and game . But there are not many researches on open attribute value extraction with RL.  One existing literature of RL-based method for value extraction is proposed by .  In their work, a RL framework is designed to improve accuracy of event-related value extraction by acquiring and incorporating external evidences.  However, their approach requires a great amount of context information about the specific event of interest during the training process.  It is not trivial to extend their framework for open attribute value extraction, because we would need to collect context words and train a new model with annotated data for each emerging attribute. Therefore, their framework cannot be generalized to open attribute value extraction task when various entities and attributes are involved.  While using the context words to construct the states in RL is not suitable in our task,  our solution is to leverage the rich, well-organized information in KG, which is not only informative but also generalizable.  %The knowledge from KG  Such information can be leveraged in answer comparisons, which addresses our second question. For example, to fill the incomplete triplet  iPhone 11, display resolution, ?, from the KG we may find that the attribute values  ``display resolutions"" of an entity that is under category ``Phone"" is commonly expressed in the format of ``xxx by xxxx Pixels"", where x stands for some digit. The typical instances of the attribute values for entities under the same category provide valuable background information.   In this paper, we propose a knowledge-guided RL framework to perform open attribute value extraction.  The RL agent is trained to make good actions for answer selection and stopping time decision.  Our experiments show that the proposed framework significantly boosts the extraction performance.  To the best of our knowledge, we are the first to integrate KG in a RL framework to perform open attribute value extraction %use KG to guide the RL-based sequential decision for open attribute value extraction.  %The experiment results demonstrate that our approach improves the extraction performances substantially. In summary, our contribution are in three folds:         Machine reading comprehension  and automated question  answering are important and longstanding topic in NLP research due to its huge potentials in wide variety of applications. An end-to-end MRC QA models are expected to have the ability to read a piece of text and then answer questions about it.   It is challenging since the model is required to have a good understanding of natural language and the ability to find answers.   Based on the benchmark dataset, Stanford Question Answering Dataset  ,  Significant progress has been made with the machine reading and QA task in recent years. Some notable works include BiDAF ,  SAN , QANet, ALBERT .  Our proposed framework can also be regarded an end-to-end MRC QA model that is built on top of an existing MRC QA model, which is used as the information extraction system in our extraction process.   The questions we are targeting are limited to questions about attribute values.  For such kind of questions, we aim to enhance the performances of an existing model by utilizing  external information from KG and by acquiring more articles when the agent does not feel confident about the extracted answer. Different from most of the previous works, our focus is to enhance the performance of an existing model by utilizing  external information from KG and by acquiring more articles when the agent does not feel confident about the extracted answer.    Attribute value extraction under the open world assumption has received many attentions in NLP community recently. There has been quite a few works on open attribute value extraction.  OpenTag  formalized the extraction problem as a sequence tagging task and proposed an end-to-end framework for open attribute value extraction. The open-world KGC  used a complex relationship dependent content masking architecture to mitigate the presence of noisy text descriptions and extract the attribute value from the denoised text.  TXtract  incorporated the categorical structure into the value tagging system.  However these methods suffer from irrelevant articles and is not able to filter out noisy answers.    RL  is a framework that enables agents to reason about sequential decision making as an optimization process. It has been widely applied in NLP tasks, including article summarization \citep[][]{paulus2017deep,li2018actor,celikyilmaz2018deep}, dialogue generation \citep[][]{li2016deep, serban2017deep,li2019dialogue}, and question answering  and so on.   To the best of our knowledge, we are the first to integrate information from KG into a RL framework to fulfill the attribute extraction task.      This paper proposed novel meta-learning approaches for low-resource UNMT. \toolnameMeta leverages multiple source domains to quickly and effectively adapt our model to the target domain. Moreover, we introduce an improved method called \ourtoolname, which enhances aggregate-domain and cross-domain generalization such that the model incorporates the knowledge learned across multiple domains. Eventually, the method prevents the model from overfitting due to a small amount of training data in a new domain, thereby leading to improved performance of low-resource UNMT. We empirically show that our proposed approaches consistently outperform the baseline models with a nontrivial margin.    In this paper, we propose novel meta-learning algorithms for low-resource UNMT. These algorithms leverages multiple source domains to learn common knowledge and then finetune the target domain. By comparing with various baseline models, we empirically show that our proposed algorithms significantly surpass others. Moreover, we introduce an enhanced algorithm, \ourtoolname, which utilizes aggregate-domain and cross-domain losses such that the model incorporates learned knowledge across multiple domains.     Owing to these losses, \ourtoolname quickly adapt a new domain in a few iterations, and further improve the performance in low-resource UNMT .    In experiment section, we demonstrate \ourtoolname quickly pretrain from source domains and finetunes a new domain in a few iteration.     Moreover, \ourtoolname shows superiority in a low resource doamain and   the importance of proposed loss and the effectiveness of proposed algorithms in varying size of a new domain. Moreover, \ourtoolname shows superiority in    Future work, we apply our extended algorithms to computer vision domain tasks, whic are suffer from the data scarcity problem.   In this paper, we propose a novel meta-learning algorithm for low-resource UNMT. The algorithm leverages multiple source domains to learn domain-general information and then finetune the target domain. Moreover, we introduce an enhanced algorithm, \ourtoolname, which utilizes aggregate-domain and cross-domain losses such that the model incorporates learned knowledge across multiple domains. Eventually, the algorithm prevents the model from being over-fitted due to a small amount of training data in a new domain and improves the performance of low-resource UNMT. We empirically show that our proposed algorithms effectively leverage domain-general knowledge and outperform baseline models with a considerable margin.     \begin{comment}      
","   Machine reading comprehension  and automated question  answering are important and longstanding topic in NLP research due to its huge potentials in wide variety of applications. An end-to-end MRC QA models are expected to have the ability to read a piece of text and then answer questions about it.   It is challenging since the model is required to have a good understanding of natural language and the ability to find answers.   Based on the benchmark dataset, Stanford Question Answering Dataset  ,  Significant progress has been made with the machine reading and QA task in recent years. Some notable works include BiDAF ,  SAN , QANet, ALBERT .  Our proposed framework can also be regarded an end-to-end MRC QA model that is built on top of an existing MRC QA model, which is used as the information extraction system in our extraction process.   The questions we are targeting are limited to questions about attribute values.  For such kind of questions, we aim to enhance the performances of an existing model by utilizing  external information from KG and by acquiring more articles when the agent does not feel confident about the extracted answer. Different from most of the previous works, our focus is to enhance the performance of an existing model by utilizing  external information from KG and by acquiring more articles when the agent does not feel confident about the extracted answer.    Attribute value extraction under the open world assumption has received many attentions in NLP community recently. There has been quite a few works on open attribute value extraction.  OpenTag  formalized the extraction problem as a sequence tagging task and proposed an end-to-end framework for open attribute value extraction. The open-world KGC  used a complex relationship dependent content masking architecture to mitigate the presence of noisy text descriptions and extract the attribute value from the denoised text.  TXtract  incorporated the categorical structure into the value tagging system.  However these methods suffer from irrelevant articles and is not able to filter out noisy answers.    RL  is a framework that enables agents to reason about sequential decision making as an optimization process. It has been widely applied in NLP tasks, including article summarization \citep[][]{paulus2017deep,li2018actor,celikyilmaz2018deep}, dialogue generation \citep[][]{li2016deep, serban2017deep,li2019dialogue}, and question answering  and so on.   To the best of our knowledge, we are the first to integrate information from KG into a RL framework to fulfill the attribute extraction task.",107
"  % NMT is good but needs lots of parallel data + we should exploit mono data more Neural machine translation  using sequence to sequence architectures  has become the dominant approach to automatic machine translation. While being able to approach human-level performance , it still requires a huge amount of parallel data, otherwise it can easily overfit. Such data, however, might not always be available. At the same time, it is generally much easier to gather large amounts of monolingual data, and therefore, it is interesting to find ways of making use of such data. The simplest strategy is to use backtranslation , %but it can be rather costly since it requires training another model in the opposite translation direction and then creating the source-side synthetic sentences by translating the target-side monolingual corpus. but it can be rather costly since it requires training a model in the opposite translation direction and then translating the monolingual corpus.  % We introduce the compositionality  It was suggested by \citet{lake2017machines} that during the development of a general human-like AI system, one of the desired characteristics of such a system is the ability to learn in a continuous manner using previously learned tasks as building blocks for mastering new, more complex tasks. %by combining the knowledge learned from the previously learned simpler tasks. Until recently, continuous learning of neural networks was problematic, among others, due to the catastrophic forgetting . Several methods were proposed , however, %they mostly focused on preserving the knowledge of each task learned by the whole network. they mainly focus only on adapting the whole network  to new tasks while maintaining good performance on the previously learned tasks.  % Summary of our method using EWC %\XXX{toto mozna posunout za nasledujici odstavec + jak resime jejich nedostatky} In this work, we present an unsupervised pretraining method for NMT models using Elastic Weight Consolidation . First, we initialize both encoder and decoder with source and target language models respectively. Then, we fine-tune the NMT model using the parallel data. To prevent the encoder and decoder from forgetting the original language modeling  task, we regularize their weights individually using Elastic Weight Consolidation based on their importance to that task. Our hypothesis is the following: by forcing the network to remember the original LM tasks we can reduce overfitting of the NMT model on the limited parallel data. %\XXX{Ukazujeme, ze metoda funguje, je rychlejis + mame odvozeno, ze by mela fungovat i pro podsite} %\XXX{Zminit rovnou strucne prinosy?}  % Summary of the method we used as a comparison We also provide a comparison of our approach with the method proposed by \citet{ramachandran2017pretraining}. They also suggest initialization of the encoder and decoder with a language model. However, during the fine-tuning phase they use the original language modeling objectives as an additional training loss in place of model regularization. Their approach has two main drawbacks: first, during the fine-tuning phase, they still require the original monolingual data which might not be available anymore in a life-long learning scenario. Second, they need to compute both machine translation and language modeling losses which increases the number of operations performed during the update slowing down the fine-tuning process. Our proposed method addresses both problems: it requires only a small held-out set to estimate the EWC regularization term and converges 2-3 times faster than the previous method.\footnote{The speedup is with regard to the wall-clock time. In our experiments both EWC and the LM-objective methods require similar number of training examples to converge.}   %Intro to compositionality %Compositional learning + using previosly learned elementary knowledge to learn more complex model   %Avoiding catastrophic forgetting as key to continual learning and compositionality -> choice of EWC  %Benefits of compositionality in greater scope  + why NMT + LM pretrain? %It is a first step in our ongoing reseach  %The paper is structured as following...      Putting aside the work of \citet{ramachandran2017pretraining}, Several other approaches towards exploiting the available monolingual data for NMT have been previously proposed.     Summary of Backtranslation Currently, the most common method is creating synthetic parallel data by backtranslating the target language monolingual corpora using machine translation . While being consistently beneficial, this method requires a pretrained model to prepare the backtranslations. Additionally, \citet{ramachandran2017pretraining} showed that the unsupervised pretraining approach reaches at least similar performance to the backtranslation approach.    Cross-lingual modelling Recently, \citet{lample2019cross} suggested using a single cross-lingual language model trained on multiple monolingual corpora as an initialization for various NLP tasks, including machine translation. While our work focuses strictly on a monolingual language model pretraining, we believe that our work can further benefit from using cross-lingual language models.     Summary of Reconstruction Another possible approach is to introduce an additional reordering  or de-noising objectives, the latter being recently employed in the unsupervised NMT scenarios . These approaches try to force the NMT model to learn useful features by presenting it with either shuffled or noisy sentences teaching it to reconstruct the original input.    Catastrophic forgetting Furthermore, \citet{khayrallah-etal-2018-regularized} show how to prevent catastrophic forgeting during domain adaptation scenarios. They fine-tune the general-domain NMT model using in-domain data adding an additional cross-entropy objective to restrict the distribution of the fine-tuned model to be similar to the distribution of the original general-domain model.     Mention the alternatives to EWC     Multi-hop QG task is more challenging and worthy of exploration compared to conventional single-hop QG. To address the additional challenges in multi-hop QG, we propose MulQG, which does multi-hop context encoding with Graph Convolutional Network and encoding fusion via a Gated Reasoning module. To the best of our knowledge, we are the first to tackle the challenge of multi-hop reasoning over paragraphs without any sentence-level information. The model performance on HotpotQA dataset demonstrates its effectiveness on aggregating scattered pieces of evidence across the paragraphs and fusing information effectively to generate multi-hop questions. The strong reasoning ability of the Multi-hop Encoder in the MulQA model can potentially be leveraged in complex generation tasks for the future work.    In addition, from the human evaluation, our proposed model is likely to generate more fluent complete questions and outperform the baseline by 20.8\  on the percentage of questions assessed as multi-hop type.       
","   Putting aside the work of \citet{ramachandran2017pretraining}, Several other approaches towards exploiting the available monolingual data for NMT have been previously proposed.     Summary of Backtranslation Currently, the most common method is creating synthetic parallel data by backtranslating the target language monolingual corpora using machine translation . While being consistently beneficial, this method requires a pretrained model to prepare the backtranslations. Additionally, \citet{ramachandran2017pretraining} showed that the unsupervised pretraining approach reaches at least similar performance to the backtranslation approach.    Cross-lingual modelling Recently, \citet{lample2019cross} suggested using a single cross-lingual language model trained on multiple monolingual corpora as an initialization for various NLP tasks, including machine translation. While our work focuses strictly on a monolingual language model pretraining, we believe that our work can further benefit from using cross-lingual language models.     Summary of Reconstruction Another possible approach is to introduce an additional reordering  or de-noising objectives, the latter being recently employed in the unsupervised NMT scenarios . These approaches try to force the NMT model to learn useful features by presenting it with either shuffled or noisy sentences teaching it to reconstruct the original input.    Catastrophic forgetting Furthermore, \citet{khayrallah-etal-2018-regularized} show how to prevent catastrophic forgeting during domain adaptation scenarios. They fine-tune the general-domain NMT model using in-domain data adding an additional cross-entropy objective to restrict the distribution of the fine-tuned model to be similar to the distribution of the original general-domain model.     Mention the alternatives to EWC",108
"   Even though machine translation  has greatly improved with the emergence of neural machine translation   and more recently the Transformer architecture , there remain challenges which can not be solved by using sentence-level NMT systems. Among other issues, this includes the problem of inter-sentential anaphora resolution  or the consistent translation across a document , for which the system inevitably needs document-level context information.  In recent years, many works have focused on changing existing NMT architectures to incorporate context information in the translation process . However, often times results are reported only on very specific tasks , making it difficult to assess the potential of the different methods in a more general setting. This, together with the fact that big improvements are typically reported on low resource tasks, gives the impression that document-level NMT mostly improves due to regularization rather than from leveraging the additional context information. In this work we want to give a more complete overview of the current state of document-level NMT by comparing various approaches on a variety of different tasks including an application-oriented E-commerce setting. We discuss both, widely used performance metrics, as well as highly task-specific observations.  Another important aspect when talking about document-level NMT is the applicability in ``real life"" settings. There, when faced with a low resource data scenario, back-translation is an established way of greatly improving system performance . However, to the best of our knowledge, the effect of back-translation data obtained and used by context-aware models has never been explored before. The main contributions of this paper are summarized below:       The discourse- or document-level translation is a long-standing and unsolved topic in the machine translation community . Although neural machine translation  has recently become the dominant translation paradigm that provides superior performance, the independence between sentences is still the fundamental assumption taken for granted by most NMT systems. This means, that discourse-level phenomena between sentences such as pronominal reference, consistent lexical choice, and verbal tenses, etc. can not be addressed by these sentence-level NMT systems . The current NMT approaches tackling inter-sentential discourse phenomena can be roughly categorized into three aspects, augmenting NMT by   To include the source-side context, \citet{tiedemann2017neural} concatenate consecutive sentences as input to the NMT system, while \citet{jean2017does, bawden2017evaluating, zhang2018improving} use an additional encoder to extract contextual information from a few previous source-side sentences.  These works only consider a local context, including a few previous sentences. Some researches seek to capture the global document context; \citet{wang2017exploiting} summarize the global context from all previous sentences in a document with a pre-trained hierarchical RNN and then use it for updating decoder states. Very recently, \citet{chen2020modeling} proposed a discourse structure-based encoder that takes account of the discourse structure information of the input document.  For adding additional target-side context, \citet{tiedemann2017neural, agrawal2018contextual} conduct multi-sentences decoding and observe only a minor improvement.  \citet{maruf2017document} apply cache-based models to store vector representations for both source- and target-side context. Similarly, \citet{tu2018learning} augment their NMT system with an external cache to memorize the translation history.  \citet{miculicich2018document} integrate two hierarchical attention networks   in the NMT model to take account for source and target context.  \citet{maruf2019selective} apply a hierarchical attention module on sentences and words in the context to select contextual information that is more relevant to the current sentence.  For incorporating document-level monolingual data from the source language, \citet{zhu2020incorporating} use BERT  to model the source-side context and integrate it with the encoder and decoder of the NMT model. \citet{junczys2019microsoft} share the parameters of a BERT-style encoder trained on monolingual documents with the MT model.  To utilize the document-level monolingual data from the target language, \citet{junczys2019microsoft} also submit a system that trained on the combination of real and synthetic document-parallel data obtained by back-translation. However, they do not consider document-level back-translation. \citet{voita2019context} proposed a document-level post-editing system which is trained only using the monolingual document-level corpus.  Recently, there has been a tendency in the community to conclude that the context used in a context-aware MT model works as regularisation or noise generator.  \citet{kim2019and} compare several multi-encoders methods and claim that including this additional information can improve translation performance, but it is mostly due to the regularization effect rather than the contextual information.  \citet{li2020does} also compare some context-aware architectures by replacing the real context with some random signal and show that random signals can achieve the same level improvement as the real context. However, it should be taken with a grain of salt since solving this task, along with the analysis, is quite challenging. There are many impact factors from the architecture, the data at hand, to the metric being used for evaluation.  One issue that can not be ignored in all discourse-related researches is the problem of evaluation. Since some discourse-level phenomena between sentences appear less frequently, although relevant, there is doubt if the metrics like BLEU score  can capture these complex relationships . To get more insights into the capacities dealing with discourse-level phenomena of their MT models, some researchers use more targeted evaluation scores , like the Accuracy of Pronoun Translation  \citet{werlen2017validation}, or they evaluate their systems on some specific test suites that contain more and more complex discourse phenomena .     We introduced our work in progress, and exploration of model regularization of NMT encoder and decoder parameters based on their importance for previously learned tasks and its application in the unsupervised pretraining scenario.  which can be used in the unsupervised pretraining scenarios based on their importance for language modeling tasks. We documented that our method slightly improves the NMT performance  when combined with a pretrained target language model. We achieve this improvement at a reduced training time.  while reducing the training time.   We also showed that the method is less effective if the original language modeling task used to pretrain the NMT encoder is too different from the task learned during the fine-tuning. We plan to further investigate whether we can gain improvements by using a different pretraining method for the encoder and how much this task mismatch relates to the learning capacity of the encoder.  
","  The discourse- or document-level translation is a long-standing and unsolved topic in the machine translation community . Although neural machine translation  has recently become the dominant translation paradigm that provides superior performance, the independence between sentences is still the fundamental assumption taken for granted by most NMT systems. This means, that discourse-level phenomena between sentences such as pronominal reference, consistent lexical choice, and verbal tenses, etc. can not be addressed by these sentence-level NMT systems . The current NMT approaches tackling inter-sentential discourse phenomena can be roughly categorized into three aspects, augmenting NMT by   To include the source-side context, \citet{tiedemann2017neural} concatenate consecutive sentences as input to the NMT system, while \citet{jean2017does, bawden2017evaluating, zhang2018improving} use an additional encoder to extract contextual information from a few previous source-side sentences.  These works only consider a local context, including a few previous sentences. Some researches seek to capture the global document context; \citet{wang2017exploiting} summarize the global context from all previous sentences in a document with a pre-trained hierarchical RNN and then use it for updating decoder states. Very recently, \citet{chen2020modeling} proposed a discourse structure-based encoder that takes account of the discourse structure information of the input document.  For adding additional target-side context, \citet{tiedemann2017neural, agrawal2018contextual} conduct multi-sentences decoding and observe only a minor improvement.  \citet{maruf2017document} apply cache-based models to store vector representations for both source- and target-side context. Similarly, \citet{tu2018learning} augment their NMT system with an external cache to memorize the translation history.  \citet{miculicich2018document} integrate two hierarchical attention networks   in the NMT model to take account for source and target context.  \citet{maruf2019selective} apply a hierarchical attention module on sentences and words in the context to select contextual information that is more relevant to the current sentence.  For incorporating document-level monolingual data from the source language, \citet{zhu2020incorporating} use BERT  to model the source-side context and integrate it with the encoder and decoder of the NMT model. \citet{junczys2019microsoft} share the parameters of a BERT-style encoder trained on monolingual documents with the MT model.  To utilize the document-level monolingual data from the target language, \citet{junczys2019microsoft} also submit a system that trained on the combination of real and synthetic document-parallel data obtained by back-translation. However, they do not consider document-level back-translation. \citet{voita2019context} proposed a document-level post-editing system which is trained only using the monolingual document-level corpus.  Recently, there has been a tendency in the community to conclude that the context used in a context-aware MT model works as regularisation or noise generator.  \citet{kim2019and} compare several multi-encoders methods and claim that including this additional information can improve translation performance, but it is mostly due to the regularization effect rather than the contextual information.  \citet{li2020does} also compare some context-aware architectures by replacing the real context with some random signal and show that random signals can achieve the same level improvement as the real context. However, it should be taken with a grain of salt since solving this task, along with the analysis, is quite challenging. There are many impact factors from the architecture, the data at hand, to the metric being used for evaluation.  One issue that can not be ignored in all discourse-related researches is the problem of evaluation. Since some discourse-level phenomena between sentences appear less frequently, although relevant, there is doubt if the metrics like BLEU score  can capture these complex relationships . To get more insights into the capacities dealing with discourse-level phenomena of their MT models, some researchers use more targeted evaluation scores , like the Accuracy of Pronoun Translation  \citet{werlen2017validation}, or they evaluate their systems on some specific test suites that contain more and more complex discourse phenomena .",109
" Automatic summarization is a fundamental task in Natural Language Processing, which aims to condense the original input into a shorter version covering salient information and has been continuously studied for decades . Recently, online multi-speaker dialogue/meeting has become one of the most important ways for people to communicate with each other in their daily works. Especially due to the spread of  COVID-19 worldwide, people are more dependent on online communication. In this paper, we focus on dialogue summarization, which can help people quickly grasp the core content of the dialogue without reviewing the complex dialogue context.   Recent works that incorporate additional commonsense knowledge in the dialogue generation  and dialogue context representation learning  show that even though neural models have strong learning capabilities, explicit knowledge can still improve response generation quality.   It is because that a dialog system can understand conversations better and thus respond more properly if it can access and make full use of large-scale commonsense knowledge. However, current dialogue summarization systems  ignore the exploration of commonsense knowledge, which may limit the performance. In this work, we examine the benefit of incorporating commonsense knowledge in the dialogue summarization task and also address the question of how best to incorporate this information. Figure  shows a positive example to illustrate the effectiveness of commonsense knowledge in the dialogue summarization task.  Bob asks Tom for help because his car has broken down. On the one hand, by introducing commonsense knowledge according to the pick up and car broke down, we can know that Bob expects Tom to give him a lift. On the other hand, commonsense knowledge can serve as a bridge between non-adjacent utterances that can help the model better understanding the dialogue.  In this paper, we follow the previous setting  and also use ConceptNet  as a large-scale commonsense knowledge base, while the difference is that we regard knowledge and text as heterogeneous data in a real multi-speaker dialogue. We propose a model named Dialogue Heterogeneous Graph Network  for incorporating commonsense knowledge by constructing the graph including both utterance and knowledge nodes. Besides, our heterogeneous graph also contains speaker nodes at the same time, which has been proved to be a useful feature in dialogue modeling. In particular, we equip our heterogeneous graph network with two additional designed modules. One is called message fusion, which is specially designed for utterance nodes to better aggregate information from both speakers and knowledge. The other one is called node embedding, which can help utterance nodes to be aware of position information. Compared to homogeneous graph network in related works , we claim that the heterogeneous graph network can effectively fuse information and contain rich semantics in nodes and links, and thus more accurately encode the dialogue representation.   We conduct experiments on the SAMSum corpus , which is a large-scale chat summarization corpus. We analyze the effectiveness of integration of knowledge and heterogeneity modeling. The human evaluation also shows that our approach can generate more abstractive and correct summaries. To evaluate whether commonsense knowledge can help our model better generalize to the new domain, we also perform zero-shot setting experiments on the Argumentative Dialogue Summary Corpus , which is a debate summarization corpus. In the end, we give a brief summary of our contributions:  We are the first to incorporate commonsense knowledge into dialogue summarization task.  We propose a D-HGN model to encode the dialogue by viewing utterances, knowledge and speakers as heterogeneous data.  Our model can outperform various methods.    Previous works used feature engineering , template-based  and graph-based  methods for extractive dialogue summarization. Although extractive methods are widely used, the results tend to be incoherent and poorly readable. Therefore, current works mainly focus on abstractive methods, which can produce more readable and 閾垮乪ncy summaries. They tend to incorporate additional auxiliary information to help better modeling the dialogue.  incorporated dialogue acts to model the interactive status of the meeting.  tackled the problem of customer service summarization, which first produced a sequence of pre-defined keywords then generated the summary.  generated summaries for nurse-patient conversation by incorporating topic information.  first removed useless utterances by utilizing discourse labels and then generated summaries.  combined vision and textual features in a unified hierarchical attention framework to generate meeting summaries.  employed a hierarchical transformer framework and incorporated part-of-speech and entity information for meeting summarization. In this paper, we facilitate dialogue summarization task by incorporating commonsense knowledge and further model utterances, commonsense knowledge and speakers as heterogeneous data.     This paper proposes an adaptive attentional network for few-shot KG completion, termed as FAAN. Previous studies solve this problem by learning static representations of entities or references, ignoring their dynamic properties. FAAN proposes to encode entity pairs adaptively, and predict facts by adaptively matching references with queries. Experiments on two public datasets demonstrate that our model outperforms current state-of-art methods with different few-shot sizes. Our future work might consider other advanced methods to model few-shot relations, and exploiting more contextual information like textual description to enhance entity embeddings.   
"," Previous works used feature engineering , template-based  and graph-based  methods for extractive dialogue summarization. Although extractive methods are widely used, the results tend to be incoherent and poorly readable. Therefore, current works mainly focus on abstractive methods, which can produce more readable and 闁惧灝涔猲cy summaries. They tend to incorporate additional auxiliary information to help better modeling the dialogue.  incorporated dialogue acts to model the interactive status of the meeting.  tackled the problem of customer service summarization, which first produced a sequence of pre-defined keywords then generated the summary.  generated summaries for nurse-patient conversation by incorporating topic information.  first removed useless utterances by utilizing discourse labels and then generated summaries.  combined vision and textual features in a unified hierarchical attention framework to generate meeting summaries.  employed a hierarchical transformer framework and incorporated part-of-speech and entity information for meeting summarization. In this paper, we facilitate dialogue summarization task by incorporating commonsense knowledge and further model utterances, commonsense knowledge and speakers as heterogeneous data.",110
" %\yy{para 1: problem is important, para 2: temporal graph, existing systems, para 3: neural networks, para 4: why difficult: lack of training data, para 5: what do we do}  %\yy{this is a comment} %\yyc{before correction}{after correction}   %The flow of time is used to chain narratives, reason about causes and effects of events, form a deeper understanding of the past, and postulate the future. Temporal reasoning is crucial for analyzing the interactions among complex events and producing   coherent interpretations of text data . There is a rich body of research on the use of temporal information in a variety of important application domains, including topic detection and tracking, information extraction, parsing of clinical records , discourse analysis, and question answering. %\yy{Aman: Please update the cites based on some quick Google search on temporal reasoning/expressions in IE/TDT/medical .} %Motivated by its ubiquity in text understanding, we undertake the task of extracting temporal graphs from documents. %and a rich understanding of temporal aspects of a document helps humans in reading comprehension.   %Temporal reasoning also plays a critical role in downstream natural language processing  tasks like    Graphs are a natural choice for representing the temporal ordering among events, where the nodes are the individual events, and the edges capture temporal relationships such as ``before'', ``after'' or ``simultaneous''. Representative work on automated extraction of such graphs from textual documents includes the early work by~\citet{chambers2009unsupervised}, where the focus is on the construction of event chains from a collection of documents, and the more recent \caevo and \cct, which extract a graph for each input document instead.   These methods focus on rule-based and statistical sub-modules to extract verb-centered events and the temporal relations among them. %Specifically, given a document, our system extracts a temporal event graph, where the nodes of the graph are the events, and the edges capture the temporal order~ between them. %Classical temporal information extraction systems focus on one of the two broad themes of relation identification and temporal relation classification. %Relation identification is the task of identifying events that can be connected by a temporal relation. %The task of temporal relation involves identifying the temporal relationship that exists between the given two events. % For example, in the sentence I had a coffee while I was getting a haircut, the phrase while I was expresses the fact that the events of drinking a coffee and getting a haircut took place at the same time. %For example, given the sentence I had a coffee while I was getting a haircut, a relation identification system would identify events had a coffee and getting a haircut. %A temporal relation classification system would then determine that the events happened simultaneously. %Our goal is to create a system that can perform both these tasks together in an end-to-end fashion over multiple sentences.  %The idea of extracting temporal graphs from a given document is not new. %Tempeval-3 introduced a task specifically to this end. %The idea of extracting events and the temporal links between them as a graph was proposed in Tempeval-3. %However, the evaluation still relied on a set of pre-identified events from the TimeBank corpus, leading most of the teams to focus on relation classification. %Despite its importance, the task has received limited attention. %Representative temporal graph extraction systems like  \caevo and \cct break down the problem into sub-tasks, like event identification and relation extraction, and then employ rule-based and statistical systems to solve each sub-task. %Additionally, they use small amounts of hand-labeled corpora for their development, limiting their generalizability and scalability. As an emerging area of nlp, large scale pre-trained language models have made strides in addressing challenging tasks like commonsense knowledge graph completion and task-oriented dialog generation. %Besides relying on an intricate arrangement of sub-systems, they have some common shortcomings: i) They either admit a lot of noisy events  or ignore events from the secondary narrative , ii) generate one-word verbs as events, without adding any context, iii) have limited generalization capabilities by way of relying on rules or small training corpora. % These systems typically fine-tune large language models like gpt or \gptz on a corpus of task-specific dataset. These systems typically fine-tune large language models on a corpus of a task-specific dataset. %However, these advances have not benefited temporal graph extraction.  However, these techniques have not been investigated for temporal graph extraction.  This paper focuses on the problem of generation of an event-level temporal graph for each document, and we refer to this task as contextualized graph generation.   We address this open challenge by proposing a novel reformulation of the task as a sequence-to-sequence mapping problem, which enables us to leverage large pre-trained models for our task. Further, our proposed approach is completely end-to-end and eliminates the need for a pipeline of sub-systems commonly used by traditional methods. %This helps approach is end-to-end, it is not only easier to implement,  approach prevents error propagation across stages and minimizes the effort required for feature engineering.  We also address a related open challenge, which is a prerequisite to our main goal: the difficulty of obtaining a large quantity of training graphs with human-annotated events and temporal relations.   %We address this second challenge with an unsupervised approach, i.e., to  % To this end, we automatically produce a large collection of document-graph pairs by applying existing information extraction and \nlp tools to textual documents, followed by a few rule-based post-processing steps for pruning and noise reduction. Specifically, using \caevo and other tools, we generate a large collection of 89,000 document/graph pairs. To this end, we automatically produce a large collection of document-graph pairs by using \caevo, followed by a few rule-based post-processing steps for pruning and noise reduction.  %Specifically, using \caevo and other tools, we generate a large collection of 89,000 document/graph pairs. %. %which facilitates both large-scale fine-tuning as well as large-scale evaluation of our new approach in comparison with other competing methods.} %The primary block for this union remains the data-hungry nature of large language models; they typically require sizeable datasets for effective training, while popular temporal corpora usually only offer tens to hundreds of hand-labeled documents. %Given that large scale pre-trained language models  %Despite its importance, temporal graph extraction has not benefited from the recent advances in large scale pre-trained language models, which have been effective for  %The limitation does not lie in their representative capabilities.  %Rather, the lack of training data forms the  %The lack of training data forms the biggest bottleneck for this unification: large scale language models typically require large datasets for effective training. Simultaneously, popular temporal corpora usually have tens to hundreds of documents. % bridge this gap by generating a large corpus of 89k document-graph pairs for the task. %We achieve this by first using \caevo as a cheap supervision mechanism for creating a large corpus of dense temporal graphs. %Admittedly, the data generated by \caevo has considerable amounts of error and noise. %We alleviate these issues by injecting human knowledge in the \caevo generated data by applying several post-processing strategies. % Specifically, we remove noisy events and relations extracted with low confidence, and use event clusters to map each graph to its correct context. We then encode the graph in each training pair as a string in the graph representation format \dotlang, transforming the text-to-graph mapping into sequence-to-sequence mapping. %task. We fine-tune \gptz on this dataset of document-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms \caevo on TimeBank-Dense on multiple metrics. Figure 1 shows an example of the input document and the generated graph by our system. %While automatic labeling cannot rival human-curation in quality, our strong experimental results show that the dataset prepared by our method provides a competitive signal to noise ratio at virtually zero cost.  %allowing strong learners to generalize on unseen data. %and use masked-language modeling with \gptz for estimating the conditional distribution of temporal graphs given a document. %Our experiments with \gptz show large gains over strong baselines on our dataset and outperforms \caevo on TimeBank-Dense on a range of metrics. %In the process, we answer several practical questions on selecting salient events, identifying the context for a temporal graph, graph representation, and evaluation. %The system trained on strong results on our data and outperforming \caevo on TimeBank-Dense on a range of metrics. %TODO we are the first to  %Qualitative analysis of nodes generated from our method shows that our approach can successfully use the large training corpus for learning generalized patterns of temporal relations, with error analysis on a held-out set revealing that \caevo fixes the labels for 10\% of the cases. % We use \caevo to label a large corpus of documents and apply novel pruning techniques on top of graphs generated by \caevo.  % These pruning techniques retain the high confidence annotations by \caevo while removing noisy events and relations. % Further, the context for each graph is automatically discovered using the notion of event communities, obviating the need for any hardcoded cutoffs typically adopted in temporal systems. In summary, our main contributions are: %\am{write about three contributions: i) annotation pipeline, ii) encoding to strings, thus allowing the use of gpt, iii) strong results on our data, very good result on \tbden, dramatic improvements over off-the-shelf \gptz}  % % File acl2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{acl2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily\small} \usepackage{color} \usepackage{xspace} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{multirow} \usepackage[multiple]{footmisc} \usepackage{array, booktabs, makecell} \usepackage{graphicx} \usepackage{colortbl} \usepackage{xcolor} \setlength{\textfloatsep}{0.1cm} % This is not strictly necessary, and may be commented out, % but it will improve the layout of the manuscript, % and will typically save some space. \usepackage{microtype}  %\aclfinalcopy % Uncomment this line for the final submission %\def\aclpaperid{***} %  Enter the acl Paper ID here  %\setlength\titlebox{5cm} % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \newcommand\BibTeX{Bib\TeX}  \title{Neural Language Modeling for Contextualized Temporal Graph Generation} \aclfinalcopy \author{Aman Madaan, Yiming Yang \\   Language Technologies Institute, Carnegie Mellon University \\   Pittsburgh, PA, USA \\    \\}  \date{}      {\bf Temporal Graph Extraction } Tempeval-3 introduced the task of temporal graph extraction as ``the ultimate task for evaluating an end-to-end system that goes from raw text to TimeML annotation''.  Tempeval-3 called it Task~.  However, the evaluation still relied on a set of pre-identified events from the TimeBank corpus, leading most of the teams to focus on relation classification. Notable systems developed in response include \caevo, followed by the more recent \cct.   Cogcomptime and \caevo,  Both \caevo and \cct use several statistical and rule-based methods like event extractors, dependency parsers, semantic role labelers, and time expression identifiers for the task.  \caevo assembles a sieve of rule-based systems and classifiers to extract dense temporal graphs from a given document.  i) the graphs generated by \caevo are extremely dense, with a relation extracted for every three words in the document, and an event extracted for every ten words.  ii) \caevo extracts simple verbs as events, without any context,  iii) As discussed later, large numbers of verbs  extracted by \caevo are secondary to the main event, with about 10\  being ``said'', iii) \caevo is a sieve of 11 rule-based and statistical sieves, where the rule-based systems are hand-crafted. The statistical sieves were trained on a small corpus of 36 documents, limiting its generalization. Our work differs from these systems in both the methodology and desired result in the following ways: i) Instead of using specialized sub-systems, we transform the task into a sequence-to-sequence mapping problem and use a single language model to generate such temporal graphs in an end-to-end fashion from text, subsuming all the intermediate-steps.  We are interested in building a single system that can generate the temporal graphs in an end-to-end fashion.  Specifically, we adapt large-scale language models to generate such temporal graphs automatically from the text, subsuming all the intermediate-steps; ii) We develop our system using a corpus of 89,000 documents, which is  300x larger compared to datasets used by \caevo~ and \cct on~;  In contrast, we use a  300x larger corpus. iii) We remove the noisy events included by \caevo, but do not limit the extracted events to any specific semantic axis as done by \cct; and finally, iv) Our method generates graphs where the nodes are not simple verbs but augmented event phrases, containing the subject and the object of each verb.  Besides relying on an intricate arrangement of sub-systems, they have some common shortcomings: i) They either admit a lot of noisy events  or ignore events from the secondary narrative , ii) generate one-word verbs as events, without adding any context, iii) have limited generalization capabilities by way of relying on rules or small training corpora. We use \caevo over \cct to generate a large-scale corpus for our task and to evaluate our system. Two factors informed our decision: i) We found \caevo to be much more scaleable, a critical feature for our task of annotating close to 100k documents, ii) \caevo over-generates  verbs from its output, giving us the flexibility to filter out noisy events without inadvertently missing out on any critical events. However, our method makes no assumption specific to \caevo and is adaptable to any other similar system .  The dataset used for the development of \caevo is  TimeBank-Dense.  Like major temporal corpora, TimeBank-Dense finds its roots in the pioneering   TimeBank corpus, which also spawned the development of the corpora developed for the TempEval tasks 1-3.  TimeBank-Dense, which proposes a dense annotation scheme to improve the coverage of temporal relations extracted from a document.  , which focuses exclusively on the start time of the events for assigning temporal events, has recently emerged as a popular choice for benchmark evaluations of temporal relation extraction systems.  We make no annotation specific assumptions in our approach, and our methods are agnostic to any particular annotation scheme.  We note that the problem of temporal graph extraction is different from the more popular task of Temporal relation extraction , which deals with classifying the temporal link between two already extracted events.  A temporal graph generation system needs to perform the  State of the art Temprel systems use neural methods, but typically use a handful of documents for their development and evaluation. \citet{vashishtha-etal-2019-fine} are a notable exception by using Amazon Mechanical Turks to obtain manual annotations over a larger dataset of 16,000 sentences. We believe that the techniques presented in our work can be applied to scale the corpus used for training Temprel systems.   {\bf Language Models for Graph Generation } The task of generating graphs using language models has gained a lot of attention. Recently, \citet{Bosselut2019COMETCT} proposed , a system that fine-tunes ~ on commonsense knowledge graphs like  and conceptnet~ for commonsense kb completion.   Unlike traditional knowledge graphs, each node in a commonsense knowledge graph is a phrase or a sentence, transforming commonsense kb completion to a problem of conditional text generation. Each node in a commonsense knowledge graph is a phrase or a sentence. Thus, commonsense kb completion is naturally a problem of conditional text generation.  For example, given X acts quickly and a relation like because X wanted, a potentially valid post-condition would be X wanted to finish first.  The input to  consists of a node in the knowledge graph  and relation, and the expected output is a node that appears in Similar to~, we adopt large-scale language models for such a conditional generation of text. However, our task differs from  in the complexity of both the conditioning text and generated text: we seek to generate temporal graphs conditioned on a document, whereas  generates a short event/concept phrase conditioned on a relation and an input event/concept phrase.  Finally, unlike our task, large commonsense knowledge bases that can readily be consumed by systems like  are available.  Other popular graph generation techniques employ a VAE and graph-structure dependent decoding processes. \citet{you2018graphrnn} formulate graphs as a sequence for learning generative models of synthetic and real-world graphs. Similar to their work, we formulate graph generation as an auto-regressive task. However, our goal is the conditional generation of temporal graphs, and not learning unconditional generative distributions. Finally, inspired by recent trends, we don't make any graph specific modifications to the model or the decoding process and formulate the problem as a straightforward sequence-to-sequence mapping task. While our approach does not rely on any particular language model, it would be interesting to see the gains achieved by the much larger  on the dataset produced by our method.\footnote{Not available for research as of September 2020.}  Finally,  show that fine-tuning typically helps the end task regardless of the architecture, and thus we believe that the methods presented are helpful irrespective of the.  It would be interesting to use with large commercial models like  as well.  In contrast, we treat the entire graph as single sequence leverage the expressivity offered by large pre-trained language models.  Crucially, different from all these works, our goal is the conditional generation of temporal graphs, and not learning unconditional generative distributions.  We cede this exploration to future work.    \paragraph{Detecting Salient Events}     In this paper, we improve abstractive dialogue summarization by incorporating commonsense knowledge. We first construct a heterogeneous dialogue graph by introducing knowledge from a large-scale commonsense knowledge base. Then we present a Dialogue Heterogeneous Graph Network  for this task by viewing utterances, knowledge and speakers in the graph as heterogeneous nodes. We additionally design two modules named message fusion and node embedding to facilitate information flow. Experiments on the SAMSum dataset show the effectiveness of our model that can outperform various methods. Zero-shot setting experiments on the Argumentative Dialogue Summary Corpus show that our model can better generalized to the new domain.   
","  {\bf Temporal Graph Extraction } Tempeval-3 introduced the task of temporal graph extraction as ``the ultimate task for evaluating an end-to-end system that goes from raw text to TimeML annotation''.  Tempeval-3 called it Task~.  However, the evaluation still relied on a set of pre-identified events from the TimeBank corpus, leading most of the teams to focus on relation classification. Notable systems developed in response include \caevo, followed by the more recent \cct.   Cogcomptime and \caevo,  Both \caevo and \cct use several statistical and rule-based methods like event extractors, dependency parsers, semantic role labelers, and time expression identifiers for the task.  \caevo assembles a sieve of rule-based systems and classifiers to extract dense temporal graphs from a given document.  i) the graphs generated by \caevo are extremely dense, with a relation extracted for every three words in the document, and an event extracted for every ten words.  ii) \caevo extracts simple verbs as events, without any context,  iii) As discussed later, large numbers of verbs  extracted by \caevo are secondary to the main event, with about 10\  being ``said'', iii) \caevo is a sieve of 11 rule-based and statistical sieves, where the rule-based systems are hand-crafted. The statistical sieves were trained on a small corpus of 36 documents, limiting its generalization. Our work differs from these systems in both the methodology and desired result in the following ways: i) Instead of using specialized sub-systems, we transform the task into a sequence-to-sequence mapping problem and use a single language model to generate such temporal graphs in an end-to-end fashion from text, subsuming all the intermediate-steps.  We are interested in building a single system that can generate the temporal graphs in an end-to-end fashion.  Specifically, we adapt large-scale language models to generate such temporal graphs automatically from the text, subsuming all the intermediate-steps; ii) We develop our system using a corpus of 89,000 documents, which is  300x larger compared to datasets used by \caevo~ and \cct on~;  In contrast, we use a  300x larger corpus. iii) We remove the noisy events included by \caevo, but do not limit the extracted events to any specific semantic axis as done by \cct; and finally, iv) Our method generates graphs where the nodes are not simple verbs but augmented event phrases, containing the subject and the object of each verb.  Besides relying on an intricate arrangement of sub-systems, they have some common shortcomings: i) They either admit a lot of noisy events  or ignore events from the secondary narrative , ii) generate one-word verbs as events, without adding any context, iii) have limited generalization capabilities by way of relying on rules or small training corpora. We use \caevo over \cct to generate a large-scale corpus for our task and to evaluate our system. Two factors informed our decision: i) We found \caevo to be much more scaleable, a critical feature for our task of annotating close to 100k documents, ii) \caevo over-generates  verbs from its output, giving us the flexibility to filter out noisy events without inadvertently missing out on any critical events. However, our method makes no assumption specific to \caevo and is adaptable to any other similar system .  The dataset used for the development of \caevo is  TimeBank-Dense.  Like major temporal corpora, TimeBank-Dense finds its roots in the pioneering   TimeBank corpus, which also spawned the development of the corpora developed for the TempEval tasks 1-3.  TimeBank-Dense, which proposes a dense annotation scheme to improve the coverage of temporal relations extracted from a document.  , which focuses exclusively on the start time of the events for assigning temporal events, has recently emerged as a popular choice for benchmark evaluations of temporal relation extraction systems.  We make no annotation specific assumptions in our approach, and our methods are agnostic to any particular annotation scheme.  We note that the problem of temporal graph extraction is different from the more popular task of Temporal relation extraction , which deals with classifying the temporal link between two already extracted events.  A temporal graph generation system needs to perform the  State of the art Temprel systems use neural methods, but typically use a handful of documents for their development and evaluation. \citet{vashishtha-etal-2019-fine} are a notable exception by using Amazon Mechanical Turks to obtain manual annotations over a larger dataset of 16,000 sentences. We believe that the techniques presented in our work can be applied to scale the corpus used for training Temprel systems.   {\bf Language Models for Graph Generation } The task of generating graphs using language models has gained a lot of attention. Recently, \citet{Bosselut2019COMETCT} proposed , a system that fine-tunes ~ on commonsense knowledge graphs like  and conceptnet~ for commonsense kb completion.   Unlike traditional knowledge graphs, each node in a commonsense knowledge graph is a phrase or a sentence, transforming commonsense kb completion to a problem of conditional text generation. Each node in a commonsense knowledge graph is a phrase or a sentence. Thus, commonsense kb completion is naturally a problem of conditional text generation.  For example, given X acts quickly and a relation like because X wanted, a potentially valid post-condition would be X wanted to finish first.  The input to  consists of a node in the knowledge graph  and relation, and the expected output is a node that appears in Similar to~, we adopt large-scale language models for such a conditional generation of text. However, our task differs from  in the complexity of both the conditioning text and generated text: we seek to generate temporal graphs conditioned on a document, whereas  generates a short event/concept phrase conditioned on a relation and an input event/concept phrase.  Finally, unlike our task, large commonsense knowledge bases that can readily be consumed by systems like  are available.  Other popular graph generation techniques employ a VAE and graph-structure dependent decoding processes. \citet{you2018graphrnn} formulate graphs as a sequence for learning generative models of synthetic and real-world graphs. Similar to their work, we formulate graph generation as an auto-regressive task. However, our goal is the conditional generation of temporal graphs, and not learning unconditional generative distributions. Finally, inspired by recent trends, we don't make any graph specific modifications to the model or the decoding process and formulate the problem as a straightforward sequence-to-sequence mapping task. While our approach does not rely on any particular language model, it would be interesting to see the gains achieved by the much larger  on the dataset produced by our method.\footnote{Not available for research as of September 2020.}  Finally,  show that fine-tuning typically helps the end task regardless of the architecture, and thus we believe that the methods presented are helpful irrespective of the.  It would be interesting to use with large commercial models like  as well.  In contrast, we treat the entire graph as single sequence leverage the expressivity offered by large pre-trained language models.  Crucially, different from all these works, our goal is the conditional generation of temporal graphs, and not learning unconditional generative distributions.  We cede this exploration to future work.    \paragraph{Detecting Salient Events}",111
" Multilingual machine translation , which can serve multiple language pairs with a single model, has attracted much attention. In contrast to bilingual MT systems which can only serve one single language pair, multilingual models can serve  language pairs  .  The amount of available training data can differ a lot across language pairs and the majority of available MT training data is English-centric  which in practice means that most non-English language pairs do not see a single training example when training multilingual models .  As a consequence, the actual performance of language pairs that do not include English on the source or target side lags behind the ones with large amounts of training data. Further, when increasing the number of languages, it gets  impractical to gather training data for each language pair and  challenging to find the right mix during training. Which is why models tasked with direct translation between non-English pairs either resort to bridging  through a pivot language , or make use of synthetic parallel data   or study the problem under zero-shot settings .     In this study, we make use of the potential pre-existing multi-way property in the training corpora and generate as many direct training examples from pre-existing English-centric training data. If we can find training examples for each language pair in a multilingual mix, we call this model complete Multilingual Neural Machine Translation . cMNMT is then trained on all bilingual pairs between source and target languages by utilizing multi-way aligned training examples that consist of translations of the same sentence into multiple languages. We resurface multi-way aligned training examples by aligning training examples from different language pairs when either their source or target sides are identical .  To make use of this data, the model samples a source and target language from the set of multi-way aligned corpus during training, which allows the model to see language pairs where originally no training data existed . As our experiments support, this method enables us to get access to training data for all tested language pairs ). We will show that it is possible to generate a complete graph for at least a 6-language WMT setup. Some of the WMT training data is multi-way parallel by construction. Nevertheless, we show that we also find many training examples where the source and target origin from different sources. We further show on our 112 languages internal dataset, that we can find sufficient training data for over 12,000 language pairs by only providing 111 English-centric training corpora. This result indicates that it is possible to generate direct training data for many language pairs without the need for crawling new training examples. Our experiments suggest that before falling back to methods like zero-shot translation, you should investigate the structure of your pre-existing training data.  To address the problem of finding the right mix of examples from different language pairs during training, we further introduce a hierarchical sampling strategy that is language-specific . In addition to fixing some chronic issues of MNMT , the proposed sampling strategy efficiently ensures all source-target pairs are covered.  Experiments demonstrate that we can train a cMNMT model on a 30-language-pair WMT setup that outperforms bilingual and multilingual baselines as well as bridging on all non-English language pairs. We further show that the performance of the English language pairs stay stable and do not suffer from the changes in both the training data and the new training data sampling strategy. Furthermore, we share experiments at scale by demonstrating that we can train a cMNMT model that can serve  12,432 language pairs.  Our contribution is three-fold:      \paragraph{Direct models} To translate between languages with little training data, three general approaches emerged, i. bridging through a third language  , ii. generating pseudo-parallel data between direct language pairs and training the direct pairs with that   and, iii. zero-shot methods where the model is asked to translate a direct pair only at test time .   Although pivot-based approaches perform sufficiently good when cascaded with strong bilingual models , their practicality is limited due to compounding errors from pipelining and doubled inference cost. The zero-resource approaches, combined with iterative-back translation  are quite powerful but their inefficiency is worth noting. For  languages, one needs to devise a training routine that could sample  pairs, generate pseudo-parallel data. The added time to generate pseudo-parallel data for every pair grows quadratically, making it challenging for systems considering a large number of languages. Recently, by devising a practical sub-sampling approach,  demonstrated zero-resource techniques could be scaled to massively multilingual setup. We find the study by  closest to our work, having the goal of any-to-any multilingual translation. But compared to sampling language pairs with no parallel data and generating pseudo-parallel data on-the-fly, our approach makes use of existing multi-way alignment information before training. Lastly, zero-shot approaches attempt to measure the generalization performance of the MNMT models, but to date, the zero-shot quality still trails behind the pivot and zero-resource methods . Our proposed cMNMT, naturally fills the gap between these three approaches, the multi-way data can be extracted offline, and efficiently be mixed with the original data using a hierarchical data sampler. It does not require extra steps to generate pseudo-parallel data, and  it handily outperforms zero-shot approaches.   \paragraph{N-way data} In this paper, we only made use of multi-way aligned data to sample bilingual pairs out of it. But there exist several approaches that make use of the multi-view structure in the data, such as , who explored the use of small multi-parallel corpora a for one-to-many NMT. Another approach is multi-source NMT . Although multi-source NMT is a promising direction, it has practical problems such as lacking multiple sources at inference time . We believe research in this direction will be the key to improve mid/high-resource NMT and address several robustness issues to the input noise.   recently released MultiParaCrawl where the authors extracted direct data for non-English language pairs from the English-centric Paracrawl corpus.  \paragraph{Sampling scheduling}  Several approaches proposed to address data sampling for multi-task models, some relying on temperature-based heuristics , others relying on adaptive schedules that incorporate the model gains, baselines or quality expectations into the data schedulers . We believe data sampling is a critical research area for not only MNMT but also multi-task learning in general. We reveal a critical failure mode of the commonly used temperature sampling strategy, and how it causes the poor translation quality while translating out of English.     In this paper, we demonstrated a dialog generation framework that mimics the data creation process employed by crowd-sourced workers. We find that our method is able to generate meaningful conversations that aids the training of end-task dialog models in both, low resource and full data settings. The use of additional simulated data to train end-task dialog models result in a performance improvement of 18-25\  in low resource settings, and when combined with full training data, we find that the performance of a simple GPT2 based end-task model becomes comparable to current state-of-the-art models. The simulation-framework does not make strict assumptions about the domain or dataset and it would be interesting to explore its use in other dialogue tasks such as Persona-Chat  in future work.  which we wish to explore in our future work.  We include qualitiatve results demonstrating the se
"," \paragraph{Direct models} To translate between languages with little training data, three general approaches emerged, i. bridging through a third language  , ii. generating pseudo-parallel data between direct language pairs and training the direct pairs with that   and, iii. zero-shot methods where the model is asked to translate a direct pair only at test time .   Although pivot-based approaches perform sufficiently good when cascaded with strong bilingual models , their practicality is limited due to compounding errors from pipelining and doubled inference cost. The zero-resource approaches, combined with iterative-back translation  are quite powerful but their inefficiency is worth noting. For  languages, one needs to devise a training routine that could sample  pairs, generate pseudo-parallel data. The added time to generate pseudo-parallel data for every pair grows quadratically, making it challenging for systems considering a large number of languages. Recently, by devising a practical sub-sampling approach,  demonstrated zero-resource techniques could be scaled to massively multilingual setup. We find the study by  closest to our work, having the goal of any-to-any multilingual translation. But compared to sampling language pairs with no parallel data and generating pseudo-parallel data on-the-fly, our approach makes use of existing multi-way alignment information before training. Lastly, zero-shot approaches attempt to measure the generalization performance of the MNMT models, but to date, the zero-shot quality still trails behind the pivot and zero-resource methods . Our proposed cMNMT, naturally fills the gap between these three approaches, the multi-way data can be extracted offline, and efficiently be mixed with the original data using a hierarchical data sampler. It does not require extra steps to generate pseudo-parallel data, and  it handily outperforms zero-shot approaches.   \paragraph{N-way data} In this paper, we only made use of multi-way aligned data to sample bilingual pairs out of it. But there exist several approaches that make use of the multi-view structure in the data, such as , who explored the use of small multi-parallel corpora a for one-to-many NMT. Another approach is multi-source NMT . Although multi-source NMT is a promising direction, it has practical problems such as lacking multiple sources at inference time . We believe research in this direction will be the key to improve mid/high-resource NMT and address several robustness issues to the input noise.   recently released MultiParaCrawl where the authors extracted direct data for non-English language pairs from the English-centric Paracrawl corpus.  \paragraph{Sampling scheduling}  Several approaches proposed to address data sampling for multi-task models, some relying on temperature-based heuristics , others relying on adaptive schedules that incorporate the model gains, baselines or quality expectations into the data schedulers . We believe data sampling is a critical research area for not only MNMT but also multi-task learning in general. We reveal a critical failure mode of the commonly used temperature sampling strategy, and how it causes the poor translation quality while translating out of English.",112
"  Machine Translation  has shown impressive progress in recent years. Neural architectures have greatly contributed to this  improvement, especially for languages with abundant training data.  This progress creates novel challenges for the evaluation of machine translation,  both for human and automated evaluation  protocols.  Both types of evaluation play an important role in machine translation. While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations  therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system development.  The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics. Most metrics such as \BLEU and TER measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed.  Orthogonal to the work of building improved metrics,  hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human  `translationese` effects. These standard references might favor systems which excel at reproducing these effects, independent of the underlying translation quality. They showed that better correlation between human and automated evaluations could be obtained when replacing standard references with paraphrased references, even when still using surface overlap metrics such as BLEU~. The novel references, collected by asking linguists to paraphrase standard references, were shown to steer evaluation away from rewarding translation artifacts. This improves the assessment of alternative, but equally good translations.  Our work builds on the success of paraphrased translations for evaluating  existing systems, and asks if different design choices could have been made when designing a system with such an evaluation protocol in mind. This examination has several potential benefits: it can help identify choices which improve BLEU on standard references but have limited impact on final human evaluations; or those that result in better translations for the human reader, but worse in terms of standard reference BLEU. Conversely, it might turn out that paraphrased references are not robust enough to support system development due to the presence of `metric honeypots': settings that produce poor translations, but which are nevertheless assigned high BLEU scores.  To address these points, we revisit the major design choices of the best EnglishGerman system from WMT2019 step-by-step, and measure their impact on standard reference BLEU as well as on paraphrased BLEU. This allows us to measure the extent to which steps such as data cleaning, back-translation, fine-tuning, ensemble decoding and reranking benefit standard reference BLEU more than paraphrase BLEU. Revisiting these development choices with the two metrics results in two systems with quite different behaviors. We conduct a human evaluation for adequacy and fluency to assess the overall impact of designing a system using paraphrased BLEU.  Our main findings show that optimizing for paraphrased BLEU is advantageous for human evaluation when compared to an identical system optimized for standard BLEU. The system optimized for paraphrased BLEU significantly improves WMT newstest19 adequacy ratings  and fluency ratings  despite scoring 5 BLEU points lower on standard references.      Collecting human paraphrases of existing references has recently been shown to be useful for system evaluation. Our work considers applying the same methodology for system tuning. There is some earlier work relying on automated paraphrases  for system tuning, especially for Statistical Machine Translation .  introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and showed that this permitted reliable system tuning using half as much data. Similar automatic paraphrasing has also been used to augment training data, e.g., but relying on standard references for evaluation. In contrast to human paraphrases, the quality of current machine generated paraphrases degrades significantly as overlap with the input decreases. This makes their use difficult for evaluation since suggests that substantial paraphrasing -- `paraphrase as much as possible` -- is necessary for evaluation.  Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning  with Minimum Error Rate Training, MERT . This work showed some specific cases where Translation Error Rate  was superior to \BLEU.  Our work is also related to the bias that the human translation process introduces in the references, including source language artifacts---Translationese---as well as source-independent artifacts---Translation Universals. The professional translation community studies both systematic biases inherent to translated texts , as well as biases resulting specifically from interference from the source text . For MT, \citet{Freitag19} point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well~. More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation . Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied.  Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation,  and  explored how the translation direction affects translation results.  noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for target-original sentences are on average higher than sentences that have their original source in a different language. Recently, a similar study was conducted for neural MT.    In this work, we introduced complete Multilingual Neural Machine Translation  that exploits the multi-way alignment information in the underlying training data to improve translation quality for language pairs where training data is scared or not available. Standard MNMT models are trained on a joint set of different training corpora for a variety of language pairs. cMNMT combines the different corpora and constructs multi-way aligned training examples that consist of translations of the same sentence into multiple languages.   In combination with a novel temperature-based sampling approach that is conditioned on the target language only, we show that cMNMT is superior to the standard MNMT model and the even better-performing bridging approach.    Experimental results on a public WMT 30 language pairs dataset and an in-house 12,432 language pairs dataset demonstrated an average BLEU increase of more than 10 BLEU points for non-English language pairs. This approach leads to a single NMT model that can serve 12,432k language pairs with reasonable quality which also surpasses the translation quality of the bridging approach, which is nowadays used in most modern MT services.    
","   Collecting human paraphrases of existing references has recently been shown to be useful for system evaluation. Our work considers applying the same methodology for system tuning. There is some earlier work relying on automated paraphrases  for system tuning, especially for Statistical Machine Translation .  introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and showed that this permitted reliable system tuning using half as much data. Similar automatic paraphrasing has also been used to augment training data, e.g., but relying on standard references for evaluation. In contrast to human paraphrases, the quality of current machine generated paraphrases degrades significantly as overlap with the input decreases. This makes their use difficult for evaluation since suggests that substantial paraphrasing -- `paraphrase as much as possible` -- is necessary for evaluation.  Our work can be seen as replacing the regular BLEU metric with a new paraphrase BLEU metric for system tuning. Different alternative automatic evaluation metric have also been considered for system tuning  with Minimum Error Rate Training, MERT . This work showed some specific cases where Translation Error Rate  was superior to \BLEU.  Our work is also related to the bias that the human translation process introduces in the references, including source language artifacts---Translationese---as well as source-independent artifacts---Translation Universals. The professional translation community studies both systematic biases inherent to translated texts , as well as biases resulting specifically from interference from the source text . For MT, \citet{Freitag19} point at Translationese as a source of mismatch between BLEU and human evaluation, raising concerns that overlap-based metrics might reward hypotheses with translationese language more than hypotheses using more natural language. The impact of Translationese on human evaluation of MT has recently received attention as well~. More generally, the question of bias to a specific reference has also been raised, in the case of monolingual manual evaluation . Different from the impact of Translationese on evaluation, the impact of Translationese in the training data has also been studied.  Finally, our work is also related to studies measuring the importance of the test data quality, looking specifically at the test set translation direction. For SMT evaluation,  and  explored how the translation direction affects translation results.  noted that the original language of the test sentences influences the BLEU score of translations. They showed that the BLEU scores for target-original sentences are on average higher than sentences that have their original source in a different language. Recently, a similar study was conducted for neural MT.",113
"     % Demonstrating intelligent behavior in complex environments requires agents that can reason about entities and their relationships, and identify regularities in structured data which can help predict the properties-of and relationships-between entities.  % Understanding natural language in realistic settings requires models that can reason about the interactions between content and context, model the dependencies between different textual elements and leverage information about authors when interpreting their content.  For example, when analyzing interactions in a social network, leveraging information about users' social behavior can help identify similarities in the contents of posts they author. Dealing with this type of relational data requires making predictions over multiple, often inter-dependent, variables.    Understanding natural language interactions in realistic settings requires models that can deal with noisy textual inputs, reason about the dependencies between different textual elements and leverage the dependencies between textual content and the context from which it emerges. Work in linguistics and anthropology has defined context as a frame that surrounds a focal communicative event and provides resources for its interpretation . %\citealt{contextualization-92} introduced the term contextualization cues as signalling mechanisms in communication that add to the shared understanding between the participants, into relationships, the situation, and the environment of the conversation    %Say something about debate networks and add some references. As a motivating example, consider the interactions in the debate network  described in Fig.. Given a debate claim , and two consecutive posts debating it , we define a textual inference task, determining whether a pair of text elements hold the same stance in the debate }). This task is similar to other textual inference tasks which have been successfully approached using complex neural representations. In addition, we can leverage the dependencies between these decisions.  For example, assuming that one post agrees with the debate claim }}), and the other one does not }}), the disagreement between the two posts can be inferred:  {\small \PRED{\neg Agree\wedge Agree \rightarrow \neg Agree}}. Finally, we consider the social context of the text. The disagreement between the posts can reflect a difference in the perspectives their authors hold on the issue. While this information might not be directly observed, it can be inferred using the authors' social interactions and behavior. % Given the principle of social homophily, stating that people with strong social ties are likely to hold similar views and authors' perspectives can be captured by representing their social interactions. Exploiting this information requires models that can align the social representation with the linguistic one.  Motivated by these challenges, we introduce \DRAIL, a Deep Relational Learning framework, which uses a combined neuro-symbolic representation for modeling the interaction between multiple decisions in relational domains. Similar to other neuro-symbolic approaches our goal is to exploit the complementary strengths of the two modeling paradigms. Symbolic representations, used by logic-based systems and by probabilistic graphical models, are interpretable, and allow domain experts to directly inject knowledge and constrain the learning problem. Neural models capture dependencies using the network architecture and are better equipped to deal with noisy data, such as text. However, they are often difficult to interpret and constrain according to domain knowledge.   Our main design goal in \DRAIL is to provide a generalized tool, specifically designed for NLP tasks. Existing approaches designed for classic relational learning tasks, such as knowledge graph completion, are not equipped to deal with the complex linguistic input. While others are designed for very specific NLP settings such as word-based quantitative reasoning problems or aligning images with text. We discuss the differences between \DRAIL and these approaches in Section.  % While the examples in this paper focus on modelings various argumentation mining tasks and their social and political context, the same principles can be applied to wide array of NLP tasks with different contextualizing information, such as images that appear next to the text, or prosody when analyzing transcribed speech, to name a few examples. %TODO: explain why DRAIL is specifically useful for NLP compared to other languages. We don't do the same type of evaluation  as we are interested in working with raw entities.     %  Entities in \DRAIL are either human-interpretable discrete entities , which we refer to as symbols, or raw entities that have a complex internal structure which cannot be easily represented as a symbol . This view allows us to define two conceptual learning tasks: relations connecting raw and symbolic entities }}), and relations connecting raw inputs to each other, which define inference tasks }}).   % \DRAIL uses a declarative language for defining deep relational models. Similar to other declarative languages, it allows users to inject their knowledge by specifying dependencies between decisions using first-order logic rules, which are later compiled into a factor graph with neural potentials.   % In addition to probabilistic inference, \DRAIL also models dependencies using a distributed knowledge representation, denoted \relnets, which provides a shared representation space for entities and their relations, trained using a relational multi-task learning approach. This provides a mechanism for explaining symbols, and aligning representations from different modalities.  %Introduce the s-s, r-r, s-r, distinction as a way to support classification, textual inference, and probabilistic inference. Following our running example, ideological standpoints, such as \PRED{Liberal} or \PRED{Conservative}, are discrete entities embedded in the same space as textual entities and social entities. These entities are initially associated with users, however using \relnets this information will propagate to texts reflecting these ideologies, by exploiting the relations that bridge social and linguistic information . % In the resulting shared embedding space, we can explain these ideological standpoints in terms of users holding them, or texts that express them.%}).    %TODO: what are the research questions    %TODO - explain the difference in task from DRAIL's perspective - argument relations inside a single text, analyzing discussions - the simple case, discussed in the literature, where we predict a symbol , and the debate.org setup where we combine textual inference  with soclia linfo    To demonstrate \DRAIL's modeling approach, we introduce the task of open-domain stance prediction with social context, which combines social networks analysis and textual inference over complex opinionated texts, as shown in Fig. . %Unlike traditional stance prediction tasks, where the prediction problem is defined over a fixed set of issues  ~ , we go beyond coarse-grained definitions, and delve into the specific arguments and questions of each discussion, as shown in Fig. . We follow the intuition that debates are part of a broader online conversation, involving multiple people that contribute or express their support for the different views, and explicitly model these interactions.  % AugensteinD16-1084,P18-2123,C18-1316} %TODO: add some discussion about qualitative evaluation % We complement our evaluation of \DRAIL with two additional tasks, issue-specific stance prediction, where we identify the views expressed in debate forums with respect to a set of fixed issues, and argumentation mining, a document-level discourse analysis task.    %We demonstrate \DRAIL's modeling approach over three challenging problems. Argumentation mining, a document-level discourse analysis task. Debate stance prediction, identifying the views expressed-in, and interactions-between, debate forum posts. Finally, we introduce a new problem, open-domain stance prediction with social context, which combines social networks analysis and textual inference over complex opinionated texts.  In all three tasks we evaluate different modeling choices, obtaining competitive results.    %TODO: contributions %Our contributions are summarized as follows: % %    %Unrealted TODO: add a discussion about globally normalized RELNETs- the constraints and the multiple objectives shape them.  %homophily, %, This phenomenon was previously used to help overcome language variation issues   % political-social representations %network embedding:we learn a graph embedding, a different way to define social context %graphical models way     }     \caption{Comparing Systems}     The difficulty of building complex machine learning models over relational data has attracted considerable attention in the machine learning community.  In this section, we survey several lines of work dealing with symbolic, neural and hybrid representations for relational learning.      The difficulty of building complex machine learning models over relational data has attracted considerable attention in the machine learning community and   Several high level languages for specifying graphical models have been suggested. BLOG and CHURCH  were suggested for generative models. For discriminative models, we have Markov Logic Networks  and Probabilistic Soft Logic . Both PSL and MLNs combine logic and probabilistic graphical models in a single representation, where each formula is associated with a weight, and the probability distribution over possible assignments is derived from the weights of the formulas that are satisfied by such assignments. Like DRaiL, PSL uses formulas in clausal form . The main difference between \DRAIL and these languages is that, in addition to graphical models, it uses distributed knowledge representations to represent dependencies. Other discriminative methods include  FACTORIE, an imperative language to define factor graphs, Constraints Conditional Models   an interface to enhance linear classifiers with declarative constraints, and ProPPR a probabilistic logic for large databases that approximates local groundings using a variant of personalized PageRank.       Graph networks,  Node embeddings,  Joint text/node embeddings,  Relational embeddings A recent alternative to graphical models is to use neural nets to  represent and learn over relational data, represented as a graph. Similar to \DRAIL's \relnets, the learned node representation can be trained by several different prediction tasks. However, unlike \DRAIL, these methods do not use probabilistic inference to ensure consistency.  Node embeddings approaches learn a feature representation for nodes capturing graph adjacency information, such that the similarity in the embedding space of any two nodes is  proportional to their graph distance and overlap in neighbouring nodes. Some frameworks allow nodes to have textual properties, which provide an initial feature representation when learning to represent the graph relations. When dealing with multi-relational data, such as knowledge graphs, both the nodes and the edge types are embedded . Finally, these methods learn to represent nodes and relations based on pair-wise node relations, without representing the broader graph context in which they appear. Graph neural nets create contextualized node representations by recursively aggregating neighbouring nodes' information.      Several recent systems explore ways to combine neural and symbolic representations in a unified way. We group them into five categories.    \subsubsection{ Lifted rules to specify compositional nets. These systems use an end-to-end approach and learn relational dependencies in a latent space. Lifted Relational Neural Networks   and RelNNs  are two examples. These systems map observed ground atoms, facts and rules to specific neurons in a network and define composition functions directly over them. While they provide for a modular abstraction of the relational inputs, they assume all inputs are symbolic and do not leverage expressive encoders.    \subsubsection{Works that focus on inference}  Differentiable inference. These systems identify classes of logical queries that can be compiled into differentiable functions in a neural network infrastructure. In this space we have Tensor Logic Networks   and TensorLog . Symbols are represented as row vectors in a parameter matrix. The focus is on implementing reasoning using a series of numeric functions.    \subsubsection{Works that focus on rule induction} Rule induction from data. These systems are designed for inducing rules from symbolic knowledge bases, which is not in the scope of our framework. In this space we find Neural Theorem Provers  , Neural Logic Programming , DRUM  and Neural Logic Machines  . NTPs use a declarative interface to specify rules that add inductive bias and perform soft proofs. The other approaches work directly over the database.   Deep classifiers and probabilistic inference. These systems propose ways to integrate probabilistic inference and neural networks for diverse learning scenarios. DeepProbLog  extends the probabilistic logic programming language ProbLog to handle neural predicates. They are able to learn probabilities for atomic expressions using neural networks. The input data consists of a combination of feature vectors for the neural predicates, together with other probabilistic facts and clauses in the logic program. Targets are only given at the output side of the probabilistic reasoner, allowing them to learn each example with respect to a single query.   Moreover, we focus on encoding entities and relations.  On the other hand, Deep Probabilistic Logic   combines neural networks with probabilistic logic for indirect supervision. They learn classifiers using neural networks and use probabilistic logic to introduce distant supervision and labeling functions. Each rule is regarded as a latent variable, and the logic defines a joint probability distribution over all labeling decisions. Then, the rule weights and the network parameters are learned jointly using variational EM. In contrast, \DRAIL focuses on learning multiple interdependent decisions from data, handling and requiring supervision for all unknown atoms in a given example.     \subsubsection Lastly, Deep Logic Models   learn a set of parameters to encode atoms in a probabilistic logic program. Similarly to \citealt{ijcai2017-221} and \citealt{DBLP:journals/jair/CohenYM20}, they use differentiable inference, allowing the model to be trained end-to-end. Like \DRAIL, DLMs can work with diverse neural architectures and backpropagate back to the base classifiers. The main difference between DLMs and \DRAIL is that \DRAIL ensures representation consistency of entities and relations across all learning tasks by employing \relnets.  Finally, our focus in this paper is highly noisy inputs such as text and social data, requiring us to employ powerful encoders. DLMs experiments rely on simple encoders for raw data  and symbolic inputs .      Deep structured models. More generally, deep structured prediction approaches have been successfully applied to various NLP tasks such as NER and dependency parsing. When the need arises to go beyond sentence-level, some works combine the output scores of independently trained classifiers using inference, while others implement joint learning for their specific domains . Our main differentiating factor is that we provide a general interface that leverages FOL clauses to specify factor graphs and express constraints.  Additionally, we have a principled way to build relational neural networks that allows us to include expressive state-of-the-art encoders for raw entities .       Works that focus on inference,  Works that use lifted rules a way to specify compositional neural architectures,  Works that focus on inducing rules for symbolic knowledge bases,  DLMs,  More generally, deep structured prediction.      To summarize these differences, we outline a feature matrix in Tab. . Given our focus in NLP tasks, we require a neural-symbolic system that  allows us to integrate state-of-the-art text encoders and NLP tools,  supports structured prediction across long texts,  lets us combine several modalities and their representations  and  results in an explainable model where domain constraints can be easily introduced.        \ml{Cover the current state on NLP approaches for argumentative tasks like the ones we care about}      Several recent models attempted to take advantage of these complimentary strengths by combining different elements of these models. These models focus on different aspects, which can limit their applicability to certain problem types, or learning scenarios. For example,  \citeauthor{Kazemi018-0} \shortcite{Kazemi018-0} propose a convolution-based architecture for combining relevant relations designed for predicting entity labels. \citeauthor{DBLP:journals/corr/abs-1806-01261} \shortcite{DBLP:journals/corr/abs-1806-01261} suggest a graph embedding approach forcing relational inductive bias on the learned representation.  While the model suggested by \citeauthor{NIPS2018_7632} \shortcite{NIPS2018_7632} focuses on learning tree structured programs.        In this paper we look into combining such declarative frameworks with deep learning models. Combining deep learning with structured models has been studied in previous work, typically in the context of a specific task or a specific inference procedure. These include dependency parsing, transition systems, named entity recognition and sequence labeling systems and argument mining .   Recently, researchers have revisited the insights of the neural-symbolic integration community  and explored more general frameworks to combine the best of both worlds. For example, introducing relational inductive biases to neural architectures using graph networks , using logical rules to specify neural networks , defining logical constraints over specific neurons  and extending probabilistic programming to process neural predicates .   	In this paper, we present the Bi-directional Cognitive Thinking Network  which corresponding to the Bi-directional Cognitive Knowledge Framework  from the perspective of psychology. The BCTN answers the question with bi-directional knowledge by simulating the inertial thinking and reverse thinking. And we decouple these two parts of knowledge rather than couple them with the same module. 	  in a bi-directional way of thinking that stemmed from cognitive psychology.  	To determine the stimulus intensity of reverse thinking in memory, we consider the decoded tokens to calculate the score based on the gate mechanism. We show that the proposed BCTN is very effective, it has competitiveness with the previous methods in literature on DuReader with a single model. Our future work will consider to use different datasets and design various models to simulate the behavior of our brain to capture human-level language understanding and intelligence. Finally, we believe that our framework can generalize to other generative tasks, 	such as summarization and image caption. 	 	
","  }     \caption{Comparing Systems}     The difficulty of building complex machine learning models over relational data has attracted considerable attention in the machine learning community.  In this section, we survey several lines of work dealing with symbolic, neural and hybrid representations for relational learning.      The difficulty of building complex machine learning models over relational data has attracted considerable attention in the machine learning community and   Several high level languages for specifying graphical models have been suggested. BLOG and CHURCH  were suggested for generative models. For discriminative models, we have Markov Logic Networks  and Probabilistic Soft Logic . Both PSL and MLNs combine logic and probabilistic graphical models in a single representation, where each formula is associated with a weight, and the probability distribution over possible assignments is derived from the weights of the formulas that are satisfied by such assignments. Like DRaiL, PSL uses formulas in clausal form . The main difference between \DRAIL and these languages is that, in addition to graphical models, it uses distributed knowledge representations to represent dependencies. Other discriminative methods include  FACTORIE, an imperative language to define factor graphs, Constraints Conditional Models   an interface to enhance linear classifiers with declarative constraints, and ProPPR a probabilistic logic for large databases that approximates local groundings using a variant of personalized PageRank.       Graph networks,  Node embeddings,  Joint text/node embeddings,  Relational embeddings A recent alternative to graphical models is to use neural nets to  represent and learn over relational data, represented as a graph. Similar to \DRAIL's \relnets, the learned node representation can be trained by several different prediction tasks. However, unlike \DRAIL, these methods do not use probabilistic inference to ensure consistency.  Node embeddings approaches learn a feature representation for nodes capturing graph adjacency information, such that the similarity in the embedding space of any two nodes is  proportional to their graph distance and overlap in neighbouring nodes. Some frameworks allow nodes to have textual properties, which provide an initial feature representation when learning to represent the graph relations. When dealing with multi-relational data, such as knowledge graphs, both the nodes and the edge types are embedded . Finally, these methods learn to represent nodes and relations based on pair-wise node relations, without representing the broader graph context in which they appear. Graph neural nets create contextualized node representations by recursively aggregating neighbouring nodes' information.      Several recent systems explore ways to combine neural and symbolic representations in a unified way. We group them into five categories.    \subsubsection{ Lifted rules to specify compositional nets. These systems use an end-to-end approach and learn relational dependencies in a latent space. Lifted Relational Neural Networks   and RelNNs  are two examples. These systems map observed ground atoms, facts and rules to specific neurons in a network and define composition functions directly over them. While they provide for a modular abstraction of the relational inputs, they assume all inputs are symbolic and do not leverage expressive encoders.    \subsubsection{Works that focus on inference}  Differentiable inference. These systems identify classes of logical queries that can be compiled into differentiable functions in a neural network infrastructure. In this space we have Tensor Logic Networks   and TensorLog . Symbols are represented as row vectors in a parameter matrix. The focus is on implementing reasoning using a series of numeric functions.    \subsubsection{Works that focus on rule induction} Rule induction from data. These systems are designed for inducing rules from symbolic knowledge bases, which is not in the scope of our framework. In this space we find Neural Theorem Provers  , Neural Logic Programming , DRUM  and Neural Logic Machines  . NTPs use a declarative interface to specify rules that add inductive bias and perform soft proofs. The other approaches work directly over the database.   Deep classifiers and probabilistic inference. These systems propose ways to integrate probabilistic inference and neural networks for diverse learning scenarios. DeepProbLog  extends the probabilistic logic programming language ProbLog to handle neural predicates. They are able to learn probabilities for atomic expressions using neural networks. The input data consists of a combination of feature vectors for the neural predicates, together with other probabilistic facts and clauses in the logic program. Targets are only given at the output side of the probabilistic reasoner, allowing them to learn each example with respect to a single query.   Moreover, we focus on encoding entities and relations.  On the other hand, Deep Probabilistic Logic   combines neural networks with probabilistic logic for indirect supervision. They learn classifiers using neural networks and use probabilistic logic to introduce distant supervision and labeling functions. Each rule is regarded as a latent variable, and the logic defines a joint probability distribution over all labeling decisions. Then, the rule weights and the network parameters are learned jointly using variational EM. In contrast, \DRAIL focuses on learning multiple interdependent decisions from data, handling and requiring supervision for all unknown atoms in a given example.     \subsubsection Lastly, Deep Logic Models   learn a set of parameters to encode atoms in a probabilistic logic program. Similarly to \citealt{ijcai2017-221} and \citealt{DBLP:journals/jair/CohenYM20}, they use differentiable inference, allowing the model to be trained end-to-end. Like \DRAIL, DLMs can work with diverse neural architectures and backpropagate back to the base classifiers. The main difference between DLMs and \DRAIL is that \DRAIL ensures representation consistency of entities and relations across all learning tasks by employing \relnets.  Finally, our focus in this paper is highly noisy inputs such as text and social data, requiring us to employ powerful encoders. DLMs experiments rely on simple encoders for raw data  and symbolic inputs .      Deep structured models. More generally, deep structured prediction approaches have been successfully applied to various NLP tasks such as NER and dependency parsing. When the need arises to go beyond sentence-level, some works combine the output scores of independently trained classifiers using inference, while others implement joint learning for their specific domains . Our main differentiating factor is that we provide a general interface that leverages FOL clauses to specify factor graphs and express constraints.  Additionally, we have a principled way to build relational neural networks that allows us to include expressive state-of-the-art encoders for raw entities .       Works that focus on inference,  Works that use lifted rules a way to specify compositional neural architectures,  Works that focus on inducing rules for symbolic knowledge bases,  DLMs,  More generally, deep structured prediction.      To summarize these differences, we outline a feature matrix in Tab. . Given our focus in NLP tasks, we require a neural-symbolic system that  allows us to integrate state-of-the-art text encoders and NLP tools,  supports structured prediction across long texts,  lets us combine several modalities and their representations  and  results in an explainable model where domain constraints can be easily introduced.        \ml{Cover the current state on NLP approaches for argumentative tasks like the ones we care about}      Several recent models attempted to take advantage of these complimentary strengths by combining different elements of these models. These models focus on different aspects, which can limit their applicability to certain problem types, or learning scenarios. For example,  \citeauthor{Kazemi018-0} \shortcite{Kazemi018-0} propose a convolution-based architecture for combining relevant relations designed for predicting entity labels. \citeauthor{DBLP:journals/corr/abs-1806-01261} \shortcite{DBLP:journals/corr/abs-1806-01261} suggest a graph embedding approach forcing relational inductive bias on the learned representation.  While the model suggested by \citeauthor{NIPS2018_7632} \shortcite{NIPS2018_7632} focuses on learning tree structured programs.        In this paper we look into combining such declarative frameworks with deep learning models. Combining deep learning with structured models has been studied in previous work, typically in the context of a specific task or a specific inference procedure. These include dependency parsing, transition systems, named entity recognition and sequence labeling systems and argument mining .   Recently, researchers have revisited the insights of the neural-symbolic integration community  and explored more general frameworks to combine the best of both worlds. For example, introducing relational inductive biases to neural architectures using graph networks , using logical rules to specify neural networks , defining logical constraints over specific neurons  and extending probabilistic programming to process neural predicates .",114
" Transformer based models  have been proven to be very effective in building the state-of-the-art Neural Machine Translation  systems via neural networks and attention mechanism . Following the standard Sequence-to-Sequence architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and position-wise feed-forward network.   Multihead attentions and position-wise feed-forward network, together as a basic unit,  plays an essential role in the success of Transformer models.  Some researchers  propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention.   Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow  in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit models advance in expressiveness.  Second, for the multi-unit setting, one unit could mitigate the deficiency of other units and compose a more expressive network, in a complementary way.  In this paper, we propose the Multi-Unit TransformErs , which aim to promote the expressiveness of transformer models by introducing diverse and complementary parallel units. Merely combining multiple identical units in parallel improves model capability and diversity by its varied feature compositions. Furthermore, inspired by the well-studied bagging  and gradient boosting algorithms  in the machine learning field, we design biased units with a sequential dependency to further boost model performance.  Specifically, with the help of a module named bias module, we apply different kinds of noises to form biased inputs for corresponding units. By doing so, we explicitly establish information gaps among units and guide them to learn from each other.  Moreover, to better leverage the power of complementariness, we introduce sequential ordering into the multi-unit setting, % by learning a permutaion matrix  to automatically shuffle the outputs of multiple units,  and force each unit to learn the residual of its preceding accumulation.  We evaluate our methods on three widely used Neural Machine Translation datasets, NIST Chinese-English,  WMT'14 English-German and WMT'18 Chinese-English. Experimental results show that our multi-unit model yields an improvement of +1.52, +1.90 and +1.10 BLEU points, over the baseline model  for three tasks with different sizes, respectively.  Our model even outperforms the Transformer-Big on the WMT'14 English-German by 0.7 BLEU points with only 54\% of parameters.  Moreover, as an interesting side effect, our model only introduces mild inference speed decrease  compared with the Transformer-Base model, and is faster than the Transformer-Big model. % which proves the practicability of our methods.   The contributions of this paper are threefold:       Neural Machine Translation  is an essential task in Natural Language Processing.    Traditional approaches  adopt RNN-based methods to learn the mapping from the source side to the target side.    Recently, studies exploit the potential of self-attention based networks  for the sake of high parallelism and larger model capacity. Recently Transformer-based models  become the de facto methods in Neural Machine Translation, owing to high parallelism and large model capacity.  Some researchers devise new modules to improve the Transformer model, including combining the transformer unit with convolution networks , improving the self-attention architecture, and deepening the Transformer architecture by dense connections.  Since our multi-unit framework makes no limitation about its unit, these models can be easily integrated into our multi-unit framework.  There are also some works utilizing the power of multiple modules to capture complex feature representations in NMT. \citet{shazeer2017outrageously} use a vast network and a sparse gated function to select from multiple experts . \citet{ahmed2017weighted} train a weighted Transformer by replacing the multi-head attention by self-attention branches. Nevertheless, these models ignore the modeling of relations among different modules. Then, some multihead attention variants  introduce modeling of diversity or interaction among heads. However, complementariness is not taken into account in their approaches. Our MUTE models differ from their methods in two aspects. First, we use a powerful unit with a strong performance in diversity . Second, we explicitly model the complementariness with bias module and sequential dependency.    Differently, our approaches apply multi-unit setting with more powerful units as described in Section   However, their work either misses the diversity or neglects the complementariness among multiple units. On the contrary, we explicitly introduce the bias module and sequential dependency to guide the diversity and complementariness among units.   , explicitly.       Also, some studies use multi-unit methods to improve other NLP tasks.   \citet{meng2019multi} aims to improve language modeling with multiple space composition and proposes a Multi-zone   Unit for Recurrent Neural Networks.    \citet{tao2018get} proposes to use Multihead Attention to capture multiple semantics in dialogue generation.   The Multi-Unit framework can be extended to other NLP tasks.     In this work\footnotemark\footnotetext{ .}, we explored different ways in which trained models can be applied to improve AMR parsing performance via self-learning. Despite the recent strong improvements in performance through novel architectures, we show that the proposed techniques improve performance further, achieving new state-of-the-art on AMR 1.0 and AMR 2.0 tasks without the need for extra human annotations.        uncomment to redo bbl    
","   Neural Machine Translation  is an essential task in Natural Language Processing.    Traditional approaches  adopt RNN-based methods to learn the mapping from the source side to the target side.    Recently, studies exploit the potential of self-attention based networks  for the sake of high parallelism and larger model capacity. Recently Transformer-based models  become the de facto methods in Neural Machine Translation, owing to high parallelism and large model capacity.  Some researchers devise new modules to improve the Transformer model, including combining the transformer unit with convolution networks , improving the self-attention architecture, and deepening the Transformer architecture by dense connections.  Since our multi-unit framework makes no limitation about its unit, these models can be easily integrated into our multi-unit framework.  There are also some works utilizing the power of multiple modules to capture complex feature representations in NMT. \citet{shazeer2017outrageously} use a vast network and a sparse gated function to select from multiple experts . \citet{ahmed2017weighted} train a weighted Transformer by replacing the multi-head attention by self-attention branches. Nevertheless, these models ignore the modeling of relations among different modules. Then, some multihead attention variants  introduce modeling of diversity or interaction among heads. However, complementariness is not taken into account in their approaches. Our MUTE models differ from their methods in two aspects. First, we use a powerful unit with a strong performance in diversity . Second, we explicitly model the complementariness with bias module and sequential dependency.    Differently, our approaches apply multi-unit setting with more powerful units as described in Section   However, their work either misses the diversity or neglects the complementariness among multiple units. On the contrary, we explicitly introduce the bias module and sequential dependency to guide the diversity and complementariness among units.   , explicitly.       Also, some studies use multi-unit methods to improve other NLP tasks.   \citet{meng2019multi} aims to improve language modeling with multiple space composition and proposes a Multi-zone   Unit for Recurrent Neural Networks.    \citet{tao2018get} proposes to use Multihead Attention to capture multiple semantics in dialogue generation.   The Multi-Unit framework can be extended to other NLP tasks.",115
"  %  %     Prior work primarily focused on exploiting visual patterns using carefully crafted features . These rendering-based methods have two major drawbacks: 1) they are expensive since they require downloading all external files including CSS, javascript, and images to render the page to compute visual features; 2) they require carefully crafted heuristics around visual proximity  to work well with these expensive features. In this paper, we propose a novel two-stage neural architecture, named FreeDOM, that can be trained on a small number of seed websites and generalize well to unseen websites without requiring any hand-engineered visual features. %we want to employ neural networks for learning transferable visual features such that we can eliminate the need of rendering and human engagement in crafting textual patterns. %We propose a novel two-stage neural architecture that can directly learn from a few annotated websites just based on their raw HTML content and transfer the models for unseen websites without using any human labels . %We parse HTML documents as DOM Trees of the page and classifies it into one of the target fields. This node-level module combines neighboring character sequences, token sequences, as well as markup  to learn a combined representation for the node. We propose a combination of CNNs and LSTMs and show that it can effectively encode useful features in DOM nodes.  These node representations are encoded individually and inevitably lose some global information useful for an extraction task. In particular, only relying on local node features can cause failure when value nodes have no obvious patterns themselves or their local features are very similar to other non-value nodes. To mimic the signal that may be available through visual features used in rendering-based methods,  we use a relational neural network as our second module . This allows us to model the relationship between a pair of elements using both distance-based and semantic features. The rationale behind this is to learn more global representations of node pairs so that we can jointly predict node labels instead of relying only on local features.   Extensive experimental results on a large-scale public dataset, the Structured Web Data Extraction  corpus, show that our model consistently outperforms competitive baseline methods by a large margin.  The proposed FreeDOM is able to generalize to unseen sites after training on a small number of seed sites. In fact, we show that with training data from just three seed sites, our approach out-performs techniques that use explicit visual rendering features by 3.7 F1 points on average. To the best of our knowledge, our framework is among the first neural architectures that efficiently obtains high-quality representations of web documents for structured information extraction.   \eat{Our framework utilizes minimal human efforts in feature engineering and does not require any rendering results, thus making large-scale information extraction on web documents much easier and more effort-light. We believe the proposed model can be promising in other applications that require neural representations of web documents.}  %The node-level module itself can predict node labels for identifying values of interested fields, but the encoded local features cannot capture the long-range dependencies between values and thus degenerate in unlabeled target websites. %To address this problem, we further propose a relational neural network. %As the second-stage module, it explicitly models the relations between DOM nodes and effectively learns the page-level constraints for producing structured predictions. % it models the relational features reflected by each node pairs, and finally conducts structured data extraction as a structured prediction problem.  %Our contributions in this paper are three-fold: %%  %}   %Our contribution is that we propose a novel neural model, FreeDom, for structured data extraction over web documents while using less information  and no hand-crafted features. Extensive experiments on a large-scale public data set  show that the proposed FreeDom outperforms other strong baseline methods while only using raw features. %%ying{The last sentence looks not complete. \yuchen{Done.}} %\tata{Don't say 'less information' emphasize that not requiring visual rendering is cheaper and not requiring hand-crafted features means you can generalize to new tasks better. Need to make this claim more focused so the contributions are clear. We also need to spell out the two stages more clearly 'First stage does blah', 'Second stage does blah'} %\tata{Might be worth adding that entity resolution is not in scope for this work -- ie, we might extract duplicate entries across websites for the same car. There are many papers dealing with that and we're not focused on that in this paper.} %     Our work builds on research in wrapper induction in the data mining community and the contributions of neural models for information extraction tasks in the NLP community.  We apply it in the context of modeling semi-structured web documents with neural networks. Specifically, we aim to build a more lightweight yet transferable model by getting rid of expensive rendering-based features and complex human-crafted algorithms.   Thus, we discuss our work in the following three context.  \smallskip Structured Data Extraction.\quad Structured data extraction from web documents has been studied extensively in supervised settings. Early works usually require a significant number of human-crafted rules or labels for inducing a wrapper , which is only used for a particular web site. These wrappers are usually brittle when testing on unseen websites in the same vertical, although they can have high precision and recall on the training sites. \quad  Some recent works propose methods that can adapt to new websites.  Zhai et al. employed active learning methods that can find the most influential new examples in target websites for human annotators to label, and therefore adapting the existing wrappers to new sites in a cheaper way. However, in practice, active-learning methods not only constantly require  human effort in building specialized annotation tools but also need humans to label samples for each new site of interest. A recent work by Lockard et al. attempted to use additional knowledge bases as distant supervision to automatically label some samples in the target websites and then learn a machine learning model on such noisy-labeled data. A large and comprehensive knowledge base is not always accessible and available for every domain. Consequently, their methods do not apply for emerging domains without first requiring the human effort of building a knowledge base. To address the problem in a more domain-general setting, we do not impose any human prior knowledge on the values, and focus on a purely unsupervised model adaptation learning scenario.  \smallskip Transferable Extraction Models.\quad Hao et al. proposed a method that was based on visual distance features on a rendered web page. This achieved promising results on unseen websites without using any new human annotations.  Following their work, there are a series of rendering-based extraction models proposed for many different settings. However,  these rendering-based methods need to download all the external files including CSS style files, javascripts and images such that they can render a page with browser engines to know page layouts. In addition, they have to design heuristic algorithms for computing human-crafted distance metrics. These drawbacks together make large-scale extraction less practical and efficient. Our method, , instead is totally based on the HTML content itself without any use of external resources or page renders. When using two or more seed websites to train,  outperforms even the approach with expensive human-crafted features on visually-rendered pages.    \yuchen{About one page.}   These rendering-based methods need to download all the external files including CSS style files, javascripts and images such that they can further render a page with browser engines to know page layouts.   \yuchen{move to related work}   Other than that, they usually have to design heuristic algorithms to obtain useful visual distance-based features from human priors.   Model Transfer with Neural Networks    \smallskip Neural Architectures for Information Extraction.~  Another advantage of our method is that it can work well without any human-crafted features or patterns. We adopt state-of-the-art sequence modeling techniques from IE and NLP to build our char-based word representations, XPath LSTMs, and position embeddings. Our pair-level network is inspired by the relational reasoning networks, which learns to induce relational features between semantic units . A similar idea of encoding word-pairs as vectors also show promising results for reasoning about cross-sentence relations in natural language processing. These together help us eliminate the need for human effort in both designing features and metric algorithms. The learned website-invariant neural features further make our model more transferable. Recent advances in natural language processing also show an emerging interest in learning powerful deep neural features for richly formatted texts and semi-structured data. Our work agrees with the findings in Fonduer: neural networks can effectively replace hand-crafted features for extracting data in richly formatted documents. To the best of our knowledge, we are among the first approach using neural networks for learning to represent DOM nodes in web pages to solve structured prediction.     Semi-Structured Document Modeling      In this paper, we propose Multi-Unit Transformers for NMT to improve the expressiveness by introducing diverse and complementary units.  In addition, we propose two novel techniques, namely bias module and sequential dependency to further improve the diversity and complementariness among units.    We show that merely integrate several identical units can improve model performance and diversity. Furthermore, we introduce Biased Multi-Unit and Sequentially Biased Multi-Unit towards explicit guidance of interaction between units.     We evaluate our methods on two widely used NMT datasets. Experimental results show that our methods can significantly outperform the baseline methods and achieve comparable / better performance compared with existing strong NMT systems. In the meantime, our methods use much fewer parameters and only introduce mild inference speed degradation, which proves the efficiency of our models.        \clearpage   
","   Our work builds on research in wrapper induction in the data mining community and the contributions of neural models for information extraction tasks in the NLP community.  We apply it in the context of modeling semi-structured web documents with neural networks. Specifically, we aim to build a more lightweight yet transferable model by getting rid of expensive rendering-based features and complex human-crafted algorithms.   Thus, we discuss our work in the following three context.  \smallskip Structured Data Extraction.\quad Structured data extraction from web documents has been studied extensively in supervised settings. Early works usually require a significant number of human-crafted rules or labels for inducing a wrapper , which is only used for a particular web site. These wrappers are usually brittle when testing on unseen websites in the same vertical, although they can have high precision and recall on the training sites. \quad  Some recent works propose methods that can adapt to new websites.  Zhai et al. employed active learning methods that can find the most influential new examples in target websites for human annotators to label, and therefore adapting the existing wrappers to new sites in a cheaper way. However, in practice, active-learning methods not only constantly require  human effort in building specialized annotation tools but also need humans to label samples for each new site of interest. A recent work by Lockard et al. attempted to use additional knowledge bases as distant supervision to automatically label some samples in the target websites and then learn a machine learning model on such noisy-labeled data. A large and comprehensive knowledge base is not always accessible and available for every domain. Consequently, their methods do not apply for emerging domains without first requiring the human effort of building a knowledge base. To address the problem in a more domain-general setting, we do not impose any human prior knowledge on the values, and focus on a purely unsupervised model adaptation learning scenario.  \smallskip Transferable Extraction Models.\quad Hao et al. proposed a method that was based on visual distance features on a rendered web page. This achieved promising results on unseen websites without using any new human annotations.  Following their work, there are a series of rendering-based extraction models proposed for many different settings. However,  these rendering-based methods need to download all the external files including CSS style files, javascripts and images such that they can render a page with browser engines to know page layouts. In addition, they have to design heuristic algorithms for computing human-crafted distance metrics. These drawbacks together make large-scale extraction less practical and efficient. Our method, , instead is totally based on the HTML content itself without any use of external resources or page renders. When using two or more seed websites to train,  outperforms even the approach with expensive human-crafted features on visually-rendered pages.    \yuchen{About one page.}   These rendering-based methods need to download all the external files including CSS style files, javascripts and images such that they can further render a page with browser engines to know page layouts.   \yuchen{move to related work}   Other than that, they usually have to design heuristic algorithms to obtain useful visual distance-based features from human priors.   Model Transfer with Neural Networks    \smallskip Neural Architectures for Information Extraction.~  Another advantage of our method is that it can work well without any human-crafted features or patterns. We adopt state-of-the-art sequence modeling techniques from IE and NLP to build our char-based word representations, XPath LSTMs, and position embeddings. Our pair-level network is inspired by the relational reasoning networks, which learns to induce relational features between semantic units . A similar idea of encoding word-pairs as vectors also show promising results for reasoning about cross-sentence relations in natural language processing. These together help us eliminate the need for human effort in both designing features and metric algorithms. The learned website-invariant neural features further make our model more transferable. Recent advances in natural language processing also show an emerging interest in learning powerful deep neural features for richly formatted texts and semi-structured data. Our work agrees with the findings in Fonduer: neural networks can effectively replace hand-crafted features for extracting data in richly formatted documents. To the best of our knowledge, we are among the first approach using neural networks for learning to represent DOM nodes in web pages to solve structured prediction.     Semi-Structured Document Modeling",116
"    Data-to-Text aims at generating natural language descriptions from structured data ; fostered by recent advances on neural approaches  and  %for data-to-text have been made possible by  the emergence of large scale datasets made of  pairs . Figure illustrates an example from the WikiBIO dataset . These datasets are either hand-crafted via crowdworkers or automatically built by aligning sources found on the Internet. As such, %training examples are imperfect reference texts might include divergences of two types, limiting the ability of generation models to produce realistic descriptions. First, reference texts might contain information not grounded in the source data;  especially for automatically constructed datasets, where references were not written with the source-data description task in mind. For instance, the phrase ``who served as lieutenant [...]'' in  Figure has no basis in the associated infobox. Second, reference texts do not always cover the entirety of the table . In most settings, this second point is referred to as content selection and is inherent of most data-to-text tasks. % and is part of the normal subtask flow of data-to-text. %; see for example Figure and information about wars.  %However, some hand-crafted datasets are designed such that annotators have been asked to transcribe every fields, and systems are also expected to do the same. In this case, incomplete references can lead models to fail to learn to transcribe all information, and only partially cover data-sources at inference. However, some hand-crafted datasets are designed where annotators are asked to transcribe every fields, with models also expected to do the same. In this case, incomplete references  can lead to models failing to learn to transcribe all information, and only partially cover data-sources at inference.   Divergence in training examples leads to hallucinated/omitted content in model output; which is a well-known problem in neural approaches for text generation . This problem arises both from the training procedure , and from the testing protocols. Indeed, current standard metrics only measure similarity  to ground truth reference texts and do not fully capture relevance to the source data. %Indeed, most evaluation metrics  work on computing precision of n-grams contained in the generated sentence w.r.t to the ground truth description.  Thus, there is no distinction between a mismatch caused by a paraphrase, poor lexicalization of content, or made-up/incorrect statement, leading to imperfect model selection. While a number of work argue for the need for novel automatic evaluation method , to the best of our knowledge only \citet{wiseman2017} and \citet{dhingra2019} propose metrics based on both the reference and the source data. %Additionally, \citet{dhingra2019} show that their proposed metric PARENT correlates more strongly with human evaluation than any other metric, while being easier to use out of the box.  Recently, different regularization methods have also been proposed to mitigate the negative influence of divergences in reference texts. These approaches can be either at the dataset level , where authors propose techniques to clean/standardize instances; or at the training level , where authors propose novel neural modules designed to limit hallucinations/omissions. However, these approaches are severely limited: e.g., they require significant annotation labor, model-specific tricks and/or manual tuning.   Furthermore, virtually all proposed neural approaches still suffer from 1)~exposure bias and 2)~inconsistency between train/test measurement. Indeed, current neural models are trained via a mechanism called teacher forcing , where the decoder is fed the previous correct token, no matter its actual prediction~, in order to maximize the log-likelihood of the target sentence , but are evaluated through the previously discussed n-gram metrics~. See Section for a more detailed discussion about this subject.\\  %On one hand, not controllable approaches have been proposed: for example, \citet{Liu2019hier} train a hierarchical encoder-decoder on three auxiliary tasks  which are meant to guide the decoding process, in order to achieve descriptions with higher fidelity with respect to the conditioning input.  To the best of our knowledge, there have been few approaches  focused on the training procedure. %We cite , where \citet{liu2019} train a hierarchical encoder-decoder on three auxiliary tasks  which are meant to guide the decoding process. %, in order to achieve descriptions with higher fidelity with respect to the conditioning input. Closest to our work, \citet{Liu2019b} propose a novel neural module for constrained attention, along with a reinforcement learning  training procedure based on BLEU and TFIDF. In our work, to remedy the above shortcomings and building upon the work of \citet{Liu2019b}, we show that no novel neural module is necessary to handle hallucinations and omissions. We propose a model-agnostic RL framework, called PARENTing, where pretrained models are further trained with a self-critical policy gradient algorithm  to limit the impact of divergences in training examples on text generation. Specifically, we use the PARENT metric   which exhibits a strong correlation with human evaluation, while being easier to use out of the box. We provide extensive automatic evaluations on two data-to-text model families  on two widely used benchmarks , as well as a more focused human evaluation %to high-light differences in several training procedures  on WikiBIO.  We report new state of the art PARENT scores on both datasets while BLEU scores are on par with previous SOTA approaches, which shows that our framework efficiently reduces pathological behaviors while keeping generation fluent.       %To remedy those shortcomings, we propose a model-agnostic reinforcement learning framework, called PARENTing, where pretrained models are further trained with a self-critical policy gradient algorithm  to limit the impact of divergences in training examples on text generation. % inspired by recent advancements in other text generation fields.  %Specifically, %we fine-tune pretrained models with a self-critical policy gradient algorithm  based on  %we use the PARENT metric   which exhibits a strong correlation with human evaluation, while being easier to use out of the box. We provide extensive evaluations on two data-to-text model families  on two widely used benchmarks . We report new state of the art PARENT scores on both datasets while BLEU scores are on par with previous approaches, which shows that our framework efficiently reduces pathological behaviors while keeping generation fluent.     %In the following, we first review in Section data-to-text approaches as well as the recent attempts at controlling hallucinations/omissions.  We then introduce in Section our model-agnostic framework limiting hallucinations/omissions in the generation. The evaluation protocol is presented in Section, followed by the obtained results . Section concludes the paper and presents perspectives.  %In the following, we first present a state-of-the art of attempts to reduce hallucinations and to address the exposure bias and inconsistencies in between train/test measurement in data-to-text literature . revoir la structure We then describe in details the PARENT metric from \citet{dhingra2019} in Section and the our proposed RL training framework in Section. The evaluation protocol is presented in Section, followed by the results . Section concludes the paper and presents perspectives.        {remettre 鑴 jour l'鑼卼at de l'art avec papiers r鑼卌ents - ne serait-ce que citer les refs}    Data-to-text refers to the task of generating natural language descriptions from structured data .  Data-to-text models    can be classified in two broad categories: knowledge-based models and stochastic/data-driven approaches .  First approaches are knowledge-based, i.e. designed by domain expert .  The former approaches   are driven by experts' knowledge, leading to a pipeline architecture split into subtasks: content selection and text structuring , sentence planning  and generating actual sentences . While accurate and efficient at inference time, these methods require significant manual efforts for new use-cases. In contrast, data-driven approaches tend to blur the distinction between these subtasks with end-to-end training on large corpora of aligned input data and output text . End-to-end methods have been proposed early, such as  who apply statistical machine translation techniques to the sportcasting domain. Recent neural approaches now propose to leverage progress in deep learning to represent these data into a semantic vector space  and stem from the neural machine translation domain . \\  , and most work  model the data records as a single sequence of facts to be entirely translated into natural language.\\  For instance, \citet{lebret2016} were the first to propose a large-scale neural language model conditioned on tabular data, where each word embedding was computed following both its place in a fixed vocabulary, and its eventual position in the tabular data being transcribed.   All subsequent work use the standard encoder-decoder architecture .  Particularly, \citet{wiseman2017} propose the now by default back-bone data-to-text architecture, with an attention mechanism , which computes a context focused on important elements from the input, and a copy mechanism  to deal with unknown or rare words.  To address domain-specific constraints, a common approach is to build architectures that explicitly model the key-value structure of the input table~.   In particular, to handle the need for content selection, \citet{puduppully2018} design a more complex two-step decoder, which first generates a plan of elements from the source data to be mentioned, and then conditions text generation on this plan.  Additional work  introduces dynamic encoding updating, where the  model updates part of the source data encoding at each decoding step in order to  more  accurately guide the decoder throughout generation.   \citet{liu2018} explore a structure-aware LSTM: the proposed encoder has an added gate for field names, while the decoder uses dual attention, where attention scores are computed on both field names and values, to form a final attention score via term-wise multiplication.  While these models produce fluent and domain-comprehensive outputs, several pathological behaviors have been identified, echoing similar issues in other text generation tasks .    Training neural model on data-to-text tasks requires large corpora . Different pathological behaviors arise from the  datasets, depending on the methodology underlying their construction. First, for hand-crafted datasets , crowdworkers sometimes fail to cover all information from the data source in reference text. Second, automatically constructed datasets from possibly different internet sources do not guarantee data sources and texts to be aligned completely.   since they might have been taken from different raw sources.  that the reference texts are self-contained in data sources since they may contain additional information.   For instance, \citet{dhingra2019} showed that more than  of references in the WikiBIO dataset contain phrases not grounded in the associated infobox.  Both these limitations induce neural generation model to omit information in the first case or suffer from hallucinations  in the other.   To deal with these pathologies, previous work operate either at the dataset level, or at the training level. At the dataset level, \citet{dusek2019} show that cleaned data can significantly improve system ability to produce fact-accurate text. In a different direction, \citet{nie2019} apply a method similar to knowledge distillation :  they train a Natural Language Understanding module to reconstruct tables from text references and show that a vanilla sequence-to-sequence model trained on the refined data has improved content correctness in both human and automatic evaluations.  , when compared with the current state-of-the-art.  At the training level,  stemming from translation techniques, \citet{wiseman2017}, for instance, propose to include a reconstruction loss aiming at reconstructing the source table from the hidden states of the decoder.  They train their model with a loss computed as a combination of the loss on the target side and the loss on the input side.  In an other direction, \citet{perez-beltrachini2018} propose a classifying neural network, trained  to label text tokens depending on their alignment with the associated table. They use these labels in an RL framework to generate sentences with a maximum of aligned tokens. However these approaches are either costly in human labor or specific to hand-crafted datasets where the input data matches exactly the reference texts . Indeed reconstruction tasks are not compatible with the content selection subtask of Data-to-Text.  Proposing both a novel coverage-constrained attention and a BLEU/TFIDF-based reward,  constitutes a first approach to a model-agnostic framework. However their proposed coverage is still task specific : while they increase the state-of-the-art BLEU on WikiBIO, they underperfom  encoder-decoder models on the PARENT benchmark. , showing that BLEU focused systems might not be ideal.  Until recently, the NLG research community cruelly lacked ways to automatically evaluate model outputs.  Until recently there was no way to correctly automatically evaluate model outputs. Despite work on effective human evaluation , and on the need for better automated metrics , to the best of our knowledge, only \citet{wiseman2017} and \citet{dhingra2019} recently proposed improvement over the widely used BLEU. \citet{wiseman2017} propose to use an auxiliary neural model, trained to extract structured records from the generated text for evaluation. Two texts can then be compared through their sequences of extracted records. This information retrieval-based approach suffers from domain specificity, as the released model only works in the closed-domain of basketball journalistic summaries, and requires precise tagging of gold references which can be impossible to provide in most settings. Furthermore, \citet{dhingra2019} propose a new metric PARENT, and show that this metric strongly correlates with human annotators and can replace previous n-gram- and information retrieval-based metrics. \\  Our contribution differs from previous work in several aspects. First, our proposed framework is model-agnostic and can be used with any neural model. Second, instead of focusing on only one domain and/or one issue , it is setting agnostic and tackles both hallucinations and omissions at once by leveraging the PARENT F-score . Finally, no manual preprocessing or pre-tagging is required: models are trained via a flexible training protocol and distantiate themselves from faulty training examples.    In this paper, we propose a neural architecture for extracting structured data from web documents. It uses training data from only a few seed sites but generalizes well to other unseen websites in the same vertical. We show that our approach, , beats the previous state-of-the-art performance on a large-scale public dataset consisting of 8 different verticals  by nearly 3.7 F1 points. In particular, it does so without using any expensive rendering-based visual features.  We also discovered that typical sequence labeling techniques from NLP do not work well for this task and presented hypotheses for why that is the case.  We believe that this work opens up multiple avenues for future research in web data extraction. What structured prediction techniques might work better at incorporating information from farther away and work well on large DOM trees with sparse labels?  An even more interesting question is if we can transfer information across verticals? That is, if we are able to do well on one vertical, can we leverage that information somehow to train a model for the next vertical? Will there be a large pre-trained neural encoding model for HTML documents, just like BERT for plain texts?  We believe our work can be also useful for future research that needs to learn a more site-general neural representations for semi-structured documents including web pages, pdf files and so on.         The next two lines define the bibliography style to be used, and    the bibliography file.   \clearpage   \clearpage     \endinput         End of file `sample-authordraft.tex'.  
","   {remettre 閼 jour l'閼煎嵓at de l'art avec papiers r閼煎崒ents - ne serait-ce que citer les refs}    Data-to-text refers to the task of generating natural language descriptions from structured data .  Data-to-text models    can be classified in two broad categories: knowledge-based models and stochastic/data-driven approaches .  First approaches are knowledge-based, i.e. designed by domain expert .  The former approaches   are driven by experts' knowledge, leading to a pipeline architecture split into subtasks: content selection and text structuring , sentence planning  and generating actual sentences . While accurate and efficient at inference time, these methods require significant manual efforts for new use-cases. In contrast, data-driven approaches tend to blur the distinction between these subtasks with end-to-end training on large corpora of aligned input data and output text . End-to-end methods have been proposed early, such as  who apply statistical machine translation techniques to the sportcasting domain. Recent neural approaches now propose to leverage progress in deep learning to represent these data into a semantic vector space  and stem from the neural machine translation domain . \\  , and most work  model the data records as a single sequence of facts to be entirely translated into natural language.\\  For instance, \citet{lebret2016} were the first to propose a large-scale neural language model conditioned on tabular data, where each word embedding was computed following both its place in a fixed vocabulary, and its eventual position in the tabular data being transcribed.   All subsequent work use the standard encoder-decoder architecture .  Particularly, \citet{wiseman2017} propose the now by default back-bone data-to-text architecture, with an attention mechanism , which computes a context focused on important elements from the input, and a copy mechanism  to deal with unknown or rare words.  To address domain-specific constraints, a common approach is to build architectures that explicitly model the key-value structure of the input table~.   In particular, to handle the need for content selection, \citet{puduppully2018} design a more complex two-step decoder, which first generates a plan of elements from the source data to be mentioned, and then conditions text generation on this plan.  Additional work  introduces dynamic encoding updating, where the  model updates part of the source data encoding at each decoding step in order to  more  accurately guide the decoder throughout generation.   \citet{liu2018} explore a structure-aware LSTM: the proposed encoder has an added gate for field names, while the decoder uses dual attention, where attention scores are computed on both field names and values, to form a final attention score via term-wise multiplication.  While these models produce fluent and domain-comprehensive outputs, several pathological behaviors have been identified, echoing similar issues in other text generation tasks .    Training neural model on data-to-text tasks requires large corpora . Different pathological behaviors arise from the  datasets, depending on the methodology underlying their construction. First, for hand-crafted datasets , crowdworkers sometimes fail to cover all information from the data source in reference text. Second, automatically constructed datasets from possibly different internet sources do not guarantee data sources and texts to be aligned completely.   since they might have been taken from different raw sources.  that the reference texts are self-contained in data sources since they may contain additional information.   For instance, \citet{dhingra2019} showed that more than  of references in the WikiBIO dataset contain phrases not grounded in the associated infobox.  Both these limitations induce neural generation model to omit information in the first case or suffer from hallucinations  in the other.   To deal with these pathologies, previous work operate either at the dataset level, or at the training level. At the dataset level, \citet{dusek2019} show that cleaned data can significantly improve system ability to produce fact-accurate text. In a different direction, \citet{nie2019} apply a method similar to knowledge distillation :  they train a Natural Language Understanding module to reconstruct tables from text references and show that a vanilla sequence-to-sequence model trained on the refined data has improved content correctness in both human and automatic evaluations.  , when compared with the current state-of-the-art.  At the training level,  stemming from translation techniques, \citet{wiseman2017}, for instance, propose to include a reconstruction loss aiming at reconstructing the source table from the hidden states of the decoder.  They train their model with a loss computed as a combination of the loss on the target side and the loss on the input side.  In an other direction, \citet{perez-beltrachini2018} propose a classifying neural network, trained  to label text tokens depending on their alignment with the associated table. They use these labels in an RL framework to generate sentences with a maximum of aligned tokens. However these approaches are either costly in human labor or specific to hand-crafted datasets where the input data matches exactly the reference texts . Indeed reconstruction tasks are not compatible with the content selection subtask of Data-to-Text.  Proposing both a novel coverage-constrained attention and a BLEU/TFIDF-based reward,  constitutes a first approach to a model-agnostic framework. However their proposed coverage is still task specific : while they increase the state-of-the-art BLEU on WikiBIO, they underperfom  encoder-decoder models on the PARENT benchmark. , showing that BLEU focused systems might not be ideal.  Until recently, the NLG research community cruelly lacked ways to automatically evaluate model outputs.  Until recently there was no way to correctly automatically evaluate model outputs. Despite work on effective human evaluation , and on the need for better automated metrics , to the best of our knowledge, only \citet{wiseman2017} and \citet{dhingra2019} recently proposed improvement over the widely used BLEU. \citet{wiseman2017} propose to use an auxiliary neural model, trained to extract structured records from the generated text for evaluation. Two texts can then be compared through their sequences of extracted records. This information retrieval-based approach suffers from domain specificity, as the released model only works in the closed-domain of basketball journalistic summaries, and requires precise tagging of gold references which can be impossible to provide in most settings. Furthermore, \citet{dhingra2019} propose a new metric PARENT, and show that this metric strongly correlates with human annotators and can replace previous n-gram- and information retrieval-based metrics. \\  Our contribution differs from previous work in several aspects. First, our proposed framework is model-agnostic and can be used with any neural model. Second, instead of focusing on only one domain and/or one issue , it is setting agnostic and tackles both hallucinations and omissions at once by leveraging the PARENT F-score . Finally, no manual preprocessing or pre-tagging is required: models are trained via a flexible training protocol and distantiate themselves from faulty training examples.",117
" Relation classification  aims to identify the relation between two specified entities in a sentence.  Previous supervised approaches on this task heavily depend on human-annotated data, which limit their performance on classifying the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community ,  first introduce the few-shot learning to RC task and propose the FewRel %   dataset. % dataset as the benchmark. Recently, many works focus on this task and achieve remarkable performance .  %distant supervision  is proposed to automatically construct training instances for RC. %However, in the dataset extracted by distant supervision, some long-tail relations only have few instances and suffer from data sparsity problem. %Inspired by the success of few-shot learning  methods in the computer vision community, e.g., Matching Network , Relation Network  and Memory-augmented network ,  first introduce FSL to RC to tackle the long tail problem. They use the Prototypical Network , which achieves the state-of-the-art performance on several FSL benchmarks. Recently, many works followed their framework have achieved remarkable performance on the Few-shot RC dataset FewRel . %The prototypical network learns the representation  for each relation based on sampled instances, and then classifies the queries into a set of pre-defined relations. %\CheckedBox   % Even though the existing works perform well, they all assume that there is only one relation in a sentence.   Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted  as evidence. When specified two entities  in the sentence, there is a great opportunity for the instance to be incorrectly categorized as a {\color{red}{confusing relation}}  instead of the {\color{blue}{true relation}} . % % Specifically,  %is that different entity pairs are usually described  in an input sentence, in which the relation classification of these entity pairs often interferes with each other. %This results in that the entity pairs of these relations are often misclassified into confusing relations by models without the ability of explicitly decoupling easily-confused relations. %Table shows three instances from the FewRel dataset , each of which contains a sentence with two given entities  on the right side, and its positive  and confusing relations  on the left side.  %Previous few-shot methods tend to misclassify these sentences into the confusing relations. the first instance should be categorized as the true relation `parents-child' based on the given entity pair and natural language  expression `a daughter of'. However, since it also includes the NL expression `his wife',  %which describes the confusing relation `husband-wife', it is probably misclassified into this confusing relation `husband-wife'. In this paper, we name it as a relation confusion problem.   %===============================================================================================  % \verb|\checkmark|: \checkmark \par % \verb|\cmark|: \cmark \par % \verb|\xmark|: \xmark   	{blue}} and {\color{red}{red}} words respectively correspond to true and confusing relations.} 	 		 \end{table} %============================================================================================== To address the relation confusion problem, it is crucial for a model to %  effectively select the information with high relevance to the given entity pair and  be aware of which NL expressions cause confusion and learn to avoid mapping the instance into its easily-confused relation. % To address the relation confusion problem, it is crucial for a model to be aware of which NL expressions cause confusion and how to explicitly distinguish the easily-confused relations.  From these perspectives, we propose two assumptions.  Firstly, in a sentence, words that keep high relevance to the given entities are more important in expressing the true relation. Intuitively, the specified entity information is crucial to identify the true relations.  Secondly, explicitly learning of mapping an instance into its confusing relation with augmented data in turn boosts a few-shot RC model on identifying the true relation. % allowing the model to explicitly learn the confusing relations can help it to identify the true relations. %Intuitively, the specific entities information is helpful to identify the positive relation.  Based on these assumptions, we propose CTEG, a few-shot RC model with two novel mechanisms:  An Entity-Guided Attention  encoder, which leverages the syntactic relations and relative positions between each word and the specified entity pair to softly select important information of words expressing the true relation and filter out the information causing confusion.  A Confusion-Aware Training  method, which explicitly learns to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. %has the ability of explicitly learning to distinguish easily-confused relations. In addition, inspired by the success of pre-trained language models, our approaches are based on BERT , which has been proved effective especially for few-shot learning tasks. %===========================================================================   % Specifically, when encoding a sentence by the attention mechanism, our EGA guides the calculation of the attention score through multiply it by an entity-aware gate. %we adopt the transformer incorporating with a self-attention mechanism to encoding an input instance,  Specifically, the backbone of the encoder of our model is a transformer equipped with the proposed EGA which guides the calculation of self-attention distributions by weighting the attention logits with entity-guided gates. % The gate is a matrix of relevance scores, which are used to measure the importance of each word according to its relevance to the entities. % The gates are used to measure the importance of each word according to its relevance to the entities. The gates are used to measure the relevance between each word and the given two entities. % Two types of position information of the words are used to calculate the scores. One of them is the relative position , which is the relative distance between a word and an entity in the sentence squence. Two types of information for each word are used to calculate its gate. % One of them is the relative position , which is the relative distance between a word and an entity in the sentence squence. One is the relative position  information, which is the relative distance between a word and an entity in the input sequence. The other is syntactic relation which is proposed in this paper, defined as the dependency relations between each word and the entities. % Besides, we further propose the syntax position, defined as the dependency relations between each word and the entities. Based on these information, the entity-guided gates in EGA are able to select those important words and control the contribution of each word in self-attention. % Based on these information, the entity-aware gate in EGA is able to select those important words and control the contribution of each word in the self-attention.    % For the proposed CAT, it allows the model to and asynchronously learn the confusing relations for each sentence. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified sentences and their confusing relations to conduct an additional training process, which aimes to learn the confusing relations explicitly.  We also propose CAT to explicitly force the model to asynchronously learn the classification from an instance to its true relation and its confusing relation. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified instances and their confusing relations as augmented data to conduct an additional training process, which aims to learn the mapping between these instances into the confusing relations.  % After that, The CAT uses these misclassified sentences and their confusing relations to  conduct an additional training process, which aims to learn the confusing relations explicitly.  Afterwards, the CAT adopts the KL divergence to teach the model to distinguish the difference between the true and confusing relations, which benefits the true relation classification from the confusing relation identification.  % Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even better results to strong baselines in terms of accuracy. % Furthermore, ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem.  The contributions of this paper are summarized as follows:   We propose an Entity-Guided Attention encoder, which can select crucial words and filter out NL expressions causing confusion based on their relevance to the specified entities.    We propose a Confusion-Aware Training process to enhance the model with the ability of distinguishing true and confusing relations.   We conduct extensive experiments on few-shot RC dataset FewRel, ans the results show that our model achieves comparable and even much better results to strong baselines. Furthermore, ablation and case studies verify the effectiveness of the proposed EGA and CAT, especially in addressing the relation confusion problem.    \paragraph{Few-shot Relation Classification} Relation classification  aims to identify the semantic relation between two entities in a sentence. It is an important task in natural language processing community and has attracted more and more attention over past few years. . Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances.  To this end,  first adopt the Distant Supervision method into relation classification which automatically acquires training data by aligning knowledge base and text. However, the dataset generated by this method has the long-tail relations problem, that  The former has drawn much attention of many researchers.  propose a PCNN model with multi-instance learning to filter out those negative instances, but it ignores a large amount of information contained in these noise instances.  propose a sentence-level attention-based model in order to consider all sentences including those noise. Additionally, many works use the reinforcement learning to automatically distinguish positive and negative instances, such as .  many relations are long-tail and still suffer from data deficiency. To address this problem,  first introduce few-shot learning to RC task. The few-shot learning paradigm has been proved effective in the computer vision community and has many applications. Earlier works on few-shot RC are based on the widely used few-shot learning model prototypical network. Recently, the pre-trained language models  has shown significant power in many natural language processing tasks. To this end,  adopt the most representative pre-trained LM BERT  to few-shot RC, and their work shows that BERT brings significant improvements on classification performance. Furthermore, the approach proposed by  are also based on BERT and achieve the state-of-art result on the few-shot RC task.  \paragraph{Syntactic Relation} Previous RC models usually use the relative position information to identify which words are the entities in a sentence, e.g., . In addition, the syntax information of the sentences is proved useful in many natural language processing tasks . Inspired by , which adopt the dependency parse tree for RC , we also introduce the dependency relation as another type of position to emphasize the specific entities, and propose a novel application of the syntax positions.     Han et al.\shortcite{Han2018FewRel}   Gao et al.\shortcite{gao2019fewrel}    In this work, we have proposed a model-agnostic reinforcement learning framework for data-to-text aimed at reducing hallucinations and improving recall/coverage of relevant information. We shaped the reward based on PARENT , which is a recently proposed metric with a high correlation with human judgement.  on a number of datasets.   This   training protocol allows for a more flexible training, where the model learns to depend less on the reference and more on the source data. Framework effectiveness is assessed via thorough experiments on two model family  and two benchmarks .   In particular, we showed that further training using the proposed framework led to models to learn non-trivial behavior, like shortening generation when the pretrained models would still output  content, or on the contrary adding relevant information from the table that were missed by the pretrained models. Furthermore, quantitative and qualitative evaluations show that our PARENTing framework   help models to reduce both hallucinated and omitted content. This approach  obtains better results than a dedicated attention module or a less source-relying reward.  .    However, this framework relies on the quality of the metric employed during training, and crafting an effective metric is still an open problem. In particular, the PARENT metric was designed specifically for datasets like WikiBIO and WebNLG,  in which values are linguistic sequences associated with a single entity and explicit semantic fields.     This  avoids possible confusion to which value is associated to which field.   However, such a metric is not reliable for more complex datasets \citet{wiseman2017,puduppully2019}. For instance, in RotoWire , tables report statistics of basketball games and regroup several entities of the same nature. In this setting, the sentence ``James Harden scored 20 points'' could achieve a high PARENT score if any player had scored  points in the game.  Therefore, the information-retrieval metrics introduced by \citet{wiseman2017} are still the only metric able to capture precision of generated texts. However, they are based on manually tuned pattern-matching and computed using an ensemble of six deep neural network: they are specific to RotoWire and are in practice not usable in a realistic training protocol.    For future work, we plan to  propose an evaluation metric more robust to dataset peculiarities with the final objective to evaluate our model-agnostic framework on  more complex and challenging datasets. Once such a metric devised, it would be interesting to apply the framework on more challenging settings which ask for more in-depth reasoning.  However, this approach relies on the metric employed and crafting an effective metric is still an open problem. In particular, PARENT is designed for single-entity datasets, like WikiBIO and WebNLG,  However, this framework relies on the quality of the metric employed during training, and crafting an effective metric is still an open problem. In particular, the PARENT metric was designed specifically for datasets relating a single entity and explicit semantic fields, like WikiBIO and WebNLG.    in which values are linguistic sequences associated with a single entity and explicit semantic fields.   This  avoids possible confusion to which value is associated to which field. which is not reliable for more complex datasets   reporting heterogeneous data and/or  containing multiple entities  , as seen in basketball games  . In this setting, the sentence ``James Harden scored 20 points.'' could achieve a high PARENT score if any player had scored  points in the game.  Therefore, the information-retrieval metrics introduced by \citet{wiseman2017} are still the only metric able to capture precision of generated texts. However, they are based on manually tuned pattern-matching and computed using an ensemble of six deep neural network: they are specific to RotoWire and are in practice not usable in a realistic training protocol.  An interesting future work would be the design of an evaluation metric more robust to dataset peculiarities.   with the final objective to evaluate our model-agnostic framework on  more complex and challenging datasets.   Once such a metric devised, it would be interesting to apply the framework on more challenging settings which ask for more in-depth reasoning.  
"," \paragraph{Few-shot Relation Classification} Relation classification  aims to identify the semantic relation between two entities in a sentence. It is an important task in natural language processing community and has attracted more and more attention over past few years. . Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances.  To this end,  first adopt the Distant Supervision method into relation classification which automatically acquires training data by aligning knowledge base and text. However, the dataset generated by this method has the long-tail relations problem, that  The former has drawn much attention of many researchers.  propose a PCNN model with multi-instance learning to filter out those negative instances, but it ignores a large amount of information contained in these noise instances.  propose a sentence-level attention-based model in order to consider all sentences including those noise. Additionally, many works use the reinforcement learning to automatically distinguish positive and negative instances, such as .  many relations are long-tail and still suffer from data deficiency. To address this problem,  first introduce few-shot learning to RC task. The few-shot learning paradigm has been proved effective in the computer vision community and has many applications. Earlier works on few-shot RC are based on the widely used few-shot learning model prototypical network. Recently, the pre-trained language models  has shown significant power in many natural language processing tasks. To this end,  adopt the most representative pre-trained LM BERT  to few-shot RC, and their work shows that BERT brings significant improvements on classification performance. Furthermore, the approach proposed by  are also based on BERT and achieve the state-of-art result on the few-shot RC task.  \paragraph{Syntactic Relation} Previous RC models usually use the relative position information to identify which words are the entities in a sentence, e.g., . In addition, the syntax information of the sentences is proved useful in many natural language processing tasks . Inspired by , which adopt the dependency parse tree for RC , we also introduce the dependency relation as another type of position to emphasize the specific entities, and propose a novel application of the syntax positions.     Han et al.\shortcite{Han2018FewRel}   Gao et al.\shortcite{gao2019fewrel}",118
" 	\com{ 		Remember to recheck: 		intro 		each section place is clear 		reiterate paragraph 		 		structure: 		taxonomy explained 		validations of our taxonomy+classification  		comparison to other taxonomies /classifications   		proof of usefulness 		various kinds of analysis this allows  		qualitative results and discussion 		related work 		conclusion 	} 	Taxonomies of grammatical errors are important for linguistic and computational analysis of learner language, as well as for Grammatical Error Correction  systems.\footnote{Code can be found \href{https://github.com/borgr/GEC_UD_divergences}{in github repo GEC\_UD\_divergences}. Matrices directly mentioned are included in the appendix.} 	Such taxonomies divide the complex space of errors into meaningful categories and enable characterizing their distribution in learner productions. This information can be beneficial for GEC: it can support the development of systems that focus on specific error types, serve as a form of inductive bias , and guide data augmentation and data filtering by controlling the distribution of error types. Error taxonomies can also improve the interpretability of system outputs for error analysis and learner feedback. 	 	%  	%  	% \end{small} 	% \end{table} 	 	 	 	 	 	A number of annotation efforts for learner language developed error taxonomies , and statistical classifiers into such taxonomies, notably ERRANT . Taking error types into consideration in learning has also been shown to improve GEC performance \citep[][{cf.  \S}]{kantor2019learning}. 	However, most existing taxonomies are fairly coarse-grained and language specific, and do not produce meaningful types for a large proportion of the errors. For example, 25\% of the errors in the standard NUCLE corpus  are mapped to the residual category OTHER . 	 	We propose \secl, a taxonomy of Syntactic Errors  and an automatic Classification. Inspired by a longstanding tradition in Machine Translation  which analyses divergences between source and translated texts based on syntactic structure , \secl\ is based on divergences between ungrammatical sentences and their corrections. 	We define SEs as errors whose correction involves changing morphological features, POS labels or the syntactic structure labels. \secl\ takes as input edits, i.e., grammatically incorrect text spans and their corrections, and compares their labels. For example, the error in Fig.  is an adjective replaced with an adverb  in POS terms, and an \ra in edge-label terms. Thus, SEs are defined by changes in form, rather than by the principles governing the choice of a correct form. 	 	\secl\ is the first taxonomy derived from a syntactic representation framework, and it uses the Universal Dependencies formalism \citep[UD;][]{nivre2016universal}.  	This approach provides three major advantages over prior learner error taxonomies. First, the \secl\ taxonomy is derived automatically from UD annotations, circumventing the need for constructing ad-hoc manually defined error categories. Second, using the UD formalism makes the method applicable across languages, allowing for consistent analyses and comparisons of learner errors across different languages within one unified framework. Third, \secl\ is compatible with standard representations and tools in NLP.  	 	Further, the UD based approach to error classification can yield finer distinctions compared to existing schemes. For example, it divides the commonly used class of adposition errors into errors in the use of prepositions as nominal modifiers , and the use of prepositions in prepositional objects or adjuncts .  	%prepositions involving verbal arguments  and errors involving spatial/temporal relations.\oa{how exactly? maybe we should say we distinguish between NP-internal PPs and clause-level PPs?} \lc{One would be obj something and the other will not . Isn't what is an object and subject a main thing syntax allows?, you can write another example. Note that the example should involve a type not usually split . }  	POS tags alone cannot distinguish them, but the UD trees expose this distinction straightforwardly. UD can also help classify agreement and case-assignment errors thanks to its morphological-feature layer containing information about case, number, gender, and other features relevant for inflection. 	 	 	We validate \secl's reliability by showing  SEs based on automatic parses are similar to ones based on manual parses.  ;  \secl\ types map well to NUCLE's manually curated taxonomy ;  \secl\ is complementary to the standard type classifier ERRANT: 60\% of the errors not classified by ERRANT are classified by \secl. 	 	We demonstrate \secl's unique features, notably cross-linguistic applicability, by analyzing SE distributions in available corpora for learner English  and learner Russian . 	 	Finally, we find in GEC systems  certain SEs are harder to correct  SEs are harder than non-SEs   the granular types can help devising rules to improve products . 	 	%\lc{yb, I gave it a try, is that better? Am I being too general?} 	 	%We validate the accuracy of relying on parsing technology  and compare \secl\ to manual  and automatic taxonomies , finding that \secl\ classifies 60\% of the errors not covered by by the leading error classifier for English ERRANT . We further examine its characteristics by using UD features and by applying it to Russian . All these findings suggest that \secl\ is a reliable, fine-grained annotation which is the only current taxonomy and classifier that is not language specific. To show its wide applicability, we use \secl\ to provide a detailed picture of the distribution of SEs in various learner English corpora . We proceed to use \secl\ to detect trends in error type distribution across learner levels . We conclude by analyzing system outputs .\yb{many people skip these paper summary paragraphs. I would instead have a list of contributions as bullet points .} 	 	 	%  	%While classifying grammatical error types . In this paper we focus on syntactic errors, i.e., errors that require changing the tree structure to fix, and  	 	 	 	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 	  	 	 	 Typologies of errors played a crucial role in many aspects of GEC. 	Error types are often used to improve performance and evaluation in GEC. Taxonomies have been used to construct classifiers and rule-based engines to correct specific error types \cite[e.g.,][]{rozovskaya2014correcting,farra2014generalized,zheng2018you}.  	When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results .  	Ensembling black-box systems relying on per-type performance has been shown superior to each system's performance and over average ensembling . 	Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type .  	The classification of grammatical error types is also used to analyze system performance \citep[e.g.,][]{Lichtarge2019CorporaGF}.   	\citet{choshen2018maege,choshen2018conservatism} showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these types may be needed.  	 	To date, several error taxonomies have been proposed and applied for annotating errors in major English learner-language corpora \citep[][inter alia]{bryant-etal-2019-bea, dahlmeier2013building,nicholls2003cambridge}. There has been interest lately in other languages, for which different datasets and taxonomies were created . 	However, different taxonomies are used by different corpora, based on commonly observed error types in the target domain and language, which impedes direct comparison across corpora. Moreover, these taxonomies are not formulated based on a specific theory or annotation scheme for morphosyntactic representation, which may promote accessibility to non-experts but often leads to non-uniform terminology and difficulty in leveraging available NLP tools. 	 	Another automatic type classification was suggested apart from ERRANT. \citet{swanson2012correction} trained a log-linear model to predict types defined by \citet{nicholls2003cambridge}. This taxonomy resembles ours in that it uses grammatical categories , but differs in that it only distinguishes types based on the POS tag of the correction and not of the source sentence. Moreover, relying solely on POS tags yields difficulties in classifying constructions that involve more than a single word. For such cases, it defines specialized error types, such as Incorrect Argument Structure, which serves as a residual category for argument structure errors that cannot be accounted for by adposition or agreement errors. However, unlike \secl,  it does not provide any information as to what particular incorrect argument structure was used or how it should be corrected.  \secl\ provides this additional information in the form of UD labels and POS tags. 	 	\citet{choshen2018usim} used a semantic annotation to show semantics, unlike syntax is kept upon changes. 	UD was previously used in GEC in the TLE corpus and in a learner language parser \citep[e.g.,][]{sakaguchi2017error} . 	 	 	                                              	 	  In this paper, we have proposed Token Drop mechanism for neural machine translation task. Inspired by self-supervised learning, we introduced Replaced Token Detection and Dropped Token Prediction training objective. We found that NMT model trained with Token Drop gains larger generalization capacity and reduction in overfitting. Even without prior knowledge and additional parameters, our proposed approach reports convincing results on neural machine translation. In future work, we plan to investigate impact of dropping on different words, e.g. word importance and word type.   
"," 	 	 	 Typologies of errors played a crucial role in many aspects of GEC. 	Error types are often used to improve performance and evaluation in GEC. Taxonomies have been used to construct classifiers and rule-based engines to correct specific error types \cite[e.g.,][]{rozovskaya2014correcting,farra2014generalized,zheng2018you}.  	When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results .  	Ensembling black-box systems relying on per-type performance has been shown superior to each system's performance and over average ensembling . 	Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type .  	The classification of grammatical error types is also used to analyze system performance \citep[e.g.,][]{Lichtarge2019CorporaGF}.   	\citet{choshen2018maege,choshen2018conservatism} showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these types may be needed.  	 	To date, several error taxonomies have been proposed and applied for annotating errors in major English learner-language corpora \citep[][inter alia]{bryant-etal-2019-bea, dahlmeier2013building,nicholls2003cambridge}. There has been interest lately in other languages, for which different datasets and taxonomies were created . 	However, different taxonomies are used by different corpora, based on commonly observed error types in the target domain and language, which impedes direct comparison across corpora. Moreover, these taxonomies are not formulated based on a specific theory or annotation scheme for morphosyntactic representation, which may promote accessibility to non-experts but often leads to non-uniform terminology and difficulty in leveraging available NLP tools. 	 	Another automatic type classification was suggested apart from ERRANT. \citet{swanson2012correction} trained a log-linear model to predict types defined by \citet{nicholls2003cambridge}. This taxonomy resembles ours in that it uses grammatical categories , but differs in that it only distinguishes types based on the POS tag of the correction and not of the source sentence. Moreover, relying solely on POS tags yields difficulties in classifying constructions that involve more than a single word. For such cases, it defines specialized error types, such as Incorrect Argument Structure, which serves as a residual category for argument structure errors that cannot be accounted for by adposition or agreement errors. However, unlike \secl,  it does not provide any information as to what particular incorrect argument structure was used or how it should be corrected.  \secl\ provides this additional information in the form of UD labels and POS tags. 	 	\citet{choshen2018usim} used a semantic annotation to show semantics, unlike syntax is kept upon changes. 	UD was previously used in GEC in the TLE corpus and in a learner language parser \citep[e.g.,][]{sakaguchi2017error} .",119
" Automatic summarization is the automated process of reducing the size of an input text while preserving its most relevant information content and its core semantics. Techniques for summarization are often characterized as being either: Extractive or Abstractive. Extractive methods construct summaries by combining the most salient passages  of a source text; a process similar to human's way of identifying the right information. One way to achieve extractive summarization is to define the problem as a sentence classification task, using some form of  representation of the sentences in a document . To avoid content overlap issues, previous work has used sentence reranking  or sentence ordering by extracting sentences recurrently . Abstractive methods generate summaries by generating new sentence constructs ``from scratch'', or from representation of document content, a process that is conceptually more similar to the notion of paraphrasing. Abstractive text summarization has attracted interest since it is capable of generating novel formulations of summaries using language generation models conditioned on the source text. Several attention-based Recurrent Neural Network  encoder-decoders have been introduced to tackle varying text generation issues of standalone abstractive sequence-to-sequence  models. Copy and pointer mechanisms , for example, have enabled decoders to better generate unseen words, out-of-vocabulary words and named entities.   Most recently, hybrid extractive and abstractive architectures have been proposed and have shown promising results in both quantitative performance measures and human evaluations. In such set-ups, the extractive model first selects salient sentences from a source article, and the abstractive model paraphrases the extracted sentences into a final summary. The majority of current state-of-the-art abstractive summarization models\footnote{Excluding summarization models using large scale pre-trained language models such as BERT } are based on the hybrid approach .  Nonetheless, hybrid models can be limited by three disadvantages. First, since ground-truth labels for extractive summarization are usually not provided, extractive labels must be generated by a potentially suboptimal algorithm . The performance of models trained with such labels is therefore bounded by the quality of the performance of the extractive heuristics. Second, since ground-truth binary labels for recurrently extracted sentences are typically teacher forced as in \citet{chen-bansal-2018-fast}, ``exposure bias''  may negatively affect content selection performance at inference. Finally, given that the hard extraction step is not differentiable, existing hybrid models typically require multi-step training   or reinforcement learning  to train the whole model.  In this paper, we introduce a novel abstractive summarization model that incorporates an intermediate extractive step but does not require labels for this type of extractive content selection, and it is fully end-to-end trainable. To achieve this, we propose a new memory augmented encoder-decoder  architecture  called Mem2Mem. Mem2Mem has 2 memorization modes:  absorb key information of the encoded source sequence via a compression mechanism, and  sequentially update the external memory during target summary generation. Without using extractive ground-truth labels, we find in our analysis that Mem2Mem's compression mechanism behaves as an implicit sentence extractor that stores sentence representations of the salient content. The choice of sentence representations is only guided by the memory regularization and conditional language modeling loss of the decoder, thus avoiding exposure bias from maximizing the likelihood of sequential binary extraction labels. Finally, the encoded memory is transferred to the decoder memory, which is iteratively refined during the decoding process. To our knowledge, Mem2MeM is the first abstractive summarization model that uses memory compression for sentence extraction and that directly employs the memorized representations during summary generation. We empirically demonstrate the merits of this approach by setting a new state-of-the-art on long text abstractive summarization tasks on the Pubmed, arXiv and Newsroom datasets . Our contributions are three fold:   % fig_architecture       Recent works in abstractive summarization have leveraged intermediate content selection. In these approaches, writing a summary is factorized into two steps: extraction and generation. An extractor is used to prioritize and select the most important part of the input text. The extractor is normally trained on a sequence of binary labels where each label indicates whether the corresponding text unit should be selected or not. The level of extraction can be word-level  or sentence-level . As the ground truth for extraction is typically missing, heuristics that measure n-gram overlap with the target summary are used to build extractive oracles. Similar to other approaches, Mem2Mem performs sentence-level extraction to deal with long source articles\footnote{In preliminary experiments, we applied word-level selection on the PubMed and arXiv datasets, which led to poor results}. Mem2Mem determines the alignment between source and target sentences in a latent space without relying on possibly suboptimal extractive heuristics. In addition, sentence extraction is not sequentially done in Mem2Mem, which addresses the exposure bias issue .  \citet{lin2017structured} used a similar approach to create multiple types of sentence embeddings called structured self-attentive sentence embedding matrix. Different from their work, we use the matrix as an external memory block with \ number of slots to store a smaller set of sentence representations. We further integrate the memory module with the baseline HRED seq2seq model for language generation. Additionally, in our approach, the memory representation \ gets updated dynamically by the decoder while the structured self-attentive sentence embedding matrix remains static. \raggedbottom \newline \par Memory Augmented Encoder Decoder  architectures  have been proposed for conditional natural language generation tasks, such as machine translation  and image captioning . Using differentiable read and write operations to an external module, MAED can represent non-local context of RNNs with enhanced memory capacity. Such models are able to store temporally distant information of large input sequences, a feature that is particularly useful for long text summarization. In the context of short text abstractive summarization, \citet{kim2018abstractive} proposed a memory architecture named multi-level memory networks . MMN can flexibly reduce representations at different levels of document hierarchy into a fixed size external memory. Authors used multi-layer dilated Convolutional Neural Networks   to build a hierarchical representation of the document. Mem2Mem also constructs memory from the hierarchical representation of the document, but by compressing it into a sparse set of sentence representations. Further, MMN's memory representations remain static throughout the decoding process while Mem2Mem dynamically updates its memory, which is more effective in learning long term dependency. Lastly but not least, our work proposes novel regularization for memory read and compression.   original related work by Jon    Recent work in deep learning has studied various new methods to augment neural networks with external memory modules. \citet{Sukhbaatar2015_memNets}, using a continuous memory representation similar to Neural Turing Machines  or Differentiable Neural Computer , showed the importance of allowing multiple reads and writes to memory between inputs in language modeling experiments. \citet{yogatama2018memory} further improved on such experiments by using an LSTM language model equipped with a multihop adaptive continuous stack memory. However, such memory mechanisms often read or write to a single memory slot, not fully taking advantage of a memory's ability to store distributed representations . Our memory mechanism is different since we first compress the sequence with self-attention that writes to all slots simultaneously. During decoding, we use a gating based writing mechanism that forces every slot to sequentially forget or input representations. As we will show in the analysis section, our memory mechanism insures that memory slots  are used to capacity  and contain a diverse set of representations.  Memory Augmented Encoder Decoders   have been proposed for conditional natural language generation tasks, such as machine translation  and image captioning . Using differentiable read and write operations to an external module, MAED can represent non-local context of RNNs with its larger memory capacity. Such models are able to store temporally distant information of large input sequences, a feature that is particularly useful for large text summarization. On short text abstractive summarization, a multi-level memory network was proposed  that can flexibly reduce representations at different levels of document hierarchy into a fixed size external memory. We also reduce the hierarchical representation of the document, but by storing a sparse set of sentence representations. As shown in Figure, each memory slot contains a much more focused set of sentence representations with the proper regularization. Another difference is that we use the memory representations to calculate the conditional probability of the next word  whereas \citet{kim2018abstractive} only use memory representations to calculate attention weights. In that sense, our method is much closer to hybrid extractive-abstractive summarization models because the decoder relies on the stored representations in the memory. The final and important difference is that our method can be attached to any type of HRED models, and it does not use Temporal Convolutional Networks which need parameter changes for different tasks or different datasets .   Discuss: \\  - unified model for extractive/abstractive summ  \\  - Abstractive Summarization of Reddit Posts with Multi-level Memory Networks  \\  - Sentence Simplification with Memory-Augmented Neural Networks   Memory based models have been used in several NLG tasks. , using a continuous memory representation similar to 's Neural Turing Machines, show the importance of allowing multiple reads and writes to memory between inputs in language modeling experiments.  further improved on such experiments by using an LSTM language model equipped with a multihop adaptive continuous stack memory. When experimenting with the multihop adaptive continuous stack memory for abstractive summarization in a HRED context\footnote{We used a GRU with a stack memory on the sentence encoder and decoder. More details can be found in the appendix.}, we have found two issues  only the top 2 slots of the encoder's stack memory were filled  the decoder memory stack rarely used or mixed memories transferred from the encoder.   Other MAED have been proposed for conditional NLG, such as machine translation , image captioning , QA . In summarization, MAED have been used on extractive summarization  and on short text abstractive summarization . These approaches excel at tasks where it is necessary to store some parts of a sequential input in a representation that can later be precisely queried. We found however that most MAED innovations focused on improving long term memories of the encoded input or read/write operations. In this work, we suggest solutions in these areas:  a memory creation mechanism that condenses the encoded input akin to an extraction step;  a conditional read/write mechanism that allows the decoder to use/update encoder memories. Further, we propose another feature --- Mem2Mem transfers memories from different context. The encoder memory stores input sentence-level information while the decoder updates are conditioned on output word-level information.      We propose that sentence boundary errors are a neglected area of study for NMT robustness, especially in the context of speech translation.  We quantitatively demonstrate that poor sentence segmentation degrades performance almost twice as much as transcript level-errors.  To address this, we developed a simple method for data augmentation with immediate gains that can serve as a baseline for future work in segmentation NMT robustness.    Given the simplicity and ease of adaptation into existing systems, we hope to integrate our approach into production systems.  \vfill\pagebreak     References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   ------------------------------------------------------------------------- 
","  Recent works in abstractive summarization have leveraged intermediate content selection. In these approaches, writing a summary is factorized into two steps: extraction and generation. An extractor is used to prioritize and select the most important part of the input text. The extractor is normally trained on a sequence of binary labels where each label indicates whether the corresponding text unit should be selected or not. The level of extraction can be word-level  or sentence-level . As the ground truth for extraction is typically missing, heuristics that measure n-gram overlap with the target summary are used to build extractive oracles. Similar to other approaches, Mem2Mem performs sentence-level extraction to deal with long source articles\footnote{In preliminary experiments, we applied word-level selection on the PubMed and arXiv datasets, which led to poor results}. Mem2Mem determines the alignment between source and target sentences in a latent space without relying on possibly suboptimal extractive heuristics. In addition, sentence extraction is not sequentially done in Mem2Mem, which addresses the exposure bias issue .  \citet{lin2017structured} used a similar approach to create multiple types of sentence embeddings called structured self-attentive sentence embedding matrix. Different from their work, we use the matrix as an external memory block with \ number of slots to store a smaller set of sentence representations. We further integrate the memory module with the baseline HRED seq2seq model for language generation. Additionally, in our approach, the memory representation \ gets updated dynamically by the decoder while the structured self-attentive sentence embedding matrix remains static. \raggedbottom \newline \par Memory Augmented Encoder Decoder  architectures  have been proposed for conditional natural language generation tasks, such as machine translation  and image captioning . Using differentiable read and write operations to an external module, MAED can represent non-local context of RNNs with enhanced memory capacity. Such models are able to store temporally distant information of large input sequences, a feature that is particularly useful for long text summarization. In the context of short text abstractive summarization, \citet{kim2018abstractive} proposed a memory architecture named multi-level memory networks . MMN can flexibly reduce representations at different levels of document hierarchy into a fixed size external memory. Authors used multi-layer dilated Convolutional Neural Networks   to build a hierarchical representation of the document. Mem2Mem also constructs memory from the hierarchical representation of the document, but by compressing it into a sparse set of sentence representations. Further, MMN's memory representations remain static throughout the decoding process while Mem2Mem dynamically updates its memory, which is more effective in learning long term dependency. Lastly but not least, our work proposes novel regularization for memory read and compression.   original related work by Jon    Recent work in deep learning has studied various new methods to augment neural networks with external memory modules. \citet{Sukhbaatar2015_memNets}, using a continuous memory representation similar to Neural Turing Machines  or Differentiable Neural Computer , showed the importance of allowing multiple reads and writes to memory between inputs in language modeling experiments. \citet{yogatama2018memory} further improved on such experiments by using an LSTM language model equipped with a multihop adaptive continuous stack memory. However, such memory mechanisms often read or write to a single memory slot, not fully taking advantage of a memory's ability to store distributed representations . Our memory mechanism is different since we first compress the sequence with self-attention that writes to all slots simultaneously. During decoding, we use a gating based writing mechanism that forces every slot to sequentially forget or input representations. As we will show in the analysis section, our memory mechanism insures that memory slots  are used to capacity  and contain a diverse set of representations.  Memory Augmented Encoder Decoders   have been proposed for conditional natural language generation tasks, such as machine translation  and image captioning . Using differentiable read and write operations to an external module, MAED can represent non-local context of RNNs with its larger memory capacity. Such models are able to store temporally distant information of large input sequences, a feature that is particularly useful for large text summarization. On short text abstractive summarization, a multi-level memory network was proposed  that can flexibly reduce representations at different levels of document hierarchy into a fixed size external memory. We also reduce the hierarchical representation of the document, but by storing a sparse set of sentence representations. As shown in Figure, each memory slot contains a much more focused set of sentence representations with the proper regularization. Another difference is that we use the memory representations to calculate the conditional probability of the next word  whereas \citet{kim2018abstractive} only use memory representations to calculate attention weights. In that sense, our method is much closer to hybrid extractive-abstractive summarization models because the decoder relies on the stored representations in the memory. The final and important difference is that our method can be attached to any type of HRED models, and it does not use Temporal Convolutional Networks which need parameter changes for different tasks or different datasets .   Discuss: \\  - unified model for extractive/abstractive summ  \\  - Abstractive Summarization of Reddit Posts with Multi-level Memory Networks  \\  - Sentence Simplification with Memory-Augmented Neural Networks   Memory based models have been used in several NLG tasks. , using a continuous memory representation similar to 's Neural Turing Machines, show the importance of allowing multiple reads and writes to memory between inputs in language modeling experiments.  further improved on such experiments by using an LSTM language model equipped with a multihop adaptive continuous stack memory. When experimenting with the multihop adaptive continuous stack memory for abstractive summarization in a HRED context\footnote{We used a GRU with a stack memory on the sentence encoder and decoder. More details can be found in the appendix.}, we have found two issues  only the top 2 slots of the encoder's stack memory were filled  the decoder memory stack rarely used or mixed memories transferred from the encoder.   Other MAED have been proposed for conditional NLG, such as machine translation , image captioning , QA . In summarization, MAED have been used on extractive summarization  and on short text abstractive summarization . These approaches excel at tasks where it is necessary to store some parts of a sequential input in a representation that can later be precisely queried. We found however that most MAED innovations focused on improving long term memories of the encoded input or read/write operations. In this work, we suggest solutions in these areas:  a memory creation mechanism that condenses the encoded input akin to an extraction step;  a conditional read/write mechanism that allows the decoder to use/update encoder memories. Further, we propose another feature --- Mem2Mem transfers memories from different context. The encoder memory stores input sentence-level information while the decoder updates are conditioned on output word-level information.",120
"  Variational Autoencoders   allow to design complex generative models of data.  % since the inference process of VAE-based approaches has the advantage of being independent from the model architecture providing high flexibility in designing new neural components. In the wake of the renewed interest for VAEs, traditional probabilistic topic models  have been revised giving rise to several Neural Topic Model  variants, such as NVDM ,  ProdLDA , NTM-R , etc. % GSM , W-LDA  However, existing topic models when applied to user reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries of movies and books . Although these approaches have achieved significant results via the neural inference process, surprisingly very little work has been done on how to disentangle the inferred topic representations.   % Despite the lack of general consensus about a formal definition of disentangled representations ,   Disentangled representations can be defined as representations where individual latent units are sensitive to variations of a single generative factor, while being relatively invariant to changes of other factors . Inducing such representations has been shown to be significantly beneficial for their generalization and interpretability .  For example, an image can be viewed as the results of several generative factors mutually interacting, as one or many sources of light, the material and reflective properties of various surfaces or the shape of the objects depicted . %  In the context of topic modeling, documents result from a generative process over mixtures of latent topics, and therefore, we propose to consider these latent topics as generative factors to be disentangled to improve their interpretability and discriminative power. Disentangled topics are topics invariant to the factors of variation of text, which for instance, in the context of book and movie reviews could be the author's opinion , the salient parts of a plot or other auxiliary information reported. An illustration of this is shown in Fig. in which opinion topics are separated from plot topics.  % where this leads to separating topics based on the ``factor of variation"" they are revealing. % For example, in generating a book review, the factors of variation involved could depend on the author's expertise in identifying the salient features of the book, %his knowledge of the book's genre, or  % his ability to summarize the plot and the feelings evoked by the book.  % % [Let's break in/the atom] % % Figure  reports a examples of polarity-disentangled topics generated from the IMDB movie reviews of ""The Hobbit"". The topics on the left and right summarize some of the positive and negative aspects described by users, while neutral topics in the middle report the main elements of the movie's plot.  % An effective approach for disentangling features in the latent space of VAEs is to adopt adversarial training . However, despite its successful applications in computer vision , the applications to text analysis has been rather limited so far , narrowed by the lack of proper tasks to evaluate the generated disentangled representations and the limited availability of suitable datasets.  % For example, in book or movie reviews, we want to disentangle topics which are related to opinions expressed in text and topics relating to book/movie plots. An illustration of this is shown in Figure in which opinion topics are separated from plot topics.   However, models relying solely on sentiment information are easily misled and not suitable to disentangle opinion from plots, since even plot descriptions frequently make large use of sentiment expressions . Consider for example the following sentence: ``The ring holds a dark power, and it soon begins to exert its evil influence on Bilbo"", an excerpt from a strong positive Amazon's review.  % This overcomes the difficulty of separating opinions from plot and auxiliary information yet containing polarised descriptions that easily mislead models merely relying on sentiment lexicon; analogously to the issue of mixed topics generated when traditional topic models are applied to review documents, as pointed out in \citet{Blei08}.  % Despite its successful employment in computer vision , the adversarial approach has had a rather limited application in text analysis so far , narrowed by the lack of proper tasks to evaluate the generated disentangled representations and the limited availability of suitable datasets.  Therefore, we propose to distinguish opinion-bearing topics from plot/neutral ones combining a neural topic model architecture with an adversarial training. In this study, we present the DIsentangled Adversarial TOpic Model \footnote{Source code and dataset omitted for the anonymous submission.}, aiming at disentangling information related to the target labels , from other distinct aspects yet possibly still polarised . We also introduce a new dataset, namely the MOBO dataset\footnotemark[\value{footnote}], made up of movie and book reviews, paired with their related plots. The reviews come from different publicly available datasets: IMDB , GoodReads  and Amazon reviews , %,  and encompass a wide spectrum of domains and styles. We conduct an extensive experimental assessment of our model. First, we assess the topic quality in terms of topic coherence and diversity and compare DIATOM with other supervised topic models on the sentiment classification task; then, we analyse the disentangling rate of topics to quantitatively assess the degree of separation between actual opinion and plot/neutral topics.    Our contributions are summarized below:    The rest of the paper is organized as follows. We review the related literature on sentiment-topic models, neural topic models and the studies on disentangled representations . Then, we present the details of our proposed DIATOM model , followed by the experimental setup  and results . Finally, we conclude with a summary of the results and suggestions for future works .     %%%%%%%%%%%%%%%%%%%%%%%%%%%     Our work is closely related to three lines of research: sentiment-topic models, neural topic models and learning disentangled representations.        Sentiment-Topic Models. Probabilistic graphical models for topic extraction have been extensively studied.  a vast literature.  Beyond LDA ; a wide spectrum of models has extended LDA to more specific tasks using contextual information . Supervised-LDA   is a general-purpose supervised extension which builds on top of LDA by adding a response variable associated with each document .    or category).  A category of extensions particularly relevant for this work is the sentiment-topic models. Examples include the Joint Sentiment-Topic  model  and Aspect and Sentiment Unification Model  . These models are able to extract informative topics grouped under different sentiment classes. Although they do not rely on document labels, they require word prior polarity information to be incorporated into the learning process in order to generate consistent results.  about words and their domain-independent polarity.    because during each Gibbs sampling iteration it has to repeatedly sample from the posterior of the sentiment-topic pair assignment for each word token throughout the entire corpus.     To address the above issues and to analyze sentiment and topic trends over time, a dynamic joint sentiment-topic model   was proposed. It derives an online inference procedure based on a stochastic expectation maximization  algorithm to better fit larger corpus, and   similarly to SLDA,  Nevertheless, when provided with document-level class labels, JST can learn document-topic distributions influenced by the class information. The possibility to supervise the learning process with document labels and to avoid the necessity of prior information over words makes it suitable for a fair comparison with the model proposed in this work.  Besides, these models require carefully tailored inference algorithms, and the standard Gibbs sampling algorithm used can have a high computational cost when fitting large-scale data, with time and memory scaling linearly with the number of documents, leading researchers to devise more sophisticated approaches to make it scalable .   Compared to DIATOM, the discussed sentiment topic models can only distinguish between polarity-bearing topics and neutral ones, remaining strictly aligned to the provided labels. Instead, DIATOM is able to generate opinion-bearing topics and plot topics which may still be polarized but not carrying any user's opinion.   Compared to these models, DIATOM not only generates polarised topics but is able to separate them from polarised or neutral topics  which however are not related to the target label .    Neural Topic Models. Neural models provide a more generic and extendable alternative to topic modeling, and therefore, have recently gained increasing interest.  Some of them use belief networks , or enforce the Dirichlet prior on the document-topic distribution by means of Wasserstein Autoencoders . Others adopt continuous representations to capture long-term dependencies or preserve word order via sequence-to-sequence VAE  whose time complexity and difficulty of training, however, have limited their applications.   Neural Variational Document Model   is a direct extension of VAE used for topic detection in text. In NVDM, the prior of latent topics is assumed to be a Gaussian distribution. This is not ideal since it cannot mimic the simplex in the latent topic space. To address this problem, LDA-VAE  instead used the logistic normal distribution to approximate the Dirichlet distribution. ProdLDA  extended LDA-VAE by replacing the mixture model of LDA with a product of experts.   which employs a Laplace approximation to make the gradient back-propagate to the variational distribution.   is a neural framework for topic models with metadata incorporation .  without the need of deriving model-specific inference   When metadata are document labels, the model infers topics that are relevant to those labels.    Although some studies have applied the adversarial approach  to topic models setting a Dirichlet prior on the generative network , it is still unexplored how to use this mechanism to disentangle opinion-bearing topics from plot or neutral topics.  high-level topic characteristics, as for instance, the carried sentiment polarity.   Since W-LDA is not based on variational inference, we cannot compute the ELBO based perplexity as a performance metric as in.   Compared to these neural topic models, DIATOM is the first attempt using an adversarial mechanism to distinguish between topic type , while not only generating topics aligned with the available target labels  but seamless incorporating the external signal of plot summaries to drive the generation of topics about salient parts of plots mentioned by users  not related to the target classes .   Representation Disentanglement. Despite the lack of general consensus about a unique definition of disentangled representations , it typically refers to representations which are only sensitive to one single generative factor of data and relatively invariant to other factors of variation . One proposed definition builds upon the concept of statistical independence by minimizing total correlation , while an alternative approach explored the possibility to measure and track the changes in a single latent dimension as degree of disentanglement . However, the disentanglement of representation achieved in DIATOM is instead analogous to the one presented in \citet{Thomas17} and \citet{Bengio17}, where they impose additional constraints to the representations in the latent space that can be controlled exploiting a reinforcement learning mechanism determining the disentangled factors. In DIATOM, we alternatively make use of an adversarial approach over the available target labels.  Both Generative Adversarial Networks   and VAEs  have been successfully employed in disentangling features in computer vision tasks.  Application in text processing has shown promising results , yet applications to topic modeling are still limited  and to the best of our knowledge, there is no work in separating opinion-bearing topics from plot/neutral topics.                              ARCHITECTURE                           In this work, we propose a series of strong transformer models for multi-hop QG. To effectively encode the context documents and the answer, we introduce answer type embeddings and a new sublayer to incorporate the extracted entity-centric graph. We also propose an auxiliary contrastive objective to identify the supporting facts and a data filtering approach to balance the training-test distribution mismatch. Experiments on the HotpotQA dataset show that our models outperform the current best approaches by a substantial margin of 5 BLEU points. Our analysis further reveals that graph-based components may not be the most critical in improving the performance, but can render complementary strengths to the transformer.  !TeX root = main.tex  
","   Our work is closely related to three lines of research: sentiment-topic models, neural topic models and learning disentangled representations.        Sentiment-Topic Models. Probabilistic graphical models for topic extraction have been extensively studied.  a vast literature.  Beyond LDA ; a wide spectrum of models has extended LDA to more specific tasks using contextual information . Supervised-LDA   is a general-purpose supervised extension which builds on top of LDA by adding a response variable associated with each document .    or category).  A category of extensions particularly relevant for this work is the sentiment-topic models. Examples include the Joint Sentiment-Topic  model  and Aspect and Sentiment Unification Model  . These models are able to extract informative topics grouped under different sentiment classes. Although they do not rely on document labels, they require word prior polarity information to be incorporated into the learning process in order to generate consistent results.  about words and their domain-independent polarity.    because during each Gibbs sampling iteration it has to repeatedly sample from the posterior of the sentiment-topic pair assignment for each word token throughout the entire corpus.     To address the above issues and to analyze sentiment and topic trends over time, a dynamic joint sentiment-topic model   was proposed. It derives an online inference procedure based on a stochastic expectation maximization  algorithm to better fit larger corpus, and   similarly to SLDA,  Nevertheless, when provided with document-level class labels, JST can learn document-topic distributions influenced by the class information. The possibility to supervise the learning process with document labels and to avoid the necessity of prior information over words makes it suitable for a fair comparison with the model proposed in this work.  Besides, these models require carefully tailored inference algorithms, and the standard Gibbs sampling algorithm used can have a high computational cost when fitting large-scale data, with time and memory scaling linearly with the number of documents, leading researchers to devise more sophisticated approaches to make it scalable .   Compared to DIATOM, the discussed sentiment topic models can only distinguish between polarity-bearing topics and neutral ones, remaining strictly aligned to the provided labels. Instead, DIATOM is able to generate opinion-bearing topics and plot topics which may still be polarized but not carrying any user's opinion.   Compared to these models, DIATOM not only generates polarised topics but is able to separate them from polarised or neutral topics  which however are not related to the target label .    Neural Topic Models. Neural models provide a more generic and extendable alternative to topic modeling, and therefore, have recently gained increasing interest.  Some of them use belief networks , or enforce the Dirichlet prior on the document-topic distribution by means of Wasserstein Autoencoders . Others adopt continuous representations to capture long-term dependencies or preserve word order via sequence-to-sequence VAE  whose time complexity and difficulty of training, however, have limited their applications.   Neural Variational Document Model   is a direct extension of VAE used for topic detection in text. In NVDM, the prior of latent topics is assumed to be a Gaussian distribution. This is not ideal since it cannot mimic the simplex in the latent topic space. To address this problem, LDA-VAE  instead used the logistic normal distribution to approximate the Dirichlet distribution. ProdLDA  extended LDA-VAE by replacing the mixture model of LDA with a product of experts.   which employs a Laplace approximation to make the gradient back-propagate to the variational distribution.   is a neural framework for topic models with metadata incorporation .  without the need of deriving model-specific inference   When metadata are document labels, the model infers topics that are relevant to those labels.    Although some studies have applied the adversarial approach  to topic models setting a Dirichlet prior on the generative network , it is still unexplored how to use this mechanism to disentangle opinion-bearing topics from plot or neutral topics.  high-level topic characteristics, as for instance, the carried sentiment polarity.   Since W-LDA is not based on variational inference, we cannot compute the ELBO based perplexity as a performance metric as in.   Compared to these neural topic models, DIATOM is the first attempt using an adversarial mechanism to distinguish between topic type , while not only generating topics aligned with the available target labels  but seamless incorporating the external signal of plot summaries to drive the generation of topics about salient parts of plots mentioned by users  not related to the target classes .   Representation Disentanglement. Despite the lack of general consensus about a unique definition of disentangled representations , it typically refers to representations which are only sensitive to one single generative factor of data and relatively invariant to other factors of variation . One proposed definition builds upon the concept of statistical independence by minimizing total correlation , while an alternative approach explored the possibility to measure and track the changes in a single latent dimension as degree of disentanglement . However, the disentanglement of representation achieved in DIATOM is instead analogous to the one presented in \citet{Thomas17} and \citet{Bengio17}, where they impose additional constraints to the representations in the latent space that can be controlled exploiting a reinforcement learning mechanism determining the disentangled factors. In DIATOM, we alternatively make use of an adversarial approach over the available target labels.  Both Generative Adversarial Networks   and VAEs  have been successfully employed in disentangling features in computer vision tasks.  Application in text processing has shown promising results , yet applications to topic modeling are still limited  and to the best of our knowledge, there is no work in separating opinion-bearing topics from plot/neutral topics.                              ARCHITECTURE",121
" \subsection{Dialogue act recognition}  Mutual understanding in interactive situations, either when several people are engaged in a dialogue or when they are interacting with a modern computer system in natural language, may not be achieved without considering both the semantic information in the speakers utterances and the pragmatic interaction level, especially relative to dialogue acts. Dialogue Acts  represent the meaning of an utterance  in the context of a dialogue, or, in other words, the function of an utterance in the dialogue. For example, the function of a~question is to request some information, while an answer shall provide this information. Dialogue acts are thus commonly represented as phrase-level labels such as statements, yes-no questions, open questions, acknowledgements, and so on.  Automatic recognition of dialogue acts is a fundamental component of many human-machine interacting systems that support natural language inputs. For instance, dialogue acts are typically used as an input to the dialogue manager to help deciding on the next action of the system: giving information when the user is asking a question, but eventually keeping quiet when the user is just acknowledging, giving a comment, or even asking for delaying the interaction. In the latter case, a system reaction may be perceived as intrusive. Beyond human-machine interaction, this task is also important for applications that rely on the analysis of human-human interactions, either oral, e.g., in recordings of meetings, or % lada - added reference according to rev 1 written, e.g., through the reply and mention-at structures in Twitter conversations. It is also essential for a large range of other applications, for example talking head animation, machine translation, automatic speech recognition or topic tracking. The knowledge of the user dialogue act is useful to render facial expressions of an avatar that are relevant to the current state of the discourse. In the machine translation domain, recognizing dialogue acts may bring relevant cues to choose between alternative translations, as the adequate syntactic structure may depend on the user intention. Automatic recognition of dialogue acts may also be used to improve the word recognition accuracy of automatic speech recognition systems, where a different language model is applied during recognition depending on the dialogue act. %lada - added reference according to rev 1,   To conclude, dialogue act recognition is an important building block of many understanding and interacting systems. %pav --I've commented the rest of the sentence, because it was not clear for 2 reviewers ) -- and typically completes semantic role labelling and dialogue management.  \subsection {Motivation and objectives} Researches on dialogue act recognition have been carried out for a long time, as detailed in Section. The majority of these works exploit supervised learning with lexical, syntactic, prosodic and/or dialogue history features. However, few approaches consider semantic features, while they may bring additional information and prove useful to improve the accuracy of the dialogue act recognition system. For instance,  a~frequent cause of recognition errors are ``unknown'' words in the testing corpus that never occur in the training sentences. Replacing specific named entities in the text  by their category has been proposed in the literature as a remedy to this issue. We investigate a more general solution that exploits lexical similarity between word vectors. These word vectors may be computed in various ways, but they typically include mostly lexical semantic information about the word itself as well as some syntactic information, e.g., related to the relative position or degree of proximity of pairs of words within a sentence. This additional information may be used to improve dialogue act recognition, in particular when the training and test conditions differ, or when the size of the training corpus is relatively small.  %goal In this work, we propose a new Deep Neural Network  based on Long Short-Term Memory  for the task of dialogue act recognition, and we compare its performance to a standard Maximum Entropy model. Our first objective is to leverage the modelling capacity of such a DNN in order to achieve dialogue act recognition with only the raw observed word forms, i.e., without any additional expert-designed feature. This model is described in Section. The second objective is to further validate this model both on a standard English DA corpus, as well as on two other languages, without changing anything in the model, in order to assess the genericity and robustness of the approach. These experiments are summarized in Section. Finally, our third objective is to study the impact of word embeddings, which have been shown to provide extremely valuable information in numerous Natural Language Processing  tasks, but which have never been used so far~\footnote{To the best of our knowledge at the time of submission} for dialogue act recognition. This study is summarized in Section. %The following Section presents a review of related works of the domain.        TODO: structure the related works:   - DA standard sets   - features   - models   multilingual ? multitasks ?  Although dialogue act recognition has been extensively studied in English and German, relatively few works have been published for Czech and French. This explains why most of the following related works concern English. Different sets of dialogue acts are defined in the literature, depending on the target application and available corpora. James Allen and Mark Core have proposed DAMSL , a scheme developed primarily for annotation of two-agent task-oriented dialogues with DAs.  This scheme has further been adapted by Daniel Jurafsky to create ``SWBD-DAMSL''. The authors describe a shallow discourse tag-set of approximately 60 basic DA tags  to annotate 1,155 5-minute conversations, including 205,000 utterances and 1.4M words, from the Switchboard corpus of English telephone conversations. The Meeting Recorder DA  tag-set is another popular tag-set, which is based on the SWBD-DAMSL taxonomy. MRDA contains 11 general DA labels and 39 specific labels.  These large DA tag-sets are often reduced for recognition into a few broad classes, because some classes occur rarely, or because some DAs are not useful for the target application. A typical grouping may be for instance:  \vskip 1.0em   \vskip 0.2em \hskip 1.0em  Automatic recognition of dialogue acts is usually achieved using one of, or a combination of, the following types of information:   Lexical information  is useful for automatic DA recognition,  because different DAs are usually composed of different word sequences. Some cue words and phrases can thus serve as explicit indicators of dialogue structure. For example, 88\  of the trigrams ``start do you'' occur in English in yes/no questions.  Their system achieves a 65\  DA detection rate based on acoustic signal on SWBD-DAMSL dataset.  pavn - these ACC are not correct in this context because they use all types of information Several methods, typically based on word unigrams, may be used to represent lexical information.  Syntactic information is related to the order of the words in the utterance. For instance, in French and Czech, the relative order of the subject and verb occurrences might be used to discriminate between declarations and questions. With , word n-grams may also be used to capture some local syntactic information. Kr\'al et al. propose to represent word position in the utterance in order to take into account global syntactic information.  Their approach gives 95.8\  DA recognition accuracy on Czech train ticket reservation corpus with 4 DA classes. A recent work in the dialogue act recognition field also successfully uses a set of syntactic features derived from a deep parse tree. The reported accuracy is 97.7\  on the same corpus.   pav - semantics for DA reco  paper from ACL 2011 -- their approach is completely different. They do not used really the semantic. A few related works include semantic features for recognizing dialogue acts. One of these works combines syntactic parsing of sentences with named entity classes to  achieve DA recognition from audio input. The proposed approach achieves 84.3\  DA detection accuracy on the Tainan-city tour-guiding  dialogue  corpus. Other researchers employ syntactic and semantic relations acquired by information extraction methods with Bayesian network classifier. The obtained dialogue act classification accuracies are 73\  on the 804 sentences of the ``CST'' corpus, and 68.5\  on the 435 sentences of the ``NPC'' corpus; both corpora are labeled with 7 dialogue acts and deal with the task of furnishing a living-room with the help of a sales agent .  Prosodic information, particularly the melody of the utterance, is often used to provide additional clues to label sentences with DAs. Finally, another important information is the ``dialogue history''. It encodes the sequence of previous dialogue acts and gives 71\  of dialogue act accuracy on the Switchboard DA corpus when combined with lexical and prosodic features.  Dialogue act recognition is usually based on supervised machine learning, such as Bayesian Networks, dynamic Bayesian networks,  new from CSL BayesNet,  Memory-based and Transformation-based Learning,  Decision Trees,  Neural Networks, but also Boosting,    Latent Semantic Analysis,  Hidden Backoff Models, Maximum Entropy Models, Conditional Random Fields, Triangular-chain CRF and probabilistic rules.    \cite {lison2015hybrid}  -- pav new from CSL to include to the paper, papers in ./papers/1-s2.0-S0885230809000254-main.pdf and 1-s2.0-S0885230815000029-main.pdf  Despite the growing importance of deep learning architectures in image processing, speech recognition and several other natural language processing tasks, deep neural models have only rarely been applied so far to dialogue act recognition. In one of these works, a multi-modal deep neural network is used to extract features and compute abstract representations of the input. Then, a CRF model takes this input to recognize a~sequence of dialogue acts. This model achieves 77\  of recognition accuracy on the Chinese CASIA-CASSIL corpus. In the only other work we are aware of that applies deep learning to DA recognition, the authors combine a deep convolutional model with a vanilla recurrent network across sentences. They report an accuracy of 73.9\  with this model on the Switchboard-DAMSL corpus.  We propose in this work an alternative approach to model within-sentence sequential dependencies that is based on the LSTM recurrent cell. An advantage of this model is that the total number of parameters of the model does not depend on the sentence length. Furthermore, none of both related works use pretrained word embeddings, and we thus explicitely study in this work the influence of pretrained word embeddings in this deep architecture.     We have described DIATOM, a new neural topic model to generate disentangled topics through the combination of VAE and adversarial learning.   We reported the results of our experimental study based on the novel  dataset highlighting the benefit of such an approach leading to topics with higher interpretability in terms of both topic coherence and topic uniqueness and more discriminative power reflected in better sentiment classification results compared to other supervised topic models.   We further discussed the model capability to consistently disentangle opinion-bearing topics from plot/neutral ones measuring the introduced disentangling rate.  Finally, we identified current limitations and viable solutions to be explored in the future.      
","     TODO: structure the related works:   - DA standard sets   - features   - models   multilingual ? multitasks ?  Although dialogue act recognition has been extensively studied in English and German, relatively few works have been published for Czech and French. This explains why most of the following related works concern English. Different sets of dialogue acts are defined in the literature, depending on the target application and available corpora. James Allen and Mark Core have proposed DAMSL , a scheme developed primarily for annotation of two-agent task-oriented dialogues with DAs.  This scheme has further been adapted by Daniel Jurafsky to create ``SWBD-DAMSL''. The authors describe a shallow discourse tag-set of approximately 60 basic DA tags  to annotate 1,155 5-minute conversations, including 205,000 utterances and 1.4M words, from the Switchboard corpus of English telephone conversations. The Meeting Recorder DA  tag-set is another popular tag-set, which is based on the SWBD-DAMSL taxonomy. MRDA contains 11 general DA labels and 39 specific labels.  These large DA tag-sets are often reduced for recognition into a few broad classes, because some classes occur rarely, or because some DAs are not useful for the target application. A typical grouping may be for instance:  \vskip 1.0em   \vskip 0.2em \hskip 1.0em  Automatic recognition of dialogue acts is usually achieved using one of, or a combination of, the following types of information:   Lexical information  is useful for automatic DA recognition,  because different DAs are usually composed of different word sequences. Some cue words and phrases can thus serve as explicit indicators of dialogue structure. For example, 88\  of the trigrams ``start do you'' occur in English in yes/no questions.  Their system achieves a 65\  DA detection rate based on acoustic signal on SWBD-DAMSL dataset.  pavn - these ACC are not correct in this context because they use all types of information Several methods, typically based on word unigrams, may be used to represent lexical information.  Syntactic information is related to the order of the words in the utterance. For instance, in French and Czech, the relative order of the subject and verb occurrences might be used to discriminate between declarations and questions. With , word n-grams may also be used to capture some local syntactic information. Kr\'al et al. propose to represent word position in the utterance in order to take into account global syntactic information.  Their approach gives 95.8\  DA recognition accuracy on Czech train ticket reservation corpus with 4 DA classes. A recent work in the dialogue act recognition field also successfully uses a set of syntactic features derived from a deep parse tree. The reported accuracy is 97.7\  on the same corpus.   pav - semantics for DA reco  paper from ACL 2011 -- their approach is completely different. They do not used really the semantic. A few related works include semantic features for recognizing dialogue acts. One of these works combines syntactic parsing of sentences with named entity classes to  achieve DA recognition from audio input. The proposed approach achieves 84.3\  DA detection accuracy on the Tainan-city tour-guiding  dialogue  corpus. Other researchers employ syntactic and semantic relations acquired by information extraction methods with Bayesian network classifier. The obtained dialogue act classification accuracies are 73\  on the 804 sentences of the ``CST'' corpus, and 68.5\  on the 435 sentences of the ``NPC'' corpus; both corpora are labeled with 7 dialogue acts and deal with the task of furnishing a living-room with the help of a sales agent .  Prosodic information, particularly the melody of the utterance, is often used to provide additional clues to label sentences with DAs. Finally, another important information is the ``dialogue history''. It encodes the sequence of previous dialogue acts and gives 71\  of dialogue act accuracy on the Switchboard DA corpus when combined with lexical and prosodic features.  Dialogue act recognition is usually based on supervised machine learning, such as Bayesian Networks, dynamic Bayesian networks,  new from CSL BayesNet,  Memory-based and Transformation-based Learning,  Decision Trees,  Neural Networks, but also Boosting,    Latent Semantic Analysis,  Hidden Backoff Models, Maximum Entropy Models, Conditional Random Fields, Triangular-chain CRF and probabilistic rules.    \cite {lison2015hybrid}  -- pav new from CSL to include to the paper, papers in ./papers/1-s2.0-S0885230809000254-main.pdf and 1-s2.0-S0885230815000029-main.pdf  Despite the growing importance of deep learning architectures in image processing, speech recognition and several other natural language processing tasks, deep neural models have only rarely been applied so far to dialogue act recognition. In one of these works, a multi-modal deep neural network is used to extract features and compute abstract representations of the input. Then, a CRF model takes this input to recognize a~sequence of dialogue acts. This model achieves 77\  of recognition accuracy on the Chinese CASIA-CASSIL corpus. In the only other work we are aware of that applies deep learning to DA recognition, the authors combine a deep convolutional model with a vanilla recurrent network across sentences. They report an accuracy of 73.9\  with this model on the Switchboard-DAMSL corpus.  We propose in this work an alternative approach to model within-sentence sequential dependencies that is based on the LSTM recurrent cell. An advantage of this model is that the total number of parameters of the model does not depend on the sentence length. Furthermore, none of both related works use pretrained word embeddings, and we thus explicitely study in this work the influence of pretrained word embeddings in this deep architecture.",122
"   As an important task in Natural Language Generation , dialogue generation empowers a wide spectrum of applications, such as chatbot and customer service automation. In the past few years, breakthroughs in dialogue generation technology focused on a series of sequence-to-sequence models .  More recently, external knowledge is employed to enhance model performance. % , for instance, propose Mem2Seq using structured knowledge in task-oriented dialogue generation.   can assist dialogue generation by using knowledge triples. Similarly,  explore document as knowledge discovery for dialogue generation, and  utilize unstructured knowledge to explore in the open-domain dialogue generation. However, unaffordable knowledge construction and defective domain adaptation restrict their utilization.   Copy-based generation models  have been widely adopted in content generation tasks and show better results compared to sequence-to-sequence models when faced with out-of-vocabulary problem. Thanks to their nature of leveraging vocabulary and context distributions for content copy, it enables to copy the aforementioned named entities  appeared in the above context) from the upper context to improve the specificity of the generated text.    In the task of dialogue generation, we can often observe the phrases/utterance patterns across different ""similar dialogue"" instances. For example, in customer service, the similar inquiries from the customers will get similar responses from the staff. It motivates us to build a model that can not only copy the content within the upper context of the target dialogue instance, but also learn the similar patterns across different similar cases of the target instance. Such external copy can be critical in some scenarios.  %Fi Judge's questions, in the target court debate case, can be copied from both internal and external sources, and this `cross-copy' can enhance the dialougue generation essentially.    Figure this paper, we are aware of the possibility of copying  from adjacent Unfortunately, these methods only enable internal copy, e.g., copy the content within the target dialogue instance. External copy, e.g., copy content across different dialougue instances, is incapable. However, as Figure. depicted,   %is another effective network structure. It solved the problem that the traditional sequence-to-sequence model cannot solve the problem that the vocabulary of the output sequence will change with the length of the input sequence. %Copynet proposed humans tend to repeat entity names or even long phrases in conversation.And then generate the entity that appeared in the previous article will be copied. %Recently, Pointer networks and Copynet's variants have played a very important role in NLG. Among them, Pointer-Generator Networks  was proposed. %In order to copy the key information from the context as well as cope with the Out-Of-Vocabulary problem. It relies on the vocabulary distribution and context distribution, the extended vocabulary is further obtained. % GLMP proposed a global memory encoder and a local memory decoder to share external knowledge by Pointer networks. %}As general domain network structure, the pointer network  and Copynet  shows fine effect in general text generation tasks. It not only can solves the problem of domain adaptability poor in dialog generation, does not introduce external knowledge, but also address Out-Of-Vocabulary  problem and enable content copy. % Pointer networks  and Copynet  provided effective approach to address Out-Of-Vocabulary  problem and enable content copy.  %The more recent effort, Pointer-Generator Networks  , inherited their advantages by leveraging vocabulary and context distributions for content copy.   As shown in Figure., we propose two different kinds of copy mechanisms in this study: vertical copy context-dependent information within the target dialogue instance, and horizontal copy logic-dependent content across different 'Similar Cases' . This framework is labeled as Cross-Copy Networks . As exemplar dialogue depicted, judges may repeat  words, phrases or utterances from historical dialogues when those SCs sharing similar content, e.g., `A sue B because of X and Y'.  %In this study, 'Similar Cases'  refers to a similar dialogue for each dialogue. When generating the next sentence based on the historical dialogue, we can refer to the similar dialogue of the dialogue to obtain it. In this paper, we propose a new network: Cross-Copy Networks, which can not only copy the previous entity, but also learn the logic of dialogue generation and copied specific words, phrases or utterance from similar cases to deal with out-of-vocabulary  words. % The CCN has two pointers, one can copy the specific entity or sentence from the context and another can copy the process discourse or a complete sentence from SC.  % As shown in Figure 1, there are two similar cases and a target case. Our copy methods are divided into two types, internal copy and external copy.  internal copy: we can directly copy some specific entities words that appear in the context as the words to be generated.  external copy: we can copy related sentences or phrases in similar cases as the directly generated sentences.   % As shown in Fig., There are three samples of CCN:  Selective copy: it can copy some specific words or phrases from SC as sentences to be generated, as sample 1.  Cross copy: it copy specific entities from the context, and then copy some process-frame nature sentences in SC, as sample 2.  Deep copy: it can copy some process discourse directly as a generated sentence, usually this sentence appears frequently in the full text, as sample 3.  In order to validate the proposed model, we employ two different dialogue datasets from two orthogonal domains - court debate and customer service. We apply proposed CCN to both datasets for dialogue generation. Experiments show that our model achieves the best results. To sum up, our contributions are as follows:  %      In this section, We will introduce our baseline model: Sequence-to-sequence attention model, Pointer Generation Networks, Transformer.   These three models are closely related to our model.        Recently, there has been a surge of work proposing to build the Natural Language Generation within a seq2seq+attention framework.    It has great potential for various tasks in natural language, such as machine translation, text summaries, etc.   The model usually uses RNN  structure and has an encoder and a decoder.   The role of the encoder maps the context to a vector, and the task of the decoder is to convert this vector into the text to be generated.   We get the encoder hidden state  of the contenxt through the encoder, in the each time step , Decoder received representation of original context information produce decoder state .        As shown in the formula, , , ,  represents the parameters to be learned during training.     With attention mechanism, we calculated as:      We can get attention distribution , it represent a probability distribution of source words. Then, we can get context vector :      We will concatenated context vector  and decoder hidden state  through linear transformation get vocabulary distribution :       Therefore, , ,  and  represents the parameters to be learned during training.    According to vocabulary probability distribution ,  we get final distribution preict words :           Unlike the sequence-to-sequence+attention model, the Pointer Generation Networks uses the attention mechanism as a pointer to select token in the context, and then obtains an expanded vocabulary distribution beyond the original vocabulary distribution.   We have got encoder hidden state , decoder state , decoder input , context vector  and attention distribution  in section 3.1.   First, we need to calculate the generation probability  at each time step :      The ,  and  are parameters which need training.   The  is sigmod function.     In Pointer Generation Networks, wo through attention distribution and vocabluary distribution get expanded vocabulary .   In the decode, the output sequence's probability distribution from expanded vocabluary :      As shown above,  is Out-Of-Ovcabulary  word.   In the trainging, model will be selected to copy the word from context by attention distribution . if the word will be generated not in context, the  is zero.         Transformer is a new network architecture which based solely on attention mechanisms.   In order to deal with the problem that RNN cannot be parallelized, there have been some solutions using CNN, such as Google's ByteNet, Facebook proposed FairSeq, etc.   This model proposed a particular attention named ""Scaled Dot-Product Attention"", it's input consists of queries  and keys  of dimension , and values  of dimension .   This is a self attention mechanism, it can be as a set of key-value pairs to an output.   The ,  and  are matrices of learning parameters.   Self attention can be computed as:      The self-attention mechanism can connect information at different positions on the input sequence, and then calculate a certain expression of the entire sequence.     And the reason for dividing by  is that when the dimension  of the key is particularly large, the dot product may become very large, causing the subsequent softmax function to enter a small gradient range, which is not conducive to training.     Therefore, in each word embedding dimension, we execute a multi-block attention mechanism in parallel and then concatenated and once again projected, which is called Multi-Head Attention.         The , ,  and   are learnable parameters.      Because the transformer does not use any recurrence or convolution structure, in order for the model to use the sequence information of the input sequence, some information that can express the absolute or relative position of each part of the input sequence must be introduced.    The method adopted by the transformer is positional encoding.   Before entering the encoder and decoder, the input is encoded.    The vector dimension after encoding is dmodeldmodel.    Specifically, encoding using sine and cosine functions:         The  is position and  is the dimension.     We compare the advantages and disadvantages of self-attention, recurrence structure, and convolutional structure from three aspects.    The first is the computational complexity of each layer, the second is the amount of computation that can be performed in parallel, and the last is the length of the path that the network depends on for a long time.   The comparison shows that self-attention performs best.      Using transformers to be able to handle longer dependencies, without recurrence structure and convolutional structure, making model training faster.     We propose in this work an LSTM-based deep neural network for dialogue act recognition. We show that this model performs as good as the state-of-the-art, even though it only uses the raw word forms as inputs, without any additional information, in particular neither part-of-speech tags nor information about the speaker. We have further applied exactly the same model with the same hyper-parameters on three different languages: English, French and Czech. The proposed model performs well on all three languages, suggesting that its performance generalizes nicely to various types of corpora and is not dependent on a specific tuning of the hyper-parameters to experimental conditions. This confirms the interesting modelling potential of deep recurrent networks for NLP in general, and supports the conclusions of recent works in the domain, which demonstrate the good performance of end-to-end training of deep neural networks for dialogue act recognition.  A more surprising conclusion of our work concerns the actual impact of pretrained word embeddings, which have been shown to be of great importance in several NLP tasks in the literature. We show in this work that standard pretrained embeddings do not help for the dialogue act recognition task in any of the three tested languages. We thus further study the embeddings that result from training the proposed model in an end-to-end manner, and show that they seem to differ from vanilla word2vec embeddings, which may explain why they do not perform as well as in other tasks. Of course, a single type of word embeddings has been tested in this work, word2vec, but some additional preliminary experiments suggest that LDA and COALS-based embeddings do not help either. More experiments with various embeddings should be made to confirm or infirm this conclusion, but it would be more convincing if they were realized with another deep network implementation and in more variable experimental conditions. To the best of our knowledge, this is the first work that exploits pretrained word embeddings for dialogue act recognition, and one of the rare published work that shows and analyzes some weakness of word2vec embeddings.  We further compare the proposed deep neural network with a standard Maximum Entropy classifier, and show that the DNN consistenly outperforms the Maximum Entropy classifier both in French and English. This is not the case in Czech, but it is likely due to the already high level of accuracy reached on this corpus, which leaves little to be gained by improving the model. A more interesting conclusion about this comparison between DNN and Maximum Entropy is that  pretrained word embeddings improve the Maximum Entropy model but not the DNN. This likely results from the limited modelling capacity of the Maximum Entropy model, which still benefits from the information brought by pretrained embeddings. But this information is not precise enough for the DNN, as shown in our qualitative analysis of word2vec.  
","    In this section, We will introduce our baseline model: Sequence-to-sequence attention model, Pointer Generation Networks, Transformer.   These three models are closely related to our model.        Recently, there has been a surge of work proposing to build the Natural Language Generation within a seq2seq+attention framework.    It has great potential for various tasks in natural language, such as machine translation, text summaries, etc.   The model usually uses RNN  structure and has an encoder and a decoder.   The role of the encoder maps the context to a vector, and the task of the decoder is to convert this vector into the text to be generated.   We get the encoder hidden state  of the contenxt through the encoder, in the each time step , Decoder received representation of original context information produce decoder state .        As shown in the formula, , , ,  represents the parameters to be learned during training.     With attention mechanism, we calculated as:      We can get attention distribution , it represent a probability distribution of source words. Then, we can get context vector :      We will concatenated context vector  and decoder hidden state  through linear transformation get vocabulary distribution :       Therefore, , ,  and  represents the parameters to be learned during training.    According to vocabulary probability distribution ,  we get final distribution preict words :           Unlike the sequence-to-sequence+attention model, the Pointer Generation Networks uses the attention mechanism as a pointer to select token in the context, and then obtains an expanded vocabulary distribution beyond the original vocabulary distribution.   We have got encoder hidden state , decoder state , decoder input , context vector  and attention distribution  in section 3.1.   First, we need to calculate the generation probability  at each time step :      The ,  and  are parameters which need training.   The  is sigmod function.     In Pointer Generation Networks, wo through attention distribution and vocabluary distribution get expanded vocabulary .   In the decode, the output sequence's probability distribution from expanded vocabluary :      As shown above,  is Out-Of-Ovcabulary  word.   In the trainging, model will be selected to copy the word from context by attention distribution . if the word will be generated not in context, the  is zero.         Transformer is a new network architecture which based solely on attention mechanisms.   In order to deal with the problem that RNN cannot be parallelized, there have been some solutions using CNN, such as Google's ByteNet, Facebook proposed FairSeq, etc.   This model proposed a particular attention named ""Scaled Dot-Product Attention"", it's input consists of queries  and keys  of dimension , and values  of dimension .   This is a self attention mechanism, it can be as a set of key-value pairs to an output.   The ,  and  are matrices of learning parameters.   Self attention can be computed as:      The self-attention mechanism can connect information at different positions on the input sequence, and then calculate a certain expression of the entire sequence.     And the reason for dividing by  is that when the dimension  of the key is particularly large, the dot product may become very large, causing the subsequent softmax function to enter a small gradient range, which is not conducive to training.     Therefore, in each word embedding dimension, we execute a multi-block attention mechanism in parallel and then concatenated and once again projected, which is called Multi-Head Attention.         The , ,  and   are learnable parameters.      Because the transformer does not use any recurrence or convolution structure, in order for the model to use the sequence information of the input sequence, some information that can express the absolute or relative position of each part of the input sequence must be introduced.    The method adopted by the transformer is positional encoding.   Before entering the encoder and decoder, the input is encoded.    The vector dimension after encoding is dmodeldmodel.    Specifically, encoding using sine and cosine functions:         The  is position and  is the dimension.     We compare the advantages and disadvantages of self-attention, recurrence structure, and convolutional structure from three aspects.    The first is the computational complexity of each layer, the second is the amount of computation that can be performed in parallel, and the last is the length of the path that the network depends on for a long time.   The comparison shows that self-attention performs best.      Using transformers to be able to handle longer dependencies, without recurrence structure and convolutional structure, making model training faster.",123
"  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  A long-standing challenge in computer science is to develop algorithms that can interact with human users via dialog in natural language~.  Of particular interest is task-oriented dialog, wherein a user interacts with a system to achieve some goal .  The system should understand the user's requests and assist them by taking the appropriate actions . In recent years, supervised learning approaches to this problem have become particularly popular, because they can potentially learn complex patterns without relying on hand-crafted rules. While such data-driven methods already demonstrate impressive performance in open-domain dialog , task-oriented dialog models face the additional difficulty of transferring skills to tasks and domains that were not present in the training data.  To address this issue, we present the Schema-guided Dialog Dataset for Transfer Learning  dataset, a collection of realistic, task-oriented dialogs, that is especially designed to test and facilitate the transfer of learned patterns between tasks.  Unlike open-domain dialogs, task-oriented dialogs are accompanied by a set of steps that are necessary to complete the task.  These steps are typically known a priori and thus do not have to be learned from the data. In fact, for practical applications it is desirable that we could make modifications to this logic without having to discard large parts of the dataset. The ideal sequences of steps that a dialog would follow to complete the task can be arranged in a graph . Together with the utterances or actions that are associated with the nodes of this graph, we hence call this a task schema, or simply schema. Note, that what we call `schema' is similar to the `task specification' of , but distinct from the `schemas' that only define slots and intents of a task as used by \citet{rastogi2019towards}. %or \citet{kimEighthDialogSystem2019}.     In a typical supervised model that is trained to, say, predict the next system action for a task-oriented dialog, the schema of the training tasks is implicitly captured by the learned model parameters. This makes generalizing to a new task difficult, as the implicitly memorized schema will no longer be appropriate .  With \DATASETNAME\ we provide explicit schema representations for each task and thereby enable models to condition on the schema .  To collect \DATASETNAME\ we use a Wizard of Oz setup , where the system's role is played by a human `wizard'. Based on our pilot studies, we found that the quality of crowd-sourced dialogs depends strongly on   We refined our approach through extensive internal testing and four rounds of pilot studies.  % All code and instructions are available as open source at \anonymous{\DATASETURL}.   Our aim is to create an ecologically valid dataset  with the following four attributes, which we believe are crucial for a dataset to be of high quality: %     The progression of difficulty allows better assessment of dialog models and potential for transfer learning across levels of difficulty.     \item Consistency on the system side. % The behavior of a task-oriented dialog system should be largely deterministic and not subject to the whims or personality of the wizard. %     In particular, we encourage wizards to follow the given task schema as closely as possible.     \item Explicit knowledge base queries. %     A large part of developing a dialog system is the implementation of application programming interface  calls, such as knowledge base queries. %     In \DATASETNAME\ we represent our dialogs as a three-party interaction wherein the system acts as the intermediary between a user and a knowledge base . %     Thus, models have to learn when to query the knowledge base, what the query should be, and how to explain the returned knowledge base item to the user.  \end{enumerate} % With these properties, we create a is ecologically valid, as described by.  With this paper, we contribute   The code for the latter setup, all collected  data, and all modeling code is freely available under \anonymous{\DATASETURL}.                                                                                                                 Before we give a detailed description of our data collection method in \S, here we briefly outline what sets our work apart from existing publications, both in terms of the collected dataset and the modeling approach.          While \DATASETNAME\ shares some aspects with existing datasets, to our knowledge it is the first that admits all of the properties listed in the Introduction . For example, similar to the dataset of the Fourth Dialogue State Tracking Challenge  and its successors, \DATASETNAME\ is composed of human-human dialog, yet provides much richer annotation. Furthermore, similar to the Microsoft Frames dataset  we make knowledge base queries explicit.  \DATASETNAME, however, covers 13 domains and encourages a consistent system behavior, while Frames does not do so and only covers 2 domains. A much larger number of 47 domains is covered by the MetaLWOz dataset  which, however, does not provide labels for the system's actions, nor explicit knowledge base queries as \DATASETNAME\ does. Finally, the data collection procedure for \DATASETNAME\ is scalable and reproducible, so larger datasets can be collected if necessary.     Footnote for tab:dataset_statistics \footnotetext{  Only half of the Taskmaster-1 dataset has been collected via real-time human-human conversations.   The other half --- ``Self-Dialogs"" --- have been completely written by a single crowd worker, who is assuming the user and wizard role, each.  }  In contrast to \DATASETNAME, the MultiWOZ  and Google Taskmaster-1  datasets do not have annotated knowledge base queries .  Furthermore, these datasets suffer from other issues such as weak history dependence and inconsistent system responses . With \DATASETNAME\ we aim to solve these issues with much more detailed instructions for the wizards and creativity-encouraging prompts for users, as we explain in \S.  For example, we occasionally inspire users to assume a certain personality, as in \citet{zhangPersonalizingDialogueAgents2018}. In addition, we provide response suggestions for the wizards, as was done for the non task oriented BlendedSkillTalk dataset .    Most similar to \DATASETNAME\ is probably the MultiDoGo dataset .   MultiDoGo contains dialogs across 6 domains, with labels for both wizard and user actions.   Moreover, as with \DATASETNAME, wizards follow detailed instructions in the MultiDoGo setting.    In contrast to MultiDoGo, however, \DATASETNAME\ provides more than twice as many domains, wizard instructions in terms of flow charts that lend themselves to be used for transfer learning , as well as explicit knowledge base queries.  Another related and recent dataset is the Schema Guided Dialogue Dataset , which consists of multi-domain dialogs across 16 domains and also includes meta information about the different domains such as lists of valid slots and intents .  Note, that while \citet{rastogi2019towards} call this meta information `schema', it is distinct from our schema that contains information about the ideal dialog flow for each task.     The value of incorporating explicit structure into neural models of dialog has been well recognized.  RavenClaw  disentangles the task specification and the dialog engine for rule-based task-oriented dialog systems.  \DATASETNAME\ shares a similar motivation and aims to extend to explicitly disentangle the task schema in neural, data-driven models.  Hybrid Code Networks  incorporate task-specific constraints to avoid illogical system actions  in neural models.  Recently, several attempts have been made to use intermediate annotations  to explicitly incorporate structure in neural dialog models .  Furthermore, the SGD dataset by \citet{rastogi2019towards} mentioned in \S comes with explicit slot and intent annotations that serve as an inductive bias for their model.     We have studied the feasibility of training a fully bilingual deep neural language model, i.e.\ a model that approaches or matches the performance of monolingual models at language-specific tasks. We trained a bilingual Finnish-English \bertbase{} model by expanding the vocabulary size to be the sum of the size of the two individual vocabularies, and compared the model performance to monolingual models. We found that, on a range of NLU tasks, the bilingual model performs comparably or nearly comparably with monolingual models. We conclude that, for the \bertbase{} architecture, it is possible to train a fully bilingual deep contextual model for two remotely related languages. We release the newly introduced \bbert{} model and all tools introduced to create the model under open licenses at .  
","                                                                                                             Before we give a detailed description of our data collection method in \S, here we briefly outline what sets our work apart from existing publications, both in terms of the collected dataset and the modeling approach.          While \DATASETNAME\ shares some aspects with existing datasets, to our knowledge it is the first that admits all of the properties listed in the Introduction . For example, similar to the dataset of the Fourth Dialogue State Tracking Challenge  and its successors, \DATASETNAME\ is composed of human-human dialog, yet provides much richer annotation. Furthermore, similar to the Microsoft Frames dataset  we make knowledge base queries explicit.  \DATASETNAME, however, covers 13 domains and encourages a consistent system behavior, while Frames does not do so and only covers 2 domains. A much larger number of 47 domains is covered by the MetaLWOz dataset  which, however, does not provide labels for the system's actions, nor explicit knowledge base queries as \DATASETNAME\ does. Finally, the data collection procedure for \DATASETNAME\ is scalable and reproducible, so larger datasets can be collected if necessary.     Footnote for tab:dataset_statistics \footnotetext{  Only half of the Taskmaster-1 dataset has been collected via real-time human-human conversations.   The other half --- ``Self-Dialogs"" --- have been completely written by a single crowd worker, who is assuming the user and wizard role, each.  }  In contrast to \DATASETNAME, the MultiWOZ  and Google Taskmaster-1  datasets do not have annotated knowledge base queries .  Furthermore, these datasets suffer from other issues such as weak history dependence and inconsistent system responses . With \DATASETNAME\ we aim to solve these issues with much more detailed instructions for the wizards and creativity-encouraging prompts for users, as we explain in \S.  For example, we occasionally inspire users to assume a certain personality, as in \citet{zhangPersonalizingDialogueAgents2018}. In addition, we provide response suggestions for the wizards, as was done for the non task oriented BlendedSkillTalk dataset .    Most similar to \DATASETNAME\ is probably the MultiDoGo dataset .   MultiDoGo contains dialogs across 6 domains, with labels for both wizard and user actions.   Moreover, as with \DATASETNAME, wizards follow detailed instructions in the MultiDoGo setting.    In contrast to MultiDoGo, however, \DATASETNAME\ provides more than twice as many domains, wizard instructions in terms of flow charts that lend themselves to be used for transfer learning , as well as explicit knowledge base queries.  Another related and recent dataset is the Schema Guided Dialogue Dataset , which consists of multi-domain dialogs across 16 domains and also includes meta information about the different domains such as lists of valid slots and intents .  Note, that while \citet{rastogi2019towards} call this meta information `schema', it is distinct from our schema that contains information about the ideal dialog flow for each task.     The value of incorporating explicit structure into neural models of dialog has been well recognized.  RavenClaw  disentangles the task specification and the dialog engine for rule-based task-oriented dialog systems.  \DATASETNAME\ shares a similar motivation and aims to extend to explicitly disentangle the task schema in neural, data-driven models.  Hybrid Code Networks  incorporate task-specific constraints to avoid illogical system actions  in neural models.  Recently, several attempts have been made to use intermediate annotations  to explicitly incorporate structure in neural dialog models .  Furthermore, the SGD dataset by \citet{rastogi2019towards} mentioned in \S comes with explicit slot and intent annotations that serve as an inductive bias for their model.",124
"   Chinese Word Segmentation  is a fundamental task for Chinese natural language processing , which aims at identifying word boundaries in a sentence composed of continuous Chinese characters. It provides a basic component for other NLP tasks like named entity recognition, dependency parsing, and semantic role labeling, etc.  Generally, most previous studies model the CWS task as a character-based sequence labeling task . Recently, pre-trained models  such as BERT  have been introduced into CWS tasks, which could provide prior semantic knowledge and boost the performance of CWS systems.  directly fine-tunes BERT on several CWS benchmark datasets.  fine-tunes BERT in a multi-criteria learning framework, where each criterion shares a common BERT-based feature extraction layer and owns a private projection layer.  combines Chinese character glyph features with pre-trained BERT representations. %  builds a unified BERT-based model for multi-criteria CWS tasks and fine-tunes it on eight CWS criteria jointly.  proposes a neural CWS framework WMSeg, which utilizes memory networks to incorporate wordhood information into the pre-trained model ZEN.  PTMs have been proved quite effective by fine-tuning on downstream CWS tasks. However, PTMs used in previous works usually adopt language modeling as pre-training tasks. Thus, they usually lack task-specific prior knowledge for CWS and ignore the discrepancy between pre-training tasks and downstream CWS tasks.     \end{table}  To deal with aforementioned problems of PTMs, we consider introducing a CWS-specific pre-trained model based on existing CWS corpora, to leverage the prior segmentation knowledge. However, there are multiple inconsistent segmentation criteria for CWS, where each criterion represents a unique style of segmenting Chinese sentence into words, as shown in Table. Meanwhile, we can easily observe that different segmentation criteria could share a large proportion of word boundaries between them, such as the boundaries between word units ``閺夊骸顭'', ``鏉╂稑鍙'' and ``閸楀﹤鍠呯挧'', which are the same for all segmentation criteria. It shows that the common prior segmentation knowledge is shared by different criteria.  In this paper, we propose a CWS-specific pre-trained model MetaSeg. To leverage shared segmentation knowledge of different criteria, MetaSeg utilizes a unified architecture and introduces a multi-criteria pre-training task. Moreover, to alleviate the discrepancy between pre-trained models and downstream unseen criteria, meta learning algorithm is incorporated into the multi-criteria pre-training task of MetaSeg.  Experiments show that MetaSeg could outperform previous works significantly, and achieve new state-of-the-art results on twelve CWS datasets. Further experiments show that  MetaSeg has better generalization performance on downstream unseen CWS tasks in low-resource settings, and improve Out-Of-Vocabulary  recalls. To the best of our knowledge, MetaSeg is the first task-specific pre-trained model especially designed for CWS.       Recently, PTMs have been used for CWS and achieve good performance. These PTMs usually exploit fine-tuning as the main way of transferring prior knowledge to downstream CWS tasks. Specifically, some methods directly fine-tune PTMs on CWS tasks, while others fine-tune them in a multi-task framework. Besides, other features are also incorporated into PTMs and fine-tuned jointly, including Chinese glyph features, wordhood features, and so on. Although PTMs promote CWS systems significantly, their pre-training tasks like language modeling still have a wide discrepancy with downstream CWS tasks and lack CWS-specific prior knowledge.  Task-specific pre-trained models are lately studied to introduce task-specific prior knowledge into multiple NLP tasks. Specifically designed pre-training tasks are introduced to obtain the task-specific pre-trained models, and then these models are fine-tune on corresponding downstream NLP tasks, such as named entity recognition, sentiment analysis and text summarization. In this paper, we propose a CWS-specific pre-trained model .   To overcome the problems of task discrepancy and knowledge deficiency in general-purpose pre-trained models, we propose a task-specific pre-trained model  for CWS.    adopts a unified architecture and a multi-criteria pre-training task with meta learning algorithm .   As far as we know, our proposed model is the first task-specific pre-trained model especially designed for CWS.      In this paper, we proposed a domain-agnostic and scalable framework for leveraging implicit user feedback, particularly user dissatisfaction and rephrase behavior, to automatically curate high-quality supervision data to continuously improve NLU in a large-scale conversational AI or digital assistant system. We showed with an extensive set of experiments on live traffic how the framework can be applied to improve NLU and analyzed its performance across 10 popular domains by traffic volume on a real production system. We further showed component-level analysis of our framework for more in-depth validation of its performance.
","   Recently, PTMs have been used for CWS and achieve good performance. These PTMs usually exploit fine-tuning as the main way of transferring prior knowledge to downstream CWS tasks. Specifically, some methods directly fine-tune PTMs on CWS tasks, while others fine-tune them in a multi-task framework. Besides, other features are also incorporated into PTMs and fine-tuned jointly, including Chinese glyph features, wordhood features, and so on. Although PTMs promote CWS systems significantly, their pre-training tasks like language modeling still have a wide discrepancy with downstream CWS tasks and lack CWS-specific prior knowledge.  Task-specific pre-trained models are lately studied to introduce task-specific prior knowledge into multiple NLP tasks. Specifically designed pre-training tasks are introduced to obtain the task-specific pre-trained models, and then these models are fine-tune on corresponding downstream NLP tasks, such as named entity recognition, sentiment analysis and text summarization. In this paper, we propose a CWS-specific pre-trained model .   To overcome the problems of task discrepancy and knowledge deficiency in general-purpose pre-trained models, we propose a task-specific pre-trained model  for CWS.    adopts a unified architecture and a multi-criteria pre-training task with meta learning algorithm .   As far as we know, our proposed model is the first task-specific pre-trained model especially designed for CWS.",125
"  Automatic question answering is a very active area of research within natural language processing. %Open-domain question answering looks for methods to re-utilize systems across multiple domains. One possible way to approach this task is to look for answers in the text passages of a collection of documents. Recent research has shown promising results on developing neural models for passage retrieval tasks, including Retrieval Question Answering, Open Domain Question Answering, and MS MARCO. The models in these systems are often trained using the dual encoder framework where questions and passages are encoded separately. Training an effective neural retrieval model usually requires a large amount of high-quality data. To alleviate the need of high-quality data, training can be approached in two-stages: pre-training on noise data and fine tuning on a smaller amount of high-quality data, also regarded as ``gold"" data. % One significant advantage of the dual encoder framework is that, once the question and passage embeddings are available, efficient nearest neighbour search can be used to retrieve the passages that contain the answers to the questions.    When used for question answering, one advantage of the dual encoder is that training in batches allows to use, for each question, the passages that answer all the other questions in the batch as negatives. Given that the training batches are randomly sampled from all the question-passage pairs, the negatives in the batch are random in nature. While effective in many retrieval tasks, random negatives have the limitation of not being targeted nor challenging enough to clearly separate the passage that answers a given question from any other passage. How to sample the negatives in a way that widens this separation and improves the contrast between the correct and incorrect passages remains an open question.  % A viable approach to negative sampling is to use ``hard"" negatives that are specific to each question and answer  pair. In this paper we systematically explore the use of ``hard'' negatives in the neural passage retrieval models that we train using a two-stage approach. Using hard negatives as part of the dual encoder framework has shown advantageous in different tasks . %Using hard negatives as part of the dual encoder framework has shown advantageous in cross-lingual tasks. %For example, \citet{guo-etal-2018-effective} show that training with hard negatives generated by retrieving ``coarse"" negatives with low-resolution model improves the quality of the translation pairs retrieved with a dual encoder model. %Similarly, \citet{dpr} showed improvement when using hard negatives retrieved with a BM25 model in the passage retrieval part of the Open Domain Question Answering task. %In contrast to previous works,  We explore different types of negatives, and experiment using them in both the pre-training and fine-tuning stages. The types of negatives we tried are:   We first use hard negatives on the data that we use to pre-train the models. We leverage the question generator model described in and generate new questions for each of the passages we use in the pre-training stage . %The new questions are paired with the original passages. %The augmented set of question-passage pairs is used to train the first stage of the neural retrieval model. % It has been shown as an effective approach to improve passage retrieval models. During pre-training we use negatives generated from strategy 4\footnote{Or strategy 1, if strategy 4 is not feasible} to improve the retrieval model, as the other strategies could introduce more false negatives into the data. %Our initial experiments showed that using retrieval models to find the hard negatives at this point, often generated very noisy question-passage pairs, especially because our pre-training data includes synthetic pairs. %As the generated question passage pairs sometimes are noise, retrieval-based approaches may create better question-passage pairs than the synthetic pairs. %We only apply a heuristic based context negatives on this pre-training task. Next, we continue with the fine tuning stage  using a small amount of gold training data. At this stage, we explore all four types of negative sampling. To the best of our knowledge, this is the first work that explores the effectiveness of hard negatives for passage retrieval in a systematic way, and integrates them in the retrieval models  pre-training stage. Our overall experimental architecture is outlined in Figure.  %For each question-passage pair in the training set, we collect negatives using the strategies listed above and augment them into the training.  We conduct experiments with this approach on two passage retrieval tasks: Open Domain QA  and SQuAD) and MS MARCO. %Open Domain QA Natural Questions~, Open Domain QA SQuAD, and MS MARCO. Our results show that all four kinds of hard negatives improve the dual encoder models significantly with consistent performance gains across both tasks. However, depending on the types of questions and their domain, one kind of hard negative may perform better than the others in a particular task. For example, context negatives work best in NQ and semantic retrieval-based negatives  work best in SQuAD. We further ensemble the models trained on different types of hard negatives. The final models achieve state-of-the-art performance on Open Domain QA task with an improvement over prior works of 0.8--2.9 points on accuracy rates.  %\hl{highlight numbers here}.  The main contribution of this paper are:       Recent advances in deep-learning have spurred a surge of interests in adopting neural models to information retrieval. Many existing works adopt a pipeline approach that includes a retrieval stage and a reranking stage. For first-stage retrieval, where the task is to find the most relevant documents in a large document collection,    one line of research focuses on using neural networks to learn better term weighting to improve term matching-based retrieval systems. one line of research investigates projecting query and documents into a shared dense space . The models in this family are regarded as dense-retrieval models or dual encoder models. This is the kind of model that we employ in this work. Ideally, a query and its relevant documents should be projected in each other's vicinity. From this point of view, finding the most relevant documents can be cast to a nearest neighbor search .     Previous attempts at improving the quality of dual encoder models can be classified into two types. The first type focuses on finding a good initialization for the model parameters. This is typically achieved by pre-training the model on various tasks .     In particular, our strategy is based on synthetic query generation .   While the approach was originally proposed for zero-shot learning, we show that dual encoder models trained on supervised data can significantly improve when combined with synthetic question pre-training. Another type of approach focuses on learning better representations using hard negatives. This strategy has proved to be effective in passage retrieval tasks , machine translation  and entity linking . These works mine hard negatives using different strategies. For example,  mine ``coarse'' negatives with a low-resolution model.  mine hard negatives using a BM25 model, and  use a model trained with random negatives and select examples that are ranked above the correct one as negative examples.  Our work systematically studies different hard negative mining strategies.   TODO  Neural networks have also become popular at the reranking stage.  This paper focus on the retrieval stage, and leaves reranking as a future line of work.  reranking stage is outside the scope of this work, it is nonetheless worth mentioning here as neural reranking  is an important research direction.   Since reranking only processes a small subset of candidates that are returned by first-stage retrieval, it is possible to use resource-hungry models to capture the interaction between query and document terms, and thus improve the discriminating power of the models .      Using dense representation for passage or document retrieval is challenging.    The search space may include millions or billions of retrieval candidates.   Some recent works adopt a pipeline approach consisting of two steps .   As a first step, the system retrieves a large number of passages using sparse encoding representations using techniques such as BM25 and DeepCT .   In a second step, it re-ranks the passages using a neural model.   However, this pipeline approach is hindered by error propagation.   In addition, powerful cross-attention models such as BERT models, can be used in the second step to jointly encode the question-passage pairs as this is not feasible in the large-scale retrieval in the first step.     Dual encoder models consist of a pair of encoders that encode questions and passages separately and score each question-passage pair by calculating the inner product of their encodings.    This type of models shows strong performance on sentence-level retrieval tasks, such as translation pair retrieval, retrieval question answering .    Its scalability makes it a good alternative for the first-step retriever in the pipeline approach.   The scalability of the dual encoder models make them an appealing alternative for the first step retriever in the pipeline approach.   Karpukhin et al.\shortcite{dpr} made the first attempt to use dual encoder models for open-domain QA.   ANCE  proposes a training framework which collects negatives from an Approximate Nearest Neighbor  index of the corpus, which is updated in parallel to the model training process.      In this work, we pursued a new research problem of M\&A prediction. Our transformer-based classifier leveraged the regularization benefits of adversarial training to enhance model robustness. More importantly, we built upon previous techniques to quantify the importance of words and help guarantee the generation of plausible counterfactual explanations with a masked language model in financial text classification. The results demonstrate superior accuracy and explanatory performance compared to state-of-the-art techniques. An obvious extension would be to include canceled deals into the classifier, or to predict novel M\&A events based on market descriptions of companies . Moreover, additional financial events  is yet another related task to be considered for further research.   
","  Recent advances in deep-learning have spurred a surge of interests in adopting neural models to information retrieval. Many existing works adopt a pipeline approach that includes a retrieval stage and a reranking stage. For first-stage retrieval, where the task is to find the most relevant documents in a large document collection,    one line of research focuses on using neural networks to learn better term weighting to improve term matching-based retrieval systems. one line of research investigates projecting query and documents into a shared dense space . The models in this family are regarded as dense-retrieval models or dual encoder models. This is the kind of model that we employ in this work. Ideally, a query and its relevant documents should be projected in each other's vicinity. From this point of view, finding the most relevant documents can be cast to a nearest neighbor search .     Previous attempts at improving the quality of dual encoder models can be classified into two types. The first type focuses on finding a good initialization for the model parameters. This is typically achieved by pre-training the model on various tasks .     In particular, our strategy is based on synthetic query generation .   While the approach was originally proposed for zero-shot learning, we show that dual encoder models trained on supervised data can significantly improve when combined with synthetic question pre-training. Another type of approach focuses on learning better representations using hard negatives. This strategy has proved to be effective in passage retrieval tasks , machine translation  and entity linking . These works mine hard negatives using different strategies. For example,  mine ``coarse'' negatives with a low-resolution model.  mine hard negatives using a BM25 model, and  use a model trained with random negatives and select examples that are ranked above the correct one as negative examples.  Our work systematically studies different hard negative mining strategies.   TODO  Neural networks have also become popular at the reranking stage.  This paper focus on the retrieval stage, and leaves reranking as a future line of work.  reranking stage is outside the scope of this work, it is nonetheless worth mentioning here as neural reranking  is an important research direction.   Since reranking only processes a small subset of candidates that are returned by first-stage retrieval, it is possible to use resource-hungry models to capture the interaction between query and document terms, and thus improve the discriminating power of the models .      Using dense representation for passage or document retrieval is challenging.    The search space may include millions or billions of retrieval candidates.   Some recent works adopt a pipeline approach consisting of two steps .   As a first step, the system retrieves a large number of passages using sparse encoding representations using techniques such as BM25 and DeepCT .   In a second step, it re-ranks the passages using a neural model.   However, this pipeline approach is hindered by error propagation.   In addition, powerful cross-attention models such as BERT models, can be used in the second step to jointly encode the question-passage pairs as this is not feasible in the large-scale retrieval in the first step.     Dual encoder models consist of a pair of encoders that encode questions and passages separately and score each question-passage pair by calculating the inner product of their encodings.    This type of models shows strong performance on sentence-level retrieval tasks, such as translation pair retrieval, retrieval question answering .    Its scalability makes it a good alternative for the first-step retriever in the pipeline approach.   The scalability of the dual encoder models make them an appealing alternative for the first step retriever in the pipeline approach.   Karpukhin et al.\shortcite{dpr} made the first attempt to use dual encoder models for open-domain QA.   ANCE  proposes a training framework which collects negatives from an Approximate Nearest Neighbor  index of the corpus, which is updated in parallel to the model training process.",126
" Neural machine translation  has been explored typically in sentence-level translation settings. Such sentence-level nmt models inevitably suffer from ambiguities when multiple  %% semantically-different translations are accepted  interpretations are possible to a source sentence.  To address this issue, context-aware nmt models have recently been presented %to address the issue  to incorporate document-level information in translation. Most of the existing context-aware nmt models are end-to-end models which take as input the current source sentence to be translated and the context sentences, and then output a translation. These models are trained on document-level parallel data, namely, sentence pairs with surrounding, usually preceding, sentences in the source and target language. However, in practical scenarios, document-level bilingual data is limited in most language pairs and domains, % posing a challenge to building context-aware nmt systems .  In this study, we propose a simple yet effective approach to context-aware nmt  % consisting of  using two primitive components, a sentence-level nmt model and a document-level language model . This approach allows us to independently train the two components on bilingual data and monolingual data, respectively, without resorting to expensive document-level bilingual data.  % and thereby no document-level bilingual data is needed. To give a probabilistic foundation to this combination of two independent models, we exploit % take advantage of  the probabilistic nature of nmt decoding. When generating a sequence, a left-to-right decoder outputs a categorical probability distribution over the vocabulary at every time step. % . The decoder assigns higher probability to the tokens that would be more suitable at that step. Therefore,  % we can assume that  when multiple valid translations are possible to the source sentence, % , which has ambiguities a sentence-level nmt is confused by,  the decoder just gives a higher  % sequence  probability to the translation that is plausible without considering contexts.  % than to wrong ones. Our idea is to adjust the probability distributions in a context-aware manner using a document-level lm of the target language which  % is capable of modeling  models inter-sentential dependencies in the target side document.  % Since a network structure of nmt models evolves very quickly, model-agnostic approach like ours is more preferable than model-tweaking approach .  We evaluate our methods on English to French, Russian and Japanese translations with OpenSubtitles2018 corpus in terms of the bleu scores and contrastive discourse test sets. Experimental results confirmed that our method achieved comparable performance with existing context-aware nmt models.  The contributions of this paper are as follows:      As an approach to context-aware  without document-level parallel data, our work is related to the context-aware translation error correction model . DocRepair is a sequence-to-sequence model to map a document containing inconsistent sentences into a consistent one. DocRepair only looks at the output translation of the sentence-level  and thus is agnostic on the confidence of the words, which poses some limitations. For example, it may perform miscorrection when the output of the sentence-level  is irregular but correct. On the other hand, our -based models modify the output of the sentence-level model more softly based on the probabilities generated by the sentence-level  and a language model. Hence an irregular output of the sentence-level  is not changed when the sentence-level  is highly confident about it.  Our work is also related to shallow fusion, in which token-wise probabilities output by an  model and a sentence-level language model are combined to be used as translation scores in decoding.  The theoretical background of shallow fusion and our  are different: in shallow fusion, the language model is intended to promote fluency of translations, whereas in our , we use the probability ratio of two language model probabilities which only provides contextual difference and fluency is still left to the translation model. As we have discussed in \S and the experimental results and analysis,  captures contextual information better than the shallow fusion score with extended context.    In this work, we presented a simple and unified representation learning framework, \modelname, for event and entity coreference. \modelname~learns a  mention-pair representation by forwarding concatenated sentences into RoBERTa, where sentences provide the context of mentions. This algorithm is applied to both event and entity coreference benchmarks and obtains state of the art performance. In addition, we augmented this pairwise representation with structured argument features to further improve its performance in event coreference.      
","  As an approach to context-aware  without document-level parallel data, our work is related to the context-aware translation error correction model . DocRepair is a sequence-to-sequence model to map a document containing inconsistent sentences into a consistent one. DocRepair only looks at the output translation of the sentence-level  and thus is agnostic on the confidence of the words, which poses some limitations. For example, it may perform miscorrection when the output of the sentence-level  is irregular but correct. On the other hand, our -based models modify the output of the sentence-level model more softly based on the probabilities generated by the sentence-level  and a language model. Hence an irregular output of the sentence-level  is not changed when the sentence-level  is highly confident about it.  Our work is also related to shallow fusion, in which token-wise probabilities output by an  model and a sentence-level language model are combined to be used as translation scores in decoding.  The theoretical background of shallow fusion and our  are different: in shallow fusion, the language model is intended to promote fluency of translations, whereas in our , we use the probability ratio of two language model probabilities which only provides contextual difference and fluency is still left to the translation model. As we have discussed in \S and the experimental results and analysis,  captures contextual information better than the shallow fusion score with extended context.",127
" A keyphrase is a multi-word text representing highly abstractive information in a long document. Keyphrase extraction  is a task that aims to generate an appropriate keyphrase set for the given document, thus helping to identify salient contents and concepts from the document. Recently, the KE task has attracted much research interest since it serves as an important component of many downstream applications such as text summarization, document  classification, information retrieval and question generation.  Early KE systems commonly operate in an extractive manner, which usually consists of two steps: 1) selecting candidates from the source document using heuristic rules,  and 2) ranking the candidates list to determine which is correct. However, the two-step ranking approaches are usually based on feature engineering, which is labor-intensive. Motivated by the progress in sequence-to-sequence applications of neural networks, KE research's focus has gradually shifted to deep learning methods. \citet{DBLP:conf/acl/MengZHHBC17} first formulate KE as a sequence generation problem and introduce an attentive Seq2Seq framework to generate the keyphrase sequence conditioned on the input document. Compared with traditional methods, the Seq2Seq based method achieves superior performance.  Seq2Seq based KE is exposed to two major challenges: 1) Document-level representation learning. For any Seq2Seq generative framework, the latent hidden representation is a very important factor, and its quality will directly affect the decoder's performance. In KE task, the input is commonly a long document instead of a sentence, which poses a greater challenge to latent representation learning. 2) Modeling the compositionality of keyphrases set. The elements in the keyphrase set are dependent and correlated. That is, better modeling the inherent composition embodied in the keyphrase set during the learning process will effectively boost the diversity and quality of final results.   Recently, various approaches have been proposed to optimize the Seq2Seq generation framework in KE task. To learn a better latent representation, previous studies try to introduce different encoding structures  to address the two issues above simultaneously. We explore to incorporate the dependency tree for document representation learning in the encoder part. The syntactic dependency tree can help to locate key information in a document. In practice, the document graph  is constructed depending on the syntactic dependency tree, and then a convolution process will be operated over .  On the other hand, we rethink the implication of compositionality in the keyphrase set. In the training process of generative models, whether a candidate keyphrase should be generated not only hinges on the document itself, but also depends on the keyphrases that have already been generated. Therefore, a dynamic graph updating mechanism is introduced to explicitly modeling the inter-dependency among keyphrases. In our method, the graph structure in the encoder part will be dynamically updated according to the keyphrases generated in the decoder part. Concretely, after one keyphrase is decoded, its information will be transferred to modify the edge weights in the document graph through a score function, and the latent hidden representation will also be updated. In this approach, we could dynamically ensure the information exchange between encoder and decoder parts in both directions.   The contribution of this work is three-fold:  1) A novel generative framework, Div-DGCN, is proposed that leverages both the dynamic syntactic graph encoder and diversified inference process for KE. 2) A dynamic computation mechanism is adopted to model the compositionality in keyphrase set explicitly and then enhancing the information interchange between the encoder and decoder parts in the Seq2Seq architecture.  3) Extensive experiments conducted on five benchmarks show that our proposed method is effective against competitive baselines on several metrics.    Keyphrase extraction problem is usually carried out via extractive or generative methods. Conventional extractive methods usually use the two-step strategy that first extracts the candidate phrases using rules  and then ranks them based on supervised or unsupervised methods. \citet{DBLP:conf/aaai/GollapalliLY17} used sequence labeling models to extract keyphrases from the document.  Our model is based on Seq2Seq generative architecture. In this line of work, CopyRNN is the first to cooperate copy mechanism to generate keyphrases. Since then, Seq2Seq based generative models have gradually become the mainstream in the KE task. \citet{DBLP:conf/aaai/ChenGZKL19} proposed a title-guided Seq2Seq network to enhance the latent document representation. \citet{DBLP:conf/acl/ZhaoZ19} introduced linguistic annotations for representation learning. Deep graph-based methods have been used for text representation learning in many NLP tasks such as text summarization, semantic role labeling and machine translation. In KE task,  proposed to leverage graph-based encoder to model document-level word salience globally. Compared to previous works, our model explores the syntactic structure and lets the global decoder side information flow to impact the encoder representation, thus generates more global-aware document latent representations. The idea to model dynamic node embedding was also studied in other tasks, however, these methods evolve the model parameters instead of the graph structure, thus is different from ours.  There are also many studies focusing on the diversity of generated keywords. The catSeqD is an extension of catSeq with orthogonal regularization and target encoding. \citet{DBLP:conf/emnlp/ChenZ0YL18} further proposed a review mechanism to model the correlation between the keyphrases explicitly. Lately,  proposed a reinforcement learning based fine-tuning method, which fine-tunes the pre-trained models with adaptive rewards for generating more sufficient and accurate keyphrases. \citet{DBLP:journals/corr/abs-2004-08511} designed a soft/hard exclusion mechanism to enhance the diversity.  The idea of enhancing diversity is roughly selecting results that have not been generated or ensuring the whole results covering the main semantic contents.     We present an approach to context-aware  based on  between the context and the current sentence. We first provide the formulation of the objective, , and the computation process of the  using a sentence-level translation model and a document-level language model. We investigate two search methods, reranking and beam search, and evaluate the methods in English-French, English-Russian, and Japanese-English translation. We also provide some analysis and visualization to better understand the nature of  between the context and the current sentence.  We plan to design context-aware  using  for context-aware  models. We will extend our method to non-autoregressive . We will release all of the code to promote the reproducibility of our results.    
"," Keyphrase extraction problem is usually carried out via extractive or generative methods. Conventional extractive methods usually use the two-step strategy that first extracts the candidate phrases using rules  and then ranks them based on supervised or unsupervised methods. \citet{DBLP:conf/aaai/GollapalliLY17} used sequence labeling models to extract keyphrases from the document.  Our model is based on Seq2Seq generative architecture. In this line of work, CopyRNN is the first to cooperate copy mechanism to generate keyphrases. Since then, Seq2Seq based generative models have gradually become the mainstream in the KE task. \citet{DBLP:conf/aaai/ChenGZKL19} proposed a title-guided Seq2Seq network to enhance the latent document representation. \citet{DBLP:conf/acl/ZhaoZ19} introduced linguistic annotations for representation learning. Deep graph-based methods have been used for text representation learning in many NLP tasks such as text summarization, semantic role labeling and machine translation. In KE task,  proposed to leverage graph-based encoder to model document-level word salience globally. Compared to previous works, our model explores the syntactic structure and lets the global decoder side information flow to impact the encoder representation, thus generates more global-aware document latent representations. The idea to model dynamic node embedding was also studied in other tasks, however, these methods evolve the model parameters instead of the graph structure, thus is different from ours.  There are also many studies focusing on the diversity of generated keywords. The catSeqD is an extension of catSeq with orthogonal regularization and target encoding. \citet{DBLP:conf/emnlp/ChenZ0YL18} further proposed a review mechanism to model the correlation between the keyphrases explicitly. Lately,  proposed a reinforcement learning based fine-tuning method, which fine-tunes the pre-trained models with adaptive rewards for generating more sufficient and accurate keyphrases. \citet{DBLP:journals/corr/abs-2004-08511} designed a soft/hard exclusion mechanism to enhance the diversity.  The idea of enhancing diversity is roughly selecting results that have not been generated or ensuring the whole results covering the main semantic contents.",128
"  %Sentiment analysis  is one of the fundamental tasks in natural language processing that aims to find the attitude that the author expressed in his/her sentence. One of the important sub-tasks of SA is aspect based sentiment analysis in which the goal is to find the sentiment polarity toward a specific aspect mentioned in the sentence. Due to the importance of ABSA, several sub-tasks has been proposed and studied for this problem, including aspect category extraction, aspect term extraction, opinion word extraction and opinion summarization . Among these sub-tasks, Targeted Opinion Word Extraction  is an important sub-task that might provide useful information to explain the prediction of the sentiment polarity from an ABSA system. In particular, the goal of TOWE is to find the words that express the attitude of the author toward a specific target mentioned in that sentence. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", the word ``good"" is the opinion word for the target ``food"" while delicious is the opinion word for the target word ``drinks"". Among different applications, TOWE can be used for target-oriented sentiment analysis  and pair-wise opinion summarization .  %Sentiment analysis  is one of the fundamental tasks in natural language processing that aims to find the attitude that the author expressed in his/her sentence. One of the important sub-tasks of SA is aspect based sentiment analysis in which the goal is to find the sentiment polarity toward a specific aspect mentioned in the sentence. Due to the importance of ABSA, several sub-tasks has been proposed and studied for this problem, including aspect category extraction, aspect term extraction, opinion word extraction and opinion summarization . Among these topics, Targeted Opinion Word Extraction  is an important task that might provide useful information to explain and/or improve the sentiment polarity prediction of the ABSA systems. In particular, given a target word  in the input sentence, the goal of TOWE is to find the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", ``good"" is the opinion word for the target word ``food"" while the opinion words for the target word ``drinks"" would involve ``delicious''. Among different applications, TOWE finds its application in target-oriented sentiment analysis  and pair-wise opinion summarization .   %Targeted Opinion Word Extraction  is an important task in aspect based sentiment analysis  of sentiment analysis . Given a target word  in the input sentence, the goal of TOWE is to find the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", ``good"" is the opinion word for the target word ``food"" while the opinion words for the target word ``drinks"" would involve ``delicious''. As the opinion words might provide useful information to explain and/or improve the sentiment prediction of the ABSA systems, TOWE can be applied in different problems, including target-oriented sentiment analysis  and pair-wise opinion summarization .  Targeted Opinion Word Extraction  is an important task in aspect based sentiment analysis  of sentiment analysis . Given a target word  in the input sentence, the goal of TOWE is to identify the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, as a running example, in the sentence ``All warranties honored by XYZ  are disappointing."", ``disappointing"" is the opinion word for the target word ``warranties"" while the opinion words for the target word ``company"" would involve ``reputable''. Among others, TOWE finds its applications in target-oriented sentiment analysis  and opinion summarization .   %As the opinion words might provide useful information to explain and/or improve the sentiment prediction of the ABSA systems, TOWE can be applied in different problems, including target-oriented sentiment analysis  and pair-wise opinion summarization .  %A notable problem is that although the related tasks of TOWE has been extensively explored in the past, there have been only a few work to explicitly consider the TOWE problem in the literature . In particular, the most related task of TOWE is opinion word extraction  that aims to locate the terms used to express attitude explicitly in the sentence . A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence  while the opinion words in TOWE should be explicitly paired with a given target word. Note that some previous works have also attempted to jointly predict the target and opinion words ; however, the target words are still not paired with their corresponding opinion words in these studies .    %Among the previous works for TOWE, t  The early approach for TOWE has involved the rule-based and lexicon-based methods  while the recent work has focused on deep learning models for this problem . One of the insights from the rule-based methods is that the syntactic structures  of the sentences can provide useful information to improve the performance for TOWE . However, these syntactic structures have not been exploited in the current deep learning models for TOWE . Consequently, in this work, we seek to fill in this gap by extracting useful knowledge from the syntactic structures to help the deep learning models learn better representations for TOWE. In particular, based on the dependency parsing trees, we envision two major syntactic information that can be complementarily beneficial for the deep learning models for TOWE, i.e., the syntax-based opinion possibility scores and syntactic word connections for representation learning. First, for the syntax-based possibility scores, our intuition is that the closer words to the target word in the dependency tree of the input sentence tend to have better chance for being the opinion words for the target in TOWE. For instance, in our running example, the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , ``disappointing"" is directly connected to ``warranties"", promoting the distance between ``disappointing"" and ``warranties""  in the dependency tree as an useful feature for TOWE. Consequently, in this work, we propose to use the distances between the words and the target word in the dependency trees to obtain a score to represent how likely a word is an opinion word for TOWE . These possibility scores would then be introduced into the deep learning models to improve the representation learning for TOWE.  In order to achieve such possibility score incorporation, we propose to employ the representation vectors for the words in the deep learning models to compute a model-based possibility score for each word in the sentence. The model-based possibility scores also aim to quantify the likelihood of being an opinion word for each word in the sentence; however, they are based on the internal representation learning mechanism of the deep learning models for TOWE. To this end, we propose to inject the information from the syntax-based possibility scores into the models for TOWE by enforcing the similarity/consistency between the syntax-based and model-based possibility scores for the words in the sentence. The rationale is to leverage the possibility score consistency to guide the representation learning process of the deep learning models  to generate more effective representations for TOWE. In this work, we employ the Ordered-Neuron Long Short-Term Memory Networks   to obtain the model-based possibility scores for the words in the sentences for TOWE. ON-LSTM introduces two additional gates into the original Long Short-Term Memory Network  cells that facilitate the computation of the model-based possibility scores via the numbers of active neurons in the hidden vectors for each word.  %The second type of syntactic information employed for TOWE in this work considers the dependency connections between the words in the sentence.   %As the deep learning models need to compute a representation vector for each word to perform opinion word prediction in TOWE,   %While the possibility scores aim to improve the representation vectors for TOWE via the syntax-based possibility features, the second type of syntactic information in this work seeks to do so by leveraging the dependency connections between the words to infer the effective context words to be encoded in the representation vector for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector for a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``warranties"" as the target word and ``reputable"" as the word we need to compute the representation vector. One the one hand, it is important to include the information of the neighboring words of ``reputable""  in the representation so the models can know the context for the current word . On the other hand, the information about the target word  should also be encoded in the representation vector for ``reputable"" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label  for ``reputable"" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``I'' in the representation for ``reputable"" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network  model  to produce the final representation vectors for opinion word prediction.  For the second type of syntactic information in this work, the main motivation is to further improve the representation vector computation for each word by leveraging the dependency connections between the words to infer the effective context words for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector of a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``warranties"" as the target word and ``reputable"" as the word we need to compute the representation vector. On the one hand, it is important to include the information of the neighboring words of ``reputable""  in the representation so the models can know the context for the current word . On the other hand, the information about the target word  should also be encoded in the representation vector for ``reputable"" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label  for ``reputable"" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``I'' in the representation for ``reputable"" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word, given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network  model  to produce the final representation vectors for opinion word prediction.  Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words and those for the other words in the sentence. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several benchmark datasets.  %Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words  and those for the other opinion words  in the sentence . Extensive experiments are conducted to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several datasets.  %Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-related opinion words  and those for the other opinion words  in the sentence . As both target-related and non-target opinion words can be used to express the opinion of the author , we expect that the explicit representation distinction would help to better separate the two types of opinion words based on the target word, eventually improving the performance for TOWE in this work. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several datasets.   %the close distance between ``disappointing"" and the target word will suggest the models to include the information of ``warranties"" into the representation vector for ``disappointing"" while the long distance between ``warranties"" and ``reputable"" can help to prevent/mitigate that for the representation vector of ``reputable"". The presence of the information from the target word in the representation vectors will help the models to successfully accept ``disappointing"" as an opinion word and reject ``reputable"" in this case.   %the close words to the target word would provide more effective information to induce the representation vectors for a word in the sentence in TOWE than the farther ones.   %we argue that the syntactic neighboring words in the dependency tree would provide effective information to induce the representation vector for a word in opinion word prediction. For instance, in the running example with the target word ``warranties"", the close distance between ``disappointing"" and the target word will suggest the models to include the information of ``warranties"" into the representation vector for ``disappointing"" while the long distance between ``warranties"" and ``reputable"" can help to prevent/mitigate that for the representation vector of ``reputable"". The presence of the information from the target word in the representation vectors will help the models to successfully accept ``disappointing"" as an opinion word and reject ``reputable"" in this case.  %employ the dependency connections between the words to infer the effective context words .    %employ the syntactic neighboring words to compute the representation vectors for a word in the sentence for TOWE.   %extends the popular Long Short-Term Memory Networks  by introducing two additional gates  in the hidden vector computation. These new gates controls how long each neuron in the hidden vectors should be activated across different time steps  in the sentence . Based on such controlled neurons, the model-based importance score for a word can be determined by the number of active neurons that the word possesses in the operation of ON-LSTM. To our knowledge, this is the first time ON-LSTM is applied for RE in the literature.  %How can we encode the syntax-based importance scores of the words into a deep model? In this paper, we propose to employ the syntax-based importance scores to retain or update the information encoded in the representations of each word. In particular, those words that are syntactically more important should retain more information in the computation graph of the deep model while the information about less important words should be discarded more frequently. In order to impose this information update policy in our model, we use the new proposed architecture Ordered-Neuron Long Short-Term Memory  . ON-LSTM is an extension of the well-known Long Short-Term Memory  with two additional gates . These new gates are employed to control the frequency of updating each neuron across different time steps  in the sentence. Concretely, the values of the master forget and input gates determine how much information in the hidden vector of the LSTM cell should be retained or updated based on the word at the current time step. Thereby, one can infer the importance scores inferred by the model  using the values of the master forget or input gates. So, based on this characteristics of ON-LSTM, to encode the syntax-based importance scores into our model, we propose to exploit the syntax-based importance scores to regulate model-based importance scores. Specifically, in training time, we encourage the model-based scores to be consistent with syntax-based importance scores.  %the two words ``disappointing"" and ``warranties"" are directly connected to each other.  %Early feature-based models  has shown that syntactical structure of the sentence is useful for TWOE. More specifically, the application of dependency tree for TOWE is two fold:  Pairwise Word Importance: Dependency tree is useful to infer the relative importance of a word toward another word  in the same sentence. This relative importance could be helpful for TOWE to attend to the important words for the target word. To infer pair-wise importance of two words using dependency tree, one can computes the distance between two words in the dependency tree. For instance, as a running example, in the sentence ``All warranties honored by HP  are disappointing"", the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , the two words ``disappointing"" and ``warranties"" are directly connected to each other. The short distance between these two words could be helpful to infer the importance of the word ``disappointing"" for the target ``warranties"".  Word Connection: Dependency tree could provide better contextual information for each word via the connections of the word with its head and dependants, thus it helps to improve word representations. Thereby, dependency tree could benefit TOWE. For instance, in the running example, the head of ``reputable"" is ``company"" while the head of ``warranties"" is ``disappointing"". Therefore, it would be easier to infer that the opinion word ``disappointing"" is related to the target word ``warranties"" and ``reputable"" is irrelevant.   %Besides the difference between the rule-based and deep learning models for TOWE regarding the representation learning methods, the rule-based methods have exploited the syntactic structures  of the sentences to improve the performance for TOWE while  %In particular, the related tasks of TOWE involves target word extraction/aspect term exaction  , and opinion word extraction   . A key difference between OWE and TOWE is that the opinion words in OWE are general and do not need to tie to any target words in the sentence while TOWE explicitly   %Despite its potential benefits, TOWE has only been studied by a few works in the past, characterizing the early rule-based and lexicon-based approaches  and very recently deep learning models .   %In the literature, feature-based models and deep learning model has been proposed for both target word extraction  and opinion word extraction . While joint models predict both the opinion and the target words, they cannot pair up them, thus being unable to solve the task of TOWE. In the literature, only a few of works have studied the task of TOWE, including the early attempts with the rule-based and lexicon-based approaches  and the recent works with deep learning models for TOWE .        Targeted opinion word extraction , is one of the sub-tasks of aspect-based sentiment analysis . While this task is relatively less studded, in the literature, other related sub-tasks including opinion target extraction  or opinion word extraction  has been explored. Early attempts for OTE employed feature-based methods . Later, deep learning methods have been also exploited for this task . For OWE task, early work proposed feature-based models  . Moreover, joint models to simultaneously predict the target and the opinion words have been explored . However, joint models cannot pair up the opinion words and the targets.    In the literature,  are the early attempts to pair up the opinion words with the targets. Their methods is based on distance or syntax based patterns. Later, this direction is continued by recent works . While these methods employ deep learning for TOWE, they ignore the syntactic information to pair the opinion words and the targets. In this work, we propose a new deep learning method to efficiently employ the structure of the sentence for TOWE.    [We can extend this part for the related work]    A notable problem is that although the related tasks of TOWE has been extensively explored in the past, there have been only a few work to explicitly consider the TOWE problem in the literature .      Comparing to the related tasks, TOWE has been relatively less explored in the literature. In particular, the most related task of TOWE is opinion word extraction  that aims to locate the terms used to express attitude in the sentences . A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction  that attempts to identify the target words in the sentences . Note that some previous works have also attempted to jointly predict the target and opinion words ; however, the target words are still not paired with their corresponding opinion words in these studies.   .  As mentioned in the introduction, among a few previous work on TOWE, the main approaches include the rule-based methods   and the recent deep learning models . Our model is different from the previous deep learning models as we exploit the syntactic information  for TOWE with deep learning.   featuring both the feature-based models  and deep learning models  to solve this problem.    While the early feature-based model  has shown syntactical structure  is useful for TWOE, recent deep learning models  ignore this information.  Early feature-based models  has shown that syntactical structure of the sentence is useful for TWOE. More specifically, the application of dependency tree for TOWE is two fold:  Pairwise Word Importance: Dependency tree is useful to infer the relative importance of a word toward another word  in the same sentence. This relative importance could be helpful for TOWE to attend to the important words for the target word. To infer pair-wise importance of two words using dependency tree, one can computes the distance between two words in the dependency tree. For instance, as a running example, in the sentence ``All warranties honored by HP  are disappointing"", the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , the two words ``disappointing"" and ``warranties"" are directly connected to each other. The short distance between these two words could be helpful to infer the importance of the word ``disappointing"" for the target ``warranties"".  Word Connection: Dependency tree could provide better contextual information for each word via the connections of the word with its head and dependants, thus it helps to improve word representations. Thereby, dependency tree could benefit TOWE. For instance, in the running example, the head of ``reputable"" is ``company"" while the head of ``warranties"" is ``disappointing"". Therefore, it would be easier to infer that the opinion word ``disappointing"" is related to the target word ``warranties"" and ``reputable"" is irrelevant.    Despite the usefulness of the dependency tree for TOWE, recent deep learning approaches ignore this structural information in their models . In order to address this limitation, in this paper, we propose a deep learning model to utilize syntactic structure for TOWE. More specifically, the proposed approach is able to explicitly encode both target-oriented importance of the words and also to incorporate the word connections in the dependency tree.    How can we encode the syntax-based importance scores of the words into a deep model? In this paper, we propose to employ the syntax-based importance scores to retain or update the information encoded in the representations of each word. In particular, those words that are syntactically more important should retain more information in the computation graph of the deep model while the information about less important words should be discarded more frequently. In order to impose this information update policy in our model, we use the new proposed architecture Ordered-Neuron Long Short-Term Memory  . ON-LSTM is an extension of the well-known Long Short-Term Memory  with two additional gates . These new gates are employed to control the frequency of updating each neuron across different time steps  in the sentence. Concretely, the values of the master forget and input gates determine how much information in the hidden vector of the LSTM cell should be retained or updated based on the word at the current time step. Thereby, one can infer the importance scores inferred by the model  using the values of the master forget or input gates. So, based on this characteristics of ON-LSTM, to encode the syntax-based importance scores into our model, we propose to exploit the syntax-based importance scores to regulate model-based importance scores. Specifically, in training time, we encourage the model-based scores to be consistent with syntax-based importance scores.   In order to incorporate the word connections between words in the dependency tree into our model, we propose to utilize Graph Convolution Network  . However, the dependency tree is a general-purpose syntactic structure of the sentence and it is not tailored to this task. Therefore, some of the connections between words might not be relevant to this task and thereby relying solely on the dependency tree could hurt the performance. To overcome this issue, we propose to combine dependency tree with a weighted dense graph. The nodes of this graph are the words of the sentence and the weight of the edge between each pair of words is a function of the distance of the two words to the target word in the dependency tree. As the weights in this dense graph are trainable and they are a function of the target word, this dense graph is more task-specific thus it could regularize the noisy connections in the dependency tree.   Finally, to distinguish the opinion words that are related to the given target from the other opinion words, we propose a new syntax-based inductive bias to be imposed to the final loss function. This inductive bias encourage the model to regulate information flow in the GCN layer in a way that the related opinion words would propagate more information to other nodes than non-related opinion words. Our extensive experiments and analysis on four benchmark datasets prove the effectiveness of the proposed approach, leading to the state-of-the-art performance on all datasets.      We present a framework to learn disentangled representations of speech with unlabeled data. The framework includes a local VQ encoder to extract a discrete sequence representation of the speech contents and a global VAE encoder to learn a continuous representation of speech styles. Our evaluation shows that the discrete sequence representation effectively captures the linguistic contents while the continuous global representation encapsulates the speaker style. Additionally, we also show the application from our pre-trained model, in which we successfully train a speaker recognition system with high accuracy  only with one sample per speaker.  \vfill\pagebreak    References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   ------------------------------------------------------------------------- 
","   Targeted opinion word extraction , is one of the sub-tasks of aspect-based sentiment analysis . While this task is relatively less studded, in the literature, other related sub-tasks including opinion target extraction  or opinion word extraction  has been explored. Early attempts for OTE employed feature-based methods . Later, deep learning methods have been also exploited for this task . For OWE task, early work proposed feature-based models  . Moreover, joint models to simultaneously predict the target and the opinion words have been explored . However, joint models cannot pair up the opinion words and the targets.    In the literature,  are the early attempts to pair up the opinion words with the targets. Their methods is based on distance or syntax based patterns. Later, this direction is continued by recent works . While these methods employ deep learning for TOWE, they ignore the syntactic information to pair the opinion words and the targets. In this work, we propose a new deep learning method to efficiently employ the structure of the sentence for TOWE.    [We can extend this part for the related work]    A notable problem is that although the related tasks of TOWE has been extensively explored in the past, there have been only a few work to explicitly consider the TOWE problem in the literature .      Comparing to the related tasks, TOWE has been relatively less explored in the literature. In particular, the most related task of TOWE is opinion word extraction  that aims to locate the terms used to express attitude in the sentences . A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction  that attempts to identify the target words in the sentences . Note that some previous works have also attempted to jointly predict the target and opinion words ; however, the target words are still not paired with their corresponding opinion words in these studies.   .  As mentioned in the introduction, among a few previous work on TOWE, the main approaches include the rule-based methods   and the recent deep learning models . Our model is different from the previous deep learning models as we exploit the syntactic information  for TOWE with deep learning.   featuring both the feature-based models  and deep learning models  to solve this problem.    While the early feature-based model  has shown syntactical structure  is useful for TWOE, recent deep learning models  ignore this information.  Early feature-based models  has shown that syntactical structure of the sentence is useful for TWOE. More specifically, the application of dependency tree for TOWE is two fold:  Pairwise Word Importance: Dependency tree is useful to infer the relative importance of a word toward another word  in the same sentence. This relative importance could be helpful for TOWE to attend to the important words for the target word. To infer pair-wise importance of two words using dependency tree, one can computes the distance between two words in the dependency tree. For instance, as a running example, in the sentence ``All warranties honored by HP  are disappointing"", the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , the two words ``disappointing"" and ``warranties"" are directly connected to each other. The short distance between these two words could be helpful to infer the importance of the word ``disappointing"" for the target ``warranties"".  Word Connection: Dependency tree could provide better contextual information for each word via the connections of the word with its head and dependants, thus it helps to improve word representations. Thereby, dependency tree could benefit TOWE. For instance, in the running example, the head of ``reputable"" is ``company"" while the head of ``warranties"" is ``disappointing"". Therefore, it would be easier to infer that the opinion word ``disappointing"" is related to the target word ``warranties"" and ``reputable"" is irrelevant.    Despite the usefulness of the dependency tree for TOWE, recent deep learning approaches ignore this structural information in their models . In order to address this limitation, in this paper, we propose a deep learning model to utilize syntactic structure for TOWE. More specifically, the proposed approach is able to explicitly encode both target-oriented importance of the words and also to incorporate the word connections in the dependency tree.    How can we encode the syntax-based importance scores of the words into a deep model? In this paper, we propose to employ the syntax-based importance scores to retain or update the information encoded in the representations of each word. In particular, those words that are syntactically more important should retain more information in the computation graph of the deep model while the information about less important words should be discarded more frequently. In order to impose this information update policy in our model, we use the new proposed architecture Ordered-Neuron Long Short-Term Memory  . ON-LSTM is an extension of the well-known Long Short-Term Memory  with two additional gates . These new gates are employed to control the frequency of updating each neuron across different time steps  in the sentence. Concretely, the values of the master forget and input gates determine how much information in the hidden vector of the LSTM cell should be retained or updated based on the word at the current time step. Thereby, one can infer the importance scores inferred by the model  using the values of the master forget or input gates. So, based on this characteristics of ON-LSTM, to encode the syntax-based importance scores into our model, we propose to exploit the syntax-based importance scores to regulate model-based importance scores. Specifically, in training time, we encourage the model-based scores to be consistent with syntax-based importance scores.   In order to incorporate the word connections between words in the dependency tree into our model, we propose to utilize Graph Convolution Network  . However, the dependency tree is a general-purpose syntactic structure of the sentence and it is not tailored to this task. Therefore, some of the connections between words might not be relevant to this task and thereby relying solely on the dependency tree could hurt the performance. To overcome this issue, we propose to combine dependency tree with a weighted dense graph. The nodes of this graph are the words of the sentence and the weight of the edge between each pair of words is a function of the distance of the two words to the target word in the dependency tree. As the weights in this dense graph are trainable and they are a function of the target word, this dense graph is more task-specific thus it could regularize the noisy connections in the dependency tree.   Finally, to distinguish the opinion words that are related to the given target from the other opinion words, we propose a new syntax-based inductive bias to be imposed to the final loss function. This inductive bias encourage the model to regulate information flow in the GCN layer in a way that the related opinion words would propagate more information to other nodes than non-related opinion words. Our extensive experiments and analysis on four benchmark datasets prove the effectiveness of the proposed approach, leading to the state-of-the-art performance on all datasets.",129
"  Aspect-based Sentiment Analysis  is a fine-grained version of sentiment analysis  that aims to find the sentiment polarity of the input sentences toward a given aspect. We focus on the term-based aspects for ABSA where the aspects correspond to some terms  in the input sentence. For instance, an ABSA system should be able to return the negative sentiment for input sentence ``The staff were very polite, but the quality of the food was terrible.'' assuming ``food'' as the aspect term.    %Aspect based sentiment analysis  is a fine-grained version of sentiment analysis . In ABSA, the goal is to find the sentiment polarity of the sentence toward a given aspect. In the literature, two versions of aspect have been proposed:  Aspect-category: Aspect categories are a set of pre-defined categories in which the given sentence contains opinion of the author toward one of them. Aspect categories may not explicitly appear in the sentence.  Aspect-term: Aspect term is a subsequent of the sentence in which the given sentence express sentiment toward it. For instance in the example The staff were very polite, but the quality of the food was terrible., the author has a positive sentiment toward aspect-category service and negative sentiment toward aspect-term food. In this paper, we introduce a novel model for sentiment analysis toward aspect-term.  %The early attempts for ABSA have performed feature engineering to produce useful features for the statistical models  for this problem . One limitation of these feature-based models is that they require significant human effort and linguistic background to design effective features. In order to overcome this limitation,  %The typical network architectures for ABSA in the literature involve convolutional neural networks  , recurrent neural networks  , memory networks , attention  and gating mechanisms .  %automatically induce effective features for ABSA and  Due to its important applications , ABSA has been studied extensively in the literature. In these studies, deep learning has been employed to produce the state-of-the-art performance for this problem . Recently, in order to further improve the performance, the syntactic dependency trees have been integrated into the deep learning models  for ABSA . Among others, dependency trees help to directly link the aspect term to the syntactically related words in the sentence, thus facilitating the graph convolutional neural networks   to enrich the representation vectors for the aspect terms.  %Although the graph-based models have achieved decent performance for ABSA, these models have   However, there are at least two major issues in these graph-based models that should be addressed to boost the performance. First, the representation vectors for the words in different layers of the current graph-based models for ABSA are not customized for the aspect terms. This might lead to suboptimal representation vectors where the irrelevant information for ABSA might be retained and affect the model's performance. Ideally, we expect that the representation vectors in the deep learning models for ABSA should mainly involve the related information for the aspect terms, the most important words in the sentences. Consequently, in this work, we propose to regulate the hidden vectors of the graph-based models for ABSA using the information from the aspect terms, thereby filtering the irrelevant information for the terms and customizing the representation vectors for ABSA. In particular, we compute a gate vector for each layer of the graph-based model for ABSA leveraging the representation vectors of the aspect terms. This layer-wise gate vector would be then applied over the hidden vectors of the current layer to produce customized hidden vectors for ABSA. In addition, we propose a novel mechanism to explicitly increase the contextual distinction among the gates to further improve the representation vectors.  %as the hidden vectors at different layers of the graph-based models tend to capture different levels of contextual information, the gate vectors for the different layers should also maintain some level of contextual distinction. To this end, we propose a novel mechanism to explicitly increase the contextual distinction among the gate vectors to further improve the quality of the representation vectors.  The second limitation of the current graph-based deep learning models is the failure to explicitly exploit the overall importance of the words in the sentences that can be estimated from the dependency trees for the ABSA problem. In particular, a motivation of the graph-based models for ABSA is that the neighbor words of the aspect terms in the dependency trees would be more important for the sentiment of the terms than the other words in the sentence. The current graph-based models would then just focus on those syntactic neighbor words to induce the representations for the aspect terms. However, based on this idea of important words, we can also assign a score for each word in the sentences that explicitly quantify its importance/contribution for the sentiment prediction of the aspect terms. In this work, we hypothesize that these overall importance scores from the dependency trees might also provide useful knowledge to improve the representation vectors of the graph-based models for ABSA. Consequently, we propose to inject the knowledge from these syntax-based importance scores into the graph-based models for ABSA via the consistency with the model-based importance scores. In particular, using the representation vectors from the graph-based models, we compute a second score for each word in the sentences to reflect the model's perspective on the importance of the word for the sentiment of the aspect terms. The syntax-based importance scores are then employed to supervise the model-based importance scores, serving as a method to introduce the syntactic information into the model. In order to compute the model-based importance scores, we exploit the intuition that a word would be more important for ABSA if it is more similar the overall representation vector to predict the sentiment for the sentence in the final step of the model. In the experiments, we demonstrate the effectiveness of the proposed model with the state-of-the-art performance on three benchmark datasets for ABSA. In summary, our contributions include:   %, we propose to obtain another important score for each word in the sentence based on the representation vectors from the models. These model-based importance scores are then   %In particular, for ABSA, some words might introduce more useful information to predict the sentiment of the aspect terms than the the other words in the sentence   %In particular, some words in a given sentence might involve more useful information for relation prediction in RE than the other words, and the dependency tree for this sentence can help to better identify those important words and assign higher importance scores for them . We expect that introducing such importance information for the words in the deep learning models might lead to improved performance for RE. Consequently, in this work, we propose to obtain an importance score for each word in the sentences from the dependency trees . These will serve as the general tree representation to incorporate the syntactic information into the deep learning models for RE.  %In particular, as the aspect terms are the most important words in the sentences for ABSA,     %Syntactical structure, e.g., dependency tree, could shorten the distance between syntactically related words thus improve the contextualized representation of the words. In order to incorporate the syntactical tree into the deep models, recent work mainly employs the graph convolutional network   to model the interaction between words based on the syntactic tree. In order to emphasize on the given aspect term, current models use the representation of the aspect term generated by GCN either directly for final classification or as a gate to filter out features of a sequential model. However, these methods cannot benefit from the information of the given aspect term to control the information flow in the graph based model. Moreover, it is expected that the words that are syntactically related to the given aspect term should convey more information about the sentiment toward it. Unfortunately, non of the existing work considers this relative importance in the final representation of the sentence. In order to address these issue, we propose a new graph based model which employs the semantic of the given aspect term to control interaction between nodes/words in the GCN model and to emphasize more on the syntactically important words in the final representation of the sentence.  %Due to the application of ABSA in other downstream applications, e.g., opinion mining, it has gained a lot of attention in natural language processing community and several methods have been proposed for this task. Early attempts employed feature engineering to extract useful features for statistical models like SVM . These methods require extensive human effort and strong linguistic knowledge. They also suffer from low generalization ability. Due to these limitations, neural networks and deep models have superseded feature based models and obtain promising results in ABSA . Early deep models for ABSA have exploited sequential models  , convolutional neural nets  or even memory networks . In order to improve the performance, attention  and gating mechanism  has also been widely adopted in deep models. Recently, it has been shown that syntactical information could also improve the performance of deep models . Syntactical structure, e.g., dependency tree, could shorten the distance between syntactically related words thus improve the contextualized representation of the words. In order to incorporate the syntactical tree into the deep models, recent work mainly employs the graph convolutional network   to model the interaction between words based on the syntactic tree. In order to emphasize on the given aspect term, current models use the representation of the aspect term generated by GCN either directly for final classification or as a gate to filter out features of a sequential model. However, these methods cannot benefit from the information of the given aspect term to control the information flow in the graph based model. Moreover, it is expected that the words that are syntactically related to the given aspect term should convey more information about the sentiment toward it. Unfortunately, non of the existing work considers this relative importance in the final representation of the sentence. In order to address these issue, we propose a new graph based model which employs the semantic of the given aspect term to control interaction between nodes/words in the GCN model and to emphasize more on the syntactically important words in the final representation of the sentence.   %In particular, in this paper, we propose a novel model which employs the representation of the given aspect term to compute a gate. This gate is applied over the output of one layer of GCN. By doing so, the information represented in the aspect term would erase non-relevant information in each node/word obtained by its interaction with its neighbors in one aggregation step in GCN. As different layers of GCN capture different substructure of the graph, e.g., 1-hop vicinity vs 2-hop vicinity, we propose to exploit different gates in different layers. To ensure the gates in different layers are not the same, we propose a novel method to encourage diversity among gates in different layers of the GCN. Moreover, in addition to exploiting the semantic of the aspect term to control interactions between nodes/words in the GCN, in this paper, we propose to encourage the model to emphasize on the words that are syntactically important to the aspect term. In particular, we use the distance between a word to the aspect term in the dependency tree as an indication of the syntactic importance of the word to the aspect term. This importance is employed as supervision signal to encourage the model to emphasize on the words that are syntactically important to the aspect term. This is obtained in the final layer of the model when the sentiment prediction is performed. More specifically, we first estimate the semantic importance of each word by employing the final representation of the word as the input to a classifier to predict the label distribution and then we compute the KL-Divergence between this label distribution predicted by the word representation and the label distribution predicted by the sentence representation. If the two label distribution are more similar, it shows that the word representation contains most of the information that the model consumes to perform the final classification. Finally, in order to ensure those words which are syntactically important to the aspect term are semantically important in the model too, we decrease the divergence between distribution of the syntactic score and semantic score for each word via KL-Divergence between these two distributions.  %Our extensive experiments on three benchmark datasets, empirically prove the effectiveness of the proposed model leading to new state-of-the-art results in all three benchmark datasets.      A novel method to regulate the GCN-based representation vectors of the words using the given aspect term for ABSA.   A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term.   Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-the-art performance for all the datasets.      Sentiment analysis has been studied under different settings in the literature  . For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models  . Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data . The typical network architectures for ABSA in the literature involve convolutional neural networks  , recurrent neural networks  , memory networks , attention  and gating mechanisms . The current state-of-the-art deep learning models for ABSA feature the graph-based models where the dependency trees are leveraged to improve the performance. . However, to the best of our knowledge, none of these works has used the information from the aspect term to filter the graph-based hidden vectors and exploited importance scores for words from dependency trees as we do in this work.   For instance,  runs a GCN model over the outputs of a LSTM layer from which the representation vector of the aspect term from GCN is used to compute the attention weights for the LSTM hidden vectors.  also employs GCN and LSTM, but their LSTM component is run across the layers of the GCN model.   Sentiment analysis is one of the important tasks in natural language processing with many work . Due to its popularity different settings for this task have been studied including document level , sentence level , aspect level , multi-modal , and cross-domain . In this paper, we study aspect level sentiment analysis. This setting has two main sub-tasks:  Aspect level sentiment classification: In this setting, the goal is to predict the sentiment polarity toward a given aspect mentioned in the sentence.  End-to-End aspect based sentiment analysis: In this setting, the model jointly predicts the aspect and the sentiment polarity toward it . Here, we focus on the former setting.    Early attempts for aspect level sentiment classification have employed feature engineering to train a statistical model . Recently, deep learning based models have superseded the feature based models and achieve new state-of-the-art results for ABSA . Most of the deep models use the sequential order of the words to obtain the representation of the sentence. This representation could be further improved for ABSA using either attention mechanism  or gating mechanism . In addition to sequential order, recent work have shown promising results by incorporating the syntactical tree . For instance,  employs a GCN based model over the outputs of a LSTM layer to find syntax-aware representation of the words. They use the representation of the aspect term from the GCN model to compute attention weights for the output of the LSTM.  employs GCN to propagate the context information to the aspect term via syntactic relations. It further applies an LSTM layer across different layers of the GCN to compute aspect related features for each word.      information flow in the graph based models, e.g., GCN. Moreover, they ignore the syntactic importance of the words to the given aspect term in order to generate the final representation of the sentence.    To the best of our knowledge, none of the current graph-based models for ABSA syntax-aware work has used the information about the aspect term to control information flow in the graph based models, e.g., GCN. Moreover, they ignore the syntactic importance of the words to the given aspect term in order to generate the final representation of the sentence.      We propose a novel deep learning model for TOWE that seeks to incorporate the syntactic structures of the sentences into the model computation. Two types of syntactic information are introduced in this work, i.e., the syntax-based possibility scores for words  and the syntactic connections between the words . We also present a novel inductive bias to improve the model, leveraging the representation distinction between the words in TOWE. Comprehensive analysis is done to demonstrate the effectiveness of the proposed model over four datasets.   Our comprehensive analysis on the model architecture together with the extensive experiments on four benchmark datasets demonstrate the effectiveness of the proposed model.   More specifically, the syntactic structure is employed to infer the target-related importance of the words and it is incorporated into the model using the newly proposed architecture Ordered-Neuron LSTM . Moreover, we employed GCN to encode word connections in the dependency tree. To overcome the noisy or non-relevant connections in the dependency tree, we propose to learn a dense graph out of the dependency tree which is customized to the given target word. Finally, we introduce a syntax-based regularization to preserve target-related information in the model. Our comprehensive analysis on the model architecture together with the extensive experiments on four benchmark datasets show the effectiveness of the proposed model, establishing new the state-of-the-art results on all of datasets.    In this work, we study the target-oriented opinion word extraction problem , one of the sub-tasks of aspect-based sentiment analysis  . We propose a deep learning model to incorporate the syntactic structure into the model computations. More specifically, the syntactic structure is employed to infer the target-related importance of the words and it is incorporated into the model using the newly proposed architecture Ordered-Neuron LSTM . Moreover, we employed GCN to encode word connections in the dependency tree. To overcome the noisy or non-relevant connections in the dependency tree, we propose to learn a dense graph out of the dependency tree which is customized to the given target word. Finally, we introduce a syntax-based regularization to preserve target-related information in the model. Our comprehensive analysis on the model architecture together with the extensive experiments on four benchmark datasets show the effectiveness of the proposed model, establishing new the state-of-the-art results on all of datasets.  
","  Sentiment analysis has been studied under different settings in the literature  . For ABSA, the early works have performed feature engineering to produce useful features for the statistical classification models  . Recently, deep learning models have superseded the feature based models due to their ability to automatically learn effective features from data . The typical network architectures for ABSA in the literature involve convolutional neural networks  , recurrent neural networks  , memory networks , attention  and gating mechanisms . The current state-of-the-art deep learning models for ABSA feature the graph-based models where the dependency trees are leveraged to improve the performance. . However, to the best of our knowledge, none of these works has used the information from the aspect term to filter the graph-based hidden vectors and exploited importance scores for words from dependency trees as we do in this work.   For instance,  runs a GCN model over the outputs of a LSTM layer from which the representation vector of the aspect term from GCN is used to compute the attention weights for the LSTM hidden vectors.  also employs GCN and LSTM, but their LSTM component is run across the layers of the GCN model.   Sentiment analysis is one of the important tasks in natural language processing with many work . Due to its popularity different settings for this task have been studied including document level , sentence level , aspect level , multi-modal , and cross-domain . In this paper, we study aspect level sentiment analysis. This setting has two main sub-tasks:  Aspect level sentiment classification: In this setting, the goal is to predict the sentiment polarity toward a given aspect mentioned in the sentence.  End-to-End aspect based sentiment analysis: In this setting, the model jointly predicts the aspect and the sentiment polarity toward it . Here, we focus on the former setting.    Early attempts for aspect level sentiment classification have employed feature engineering to train a statistical model . Recently, deep learning based models have superseded the feature based models and achieve new state-of-the-art results for ABSA . Most of the deep models use the sequential order of the words to obtain the representation of the sentence. This representation could be further improved for ABSA using either attention mechanism  or gating mechanism . In addition to sequential order, recent work have shown promising results by incorporating the syntactical tree . For instance,  employs a GCN based model over the outputs of a LSTM layer to find syntax-aware representation of the words. They use the representation of the aspect term from the GCN model to compute attention weights for the output of the LSTM.  employs GCN to propagate the context information to the aspect term via syntactic relations. It further applies an LSTM layer across different layers of the GCN to compute aspect related features for each word.      information flow in the graph based models, e.g., GCN. Moreover, they ignore the syntactic importance of the words to the given aspect term in order to generate the final representation of the sentence.    To the best of our knowledge, none of the current graph-based models for ABSA syntax-aware work has used the information about the aspect term to control information flow in the graph based models, e.g., GCN. Moreover, they ignore the syntactic importance of the words to the given aspect term in order to generate the final representation of the sentence.",130
"  Coreference resolution is the task of grouping mentions in a text that refer to the same real-world entity into clusters  . %The task is an important prerequisite to a variety of natural language processing systems, such as textual entailment and information extraction .  Coreference resolution is a difficult task  that %since it  requires reasoning, context understanding, and background knowledge of real-world entities,  and %Therefore, the task  has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 {\CONLL} shared tasks . Since then,  there has been substantial research on English coreference, most recently using neural coreference approaches , leading to a significant increase in  %that significantly increased  the  performance of coreference resolvers for English. % % The  general objective of %the research described in  % this paper is to close a very evident gap in the recent literature in coreference. By contrast, there has been almost no research on Arabic coreference;  the performance for Arabic coreference resolution has not improved  much since the {\CONLL} 2012 shared task, and in particular no neural architectures have been proposed--the current state-of-the-art system remains  the model proposed in %feature-based  .  In this paper we close this very obvious gap by proposing what to our knowledge is the first neural coreference resolver for Arabic.  One explanation for this lack of research might simply be the lack of  training data large enough for the task.  Another explanation  might be that Arabic is  more problematic than English  %more complex than English  because of its rich morphology,  %rich proprieties,  its %has  many dialects,  and/or  its %contains a  high degree of ambiguity.  We explore the first of these possibilities.  %Our proposal does address some of these aspects.  %one aspect of the problem. % Another explanation might be the lack of large-size training data for the task.  % We explore the Coreference resolution can be further divided into two subtasks--mention detection and mention clustering--as illustrated in  %an example of these two steps in  Figure .   % % Coreference resolution is a difficult task  % that %since it  % requires reasoning, context understanding, and background knowledge of real-world entities,  % and % %Therefore, the task  % has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English . % %and various approaches have been applied.  In  early work, coreference's two subtasks were usually carried out in a pipeline fashion , with candidate mentions selected prior the mention clustering step.  Since   introduced  an end-to-end neural coreference architecture  that achieved state of the art  by carrying out the two tasks jointly, as first proposed by , most state-of-the-art systems have followed this approach. % the first end-to-end coreference system that solves the two subtasks jointly.  % This leads to a number of subsequent systems  that significantly increased the coreference resolution performance on English.  % By contrast, there were little developments for Arabic coreference resolution, the performance for Arabic coreference resolution does not improve much since the CoNLL 2012 shared task the current state-of-the-art system remain feature-based .  However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size.  % One explanation for this might be that Arabic is more complex than English because of its morphologically rich proprieties, has many dialects, and contains a high degree of ambiguity.  % Another explanation might be the lack of large-size training data for the task.   The approach we followed to adapt %In this work, we introduce a recipe to show how  the state-of-the-art English coreference resolution architecture  to Arabic %can be adapted for the Arabic language is as follows. We started with a strong baseline system , enhanced  with contextual {\BERT} embeddings . We then explored three methods for improving the model's performance for  Arabic.  %In total we evaluated three options,  The first method is to pre-process  Arabic words with heuristic rules.  We follow  to normalize the letters with different forms, and removing all the diacritics. This results in a substantial improvement of 7 percentage points over our baseline.  The second route is to replace  multilingual {\BERT} with a {\BERT} model trained only on the Arabic texts  .  Multilingual {\BERT} is trained with 100+ languages; as a result,  it is not optimized for any of them. %needs to balance the tread of between languages.  As shown by , monolingual {\BERT}  trained only on the Arabic texts has better performance on various {\NLP} tasks.   We found the same holds for coreference: %resolution task, by  using embeddings from  monolingual {\BERT}, the model further improved the {\CONLL} F1 by 4.8 percentage points. Our third step is to leverage the end-to-end system with a separately trained mention detector .  We show that a better mention detection performance can be achieved by using a separately trained mention detector.  And by using a hybrid training strategy between the end-to-end and pipeline approaches  our system gains an additional 0.8 percentage points.  Our final system achieved a {\CONLL} F1 score of 63.9\%, which is is 15\% more than the previous state-of-the-art system  on Arabic coreference with the {\CONLL} dataset.  Overall, we show that the state-of-the-art English coreference model can be adapted to Arabic coreference  leading to a substantial improvement in performance when compared to previous feature-based systems.        Like with other natural language processing tasks, most state-of-the-art coreference resolution systems are evaluated  on English data.  Coreference resolution for English is an active area of research.   Early systems  Until the appearance of neural systems,  state-of-the-art systems for English coreference resolution   were either  are mainly  rule-based  or feature-based .   introduced a neural network-based approach to solving the task in a non-linear way.  In their system, the heuristic features  that commonly used in linear models are transformed by a  function to be used as the mention representations.   integrated reinforcement learning to let the model optimize directly on the B scores.   first presented a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a bi-directional LSTM.    is an extended version of  mainly enhanced by using {\ELMO} embeddings ,  in addition, the use of second-order inference enabled the system to explore partial entity level features and further improved the system by 0.4 percentage points.  Later the model was further improved by  who use  {\BERT} embeddings  instead of {\ELMO} embeddings.  In these systems,  At this stage,  both {\BERT} and {\ELMO} embeddings are used in a pre-trained fashion.  More recently,  fine-tuned the {\BERT} model for  coreference, resulting in a small further improvement.  Later,  introduces \ACRO{SpanBERT} which is trained for  tasks that involve spans. Using  \ACRO{SpanBERT},  they  their system  achieved a substantial gain of 2.7\  when compared with the  model.  reformulate the coreference resolution task as question answering task and achieved the state-of-the-art results by pretrain the system first on the large question answering corpora.     There have been several studies of Arabic coreference resolution; in particular, several of the systems involved in the {\CONLL} 2012 shared task attempted Arabic as well.   used syntactic parse trees to detect mentions, and compared pairs of mention based on their semantic and syntactic features.  proposed a language independent module that requires only  syntactic information and clusters mentions using the memory-based learner TiMBL .   detected mentions by employing named entity and language-dependent heuristics.  They employed multiple sieves  for English and Chinese, but  only used an exact match sieve for Arabic because other sieves did not provide better results.   considered all noun phrases and possessive pronouns as mentions, and trained two types of classifier: logistic regression and decision trees.  extracted all noun phrases, pronouns, and possessive pronouns as mentions. Then they applied 's solver which consists of various lexical and graph dependency features.   adapted  for Arabic the BART  coreference resolution system, which consists of five components:  pre-processing pipeline, mention factory, feature extraction module, decoder and encoder.  These components interacts through a language plugin module to create a set of features for a language specifically and then cluster mentions based on these features.   defined a set of rules  based on parse tree information to detect mentions, and utilized a latent tree representation to learn coreference chains.  Similarly  adopted a tree representation approach to cluster mentions, but improved the learning strategy and introduced non-local features to capture more information about coreference relations.  There have been other research studies related to anaphora resolution , but they only considered  pronominal anaphora.  also considered a specific type of pronominal anaphora,  zero-pronoun anaphora.  All current approaches suffer from a number of limitations, one of which is that most of them rely on an extensive set of hand-chosen features.    Building trust and confidence in agents with conversational interfaces requires smooth dialogue that avoids breakdown. Detecting dialogue breakdown, either before or while it happens, is an essential part in ensuring users have satisfactory  experiences with these agents. We investigate two semi-supervised learning methods to leverage unlabelled data to improve breakdown detection, including continued pre-training on the Reddit dataset and SSMBA data augmentation. We utilize these methods in our submission to the 5th Dialogue Breakdown Detection Challenge, beating other baselines and submissions by a large margin. In ablations on previous test sets, we show that the addition of our semi-supervised methods improves our baseline models by over 2\  accuracy and reduces JS divergence by over 0.003. These methods are simple and applicable to any dialogue task. In the future we will continue to investigate applying these methods for intent prediction, slot filling, state tracking and language generation.  
","    Like with other natural language processing tasks, most state-of-the-art coreference resolution systems are evaluated  on English data.  Coreference resolution for English is an active area of research.   Early systems  Until the appearance of neural systems,  state-of-the-art systems for English coreference resolution   were either  are mainly  rule-based  or feature-based .   introduced a neural network-based approach to solving the task in a non-linear way.  In their system, the heuristic features  that commonly used in linear models are transformed by a  function to be used as the mention representations.   integrated reinforcement learning to let the model optimize directly on the B scores.   first presented a neural joint approach for mention detection and coreference resolution. Their model does not rely on parse trees; instead, the system learns to detect mentions by exploring the outputs of a bi-directional LSTM.    is an extended version of  mainly enhanced by using {\ELMO} embeddings ,  in addition, the use of second-order inference enabled the system to explore partial entity level features and further improved the system by 0.4 percentage points.  Later the model was further improved by  who use  {\BERT} embeddings  instead of {\ELMO} embeddings.  In these systems,  At this stage,  both {\BERT} and {\ELMO} embeddings are used in a pre-trained fashion.  More recently,  fine-tuned the {\BERT} model for  coreference, resulting in a small further improvement.  Later,  introduces \ACRO{SpanBERT} which is trained for  tasks that involve spans. Using  \ACRO{SpanBERT},  they  their system  achieved a substantial gain of 2.7\  when compared with the  model.  reformulate the coreference resolution task as question answering task and achieved the state-of-the-art results by pretrain the system first on the large question answering corpora.     There have been several studies of Arabic coreference resolution; in particular, several of the systems involved in the {\CONLL} 2012 shared task attempted Arabic as well.   used syntactic parse trees to detect mentions, and compared pairs of mention based on their semantic and syntactic features.  proposed a language independent module that requires only  syntactic information and clusters mentions using the memory-based learner TiMBL .   detected mentions by employing named entity and language-dependent heuristics.  They employed multiple sieves  for English and Chinese, but  only used an exact match sieve for Arabic because other sieves did not provide better results.   considered all noun phrases and possessive pronouns as mentions, and trained two types of classifier: logistic regression and decision trees.  extracted all noun phrases, pronouns, and possessive pronouns as mentions. Then they applied 's solver which consists of various lexical and graph dependency features.   adapted  for Arabic the BART  coreference resolution system, which consists of five components:  pre-processing pipeline, mention factory, feature extraction module, decoder and encoder.  These components interacts through a language plugin module to create a set of features for a language specifically and then cluster mentions based on these features.   defined a set of rules  based on parse tree information to detect mentions, and utilized a latent tree representation to learn coreference chains.  Similarly  adopted a tree representation approach to cluster mentions, but improved the learning strategy and introduced non-local features to capture more information about coreference relations.  There have been other research studies related to anaphora resolution , but they only considered  pronominal anaphora.  also considered a specific type of pronominal anaphora,  zero-pronoun anaphora.  All current approaches suffer from a number of limitations, one of which is that most of them rely on an extensive set of hand-chosen features.",131
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.      Graph neural networks  -- a family of neural models for learning latent node representations in a graph, have achieved remarkable success in different graph learning tasks. Most of the prevailing GNN models follow the paradigm of neighborhood aggregation, aiming to learn latent node representations via message passing among local neighbors in the graph. With deep roots in graph spectral theory, the learning process of graph convolutional networks  can be considered as a mean-pooling neighborhood aggregation. Later on, GraphSAGE was developed to concatenate the node's feature with mean/max/LSTM pooled neighborhood information, which enables inductive representation learning on large graphs. Graph attention networks ~ incorporate trainable attention weights to specify fine-grained weights on neighbors when aggregating neighborhood information of a node. Recent research further extend GNN models to consider global graph information and edge information during aggregation. More recently, hypergraph neural networks are proposed to capture high-order dependency between nodes. Our model HyperGAT is the first attempt to shift the power of hypergraph to the canonical text classification task.       Grounded on the fast development of deep learning techniques, various neural models that automatically represent texts as embeddings have been developed for text classification. Two representative deep neural models, CNNs~ and RNNs~ have shown their superior power in the text classification task. To further improve the model expressiveness, a series of attentional models have been developed, including hierarchical attention networks~, attention over attention~, etc. More recently, graph neural networks have shown to be a powerful tool for solving the problem of text classification by considering the long-distance dependency between words. Specifically, TextGCN~ applies the graph convolutional networks ~ on a single large graph built from the whole corpus, which achieves state-of-the-art performance on text classification. Later on, SGC~ is proposed to reduce the unnecessary complexity and redundant computation of GCNs, and shows competitive results with superior time efficiency. TensorGCN~ proposes a text graph tensor to learn word and document embeddings by incorporating more context information.  propose to learn text representations on document-level graphs. However, those transductive methods are computationally inefficient and cannot capture the high-order interactions between words for improving model expressive power.               In this paper, we designed an end-to-end attention-based CNN-LSTM-DNN emotion classifier.   In our classifier, the convolutional layers  extract salient features, the bi-directional long short-term memory  layers handle the sequential phenomena of the speech signal. This is followed by an attention layer, which extracts a summary vector that is fed to the fully connected dense layer , which finally connects to a softmax layer. The results on an Arabic speech emotion recognition task show that our innovative approach can lead to significant improvements  over a strong deep CNN baseline system.  However, the deep CNN models are significantly faster than the attention-based CNN-LSTM-DNN models in training and classification phases.   Future work will focus on training an ensemble classifier and interpolating the predictions to improve the classification accuracy. We plan to use large Arabic emotion databases using our powerful attention-based CNN-LSTM-DNN models. In addition, joint estimation of the emotion, dialect, language, and accent using multitask learning will be investigated.   In addition, the  separate label per  frame  methods developed in will be compared with  the  single label per utterance methods commonly used in the field in a unified framework.     \pagebreak        
","  Graph neural networks  -- a family of neural models for learning latent node representations in a graph, have achieved remarkable success in different graph learning tasks. Most of the prevailing GNN models follow the paradigm of neighborhood aggregation, aiming to learn latent node representations via message passing among local neighbors in the graph. With deep roots in graph spectral theory, the learning process of graph convolutional networks  can be considered as a mean-pooling neighborhood aggregation. Later on, GraphSAGE was developed to concatenate the node's feature with mean/max/LSTM pooled neighborhood information, which enables inductive representation learning on large graphs. Graph attention networks ~ incorporate trainable attention weights to specify fine-grained weights on neighbors when aggregating neighborhood information of a node. Recent research further extend GNN models to consider global graph information and edge information during aggregation. More recently, hypergraph neural networks are proposed to capture high-order dependency between nodes. Our model HyperGAT is the first attempt to shift the power of hypergraph to the canonical text classification task.       Grounded on the fast development of deep learning techniques, various neural models that automatically represent texts as embeddings have been developed for text classification. Two representative deep neural models, CNNs~ and RNNs~ have shown their superior power in the text classification task. To further improve the model expressiveness, a series of attentional models have been developed, including hierarchical attention networks~, attention over attention~, etc. More recently, graph neural networks have shown to be a powerful tool for solving the problem of text classification by considering the long-distance dependency between words. Specifically, TextGCN~ applies the graph convolutional networks ~ on a single large graph built from the whole corpus, which achieves state-of-the-art performance on text classification. Later on, SGC~ is proposed to reduce the unnecessary complexity and redundant computation of GCNs, and shows competitive results with superior time efficiency. TensorGCN~ proposes a text graph tensor to learn word and document embeddings by incorporating more context information.  propose to learn text representations on document-level graphs. However, those transductive methods are computationally inefficient and cannot capture the high-order interactions between words for improving model expressive power.",132
"   Publicly available biomedical articles keep increasing rapidly. Automated systems that utilize biomedical text mining methods are necessary to be able to handle this large amount of data with minimal manual effort. An important first step of any biomedical text mining method is the detection and classification of biomedical entities such as disease, drug or chemical mentions in biomedical articles. This task is referred to as biomedical named entity recognition .  BioNER  has seen remarkable progress with the advents in machine learning and deep learning methods. These methods require labeled datasets and benefit from increasing the amount of labeled examples. Artificial neural networks form the core of almost all state-of-the-art bioNER systems. The main drawback of these methods is that the networks must be trained from scratch for each dataset, separately. Even though recent progress in BioNER is promising, the overall performance is significantly lower than general domain NER. This is mainly due to the scarcity and sub-optimal utilization of the labeled datasets in the biomedical domain.  Transfer learning is a training paradigm that mitigates the above mentioned issues of current approaches. It attempts to utilize the information obtained from a source task to improve the performance on a target task. Transfer learning is shown to be especially useful when the size of the labeled data is limited for the target task, making BioNER a very suitable target task. Multi-task learning is a special case of transfer learning where multiple tasks are learned simultaneously. In this context, this corresponds to learning multiple biomedical named entity datasets using a single neural network.  Recently, the seminal work of Devlin et al., i.e. the BERT model, enabled progress in various NLP tasks, including NER. BERT uses self-supervised learning which relieves the need for having labeled examples to train the neural network. Lee et al. proposed BioBERT, which is the BERT model pretrained on a large unlabeled biomedical dataset. They finetuned the BioBERT model on labeled datasets using supervised learning and obtained improvements on several downstream biomedical NLP tasks. Yet, BioBERT has not been applied before in the context of multi-task learning, to the best of our knowledge. This motivated us to use BioBERT as the shared network across all biomedical datasets. We claim that sharing information across datasets help improve the overall performance as the representations obtained on one biomedical dataset is relevant to others, even though the annotated entities are different.  Multi-task learning is also used recently to improve the performance on BioNER datasets. Yet, the analysis of where the improvements come from is limited and the effect of transfer learning is unclear. Thus, there is a lack of theoretical understanding of when and why transfer learning and multi-task learning bring improvements.  In this study, we analyze the effect of multi-task learning for biomedical named entity recognition. To this end, we experimented on seven BioNER benchmark datasets and analyzed the effect of multi-task learning by using ten different measures. We evaluate the usefulness of these measures with three different metrics. Besides, we propose combining transfer learning and multi-task learning for BioNER which is not employed before to the best of our knowledge. The main contributions of this study are as follows:           Multi-task learning and transfer learning have both been applied for BioNER before. Lee et al. obtained state-of-the-art results on several biomedical entity recognition datasets by using the BERT language model pretrained on the Biomedical abstracts and papers.  Crichton et al. is the first work that attempts to apply multi-task learning for detecting biomedical named entities. They used pretrained biomedical word embeddings and used a CNN based neural network to detect the named entities. They also use Viterbi decoding for the tag transition probabilities. They only calculate binary probabilities for tag transitions , whereas we use a CRF layer with trainable parameters to output a score for each possible transition. Wang et al. proposed a BiLSTM-CRF based neural network where  character and word embeddings are shared by all dataset-specific components. Unlike  Crichton et al., who propose sharing a convolutional layer, they propose sharing only word/character embeddings across different datasets. Mehmood et al. also analyzed the effect of multi-task learning for BioNER datasets. They propose using stack LSTMs for multi-task learning of BioNER datasets. Their analysis of multi-task learning focus on architectural variations, whereas we focus on analyzing dataset characteristics for transferability. Yoon et al. proposed using an expert BiLSTM-CRF based network for each dataset, separately. During training, they use the output of all other trained models as an additional input to the neural network. They claim that the output of each expert model helps prevent mislabelings caused by polysemy.  Even though both transfer learning and multi-task learning have already been used for BioNER, the analysis of the effect of using these methods is highly limited. A similar analysis to ours is done before on general domain NLP datasets. Bingel et al. analyzed the task relations for multi-task learning on ten NLP tasks. Alonzo et al. analyzed the effect of multi-task learning on several sequence labeling tasks. Different from these previous studies,w we focus on the datasets from the biomedical domain. These datasets have unique characteristics and we claim that findings on the general domain datasets might not be directly applicable to them.       This paper presents a comprehensive review of text style transfer with deep learning methods. We have surveyed recent research efforts in TST and developed schemes to categorize and distill the existing literature. This survey has covered the task formulation, evaluation metrics, and methods on parallel and non-parallel data. We also discussed several important topics on TST, its connection to other NLP tasks, and important future directions. This survey provide a reference for future researchers working on TST.  
"," Multi-task learning and transfer learning have both been applied for BioNER before. Lee et al. obtained state-of-the-art results on several biomedical entity recognition datasets by using the BERT language model pretrained on the Biomedical abstracts and papers.  Crichton et al. is the first work that attempts to apply multi-task learning for detecting biomedical named entities. They used pretrained biomedical word embeddings and used a CNN based neural network to detect the named entities. They also use Viterbi decoding for the tag transition probabilities. They only calculate binary probabilities for tag transitions , whereas we use a CRF layer with trainable parameters to output a score for each possible transition. Wang et al. proposed a BiLSTM-CRF based neural network where  character and word embeddings are shared by all dataset-specific components. Unlike  Crichton et al., who propose sharing a convolutional layer, they propose sharing only word/character embeddings across different datasets. Mehmood et al. also analyzed the effect of multi-task learning for BioNER datasets. They propose using stack LSTMs for multi-task learning of BioNER datasets. Their analysis of multi-task learning focus on architectural variations, whereas we focus on analyzing dataset characteristics for transferability. Yoon et al. proposed using an expert BiLSTM-CRF based network for each dataset, separately. During training, they use the output of all other trained models as an additional input to the neural network. They claim that the output of each expert model helps prevent mislabelings caused by polysemy.  Even though both transfer learning and multi-task learning have already been used for BioNER, the analysis of the effect of using these methods is highly limited. A similar analysis to ours is done before on general domain NLP datasets. Bingel et al. analyzed the task relations for multi-task learning on ten NLP tasks. Alonzo et al. analyzed the effect of multi-task learning on several sequence labeling tasks. Different from these previous studies,w we focus on the datasets from the biomedical domain. These datasets have unique characteristics and we claim that findings on the general domain datasets might not be directly applicable to them.",133
" Aspect-based sentiment analysis  is a fine-grained sentiment analysis task, which analyzes the sentiment or opinions toward a given aspect in a sentence. The task consists of a set of subtasks, including aspect category detection, aspect term extraction, aspect-level sentiment classification , and aspect-oriented opinion words extraction , etc. Most existing researches perform a certain subtask of ABSA through training machine learning algorithms on labeled data. However, the public corpora of ABSA are all small-scale due to the expensive and labor-intensive manual annotation. Scarce training data limits the performance of data-driven approaches for ABSA. Therefore, an interesting and valuable research question is how to mine and exploit internal connections between ABSA subtasks to achieve the goal of facilitating them simultaneously. In this work, we focus on two subtasks ALSC and AOWE because they are highly mutually indicative. We first introduce them briefly before presenting our motivations.    Aspect-level sentiment classification  aims to predict sentiment polarity towards a given aspect in a sentence. As Figure shows, there are two aspects mentioned in the sentence ``waiters are unfriendly but the pasta is out of this world.'', namely ``waiters'' and ``pasta''. The sentiments expressed towards each aspect are negative and positive respectively. Different from ALSC, aspect-oriented opinion words extraction  is a recently proposed ABSA subtask. The objective of this task is to extract the corresponding opinion words towards a given aspect from the sentence. Opinion words refer to the word/phrase of a sentence used to express attitudes or opinions explicitly. In the example above, ``unfriendly'' is the opinion word towards the aspect ``waiters'', and ``out of this world'' is the opinion words towards the aspect ``pasta''.  It is a common sense that positive opinion words imply positive sentiment polarity, while negative opinion words correspond to negative sentiment polarity. Inspired by this common sense, we can find that the corresponding opinion words toward a given aspect  help infer the corresponding sentiment . Correspondingly, the sentiment determined in ALSC also can provide some clues to help extract polarity-related opinion words for the AOWE task. Therefore, the goals of AOWE and ALSC are mutually indicative and they can benefit each other.  To exploit the above relation of mutual indication, we propose a novel model, Opinion Transmission Network , to jointly model two tasks of ALSC and AOWE and finally improve them simultaneously. Overall, OTN contains two base modules, namely the attention-based ALSC module and the CNN-based AOWE module, and two tailor-made opinion transmission mechanisms, respectively from AOWE to ALSC and ALSC to AOWE. Specifically, we utilize the extracted results of AOWE as complementary opinions information and inject them into the ALSC module in the form of additional attention. To successfully transmit implicit opinions from ALSC to AOWE, we unearth that the features in attention layer of the ALSC module keep abundant useful aspect-related opinions, which can be utilized to facilitate AOWE. It is worth noting that our proposed model works without requiring simultaneous annotations of AOWE and ALSC on the same data, thus it can be applied in more practical scenarios.  The main contributions of this work can be summarized as follows:       Most recent ALSC research utilizes the attention-based networks to capture the latent sentiment clues from the sentence for the given aspect, such as ATAE-LSTM, IAN and PBAN, etc. On this basis,  employs memory network to conduct multi-hop attention to obtain more powerful sentiment clues for detecting the sentiment polarity of the aspect. Following the idea, memory-based methods achieve competitive performance on the ALSC. In addition, CNN, capsule network, and additional document sentiment data are also applied for this task.    AOWE is a relatively new ABSA subtask. formalizes it as an aspect-oriented sequence labeling task, and designs a state-of-the-art sequence labeling model based on LSTMs. Before them, a few works focus on the pair of aspect and opinion words. proposes a rule-mining method to extract aspect words and regards the nearest adjective of aspect as the corresponding opinion words. uses dependency-tree templates to extract valid aspect-opinion pairs.     In this work, we proposed combining transfer learning and multi-task learning for BioNER, which is not done before to the best of our knowledge. The proposed method achieved state-of-the-art results on several biomedical named entity datasets. The main purpose of this study was to analyze and understand under which conditions transferring information from an auxiliary dataset helps improve performance on a target dataset. To this end, we used various dataset measures and evaluated their ability to predict the MTL gains using three different evaluation metrics. The analysis showed that the dataset measures contain strong signals about the benefits of multi-task learning.  
","  Most recent ALSC research utilizes the attention-based networks to capture the latent sentiment clues from the sentence for the given aspect, such as ATAE-LSTM, IAN and PBAN, etc. On this basis,  employs memory network to conduct multi-hop attention to obtain more powerful sentiment clues for detecting the sentiment polarity of the aspect. Following the idea, memory-based methods achieve competitive performance on the ALSC. In addition, CNN, capsule network, and additional document sentiment data are also applied for this task.    AOWE is a relatively new ABSA subtask. formalizes it as an aspect-oriented sequence labeling task, and designs a state-of-the-art sequence labeling model based on LSTMs. Before them, a few works focus on the pair of aspect and opinion words. proposes a rule-mining method to extract aspect words and regards the nearest adjective of aspect as the corresponding opinion words. uses dependency-tree templates to extract valid aspect-opinion pairs.",134
" With the development of large-scale pre-trained Language Models  such as BERT , XLNet , and T5 , tremendous progress has been made in Question Answering . Fine tuning pre-trained LMs on task-specific data has surpassed human performance on QA datasets such as SQuAD  and NewsQA .  Nevertheless, most existing QA systems largely deal with factoid questions and assume a simplified setup such as multiple-choice questions, retrieving spans of text from given documents, and filling in the blanks. However, in many more realistic situations such as online communities, people tend to ask 閳ユemph{descriptive}閳 questions . Answering such questions requires the identification, linking, and integration of relevant information scattered over long-form multiple documents for the generation of free-form answers.  We are particularly interested in developing a QA system for questions from e-shopping communities using customer reviews. Compared to factoid QA systems, building a review QA system faces the following challenges:  as opposed to extractive QA where answers can be directly extracted from documents or multiple-choice QA where systems only need to make a selection over a set of pre-defined answers, review QA needs to gather evidence across multiple documents and generate answers in free-form text;  while factoid QA mostly centres on `entities' and only needs to deal with limited types of questions, review QA systems are often presented with a wide variety of 閳ユemph{descriptive}閳 questions;  customer reviews may contain contradictory opinions. Review QA systems need to automatically identify the most prominent opinion given a question for answer generation.    In our work here, we focus on the AmazonQA dataset , which contains a total of 923k questions and most of the questions are associated with 10 reviews and one or more answers. We propose a novel Cross-passage Hierarchical Memory Network named Chime to address the aforementioned challenges. Regular neural QA models search answers by interactively comparing the question and supporting text, which is in line with human cognition in solving factoid questions . While for opinion questions, the cognition process is deeper: reading larger scale and more complex texts, building cross-text comprehension, continually refine the opinions, and finally form  the answers . Therefore, Chime is designed to maintain hierarchical dual memories to closely simulates this cognition process. In this model, a context memory dynamically collect cross-passage evidences, an answer memory stores and continually refines answers generated as Chime reads supporting text in a sequential manner. Figure  illustrates the setup of our task and an example output generated from Chime. The top box shows a question extracted from our test set while the left panel and the right upper panel show the related 10 reviews and the paired 4 actual answers. We can observe that the question can be decomposed into complex sub-questions and both reviews and answers contain contradictory information. However, Chime can deal with such information effectively and generate appropriate answers as shown in the right-bottom box.  In summary, we have made the following contributions:     %%%%%%%%%%%%%%%% % Related Work % %%%%%%%%%%%%%%%%   Our work is related to the following three lines of research:  \paragraph{Opinion/Review Question-Answering} In Opinion or Review QA, questions may concern about finding subjective personal experiences or opinions of certain products and services. The Amazon QA dataset was first released in  which contains 1.4 million questions  and 13 million reviews on 191 thousand products collected from Amazon product pages. They developed a Mixture of Expert  model which automatically detects whether a review of a product is relevant to a given query. In their subsequent work, Wan and McAuley \shortcite{wan2016modeling} noticed that users tend to ask for subjective information and answers might also be highly subjective and possibly contradictory. They, therefore, built a new dataset with 800 thousand questions and over 3 million answers from Amazon, in which each question is paired with multiple answers, and extended their previous MoE model with subjective information such as review rating scores and reviewer's bias incorporated. But they found that subjective information is only effective in predicting `yes/no' answers to binary questions and does not help in distinguishing `true閳 answers from alternatives in open-ended 'descriptive' questions. More recently, Yu and Lam \shortcite{yu2018aware} only focused on the yes/no questions in the Amazon QA dataset  and trained a binary answer prediction model by leveraging latent aspect-specific representations of both questions and reviews learned by an autoencoder. Gao et al. \shortcite{gao2019product} focused on factual QA in e-commerce and proposed a Product-Aware Answer Generator that combines reviews and product attributes for answer generation, and uses a discriminator to determine whether the generated answer contains question-related facts. Xu et al. \shortcite{xu2019bert} proposed an extractive review-based QA task and manually created just over 2,500 questions and annotated the corresponding answer spans in less than 1,000 reviews relating to laptops and restaurants from the review data of SemEval 2016 Task 5\footnote{}. They first jointly fine-tuned BERT for answer span detection, aspect extraction and aspect sentiment classification on the SemEval 2016 Task 5 data, and then post-trained BERT on over 3 million unlabelled Amazon and Yelp reviews in order to fuse domain knowledge, and also on SQuAD 1.1  in order to gain task-relevant but out-of-domain knowledge. Gupta et al. \shortcite{gupta2019amazonqa} created a subset from the Amazon QA product review dataset , consisting of 923k questions with 3.6M answers and 14M reviews on 156k Amazon products. They trained an answerability classifier from 3,297 question-context pairs labeled by Mechanical Turk and used it to classify answerability for the whole dataset. They then converted the dataset into a span-based format by heuristically creating an answer span from reviews that best answers a question based on users' actual answers, and trained R-Net , which uses a gated self-attention mechanism and pointer networks, to predict answer boundaries. There are few studies using generative models to deal with opinion/review-based QA.  \paragraph{Multi-passage QA}  There are mainly two types of methods for multi-passage QA. One is to use retrieval-based methods to first identify text passages that are most likely to contain answer information, and then perform QA on the extracted text passages which are essentially considered as a single passage. The other one is to separately run single-passage QA over each passage, obtaining multiple answer candidates, and then determine the best answer through mutual verification among the answers.  Examples in the first type of methods include S-NET , Multi-passage BERT , and Masque . These models require supporting text passages to be explicitly annotated. S-NET  follows an extraction-then-synthesis framework. First, relevant passages are extracted from context using a variant of R-NET , which learns to rank passages and extract the most possible evidence span from the selective passage; then, the evidence-notated selective passage is used for the GRU decoder synthesizing answers. In Multi-passage BERT , two independent BERTs were used to perform multi-passage QA. One BERT takes the question and a text passage as input and then uses the hidden states of the CLS token to train a classifier to determine if the text passage is relevant to the given question. The other BERT is used for extracting candidate answers from relevant text passages. The Masque model  is a generative reading comprehension approach based on multi-source abstractive summarization. Masque uses a joint-learning framework, comprising of a question answerability classifier, a passage ranker, and an answer generator. At each step of answer generation, the decoder chooses a word from the mixture of three distributions derived from a vocabulary, from the question and associated multiple passages.  A representative example of the second type of methods is V-Net . The main assumption of V-Net is that correct answers often appear in multiple documents with high frequency and similarity, and wrong answers are usually different from each other. Therefore, V-Net builds a mutual verification mechanism between all answer candidates, which are separately extracted from different passages, to select the best final answer.   Most existing approaches require explicit annotations of supporting text passages in order to train multi-passage QA models in a supervised way. In our setup here, supporting review passages to a question was unsupervised ranked by BM25, which may introduce noises to QA model training and poses a more significant challenge.  \paragraph{Memory Network}  Memory network has been first proposed to model the relation between a story and a query for QA systems . Apart from its application in QA, memory networks have also achieved great successes in other NLP tasks, such as machine translation , sentiment analysis , visual question answering , social networks , and summarization . The main idea of memory networks is to use the attention mechanism to assign different weights to text passages so as to identify the most relevant passages for answer generation . Kumar et al.   proposed a gated memory network to represent facts in different iterations during the learning process to verify the potentially related passages to generate an answer. Gui et al. \shortcite{gui2017question} used a convolutional architecture to capture attention signals in memory networks.  Xu et al. leveraged the memory network as an information retrieval system to search possible entities in knowledge bases for complex questions. Chen et al. used the memory network to verify items in knowledge bases as passages and then generate answers. Generally speaking, existing memory-network-based QA methods mainly focus on using memory networks to weigh and derive representations of question-aware text passages and knowledge entities for answer generation. We instead explore a novel structure of a hierarchical memory network composing of both context and answer memories for better capturing review context and generating more appropriate answers.                CHIME               In ABSA research, Aspect-level sentiment classification  and aspect-oriented opinion words extraction  are two highly relevant tasks. Previous works usually focus on one of the two tasks and neglect mutual indication between them. In this paper, we propose a novel joint model, Opinion Transmission Network , to exploit the potential connection between ALSC and AOWE to benefit them simultaneously. In OTN, two tailor-made opinion transmission mechanisms are designed to control opinion clues to flow respectively from ALSC to AOWE and AOWE to ALSC. Experiment results on two tasks validate the effectiveness of our method.   \subsubsection{Acknowledgements.} This work was supported by the NSFC  and National Key R\&D Program of China .      ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.   
"," Our work is related to the following three lines of research:  \paragraph{Opinion/Review Question-Answering} In Opinion or Review QA, questions may concern about finding subjective personal experiences or opinions of certain products and services. The Amazon QA dataset was first released in  which contains 1.4 million questions  and 13 million reviews on 191 thousand products collected from Amazon product pages. They developed a Mixture of Expert  model which automatically detects whether a review of a product is relevant to a given query. In their subsequent work, Wan and McAuley \shortcite{wan2016modeling} noticed that users tend to ask for subjective information and answers might also be highly subjective and possibly contradictory. They, therefore, built a new dataset with 800 thousand questions and over 3 million answers from Amazon, in which each question is paired with multiple answers, and extended their previous MoE model with subjective information such as review rating scores and reviewer's bias incorporated. But they found that subjective information is only effective in predicting `yes/no' answers to binary questions and does not help in distinguishing `true闁 answers from alternatives in open-ended 'descriptive' questions. More recently, Yu and Lam \shortcite{yu2018aware} only focused on the yes/no questions in the Amazon QA dataset  and trained a binary answer prediction model by leveraging latent aspect-specific representations of both questions and reviews learned by an autoencoder. Gao et al. \shortcite{gao2019product} focused on factual QA in e-commerce and proposed a Product-Aware Answer Generator that combines reviews and product attributes for answer generation, and uses a discriminator to determine whether the generated answer contains question-related facts. Xu et al. \shortcite{xu2019bert} proposed an extractive review-based QA task and manually created just over 2,500 questions and annotated the corresponding answer spans in less than 1,000 reviews relating to laptops and restaurants from the review data of SemEval 2016 Task 5\footnote{}. They first jointly fine-tuned BERT for answer span detection, aspect extraction and aspect sentiment classification on the SemEval 2016 Task 5 data, and then post-trained BERT on over 3 million unlabelled Amazon and Yelp reviews in order to fuse domain knowledge, and also on SQuAD 1.1  in order to gain task-relevant but out-of-domain knowledge. Gupta et al. \shortcite{gupta2019amazonqa} created a subset from the Amazon QA product review dataset , consisting of 923k questions with 3.6M answers and 14M reviews on 156k Amazon products. They trained an answerability classifier from 3,297 question-context pairs labeled by Mechanical Turk and used it to classify answerability for the whole dataset. They then converted the dataset into a span-based format by heuristically creating an answer span from reviews that best answers a question based on users' actual answers, and trained R-Net , which uses a gated self-attention mechanism and pointer networks, to predict answer boundaries. There are few studies using generative models to deal with opinion/review-based QA.  \paragraph{Multi-passage QA}  There are mainly two types of methods for multi-passage QA. One is to use retrieval-based methods to first identify text passages that are most likely to contain answer information, and then perform QA on the extracted text passages which are essentially considered as a single passage. The other one is to separately run single-passage QA over each passage, obtaining multiple answer candidates, and then determine the best answer through mutual verification among the answers.  Examples in the first type of methods include S-NET , Multi-passage BERT , and Masque . These models require supporting text passages to be explicitly annotated. S-NET  follows an extraction-then-synthesis framework. First, relevant passages are extracted from context using a variant of R-NET , which learns to rank passages and extract the most possible evidence span from the selective passage; then, the evidence-notated selective passage is used for the GRU decoder synthesizing answers. In Multi-passage BERT , two independent BERTs were used to perform multi-passage QA. One BERT takes the question and a text passage as input and then uses the hidden states of the CLS token to train a classifier to determine if the text passage is relevant to the given question. The other BERT is used for extracting candidate answers from relevant text passages. The Masque model  is a generative reading comprehension approach based on multi-source abstractive summarization. Masque uses a joint-learning framework, comprising of a question answerability classifier, a passage ranker, and an answer generator. At each step of answer generation, the decoder chooses a word from the mixture of three distributions derived from a vocabulary, from the question and associated multiple passages.  A representative example of the second type of methods is V-Net . The main assumption of V-Net is that correct answers often appear in multiple documents with high frequency and similarity, and wrong answers are usually different from each other. Therefore, V-Net builds a mutual verification mechanism between all answer candidates, which are separately extracted from different passages, to select the best final answer.   Most existing approaches require explicit annotations of supporting text passages in order to train multi-passage QA models in a supervised way. In our setup here, supporting review passages to a question was unsupervised ranked by BM25, which may introduce noises to QA model training and poses a more significant challenge.  \paragraph{Memory Network}  Memory network has been first proposed to model the relation between a story and a query for QA systems . Apart from its application in QA, memory networks have also achieved great successes in other NLP tasks, such as machine translation , sentiment analysis , visual question answering , social networks , and summarization . The main idea of memory networks is to use the attention mechanism to assign different weights to text passages so as to identify the most relevant passages for answer generation . Kumar et al.   proposed a gated memory network to represent facts in different iterations during the learning process to verify the potentially related passages to generate an answer. Gui et al. \shortcite{gui2017question} used a convolutional architecture to capture attention signals in memory networks.  Xu et al. leveraged the memory network as an information retrieval system to search possible entities in knowledge bases for complex questions. Chen et al. used the memory network to verify items in knowledge bases as passages and then generate answers. Generally speaking, existing memory-network-based QA methods mainly focus on using memory networks to weigh and derive representations of question-aware text passages and knowledge entities for answer generation. We instead explore a novel structure of a hierarchical memory network composing of both context and answer memories for better capturing review context and generating more appropriate answers.                CHIME",135
"  .     %      % % final paper: en-us version      %        % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. }  %1Yang 閺堫剚顔屽楦款唴婵″倷绗呴幓蹇氬牚閿涙瓊MT閸欐牕绶辨禍鍝燨TA閻ㄥ嫬鐤勬灞炬櫏閺嬫粌鑻熷妤鍩屾禍鍡楃畭濞夋稓娈戞惔鏃傛暏閿涘牐顔戦弬鍥х穿閻㈩煉绱氶妴鍌滄暠娴滃孩婀佹径褔鍣洪惃鍕棘閺佸府绱濋幍娴狀檾MT闂囩憰浣搞亣闁插繒娈戠拋顓犵矊閺佺増宓侀弬鐟板讲閸忓懎鍨庨崣鎴炲皩鐎瑰啰娈戞导妯哄◢閵嗗倻鍔ч懓灞芥躬鐎圭偤妾惔鏃傛暏娑擃叏绱濋弫鐗堝祦瀵板娴兼艾鍨庣敮鍐х瑝閸у浄绱濋幋鏍懏绉归崣濠傚煂妫板棗鐓欓懛顏堝倸绨查惃鍕６妫版ǜ鍌氭躬鏉╂瑧顫掗幆鍛枌娑撳绱漀MT濡崇峰瀵版导姘朵粣韫囨ê鍑＄涳箑鍩岄惃鍕叀鐠囧棴绱濋懓灞藉箵閹风喎鎮庨弬鐗堝潑閸旂姷娈戦弫鐗堝祦閿涘瞼鍔ч懓灞炬付缂佸牆绶遍崚鎵畱濡崇风憰浣割槱閻炲棗宓堥弰顖氬弿閸掑棗绔烽惃鍕殶閹诡噯绱濇潻娆戭潚閻滄媽钖勭亸杈ㄦЦ鏉╃偟鐢荤涳缚绡勬稉顓犳畱閻忛箖姣﹂幀褔浠愯箛姗堢礄瀵洜鏁ら惄绋垮彠閺傚洨灏為敍澶堝倹寮挎潻鏉挎禈1閻ㄥ嫬鐤勬宀娲伴惃鍕簰閸欏﹦绮ㄧ拋鐑樻降妤犲矁鐦夐幋鎴滄粦娑撳﹪娼伴惃鍕鏉╁府绱欓崶鍓у娑撳秴顧勫〒鍛珰閿涘苯鎸ㄩ崗鑸垫Ц閺傚洤鐡ч敍宀鏃辨潪瀵告畱閳ユ窂LEU閳ユ粏顕柈鐣屾暏婢堆冨晸閿涘 Neural machine translation  models have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available.  In this situation, continual training, which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations  in the in-domain data but forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure shows the performance trends on the in-domain and general-domain. %As the size of the training corpus grows, the NMT model is trained in the manner of continual learning  from a stream of data.  %Unfortunately, there usually exists a distribution bias in large data set especially when the data is collected from different domains. In this situation, the NMT model has a tendency towards over-fitting to frequent observations  in the newly added data, but forgetting previously learned patterns from the old data, leading to poor performance in the old data.  In the example of domain adaptation shown in Figure, as the training goes, the performance surges for in-domain while slides fast for general-domain. This phenomenon is the catastrophic forgetting of neural network.  %when there are large amounts of parallel training sentences available. However, similar to many other successful neural network-based methods, it also has limited continual learning ability to learn from a stream of training data, which could have different distributions . It is because the NMT system suffers from catastrophic forgetting which refers to that model has a tendency towards over-fitting to frequent observations  in newly added training data, but forgetting previously learned features in the old data.   %Figure denotes this phenomenon in NMT.  %1Yang 娑撳娼版潻娆愵唽閸樼粯甯 %Improving the continual learning ability of the NMT system is of significant importance both in theory and practice. From the artificial intelligence perspective, it can be seen as another step towards the grand goal of creating a real intelligent translation system that can learn continuously new translation skills without forgetting old knowledge as a human does. From a practical perspective, it enables the model to update the model with only recent new data to improve the model's overall performance. We don't need to retrain the model from scratch which is very time-consuming. Moreover, considering that a well-trained model maybe is already deployed in an application, the original training data may not be available at that time. Therefore it is very necessary to improve the continual learning ability of the NMT system.      %1Yang 閺堫剚顔屽楦款唴婵″倹妲搁幓蹇氬牚閿涙氨浼ㄩ梾鐐囦粣韫囨ü绔撮惄瀛樻Ц缁佺偟绮＄純鎴犵捕鐠侇厾绮屾稉顓犳畱娑撴径褔姣︽０姗堢礉閾忕晫鍔ч惄顔煎瀹歌尙绮￠張澶夌娴滄稑浼愭担婊嗗毀閸旀稐绨憴锝呭枀鏉╂瑤閲滈梻顕顣介敍灞借嫙缂佹瑥鍤禍鍡曠娴滄稖顢戞稊瀣箒閺佸牏娈戠憴锝呭枀閺傝纭堕敍鍫濈穿閻€劎娴夐崗铏瀮閻氼噯绱氶敍灞肩稻閺勵垳娲伴崜宥呰嫙濞屸剝婀佸銉ょ稊閸樼粯甯扮槐銏犳躬閻忛箖姣﹂幀褔浠愯箛妯跨箖缁嬪鑵戦崘鍛村劥閺佺増宓侀惃鍕綁閸栨牗鍎忛崘纰夌礉鏉╂瑧顫掗幒銏㈠偍娴兼碍婀侀崝鈺绨幋鎴滄粦閻炲棜袙閻忛箖姣﹂幀褔浠愯箛妯哄絺閻㈢喓娈戦崢鐔锋礈楠炲爼鍣伴崣鏍祲鎼存梻娈戦幒顏呮煢閵  Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning.  ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains.   introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved.  , , and  propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain. All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don't know what happened inside the model during continual training and why these methods can alleviate the catastrophic forgetting problem. The study on these can help to understand the working mechanism of continual training and inspire more effective solutions to the problem in return.    %Catastrophic forgetting is a long-known problem in the training of neural networks. Some researchers have managed to alleviate this problem with different strategies, such as changing the model structure, adding an extra regularization term, employing complementary learning systems  theory-based strategies and so on. However, to the best of our knowledge, these methods mainly focus on how to solve the problem, not what causes the problem. %Understanding the cause of the problem will inspire effective solutions. %, there is still no work trying to figure out the inner reason for catastrophic phenomenon and no direct evidence to show the change of model parameters in NMT. We believe that the attempt to understand this phenomenon can help us adopt appropriate measures to solve this problem.  %it is still not clear what happens during the continual learning process and what causes catastrophic forgetting indeed.  %1Yang 閺堫剚顔岄崣顖欎簰鏉╂瑦鐗遍崘娆欑窗閸︺劍婀伴弬鍥风礉閹存垳婊戠亸婵婄槸閸︺劑顣崺鐔诲殰闁倸绨查惃鍕攱閺嬫湹绗呴崢缁樺赴缁鳖晼arameters閸滃瞼浼ㄩ梾鐐囦粣韫囨娈戦崗宕囬兇閿涘苯鑻熼崚鑽ゆ暰閸戠皢arameters閸︺劎浼ㄩ梾鐐囦粣韫囨ǹ绻冪粙瀣╄厬閻ㄥ嫬褰夐崠鏍Ъ閸旇￥鍌欒礋娴滃棜鎻崚鎷岀箹娑擃亞娲伴惃鍕剁礉閹存垳婊戦柅姘崇箖Absolute value閸滃瓗IM閺夈儴鐦庢导鏉垮棘閺佹澘婀Ο鈥崇风拋顓犵矊娑擃厾娈戦柌宥堫洣閹嶇礄閸欏倽鍐╂瀮閻氼噯绱氶敍灞借嫙闁俺绻冮崣鍌涙殶閹匡箓娅庨惃鍕煙濞夋洘娼甸幒銏㈠偍鏉╂瑤绨洪崡鏇熸殶鐎靛湱鐐曠拠鎴炑嗗厴閻ㄥ嫬濂栭崫宥冨倿姘崇箖鐎圭偤鐛欑紒鎾寸亯閿涘本鍨滄禒顒褰傞悳鏉款嚠娴滃酣姘辨暏妫板棗鐓欓柌宥堫洣閻ㄥ嫬寮弫鏉款嚠娴滃穼n-domain娴犲秶鍔у鍫ュ櫢鐟曚緤绱濋懓灞芥躬妫板棗鐓欓懛顏堝倸绨查惃鍕箖缁嬪鑵戞潻娆庣昂閸欏倹鏆熼惃鍕綁閸栨牕绶㈡径褋鍌氱唨娴滃氦绻栨禍娑樺絺閻滃府绱濈电懓绨叉禍搴ょ槑娴兼澘寮弫浼村櫢鐟曚焦褏娈戞稉銈囶潚閺傝纭堕敍灞惧灉娴狀剛娴夋惔鏃傛畱閹绘劕鍤稉銈囶潚閺傝纭堕弶銉﹀付閸掓儼绻栨禍娑㈠櫢鐟曚胶娈戦崣鍌涙殶閸︺劑顣崺鐔诲殰闁倸绨查惃鍕箖缁嬪鑵戞稉宥勭窗閸欐ê瀵叉潻鍥с亣閿涘矁宀娼冮柌宥勭艾鐠嬪啯鏆ｉ柇锝勭昂娑撳秹鍋呮稊鍫ュ櫢鐟曚胶娈戦崣鍌涙殶閵嗗倸鐤勬宀绮ㄩ弸婊嗐冮弰搴㈠灉娴狀剛娈戦弬瑙勭《閼宠棄婀穱婵婄槈in-domain閺佺増宓佹稉濠勬畱缂堟槒鐦ч幀褑鍏橀崣妯哄娑撳秵妲戦弰鍓ф畱閹懎鍠屾稉瀣亣楠炲懎瀹抽惃鍕絹妤傛﹢姘辨暏妫板棗鐓欓惃鍕倳鐠囨垶褑鍏橀妴  %Given this, we seek to understand the relationship between catastrophic forgetting phenomenon and model parameters under the task of domain adaptation. More specifically, we aim to figure out the trend of model parameters during catastrophic forgetting. To fulfill this goal, we propose two methods to evaluate the importance of the model parameters. The first is to use the absolute value of model parameters and the second is to use the empirical Fisher Information Matrix . To verify the effectiveness and correctness of the proposed methods, we then do parameter erasure experiments. According to the experimental results, we find that some parameters are important for both the general-domain and in-domain. Based on these findings, we try to alleviate catastrophic forgetting by designing learning strategies based on the importance of the parameters. We put more constrains on those important parameters to make them change more conservatively while encourage those less important parameters to change more aggressively during the continual learning process. The experiments on multiple translation tasks show that our methods can improve the translation quality on the new domain without degrading the performance on the old domain too much.  Given above, in this paper, we focus on the catastrophic forgetting phenomenon and investigate the roles of different model parts during continual training. To this end, we explore the model from the granularities of modules and parameters . In the module analyzing experiments, we operate the model in two different ways, by freezing one particular module or freezing the whole model except for this module. We find that different modules preserve knowledge for different domains. In the parameter analyzing experiments, we erase parameters according to their importance which is evaluated by the Taylor expansion-based method . According to the experimental results, we find that some parameters are important for both of the general-domain and in-domain and meanwhile they change greatly during domain adaptation which may result in catastrophic forgetting.  To ensure the validity and reliability of the findings, we conduct experiments over different language pairs and domains.   \iffalse Given this, we step into the catastrophic forgetting phenomenon by investigating the influence of different model parts from different granularities, depicting the different roles played by them during continual training. Inspired by the work of  and , we conducted two kinds of analyzing experiments. The first, focusing on the macro parts of the model, is the module analyzing experiment, where we freeze the target module of the model or freeze the whole model except for the target module during continual training to study the influence of each module on the translation performance. We found that some modules are of higher capacity to preserve the general-domain knowledge while some modules are more essential for adapting to the in-domain.  The second, focusing on the micro parts of the model is the parameter analyzing experiment based on the parameter importance, where the Taylor expansion-based method is adopted as the importance evaluation criterion.  According to the experimental results, we found that some parameters are important for both of the general-domain and in-domain and meanwhile they fluctuate greatly during domain adaptation which may result in performance slipping. To ensure the validity and reliability of our conclusions, we conducted our experiments across different language pairs and domains.  \fi  Our main contributions are summarized as follows:   \iffalse To answer these questions, we put forward two ways of evaluating the importance of the model parameters. The first is to use the absolute value of model parameters and a larger absolute value stands for they are more important for the model. Inspired by the work of, we use the diagonal of the Fisher information matrix  of the model parameters to evaluate the importance. To verify the effectiveness and correctness of the proposed methods, we then did parameter erasure experiments which is an effective analysis approach. The results show that some model parameters are more important than others and have much more impact on the final translation quality.  this phenomenon by analyzing the change of model parameters during the continual learning process. We focus on the domain adaptation task of NMT under the continual learning scenario which means first we make the model well-trained using large amounts of general-domain data, and then this model is further trained using limited amounts of in-domain data which is from another domain. It should be noted that no general-domain data is available during the further trained process which is a common practice of continual learning. We aim to investigate the following questions:    Based on our findings of parameter importance above, we then investigate their changes during the continual learning process. We find that the important parameters for the general-domain translation still play major roles for the in-domain translation by doing another parameter erasure experiments. What's more, the substantial decline of general-domain translation quality and the rise of in-domain translation quality is also due to the change of these parameters.   Finally, based on our findings, we propose some practical methods to overcome the catastrophic forgetting phenomenon by parameter regularization method and learning rate adjustment method based on their importance to the model. We change the important parameters slightly while changing the less important parameters more aggressively. The results show that our approach can alleviate catastrophic forgetting significantly.      Our work indicates that some parameters are more important than others and the change of these parameters can influence translation results a lot. Therefore, we can try to alleviate catastrophic forgetting by designing different learning strategies based on the importance of the parameters. As far as we know, this is the first work trying to analyze the catastrophic forgetting phenomenon in NMT. Moreover, the analyzing methods we put forward in this work are task-independent and can be applied to other neural network-based methods in other tasks. \fi \iffalse extra space to store all the old training data or even retrain from scratch with the and without storing old training data or even retraining with   This work focuses on the domain adaptation problem of NMT which is a special case of the continual learning scenario of the neural network. They share the same training task but the distribution of the training data is different.  Domain adaptation deals with the problem of improving the performance of a model trained on a general domain data over test instances from a new domain. In such a scenario, we usually have large amounts of general-domain training data and a welled trained model based on it. In contrast, we only have a limited number of in-domain training data which will lead the NMT system to overfit soon and perform poorly when only trained with these data. Some researchers solve this problem by combining the training data from the general-domain and in-domain together and train a new system from scratch. They usually make use of the domain information to improve the translating performance by adding domain labels to training data or using domain discriminator to find the domain invariant features. On the one hand, these methods are very time consuming and need extra space to store all the training data which is not efficient in real-life applications. On the other hand, due to the relatively small size of in-domain data, it will lead the model to overfit the general-domain data which has been observed in the results.   Fine-tuning is a fast and efficient method for continual learning of neural networks which has already been applied for NMT. NMT system is first trained on general-domain data and then further trained on in-domain data.   Domain adaptation is the most common application scenario of continual learning in NMT which has drawn much attention recently. Under this scenario, we   The translation quality drops quickly when the distribution of the training data changes. It suffers a catastrophic forgetting in the continual training process. \fi        1Yang 鐠囬鈥樻穱婵囩梾閺堝浠愬⿻蹇ョ礉楠炶泛濮炴稉濠冩付閺傛澘浼愭担 Analyzing Work Recently, much work has been concerned with analyzing and evaluating the NMT model from different perspectives.  investigates how NMT models output target strings of appropriate lengths.  analyzes the contribution of each contextual word to arbitrary hidden states.  analyzes when the pre-trained word embeddings can help in NMT tasks.  analyzes the importance of different attention heads.  investigates the importance and function of different neurons in NMT.   finds that a large proportion of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity.  links the exposure bias problem to the phenomenon of NMT tends to generate hallucinations under domain shift.  finds that the NMT tends to generate more high-frequency tokens and less low-frequency tokens than reference. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training.  In this sense, the work of~ is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron  level.   every aspect: individual neurons, words embedding, attention heads, hidden representations, etc.   Much of these works try to predict linguistic properties from the features generated by the neural network, such as sentence length, morphology, and syntax.   On the whole, these work try to understand neural networks at the representation or the neuron level.   These work may show us how correlated are neural network modules with linguistic properties but can't   In contrast, we try to understand the NMT system at a more fundamental level by analyzing the behavior of model parameters.   By doing so, we can better investigate how the change of the model affects the system out dynamically.   To the best of knowledge, this idea is the first time to be applied to the NMT system.   Continual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task.   The biggest challenge for this kind of work is the catastrophic forgetting problem.   fine tunes the general-domain model with the in-domain data.   fine tunes the model with the mix of the general-domain data and over-sampled in-domain data.  and  add regularization terms to let the model parameters stay close to their original values.  minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model.  adds a discriminator to help preserve the domain-shared features and fine tunes the whole model on the mixed training data.   proposes to obtain the word representations by mixing their embedding in individual domains based on the domain proportions.  presents a theoretical analysis of catastrophic forgetting in the Neural Tangent Kernel regime. Compared with them, our work pays attention to exploring the inner change of the model during continual training as well as the cause of the catastrophic forgetting phenomenon.  The ability to continually learn over time by accommodating new knowledge while retaining previously learned experiences is referred to as continual learning or life-long learning.   It has been seen as a long-standing challenge and an important step towards true artificial intelligence.   Catastrophic forgetting is the main problem in this field.  front of this goal.   ~ studied the relationship between the catastrophic forgetting and properties of task sequences. Our work focus on investigating the inner reason for catastrophic forgetting.   It showed that task complexity should be considered when designing new continual learning algorithms. More work needs to be done to understand catastrophic forgetting further.    Domain Adaptation Catastrophic forgetting is also a common challenge for domain adaptation task. Most of these work assume that the general-domain data is still accessible during the domain adaptation process, which is not the case of this paper.     tries to solve this problem by multi-objective learning based on the knowledge distillation framework.  adapts Elastic Weight Consolidation  method to avoid performance degradation in the general-domain. This work shares the same motivation with ours but we have shown more direct evidence and thorough analysis to support our idea.  They find that it is useful to put more constraints on the important parameters for the general-domain which is similar to our final findings. And we will show more evidence and analysis to support this idea.       We have presented a first empirical study of practical concerns of targeted attacks on black-box NMT system driven by parallel data poisoning. We evaluated scenarios of poisoning the from-scratch training, pre-training, and fine-tuning of NMT systems trained on parallel data. We show that with very small poisoning budgets , systems can be severely compromised, even when they are trained on tens of millions of clean samples. We hope to raise the awareness of the risk of training NMT systems with malicious inputs from untrusted sources. As our end goal is an effective defence, one of our next steps is to look into developing countermeasures to this attack, such as designing algorithms for more robust parallel data filtering, as well as for detecting and protecting the named entities under attack.    \paragraph{\bf Ethical Considerations} Our aim in this work is to identify and mitigate potential threats to NMT systems, by adopting established threat modelling for machine learning systems, to identify and prioritise need to devise effective defences and develop robust systems. Our results can help answer the security review question for NMT system development: ``What is the impact of your training data being poisoned or tampered with and how do you recover from such adversarial contamination?'' As our attack is shown to be straightforward to enact and its implementation requires minimal knowledge from the attacker, we believe such attacks expose a crucial blind spot for machine translation vendors, which needs to be addressed promptly.    
","  1Yang 閻犲洭顥撻垾妯荤┍濠靛洨姊鹃柡鍫濐樀娴犳劕饪昏箛銉х妤犵偠娉涙慨鐐寸▔婵犲啯浠橀柡鍌涙緲娴兼劖鎷 Analyzing Work Recently, much work has been concerned with analyzing and evaluating the NMT model from different perspectives.  investigates how NMT models output target strings of appropriate lengths.  analyzes the contribution of each contextual word to arbitrary hidden states.  analyzes when the pre-trained word embeddings can help in NMT tasks.  analyzes the importance of different attention heads.  investigates the importance and function of different neurons in NMT.   finds that a large proportion of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity.  links the exposure bias problem to the phenomenon of NMT tends to generate hallucinations under domain shift.  finds that the NMT tends to generate more high-frequency tokens and less low-frequency tokens than reference. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training.  In this sense, the work of~ is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron  level.   every aspect: individual neurons, words embedding, attention heads, hidden representations, etc.   Much of these works try to predict linguistic properties from the features generated by the neural network, such as sentence length, morphology, and syntax.   On the whole, these work try to understand neural networks at the representation or the neuron level.   These work may show us how correlated are neural network modules with linguistic properties but can't   In contrast, we try to understand the NMT system at a more fundamental level by analyzing the behavior of model parameters.   By doing so, we can better investigate how the change of the model affects the system out dynamically.   To the best of knowledge, this idea is the first time to be applied to the NMT system.   Continual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task.   The biggest challenge for this kind of work is the catastrophic forgetting problem.   fine tunes the general-domain model with the in-domain data.   fine tunes the model with the mix of the general-domain data and over-sampled in-domain data.  and  add regularization terms to let the model parameters stay close to their original values.  minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model.  adds a discriminator to help preserve the domain-shared features and fine tunes the whole model on the mixed training data.   proposes to obtain the word representations by mixing their embedding in individual domains based on the domain proportions.  presents a theoretical analysis of catastrophic forgetting in the Neural Tangent Kernel regime. Compared with them, our work pays attention to exploring the inner change of the model during continual training as well as the cause of the catastrophic forgetting phenomenon.  The ability to continually learn over time by accommodating new knowledge while retaining previously learned experiences is referred to as continual learning or life-long learning.   It has been seen as a long-standing challenge and an important step towards true artificial intelligence.   Catastrophic forgetting is the main problem in this field.  front of this goal.   ~ studied the relationship between the catastrophic forgetting and properties of task sequences. Our work focus on investigating the inner reason for catastrophic forgetting.   It showed that task complexity should be considered when designing new continual learning algorithms. More work needs to be done to understand catastrophic forgetting further.    Domain Adaptation Catastrophic forgetting is also a common challenge for domain adaptation task. Most of these work assume that the general-domain data is still accessible during the domain adaptation process, which is not the case of this paper.     tries to solve this problem by multi-objective learning based on the knowledge distillation framework.  adapts Elastic Weight Consolidation  method to avoid performance degradation in the general-domain. This work shares the same motivation with ours but we have shown more direct evidence and thorough analysis to support our idea.  They find that it is useful to put more constraints on the important parameters for the general-domain which is similar to our final findings. And we will show more evidence and analysis to support this idea.",136
"  Pre-trained contextualized language models such as BERT are state-of-the-art for a wide variety of natural language processing tasks. Similarly, in Information Retrieval , these models have brought about large improvements in the task of ad-hoc retrieval---ranking documents by their relevance to a textual query, where the models increasingly dominate the leaderboards of ad-hoc retrieval competitions.  Despite this success, little is understood about why pretrained language models are effective for ad-hoc ranking. What new aspects of the task do neural models solve that previous approaches do not?  Previous work has shown that traditional IR axioms, e.g. that increased term frequency should correspond to higher relevance, do not explain the behavior of recent neural models . Outside of IR, others have examined what characteristics contextualized language models learn in general , but it remains unclear if these qualities are valuable to the ad-hoc ranking task specifically. Thus, new approaches are necessary to characterize the models.  We propose a new framework aimed at Analyzing the Behavior of Neural IR ModeLs  based on three testing strategies: ``measure and match'', ``textual manipulation'', and ``dataset transfer''. The ``measure and match'' strategy, akin to the diagnostic tests proposed by~\citet{Rennings2019AnAA}, constructs test samples by controlling one measurement  and varying another  using samples from an existing IR collection. The ``textual manipulation'' strategy tests the effect that altering the document text has on ranking. The ``dataset transfer'' strategy constructs tests from non-IR datasets.  The new tests allow us to isolate model characteristics---such as sensitivity to word order, or preference for summarized rather than full documents---that are imperceptible using other approaches.  We also release an open-source implementation of our framework that makes it easy to define new diagnostics and to replicate the analysis on new models.  Using our new framework, we perform the first large-scale analysis of neural IR models. We compare today's leading ranking techniques, including those using BERT and T5, as well as methods focused on efficiency like DocT5Query and EPIC. We find evidence showing that neural models are able to make effective use of textual signals that are not reflected by classical term matching methods like BM25.  For example, when controlling for term frequency match, the neural models detect document relevance much more accurately than the BM25 baseline, and the effect is more pronounced in larger neural models.  Further, unlike prior approaches, rankers based on BERT and T5 are heavily influenced by word order: shuffling the words in a document consistently lowers the document's score relative to the unmodified version. We also find significant differences between different neural models: e.g., while most models treat queries navigationally , the BERT-based EPIC model and T5 do not exhibit such behaviors. Finally, these models can exhibit unexpected : adding additional relevant text to the end of a document frequently can reduce its ranking score, and adding non-relevant content can increase it---despite document length itself having a limited effect on the ranking scores. %  In summary, we present a new framework  for performing analysis of ad-hoc ranking models. We then demonstrate how the framework can provide insights into ranking model characteristics by providing the most comprehensive analysis of neural ranking models to date. Our released software framework facilitates conducting further analyses in future work.                      Pretrained contextualized language models are neural networks that are initially trained on language modeling objectives and are later fine-tuned on task-specific objectives. Signals from language modeling can be beneficial for downstream tasks, reducing the amount of in-domain training data required. Among the most well-known of these models are ELMo, BERT, and T5.  Ad-hoc retrieval is the task of ranking a collection of documents by relevance to a particular natural-language query. Researchers have found that pretrained contextualized language models can effectively transfer signals to this task, either by using the model directly  or by using the outputs as features into a larger model. There has been a multitude of work in this area, such as strategies for long passage aggregation and efficiency-conscious approaches. We refer the readers to for a comprehensive survey on these techniques. These models significantly outperform prior attempts to use neural networks for ad-hoc ranking and represent the most substantial gains in effectiveness for this task in the past decade. Our goal in this work is to help shed light on the mechanisms, strengths and weaknesses of this burgeoning body of work.  Analyses of recent learning-to-rank models tend to take an empirical approach, because analytic methods are impractical given the models' large number of parameters.   Diagnostic datasets, proposed by \citet{Rennings2019AnAA}, reformulate traditional ranking axioms---e.g., that documents with a higher term frequency should receive a higher ranking score---as empirical tests. \citeauthor{Rennings2019AnAA} studied neural ranking architectures that predate the rise of contextualized language models for ranking, and focused on just four axioms. \citet{Cmara2020DiagnosingBW} extended this work by adding five more previously-proposed ranking axioms  and evaluating on a distilled BERT model. They found that the axioms are inadequate to explain the ranking effectiveness of their model. Unlike these prior lines of work, we propose new tests that shed light onto possible sources of effectiveness, and test against current leading neural ranking architectures.  Others have attempted to characterize the strengths of BERT-based ranking models. For example, \citet{Dai2019DeeperTU} found a BERT model to be more effective at ranking documents for question queries ---an interesting finding given that most prior ranking techniques  tend to perform better with keyword-based queries. However, the effectiveness may be due to additional information provided by the question queries in the collection, rather than the linguistic characteristics of the queries themselves. Others have found that contextualized language models are more effective at identifying salient terms. For example, doc2query identifies salient parts of a document to generate plausible queries. DeepCT models term salience explicitly, by trying to predict which document terms will appear in a query. EPIC performs both, by jointly modeling query and document term salience, while also performing document expansion. However, these techniques use specialized architectures, and do not necessarily imply that vanilla models are effective due to this type of modeling. For instance, doc2query uses a sequential decoder to produce terms to add to the document---a component that is not present in vanilla models. Still others have found that contextualized value similarity can be a useful signal for ranking. However, these architectures only show that contextualized embedding similarity signals can be used for ranking, not what characteristics these embeddings capture. Rather than proposing new ranking models, in this work we analyze the effectiveness of existing models using controlled diagnostic tests., which allow us to gain insights into the particular behaviors and preferences of the ranking models.  Outside of the work in IR, others have developed techniques for investigating the behavior of contextualized language models in general. Although probing techniques and attention analysis can be beneficial for understanding model capabilities, these techniques cannot help us characterize and quantify the behaviors of neural ranking models. CheckList and other challenge set techniques differ conceptually from our goals; we aim to characterize the behaviors to understand the qualities of ranking models, rather than provide additional measures of model quality.        Because of their abstract meaning, reflexive anaphora present a distinctive challenge for semantic parsing that had been thought to be beyond the capabilities of recurrent networks. The experiments described here demonstrate that this was incorrect. Sequence-to-sequence networks with a range of recurrent unit types are in fact capable of learning an interpretation of reflexive pronouns that generalizes to novel antecedents. Our results also show  that such generalization is nonetheless contingent on the appearance of the held-out  antecedent in a variety of syntactic positions as well as the diversity of antecedents providing support for the reflexive generalization. Additionally successful generalization depends on the  network architecture in ways that we do not fully understand. It is at present unknown whether the demands that any of these architecture impose on the learning environment for successful learning of reflexives are consistent with what children experience, but this could be explored with both corpus and experimental work.  Future work will also be necessary to elucidate the nature of the networks' representations of reflexive interpretation and to understand how they support lexical generalization .    The question we have explored here is related to, but distinct from, the issue of systematicity , according to which pieces of representations learned in distinct contexts can freely recombine.  This issue has been addressed  using sequence-to-sequence architectures  in recent work with the synthetic SCAN robot command interpretation dataset  and on  language modeling , in both cases with limited success.  One aspect of the SCAN domain that is particularly relevant to reflexive interpretation is commands involving adverbial modifiers such as \lex{twice}.  Commands like \lex{jump twice} must be interpreted by duplicating the meaning of the verb, i.e., as , which is similar to what we require for the interpretation of the reflexive object, though in a way that does not require sensitivity to syntactic structure that we have not explored here. Recently, \citet{lake2019compositional}, \citet{li-etal-2019-compositional} and \citet{Gordon2020Permutation} have proposed novel architectures that increase systematic behavior, and we look forward to exploring the degree to which these impact performance on reflexive interpretation.   Our current work has focused  exclusively on recurrent networks, ranging from  SRNs to GRUs and LSTMs. Recent work by \citet{transformer} shows that Transformer networks  attain superior performance on a variety of sequence-to-sequence tasks while dispensing with recurrent units altogether. Examining both the performance and training characteristics of Transformers will allow us to compare the effects of attention and recurrence on the anaphora interpretation task. This is especially interesting given the impact that attention had on performance in our experiments.  Finally, while our current experiments are revealing about the capacity of recurrent networks to learn generalizations about context-sensitive interpretation, there are nonetheless limited in a number of respects because of simplifications in the English fragment we use to create our synthetic data. Reflexives famously impose a structural requirement on their antecedents . In the following example, the reflexive's antecedent must be  and cannot be . \ex The student near the teacher sees herself    \xe We do not know whether the architectures that have succeed on our experiments would do similarly well if the relevant generalization required reference to  structure. Past work has explored the sensitivity of recurrent networks to hierarchical structure, with mixed results . In ongoing work, we are exploring this question by studying  more complex synthetic domains both with the kind of recurrent sequence-to-sequence network used here as well networks that explicitly encode or decode sentences in a hierarchical manner.   A second simplification concerns the distribution of reflexives themselves. English reflexives can appear in a broader range of syntactic environments apart from transitive objects . It would be of considerable interest to explore the reflexive interpretation in a naturalistic setting that incorporate this broader set of distributions.   
","   Pretrained contextualized language models are neural networks that are initially trained on language modeling objectives and are later fine-tuned on task-specific objectives. Signals from language modeling can be beneficial for downstream tasks, reducing the amount of in-domain training data required. Among the most well-known of these models are ELMo, BERT, and T5.  Ad-hoc retrieval is the task of ranking a collection of documents by relevance to a particular natural-language query. Researchers have found that pretrained contextualized language models can effectively transfer signals to this task, either by using the model directly  or by using the outputs as features into a larger model. There has been a multitude of work in this area, such as strategies for long passage aggregation and efficiency-conscious approaches. We refer the readers to for a comprehensive survey on these techniques. These models significantly outperform prior attempts to use neural networks for ad-hoc ranking and represent the most substantial gains in effectiveness for this task in the past decade. Our goal in this work is to help shed light on the mechanisms, strengths and weaknesses of this burgeoning body of work.  Analyses of recent learning-to-rank models tend to take an empirical approach, because analytic methods are impractical given the models' large number of parameters.   Diagnostic datasets, proposed by \citet{Rennings2019AnAA}, reformulate traditional ranking axioms---e.g., that documents with a higher term frequency should receive a higher ranking score---as empirical tests. \citeauthor{Rennings2019AnAA} studied neural ranking architectures that predate the rise of contextualized language models for ranking, and focused on just four axioms. \citet{Cmara2020DiagnosingBW} extended this work by adding five more previously-proposed ranking axioms  and evaluating on a distilled BERT model. They found that the axioms are inadequate to explain the ranking effectiveness of their model. Unlike these prior lines of work, we propose new tests that shed light onto possible sources of effectiveness, and test against current leading neural ranking architectures.  Others have attempted to characterize the strengths of BERT-based ranking models. For example, \citet{Dai2019DeeperTU} found a BERT model to be more effective at ranking documents for question queries ---an interesting finding given that most prior ranking techniques  tend to perform better with keyword-based queries. However, the effectiveness may be due to additional information provided by the question queries in the collection, rather than the linguistic characteristics of the queries themselves. Others have found that contextualized language models are more effective at identifying salient terms. For example, doc2query identifies salient parts of a document to generate plausible queries. DeepCT models term salience explicitly, by trying to predict which document terms will appear in a query. EPIC performs both, by jointly modeling query and document term salience, while also performing document expansion. However, these techniques use specialized architectures, and do not necessarily imply that vanilla models are effective due to this type of modeling. For instance, doc2query uses a sequential decoder to produce terms to add to the document---a component that is not present in vanilla models. Still others have found that contextualized value similarity can be a useful signal for ranking. However, these architectures only show that contextualized embedding similarity signals can be used for ranking, not what characteristics these embeddings capture. Rather than proposing new ranking models, in this work we analyze the effectiveness of existing models using controlled diagnostic tests., which allow us to gain insights into the particular behaviors and preferences of the ranking models.  Outside of the work in IR, others have developed techniques for investigating the behavior of contextualized language models in general. Although probing techniques and attention analysis can be beneficial for understanding model capabilities, these techniques cannot help us characterize and quantify the behaviors of neural ranking models. CheckList and other challenge set techniques differ conceptually from our goals; we aim to characterize the behaviors to understand the qualities of ranking models, rather than provide additional measures of model quality.",137
"  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Commonsense is the knowledge shared by the majority of people in society and acquired naturally in everyday life. Commonsense reasoning is the process of logical inference by using commonsense information. Commonsense to answer the questions that is ``'' in Figure  is depicted as: ``'', ``'', and ``.'' An enormous amount of pre-defined commonsense knowledge is available and people can make inferences using this commonsense such as in the following example: ``''  ``''  ``''  ``'' This chain of commonsense reasoning is naturally deduced by humans without substantial difficulty. Whereas people acquire commonsense in their lives, machines cannot learn this knowledge without any assistance. A large amount of external knowledge and several reasoning steps are required for machines to learn commonsense. In recent years, various datasets  have been constructed to enable machines to reason commonsense.    is one of the most widely researched datasets and is presented in Figure  \subref{subfig:examplea}. The studies of commonsense reasoning based on this dataset can be categorized into two mainstream approaches. The first approach uses pre-trained language models with distributed representations, which exhibit high performances on most Natural Language Processing  tasks. However, despite their high performance, these models must be trained with an excessive number of parameters and cannot explain the process of commonsense reasoning. The second approach is reasoning with a commonsense knowledge graph. The generally used commonsense knowledge graph is ConceptNet 5.5 , which includes parsed representation from Open Mind Commonsense  and other different language sources such as WordNet  or DBPedia . In this approach, the subgraph of  ConceptNet corresponding to the questions are transformed into node embeddings by the graph encoder. The candidate with the highest attention score is selected as an answer that is computed between the node embeddings and the word vectors from the language models. To learn the commonsense knowledge that is not observed or understood by the language models, relations from ConceptNet serve as a critical role in this method. The performance is improved by utilizing the relations that are not represented in the text; however, the interpretation of the question is still not enough.   Unlike , the most commonly used method of solving this problem is Knowledge-Based Question-Answering   employing semantic representations. As this method infers the answer with the logical structure of the question using the knowledge base, the question-answering process can be explained in a logical form. In our work, Abstract Meaning Representation  , which is one of the logical structure, is used to understand the overall reasoning process, from the question to the answer.  AMR is a graph for meaning representation that symbolizes the meaning of sentences. AMR illustrates ``who is doing what to whom'' that is implied in a sentence with a graph.  The components of these graphs are not the words, but rather the concepts and their relations. Each concept denotes an event or an entity, and each relation represents the semantic role of the concepts.   In this paper, we enable the language models to exploit the AMR graph to understand the logical structure of sentences. However, it is difficult to infer commonsense information with only an AMR graph, owing to its deficiency of commonsense knowledge of the given sentence. For example, in Figure  \subref{subfig:exampleb}, the AMR graph indicates the path of the logical structure of the sentence ``'' ; in other words, these paths from the single AMR graph lack the proficient information to predict the right answer. Therefore, for commonsense reasoning, dynamic interactions between the AMR graph and ConceptNet are inevitable to reach the correct answer.   Thus, we propose a new compact AMR graph expanded with the ConceptNet's commonsense relations with pruning, and it is called ACP graph. The proposed method can interpret the path from the question to the answer by performing commonsense reasoning within the connected graph, such as ``'' .    The contributions of our study are as follows.      The remainder of this paper is organized as follows. In Section 2, we present the entire process of our method in detail. The experimental setup and results are explained in Section 3. A discussion of the proposed model is provided in Section 4, and Section 5 presents the conclusions. Appendix A provides related works including ConceptNet, previous works on commonsense reasoning, and AMR.         We propose a commonsense reasoning framework that uses a commonsense knowledge base on the basis of the AMR logic structure. Our framework consists of the AMR graph integrating and pruning module, language model encoder, and graph path learning module\footnote[4]{Code  available at }. As illustrated in Figure , we first generate the AMR graph from every question in the \texttt{CommonsenseQA} dataset and integrate all the nodes of AMR with ConceptNet graphs.   As this AMR-ConceptNet full graph also includes some irrelevant relations to the question, interpreting questions can be guided in the wrong way. For this reason, we suggest a new method, the ACP graph, pruned according to the relation type. Thereafter, the graph path learning module takes the pruned graph as an input and computes the attention score of each path by using the Graph Transformer  which results in the whole graph vector. The graph vector is finally fed into the Transformer  to model the interactions between the AMR and ConceptNet graph and transforms to the final graph representation. Meanwhile, the question and candidate answer from the dataset are passed through the language model encoder, producing the language vector. The concatenation of the language and graph vectors turns out to the final representation that is used to predict the correct answer.    \footnotetext{\footnote[]{} }   \footnotetext{\footnote[1]{} Equal contribution}  In contrast to other models mentioned in Talmor \shortcite{talmor2019commonsenseqa}, which cannot provide interpretable reasons for predicting the correct answer from the question, our proposed method produces the reasoning paths that make the model transparent and interpretable. That is, the reasoning paths that have high attention weights from the graph encoder possess potentially accurate information for reasoning. These reasoning paths are depicted in Figure .         As each word plays a certain role as a predicate or an argument in a sentence, the concepts of the AMR graph also carry semantic meanings in the graph structure. Hence, the AMR graph is capable of interpreting the questions as paths, semantically . Owing to these advantages of the graph structure and preserved semantic interpretation, we use the AMR graph for extracting commonsense knowledge graph. To generate an AMR graph from the raw text, we use the pre-trained model of Zhang \shortcite{zhang2019amr:2019}, which is an attention-based model that treats AMR parsing as sequence-to-graph transduction. Though most of the AMR graphs generated from the model properly, they might have some inevitable errors in the type of relations or concept.     	} 	\caption{Statistics of core roles in \texttt{CommonsenseQA} AMR graph. We split the given training set into the new training and test sets randomly to conduct diverse experiments with efficiency. The new training, development, and test sets included 8,500, 1,221, and 1,241 examples, respectively. } 	  }       We suggest effective AMR expansion and pruning rules for commonsense reasoning. We expand the AMR graph on all nodes with the ConceptNet as illustrated in Figure  \subref{fig:amr_cn_a}, and prune the nodes that have edges known as \texttt{ARG0} and \texttt{ARG1} with ConceptNet. Considering that \texttt{ARG0} and \texttt{ARG1} are the top two frequent relations among any other relations as shown in Table , we prune the full AMR-CN graph into a more compact graph that only contains \texttt{ARG0} and \texttt{ARG1} relations, which is called ACP graph. This procedure prevents the graph from discovering a tremendous number of paths iteratively. As described in Appendix A, since the frame node is defined as a central point in the AMR graph like \texttt{require-01} in Figure , combining other ConceptNet relations with the root node may distract the process of path reasoning. Also, the frame node's specific meaning additionally annotated by the number like ``\texttt{-01}'' at the end of the word is different from the meaning in ConceptNet's node even though it has identical letters. For example, the specific meaning of the frame node ``\texttt{play-11}'' is ``play/perform music'' defined in Propbank frameset while ConceptNet's node ``\texttt{play}'' includes more diverse meanings such as ``engage in an activity like game''or ``bet or wager''. Therefore, we remove the ConceptNet relations and nodes connected to the frame node. The proposed method is depicted in Figure  \subref{fig:amr_cn_b}.    The graph G =  expresses fixed set of nodes V, and relation edges E. Following this notation, the ACP graph is defined as follows:    The ACP graph expressed in equation  is the union set of the AMR and the subgraph of ConceptNet that contains AMR concepts that are connected to \texttt{ARG0} and \texttt{ARG1}, respectively. The AMR graph is denoted as . The subgraph of ConceptNet matched with the concepts that are connected to \texttt{ARG0} and \texttt{ARG1} is defined as     The proposed method performs commonsense reasoning over the ACP graph and predicts the correct answer with the corresponding inference. Our model receives two types of inputs, which are text and graph, and converts semantic representation to distributed representation. To encode the text input into the distributed representation, the language encoder which is the pre-trained language model with a massive amount of corpus takes an input that is formalized as ``\texttt{[CLS]+Question+[SEP]+candidate answer}.''  Given the ACP graph from the graph integrating and pruning module, the graph path learning module initializes the concept node vectors as the sum of the concept embedding using GloVe  and absolute position embedding. Inspired by the works of Cai \shortcite{cai2019graph:2019}, we modify the graph transformer to make the model reason over the relation paths of the ACP graph. To let the model recognize the explicit graph paths, we first encode the relation between two concepts into a distributed representation using the relation encoder. The relation encoder identifies the shortest path between two concepts and represents the sequence as a relation vector by employing recurrent neural networks with a  Gated Recurrent Unit  . The equation for the represented relation is expressed as follows:    where  indicates the shortest path of the relation between two nodes. The final relation encoding  between concepts  and  is the concatenation of the final hidden states from the forward and backward GRU networks, which are presented in the equation .     To inject this relation information into the concept representation, we follow the idea of relative position embedding , which introduces the attention score method based on both the concept representations and their relation representation. To compute the attention score, we split the relation vector  passed from the linear layer into forward relation encoding  and backward relation encoding , as follows:   where  is the parameter matrix. This split renders the model consider bidirectionality of the path.  Thereafter, we compute the attention score considering the concepts and their relations. Note that  and  are the concept embedding. The equation is presented below:    The first term in the last line of equation  is the original term in the vanilla attention mechanism, which includes the pure contents of the concept. The second and third terms capture the relation bias with respect to the source and target, respectively. The final term represents the universal relation bias. As a result, the computed attention score updates the concept embedding while maintaining fully-connected communication . Therefore, concept--relation interactions can be injected into the concept node vector. The resulted concept representations are summed into the whole graph vector and fed into the Transformer Layers to model the interaction between AMR and ConceptNet concept representation.  The major advantage of this relation-enhanced attention mechanism is that it provides a fully connected view of input graphs by making use of the relation multi-head attention mechanisms. Since we integrate two different concept types from the AMR graph and ConceptNet into a single graph, the model globally recognizes which path has high relevance to the question during the interpretation. After obtaining the language and graph vectors, the model concatenates the two vectors, feed these into the Softmax layer, and selects the correct answer.        We presented a new framework  for analyzing ranking models based on three testing strategies: Measure and Match Tests , Textual Manipulation Tests , and Dataset Transfer Tests . By using these tests, we demonstrated that a variety of insights can be gained about the behaviors of recently-proposed ranking models, such as those based on BERT and T5. Our analysis is, to date, the most extensive analysis of the behaviors of neural ranking models, and sheds light on several unexpected model behaviors. For instance, adding non-relevant text can increase a document's ranking score, even though the models are largely not biased towards longer documents. We also see that the same base language model used with a different ranking architecture can yield different behaviors, such as higher sensitivity to shuffling a document's text. Meanwhile, different language models can be sensitive to different characteristics, such as the importance of prepositions.   \documentclass[11pt,a4paper]{article} \usepackage[table]{xcolor} \usepackage{times,latexsym} \usepackage{url} \usepackage[T1]{fontenc} \usepackage{amsfonts} \usepackage{amsmath} \usepackage{todonotes} \usepackage{booktabs} \usepackage{adjustbox} \usepackage{rotating} \usepackage{layouts} \usepackage{enumitem}  \usepackage{afterpage}  \usepackage{float}  \usepackage{lipsum}  \DeclareMathOperator{\sgn}{sgn}  \definecolor{pos}{RGB}{76,144,186} \definecolor{neg}{RGB}{222,102,62} \definecolor{art}{RGB}{200,200,200}  \setlist[itemize]{noitemsep, topsep=0pt}  \hyphenation{Conv-KNRM}   \usepackage[acceptedWithA]{tacl2018v2}     \usepackage{xspace,mfirstuc,tabulary} \newcommand{\dateOfLastUpdate}{Sept. 20, 2018} \newcommand{\styleFileVersion}{tacl2018v2}  \newcommand{\ex}[1]{{\sf #1}} \newcommand{\sys}{ABNIRML}  \newif\iftaclinstructions \taclinstructionsfalse   \iftaclinstructions \renewcommand{\confidential}{} \renewcommand{\anonsubtext}{} \newcommand{\instr} \fi  \iftaclpubformat   \newcommand{\taclpaper}{final version\xspace} \newcommand{\taclpapers}{final versions\xspace} \newcommand{\Taclpaper}{Final version\xspace} \newcommand{\Taclpapers}{Final versions\xspace} \newcommand{\TaclPapers}{Final Versions\xspace} \else \newcommand{\taclpaper}{submission\xspace} \newcommand{\taclpapers}{{\taclpaper}s\xspace} \newcommand{\Taclpaper}{Submission\xspace} \newcommand{\Taclpapers}{{\Taclpaper}s\xspace} \newcommand{\TaclPapers}{Submissions\xspace} \fi   \newcommand\ac[1]{{\color{brown}\{#1\}}} \newcommand\sergey[1]{{\color{blue}\{#1\}}} \newcommand\doug[1]{{\color{orange}\{#1\}}} \newcommand\sm[1]{{\color{purple}\{#1\}}}  \title{\sys: Analyzing the Behavior of Neural IR Models}   \author{ {\bf Sean MacAvaney}\thanks{\xspace \xspace Work done during internship at AI2} \qquad Sergey Feldman \qquad {\bf Nazli Goharian} \\ {\bf Doug Downey} \qquad {\bf Arman Cohan}   \\      IR Lab, Georegetown University, Washington, DC \\      Allen Institute for AI, Seattle, WA \\   {\tt\small \{sean,nazli\}@ir.cs.georgetown.edu} \\   {\tt\small \{sergey,dougd,armanc\}@allenai.org} }  \date{}          
","   We propose a commonsense reasoning framework that uses a commonsense knowledge base on the basis of the AMR logic structure. Our framework consists of the AMR graph integrating and pruning module, language model encoder, and graph path learning module\footnote[4]{Code  available at }. As illustrated in Figure , we first generate the AMR graph from every question in the \texttt{CommonsenseQA} dataset and integrate all the nodes of AMR with ConceptNet graphs.   As this AMR-ConceptNet full graph also includes some irrelevant relations to the question, interpreting questions can be guided in the wrong way. For this reason, we suggest a new method, the ACP graph, pruned according to the relation type. Thereafter, the graph path learning module takes the pruned graph as an input and computes the attention score of each path by using the Graph Transformer  which results in the whole graph vector. The graph vector is finally fed into the Transformer  to model the interactions between the AMR and ConceptNet graph and transforms to the final graph representation. Meanwhile, the question and candidate answer from the dataset are passed through the language model encoder, producing the language vector. The concatenation of the language and graph vectors turns out to the final representation that is used to predict the correct answer.    \footnotetext{\footnote[]{} }   \footnotetext{\footnote[1]{} Equal contribution}  In contrast to other models mentioned in Talmor \shortcite{talmor2019commonsenseqa}, which cannot provide interpretable reasons for predicting the correct answer from the question, our proposed method produces the reasoning paths that make the model transparent and interpretable. That is, the reasoning paths that have high attention weights from the graph encoder possess potentially accurate information for reasoning. These reasoning paths are depicted in Figure .         As each word plays a certain role as a predicate or an argument in a sentence, the concepts of the AMR graph also carry semantic meanings in the graph structure. Hence, the AMR graph is capable of interpreting the questions as paths, semantically . Owing to these advantages of the graph structure and preserved semantic interpretation, we use the AMR graph for extracting commonsense knowledge graph. To generate an AMR graph from the raw text, we use the pre-trained model of Zhang \shortcite{zhang2019amr:2019}, which is an attention-based model that treats AMR parsing as sequence-to-graph transduction. Though most of the AMR graphs generated from the model properly, they might have some inevitable errors in the type of relations or concept.     	} 	\caption{Statistics of core roles in \texttt{CommonsenseQA} AMR graph. We split the given training set into the new training and test sets randomly to conduct diverse experiments with efficiency. The new training, development, and test sets included 8,500, 1,221, and 1,241 examples, respectively. } 	  }       We suggest effective AMR expansion and pruning rules for commonsense reasoning. We expand the AMR graph on all nodes with the ConceptNet as illustrated in Figure  \subref{fig:amr_cn_a}, and prune the nodes that have edges known as \texttt{ARG0} and \texttt{ARG1} with ConceptNet. Considering that \texttt{ARG0} and \texttt{ARG1} are the top two frequent relations among any other relations as shown in Table , we prune the full AMR-CN graph into a more compact graph that only contains \texttt{ARG0} and \texttt{ARG1} relations, which is called ACP graph. This procedure prevents the graph from discovering a tremendous number of paths iteratively. As described in Appendix A, since the frame node is defined as a central point in the AMR graph like \texttt{require-01} in Figure , combining other ConceptNet relations with the root node may distract the process of path reasoning. Also, the frame node's specific meaning additionally annotated by the number like ``\texttt{-01}'' at the end of the word is different from the meaning in ConceptNet's node even though it has identical letters. For example, the specific meaning of the frame node ``\texttt{play-11}'' is ``play/perform music'' defined in Propbank frameset while ConceptNet's node ``\texttt{play}'' includes more diverse meanings such as ``engage in an activity like game''or ``bet or wager''. Therefore, we remove the ConceptNet relations and nodes connected to the frame node. The proposed method is depicted in Figure  \subref{fig:amr_cn_b}.    The graph G =  expresses fixed set of nodes V, and relation edges E. Following this notation, the ACP graph is defined as follows:    The ACP graph expressed in equation  is the union set of the AMR and the subgraph of ConceptNet that contains AMR concepts that are connected to \texttt{ARG0} and \texttt{ARG1}, respectively. The AMR graph is denoted as . The subgraph of ConceptNet matched with the concepts that are connected to \texttt{ARG0} and \texttt{ARG1} is defined as     The proposed method performs commonsense reasoning over the ACP graph and predicts the correct answer with the corresponding inference. Our model receives two types of inputs, which are text and graph, and converts semantic representation to distributed representation. To encode the text input into the distributed representation, the language encoder which is the pre-trained language model with a massive amount of corpus takes an input that is formalized as ``\texttt{[CLS]+Question+[SEP]+candidate answer}.''  Given the ACP graph from the graph integrating and pruning module, the graph path learning module initializes the concept node vectors as the sum of the concept embedding using GloVe  and absolute position embedding. Inspired by the works of Cai \shortcite{cai2019graph:2019}, we modify the graph transformer to make the model reason over the relation paths of the ACP graph. To let the model recognize the explicit graph paths, we first encode the relation between two concepts into a distributed representation using the relation encoder. The relation encoder identifies the shortest path between two concepts and represents the sequence as a relation vector by employing recurrent neural networks with a  Gated Recurrent Unit  . The equation for the represented relation is expressed as follows:    where  indicates the shortest path of the relation between two nodes. The final relation encoding  between concepts  and  is the concatenation of the final hidden states from the forward and backward GRU networks, which are presented in the equation .     To inject this relation information into the concept representation, we follow the idea of relative position embedding , which introduces the attention score method based on both the concept representations and their relation representation. To compute the attention score, we split the relation vector  passed from the linear layer into forward relation encoding  and backward relation encoding , as follows:   where  is the parameter matrix. This split renders the model consider bidirectionality of the path.  Thereafter, we compute the attention score considering the concepts and their relations. Note that  and  are the concept embedding. The equation is presented below:    The first term in the last line of equation  is the original term in the vanilla attention mechanism, which includes the pure contents of the concept. The second and third terms capture the relation bias with respect to the source and target, respectively. The final term represents the universal relation bias. As a result, the computed attention score updates the concept embedding while maintaining fully-connected communication . Therefore, concept--relation interactions can be injected into the concept node vector. The resulted concept representations are summed into the whole graph vector and fed into the Transformer Layers to model the interaction between AMR and ConceptNet concept representation.  The major advantage of this relation-enhanced attention mechanism is that it provides a fully connected view of input graphs by making use of the relation multi-head attention mechanisms. Since we integrate two different concept types from the AMR graph and ConceptNet into a single graph, the model globally recognizes which path has high relevance to the question during the interpretation. After obtaining the language and graph vectors, the model concatenates the two vectors, feed these into the Softmax layer, and selects the correct answer.",138
"  Part-Of-Speech  tagging is a crucial step for language understanding, both being used in automatic language understanding applications such as named entity recognition  and question answering , but also being used in manual language understanding by linguists who are attempting to answer linguistic questions or document less-resourced languages .   Much prior work  on developing high-quality POS taggers uses neural network methods which rely on the availability of large amounts of labelled data. However, such resources are not readily available for the majority of the world's 7000 languages .  Furthermore, manually annotating large amounts of text with trained experts is an expensive and time-consuming task, even more so when linguists/annotators might not be native speakers of the language.    Active Learning \cite[AL]{lewis1995evaluating,settles2009active} is a family of methods that aim to train effective models with less human effort and cost by selecting such a subset of data that maximizes the end model performance. While many methods have been proposed for AL in sequence labeling , through an empirical study across six typologically diverse languages we show that within the same task setup these methods perform inconsistently. Furthermore, even in an oracle scenario  %  where we have access to the true labels during data selection, existing methods are far from optimal.  We posit that the primary reason for this inconsistent performance is that while existing methods consider uncertainty in predictions, they do not consider the direction of the uncertainty with respect to the output labels. For instance, in Figure  we consider the German token ``die,'' which may be either a pronoun  or determiner . According to the initial model , ``die'' was labeled as PRO majority of the time, but a significant amount of probability mass was also assigned to other output tags  for many examples. Based on this, existing AL algorithms that select uncertain tokens will likely select ``die'' because it is frequent and its predictions are not certain, but they may select an instance of ``die'' with either a gold label of PRO or DET. Intuitively, because we would like to correct errors where tokens with true labels of DET are mis-labeled by the model as PRO, asking the human annotator to tag an instance with a true label of PRO, even if it is uncertain, is not likely to be of much benefit.  Inspired by this observation, we pose the problem of AL for part-of-speech tagging as selecting tokens which maximally reduce the confusion between the output tags. For instance, in the example we would attempt to pick a token-tag pair ``die/DET'' to reduce potential errors of the model over-predicting PRO despite its belief that DET is also a plausible option. We demonstrate the features of this model in an oracle setting where we know true model confusions , and also describe how we can approximate this strategy when we do not know the true confusions.  We evaluate our proposed AL method by running simulation experiments on six typologically diverse languages namely German, Swedish, Galician, North Sami, Persian, and Ukrainian, improving upon models seeded with cross-lingual transfer from related languages . In addition, we conduct human annotation experiments on Griko, an endangered language that truly lacks significant resources.   Our contributions are as follows:           % File tacl2018v2.tex % Sep 20, 2018  % The English content of this file was modified from various *ACL instructions % by Lillian Lee and Kristina Toutanova % % LaTeXery is mostly all adapted from acl2018.sty.  \documentclass[11pt,a4paper]{article} \usepackage{times,latexsym} \usepackage{url} \usepackage[T1]{fontenc} \usepackage{amsmath} \usepackage{amssymb} \usepackage{tabularx} \usepackage{mathtools} \usepackage{booktabs} \usepackage{url} \usepackage{longtable} \usepackage{tabu} \usepackage{multirow} \usepackage{amsfonts} \usepackage{tabu} \usepackage{algorithm} \usepackage{bbm} \usepackage{subfigure} \usepackage[noend]{algpseudocode} \usepackage[normalem]{ulem} \usepackage{enumitem} \makeatletter  \def\BState{\State\hskip-\ALG@thistlm} \usepackage{bbm} \usepackage{xcolor} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\b-argmax}{ b\text{-}arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}   %% Package options: %% Short version: ""hyperref"" and ""submission"" are the defaults. %% More verbose version: %% Most compact command to produce a submission version with hyperref enabled %%    \usepackage[]{tacl2018v2} %% Most compact command to produce a ""camera-ready"" version \usepackage[acceptedWithA]{tacl2018v2} %% Most compact command to produce a double-spaced copy-editor's version %\usepackage[acceptedWithA]{tacl2018v2} % %% If you need to disable hyperref in any of the above settings  in the TACL instructions), add "",nohyperref"" in the square %% brackets.  %\usepackage[nohyperref]{tacl2018v2}  %%%% Material in this block is specific to generating TACL instructions \usepackage{xspace,mfirstuc,tabulary} \newcommand{\dateOfLastUpdate}{Sept. 20, 2018} \newcommand{\styleFileVersion}{tacl2018v2}  \newcommand{\gn}[1]{\textcolor{magenta}{\small [#1 --GN]}} \newcommand{\an}[1]{\textcolor{blue}{\small [#1 --AA]}}   \newcommand{\ex}[1]{{\sf #1}}  \newif\iftaclinstructions \taclinstructionsfalse % AUTHORS: do NOT set this to true \iftaclinstructions \renewcommand{\confidential}{} \renewcommand{\anonsubtext}{} \newcommand{\instr} \fi  % \iftaclpubformat % this ""if"" is set by the choice of options \newcommand{\taclpaper}{final version\xspace} \newcommand{\taclpapers}{final versions\xspace} \newcommand{\Taclpaper}{Final version\xspace} \newcommand{\Taclpapers}{Final versions\xspace} \newcommand{\TaclPapers}{Final Versions\xspace} \else \newcommand{\taclpaper}{submission\xspace} \newcommand{\taclpapers}{{\taclpaper}s\xspace} \newcommand{\Taclpaper}{Submission\xspace} \newcommand{\Taclpapers}{{\Taclpaper}s\xspace} \newcommand{\TaclPapers}{Submissions\xspace} \fi  %%%% End TACL-instructions-specific macro block %%%%  \title{Reducing Confusion in Active Learning for Part-Of-Speech Tagging}  \author{Aditi Chaudhary\textsuperscript{1},      Antonios Anastasopoulos\textsuperscript{2,\Thanks{ Work done at Carnegie Mellon University.}},      Zaid Sheikh\textsuperscript{1}, Graham Neubig\textsuperscript{1} \\   \textsuperscript{1}Language Technologies Institute, Carnegie Mellon University\\   \textsuperscript{2}Department of Computer Science, George Mason University\\   { @cs.cmu.edu}}    { }  }    \date{}   %              In this section, we list the most relevant research for active learning in POS tagging. \paragraph{Active Learning for POS tagging:} Active Learning  has been widely-used for POS tagging.  use a graph-based label propagation to generalize initial POS annotations to the unlabeled corpus. Further, they find that under a constrained time setting, type-level annotations prove to be more useful than token-level annotations. In line with this,  also select informative word types based on uncertainty sampling for low-resource POS tagging. They also construct a tag dictionary from these type-level annotations and then propagate the labels across the entire unlabeled corpus. However, in our initial analysis on uncertainty sampling, we found adding label-propagation harmed the accuracy in certain languages because of prevalent syncretism.  present different variations of uncertainty-sampling and query-by-committee methods for POS tagging. Similar to , they find uncertainty sampling with frequency bias to be the best strategy. \citet{settles2008analysis} present a nice survey on the different active learning strategies for sequence labeling tasks, whereas \citet{marcheggiani2014experimental} discuss the strategies for acquiring partially labeled data.  propose a core-set selection strategy aimed at finding the subset that is competitive across the unlabeled dataset. This work is most similar to ours with respect to using geometric center points as being the most representative.  However, to the best of our knowledge, none of the existing works are targeted at reducing confusion within the output classes.   \iffalse   }  \caption{Inter-annotator agreement against an expert Griko linguist. }         \fi  \paragraph{Low-resource POS tagging:} Several cross-lingual transfer techniques have been used for improving low-resource POS tagging. \citet{cotterell-heigold-2017-cross,malaviya2018neural} train a joint neural model on related high-resource languages and find it be very effective on low-resource languages. The main advantage of these methods is that they do not require any parallel text or dictionaries. \citet{das2011unsupervised, tackstrom2013token,yarowsky2001inducing,nicolai-yarowsky-2019-learning} use annotation projection methods to project POS annotations from one language to another.  Similarly, \citet{buys-botha-2016-cross} project annotations from English by projecting token and type constraints.  However, annotation projection methods use parallel text, which often might not be of good quality for low-resource languages.  { \algrenewcommand\algorithmicindent{1.2em}      }    In some cases of failure, our model exhibits two problems as follows:  As the concept node \texttt{illness} disappeared while generating the graph, our model may not have enough information for extracting the subgraph from ConceptNet.         The red edges in Figure  present the paths that have high attention weight for the question ``\texttt{What home entertainment equipment requires cable?}'' In Figure  \subref{subfig:case-heatmap}, the top four paths with high attention weights are described. As opposed to predicting the answers simply with the ConceptNet graph connected to the question, we allow our model to learn relevant paths inherent in the ACP graph. That is, our graph path learning module with ACP graph is capable of commonsense reasoning exploring the paths.     \section{Conclusions and Future Works} We introduce a new commonsense reasoning method, using the proposed ACP graph. This method outperformed the model that simply learns the ConceptNet graph. Furthermore, our method can explain the answer-inference process by interpreting the logical structure of the sentences within commonsense reasoning process. Models that applied our method exhibit higher performance compared to the  previous models. However, certain problems still remain. Though the relations \texttt{ARG0} and \texttt{ARG1} occupy most of the core roles in the AMR graph, it is still arguable that the other choice of relations may lead to better results. Therefore, we will show the experimental results according to the different pruning rules on the \texttt{CommonsenseQA} task in the future. Also, we plan to develop an end-to-end learning model that incorporates the AMR generation model and the question-answering model to reduce the error propagation from the AMR generation.  \section{Acknowledgement} This work was supported by Institute for Information \& communications Technology Planning \& Evaluation grant funded by the Korea government . Also, this research was supported by the MSIT, Korea, under the ITRC support program supervised by the IITP      \section*{Acknowledgements}    The acknowledgements should go immediately before the references.  Do   not number the acknowledgements section. Do not include this section   when submitting your paper for review.    include your own bib file like this:       ConceptNet. In ConceptNet , real-world assertions are represented as two nodes and directed edges, which denote certain concepts and their relations, respectively. The nodes represent words or phrases from natural language sentences. The edges represent the relations between nodes, and they contain lexical as well as commonsense relation information. As ConceptNet is created by collecting data from various types of knowledge bases, nodes of different types also exist. Each node represents a slightly different meaning considering its role in the sentence. For example, the word ``\texttt{person}'' can be found in the concept of ``\texttt{person/n},'' which is analyzed as a noun with a POS tagger, and with more detailed semantic information, it can be identified as ``\texttt{person/n/wn/body}.'' This information makes possible the detailed extraction of knowledge that considers the purpose of each sentence. Meanwhile, one or more edges may be defined between two nodes. For example, the edge between the nodes ``\texttt{person}'' and ``\texttt{eat}'' can be defined independently as ``\texttt{CapableOf}'' and ``\texttt{Desires}.'' Various concepts and their relations are defined as nodes and edges in ConceptNet, considering the ambiguity in the sentences.   Commonsense reasoning.  Commonsense reasoning is the process of logical inference by using commonsense information. In \texttt{CommonsenseQA}\footnote[1]{https://www.tau-nlp.org/csqa-leaderboard} task, the fine-tuning approach with pre-trained language representations makes use of external commonsense knowledge. There are two means of exploiting external knowledge. The first\footnote[7]{ttps://drive.google.com/file/d/1sGJBV38aG706EAR75F7LYwCqci9ocG9i/view,\\ https://gist.github.com/commonsensepretraining/507aefddcd00f891c83ebf6936df15e8} is the method that post-trained with some commonsense sentence corpus. It then performs fine-tuning with evidence derived from questions and answers. The second method  is to encode commonsense knowledge graphs and train with language models. The language models that have exhibited high performance in this method are BERT , RoBERTa , which use bidirectional transformer encoders. They also include XLNet , which is based on autoregressive language modeling, ALBERT , which adopts cross-layer parameter sharing and factorized embedding parameterization and ELECTRA  that is pre-trained with Replaced Token Detection  task.  AMR. AMR  represents the relations between concept nodes using the PropBank frameset and vocabularies from the sentences. The edges between two or more concept nodes or the argument nodes are relations. AMR represents semantic roles such as core and numbered roles, and uses more than 100 semantic relations, including negation, conjunction, command, and wikification. In PropBank , the semantic roles are labeled in the form of \texttt{ARG0}\texttt{4} and \texttt{ARGM}. In general, \texttt{ARG0} denotes the agent of the verb, \texttt{ARG1} is the patient, \texttt{ARG2} means the instrument, benefactive, or attribute, \texttt{ARG3} is interpreted as the starting point, benefactive, or attribute, and \texttt{ARG4} represents the ending point. The root node serves as the central point of the representation and is called frame node. Thereafter, other concept nodes are sequentially combined according to the semantic relations. AMR consists of concept nodes in a single graph that is traversable to all nodes, similar to a parse tree. However, unlike the parse tree, which represents the explicit structure of sentences, AMR aims to describe the conceptual and semantic structure. That is, if the semantic meanings of explicitly different sentences are the same, they can be represented by the same AMR graph. For example, the two sentences ``\texttt{The boy is a hard worker}'' and ``\texttt{The boy works hard}'' are represented by the same PENMAN graph, namely \texttt{ :manner )}. The data constructed to generate and evaluate these representations are AMR 2.0  and AMR 1.0 . The model with the highest performance on these data was presented by Zhang et al. \shortcite{zhang2019amr:2019}, using BERT. Various NLP fields have exploited AMR, such as sentence generation , summarization , question and answering , dialogue systems , paraphrase detection , and biomedical text mining .  
","    In this section, we list the most relevant research for active learning in POS tagging. \paragraph{Active Learning for POS tagging:} Active Learning  has been widely-used for POS tagging.  use a graph-based label propagation to generalize initial POS annotations to the unlabeled corpus. Further, they find that under a constrained time setting, type-level annotations prove to be more useful than token-level annotations. In line with this,  also select informative word types based on uncertainty sampling for low-resource POS tagging. They also construct a tag dictionary from these type-level annotations and then propagate the labels across the entire unlabeled corpus. However, in our initial analysis on uncertainty sampling, we found adding label-propagation harmed the accuracy in certain languages because of prevalent syncretism.  present different variations of uncertainty-sampling and query-by-committee methods for POS tagging. Similar to , they find uncertainty sampling with frequency bias to be the best strategy. \citet{settles2008analysis} present a nice survey on the different active learning strategies for sequence labeling tasks, whereas \citet{marcheggiani2014experimental} discuss the strategies for acquiring partially labeled data.  propose a core-set selection strategy aimed at finding the subset that is competitive across the unlabeled dataset. This work is most similar to ours with respect to using geometric center points as being the most representative.  However, to the best of our knowledge, none of the existing works are targeted at reducing confusion within the output classes.   \iffalse   }  \caption{Inter-annotator agreement against an expert Griko linguist. }         \fi  \paragraph{Low-resource POS tagging:} Several cross-lingual transfer techniques have been used for improving low-resource POS tagging. \citet{cotterell-heigold-2017-cross,malaviya2018neural} train a joint neural model on related high-resource languages and find it be very effective on low-resource languages. The main advantage of these methods is that they do not require any parallel text or dictionaries. \citet{das2011unsupervised, tackstrom2013token,yarowsky2001inducing,nicolai-yarowsky-2019-learning} use annotation projection methods to project POS annotations from one language to another.  Similarly, \citet{buys-botha-2016-cross} project annotations from English by projecting token and type constraints.  However, annotation projection methods use parallel text, which often might not be of good quality for low-resource languages.  { \algrenewcommand\algorithmicindent{1.2em}      }",139
" With an increasing submission of academic papers in recent years, the task of making final decisions manually incurs significant overheads to the program chairs, it is desirable to automate the process.  In this study, we aim at utilizing document-level semantic analysis for paper review rating prediction and recommendation.  Given the reviews of each paper from several reviewers as input, our goal is to infer the final acceptance decision for that paper and the reviewers' evaluation with respect to a numeric rating .  Paper review rating prediction and recommendation is a practical and important task in AI applications which will help improve the efficiency of the paper review process. It is also intended to enhance the consistency of the assessment procedures and outcomes, and to diversify the paper review process by comparing human recommended rating with machine recommended rating.  In the literature, most of existing studies cast review rating prediction as a multi-class classification/regression task .  They build a predictor by using supervised machine learning models with review texts and corresponding ratings.  Due to the importance of features, most researches focus on extracting effective features such as context-level features  and user features  to boost prediction performance.  However, feature engineering is time-consuming and labor-intensive.   Recently, with the development of neural networks and its wide applications, various deep learning-based models have been proposed for automatically learning features from text data .  Existing deep learning models usually learn continuous representations of different grains  from text corpus .  Although deep learning models can automatically learn extensive feature representation, they cannot efficiently capture the hierarchical relationship inherent to the review data.  To address this problem,  studied a hierarchical architecture and implemented it in deep learning framework to learn a better document-level representation.  Also, with the success of attention mechanism in many tasks such as machine translation, question answering and so on ,   designed a directional self-attention network to gain context-aware embeddings for words and sentences.  Despite great progress made by these models, they do not focus on the task of paper review rating recommendation and are not effective enough to be directly used for this task because of the following reasons: First, the review data is hierarchical in nature.  There exists a three-level hierarchical structure in the review data: word level, intra-review level and inter-review level, while previous models only capture two-levels  of this hierarchy.  Second, paper reviews are usually much longer than other reviews , while most of these models are working on those shorter reviews stated above and they do not leverage the up to date representation techniques such as BERT  and SciBERT .   In this paper, we propose a novel neural network framework for paper review rating recommendation by taking word, intra-review and inter-review information into account.  Specifically, inspired by HAN  and DiSAN , we introduce a Hierarchical Bi-directional self-Attention Network  framework to effectively incorporate different levels of hierarchical information.  The proposed framework consists of three main modules in end-to-end relationship: sentence encoder, intra-review encoder and inter-review encoder, which can consider hierarchical structures of review data as comprehensive as possible. The outputs of inter-review encoder are leveraged as features to build the rating predictor without any feature engineering. We release the code and data collected by us to enable replication and application to new tasks, available at https://github.com/RingBDStack/HabNet.  The contributions of this work are as follows:       Review rating prediction is a basic task in sentiment analysis.  It was initially studied by  who cast this problem as a multi-class classification/regression task.  In the literature, most of studies following this approach used supervised machine learning models to do review rating prediction.  Since the features used by these models are critical for prediction performance, more refined textual features are exploited.   introduced bag of opinions representation, where an opinion was composed of a root word, a set of modifier words and one or more negation words.  used user-specific and product-specific features to increase the reliability of sentiment classification.  With the popularity of deep learning model, instead of hand-crafted features, many works were proposed to automatically learn features from text corpora.  applied a recurrent structure for convolutional neural network to capture contextual information for learning word representation.   used very deep convolutional networks to learn hierarchical representations of whole sentences.   studied deepening word-level CNNs to capture global representations of text.   designed a deep Graph-CNN to learn both non-consecutive and long-distance features of text.    collected a dataset of peer reviews from several conferences and predicted paper acceptance decision by using paper draft.  focused on  predicting after-rebuttal scores by using their presented corpus.   applied argument mining on their AMPERE dataset to assess the efficiency of reviewing process.   designed a neural model to predict citation count of accepted papers.  designed a hierarchical attention-based CNN for automatic academic paper rating by using source paper, it adopts original attention mechanism which cannot capture the interactions between elements in the same level.   proposed DeepReviewer for automatic paper review utilizing paper's grammar and innovation to help learn better representation and predict paper's final review score.  Different from above works, we aim at predicting the final acceptance decisions for papers and ratings for reviews with self-attention based framework using raw review texts. And our collected dataset contains the rating score of each review and the final decision of each paper.     Attention mechanism was proposed by researchers to improve the performance of different NLP tasks. There are two common attention mechanisms: additive attention  and multiplicative attention , they use different compatibility functions to compute the attention weights.   introduced self-attention to extract an interpretable sentence embedding.   proposed a hierarchical attention network for document classification, which applied attention mechanism at word and sentence level.  \newcite {vaswani2017attention} built a simple network architecture based only on attention mechanism without convolutions and recurrence.  proposed an attentive convolution network which enables deriving higher-level features for a word from information extracted from nonlocal context.   designed a new attention mechanism which is directional and multi-dimensional, and a neural network solely based on this attention mechanism was proposed to learn sentence embedding.   proposed a memory-efficient bi-directional self-attention network which splits sequence into blocks to save memory.  Our framework is also based on self-attention mechanism, which makes use of the hierarchical characteristic of HAN  and the ability of capturing relationships between words from two directions in DiSAN .       We have presented a novel active learning method for low-resource POS tagging which works by reducing confusion between output tags. Using simulation experiments across  six typologically diverse languages, we show that our confusion-reducing strategy achieves higher accuracy than existing methods. Further, we test our approach under a true setting of active learning where we ask linguists to document POS information for an endangered language, Griko. Despite being unfamiliar with the language, our proposed method achieves performance gains over the other methods in most iterations. For our next steps, we plan to explore the possibility of adapting our proposed method for complete morphological analysis, which poses an even harder challenge for AL data selection due to the complexity of the task. 
","  Review rating prediction is a basic task in sentiment analysis.  It was initially studied by  who cast this problem as a multi-class classification/regression task.  In the literature, most of studies following this approach used supervised machine learning models to do review rating prediction.  Since the features used by these models are critical for prediction performance, more refined textual features are exploited.   introduced bag of opinions representation, where an opinion was composed of a root word, a set of modifier words and one or more negation words.  used user-specific and product-specific features to increase the reliability of sentiment classification.  With the popularity of deep learning model, instead of hand-crafted features, many works were proposed to automatically learn features from text corpora.  applied a recurrent structure for convolutional neural network to capture contextual information for learning word representation.   used very deep convolutional networks to learn hierarchical representations of whole sentences.   studied deepening word-level CNNs to capture global representations of text.   designed a deep Graph-CNN to learn both non-consecutive and long-distance features of text.    collected a dataset of peer reviews from several conferences and predicted paper acceptance decision by using paper draft.  focused on  predicting after-rebuttal scores by using their presented corpus.   applied argument mining on their AMPERE dataset to assess the efficiency of reviewing process.   designed a neural model to predict citation count of accepted papers.  designed a hierarchical attention-based CNN for automatic academic paper rating by using source paper, it adopts original attention mechanism which cannot capture the interactions between elements in the same level.   proposed DeepReviewer for automatic paper review utilizing paper's grammar and innovation to help learn better representation and predict paper's final review score.  Different from above works, we aim at predicting the final acceptance decisions for papers and ratings for reviews with self-attention based framework using raw review texts. And our collected dataset contains the rating score of each review and the final decision of each paper.     Attention mechanism was proposed by researchers to improve the performance of different NLP tasks. There are two common attention mechanisms: additive attention  and multiplicative attention , they use different compatibility functions to compute the attention weights.   introduced self-attention to extract an interpretable sentence embedding.   proposed a hierarchical attention network for document classification, which applied attention mechanism at word and sentence level.  \newcite {vaswani2017attention} built a simple network architecture based only on attention mechanism without convolutions and recurrence.  proposed an attentive convolution network which enables deriving higher-level features for a word from information extracted from nonlocal context.   designed a new attention mechanism which is directional and multi-dimensional, and a neural network solely based on this attention mechanism was proposed to learn sentence embedding.   proposed a memory-efficient bi-directional self-attention network which splits sequence into blocks to save memory.  Our framework is also based on self-attention mechanism, which makes use of the hierarchical characteristic of HAN  and the ability of capturing relationships between words from two directions in DiSAN .",140
"  % What is QG and Why it is important Question Generation  aims to endow machines with the ability to ask relevant and to-the-point questions about a document.  QG has important practical applications, such as  generating assessments for course materials in education, prompting user interaction in dialog systems, enabling machines to ask clarification questions such as FAQs, and automatically building large-scale QA datasets for the research community.   % How tranditional works do it? Recent QG approaches have used Seq2Seq models with attention, which feeds the input document into an encoder, and generates a question about the document through a decoder.  % Why it needs RL? The training objective is to maximize the log likelihood of the ground-truth question paired with each input document using teacher forcing. However, as the ground-truth questions are insufficient to account for the many equivalent ways of asking a question, this likelihood-based training suffers from the problem of exposure bias, i.e., the model does not learn how to distribute probability mass over sequences that are valid but different from the ground truth.  % How RL addresses the problem? %   To address this issue, previous QG works proposed to optimize the model directly on question-specific rewards via Reinforcement Learning .  This process decouples the training procedure from the ground truth data, so that the space of possible questions can be better explored. Moreover, it allows the training to target on specific properties we want the question to exhibit, such as relevant to a specific topic or answerable by the document.  % What is the problem for RL-based method? Although various rewards have been employed for QG --- such as BLEU, the answerability reward, and the word movers distance --- optimizing the reward scores does not always lead to higher question quality in practice, as observed by Hosking and Riedel~\shortcite{Hosking2019EvaluatingRF}. How to define robust and effective QG-specific rewards still requires further investigation.   % What we want to do? We aim to analyze the effectiveness of question-specific rewards in QG. Instead of using general natural language generation metrics such as BLEU, we target three QG-related metrics that are commonly cited in human evaluations of question quality:  Fluency indicates whether the question follows the grammar and accords with the correct logic;  Relevance indicates whether the question is relevant to the document; and  Answerability indicates whether the question is answerable given the document. We design a specific RL reward for each metric: a language model based reward for fluency, a discriminator-based reward for relevance, and a QA-based reward for answerability.  After optimizing each reward via RL, we conduct comprehensive analysis, including automatic and human evaluation, to arrive at the following conclusions:  both individual and joint optimization of these rewards can lead to performance gain in automated metrics, but this does not guarantee an improvement in the real question quality;  the reward for relevance substantially helps to improve the question quality, while the reward for answerability reduces the quality due to the bias brought by the QA model; and  a reward is more likely to improve the question quality if the reward score correlates well with human judgement.       NQG related works Early QG studies focused on using manually-designed rules or templates to transform a piece of given text to questions, with low generalizability and scalability. To address this, recent neural question generation  models take advantage of the Seq2Seq framework with attention, which are trained in an end-to-end manner, requiring far less labor and enabling better language flexibility. Many improvements have been made to the original Seq2Seq NQG model, such as encoding answer information and incorporating linguistic features. A comprehensive survey of QG can be found in.     Existing Rewards   replaced these -gram based similarities with Among these attempts, utilizing RL to optimize QG-specific rewards has been adopted by recent works to address the exposure bias problem. To find a good proxy for question quality, various rewards have been proposed. One common type of reward is the similarity between the generated question and the reference question written by human. Kumar et al. adopted BLEU, ROUGE, and METEOR as rewards. Followup works employed more semantic-relevant metrics, such as the word movers distance and the paraphrasing probability. To generate more passage-relevant questions, Kumar et al. designed a reward to measure the relevance between the input passage and the generated question based on their degree of overlapping. The answerability reward measures whether the generated question can be answered by the input passage. It is designed as either the confidence score that a pre-trained QA model can correctly answer the generated question, or the overlapping degree between the target answer and the answer predicted by the QA model. Other types of rewards include Yao et al., which train a discriminator to measure the naturalness, i.e., the question is human-written or generated.   Most question-specific rewards are empirically successful since they achieve performance gain in automatic evaluation metrics after RL training. However, this brings several followup questions that existing works have failed to answer:  does optimizing RL rewards really improve the question quality from the human standard,  which reward is more effective in improving the question quality, and  how the rewards interfere with each other when jointly optimized. This paper aims to bridge this gap through human evaluation and analytic experiments, aiming to provide a better understanding of how different rewards affect the question generation process.     In this paper, a scientific paper review dataset called OpenReview is collected from ICLR openreview website and released. We observe that there is a three-level hierarchical structure in this dataset  -- the information and relationships between reviews of one paper may affect the final decision, and so may relationships between words and sentences in each review. Based on these observations, a hierarchical bi-directional self-attention network  framework is proposed for paper review rating prediction and recommendation that can model the interactions among words, sentences, intra- and inter-reviews in an end-to-end manner. Moreover, considering the imbalanced distribution of different classes  in the review rating prediction task, we design two new metrics to better evaluate models.  It is seen that both experimental results of predicting final decisions for submitted papers and identifying ratings for reviews on two datasets  demonstrate our proposed framework has sufficient ability to capture the hierarchical structures of words, sentences and reviews in the datasets and outperforms other models. In the future, we plan to investigate multi-task learning for paper review rating recommendation.   
","   NQG related works Early QG studies focused on using manually-designed rules or templates to transform a piece of given text to questions, with low generalizability and scalability. To address this, recent neural question generation  models take advantage of the Seq2Seq framework with attention, which are trained in an end-to-end manner, requiring far less labor and enabling better language flexibility. Many improvements have been made to the original Seq2Seq NQG model, such as encoding answer information and incorporating linguistic features. A comprehensive survey of QG can be found in.     Existing Rewards   replaced these -gram based similarities with Among these attempts, utilizing RL to optimize QG-specific rewards has been adopted by recent works to address the exposure bias problem. To find a good proxy for question quality, various rewards have been proposed. One common type of reward is the similarity between the generated question and the reference question written by human. Kumar et al. adopted BLEU, ROUGE, and METEOR as rewards. Followup works employed more semantic-relevant metrics, such as the word movers distance and the paraphrasing probability. To generate more passage-relevant questions, Kumar et al. designed a reward to measure the relevance between the input passage and the generated question based on their degree of overlapping. The answerability reward measures whether the generated question can be answered by the input passage. It is designed as either the confidence score that a pre-trained QA model can correctly answer the generated question, or the overlapping degree between the target answer and the answer predicted by the QA model. Other types of rewards include Yao et al., which train a discriminator to measure the naturalness, i.e., the question is human-written or generated.   Most question-specific rewards are empirically successful since they achieve performance gain in automatic evaluation metrics after RL training. However, this brings several followup questions that existing works have failed to answer:  does optimizing RL rewards really improve the question quality from the human standard,  which reward is more effective in improving the question quality, and  how the rewards interfere with each other when jointly optimized. This paper aims to bridge this gap through human evaluation and analytic experiments, aiming to provide a better understanding of how different rewards affect the question generation process.",141
"  % In daily bases plethora of opinion data is published about different topics and in response to different stimuli using Social Media.  % Aiming to analyse and gain insights from opinions posted in social media, research in stance detection has become increasingly popular in recent years. Framed as a classification task, the stance detection consists in determining if a textual utterance expresses a supportive, opposing or neutral viewpoint with respect to a target or topic . Research in stance detection has largely been limited to analysis of single utterances in social media. Furthering this research, the SardiStance 2020 shared task  focuses on incorporating contextual knowledge around utterances, including metadata from author profiles and network interactions. The task included two subtasks, one solely focused on the textual content of social media posts for automatically determining their stance, whereas the other allowed incorporating additional features available through profiles and interactions. This paper describes and analyses our participation in the SardiStance 2020 shared task, which was held as part of the EVALITA  campaign and focused on detecting stance expressed in tweets associated with the Sardines movement. %  %   % For a network interaction graph, we generate user embeddings, using variations of graph neural network  embedding methods, and then concatenate author's vector with its corresponding utterance features for each stance. We also extract two types of text embedding representations for each utterance, embedding-based features, namely word embedding vectors and cosine similarity vectors, using different models including variations of CNN and bidirectional LSTM models. Further, the results of these two feature extraction methods are concatenated for the final classification step. We also consider the standard methods that extract frequency-based representations from author profiles and stance utterances including unigrams and Tfidf vectors. All these four features where combined and fed into the drop out and dense layers, to finally generate the final label using a softmax activation function. Though, we deactivate some of these four sources of features and alter the frequency-based vector by excluding some features, changing the embedding source and reducing the dimensionality for highly dimensional vectors  using PCA.}       Our work is related to three research areas in   natural language processing and machine learning. We briefly describe related studies in each area.       In broader view, when analysing textual data, features usually transformed into a numerical matrix using variations of two main approaches: classical approached using frequency-based methods and vector representations using context-based methods. Frequency-based method are based on counting , while prediction-based methods are based on predicting contextual meaning form surrounding words .   \paragraph{Classic Feature Engineering}   In social media, classical features can be extracted by using stylistic signals from text such as bag of n-grams, char-grams, part-of-speech labels, and lemmas , structural signals such as hashtags, mentions, uppercase characters, punctuation marks, and the length of the tweet , and pragmatic signals related to author's profile . With modern deep learning models, there is shift towards contextualised representations using word vector representation algorithms, either by having personalised language models trained on task specific language or as a pre-trained language model offered after training using complex architecture and billions of documents. Using deep learning layers as automated feature engineering methods can be implemented to train the model afterwards. In , they  utilized Bidirectional Conditional Encoding using LSTM achieving state-of-the-art results on stance detection task. Recently, there is a resurgence of research in incorporating network homophily  to represent social interactions within a network. Moreover, Knowledge graphs  can in turn represent these complex network relationships  as simple embedded vectors sampled considering the nodes and weighted edges within the network complexity structure.         Most NLP research focuses on methods, algorithms, data structures and reliability of predictive techniques mainly machine learning to understand representative textual data. Previous works on stance detection modeling considered     In this paper we designed and developed a pipeline for representing the knowledge of scientific publication into a structured graph that we called scientific knowledge graph. We employed various state-of-the-art NLP tools and machine learning, and provided a workflow to merge their results. Moreover, we integrated the knowledge coming from many scientific publications into a single knowledge graph with the purpose to represent detailed knowledge of the scientific literature about the Semantic Web domain. The evaluation proved that this solution is able to automatically produce good quality scientific knowledge graphs and that the integration of different tools yields a better overall performance.     There are a number of limitations that need to be still addressed in future work. In the first instance, the current version does not take full advantage of the semantic characterization of the research entities to verify the resulting triples. For instance, it is currently possible for an entity of kind Material to include a entity of kind Task, which may be semantically incorrect. For this reason, we plan to develop a more robust semantic framework that could drive the extraction process and discard triples that do not follow specific constraints. For example, we could state that a material could include another material, but not a task or a method. These requirements could be enforced and verified with the use of specific semantic technologies for expressing constraints such as SHACL\footnote{https://www.w3.org/TR/shacl/}.  A second limitation is that the current prototype can only extract one relationship between two entities. This is not completely realistic since two entities can be linked by many kinds of relationships.  This could also lead to a higher number of relationships that could suggest different applications or uses of entities, increasing the probability of finding unconsidered issues and solutions within a research field. We intend to explore this possibility in future work. Additionally, we will thoroughly investigate the conjunction construct which might hide rich knowledge about the relationship that frequently occurs between two research entities .  We also plan to improve the knowledge graph by considering cross document relations  to further link our entities, in order to better support tools for scientific inquiry.  A third limitation regards our ability to recognize synonyms that are not defined in existent knowledge bases, such as CSO. For instance, the current version may still fail to recognize that two quite different strings  actually refer to the same entity.  We intend to address this issue by computing the semantic similarity between word and graph embeddings representing the entities in order to detect and merge synonyms more effectively.   A fourth limitation regards the scalability of our pipeline.  The current implementation presents a few bottlenecks that could make difficult to apply it on very large-scale datasets. First, the Extractor Framework requires a lot of hard disk space. This entails that data must be sampled to be processed. Second, the current pipeline only adopts the Stanford Core NLP server with just one thread, which requires a long time to mine textual resources sentence-by-sentence.  However, this is not a big issue since it would be possible to run the Stanford Core NLP server in multi-thread mode, speeding up the extraction process.    An important next step will also be  to perform an extrinsic evaluation of the proposed knowledge base within different tasks. In particular, we would like to assess how AI tasks such as those tackled by recommender systems or graph embeddings creation strategies can benefit from it.    
","   Our work is related to three research areas in   natural language processing and machine learning. We briefly describe related studies in each area.       In broader view, when analysing textual data, features usually transformed into a numerical matrix using variations of two main approaches: classical approached using frequency-based methods and vector representations using context-based methods. Frequency-based method are based on counting , while prediction-based methods are based on predicting contextual meaning form surrounding words .   \paragraph{Classic Feature Engineering}   In social media, classical features can be extracted by using stylistic signals from text such as bag of n-grams, char-grams, part-of-speech labels, and lemmas , structural signals such as hashtags, mentions, uppercase characters, punctuation marks, and the length of the tweet , and pragmatic signals related to author's profile . With modern deep learning models, there is shift towards contextualised representations using word vector representation algorithms, either by having personalised language models trained on task specific language or as a pre-trained language model offered after training using complex architecture and billions of documents. Using deep learning layers as automated feature engineering methods can be implemented to train the model afterwards. In , they  utilized Bidirectional Conditional Encoding using LSTM achieving state-of-the-art results on stance detection task. Recently, there is a resurgence of research in incorporating network homophily  to represent social interactions within a network. Moreover, Knowledge graphs  can in turn represent these complex network relationships  as simple embedded vectors sampled considering the nodes and weighted edges within the network complexity structure.         Most NLP research focuses on methods, algorithms, data structures and reliability of predictive techniques mainly machine learning to understand representative textual data. Previous works on stance detection modeling considered",142
"   State-of-the-art for most existing natural language processing  classification tasks is currently achieved by systems that are first pre-trained on auxiliary language modeling tasks and then fine-tuned on the task of interest with cross-entropy loss . Although commonly used, cross-entropy loss -- the KL-divergence between one-hot vectors of labels and the distribution of model's output logits -- has several shortcomings. Cross entropy loss leads to poor generalization performance due to poor margins , and it lacks robustness to noisy labels  or adversarial examples . Effective alternatives have been proposed to change the reference label distributions through label smoothing , Mixup , CutMix , knowledge distillation  or self-training~.  Additionally, it has been recently demonstrated in NLP that fine-tuning using cross entropy loss tends to be unstable , especially when supervised data is limited, a scenario in which pre-training is particularly helpful. To tackle the issue of unstable fine-tuning, recent work proposes local smoothness-inducing regularizers  and regularization methods inspired by the trust region theory  to prevent representation collapse that lead to poor generalization performance. Empirical analysis suggests that fine-tuning for longer, reinitializing top few layers~, and using debiased Adam optimizer during fine-tuning~ can make the fine-tuning procedure more stable.  We are inspired by the learning strategy that humans deploy when given a few examples -- try to find the commonalities between the examples of each class and contrast them with examples from other classes. We hypothesize that a similarity-based loss will be able to hone in on the important dimensions of the multidimensional hidden representations and lead to better few-shot learning results and be more stable while fine-tuning pre-trained models. We propose a novel objective for fine-tuning pre-trained language models that includes a supervised contrastive learning term that pushes examples from the same class close and examples of different classes further apart. The new term is similar to the contrastive objective used for self-supervised representation learning in various domains such as image, speech, and video domains. . In constrast to these methods, however, we use a contrastive objective for supervised learning of the final task, instead of contrasting different augmented views of examples.  Adding supervised contrastive learning  term to the fine-tuning objective improves performance on several natural language understanding tasks from the GLUE benchmark , including SST-2, CoLA, MRPC, RTE, and QNLI over the state-of-the-art models fine-tuned with cross entropy loss. The improvements are particularly strong in few-shot learning settings , and models trained with SCL are not only robust to the noise in the training data, but also have better generalization ability to related tasks with limited labeled data. Our approach does not require any specialized architectures , memory banks , data augmentation of any kind, or additional unsupervised data. To the best of our knowledge, our work is the first to successfully integrate a supervised contrastive learning objective for fine-tuning pre-trained language models.   % \ves{end of alternative}  % State-of-the-art models for most existing natural language processing  tasks are currently learned by fine-tuning pre-trained large language models  that have been shown to capture semantic, syntactic, and world knowledge.  Recent attempts at improving the pre-training stage over masked language modeling~ has led to improvements on natural language understanding tasks, but fine-tuning stage has stayed the same for all downstream NLP classification tasks: add a task-specific output layer to the pre-trained language model and continue training on the labeled task data using cross-entropy loss.  % Cross-entropy loss is the most widely adopted objective for supervised classification models, defined as the KL-divergence between one-hot vectors of labels and the distribution of model's output logits. Although commonly used by the state-of-the-art models across many fields including NLP, there has been several works demonstrating the shortcomings of the cross-entropy loss, showing that it leads to poor generalization performance due to poor margins , and lack of robustness to noisy labels  or adversarial examples . Among the alternative objective functions proposed, more effective approaches in practice have been the ones that change the reference label distributions such as label smoothing , Mixup , CutMix , knowledge distillation  or self-training~.  % Several recent studies show that the fine-tuning procedure is unstable , especially for the case where supervised data is limited, a scenario in which pre-training is particularly helpful. To tackle the issue of unstable fine-tuning, local smoothness-inducing regularizers  and regularization methods inspired by the trust region theory  have been proposed to prevent representation collapse that leads to poor generalization performance of task models. There has also been empirical analysis that suggests fine-tuning for longer, reinitializing top few layers~, and using debiased Adam optimizer during fine-tuning~ make the fine-tuning procedure more stable.   % On the other hand, contrastive learning methods have seen remarkable success for self-supervised representation learning on various downstream tasks, particularly in the image, speech, and video domains.   % These self-supervised contrastive learning methods primarily try to reduce the distance between representations of the positive pairs while increasing the distance between representations of the negative pairs. Positive pairs are constructed as the different augmented views of the same labeled example, and negative pairs are simply augmented views of all the other examples. Augmented views of the examples are often constructed with state-of-the-art data augmentation methods such as RandAugment  or AutoAugment  for the computer vision domain, and distance metric is often chosen as the inner product or the Euclidean distance between the representations of the pairs in a low-dimensional embedding space.    % Recently, \citet{Khosla2020SupervisedCL} extended contrastive learning to a fully supervised setting through using label information while constructing positive and negative pairs, showed improved performance over cross-entropy loss baseline on ImageNet image classification accuracy and robustness benchmarks, and demonstrated that supervised contrastive learning is less sensitive to hyperparameter changes. Similarly, \citet{Liu2020HybridDT} propose a hybrid discriminative-generative training of energy-based models, where they approximate the generative term with a contrastive objective and demonstrate improved image classification accuracy on CIFAR-10 and CIFAR-100, along with improved performance on robustness, out-of-distribution detection, and calibration.  % In this paper, we propose a supervised contrastive learning regularization for fine-tuning of large pre-trained language models that helps the model leverage label information more effectively across different labeled data regimes. Our approach does not require specialized architectures , memory banks , or very large batch sizes , but still outperforms the strong baseline of fine-tuning RoBERTa-Large on labeled task data with cross-entropy loss, unlike some previous works. To the best of our knowledge, our work is the first to successfully integrate a supervised contrastive learning objective for fine-tuning pre-trained language models. % while sho results on few-shot learning, robustness, and generalization ability.  % We summarize our key contributions in the following: %     Traditional Machine Learning and Theoretical Understanding Several works have analyzed the shortcomings of the widely adopted cross-entropy loss, demonstrating that it leads to poor generalization performance due to poor margins , and lack of robustness to noisy labels  or adversarial examples . On the other hand, there has been a body of work that has explored the performance difference for classifiers trained with discriminative  losses such as cross-entropy loss and generative losses . \citet{Ng2001OnDV} show that classifiers trained with generative losses can outperform their counterparts trained with discriminative losses in the context of Logistic Regression and Naive Bayes. \citet{Raina2003ClassificationWH} show that a hybrid discriminative and generative objective outperforms both solely discriminative and generative approaches. In the context of contrastive learning, \citet{Arora2019ATA} propose a theoretical framework for analyzing contrastive learning algorithms through hypothesizing that semantically similar points are sampled from the same latent class, which allows showing formal guarantees on the quality of learned representations.   Contrastive Learning There has been several investigations for the use of contrastive loss formulations for self-supervised, semi-supervised, and supervised learning methods, primarily in the computer vision domain. \citet{Chen2020ASF} propose a framework for contrastive learning to learn visual representations without specialized architectures or a memory bank and show state-of-the-art results on ImageNet ILSVRC-2012 , outperforming previous methods for self-supervised, semi-supervised and transfer learning. Similarly, \citet{Khosla2020SupervisedCL} propose a supervised contrastive loss that outperforms cross entropy loss and gets state-of-the-art results on ImageNet on both ResNet-50 and ResNet-200  with AutoAugment  data augmentation. They also show increased robustness on the ImageNet-C dataset , and demonstrate that supervised contrastive loss is less sensitive to hyperparameter settings such as optimizers or data augmentations compared to cross-entropy loss. \citet{Liu2020HybridDT} propose a hybrid discriminative-generative training of energy-based models where they approximate the generative term with a contrastive loss using large batch sizes and show improved classification accuracy of WideResNet-28-10  on CIFAR-10 and CIFAR-100  datasets, outperforming state-of-the-art discriminative and generative classifiers. They also demonstrate improved performance for WideResNet-28-10 on robustness, out-of-distribution detection, and calibration, compared to other state-of-the-art generative and hybrid models. Finally, \citet{Fang2020CERTCS} propose pre-training language models using a self-supervised contrastive learning objective at the sentence level using back-translation as the augmentation method, followed by fine-tuning by predicting whether two augmented sentences originate from the same sentence -- showing improvements over fine-tuning BERT on a subset of GLUE tasks.   Stability and Robustness of Fine-tuning Language Models There has been several works on analyzing robustness of fine-tuning large pre-trained language models, since they tend to overfit to the labeled task data and fail to generalize to unseen data when there is limited labeled data for the downstream task. To improve the generalization performance, \citet{Jiang2020SMARTRA}  propose a local smoothness-inducing regularizer to manage the complexity of the model and a Bregman proximal point optimization method, an instance of trust-region methods, to prevent aggressive updating of the model during fine-tuning. They show state-of-the-art performance on GLUE, SNLI , SciTail , and ANLI  natural language understanding benchmarks. Similarly, \citet{Aghajanyan2020BetterFB} propose a regularized fine-tuning procedure inspired by trust-region theory that replaces adversarial objectives with parametric noise sampled from normal or uniform distribution in order to prevent representation collapse during fine-tuning for better generalization performance, without hurting the performance. They show improved performance on a range of natural language understanding and generation tasks including DailyMail/CNN , Gigaword , Reddit TIFU , and the GLUE benchmark. There has also been some empirical analysis that suggests fine-tuning for more epochs, reinitializing top few layers~ instead of only the classification head, and using debiased Adam optimizer instead of BERTAdam  during fine-tuning~ make the fine-tuning procedure more stable across different runs.      In this work, we described a state-of-the-art stance detection system leveraging different features including author profiling, word meaning context and social interactions. Using different random runs, our best model achieved  leveraging deepwalk-based knowledge graphs embeddings, FastText and similarity feature vectors extracted by two multi-headed convolutional neural networks from auther's utterance. This motivates our future, aiming to reduce the model complexity and automate the feature selection process.   
"," Traditional Machine Learning and Theoretical Understanding Several works have analyzed the shortcomings of the widely adopted cross-entropy loss, demonstrating that it leads to poor generalization performance due to poor margins , and lack of robustness to noisy labels  or adversarial examples . On the other hand, there has been a body of work that has explored the performance difference for classifiers trained with discriminative  losses such as cross-entropy loss and generative losses . \citet{Ng2001OnDV} show that classifiers trained with generative losses can outperform their counterparts trained with discriminative losses in the context of Logistic Regression and Naive Bayes. \citet{Raina2003ClassificationWH} show that a hybrid discriminative and generative objective outperforms both solely discriminative and generative approaches. In the context of contrastive learning, \citet{Arora2019ATA} propose a theoretical framework for analyzing contrastive learning algorithms through hypothesizing that semantically similar points are sampled from the same latent class, which allows showing formal guarantees on the quality of learned representations.   Contrastive Learning There has been several investigations for the use of contrastive loss formulations for self-supervised, semi-supervised, and supervised learning methods, primarily in the computer vision domain. \citet{Chen2020ASF} propose a framework for contrastive learning to learn visual representations without specialized architectures or a memory bank and show state-of-the-art results on ImageNet ILSVRC-2012 , outperforming previous methods for self-supervised, semi-supervised and transfer learning. Similarly, \citet{Khosla2020SupervisedCL} propose a supervised contrastive loss that outperforms cross entropy loss and gets state-of-the-art results on ImageNet on both ResNet-50 and ResNet-200  with AutoAugment  data augmentation. They also show increased robustness on the ImageNet-C dataset , and demonstrate that supervised contrastive loss is less sensitive to hyperparameter settings such as optimizers or data augmentations compared to cross-entropy loss. \citet{Liu2020HybridDT} propose a hybrid discriminative-generative training of energy-based models where they approximate the generative term with a contrastive loss using large batch sizes and show improved classification accuracy of WideResNet-28-10  on CIFAR-10 and CIFAR-100  datasets, outperforming state-of-the-art discriminative and generative classifiers. They also demonstrate improved performance for WideResNet-28-10 on robustness, out-of-distribution detection, and calibration, compared to other state-of-the-art generative and hybrid models. Finally, \citet{Fang2020CERTCS} propose pre-training language models using a self-supervised contrastive learning objective at the sentence level using back-translation as the augmentation method, followed by fine-tuning by predicting whether two augmented sentences originate from the same sentence -- showing improvements over fine-tuning BERT on a subset of GLUE tasks.   Stability and Robustness of Fine-tuning Language Models There has been several works on analyzing robustness of fine-tuning large pre-trained language models, since they tend to overfit to the labeled task data and fail to generalize to unseen data when there is limited labeled data for the downstream task. To improve the generalization performance, \citet{Jiang2020SMARTRA}  propose a local smoothness-inducing regularizer to manage the complexity of the model and a Bregman proximal point optimization method, an instance of trust-region methods, to prevent aggressive updating of the model during fine-tuning. They show state-of-the-art performance on GLUE, SNLI , SciTail , and ANLI  natural language understanding benchmarks. Similarly, \citet{Aghajanyan2020BetterFB} propose a regularized fine-tuning procedure inspired by trust-region theory that replaces adversarial objectives with parametric noise sampled from normal or uniform distribution in order to prevent representation collapse during fine-tuning for better generalization performance, without hurting the performance. They show improved performance on a range of natural language understanding and generation tasks including DailyMail/CNN , Gigaword , Reddit TIFU , and the GLUE benchmark. There has also been some empirical analysis that suggests fine-tuning for more epochs, reinitializing top few layers~ instead of only the classification head, and using debiased Adam optimizer instead of BERTAdam  during fine-tuning~ make the fine-tuning procedure more stable across different runs.",143
" With the rapid growth of textual documents on the internet, accessing information from the web has become a challenging issue . Often users want the summary of a topic from various sources to fulfill their information needs . The QF-MDS task deals with such problems where the goal is to summarize a set of documents to answer a given query.     In the QF-MDS task, the summaries generated by the summarizer can be either extractive or abstractive. An extractive summarizer extracts relevant text spans from the source document, whereas an abstractive summarizer generates a summary in natural language that may contain some words which did not appear in the source document . With the rising popularity of virtual assistants in recent years, there is a growing interest to integrate abstractive summarization capabilities in these systems for natural response generation .   One major challenge for the QF-MDS task is that the datasets  used for such tasks do not contain any labeled training data. Therefore, neural summarization models that leverage supervised training cannot be used in these datasets. Note that for other related tasks , how to reduce the demands for labeling the data and how to leverage unlabeled data were also identified as a major challenge. While using datasets similar to the target dataset as the training data for the QF-MDS task, we find that these datasets only contain multi-document gold summaries. However, the state-of-the-art transformer-based  summarization models  cannot be used in long documents due to computational complexities . To tackle these issues, we propose a novel weakly supervised approach by utilizing distant supervision to generate weak reference summary of each single-document from multi-document gold reference summaries. We train our model on each document with weak supervision and find that our proposed approach that generates abstractive summaries is very effective for the QF-MDS task. More concretely, we make the following contributions:        Early work on multi-document summarization was mostly focused on generic summarization , whereas the amount of work for QF-MDS had been very limited . Due to the lack of training data for the QF-MDS task, most previous works were based on various unsupervised approaches that could only generate extractive summaries .  To generate the abstractive summaries for the QF-MDS task,  proposed a transfer learning technique to tackle the issue of no training data. They adopted the Pointer Generation Network   pre-trained for the generic abstractive summarization task in a large dataset to predict the query focused summaries in the target dataset via modifying the attention mechanism of the PGN model. However, their model failed to outperform different extractive approaches in terms of various ROUGE scores .   Identifying sentences which are relevant to the query is an important step for the QF-MDS task. For this purpose, various approaches were utilized such as counting word overlaps  or the Cross-Entropy Method . Though neural models based on supervised training have significantly outperformed various non-neural models for the answer selection task in recent years , such neural models have not been effectively used for the QF-MDS task yet due to the absence of labeled data for the relevant sentences in the QF-MDS datasets.   Recently,  showed that neural models pre-trained in a large Question Answering  dataset could effectively select answers in other QA datasets. More recently, such pre-trained answer selection models for the QF-MDS task were used by . In their work, they utilized distant supervision from various QA datasets using the fine-tuned BERT  model to filter out the irrelevant sentences from the documents. However,  showed that filtering sentences as an early step could lead to performance deterioration for the QF-MDS task.  Thus, instead of applying distant supervision to filter out some sentences from the document, we apply it to generate the weak reference summary of each unlabeled document in our training datasets. Our proposed weakly supervised learning approach not only allows us to leverage the advantage of fine-tuning pre-trained generic summarization models , but also allows us to overcome the limitation of training neural models in long documents .       We propose a supervised contrastive learning objective for fine-tuning pre-trained language models and demonstrate improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both high-data and low-data regimes. We also show that our proposed objective leads to models that are more robust to different levels of noise in the training data and can generalize better to related tasks with limited labeled task data. Currently, data augmentation methods in NLP and their effects on the downstream tasks are neither as effective nor as well understood as their counterparts in the computer vision domain. In future work, we plan to study principled and automated data augmentation techniques for NLP that would allow extending our supervised contrastive learning objective to both semi-supervised and self-supervised learning settings.            
"," Early work on multi-document summarization was mostly focused on generic summarization , whereas the amount of work for QF-MDS had been very limited . Due to the lack of training data for the QF-MDS task, most previous works were based on various unsupervised approaches that could only generate extractive summaries .  To generate the abstractive summaries for the QF-MDS task,  proposed a transfer learning technique to tackle the issue of no training data. They adopted the Pointer Generation Network   pre-trained for the generic abstractive summarization task in a large dataset to predict the query focused summaries in the target dataset via modifying the attention mechanism of the PGN model. However, their model failed to outperform different extractive approaches in terms of various ROUGE scores .   Identifying sentences which are relevant to the query is an important step for the QF-MDS task. For this purpose, various approaches were utilized such as counting word overlaps  or the Cross-Entropy Method . Though neural models based on supervised training have significantly outperformed various non-neural models for the answer selection task in recent years , such neural models have not been effectively used for the QF-MDS task yet due to the absence of labeled data for the relevant sentences in the QF-MDS datasets.   Recently,  showed that neural models pre-trained in a large Question Answering  dataset could effectively select answers in other QA datasets. More recently, such pre-trained answer selection models for the QF-MDS task were used by . In their work, they utilized distant supervision from various QA datasets using the fine-tuned BERT  model to filter out the irrelevant sentences from the documents. However,  showed that filtering sentences as an early step could lead to performance deterioration for the QF-MDS task.  Thus, instead of applying distant supervision to filter out some sentences from the document, we apply it to generate the weak reference summary of each unlabeled document in our training datasets. Our proposed weakly supervised learning approach not only allows us to leverage the advantage of fine-tuning pre-trained generic summarization models , but also allows us to overcome the limitation of training neural models in long documents .",144
" One ultimate goal of language modelling is to construct a model like human, to grasp general, flexible and robust meaning in language. One reflection of obtaining such model is be able to master new tasks or domains on same task quickly. However, NLU models have been building from specific task on given data domain but fail when dealing with out-of-domain data or performing on a new task. To combat this issue, several research areas in transfer learning including domain adaptation, cross lingual learning, multi-task learning and sequential transfer learning have been developed to extend model handling on multiple tasks. However, transfer learning tends to favor high-resources tasks if not trained carefully, and it is also computationally expensive .  Meta learning algorithm tries to solve this problem by training model in a variety of tasks which equip the model the ability to adapt to new tasks with only a few samples.  In our case, we adopt the idea of model-agnostic meta learning  which is an optimization method of meta learning that directly optimized the model by constructing an useful initial representation that could be efficiently trained to perform well on various tasks . However, in an continual learning where data comes into the model sequentially, there is still a potential problem of catastrophic forgetting where a model trained with new tasks would start to perform worse on previous tasks. The two objectives of designing a continual learning architecture are to accelerate future learning where it exploits existing knowledge of a task quickly together with general knowledge from previous tasks to learn prediction on new samples and to avoid interference in previous tasks by updates from new tasks. .   % new In this paper, we utilize algorithm derived from Jave and White \shortcite{MLRCL:19} which applies Meta-Learning under continual learning. Our objective is to apply this framework in NLP field, specifically on NLU tasks. By taking advantage of this model-agnostic approach, Meta-Learning under continual learning should be applicable on any language model that is optimized by gradient-based methods. We compare our results with Duo et al \shortcite{dou:19} which applies meta-learning on Glue tasks, our MAML-Rep shows comparable results. We hope to bring new research direction in NLP fields focusing on such method. The implementation of our code can be found at \url{https://github.com/lexili24/NLUProject}.  % old % This paper aims to develop a framework that incorporate meta learning under the continual learning framework. Hypothetically, our approach is efficient in training by relying on low-resources on various tasks adapted from meta learning characteristics. By training a meta learner under continual learning framework, our model should have consistent results on various tasks with little catastrophic forgetting and learning general representation for all tasks. Finally, our approach is model agnostic, and could essentially apply on any existing language models as long as the model can be optimized by gradient descent. Moreover, our method can be put into the framework of some other continual learning techniques like GEM. The implementation of our code can be found at \url{https://github.com/lexili24/NLUProject}.     The section is dedicated to examine the implementation of methods solely or combined, in natural language and other fields, which leads us to develop our framework tackling NLU tasks. Plenty of research have been focused in these two areas and some efforts have succeeded in combining these two goals in other field.     There has been success in implementing MAML in NLU tasks . In their work, they explored the model-agnostic meta-learning algorithm  and its variants for low-resource NLU tasks and obtained impressive results on the GLUE benchmark. This proves that MAML can be applied to NLU tasks, and achieve comparable results on complex architectures like BERT and MT-DNN. However, this method does not address the potential problem of catastrophic forgetting, as they test Meta-Trained model on one task at a time.  In addition, meta learning is proved to excel in other natural language domains. Mi et al\shortcite{mi:19} has shown promising results of incorporating MAML in natural language generation . NLG models, like many NLU tasks, are heavily affected by the domain they are trained on and are data-intensive but data resource is low due to high annotation cost. Therefore, authors \shortcite{mi:19} approach to generalize a NLG model with MAML to train on the optimization procedure and derive a meaningful initialization serves to adapt new low-resource NLG scenarios efficiently. In comparison, Meta-Learning approach outperformed multi-task approach with higher BLEU score and lower error percentage.     Continual learning is proved to boost model performance in Liu et al \shortcite{liu:19}'s writing on computing sentence similarities. Liu et al \shortcite{liu:19} leveraged continual learning to construct a simple linear sentence encoder to learn representations and compute similarities between sentences, such application can be fed into a chat bot. A general concern is that in practice, the encoder is fed into a series of input from inconsistent corpora, and might degrade performance if fails to generalize common knowledge across domains. Continual learning enables zero-shot learning and allows a sentence encoder to perform well on new text domains while avoiding catastrophic forgetting. Authors evaluate result on semantic textual similarity  datasets with Pearson correlation coefficient . With a structure utilizing continual learning approach, Liu et al \shortcite{liu:19} showed consistent results cross various corpora.   Continual learning implemented in NLU tasks on top of transfer learning presented by Yogatama \shortcite{Yogatama:2019} did not show generalization of the model. Yogatama et al followed the continual learning setup to train a new task on best SQuAD-trained BERT and ELMo model, and both architectures show catastrophic forgetting after TriviaQA or MNLI is trained, which degrades model performance on SQuAD dataset. Their work shows an attempt to derive a generative language model and provides a solid ground of continual learning in language modelling.   An implementation of meta-learning under continual framework is proposed in reinforcement learning  by Alshedivat et al\shortcite{Al-Shedivat:2017}. In their paper, MAML is proved to be a complementary solution adding onto continual adaption in reinforcement learning  fields. Al-Shedivat et al\shortcite{Al-Shedivat:2017} considered nonstationary environments as sequences of stationary tasks for RL agents, which transferred nonstationary environment to learning-to-learning tasks. They developed a gradient-based Meta-Learning algorithm for quick adaption to continuously changing environment. They found that Meta-Learning is capable of adapting far more efficiently than baseline models in the few-shot regime. Although the implementation is outside the domain of Natural Language Processing, it is worth-noting that experts from different domains have implemented this method and sheds lights on authors to implement in NLU tasks.   To sum up,  MAML and continual learning have been applied on NLP tasks separately but not both. In reinforcement learning, Meta-Continual learning can solve non-stationary environments . In next section, we extend on the work done by Javed and White \shortcite{MLRCL:19} and propose implementations on combining both methods for NLP tasks.       In this paper, we propose a novel weakly supervised approach for the Query Focused Multi-Document Abstractive Summarization task to  tackle the issue of no available labeled training data for such tasks. We also propose an iterative approach to address the computational problem that occurs while training neural models in long documents . Experimental results in three datasets show that our proposed approach sets a new state-of-the-art result in various evaluation metrics. In the future, we will apply our models on more tasks, such as information retrieval applications , sentiment analysis , learning from imbalanced or unlabeled datasets , and automatic chart question answering .   
"," The section is dedicated to examine the implementation of methods solely or combined, in natural language and other fields, which leads us to develop our framework tackling NLU tasks. Plenty of research have been focused in these two areas and some efforts have succeeded in combining these two goals in other field.     There has been success in implementing MAML in NLU tasks . In their work, they explored the model-agnostic meta-learning algorithm  and its variants for low-resource NLU tasks and obtained impressive results on the GLUE benchmark. This proves that MAML can be applied to NLU tasks, and achieve comparable results on complex architectures like BERT and MT-DNN. However, this method does not address the potential problem of catastrophic forgetting, as they test Meta-Trained model on one task at a time.  In addition, meta learning is proved to excel in other natural language domains. Mi et al\shortcite{mi:19} has shown promising results of incorporating MAML in natural language generation . NLG models, like many NLU tasks, are heavily affected by the domain they are trained on and are data-intensive but data resource is low due to high annotation cost. Therefore, authors \shortcite{mi:19} approach to generalize a NLG model with MAML to train on the optimization procedure and derive a meaningful initialization serves to adapt new low-resource NLG scenarios efficiently. In comparison, Meta-Learning approach outperformed multi-task approach with higher BLEU score and lower error percentage.     Continual learning is proved to boost model performance in Liu et al \shortcite{liu:19}'s writing on computing sentence similarities. Liu et al \shortcite{liu:19} leveraged continual learning to construct a simple linear sentence encoder to learn representations and compute similarities between sentences, such application can be fed into a chat bot. A general concern is that in practice, the encoder is fed into a series of input from inconsistent corpora, and might degrade performance if fails to generalize common knowledge across domains. Continual learning enables zero-shot learning and allows a sentence encoder to perform well on new text domains while avoiding catastrophic forgetting. Authors evaluate result on semantic textual similarity  datasets with Pearson correlation coefficient . With a structure utilizing continual learning approach, Liu et al \shortcite{liu:19} showed consistent results cross various corpora.   Continual learning implemented in NLU tasks on top of transfer learning presented by Yogatama \shortcite{Yogatama:2019} did not show generalization of the model. Yogatama et al followed the continual learning setup to train a new task on best SQuAD-trained BERT and ELMo model, and both architectures show catastrophic forgetting after TriviaQA or MNLI is trained, which degrades model performance on SQuAD dataset. Their work shows an attempt to derive a generative language model and provides a solid ground of continual learning in language modelling.   An implementation of meta-learning under continual framework is proposed in reinforcement learning  by Alshedivat et al\shortcite{Al-Shedivat:2017}. In their paper, MAML is proved to be a complementary solution adding onto continual adaption in reinforcement learning  fields. Al-Shedivat et al\shortcite{Al-Shedivat:2017} considered nonstationary environments as sequences of stationary tasks for RL agents, which transferred nonstationary environment to learning-to-learning tasks. They developed a gradient-based Meta-Learning algorithm for quick adaption to continuously changing environment. They found that Meta-Learning is capable of adapting far more efficiently than baseline models in the few-shot regime. Although the implementation is outside the domain of Natural Language Processing, it is worth-noting that experts from different domains have implemented this method and sheds lights on authors to implement in NLU tasks.   To sum up,  MAML and continual learning have been applied on NLP tasks separately but not both. In reinforcement learning, Meta-Continual learning can solve non-stationary environments . In next section, we extend on the work done by Javed and White \shortcite{MLRCL:19} and propose implementations on combining both methods for NLP tasks.",145
"  	 	%  	% % final paper: en-us version  	% 	  % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/} }   Neural Machine Translation  adopts the encoder-decoder paradigm to model the entire translation process . Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism . However, such over-reliance on the topmost encoding layer is problematic in two aspects:  Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks ;  It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers  .   Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder . The differences between them lie in the design of the merge function: through self-attention , recurrent neural network , or tree-like hierarchical merge . Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer  or all encoder layers .  However, the above methods either complicate the original model  or limit the model's flexibility, such as requiring the number of the encoder layers to be the same as the decoder layers .   Instead, in this work, we propose layer-wise multi-view learning to address this problem from the perspective of model training, without changing the model structure. Our method's highlight is that only the training process is concerned, while the inference speed is guaranteed to be the same as that of the standard model. The core idea is that we regard the off-the-shelf output of each encoding layer as a view for the input sentence. Therefore, it is straightforward and cheap to construct multiple views during a standard layer-by-layer encoding process.  Further, in addition to the output of the topmost encoder layer used in standard models , we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder for independent predictions. An additional regularization loss based on prediction consistency between views is used to encourage the auxiliary view to mimic the primary view. Thanks to the co-training on the two views, the gradients during back-propagation can simultaneously flow into the two views, which implicitly realizes the knowledge transfer.  Extensive experimental results on five translation tasks  show that our method can stably outperform multiple baseline models . In particular, we have achieved new state-of-the-art results of 10.8 BLEU on KoEn and 36.23 BLEU on IWSLT'14 DeEn. Further analysis shows that our method's success lies in the robustness to encoding representations and dark knowledge  provided by consistency regularization.  \iffalse Our contributions are threefold:  \fi       In addition to the methods which incorporate different encoder layers by adjusting network structure , our work is also related to:       In this work, we are able to extend Meta-Learning under continual learning framework to learn a general presentation that is robust on a set of continual tasks with efficiency. We replicate \shortcite{MLRCL:19} method and and implement on NLU tasks. Results show that with less datapoints, we could derive a MAML like model that is robust on testing tasks, however extending it to continual setting during training phrase, the performance drastically worsen. Future direction would be extending this approach to other language models, as wells as experiment with a combination of high and low resources other than Glue and SuperGlue benchmark to evaluate model performance.       
","   In addition to the methods which incorporate different encoder layers by adjusting network structure , our work is also related to:",146
" In recent years, the best results for coreference resolution of English have been obtained with end-to-end neural models~. However for Dutch, the existing systems are still using either a  rule-based~ or a machine learning approach~. The rule-based system dutchcoref~ outperformed previous systems on two existing datasets and also presented a corpus and evaluation of literary novels .  In this paper we compare this rule-based system to an end-to-end neural coreference resolution system: e2e-Dutch. This system is a variant of \citet{lee2018higher} with BERT token representations. We evaluate and compare the performance of e2e-Dutch to dutchcoref on two different datasets:  the SoNaR-1 corpus , a genre-balanced corpus of 1 million words, and  the RiddleCoref corpus of contemporary novels . This provides insights into  the relative strengths of a neural system versus a rule-based system for Dutch coreference, and  the effect of domain differences .  The two datasets we consider vary greatly in terms of overall size and length of the individual documents; the training subset of RiddleCoref contains only 23 documents  compared to 581 documents for SoNaR-1. However, the average number of sentences per document is higher for RiddleCoref than for SoNaR-1 .  We also conduct an error analysis for both of the systems to examine the types of errors that the systems make.     The main differences between traditional and neural approaches can be summarized as follows:    The rest of this section discusses the current best systems for Dutch and English.    The largest dataset available for Dutch coreference resolution is the SoNaR-1 dataset  which consists of 1 million words annotated for coreference. This corpus was a continuation of the Corea project . \citet{declercq2011cross} present a cross-domain coreference resolution study conducted on this corpus. They use a mention-pair system, which was originally developed with the KNACK-2002 corpus and then further improved in the Corea project, and observe that the influence of domain and training size is large, thus underlining the importance of this large and genre-balanced SoNaR-1 dataset.  The current best coreference resolution system for Dutch is called ``dutchcoref""  and is based on the rule-based Stanford system . This system improved on the systems in the SemEval-2010 shared task~ and a previous implementation of the Stanford system for Dutch \citep[GroRef; ][]{vandergoot2015}. The main focus of \citet{vancranenburgh2019coref} was evaluating coreference on literary texts, for which a corpus and evaluation is presented. Most coreference resolution systems are evaluated using newswire texts, but a domain such as literary text presents its own challenges ; for example, novels are longer than news articles, and novels can therefore contain longer coreference chains.     ~~~~~~~~~    The main benchmark for English is the CoNLL 2012 shared task . \autoref{tblenglishsota} reports a timeline of results for this task, which shows the dramatic improvements brought by neural networks, especially the end-to-end systems on the right. Neural coreference systems improved on previous work but were still relying on mention detection rules, syntactic parsers, and heavy feature engineering . They were outperformed by the first end-to-end coreference resolution system by \citet{lee2017neural}. This system looks at all  the spans  in a text, up to a maximum length, and then uses a span-ranking model that decides for each span which previous spans are good antecedents, if any. The spans themselves are represented by word embeddings.  Although the models by \citet{clark2016deep} and \citet{lee2017neural} are computationally efficient and scalable to long documents,  they are heavily relying on first order models where they are only scoring pairs of mentions. Because they make independent decisions regarding coreference links, they might make predictions which are locally consistent but globally inconsistent . \citet{lee2018higher} introduce an approximation of higher-order inference, which uses the span-ranking architecture from \citet{lee2017neural} described above in an  iterative fashion, and also propose a coarse-to-fine approach to  lower the computational cost of this iterative higher-order approximation. Further improvements over \citet{lee2017neural}  were obtained through the use of deep contextualized ELMo  word embeddings. The current state-of-the-art scores are even higher by using BERT finetuning  However, this paper focuses on the model by \citet{lee2018higher}.  \citet{bamman2019coref} present coreference results on English literature with an end-to-end model comparable to the one used in this paper, except for using a separate mention detection step. However, their dataset consist of a larger number of shorter novel fragments . They report a CoNLL score of 68.1 on the novel fragments.    \caption{Dataset statistics}       Our experiments show that the importance of semantic roles for emotion classification differs between datasets and roles: The stimulus and cue are critical for classification, which correspond to the direct report of a feeling and the description that triggered an emotion. This result is shown in the drop in performance when removing these roles. This information is not redundantly available outside of these arguments.  It is particularly beneficial for the model's performance to have access to the position of cues and stimuli. This suggests that the classifier learns to tackle the problem differently when this information is available, especially so for ECA and ES -- the cases in which literature has been annotated and the instances are comparably long.  The bi-LSTM model indicates that the experiencer role is a confounder in GNE.  The performance can be increased when the model does not have access to its content. Similar results are observed for ET, in which the target role is a confounder. However, these results should be taken with a grain of salt given that they are not confirmed while switching to the transformer-based model. The differences in results between the bi-LSTM and the transformer also motivate further research, as they suggest that the contextualized representation might compensate for missing information, and is, therefore, more robust.  Finally, our results across both models and multiple datasets indicate that emotion classification approaches indeed benefit from semantic roles' information by adding the positional information. Similarly to targeted and aspect-based sentiment analysis, this motivates future work, in which emotion classification and role labeling should be modelled jointly. In this case, it can also be interesting to investigate what happens when the positional indicators are added to all roles jointly.  
"," The main differences between traditional and neural approaches can be summarized as follows:    The rest of this section discusses the current best systems for Dutch and English.    The largest dataset available for Dutch coreference resolution is the SoNaR-1 dataset  which consists of 1 million words annotated for coreference. This corpus was a continuation of the Corea project . \citet{declercq2011cross} present a cross-domain coreference resolution study conducted on this corpus. They use a mention-pair system, which was originally developed with the KNACK-2002 corpus and then further improved in the Corea project, and observe that the influence of domain and training size is large, thus underlining the importance of this large and genre-balanced SoNaR-1 dataset.  The current best coreference resolution system for Dutch is called ``dutchcoref""  and is based on the rule-based Stanford system . This system improved on the systems in the SemEval-2010 shared task~ and a previous implementation of the Stanford system for Dutch \citep[GroRef; ][]{vandergoot2015}. The main focus of \citet{vancranenburgh2019coref} was evaluating coreference on literary texts, for which a corpus and evaluation is presented. Most coreference resolution systems are evaluated using newswire texts, but a domain such as literary text presents its own challenges ; for example, novels are longer than news articles, and novels can therefore contain longer coreference chains.     ~~~~~~~~~    The main benchmark for English is the CoNLL 2012 shared task . \autoref{tblenglishsota} reports a timeline of results for this task, which shows the dramatic improvements brought by neural networks, especially the end-to-end systems on the right. Neural coreference systems improved on previous work but were still relying on mention detection rules, syntactic parsers, and heavy feature engineering . They were outperformed by the first end-to-end coreference resolution system by \citet{lee2017neural}. This system looks at all  the spans  in a text, up to a maximum length, and then uses a span-ranking model that decides for each span which previous spans are good antecedents, if any. The spans themselves are represented by word embeddings.  Although the models by \citet{clark2016deep} and \citet{lee2017neural} are computationally efficient and scalable to long documents,  they are heavily relying on first order models where they are only scoring pairs of mentions. Because they make independent decisions regarding coreference links, they might make predictions which are locally consistent but globally inconsistent . \citet{lee2018higher} introduce an approximation of higher-order inference, which uses the span-ranking architecture from \citet{lee2017neural} described above in an  iterative fashion, and also propose a coarse-to-fine approach to  lower the computational cost of this iterative higher-order approximation. Further improvements over \citet{lee2017neural}  were obtained through the use of deep contextualized ELMo  word embeddings. The current state-of-the-art scores are even higher by using BERT finetuning  However, this paper focuses on the model by \citet{lee2018higher}.  \citet{bamman2019coref} present coreference results on English literature with an end-to-end model comparable to the one used in this paper, except for using a separate mention detection step. However, their dataset consist of a larger number of shorter novel fragments . They report a CoNLL score of 68.1 on the novel fragments.    \caption{Dataset statistics}",147
"  A relational triple consists of two entities connected by a semantic relation, which is in the form of . The extraction of relational triples from unstructured raw texts is a key technology for automatic knowledge graph construction, which has received growing interest in recent years.  There have been several studies addressing technical solutions for relational triple extraction. Early researches, such as \citet{zelenko2003kernel,chan2011exploiting}, employ a pipeline manner to extract both of entities and relations, where entities are recognized first and then the relation between the extracted entities is predicted. Such a pipeline approach ignores the relevance of entity identification and relation prediction  and tends to suffer from the error propagation problem.  %    To model cross-task dependencies explicitly and prevent error propagation in the pipeline approach, subsequent studies propose joint entity and relation extraction. These studies can be roughly categorized into three main paradigms. The first stream of work, such as \citet{miwa2016end,gupta2016table,zhang2017end}, treats joint entity and relation extraction task as an end-to-end table filling problem. Although these methods represent entities and relations with shared parameters in a single model, they extract the entities and relations separately and produce redundant information . The second stream of work, such as \citet{zheng2017joint,dai2019joint,wei-etal-2020-novel}, transforms joint entity and relation extraction into sequence labeling. To do this, human experts need to design a complex tagging schema. The last stream of work, including \citet{zeng2018extracting,zeng2019learning,nayak2019ptrnetdecoding,zeng2020copymtl}, is driven by the sequence-to-sequence  model  to generate relational triples directly, which is a flexible framework to handle overlapping triples and does not require the substantial effort of human experts.  We follow the seq2seq based models for joint entity and relation extraction. Despite the success of existing seq2seq based models, they are still limited by the autoregressive decoder and the cross-entropy loss. The reasons are as follows: the relational triples contained in a sentence have no intrinsic order in essence. However, in order to adapt the autoregressive decoder, whose output is a sequence,  the unordered target triples must be sorted in a certain order during the training phase. Meanwhile, cross-entropy is a permutation-sensitive loss function, where a penalty is incurred for every triple that is predicted out of the position. Consequently, current seq2seq base models not only need to learn how to generate triples, but also are required to consider the extraction order of multiple triples.   % consists of three parts  featured by transformers with non-autoregressive parallel decoding and the bipartite matching loss.  In detail, there are three parts in the proposed set prediction networks :  to avoid introducing the order of triplets  % restoring to the original form of this task without considering the order of multiple triples In this work, we formulate the joint entity and relation extraction task as a set prediction problem, avoiding considering the order of multiple triples. In order to solve the set prediction problem, we propose an end-to-end network featured by transformers with non-autoregressive parallel decoding and bipartite matching loss. In detail, there are three parts in the proposed set prediction networks : a sentence encoder, a set generator, and a set based loss function. First of all, we adopt the BERT model  as the encoder to represent the sentence. Then, since an autoregressive decoder must generate items one by one in order, such a decoder is not suitable for generating unordered sets. In contrast, we leverage the transformer-based non-autoregressive decoder  as the set generator, which can predict all triples at once and avoid sorting triples. Finally, in order to assign a predicted triple to a unique ground truth triple, we propose bipartite matching loss function inspired by the assigning problem in operation research . Compared with  cross-entropy loss  that highly penalizes small shifts in  triple order, the proposed loss function is invariant to any permutation of predictions; thus it is suitable for evaluating the difference between ground truth set and prediction set.  % To summarize, our contributions are as follows: In a nutshell, our main contributions are: % the main contributions of our work are as follows:   % the conjunction of the bipartite matching loss and transformers with %  parallel decoding  % Our work build on prior work in several domains:relation extraction, non-autoregressive model, andbipartite matching losses for set prediction. % Relation Extraction.   Non-autoregressive Model.       Our work builds on prior work in relation extraction and non-autoregressive model.  Relation extraction is a long-standing natural language process task of mining factual knowledge from free texts. When giving a sentence with annotated entities, this task degenerates into a simple task, namely relation classification. Some studies, such as \citet{zeng-etal-2014-relation,xu2015classifying}, leveraged CNN or RNN to solve the relation classification task. However, these methods ignore the extraction of entities from sentences and could not truly extract relational facts.   When giving a sentence without any annotated entities, researchers proposed several methods to extract entities and relations jointly. Existing studies on multiple relation extraction task can be divided into four paradigms:  Pipeline based methods, such as \citet{zelenko2003kernel,chan2011exploiting}, firstly recognize entities and then conduct relation classification;  Table filling based methods, like \citet{miwa2016end,gupta2016table,zhang2017end}, represent entities and relations with shared parameters, but extract the entities and relations separately;  Tagging based methods, such as \citet{zheng2017joint,dai2019joint,wei-etal-2020-novel}, treat this task as a sequence labeling problem and need to design complex tagging schema;  Seq2seq based methods, like \citet{zeng2018extracting,zeng2019learning,nayak2019ptrnetdecoding,zeng2020copymtl},  apply seq2seq model to generate relational triples directly. Our work is in line with seq2seq based methods.  In contrast with the previous studies, we reckon the triples in a sentence are in the form of a set instead of a sequence, and treat the joint entity and relation extraction as a set prediction problem.   Non-autoregressive models  generate all the tokens of a target in parallel and can speed up inference. Non-autoregressive models are widely explored in natural language and speech processing tasks such as neural machine translation  and automatic speech recognition . To the best of our knowledge, this is the first work to apply non-autoregressive models to information extraction. In this work, we resort to the  non-autoregressive model to generate the set of triples in one shot.       There is no canonical deep learning model to directly predict sets. For constant-size set prediction, dense fully connected networks  are sufficient but costly. A general approach is to use autoregressive sequence models such as recurrent neural networks . In all cases, the loss function should be invariant by a permutation of the predictions. The usual solution is to design a loss based on the Hungarian algorithm [20], to find a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match. We follow the bipartite matching loss approach. In contrast to most prior work however, we step away from autoregressive models and use transformers with parallel decoding, which we describe below.         We found large gaps in performance for the two systems across the two domains, but this result is not conclusive due to several reasons, which are as follows.   1, 2 The neural system shows a weakness with the long documents in the novel corpus, but also needs more training data to reach its full potential.   3, 4 The rule-based system should be better adapted to the SoNaR-1 annotation scheme, but the neural system's capacity to adapt to arbitrary annotation conventions does not necessarily imply better linguistic performance.   5 To maximize the comparability and usefulness of the corpora, their annotations should be harmonized, which involves manual mention annotation. In future work we want to improve the neural system by using genre metadata and finetuning BERT, and the rule-based system should be extended to a hybrid system by adding supervised classifiers.    
","   Our work builds on prior work in relation extraction and non-autoregressive model.  Relation extraction is a long-standing natural language process task of mining factual knowledge from free texts. When giving a sentence with annotated entities, this task degenerates into a simple task, namely relation classification. Some studies, such as \citet{zeng-etal-2014-relation,xu2015classifying}, leveraged CNN or RNN to solve the relation classification task. However, these methods ignore the extraction of entities from sentences and could not truly extract relational facts.   When giving a sentence without any annotated entities, researchers proposed several methods to extract entities and relations jointly. Existing studies on multiple relation extraction task can be divided into four paradigms:  Pipeline based methods, such as \citet{zelenko2003kernel,chan2011exploiting}, firstly recognize entities and then conduct relation classification;  Table filling based methods, like \citet{miwa2016end,gupta2016table,zhang2017end}, represent entities and relations with shared parameters, but extract the entities and relations separately;  Tagging based methods, such as \citet{zheng2017joint,dai2019joint,wei-etal-2020-novel}, treat this task as a sequence labeling problem and need to design complex tagging schema;  Seq2seq based methods, like \citet{zeng2018extracting,zeng2019learning,nayak2019ptrnetdecoding,zeng2020copymtl},  apply seq2seq model to generate relational triples directly. Our work is in line with seq2seq based methods.  In contrast with the previous studies, we reckon the triples in a sentence are in the form of a set instead of a sequence, and treat the joint entity and relation extraction as a set prediction problem.   Non-autoregressive models  generate all the tokens of a target in parallel and can speed up inference. Non-autoregressive models are widely explored in natural language and speech processing tasks such as neural machine translation  and automatic speech recognition . To the best of our knowledge, this is the first work to apply non-autoregressive models to information extraction. In this work, we resort to the  non-autoregressive model to generate the set of triples in one shot.       There is no canonical deep learning model to directly predict sets. For constant-size set prediction, dense fully connected networks  are sufficient but costly. A general approach is to use autoregressive sequence models such as recurrent neural networks . In all cases, the loss function should be invariant by a permutation of the predictions. The usual solution is to design a loss based on the Hungarian algorithm [20], to find a bipartite matching between ground-truth and prediction. This enforces permutation-invariance, and guarantees that each target element has a unique match. We follow the bipartite matching loss approach. In contrast to most prior work however, we step away from autoregressive models and use transformers with parallel decoding, which we describe below.",148
" Zero-shot translation has first been introduced by \citet{firat-etal-2016-zero} and refers to the ability of a multilingual NMT model to translate between all its source and target languages, even those pairs for which no parallel data was seen in training. In the simplest setting, all parameters in the network are shared between the different languages and the translation is guided only by special tags to indicate the desired output language .  While this capability is attractive because it is an alternative to building  dedicated translation systems to serve  languages, performance on zero-shot pairs tends to lag behind pivot translation. Recent papers, such as \citet{Arivazhagan2019}, \citet{Gu2019} and \citet{Zhang2020}, have suggested training techniques to improve the generalization to unseen language pairs, but performance varies considerably across settings.  In this paper, we examine in detail the behavior of the multilingual model proposed by \citet{Johnson2017} on zero-shot translation directions. Our experiments show the following:     Overall, we observe improvements of 8.1 BLEU  on 6 zero-shot directions with simple changes to the multilingual training setup.    Our experiments are based on the multilingual model proposed by : A single model is trained on multiple language pairs with a standard encoder-decoder architecture, all parameters in the network are shared for all languages, including the vocabulary. An artificial target language token determines the output language. We prefix this special token to the source sentence as in \citet{Johnson2017}.  The major advantage of this model lies in its simplicity, since it does not require changing the architecture or training objective.  Several recent studies have explored approaches to improve generalization to zero-shot language pairs, for example through semi-supervised training  or alignment of encoder representations .  Our study is concerned with data conditions that enable zero-shot generalization for multilingual NMT, specifically preprocessing and data settings. While initial work used separate encoders and decoders for different languages , sharing of encoder and decoder parameters was established by \citet{Johnson2017,Ha2016TowardMN} and has since been widely adopted. \citet{Johnson2017} use a shared subword segmentation model across languages, and this strategy is followed by later work \citep[e.g.][]{Aharoni2019,Zhang2020}. \citet{Ha2016TowardMN} do not share embeddings across languages, but use language-specific codes. We will show that both strategies cause errors.  In terms of data settings, the number of languages involved in multilingual models has increased from 3--4  to over 100 . The most popular setup are English-centric datasets, where the model is trained on translations between English and a number of other languages.  A multi-way parallel corpus between 5 languages has been provided for the IWSLT17 multilingual task . Results on this dataset show strong zero-shot generalization, close or even exceeding the supervised condition , but multi-way parallel corpora are only available in small amounts and specific domains, so we investigate alternatives to English-centric models that do not rely on multi-way parallelism.      In this paper, we introduce set prediction networks for joint entity and relation extraction. Compared with previous seq2seq based models, We formulate the  joint entity and relation extraction task as a set prediction problem. In such a way, the extraction model will be relieved of predicting the extraction order of multiple triples. To solve the set prediction problem, We combine non-autoregressive parallel decoding with bipartite matching loss function. We conduct extensive experiments on two widely used datasets to validate the effectiveness of the proposed set prediction networks. Experimental results show that our proposed networks outperforms state-of-the-art baselines over different scenarios. This challenging task is far from being solved. We find that relation types exhibit an imbalanced or long-tailed distribution in NYT dataset and WebNLG dataset. Our future work will concentrate on how to combine cost-sensitive learning with the proposed set prediction networks.          
"," Our experiments are based on the multilingual model proposed by : A single model is trained on multiple language pairs with a standard encoder-decoder architecture, all parameters in the network are shared for all languages, including the vocabulary. An artificial target language token determines the output language. We prefix this special token to the source sentence as in \citet{Johnson2017}.  The major advantage of this model lies in its simplicity, since it does not require changing the architecture or training objective.  Several recent studies have explored approaches to improve generalization to zero-shot language pairs, for example through semi-supervised training  or alignment of encoder representations .  Our study is concerned with data conditions that enable zero-shot generalization for multilingual NMT, specifically preprocessing and data settings. While initial work used separate encoders and decoders for different languages , sharing of encoder and decoder parameters was established by \citet{Johnson2017,Ha2016TowardMN} and has since been widely adopted. \citet{Johnson2017} use a shared subword segmentation model across languages, and this strategy is followed by later work \citep[e.g.][]{Aharoni2019,Zhang2020}. \citet{Ha2016TowardMN} do not share embeddings across languages, but use language-specific codes. We will show that both strategies cause errors.  In terms of data settings, the number of languages involved in multilingual models has increased from 3--4  to over 100 . The most popular setup are English-centric datasets, where the model is trained on translations between English and a number of other languages.  A multi-way parallel corpus between 5 languages has been provided for the IWSLT17 multilingual task . Results on this dataset show strong zero-shot generalization, close or even exceeding the supervised condition , but multi-way parallel corpora are only available in small amounts and specific domains, so we investigate alternatives to English-centric models that do not rely on multi-way parallelism.",149
"  The proliferation of online hate speech has become prevalent in recent times. Numerous social media outlets and the computational social science community are looking at various automated techniques to detect and classify hate speech. However, most models, nascent in nature, have significant limitations due to the complexity of the problem. Primarily, the lack of a reliable baseline coupled with an evolving vocabulary of hateful content makes this a particularly challenging issue. For instance, many studies have classified this problem as a binary classification task, but this fails to address the subtleties of hate speech, such as direct  vs. indirect  hate speech. These binary classification models also fail to identify different types of hate speech like racism, sexism, antisemitism, etc. or their varying degrees. Another key obstacle that plagues these binary models is their inability to distinguish between general offensive language and hate speech. A third issue that arises in designing automated approaches is class imbalance---hate speech is usually a small percentage of the overall data---and the need to adequately upsample hate observations without model overfitting.  In our work, inspired by the recent successes in developing multi-class hate speech models that separate hate speech from offensive content, we propose DeL-haTE, an ensemble of tunable deep learning models that leverages CNN and GRU layers. The CNN layer extracts higher-order features from the word embedding matrix that then inform the GRU layer, which extracts informative features from the sequence of words. These features are utilized for automatic detection of hate speech on social media. Our novelty lies in using a tuning procedure to adapt the model to individual dataset characteristics.   %Issues particular to developing hate speech detection models %	- Class imbalance issue %		- Hate speech is a minute portion of the overall content on social media both generally and in published datasets %	- How to adequately upsample hate observations for training without leading to model overfitting? % %	- We, like others, utilize a downsampling approach during training to ensure a class-balanced dataset passes through the model at each epoch %	- We combine this with an early stopping procedure that utilizes a validation dataset and saves the model state at the epoch with minimal validation loss % %	- These procedures, and other factors, lead to variability in resultant models   %To maintain the necessity of downsampling during training while mitigating the problems of overfitting and variability, we develop an ensemble approach for hate speech classification, extending the CNN-RNN-FC model topology that has been shown to be successful for hate speech classification.   Our major contributions can be summarized by answering the following questions.   	 \end{enumerate}  Summary of Results: Our best ensemble on the HON dataset achieves a 65\% F1 Macro and an 83\% hate recall, surpassing the performance on the HON dataset of current state of the art models by 33\%. We show that the ensemble models outperform individual models by an average of 5\% hate recall and 8\% F1 macro across all datasets. When applied to unlabeled Gab data, tuning improved the pretrained models by an average of 12\%, with the best tuned ensemble models achieving 57\% hate recall. Our model trained using weak supervision achieved a 67\% hate recall on posts from Gab.  %\sidd{We show that the ensemble models outperform their individual components by an average of 5\% hate recall and 8\% F1 macro. % %We then examine the generalizability of our model framework to novel data from Gab, experimenting with both transfer learning and weak supervision %	- Transfer learning using a small manually labeled set of posts improved the hate recall ensembles pre-trained on the HON and OLID datasets by 10\% on the Gab data. % %	- We hypothesized that integrating the labeling of the HON and OLID datasets and combining them would lead to better generalizability for our model framework by increasing both the size and diversity of training examples %		This was confirmed by our experiments with transfer learning as the combined ensembles outperformed the single dataset models on Gab data by an average 8\% Hate recall over the HON models and 5\% on F1 Macro.}     Developing a consistent definition of hate speech is difficult due to its controversial and subjective nature. Social media sites define hate speech in legal terms as a 閳ユ泛emph{direct attack}閳 or 閳ユ泛emph{promoting violence}閳 against various characteristics of people, including race, ethnicity, nationality, religion, gender, and others. Many previous analyses have approached the study of hate speech analysis through the lenses of these characteristics. Examples include automatic hate speech detection modeled as - a binary classification problem, an attention-based multi-task learning model to identify toxic comments, a quantification of conflicting opinion among communities, and a racism/sexism classifier with embeddings learned from multiple deep learning architectures. A binary classification based approach, although simple, ignores the many subtleties of hate speech, such as indirect vs. direct hate speech and different forms of hate speech, such as racism, sexism, or antisemitism. Furthermore, the high prevalence of offensive language on social media presents an additional challenge for automatic hate speech detection online.     	    Several multi-class classification models have been introduced recently to better distinguish hate speech from offensive content on social media and to improve the automatic identification of various types of hate speech. In a similar vein, hierarchical annotation systems have been proposed that further distinguish the type and target of offensive posts, providing additional granularity for automatic detection models. One of the major issues with automatic hate speech detection research is the limited amount of manually labeled data. Weak supervised training allows for the use of large unannotated datasets by programmatically generating ``weak"" labels using heuristic approaches. Weak supervised learning has been applied for problems like cyberbullying detection to give better performance than traditional approaches.    , so while datasets annotated using multi-class and hierarchical labels provide a greater depth of information for model training, the tradeoff is that further partitioning of social media posts leads to fewer training observations within each class    recent reviews of work on automatic hate speech detection have been more conservative in their outlook \arun{}. Despite ,  Despite the strong performance of recent automatic hate speech detection models, with high reported recall and F1 scores , efforts to replicate reported findings and to generalize models to other similar datasets have often failed. While some of these failings are due to methodological shortcomings such as overfitting, they are also related to the inherent subjectivity of hate speech, the noisiness of short-text social media posts, and the biases present in datasets. With binary classification, it is also difficult to report the extent to which hate speech detection models are conflating hate speech with general offensive speech online. In addition to these challenges, hate speech constitutes a small portion of the overall content on social media, leading to the presence of severe class imbalance in hate speech datasets. Example include the datasets we use in this work as given in Table. Despite recent efforts to identify and address these challenges, there remains room for improvement in developing robust and generalized frameworks for automatic detection of hate speech on social media.  In this work, we develop a number of multi-class classifiers comparing four sets of pretrained word embeddings and three different deep model architectures. We leverage an ensemble approach to address issues of class imbalance and the limited number of per-class observations during model training. In order to assess the generalizability of our model frameworks, we apply transfer learning to tune classifiers trained on existing labeled Twitter datasets and test using a small sample of manually labeled posts from Gab.ai.    The tuned classifiers are juxtaposed against models trained using weak supervision on a large unannotated set of Gab.ai posts.   In this work, we develop a number of multi-class classifiers comparing five sets of pretrained word embeddings and three different deep model architectures. We leverage an ensemble approach to address issues of class imbalance and the limited number of per-class observations during model training. In order to assess the generalizability of our model frameworks, we apply transfer learning to tune classifiers trained on the HON and OLID datasets using a small sample of manually labeled posts from Gab.ai. The tuned classifiers are juxtaposed against models trained using weak supervision on a large unannotated set of Gab.ai posts.    We propose two neural measures of entrainment that control for consistency. We empirically validate these measures by demonstrating their ability to discriminate between real and fake sessions. Although our measures perform slightly worse than the one reported by \citet{Nasir2018}, we believe this is because their measure captures both entrainment and consistency and therefore better describes the expected similarity between two turns, but is overly broad as a measure of entrainment.  Most intriguingly, the strict separation of consistency and entrainment leads to correlations that are very different from those with other entrainment measures that do not account for consistency, even on the same corpus. This resembles the results of \citet{Perez2016}, who found that correlations differ based on how disentrainment is treated.   Our findings cast previous links between conversation quality and entrainment measures that do not account for consistency in a new light. It is worth revisiting those with the new ability to distinguish between consistency and entrainment.  In our future work, we intend to expand the network inputs for each prediction to the entire prior conversation context using RNNs with attention. We will also conduct further analysis of these entrainment measures, e.g., by feature, speaker sex, role, and dialogue act.            \section{Multiple testing for correlations with social variables}  The correlations between social variables and our entrainment measures vary greatly across retrainings of the underlying networks. This is especially true for , with -values for correlations with dom ranging from 7.5e-13 to almost 1.  To address this, we retrained both networks 100 times, recomputing the Pearson correlations each time. To control the false discovery rate resulting from this multiple testing, we use the procedure of \citet{Benjamini1995}. Each run consists of three tests per measure. We sort each group of three tests by their  values and determine the smallest value  such that  for at least one  value, where  is its position after sorting. Finally, we determine the largest  such that  where  is the -th smallest  value for any run of the respective measure, the level at which at least one of three correlations is significant for that run and measure.   Using this method, we find that 65 out of 100 times the correlation between  and dom is significant as well as 36 times for lik. None of the correlations for  reach the level of significance, not even in terms of the ``raw''  values.   For all but three of the 65 runs with significant correlations between  and dom, the correlation has the same valence. The three with opposite valence are among the weakest, the most significant one having only the 47th smallest  value. All 36 significant correlations between  and lik have the same valence. Considering the clear overall trends, we conclude that  correlates positively with dom and to a lesser degree with lik.  
","  Developing a consistent definition of hate speech is difficult due to its controversial and subjective nature. Social media sites define hate speech in legal terms as a 闁炽儲娉沞mph{direct attack}闁 or 闁炽儲娉沞mph{promoting violence}闁 against various characteristics of people, including race, ethnicity, nationality, religion, gender, and others. Many previous analyses have approached the study of hate speech analysis through the lenses of these characteristics. Examples include automatic hate speech detection modeled as - a binary classification problem, an attention-based multi-task learning model to identify toxic comments, a quantification of conflicting opinion among communities, and a racism/sexism classifier with embeddings learned from multiple deep learning architectures. A binary classification based approach, although simple, ignores the many subtleties of hate speech, such as indirect vs. direct hate speech and different forms of hate speech, such as racism, sexism, or antisemitism. Furthermore, the high prevalence of offensive language on social media presents an additional challenge for automatic hate speech detection online.     	    Several multi-class classification models have been introduced recently to better distinguish hate speech from offensive content on social media and to improve the automatic identification of various types of hate speech. In a similar vein, hierarchical annotation systems have been proposed that further distinguish the type and target of offensive posts, providing additional granularity for automatic detection models. One of the major issues with automatic hate speech detection research is the limited amount of manually labeled data. Weak supervised training allows for the use of large unannotated datasets by programmatically generating ``weak"" labels using heuristic approaches. Weak supervised learning has been applied for problems like cyberbullying detection to give better performance than traditional approaches.    , so while datasets annotated using multi-class and hierarchical labels provide a greater depth of information for model training, the tradeoff is that further partitioning of social media posts leads to fewer training observations within each class    recent reviews of work on automatic hate speech detection have been more conservative in their outlook \arun{}. Despite ,  Despite the strong performance of recent automatic hate speech detection models, with high reported recall and F1 scores , efforts to replicate reported findings and to generalize models to other similar datasets have often failed. While some of these failings are due to methodological shortcomings such as overfitting, they are also related to the inherent subjectivity of hate speech, the noisiness of short-text social media posts, and the biases present in datasets. With binary classification, it is also difficult to report the extent to which hate speech detection models are conflating hate speech with general offensive speech online. In addition to these challenges, hate speech constitutes a small portion of the overall content on social media, leading to the presence of severe class imbalance in hate speech datasets. Example include the datasets we use in this work as given in Table. Despite recent efforts to identify and address these challenges, there remains room for improvement in developing robust and generalized frameworks for automatic detection of hate speech on social media.  In this work, we develop a number of multi-class classifiers comparing four sets of pretrained word embeddings and three different deep model architectures. We leverage an ensemble approach to address issues of class imbalance and the limited number of per-class observations during model training. In order to assess the generalizability of our model frameworks, we apply transfer learning to tune classifiers trained on existing labeled Twitter datasets and test using a small sample of manually labeled posts from Gab.ai.    The tuned classifiers are juxtaposed against models trained using weak supervision on a large unannotated set of Gab.ai posts.   In this work, we develop a number of multi-class classifiers comparing five sets of pretrained word embeddings and three different deep model architectures. We leverage an ensemble approach to address issues of class imbalance and the limited number of per-class observations during model training. In order to assess the generalizability of our model frameworks, we apply transfer learning to tune classifiers trained on the HON and OLID datasets using a small sample of manually labeled posts from Gab.ai. The tuned classifiers are juxtaposed against models trained using weak supervision on a large unannotated set of Gab.ai posts.",150
"     In addition to other challenges in multiword expression  processing that were addressed in previous work, such as non-compositionality , discontinuity , and syntactic variability , The PARSEME shared task edition 1.2 has focused on another prominent challenge in detecting MWEs, namely detection of unseen MWEs. The problem with unseen data is common for many NLP tasks. While rule-based and unsupervised ML approaches are less affected by unseen data, supervised ML techniques are often found to be prone to overfitting. In this respect, the introduction of language modelling objectives to be added to different NLP tasks and their effect on generalisation have shown promising results. Further improvements brought by pre-trained language models made them a popular approach to a multitude of NLP tasks. One particular advantage of such models is that they facilitate generalisation beyond task-specific annotations .  MWEs are inherent in all natural languages and distinguishable for their syntactic and semantic idiosyncracies . Since language models are good at capturing syntactic and semantic features, we believe they are a suitable approach for modelling MWEs.    In particular, our system relies on BERT pre-trained language models .  Additionally, we render the system semi-supervised by means of multi-task learning. The most promising feature to be jointly learned with MWEs is dependency parse information . Accordingly, we fine-tune BERT for two different objectives: MWE detection and dependency parsing. MWE learning is done via token classification using a linear layer on top of BERT, and dependency parse trees are learned using dependency tree CRF network .  Our experiments confirm that this joint learning architecture is effective for capturing MWEs in most languages represented in the shared task.~       In earlier systems, MWEs were extracted using pre-defined patterns or statistical measures that either indicated associations among MWE components or compositionality of the expressions with regard to the components . For example,  employed such a system for identifying MWEs.  in running texts.  While these models can be effective for some frequent MWEs, their main disadvantage is that they capture MWE types  and they are unable to take context into account in running texts.   The use of supervised machine learning was facilitated by the availability of resources tagged for MWEs .  proposed a transition-based system based on an arc-standard dependency parser  which ranked first in the first edition of PARSEME shared task on automatic identification of verbal MWEs .  proposed a CNN-LSTM system which exploited fastText word representations and ranked first in the open track of the PARSEME shared task edition 1.1.  Previous systems such as TRAVERSAL  , and CRF-Seq/Dep  employed tree CRF using dependency parse features in non-deep learning settings. They showed strengths of this approach particularly in the case of discontinuous VMWEs. In SHOMA ,  using a linear-chain CRF layer on top of the CNN-biLSTM model did not result in improvements. In this work, we use tree CRF, implemented as part of the Torch-Struct library , to model dependency trees, and we show that when it is jointly trained with a transformer-based MWE detection system, it improves MWE prediction for a number of languages.  Recently,  proposed that learning MWE lexicons in an unsupervised setting is an important step that can be used in combination with a supervised model, especially when the latter is trained on a small amount of data. While we do not specifically learn MWE lexicons from external unannotated data, we believe that state-of-the-art pre-trained language representation models can capture crucial information about MWEs similar to other NLP phenomena . For instance,  showed how a semi-supervised system may benefit from pre-trained language model-based embeddings for named entity recognition  and chunking.  The joint learning of MWEs and dependency parsing has been proved effective in . They proposed an arc-standard transition-based system which draws on a new representation that has two linguistic layers  sharing lexical nodes.  The closest to our work is  where they have trained a multi-task neural network which jointly learns VMWEs and dependency parsing on a small English dataset and uses ELMo pre-trained embeddings.   Our work here is different in that we fine-tune the BERT architecture and we use a tree CRF for dependency parsing.  We show the effectiveness of using BERT and multi-task learning for detecting unseen expressions.     There is large potential in NLP to leverage user interaction logs for system improvement. We discussed how algorithms for offline RL can offer promising solutions to this type of learning problem. However, there are specific challenges in offline RL that arise due to the particular nature of NLP systems that collect human feedback in real-world applications. We presented cases where such challenges have been found and offered solutions that have helped. Furthermore, we related the identified challenges to the Challenges of Real-World Reinforcement Learning . This overview may serve as a guide for both NLP researchers to explore solutions of offline RL, and for RL researchers to test and equip their algorithms for the real-world challenges in NLP applications.
","  In earlier systems, MWEs were extracted using pre-defined patterns or statistical measures that either indicated associations among MWE components or compositionality of the expressions with regard to the components . For example,  employed such a system for identifying MWEs.  in running texts.  While these models can be effective for some frequent MWEs, their main disadvantage is that they capture MWE types  and they are unable to take context into account in running texts.   The use of supervised machine learning was facilitated by the availability of resources tagged for MWEs .  proposed a transition-based system based on an arc-standard dependency parser  which ranked first in the first edition of PARSEME shared task on automatic identification of verbal MWEs .  proposed a CNN-LSTM system which exploited fastText word representations and ranked first in the open track of the PARSEME shared task edition 1.1.  Previous systems such as TRAVERSAL  , and CRF-Seq/Dep  employed tree CRF using dependency parse features in non-deep learning settings. They showed strengths of this approach particularly in the case of discontinuous VMWEs. In SHOMA ,  using a linear-chain CRF layer on top of the CNN-biLSTM model did not result in improvements. In this work, we use tree CRF, implemented as part of the Torch-Struct library , to model dependency trees, and we show that when it is jointly trained with a transformer-based MWE detection system, it improves MWE prediction for a number of languages.  Recently,  proposed that learning MWE lexicons in an unsupervised setting is an important step that can be used in combination with a supervised model, especially when the latter is trained on a small amount of data. While we do not specifically learn MWE lexicons from external unannotated data, we believe that state-of-the-art pre-trained language representation models can capture crucial information about MWEs similar to other NLP phenomena . For instance,  showed how a semi-supervised system may benefit from pre-trained language model-based embeddings for named entity recognition  and chunking.  The joint learning of MWEs and dependency parsing has been proved effective in . They proposed an arc-standard transition-based system which draws on a new representation that has two linguistic layers  sharing lexical nodes.  The closest to our work is  where they have trained a multi-task neural network which jointly learns VMWEs and dependency parsing on a small English dataset and uses ELMo pre-trained embeddings.   Our work here is different in that we fine-tune the BERT architecture and we use a tree CRF for dependency parsing.  We show the effectiveness of using BERT and multi-task learning for detecting unseen expressions.",151
"  % \gn{Title candidate: ``Detecting Hallucinated Content ...'' . I wonder if you could also run your methods over extractive summarization outputs or the true references and see how many hallucinations they detect? Just an idea.} % However, recent studies on abstractive text summarization   % and neural machine translation~ have shown that conditional neural sequence models are prone to hallucinate content that is not faithful to the input text.  This risk of generating unfaithful content impedes the safe deployment of neural sequence generation models~. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for sequence evaluation, such as BLEU scores , ROUGE  and BERTScores , do not correlate well with the faithfulness of model outputs~. They also require reference output text, limiting their applicability to detecting halluciations in a deployed system at run-time. Very recent efforts~ have started to develop automatic metrics to measure the faithfulness of output sequences. These methods use external semantic models, e.g. the question-generation and question-answering systems~ or textual entailment inference models, to score faithfulness tailored for abstract text summarization.  However, these scores do not directly measure the number of hallucinated tokens %In addition, these metrics are often tailored for the evaluation of summaries in abstract text summarization  and only correlate weakly with human judgements.  % \gn{Big question: what is the difference from word-level quality estimation, which has been around for a very long time, since at least: \citet{bach-etal-2011-goodness} and has been covered in many WMT quality estimation shared tasks . This seems more related than the works cited below, and describing why we'd need to do something new over these works would probably be a big question in the minds of anyone familiar with the MT field. Also, would the proposed methods for detecting hallucination do better than SOTA word-level QE models?}  % \gn{Similar motivation: Moreover, they do not distinguish the types of errors in terms of fluency and adequacy: a substitution error referring to a simple morphological variation  is % considered in the same way as a content word substitution changing the meaning of the sentence.~.}  We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is a hallucinated or faithful to the source input.  This task does not use the reference output to assess faithfulness, which offers us the ability to apply it in the online generation scenario where references are not available. Similar to the spirit of our proposed task, word-level quality estimation~ in the machine translation community predicts if tokens are correctly translated based on human post-editing. However, they do not distinguish errors in terms of fluency and adequacy~.  % A substitution error referring to a simple morphological variation  is considered the same as a content word substitution changing the meaning of the sentence.~.  In contrast to estimating the amount of human post-editing work required to fix errors, we specifically focus only on hallucination  errors.  We measure hallucination for two conditional sequence generation tasks -- abstractive summarization and machine translation . For the former, we produce a benchmark dataset from recently released annotations ~. For MT, we carefully design the human assessment guideline and create high-quality annotations. We will also release our human annotated data for future research. To learn token-level hallucination prediction for general conditional sequence generations tasks, we propose a novel method that creates synthetic ``hallucinated"" data and finetunes a pretrained language model~ on it. Without any human annotated supervised training data, we achieve an average F1 of around 0.6 across all the benchmark datasets, setting initial performance levels for this new task. % \cz{\st{We have also computed sentence-level aggregated predictions and achieve significantly higher correlations with human scores than previous methods. Finally, we use our new data to study the effect of pretraining on MT hallucination and show it can actually produce more faithful translations, }} We also show that pretraining on MT can actually produce more faithful translations, confirming recent findings in abstractive summarization~.  Predicting hallucination labels at token-level provides a tool for diagnosing and interpreting model outputs, which allows us to flag potential risks at inference time for previously unseen inputs. On the other hand, the token-level labels also allow for fine-grained controls over the target sequence during learning full translation models.  We show how to use these token-level hallucination labels in two case studies to improve self-training and learning from noisy mined bitext in low-resource MT. In both cases, there can be noise in the target text, either produced by the self-training teacher or mining errors. However, most outputs are only partially hallucinated  and the rest of the output is still useful for training, as we show by introducing different token-level loss truncation schemes. %To further benefit self-training, we filter out the noisy part and also glean useful part of model predictions by applying token-level loss truncation or control of information flows to the target sequence at training time.  Our best methods outperform strong baselines by a large margin both in translation quality and hallucination reduction.            %        \gn{TBH: this seems pretty incomplete, there's a lot of work on error typologies for MT, evaluation for summarization, etc. that wasn't published in the past two years...}      \lz{This paragraph could be cut?} Neural sequence models are highly flexible, and do not have any inherent inductive bias to prevent them from generating outputs that are not entailed by the inputs. This flexibility can be an asset, it allows them to handle free-form text generation tasks like language modeling~, story generation~ or dialogue response generation~, where this strict entailment is not a prerequisite. However, for tasks like machine translation~ and text summarization~, where the semantic content of the output must be entirely reflective of the input, this is a major problem -- a user of the systems will not be able to trust that any generated text is an accurate reflection of the text they meant to translate or summarize. In addition, these problems are further exacerbated by the fact that most existing state-of-the-art models are trained using factuality-agnostic training algorithms such as maximum likelihood estimation~, and is especially prominent when models are trained on a domain outside of that they are tested on~.      \gn{True, but also probably true for any neural seq2seq system? I don't think there's any system where you can guarantee that it won't hallucinate content .} due to the discrepancies between training and inference time under domain shift  or vulnerabilities to artifacts or noises in the training data~ \gn{Are you sure it's actually due to this? For example, even if there's no domain shift and little or no noise during training I'm pretty sure that NMT or summarization systems would hallucinate content, especially if the training data is small. I would argue it's rather due to over-reliance on the decoder and its desire to generate fluent sentences at the cost of not being faithful to the input.}.         Commonly adopted metrics for sequence generations evaluations such as BLEU scores , ROUGE  and recently proposed quality estimation metrics including BLEURT  and BERTScores  have been shown that they correlate poorly with faithfulness~.   They also require reference text, limiting their applicability to detecting halluciations in a deployed system at run-time.    In addition, these metrics compute the similarity between the candidate outputs and the annotated references which requires the availability of annotations. However, assessing the faithfulness of generations is concerned with the safety of generation, which should be triggered before the machine outputs are exposed to users.    Recently, several efforts have been made on automatic evaluation of faithfulness and truthfulness in abstract text summarization~ by leveraging existing semantic inference systems:     question answering  system-based methods~ first employ a question generation system to generate natural questions based on the machine generated summary, then questions are answered with a QA system using the source article and the summary respectively. The final faithfulness score of the summary is given by comparing the corresponding answers.     For the textual entailment-based methods, a natural language inference  system is trained to predict whether the source article entails the summary or not. Then the entailment scores are used as the proxy to assess the faithfulness of the summary.   Nonetheless, these methods tailored for abstract text summarization are shown to have a weak correlation with human evaluation of faithfulness.     In addition, the absolute value of these metrics can not accurately assess how much hallucinations are within the summaries.    For example, in the QA-based methods it's difficult to generate the same number of questions for all the summaries, thus fair comparisons between different outputs can not be performed.      \gn{I looked around a bit more, and there is work on quality estimation that annotates span level errors, along with severity on three levels. For example, the Multidimensional Quality Metrics  standard , which was also used in Task 3 of the shared task on quality estimation . The severity of errors are classified as, for example ``An error can be minor , major  or critical .'' and also labeled with a type ``A label specifying the error type, such as wrong word order, missing words, agreement, etc. They may provide additional information, but systems don't need to predict them.'' I haven't looked closely at the annotations, but this almost seems a strict superset of what is annotated here.}    Similar to the spirit of our proposed task, word-level quality estimation~ in the machine translation community predicts if tokens are correctly translated based on human post-editing. However, they do not distinguish errors in terms of fluency and adequacy~.      A substitution error referring to a simple morphological variation  is considered the same as a content word substitution changing the meaning of the sentence.~.    In contrast to estimating the amount of human post-editing work required to fix errors, we specifically identify hallucinations corresponding to only hallucination  errors.      \gn{Even if the annotation standard of quality estimation is slightly different, the task of sequence labeling with both a sequence and the original input is the same. Why could we not just use a standard quality-estimation model out-of-the-box? And if we can, why should people use the method you propose below instead of other methods that have already proven to be good at the task?}        NEW MATH DEFINITIONS        \usepackage{amsmath,amsfonts,bm}    Mark sections of captions for referring to divisions of figures \newcommand{\figleft}{} \newcommand{\figcenter}{} \newcommand{\figright}{} \newcommand{\figtop}{} \newcommand{\figbottom}{} \newcommand{\captiona}{} \newcommand{\captionb}{} \newcommand{\captionc}{} \newcommand{\captiond}{}    Highlight a newly defined term \newcommand{\newterm}[1]{{\bf #1}}     Figure reference, lower-case. \def\figref#1{figure}   Figure reference, capital. For start of sentence \def\Figref#1{Figure} \def\twofigref#1#2{figures  and } \def\quadfigref#1#2#3#4{figures , ,  and }   Section reference, lower-case. \def\secref#1{section}   Section reference, capital. \def\Secref#1{Section}   Reference to two sections. \def\twosecrefs#1#2{sections  and }   Reference to three sections. \def\secrefs#1#2#3{sections ,  and }   Reference to an equation, lower-case. \def\eqref#1{equation}   Reference to an equation, upper case \def\Eqref#1{Equation}   A raw reference to an equation---avoid using if possible \def\plaineqref#1{}   Reference to a chapter, lower-case. \def\chapref#1{chapter}   Reference to an equation, upper case. \def\Chapref#1{Chapter}   Reference to a range of chapters \def\rangechapref#1#2{chapters--}   Reference to an algorithm, lower-case. \def\algref#1{algorithm}   Reference to an algorithm, upper case. \def\Algref#1{Algorithm} \def\twoalgref#1#2{algorithms  and } \def\Twoalgref#1#2{Algorithms  and }   Reference to a part, lower case \def\partref#1{part}   Reference to a part, upper case \def\Partref#1{Part} \def\twopartref#1#2{parts  and }  \def\ceil#1{\lceil #1 \rceil} \def\floor#1{\lfloor #1 \rfloor} \def\1{\bm{1}} \newcommand{\train}{\mathcal{D}} \newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}} \newcommand{\test}{\mathcal{D_{\mathrm{test}}}}  \def\eps{{\epsilon}}     Random variables \def\reta{{\textnormal{}}} \def\ra{{\textnormal{a}}} \def\rb{{\textnormal{b}}} \def\rc{{\textnormal{c}}} \def\rd{{\textnormal{d}}} \def\re{{\textnormal{e}}} \def\rf{{\textnormal{f}}} \def\rg{{\textnormal{g}}} \def\rh{{\textnormal{h}}} \def\ri{{\textnormal{i}}} \def\rj{{\textnormal{j}}} \def\rk{{\textnormal{k}}} \def\rl{{\textnormal{l}}}   rm is already a command, just don't name any random variables m \def\rn{{\textnormal{n}}} \def\ro{{\textnormal{o}}} \def\rp{{\textnormal{p}}} \def\rq{{\textnormal{q}}} \def\rr{{\textnormal{r}}} \def\rs{{\textnormal{s}}} \def\rt{{\textnormal{t}}} \def\ru{{\textnormal{u}}} \def\rv{{\textnormal{v}}} \def\rw{{\textnormal{w}}} \def\rx{{\textnormal{x}}} \def\ry{{\textnormal{y}}} \def\rz{{\textnormal{z}}}    Random vectors \def\rvepsilon{{\mathbf{\epsilon}}} \def\rvtheta{{\mathbf{\theta}}} \def\rva{{\mathbf{a}}} \def\rvb{{\mathbf{b}}} \def\rvc{{\mathbf{c}}} \def\rvd{{\mathbf{d}}} \def\rve{{\mathbf{e}}} \def\rvf{{\mathbf{f}}} \def\rvg{{\mathbf{g}}} \def\rvh{{\mathbf{h}}} \def\rvu{{\mathbf{i}}} \def\rvj{{\mathbf{j}}} \def\rvk{{\mathbf{k}}} \def\rvl{{\mathbf{l}}} \def\rvm{{\mathbf{m}}} \def\rvn{{\mathbf{n}}} \def\rvo{{\mathbf{o}}} \def\rvp{{\mathbf{p}}} \def\rvq{{\mathbf{q}}} \def\rvr{{\mathbf{r}}} \def\rvs{{\mathbf{s}}} \def\rvt{{\mathbf{t}}} \def\rvu{{\mathbf{u}}} \def\rvv{{\mathbf{v}}} \def\rvw{{\mathbf{w}}} \def\rvx{{\mathbf{x}}} \def\rvy{{\mathbf{y}}} \def\rvz{{\mathbf{z}}}    Elements of random vectors \def\erva{{\textnormal{a}}} \def\ervb{{\textnormal{b}}} \def\ervc{{\textnormal{c}}} \def\ervd{{\textnormal{d}}} \def\erve{{\textnormal{e}}} \def\ervf{{\textnormal{f}}} \def\ervg{{\textnormal{g}}} \def\ervh{{\textnormal{h}}} \def\ervi{{\textnormal{i}}} \def\ervj{{\textnormal{j}}} \def\ervk{{\textnormal{k}}} \def\ervl{{\textnormal{l}}} \def\ervm{{\textnormal{m}}} \def\ervn{{\textnormal{n}}} \def\ervo{{\textnormal{o}}} \def\ervp{{\textnormal{p}}} \def\ervq{{\textnormal{q}}} \def\ervr{{\textnormal{r}}} \def\ervs{{\textnormal{s}}} \def\ervt{{\textnormal{t}}} \def\ervu{{\textnormal{u}}} \def\ervv{{\textnormal{v}}} \def\ervw{{\textnormal{w}}} \def\ervx{{\textnormal{x}}} \def\ervy{{\textnormal{y}}} \def\ervz{{\textnormal{z}}}    Random matrices \def\rmA{{\mathbf{A}}} \def\rmB{{\mathbf{B}}} \def\rmC{{\mathbf{C}}} \def\rmD{{\mathbf{D}}} \def\rmE{{\mathbf{E}}} \def\rmF{{\mathbf{F}}} \def\rmG{{\mathbf{G}}} \def\rmH{{\mathbf{H}}} \def\rmI{{\mathbf{I}}} \def\rmJ{{\mathbf{J}}} \def\rmK{{\mathbf{K}}} \def\rmL{{\mathbf{L}}} \def\rmM{{\mathbf{M}}} \def\rmN{{\mathbf{N}}} \def\rmO{{\mathbf{O}}} \def\rmP{{\mathbf{P}}} \def\rmQ{{\mathbf{Q}}} \def\rmR{{\mathbf{R}}} \def\rmS{{\mathbf{S}}} \def\rmT{{\mathbf{T}}} \def\rmU{{\mathbf{U}}} \def\rmV{{\mathbf{V}}} \def\rmW{{\mathbf{W}}} \def\rmX{{\mathbf{X}}} \def\rmY{{\mathbf{Y}}} \def\rmZ{{\mathbf{Z}}}    Elements of random matrices \def\ermA{{\textnormal{A}}} \def\ermB{{\textnormal{B}}} \def\ermC{{\textnormal{C}}} \def\ermD{{\textnormal{D}}} \def\ermE{{\textnormal{E}}} \def\ermF{{\textnormal{F}}} \def\ermG{{\textnormal{G}}} \def\ermH{{\textnormal{H}}} \def\ermI{{\textnormal{I}}} \def\ermJ{{\textnormal{J}}} \def\ermK{{\textnormal{K}}} \def\ermL{{\textnormal{L}}} \def\ermM{{\textnormal{M}}} \def\ermN{{\textnormal{N}}} \def\ermO{{\textnormal{O}}} \def\ermP{{\textnormal{P}}} \def\ermQ{{\textnormal{Q}}} \def\ermR{{\textnormal{R}}} \def\ermS{{\textnormal{S}}} \def\ermT{{\textnormal{T}}} \def\ermU{{\textnormal{U}}} \def\ermV{{\textnormal{V}}} \def\ermW{{\textnormal{W}}} \def\ermX{{\textnormal{X}}} \def\ermY{{\textnormal{Y}}} \def\ermZ{{\textnormal{Z}}}    Vectors \def\vzero{{\bm{0}}} \def\vone{{\bm{1}}} \def\vmu{{\bm{\mu}}} \def\vtheta{{\bm{\theta}}} \def\va{{\bm{a}}} \def\vb{{\bm{b}}} \def\vc{{\bm{c}}} \def\vd{{\bm{d}}} \def\ve{{\bm{e}}} \def\vf{{\bm{f}}} \def\vg{{\bm{g}}} \def\vh{{\bm{h}}} \def\vi{{\bm{i}}} \def\vj{{\bm{j}}} \def\vk{{\bm{k}}} \def\vl{{\bm{l}}} \def\vm{{\bm{m}}} \def\vn{{\bm{n}}} \def\vo{{\bm{o}}} \def\vp{{\bm{p}}} \def\vq{{\bm{q}}} \def\vr{{\bm{r}}} \def\vs{{\bm{s}}} \def\vt{{\bm{t}}} \def\vu{{\bm{u}}} \def\vv{{\bm{v}}} \def\vw{{\bm{w}}} \def\vx{{\bm{x}}} \def\vy{{\bm{y}}} \def\vz{{\bm{z}}}    Elements of vectors \def\evalpha{{\alpha}} \def\evbeta{{\beta}} \def\evepsilon{{\epsilon}} \def\evlambda{{\lambda}} \def\evomega{{\omega}} \def\evmu{{\mu}} \def\evpsi{{\psi}} \def\evsigma{{\sigma}} \def\evtheta{{\theta}} \def\eva{{a}} \def\evb{{b}} \def\evc{{c}} \def\evd{{d}} \def\eve{{e}} \def\evf{{f}} \def\evg{{g}} \def\evh{{h}} \def\evi{{i}} \def\evj{{j}} \def\evk{{k}} \def\evl{{l}} \def\evm{{m}} \def\evn{{n}} \def\evo{{o}} \def\evp{{p}} \def\evq{{q}} \def\evr{{r}} \def\evs{{s}} \def\evt{{t}} \def\evu{{u}} \def\evv{{v}} \def\evw{{w}} \def\evx{{x}} \def\evy{{y}} \def\evz{{z}}    Matrix \def\mA{{\bm{A}}} \def\mB{{\bm{B}}} \def\mC{{\bm{C}}} \def\mD{{\bm{D}}} \def\mE{{\bm{E}}} \def\mF{{\bm{F}}} \def\mG{{\bm{G}}} \def\mH{{\bm{H}}} \def\mI{{\bm{I}}} \def\mJ{{\bm{J}}} \def\mK{{\bm{K}}} \def\mL{{\bm{L}}} \def\mM{{\bm{M}}} \def\mN{{\bm{N}}} \def\mO{{\bm{O}}} \def\mP{{\bm{P}}} \def\mQ{{\bm{Q}}} \def\mR{{\bm{R}}} \def\mS{{\bm{S}}} \def\mT{{\bm{T}}} \def\mU{{\bm{U}}} \def\mV{{\bm{V}}} \def\mW{{\bm{W}}} \def\mX{{\bm{X}}} \def\mY{{\bm{Y}}} \def\mZ{{\bm{Z}}} \def\mBeta{{\bm{\beta}}} \def\mPhi{{\bm{\Phi}}} \def\mLambda{{\bm{\Lambda}}} \def\mSigma{{\bm{\Sigma}}}    Tensor \DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl} \SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n} \newcommand{\tens}[1]{\bm{\mathsfit{#1}}} \def\tA{{\tens{A}}} \def\tB{{\tens{B}}} \def\tC{{\tens{C}}} \def\tD{{\tens{D}}} \def\tE{{\tens{E}}} \def\tF{{\tens{F}}} \def\tG{{\tens{G}}} \def\tH{{\tens{H}}} \def\tI{{\tens{I}}} \def\tJ{{\tens{J}}} \def\tK{{\tens{K}}} \def\tL{{\tens{L}}} \def\tM{{\tens{M}}} \def\tN{{\tens{N}}} \def\tO{{\tens{O}}} \def\tP{{\tens{P}}} \def\tQ{{\tens{Q}}} \def\tR{{\tens{R}}} \def\tS{{\tens{S}}} \def\tT{{\tens{T}}} \def\tU{{\tens{U}}} \def\tV{{\tens{V}}} \def\tW{{\tens{W}}} \def\tX{{\tens{X}}} \def\tY{{\tens{Y}}} \def\tZ{{\tens{Z}}}     Graph \def\gA{{\mathcal{A}}} \def\gB{{\mathcal{B}}} \def\gC{{\mathcal{C}}} \def\gD{{\mathcal{D}}} \def\gE{{\mathcal{E}}} \def\gF{{\mathcal{F}}} \def\gG{{\mathcal{G}}} \def\gH{{\mathcal{H}}} \def\gI{{\mathcal{I}}} \def\gJ{{\mathcal{J}}} \def\gK{{\mathcal{K}}} \def\gL{{\mathcal{L}}} \def\gM{{\mathcal{M}}} \def\gN{{\mathcal{N}}} \def\gO{{\mathcal{O}}} \def\gP{{\mathcal{P}}} \def\gQ{{\mathcal{Q}}} \def\gR{{\mathcal{R}}} \def\gS{{\mathcal{S}}} \def\gT{{\mathcal{T}}} \def\gU{{\mathcal{U}}} \def\gV{{\mathcal{V}}} \def\gW{{\mathcal{W}}} \def\gX{{\mathcal{X}}} \def\gY{{\mathcal{Y}}} \def\gZ{{\mathcal{Z}}}    Sets \def\sA{{\mathbb{A}}} \def\sB{{\mathbb{B}}} \def\sC{{\mathbb{C}}} \def\sD{{\mathbb{D}}}   Don't use a set called E, because this would be the same as our symbol   for expectation. \def\sF{{\mathbb{F}}} \def\sG{{\mathbb{G}}} \def\sH{{\mathbb{H}}} \def\sI{{\mathbb{I}}} \def\sJ{{\mathbb{J}}} \def\sK{{\mathbb{K}}} \def\sL{{\mathbb{L}}} \def\sM{{\mathbb{M}}} \def\sN{{\mathbb{N}}} \def\sO{{\mathbb{O}}} \def\sP{{\mathbb{P}}} \def\sQ{{\mathbb{Q}}} \def\sR{{\mathbb{R}}} \def\sS{{\mathbb{S}}} \def\sT{{\mathbb{T}}} \def\sU{{\mathbb{U}}} \def\sV{{\mathbb{V}}} \def\sW{{\mathbb{W}}} \def\sX{{\mathbb{X}}} \def\sY{{\mathbb{Y}}} \def\sZ{{\mathbb{Z}}}    Entries of a matrix \def\emLambda{{\Lambda}} \def\emA{{A}} \def\emB{{B}} \def\emC{{C}} \def\emD{{D}} \def\emE{{E}} \def\emF{{F}} \def\emG{{G}} \def\emH{{H}} \def\emI{{I}} \def\emJ{{J}} \def\emK{{K}} \def\emL{{L}} \def\emM{{M}} \def\emN{{N}} \def\emO{{O}} \def\emP{{P}} \def\emQ{{Q}} \def\emR{{R}} \def\emS{{S}} \def\emT{{T}} \def\emU{{U}} \def\emV{{V}} \def\emW{{W}} \def\emX{{X}} \def\emY{{Y}} \def\emZ{{Z}} \def\emSigma{{\Sigma}}    entries of a tensor   Same font as tensor, without \bm wrapper \newcommand{\etens}[1]{\mathsfit{#1}} \def\etLambda{{\etens{\Lambda}}} \def\etA{{\etens{A}}} \def\etB{{\etens{B}}} \def\etC{{\etens{C}}} \def\etD{{\etens{D}}} \def\etE{{\etens{E}}} \def\etF{{\etens{F}}} \def\etG{{\etens{G}}} \def\etH{{\etens{H}}} \def\etI{{\etens{I}}} \def\etJ{{\etens{J}}} \def\etK{{\etens{K}}} \def\etL{{\etens{L}}} \def\etM{{\etens{M}}} \def\etN{{\etens{N}}} \def\etO{{\etens{O}}} \def\etP{{\etens{P}}} \def\etQ{{\etens{Q}}} \def\etR{{\etens{R}}} \def\etS{{\etens{S}}} \def\etT{{\etens{T}}} \def\etU{{\etens{U}}} \def\etV{{\etens{V}}} \def\etW{{\etens{W}}} \def\etX{{\etens{X}}} \def\etY{{\etens{Y}}} \def\etZ{{\etens{Z}}}    The true underlying data generating distribution \newcommand{\pdata}{p_{\rm{data}}}   The empirical distribution defined by the training set \newcommand{\ptrain}{\hat{p}_{\rm{data}}} \newcommand{\Ptrain}{\hat{P}_{\rm{data}}}   The model distribution \newcommand{\pmodel}{p_{\rm{model}}} \newcommand{\Pmodel}{P_{\rm{model}}} \newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}   Stochastic autoencoder distributions \newcommand{\pencode}{p_{\rm{encoder}}} \newcommand{\pdecode}{p_{\rm{decoder}}} \newcommand{\precons}{p_{\rm{reconstruct}}}  \newcommand{\laplace}{\mathrm{Laplace}}   Laplace distribution  \newcommand{\E}{\mathbb{E}} \newcommand{\Ls}{\mathcal{L}} \newcommand{\R}{\mathbb{R}} \newcommand{\emp}{\tilde{p}} \newcommand{\lr}{\alpha} \newcommand{\reg}{\lambda} \newcommand{\rect}{\mathrm{rectifier}} \newcommand{\softmax}{\mathrm{softmax}} \newcommand{\sigmoid}{\sigma} \newcommand{\softplus}{\zeta} \newcommand{\KL}{D_{\mathrm{KL}}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\standarderror}{\mathrm{SE}} \newcommand{\Cov}{\mathrm{Cov}}   Wolfram Mathworld says  is for function spaces and  is for vectors   But then they seem to use  for vectors throughout the site, and so does   wikipedia. \newcommand{\normlzero}{L^0} \newcommand{\normlone}{L^1} \newcommand{\normltwo}{L^2} \newcommand{\normlp}{L^p} \newcommand{\normmax}{L^\infty}  \newcommand{\parents}{Pa}   See usage in notation.tex. Chosen to match Daphne's book.  \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}  \DeclareMathOperator{\sign}{sign} \DeclareMathOperator{\Tr}{Tr} \let\ab\allowbreak      We described MTLB-STRUCT, a semi-supervised system that is based on pre-trained BERT masked language modelling and that jointly learns VMWE tags and dependency parse trees. The system ranked first in the open track of the PARSEME shared task - edition 1.2 and shows the overall state-of-the-art performance for detecting unseen VMWEs.  In future, we plan to augment the dependency parsing architecture to train on dependency relation categories  as well as dependency arcs. We also plan to improve our system by making it more efficient in order to train the dependency parsing module on the extra available unannotated datasets.  
","      \gn{TBH: this seems pretty incomplete, there's a lot of work on error typologies for MT, evaluation for summarization, etc. that wasn't published in the past two years...}      \lz{This paragraph could be cut?} Neural sequence models are highly flexible, and do not have any inherent inductive bias to prevent them from generating outputs that are not entailed by the inputs. This flexibility can be an asset, it allows them to handle free-form text generation tasks like language modeling~, story generation~ or dialogue response generation~, where this strict entailment is not a prerequisite. However, for tasks like machine translation~ and text summarization~, where the semantic content of the output must be entirely reflective of the input, this is a major problem -- a user of the systems will not be able to trust that any generated text is an accurate reflection of the text they meant to translate or summarize. In addition, these problems are further exacerbated by the fact that most existing state-of-the-art models are trained using factuality-agnostic training algorithms such as maximum likelihood estimation~, and is especially prominent when models are trained on a domain outside of that they are tested on~.      \gn{True, but also probably true for any neural seq2seq system? I don't think there's any system where you can guarantee that it won't hallucinate content .} due to the discrepancies between training and inference time under domain shift  or vulnerabilities to artifacts or noises in the training data~ \gn{Are you sure it's actually due to this? For example, even if there's no domain shift and little or no noise during training I'm pretty sure that NMT or summarization systems would hallucinate content, especially if the training data is small. I would argue it's rather due to over-reliance on the decoder and its desire to generate fluent sentences at the cost of not being faithful to the input.}.         Commonly adopted metrics for sequence generations evaluations such as BLEU scores , ROUGE  and recently proposed quality estimation metrics including BLEURT  and BERTScores  have been shown that they correlate poorly with faithfulness~.   They also require reference text, limiting their applicability to detecting halluciations in a deployed system at run-time.    In addition, these metrics compute the similarity between the candidate outputs and the annotated references which requires the availability of annotations. However, assessing the faithfulness of generations is concerned with the safety of generation, which should be triggered before the machine outputs are exposed to users.    Recently, several efforts have been made on automatic evaluation of faithfulness and truthfulness in abstract text summarization~ by leveraging existing semantic inference systems:     question answering  system-based methods~ first employ a question generation system to generate natural questions based on the machine generated summary, then questions are answered with a QA system using the source article and the summary respectively. The final faithfulness score of the summary is given by comparing the corresponding answers.     For the textual entailment-based methods, a natural language inference  system is trained to predict whether the source article entails the summary or not. Then the entailment scores are used as the proxy to assess the faithfulness of the summary.   Nonetheless, these methods tailored for abstract text summarization are shown to have a weak correlation with human evaluation of faithfulness.     In addition, the absolute value of these metrics can not accurately assess how much hallucinations are within the summaries.    For example, in the QA-based methods it's difficult to generate the same number of questions for all the summaries, thus fair comparisons between different outputs can not be performed.      \gn{I looked around a bit more, and there is work on quality estimation that annotates span level errors, along with severity on three levels. For example, the Multidimensional Quality Metrics  standard , which was also used in Task 3 of the shared task on quality estimation . The severity of errors are classified as, for example ``An error can be minor , major  or critical .'' and also labeled with a type ``A label specifying the error type, such as wrong word order, missing words, agreement, etc. They may provide additional information, but systems don't need to predict them.'' I haven't looked closely at the annotations, but this almost seems a strict superset of what is annotated here.}    Similar to the spirit of our proposed task, word-level quality estimation~ in the machine translation community predicts if tokens are correctly translated based on human post-editing. However, they do not distinguish errors in terms of fluency and adequacy~.      A substitution error referring to a simple morphological variation  is considered the same as a content word substitution changing the meaning of the sentence.~.    In contrast to estimating the amount of human post-editing work required to fix errors, we specifically identify hallucinations corresponding to only hallucination  errors.      \gn{Even if the annotation standard of quality estimation is slightly different, the task of sequence labeling with both a sequence and the original input is the same. Why could we not just use a standard quality-estimation model out-of-the-box? And if we can, why should people use the method you propose below instead of other methods that have already proven to be good at the task?}        NEW MATH DEFINITIONS        \usepackage{amsmath,amsfonts,bm}    Mark sections of captions for referring to divisions of figures \newcommand{\figleft}{} \newcommand{\figcenter}{} \newcommand{\figright}{} \newcommand{\figtop}{} \newcommand{\figbottom}{} \newcommand{\captiona}{} \newcommand{\captionb}{} \newcommand{\captionc}{} \newcommand{\captiond}{}    Highlight a newly defined term \newcommand{\newterm}[1]{{\bf #1}}     Figure reference, lower-case. \def\figref#1{figure}   Figure reference, capital. For start of sentence \def\Figref#1{Figure} \def\twofigref#1#2{figures  and } \def\quadfigref#1#2#3#4{figures , ,  and }   Section reference, lower-case. \def\secref#1{section}   Section reference, capital. \def\Secref#1{Section}   Reference to two sections. \def\twosecrefs#1#2{sections  and }   Reference to three sections. \def\secrefs#1#2#3{sections ,  and }   Reference to an equation, lower-case. \def\eqref#1{equation}   Reference to an equation, upper case \def\Eqref#1{Equation}   A raw reference to an equation---avoid using if possible \def\plaineqref#1{}   Reference to a chapter, lower-case. \def\chapref#1{chapter}   Reference to an equation, upper case. \def\Chapref#1{Chapter}   Reference to a range of chapters \def\rangechapref#1#2{chapters--}   Reference to an algorithm, lower-case. \def\algref#1{algorithm}   Reference to an algorithm, upper case. \def\Algref#1{Algorithm} \def\twoalgref#1#2{algorithms  and } \def\Twoalgref#1#2{Algorithms  and }   Reference to a part, lower case \def\partref#1{part}   Reference to a part, upper case \def\Partref#1{Part} \def\twopartref#1#2{parts  and }  \def\ceil#1{\lceil #1 \rceil} \def\floor#1{\lfloor #1 \rfloor} \def\1{\bm{1}} \newcommand{\train}{\mathcal{D}} \newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}} \newcommand{\test}{\mathcal{D_{\mathrm{test}}}}  \def\eps{{\epsilon}}     Random variables \def\reta{{\textnormal{}}} \def\ra{{\textnormal{a}}} \def\rb{{\textnormal{b}}} \def\rc{{\textnormal{c}}} \def\rd{{\textnormal{d}}} \def\re{{\textnormal{e}}} \def\rf{{\textnormal{f}}} \def\rg{{\textnormal{g}}} \def\rh{{\textnormal{h}}} \def\ri{{\textnormal{i}}} \def\rj{{\textnormal{j}}} \def\rk{{\textnormal{k}}} \def\rl{{\textnormal{l}}}   rm is already a command, just don't name any random variables m \def\rn{{\textnormal{n}}} \def\ro{{\textnormal{o}}} \def\rp{{\textnormal{p}}} \def\rq{{\textnormal{q}}} \def\rr{{\textnormal{r}}} \def\rs{{\textnormal{s}}} \def\rt{{\textnormal{t}}} \def\ru{{\textnormal{u}}} \def\rv{{\textnormal{v}}} \def\rw{{\textnormal{w}}} \def\rx{{\textnormal{x}}} \def\ry{{\textnormal{y}}} \def\rz{{\textnormal{z}}}    Random vectors \def\rvepsilon{{\mathbf{\epsilon}}} \def\rvtheta{{\mathbf{\theta}}} \def\rva{{\mathbf{a}}} \def\rvb{{\mathbf{b}}} \def\rvc{{\mathbf{c}}} \def\rvd{{\mathbf{d}}} \def\rve{{\mathbf{e}}} \def\rvf{{\mathbf{f}}} \def\rvg{{\mathbf{g}}} \def\rvh{{\mathbf{h}}} \def\rvu{{\mathbf{i}}} \def\rvj{{\mathbf{j}}} \def\rvk{{\mathbf{k}}} \def\rvl{{\mathbf{l}}} \def\rvm{{\mathbf{m}}} \def\rvn{{\mathbf{n}}} \def\rvo{{\mathbf{o}}} \def\rvp{{\mathbf{p}}} \def\rvq{{\mathbf{q}}} \def\rvr{{\mathbf{r}}} \def\rvs{{\mathbf{s}}} \def\rvt{{\mathbf{t}}} \def\rvu{{\mathbf{u}}} \def\rvv{{\mathbf{v}}} \def\rvw{{\mathbf{w}}} \def\rvx{{\mathbf{x}}} \def\rvy{{\mathbf{y}}} \def\rvz{{\mathbf{z}}}    Elements of random vectors \def\erva{{\textnormal{a}}} \def\ervb{{\textnormal{b}}} \def\ervc{{\textnormal{c}}} \def\ervd{{\textnormal{d}}} \def\erve{{\textnormal{e}}} \def\ervf{{\textnormal{f}}} \def\ervg{{\textnormal{g}}} \def\ervh{{\textnormal{h}}} \def\ervi{{\textnormal{i}}} \def\ervj{{\textnormal{j}}} \def\ervk{{\textnormal{k}}} \def\ervl{{\textnormal{l}}} \def\ervm{{\textnormal{m}}} \def\ervn{{\textnormal{n}}} \def\ervo{{\textnormal{o}}} \def\ervp{{\textnormal{p}}} \def\ervq{{\textnormal{q}}} \def\ervr{{\textnormal{r}}} \def\ervs{{\textnormal{s}}} \def\ervt{{\textnormal{t}}} \def\ervu{{\textnormal{u}}} \def\ervv{{\textnormal{v}}} \def\ervw{{\textnormal{w}}} \def\ervx{{\textnormal{x}}} \def\ervy{{\textnormal{y}}} \def\ervz{{\textnormal{z}}}    Random matrices \def\rmA{{\mathbf{A}}} \def\rmB{{\mathbf{B}}} \def\rmC{{\mathbf{C}}} \def\rmD{{\mathbf{D}}} \def\rmE{{\mathbf{E}}} \def\rmF{{\mathbf{F}}} \def\rmG{{\mathbf{G}}} \def\rmH{{\mathbf{H}}} \def\rmI{{\mathbf{I}}} \def\rmJ{{\mathbf{J}}} \def\rmK{{\mathbf{K}}} \def\rmL{{\mathbf{L}}} \def\rmM{{\mathbf{M}}} \def\rmN{{\mathbf{N}}} \def\rmO{{\mathbf{O}}} \def\rmP{{\mathbf{P}}} \def\rmQ{{\mathbf{Q}}} \def\rmR{{\mathbf{R}}} \def\rmS{{\mathbf{S}}} \def\rmT{{\mathbf{T}}} \def\rmU{{\mathbf{U}}} \def\rmV{{\mathbf{V}}} \def\rmW{{\mathbf{W}}} \def\rmX{{\mathbf{X}}} \def\rmY{{\mathbf{Y}}} \def\rmZ{{\mathbf{Z}}}    Elements of random matrices \def\ermA{{\textnormal{A}}} \def\ermB{{\textnormal{B}}} \def\ermC{{\textnormal{C}}} \def\ermD{{\textnormal{D}}} \def\ermE{{\textnormal{E}}} \def\ermF{{\textnormal{F}}} \def\ermG{{\textnormal{G}}} \def\ermH{{\textnormal{H}}} \def\ermI{{\textnormal{I}}} \def\ermJ{{\textnormal{J}}} \def\ermK{{\textnormal{K}}} \def\ermL{{\textnormal{L}}} \def\ermM{{\textnormal{M}}} \def\ermN{{\textnormal{N}}} \def\ermO{{\textnormal{O}}} \def\ermP{{\textnormal{P}}} \def\ermQ{{\textnormal{Q}}} \def\ermR{{\textnormal{R}}} \def\ermS{{\textnormal{S}}} \def\ermT{{\textnormal{T}}} \def\ermU{{\textnormal{U}}} \def\ermV{{\textnormal{V}}} \def\ermW{{\textnormal{W}}} \def\ermX{{\textnormal{X}}} \def\ermY{{\textnormal{Y}}} \def\ermZ{{\textnormal{Z}}}    Vectors \def\vzero{{\bm{0}}} \def\vone{{\bm{1}}} \def\vmu{{\bm{\mu}}} \def\vtheta{{\bm{\theta}}} \def\va{{\bm{a}}} \def\vb{{\bm{b}}} \def\vc{{\bm{c}}} \def\vd{{\bm{d}}} \def\ve{{\bm{e}}} \def\vf{{\bm{f}}} \def\vg{{\bm{g}}} \def\vh{{\bm{h}}} \def\vi{{\bm{i}}} \def\vj{{\bm{j}}} \def\vk{{\bm{k}}} \def\vl{{\bm{l}}} \def\vm{{\bm{m}}} \def\vn{{\bm{n}}} \def\vo{{\bm{o}}} \def\vp{{\bm{p}}} \def\vq{{\bm{q}}} \def\vr{{\bm{r}}} \def\vs{{\bm{s}}} \def\vt{{\bm{t}}} \def\vu{{\bm{u}}} \def\vv{{\bm{v}}} \def\vw{{\bm{w}}} \def\vx{{\bm{x}}} \def\vy{{\bm{y}}} \def\vz{{\bm{z}}}    Elements of vectors \def\evalpha{{\alpha}} \def\evbeta{{\beta}} \def\evepsilon{{\epsilon}} \def\evlambda{{\lambda}} \def\evomega{{\omega}} \def\evmu{{\mu}} \def\evpsi{{\psi}} \def\evsigma{{\sigma}} \def\evtheta{{\theta}} \def\eva{{a}} \def\evb{{b}} \def\evc{{c}} \def\evd{{d}} \def\eve{{e}} \def\evf{{f}} \def\evg{{g}} \def\evh{{h}} \def\evi{{i}} \def\evj{{j}} \def\evk{{k}} \def\evl{{l}} \def\evm{{m}} \def\evn{{n}} \def\evo{{o}} \def\evp{{p}} \def\evq{{q}} \def\evr{{r}} \def\evs{{s}} \def\evt{{t}} \def\evu{{u}} \def\evv{{v}} \def\evw{{w}} \def\evx{{x}} \def\evy{{y}} \def\evz{{z}}    Matrix \def\mA{{\bm{A}}} \def\mB{{\bm{B}}} \def\mC{{\bm{C}}} \def\mD{{\bm{D}}} \def\mE{{\bm{E}}} \def\mF{{\bm{F}}} \def\mG{{\bm{G}}} \def\mH{{\bm{H}}} \def\mI{{\bm{I}}} \def\mJ{{\bm{J}}} \def\mK{{\bm{K}}} \def\mL{{\bm{L}}} \def\mM{{\bm{M}}} \def\mN{{\bm{N}}} \def\mO{{\bm{O}}} \def\mP{{\bm{P}}} \def\mQ{{\bm{Q}}} \def\mR{{\bm{R}}} \def\mS{{\bm{S}}} \def\mT{{\bm{T}}} \def\mU{{\bm{U}}} \def\mV{{\bm{V}}} \def\mW{{\bm{W}}} \def\mX{{\bm{X}}} \def\mY{{\bm{Y}}} \def\mZ{{\bm{Z}}} \def\mBeta{{\bm{\beta}}} \def\mPhi{{\bm{\Phi}}} \def\mLambda{{\bm{\Lambda}}} \def\mSigma{{\bm{\Sigma}}}    Tensor \DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl} \SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n} \newcommand{\tens}[1]{\bm{\mathsfit{#1}}} \def\tA{{\tens{A}}} \def\tB{{\tens{B}}} \def\tC{{\tens{C}}} \def\tD{{\tens{D}}} \def\tE{{\tens{E}}} \def\tF{{\tens{F}}} \def\tG{{\tens{G}}} \def\tH{{\tens{H}}} \def\tI{{\tens{I}}} \def\tJ{{\tens{J}}} \def\tK{{\tens{K}}} \def\tL{{\tens{L}}} \def\tM{{\tens{M}}} \def\tN{{\tens{N}}} \def\tO{{\tens{O}}} \def\tP{{\tens{P}}} \def\tQ{{\tens{Q}}} \def\tR{{\tens{R}}} \def\tS{{\tens{S}}} \def\tT{{\tens{T}}} \def\tU{{\tens{U}}} \def\tV{{\tens{V}}} \def\tW{{\tens{W}}} \def\tX{{\tens{X}}} \def\tY{{\tens{Y}}} \def\tZ{{\tens{Z}}}     Graph \def\gA{{\mathcal{A}}} \def\gB{{\mathcal{B}}} \def\gC{{\mathcal{C}}} \def\gD{{\mathcal{D}}} \def\gE{{\mathcal{E}}} \def\gF{{\mathcal{F}}} \def\gG{{\mathcal{G}}} \def\gH{{\mathcal{H}}} \def\gI{{\mathcal{I}}} \def\gJ{{\mathcal{J}}} \def\gK{{\mathcal{K}}} \def\gL{{\mathcal{L}}} \def\gM{{\mathcal{M}}} \def\gN{{\mathcal{N}}} \def\gO{{\mathcal{O}}} \def\gP{{\mathcal{P}}} \def\gQ{{\mathcal{Q}}} \def\gR{{\mathcal{R}}} \def\gS{{\mathcal{S}}} \def\gT{{\mathcal{T}}} \def\gU{{\mathcal{U}}} \def\gV{{\mathcal{V}}} \def\gW{{\mathcal{W}}} \def\gX{{\mathcal{X}}} \def\gY{{\mathcal{Y}}} \def\gZ{{\mathcal{Z}}}    Sets \def\sA{{\mathbb{A}}} \def\sB{{\mathbb{B}}} \def\sC{{\mathbb{C}}} \def\sD{{\mathbb{D}}}   Don't use a set called E, because this would be the same as our symbol   for expectation. \def\sF{{\mathbb{F}}} \def\sG{{\mathbb{G}}} \def\sH{{\mathbb{H}}} \def\sI{{\mathbb{I}}} \def\sJ{{\mathbb{J}}} \def\sK{{\mathbb{K}}} \def\sL{{\mathbb{L}}} \def\sM{{\mathbb{M}}} \def\sN{{\mathbb{N}}} \def\sO{{\mathbb{O}}} \def\sP{{\mathbb{P}}} \def\sQ{{\mathbb{Q}}} \def\sR{{\mathbb{R}}} \def\sS{{\mathbb{S}}} \def\sT{{\mathbb{T}}} \def\sU{{\mathbb{U}}} \def\sV{{\mathbb{V}}} \def\sW{{\mathbb{W}}} \def\sX{{\mathbb{X}}} \def\sY{{\mathbb{Y}}} \def\sZ{{\mathbb{Z}}}    Entries of a matrix \def\emLambda{{\Lambda}} \def\emA{{A}} \def\emB{{B}} \def\emC{{C}} \def\emD{{D}} \def\emE{{E}} \def\emF{{F}} \def\emG{{G}} \def\emH{{H}} \def\emI{{I}} \def\emJ{{J}} \def\emK{{K}} \def\emL{{L}} \def\emM{{M}} \def\emN{{N}} \def\emO{{O}} \def\emP{{P}} \def\emQ{{Q}} \def\emR{{R}} \def\emS{{S}} \def\emT{{T}} \def\emU{{U}} \def\emV{{V}} \def\emW{{W}} \def\emX{{X}} \def\emY{{Y}} \def\emZ{{Z}} \def\emSigma{{\Sigma}}    entries of a tensor   Same font as tensor, without \bm wrapper \newcommand{\etens}[1]{\mathsfit{#1}} \def\etLambda{{\etens{\Lambda}}} \def\etA{{\etens{A}}} \def\etB{{\etens{B}}} \def\etC{{\etens{C}}} \def\etD{{\etens{D}}} \def\etE{{\etens{E}}} \def\etF{{\etens{F}}} \def\etG{{\etens{G}}} \def\etH{{\etens{H}}} \def\etI{{\etens{I}}} \def\etJ{{\etens{J}}} \def\etK{{\etens{K}}} \def\etL{{\etens{L}}} \def\etM{{\etens{M}}} \def\etN{{\etens{N}}} \def\etO{{\etens{O}}} \def\etP{{\etens{P}}} \def\etQ{{\etens{Q}}} \def\etR{{\etens{R}}} \def\etS{{\etens{S}}} \def\etT{{\etens{T}}} \def\etU{{\etens{U}}} \def\etV{{\etens{V}}} \def\etW{{\etens{W}}} \def\etX{{\etens{X}}} \def\etY{{\etens{Y}}} \def\etZ{{\etens{Z}}}    The true underlying data generating distribution \newcommand{\pdata}{p_{\rm{data}}}   The empirical distribution defined by the training set \newcommand{\ptrain}{\hat{p}_{\rm{data}}} \newcommand{\Ptrain}{\hat{P}_{\rm{data}}}   The model distribution \newcommand{\pmodel}{p_{\rm{model}}} \newcommand{\Pmodel}{P_{\rm{model}}} \newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}   Stochastic autoencoder distributions \newcommand{\pencode}{p_{\rm{encoder}}} \newcommand{\pdecode}{p_{\rm{decoder}}} \newcommand{\precons}{p_{\rm{reconstruct}}}  \newcommand{\laplace}{\mathrm{Laplace}}   Laplace distribution  \newcommand{\E}{\mathbb{E}} \newcommand{\Ls}{\mathcal{L}} \newcommand{\R}{\mathbb{R}} \newcommand{\emp}{\tilde{p}} \newcommand{\lr}{\alpha} \newcommand{\reg}{\lambda} \newcommand{\rect}{\mathrm{rectifier}} \newcommand{\softmax}{\mathrm{softmax}} \newcommand{\sigmoid}{\sigma} \newcommand{\softplus}{\zeta} \newcommand{\KL}{D_{\mathrm{KL}}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\standarderror}{\mathrm{SE}} \newcommand{\Cov}{\mathrm{Cov}}   Wolfram Mathworld says  is for function spaces and  is for vectors   But then they seem to use  for vectors throughout the site, and so does   wikipedia. \newcommand{\normlzero}{L^0} \newcommand{\normlone}{L^1} \newcommand{\normltwo}{L^2} \newcommand{\normlp}{L^p} \newcommand{\normmax}{L^\infty}  \newcommand{\parents}{Pa}   See usage in notation.tex. Chosen to match Daphne's book.  \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}  \DeclareMathOperator{\sign}{sign} \DeclareMathOperator{\Tr}{Tr} \let\ab\allowbreak",152
"  With rise in social media and e-commerce websites, there is a huge interest in analyzing these networks for tasks like link prediction, recommendation, community detection, etc. Traditionally, this is done by learning finite-dimensional vector embeddings/representations  for nodes in these networks and then used it for downstream tasks. One of the challenges is that the quality of these learned representation decreases if the network has many missing links. This affects its performance in downstream tasks. This can be addressed by using attribute similarity of nodes as connected usually have similar attributes. For example, in citation networks, papers on related works will cite each other, and in social media, people with similar interest follow each other. In real-world graphs, nodes of these networks themselves contain rich textual information  as attributes. So, we need techniques which can exploit this textual information while learning node embeddings. The representation learning of textual networks deals with this problem.   \iffalse While networks are sources of relational information, in many practical scenarios, nodes of networks themselves contain rich information as attributes. When this data is in the form of text  these networks are referred to as textual networks, and representation learning of these networks has several applications in diverse fields from analyzing social media profiles to biomedical networks. One of the challenges in this problem is that the quality of these learned representation decreases if the network has many missing links. This can be addressed by using attribute similarity of nodes as connected usually have similar attributes. For example, in citation networks, papers on related works will cite each other, and in social media, people with similar interest follow each other. So, by exploiting this, one can predict the edges in the network.   The main aim of representation learning of network is to learn embeddings, which are finite-dimensional vector representations for nodes in the graph.   %Representation learning in networks uses the edge/link weights as labels in the objective function to learn embeddings. These are finite-dimensional vector representations for each node in the graph.  In this paper we study this problem for textual networks, where nodes of the networks are equipped with attributes or content in the form of textual information . These learned embeddings can then be used in problems like link prediction, community detection, social network analysis, and so on. One of the challenges in this problem is that the quality of these learned representation decreases if the network has many missing links. This can be addressed by using attribute similarity of nodes as connected usually have similar attributes. For example, in citation networks, papers on related works will cite each other, and in social media, people with similar interest follow each other. So, by exploiting this, one can predict the edges in the network. %For achieving this in representation learning of textual networks, we propose an adversarial framework using textual similarity for discriminator and structural similarity for generator.  \fi  Recent methods for representation learning of textual networks involves learning two embeddings, one for the structure information , and the other for the textual information . The embeddings are learned to be similar for nodes that are connected by an edge. The most challenging task is to learn the combined text and structure embeddings, all the previous approaches  use a joint learning framework by defining a loss function that models the inter-modal similarities between structure and textual information between nodes connected by an edge, in addition to the intra-modal similarities. For example, consider the nodes  and  with their embeddings   and . The similarity between embeddings  and  is used for modelling intra-model similarity in structure information, on the other hand the similarity between  and  is used for intra-model similarity in text information. For inter-model similarity, the similarity between  and  is used for modelling the similarity between structure and text, and vice versa. All these similarities are modelled using skip-gram loss function .   The main disadvantage of these models is that they dependent on edge labels for embedding learning. This will make them unable to learn embeddings of nodes which are not present during the training stage. The only way they can be modelled to learn unseen nodes embeddings is by a mapper function between textual information and structure embeddings on seen nodes and apply it to unseen nodes for getting structure embeddings. This can result in a poor performance in downstream tasks involving unseen nodes as the mapping function cannot fully capture the structural information in the nodes. Recenlty, this issue has been addressed by using variational autoencoder framework on the structure and text embeddings. Although it has achieved better performance than the mapper function-based models, the disadvantage of autoencoder framework is that it limits the information learned in the structure embeddings as it is used for predicting the text features by the decoder.  In this paper, we propose an adversarial model where the generator learns the structure embeddings and between text embedding based discriminator and structure embeddings based generator.  For generator, we use the supervision from edge-connectivity and text embedding similarity to learn the structure embeddings. For discriminator model, text embeddings are made dissimilar for node pair generated by the generator and similar for the node pairs from the graph. This training will make the text similarity from the discriminator to approximate the actual similarity in the network. Through this framework we establish that this model efficiently amalgamate or fuse information from both text and graph as both text and structure embeddings use information from both modality for learning. In addition to this, our proposed adversarial approach can be extended for embedding learning of unseen nodes in the training dataset. This is achieved by directly using discriminator based text-similarity as supervision in a post-training stage. This will help in efficiently learning unseen structure embeddings as it does not restrict the embedding learning by using it to predict the text features like in VHE .  The performance of the model depends upon how well we can exploit the unstructured textual information, so we need a powerful discriminator. To achieve this, we use context-aware embeddings, where a node has different text embedding for each of its edges. We address this problem by proposing a novel technique by combining two context-aware attention mechanism. The first is based on mutual attention  between word embeddings in text across a pair of nodes.  The other is a topological attention mechanism. This uses structure embeddings of a node pairs to attend over text to learn a topology-aware text embedding. It can reduce the adverse effects of trying to make text embeddings similar where the textual information of connected nodes need not match. Because, this model has better representation capacity as it learns similarity through topological and mutual attention.    The following are the main contributions of this paper.  An adversarial technique for attributed network representation learning. Here, in addition to the supervision from training data, a discriminator using text embeddings is used to give supervision to structure embeddings.  A novel text embedding learning technique which uses both mutual and topological attention.  Extensive comparative study on downstream tasks of link prediction and node classification.  Experiments on link prediction on unseen nodes.    \iffalse We have evaluated our proposed method on three datasets Cora, Zhihu, and Hepth for link prediction. We observed that our model performs better than state-of-the-art methods in almost all settings in all three datasets. The performance of our model is especially high in low data regime. In Zhihu dataset, our model show a performance improvement of  over the previous state-of-the-art in the lowest supervision setting. A similar observation was made on the node classification task on Cora dataset, where our adversarial technique achieve state-of-the-art performance. As we mentioned earlier, the main advantage of this model is its ability to the care of representation learning in unseen nodes. We evaluated the quality of these embeddings in link prediction task for edges involving unseen nodes, and ACNE achieves state-of-the-art performance for all settings in all three datasets. On Zhihu dataset, it gave an impressive improvement of   improvement over previous methods in the low-data regime.  \fi  \iffalse  \fi  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    The techniques for textual network representation can be categorized into two kinds. The first uses fixed embeddings for representing textual information. These include models like TADW , which uses matrix factorization to learn text embeddings, and models like  Tri-DNR , CENE , and DMTE , which uses a skip-gram model to learn text embeddings.   The second kind of models use context-aware embeddings. Recent state-of-the-art models are on this line of work. CANE  uses a mutual attention mechanism to learn affinity/similarity matrix between text features across a node pair. That is then used for finding context-aware embeddings by  weighted average of the text features. WANE  uses a mutual attention mechanism for aligning word embeddings of the textual content in one node to the other. Then pooling is applied to find the final embedding.  GANE  uses optimal transport framework for attention instead of dot product for finding the affinity matrix between words in a node-pair. NEIFA  uses gating mechanism for getting context-aware embedding and also it uses gating for extracting information from structure embedding that is complementary to textual information. VHE  uses a variational framework for learning context-aware embedding using a mutual attention mechanism.   Apart from previously discussed methods on textual networks, GraphGAN  used for representation learning in homogeneous networks uses a similar adversarial framework as ours by using generator and discriminator networks. Here, both generator and discriminator separately learn embeddings for a node, but this model uses only the structural information. So, the performance of this model will be poor in the case of representation learning of sparse textual networks.  \iffalse Unlike previous work, the model we propose uses an adversarial framework between text embedding based discriminator and structure embedding based generator to learn efficient representations. This will help in combining information from both text and graph modalities efficiently. Another advantage of our model is that it can extend to unseen nodes using the text-similarity based supervision from the definition of the adversarial framework. Also, our new architecture for learning text embedding can combine both mutual attention and topological attention mechanism, which resulted in more flexible text embeddings.    \paragraph{Generative Adversarial Networks .} Recently, there is a huge interest in using GAN  for network representation learning because of its success in domains like image generation , dialogue generation , etc. These are unsupervisedly trained models used for generating synthetic samples from the distribution same as that of the training data.  For that, they use a generator network for generating the data and discriminator network for predicting whether samples are from the true distribution or the generator. The learning is done by exploiting a game-theoretic minimax optimization between discriminator and generator. GraphGAN  is one such work where GAN technique is used for representation learning in homogeneous networks. It learns a generator that samples possible neighbours of a node and a discriminator for predicting links between a node and its sampled neighbour. Both generator and discriminator separately learn embeddings for a node, and the training of each model is done by alternating applying gradient steps on them. This model uses only the structural information and ignore all the textual data in the nodes. So, the performance of this model will be poor in the case of representation learning of sparse textual networks. \fi                                                                                                                                                                                                                                  In this work, we proposed a new evaluation task for hallucination detection in conditional sequence generation and created human-annotated benchmark datasets. We also proposed a novel and general-purpose method to learn this task, and showed that the models can be used to define fine grained losses that improve low resource models for machine translation. In the future, we hope to create a large-scale pretrained evaluation model for any datasets or models to be evaluated, and also would extend our method to data-to-text generation scenarios.   We are also interested in investigating how to leverage our detection methods to mitigate hallucination problems in conditional sequence generation.   
","  The techniques for textual network representation can be categorized into two kinds. The first uses fixed embeddings for representing textual information. These include models like TADW , which uses matrix factorization to learn text embeddings, and models like  Tri-DNR , CENE , and DMTE , which uses a skip-gram model to learn text embeddings.   The second kind of models use context-aware embeddings. Recent state-of-the-art models are on this line of work. CANE  uses a mutual attention mechanism to learn affinity/similarity matrix between text features across a node pair. That is then used for finding context-aware embeddings by  weighted average of the text features. WANE  uses a mutual attention mechanism for aligning word embeddings of the textual content in one node to the other. Then pooling is applied to find the final embedding.  GANE  uses optimal transport framework for attention instead of dot product for finding the affinity matrix between words in a node-pair. NEIFA  uses gating mechanism for getting context-aware embedding and also it uses gating for extracting information from structure embedding that is complementary to textual information. VHE  uses a variational framework for learning context-aware embedding using a mutual attention mechanism.   Apart from previously discussed methods on textual networks, GraphGAN  used for representation learning in homogeneous networks uses a similar adversarial framework as ours by using generator and discriminator networks. Here, both generator and discriminator separately learn embeddings for a node, but this model uses only the structural information. So, the performance of this model will be poor in the case of representation learning of sparse textual networks.  \iffalse Unlike previous work, the model we propose uses an adversarial framework between text embedding based discriminator and structure embedding based generator to learn efficient representations. This will help in combining information from both text and graph modalities efficiently. Another advantage of our model is that it can extend to unseen nodes using the text-similarity based supervision from the definition of the adversarial framework. Also, our new architecture for learning text embedding can combine both mutual attention and topological attention mechanism, which resulted in more flexible text embeddings.    \paragraph{Generative Adversarial Networks .} Recently, there is a huge interest in using GAN  for network representation learning because of its success in domains like image generation , dialogue generation , etc. These are unsupervisedly trained models used for generating synthetic samples from the distribution same as that of the training data.  For that, they use a generator network for generating the data and discriminator network for predicting whether samples are from the true distribution or the generator. The learning is done by exploiting a game-theoretic minimax optimization between discriminator and generator. GraphGAN  is one such work where GAN technique is used for representation learning in homogeneous networks. It learns a generator that samples possible neighbours of a node and a discriminator for predicting links between a node and its sampled neighbour. Both generator and discriminator separately learn embeddings for a node, and the training of each model is done by alternating applying gradient steps on them. This model uses only the structural information and ignore all the textual data in the nodes. So, the performance of this model will be poor in the case of representation learning of sparse textual networks. \fi",153
"  Streaming Automatic Speech Recognition  researches have made their way into our everyday products. Smart speakers can now transcribe utterances in a streaming fashion, allowing users and downstream applications to see instant output in terms of partial transcriptions. There is a growing interest in the community to develop end-to-end  streaming ASR models, because they can transcribe accurately and run compactly on edge devices. Amongst these streaming E2E models, Recurrent Neural Network Transducer  is a candidate for many applications. RNN-T is trained with a loss function that does not enforce on the temporal alignment of the training transcripts and audio. As a result, RNN-T suffers from token emission delays - time from when the token is spoken to when the transcript of the token is emitted. Delayed emissions of tokens adversely affects user experiences and downstream applications such as the end-pointer.   Some existing work tried to mitigate the token emission delays in streaming RNN-Ts. We introduce them in Section. Other works utilized semi-streaming or non-streaming models to predict better token emission time, at the cost of the overall latency of the transcripts. In this work, we propose a novel loss function for streaming RNN-T, and the resultant trained model is called Alignment Restricted RNN-T . It utilizes audio-text alignment information to guide the loss computation. In Section, we show that theoretically, Ar-RNN-T loss function is faster to compute and results in better audio-token alignment. In Section, we empirically compare our proposed method with existing works such as monotonic RNN-T training on two data set: LibriSpeech and voice command. In the results section, Section, we show improvement in training speed and that when used in tandem with an end-pointer, Ar-RNN-T provides an unprecedentedly refined control over the latency-WER trade-offs of RNN-T models.       First talk about RNN-T. The RNN-T model consists of an encoder, a predictor and a joint network. The encoder processes incoming audio acoustic features and the predictor processes the previously emitted tokens. The joint representation of the encoder and predictor is then fed into the joint network to predict the next likely token or a `blank' symbol. Section explains the loss function used to train the standard RNN-T model. Our work uses an RNN-T architecture setting similar to the RNN-T presented in.   Several works have observed that the RNN-T loss function does not enforce alignment between audio and token emission time. Tripathi et al. noticed that the RNN-T models sometimes do not emit anything for a while before outputting multiple tokens all together. To mitigate audio-token timing misalignment, the authors proposed Mono-RNN-T models that requires the model to output at most one label at a frame. Similarly, Sak et al. proposed Neural Network Aligner  to train RNN models that do not output multiple labels without processing one input vector. The HAT variant of the RNN-T loss function is also proposed to introduce an acoustic-language modality separation in the ASR pipeline. In all these works, the exact alignment between spoken token and emitted token is implicitly modeled. Our proposed Ar-RNN-T training imposes more restriction to the audio-token and emission token alignment than the above-mentioned RNN-T variants. By leveraging audio-text alignment, we strictly enforce that the intervals between emitted tokens should be bound by a reasonable span of time.\\ \indent Zeyer et al. also explored the idea of incorporating external alignment and using approximations to compute the RNN-T loss function efficiently. They reported training time and WER of non-streaming bidirectional-LSTM  models with their approximation framework, whereas we focus on training streaming RNN-T models, constructed with single-directional LSTMs. Our empirical results show -- similar to what has discovered -- that the BLSTM-based RNN-T models with more future audio context behave differently from the single-directional LSTM-based RNN-T models in their token emission delays. We analyze the time emission delay of each token, as well as the end-pointing effects of the Ar-RNN-T training, giving users a detailed trade-off between WER and token emissions delays.\\ \indent Token emission delays are also detrimental for the RNN-T model to decide when to end-point. Li et al. trained the RNN-T to predict a special end-of-query symbol , , to aid the end-pointer. They added penalty losses over the early and late emissions of the  token to ensure that it is emitted at the right time. Their idea of early-emission and late-emission penalties is similar to Ar-RNN-T loss function. However, instead of applying these penalties only to the  token, Ar-RNN-T imposes such penalty to all tokens. \\ \indent Other works introduced audio-text alignment information into the RNN-T encoder by incorporating an additional alignment restrictive loss function into model pre-training processes. In, for instance, Hu et al. used the Cross Entropy  loss function to pre-train the RNN-T encoder. Instead of imposing alignment in the pre-training step, our work directly enforces the alignment during RNN-T training. \\ \indent Besides RNN-T, there exist works sharing similar inspirations to tackle delayed emission problem for other types of model loss functions. Senior et al. investigated applying constraints on the Connectionist Temporal Classification  alignment to reduce the latency in decoding CTC based models by limiting the set of search paths used in the forward-backward algorithm and Povey et al. used lattice-based technique for constraining phone emission time. Our algorithm applies to the RNN-T loss lattice to align word-piece emission times.               include your own bib file like this: 
","   First talk about RNN-T. The RNN-T model consists of an encoder, a predictor and a joint network. The encoder processes incoming audio acoustic features and the predictor processes the previously emitted tokens. The joint representation of the encoder and predictor is then fed into the joint network to predict the next likely token or a `blank' symbol. Section explains the loss function used to train the standard RNN-T model. Our work uses an RNN-T architecture setting similar to the RNN-T presented in.   Several works have observed that the RNN-T loss function does not enforce alignment between audio and token emission time. Tripathi et al. noticed that the RNN-T models sometimes do not emit anything for a while before outputting multiple tokens all together. To mitigate audio-token timing misalignment, the authors proposed Mono-RNN-T models that requires the model to output at most one label at a frame. Similarly, Sak et al. proposed Neural Network Aligner  to train RNN models that do not output multiple labels without processing one input vector. The HAT variant of the RNN-T loss function is also proposed to introduce an acoustic-language modality separation in the ASR pipeline. In all these works, the exact alignment between spoken token and emitted token is implicitly modeled. Our proposed Ar-RNN-T training imposes more restriction to the audio-token and emission token alignment than the above-mentioned RNN-T variants. By leveraging audio-text alignment, we strictly enforce that the intervals between emitted tokens should be bound by a reasonable span of time.\\ \indent Zeyer et al. also explored the idea of incorporating external alignment and using approximations to compute the RNN-T loss function efficiently. They reported training time and WER of non-streaming bidirectional-LSTM  models with their approximation framework, whereas we focus on training streaming RNN-T models, constructed with single-directional LSTMs. Our empirical results show -- similar to what has discovered -- that the BLSTM-based RNN-T models with more future audio context behave differently from the single-directional LSTM-based RNN-T models in their token emission delays. We analyze the time emission delay of each token, as well as the end-pointing effects of the Ar-RNN-T training, giving users a detailed trade-off between WER and token emissions delays.\\ \indent Token emission delays are also detrimental for the RNN-T model to decide when to end-point. Li et al. trained the RNN-T to predict a special end-of-query symbol , , to aid the end-pointer. They added penalty losses over the early and late emissions of the  token to ensure that it is emitted at the right time. Their idea of early-emission and late-emission penalties is similar to Ar-RNN-T loss function. However, instead of applying these penalties only to the  token, Ar-RNN-T imposes such penalty to all tokens. \\ \indent Other works introduced audio-text alignment information into the RNN-T encoder by incorporating an additional alignment restrictive loss function into model pre-training processes. In, for instance, Hu et al. used the Cross Entropy  loss function to pre-train the RNN-T encoder. Instead of imposing alignment in the pre-training step, our work directly enforces the alignment during RNN-T training. \\ \indent Besides RNN-T, there exist works sharing similar inspirations to tackle delayed emission problem for other types of model loss functions. Senior et al. investigated applying constraints on the Connectionist Temporal Classification  alignment to reduce the latency in decoding CTC based models by limiting the set of search paths used in the forward-backward algorithm and Povey et al. used lattice-based technique for constraining phone emission time. Our algorithm applies to the RNN-T loss lattice to align word-piece emission times.",154
" .     %       % final paper: en-us version         % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. } Discourse parsing is an important upstream task within the area of Natural Language Processing   which has been an active field of research over the last decades. In this work, we focus on discourse representations for the English language, where most research %on the discourse analysis of English language  has been surrounding one of the two main theories behind discourse, the Rhetorical Structure Theory  proposed by  or interpreting discourse according to PDTB . While both theories have their strengths, the application of the RST theory, encoding documents into complete constituency discourse trees , has been shown to have many crucial implications on real world problems. A tree is defined on a set of EDUs , approximately aligning with clause-like sentence fragments, acting as the leaves of the tree. Adjacent EDUs or sub-trees are hierarchically aggregated to form larger  constituents, with internal nodes containing  a nuclearity label, defining the importance of the subtree  in the local context and  a relation label, defining the type of semantic connection between the two subtrees . In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization . More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular  BERT approach . Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis .  Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically inspired approaches for downstream applications has not been unleashed yet. The main open challenges of integrating discourse into more NLP downstream tasks and to deliver even greater benefits have been a combination of  discourse parsing being a difficult task itself, with an inherently high degree of ambiguity and uncertainty and  the lack of large-scale annotated datasets, rendering the initial problem more severe, as data-driven approaches cannot be applied to their full potential.  The combination of these two limitations has been one of the main reasons for the limited application of neural discourse parsing for more diverse downstream tasks. While there have been neural discourse parsers proposed , they still cannot consistently %strongly  outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches.  %due to the extra effort to integrate discourse trees into models as well as two major problems, the big breakthrough in the usage of discourse parsing has still not happened.   In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT ``silver-standard"" discourse treebank published by  containing over 250,000 discourse annotated documents from the Yelp'13 sentiment dataset , nearly three orders of magnitude larger than commonly used RST-style annotated discourse treebanks . Given this new dataset with a previously unseen number of full RST-style discourse trees, we revisit the task of neural discourse parsing, which has been previously attempted by   and others with rather limited success. We believe that one reason why previous neural models could not yet consistently outperform more traditional approaches, heavily relying on feature engineering , is the lack of generalisation when using deep learning approaches on the small RST-DT dataset, containing only 385 discourse annotated documents. This makes us believe that using a more advanced neural discourse parser in combination with a large training dataset can lead to significant performance gains. %, but also across datasets, capturing more general discourse phenomena and avoiding potential overfitting on the training corpus. Admittedly, even though MEGA-DT contains a huge number of datapoints to train on, it has been automatically annotated, potentially introducing noise and biases, which can negatively influence the performance of our newly proposed neural discourse parser when solely trained on this dataset. A natural and intuitive approach to make use of the neural discourse parser and both datasets  is to combine them during training, pretraining on the large-scale ``silver-standard"" corpus and subsequently fine-tuning on RST-DT or further human annotated datasets. This way, general discourse structures could be learned from the large-scale treebank and then enhanced with human-annotated trees. With the results shown in this paper strongly suggesting that our new discourse parser can encode discourse more effectively, we hope that our efforts will prompt researchers to develop more linguistically inspired applications based on our discourse parser. % for downstream models in the area of NLP.  Our contributions in this paper are: % %on how to train a neural discourse parser with large scale ``silver-standard"" discourse trees. With this new approach, we  drastically increase the amount of available training data available for discourse parsers is not sufficiently large to train modern, data-driven deep learning approaches for the task, hindering the application of new methodologies and  the shift in domain between the discourse parsers training data and the domain of application deminishes the applicability and performance of generated discourse trees for any domain outside of news , instructions  and a few other domains.    The field of discourse parsing has been mainly dominated by traditional machine learning approaches, frequently outperforming initial attempts to apply deep learning and neural networks to the task. Independent of the specific approach used, three  general methodologies have been followed to learn discourse trees from small datasets, such as RST-DT  or Instructional-DT :  Top-down discourse parsers, splitting the document into non-overlapping text-constituents starting from the representation of the complete discourse down to individual EDUs, assigning the two resulting sub-spans a nuclearity attribute and predicting the relation holding between the sub-trees .  Bottom-up parsing, starting from the discourse-segmented list of EDUs and aggregating two adjacent units in every step. This approach is mostly realized using the CKY dynamic programming strategy to obtain optimal trees as in  and  or using a greedy method .  A frequently used and more locally inspired approach of bottom-up discourse parsing using the linear shift-reduce framework, adopted from previous work in syntactic parsing.  While the current traditional state-of-the-art discourse parser by  uses the bottom-up shift-reduce method predicted by two separate Support Vector Machines  for structure/nuclearity prediction and relation estimation, neural models  utilize multi-layer perceptrons  for classifying possible actions. In our work we also follow the bottom-up shift-reduce strategy, with the detailed description of our system provided in the following section.  Besides the active research area on discourse parsing, a second line of work has emerged recently, trying to generate large-scale discourse treebanks through automated annotations from downstream tasks, such as sentiment analysis , text classification , summarization  and fake news detection . The majority of these approaches follows the intuition that discourse trees can be inferred from downstream tasks by predicting latent representations during the learning process of the task itself  in an end-to-end manner. However, recent work by  has shown that the trees resulting  from this approach are not only poorly aligned with general discourse structures, but furthermore are oftentimes too shallow. A rather different strategy has been employed by , trying to explicitly generate a discourse augmented treebank through distant supervision from sentiment annotations in combination with Multiple-Instance Learning  and a CKY bottom-up tree generation approach. While the resulting MEGA-DT dataset has only been proposed and released recently, the authors show promising results in their work, reaching the best inter-domain performance when comparing their dataset against RST-DT and the Instruction-DT. This leads us to believe that their treebank does not only learn sentiment-related information, but can also be used to infer general discourse structures on a large scale. We will further evaluate this in section .    Traditional discourse parsers\\  Neural discourse parsers\\  Datasets \\  Sentiment analysis\\    URL segmentation has applications in TTS and web search. Our contributions include a curated URL data set and a highly accurate RNN model boosted by pre-training on Knowledge Graph entities.   We plan on releasing a version of the dataset before the conference.    \paragraph{\LaTeX-specific details:}   For an anonymized submission, ensure that {\small\verb|\aclfinalcopy|} at the top of this document is commented out, and that you have filled in the paper ID number  where {\small\verb|***|} appears in the {\small\verb|\def\aclpaperid{***}|} definition at the top of this document.   For a camera-ready submission, ensure that {\small\verb|\aclfinalcopy|} at the top of this document is not commented out.   
","  The field of discourse parsing has been mainly dominated by traditional machine learning approaches, frequently outperforming initial attempts to apply deep learning and neural networks to the task. Independent of the specific approach used, three  general methodologies have been followed to learn discourse trees from small datasets, such as RST-DT  or Instructional-DT :  Top-down discourse parsers, splitting the document into non-overlapping text-constituents starting from the representation of the complete discourse down to individual EDUs, assigning the two resulting sub-spans a nuclearity attribute and predicting the relation holding between the sub-trees .  Bottom-up parsing, starting from the discourse-segmented list of EDUs and aggregating two adjacent units in every step. This approach is mostly realized using the CKY dynamic programming strategy to obtain optimal trees as in  and  or using a greedy method .  A frequently used and more locally inspired approach of bottom-up discourse parsing using the linear shift-reduce framework, adopted from previous work in syntactic parsing.  While the current traditional state-of-the-art discourse parser by  uses the bottom-up shift-reduce method predicted by two separate Support Vector Machines  for structure/nuclearity prediction and relation estimation, neural models  utilize multi-layer perceptrons  for classifying possible actions. In our work we also follow the bottom-up shift-reduce strategy, with the detailed description of our system provided in the following section.  Besides the active research area on discourse parsing, a second line of work has emerged recently, trying to generate large-scale discourse treebanks through automated annotations from downstream tasks, such as sentiment analysis , text classification , summarization  and fake news detection . The majority of these approaches follows the intuition that discourse trees can be inferred from downstream tasks by predicting latent representations during the learning process of the task itself  in an end-to-end manner. However, recent work by  has shown that the trees resulting  from this approach are not only poorly aligned with general discourse structures, but furthermore are oftentimes too shallow. A rather different strategy has been employed by , trying to explicitly generate a discourse augmented treebank through distant supervision from sentiment annotations in combination with Multiple-Instance Learning  and a CKY bottom-up tree generation approach. While the resulting MEGA-DT dataset has only been proposed and released recently, the authors show promising results in their work, reaching the best inter-domain performance when comparing their dataset against RST-DT and the Instruction-DT. This leads us to believe that their treebank does not only learn sentiment-related information, but can also be used to infer general discourse structures on a large scale. We will further evaluate this in section .    Traditional discourse parsers\\  Neural discourse parsers\\  Datasets \\  Sentiment analysis\\",155
"  % Images  are another important approach for expressing feelings and emotions in addition to using text in communication. In mobile messaging apps, these images can generally be classified into emojis and stickers. Emoji is a kind of small picture which is already stored in most of the keyboard of the mobile operational systems, \ie iOS or Android. Emojis are pre-designed by the mobile phone vendor  and the number of emoji is limited, and users can not design emoji by themselves. Different with the inflexible emojis, sticker is image or graphicon essentially, which users can draw or modify images as a sticker and upload to the chatting app by themselves. The using of stickers on online chatting usually brings diversity of expressing emotion. Since emojis are sometimes used to help reinforce simple emotions in a text message due to their small size, and their variety is limited. Stickers, on the other hand, can be regarded as an alternative for text messages, which usually include cartoon characters and are of high definition. They can express much more complex and vivid emotion than emojis. Most messaging apps, such as WeChat, Telegram, WhatsApp, and Slack provide convenient ways for users to download stickers for free, or even share self-designed ones. We show a chat window including stickers in Figure.     % Stickers are becoming more and more popular in online chat. First, sending a sticker with a single click is much more convenient than typing text on the 26-letter keyboard of a small mobile phone screen. Second, there are many implicit or strong emotions that are difficult to express in words but can be captured by stickers with vivid facial expressions and body language. However, the large scale use of stickers means that it is not always straightforward to think of the sticker that best expresses one's feeling according to the current chatting context. Users need to recall all the stickers they have collected and selected the appropriate one, which is both difficult and time-consuming.  % Consequently, much research has focused on recommending appropriate emojis to users according to the chatting context. Existing works such as, are mostly based on emoji recommendation, where they predict the probable emoji given the contextual information from multi-turn dialog systems. In contrast, other works recommend emojis based on the text and images posted by a user. As for sticker recommendation, existing works such as and apps like Hike or QQ directly match the text typed by the user to the short text tag assigned to each sticker. However, since there are lots of ways of expressing the same emotion, it is very hard to capture all variants of an utterance as tags.  % To overcome the drawbacks, we propose a sticker response selector  for sticker selection in our early work, where we address the task of sticker response selection in multi-turn dialog. We focus on the two main challenges in this work:  Since existing image recognition methods are mostly built with real-world images, and how to capture the semantic meaning of sticker is challenging.  Understanding multi-turn dialog history information is crucial for sticker recommendation, and jointly modeling the candidate sticker with multi-turn dialog is challenging. % % % % % % Herein, we propose a novel sticker recommendation model, namely sticker response selector , for sticker response selection in multi-turn dialog. Specifically, SRS first learns representations of dialog context history using a self-attention mechanism and learns the sticker representation by a convolutional neural network .  % % % Next, SRS conducts deep matching between the sticker and each utterance and produces the interaction results for every utterance. % % Finally, SRS employs a fusion network which consists of a sub-network fusion RNN and fusion transformer to learn the short and long term dependency of the utterance interaction results. The final matching score is calculated by an interaction function. To evaluate the performance of our model, we propose a large number of multi-turn dialog dataset associated with stickers from one of the popular messaging apps.  Extensive experiments conducted on this dataset show that SRS significantly outperforms the state-of-the-art baseline methods in commonly-used metrics.  % However, the user's sticker selection does not only depend on the matching degree between dialog context and candidate sticker image, but also depends on the user's preference of using sticker. When users decide to use a sticker as their response in multi-turn dialog, they may choose their favorite one from all appropriate stickers as the final response.  % % % We assume that user tends to use the recently used sticker in their dialog history, and the recently-used-sticker can represent the user's preference of sticker selection. An example is shown in Figure. To verify this assumption, we retrieve 10 recently-used-stickers of each user and calculate the proportion of whether the currently used sticker appeared in these 10 stickers. The result shows that 54.09\% of the stickers exist in the 10 recently used sticker set. Hence, we reach to the conclusion that users have strong personal preference when selecting the sticker as their response for the current dialog context. However, in some cases, this also indicates a tendency to re-use stickers, but not necessarily a preference.  % Motivated by this observation, in this work, we take one step further and improve our previously proposed SRS framework with user preference modeling. Overall, we propose a novel sticker recommendation model which considers the user preference, namely Preference Enhanced Sticker Response Selector . Specifically, PESRS first employs a convolutional network to extract features from the candidate stickers. Then, we retrieve the recent user sticker selections then a user preference modeling module is employed to obtain a user preference representation. Next, we conduct the deep matching between the candidate sticker and each utterance as the same as SRS. Finally, we use a gated fusion method to combine the deep matching result and user preference into final sticker prediction.    % The key to the success of PESRS lies in how to design the user preference modeling module, which should not only identify the user's favorite sticker and but also consider the current dialog context. % Motivated by this, we first propose a recurrent neural network  based position-aware sticker modeling module which encodes the recently used stickers in chronological order. Then, we employ a key-value memory network to store these sticker representations as values and the corresponding dialog context as keys. Finally, we use the current dialog context to query the key-value memory and obtain the dynamic user preference of the current dialog context.  % We empirically compare PESRS and SRS on the public dataset\footnote{https://github.com/gsh199449/stickerchat} proposed by our early work. This is a large-scale real-world Chinese multi-turn dialog dataset, which dialog context is multiple text utterances and the response is a sticker image. Experimental results show that on this dataset, our newly proposed PESRS model can significantly outperform the existing methods.  Particularly, PESRS yields 4.8\% and 7.1\% percentage point improvement in terms of  and  compared with our early work SRS. % In addition to the comprehensive evaluation, we also evaluate our proposed user preference memory by a fine-grained analysis. The analysis reveals how the model leverages the user's recent sticker selection history and provides us insights on why they can achieve big improvement over state-of-the-art methods.  This work is a substantial extension of our previous work reported at WWW 2020.  The extension in this article includes the user preference modeling framework for the existing methods, a proposal of a new framework for sticker selection in the multi-turn dialog. Specifically, the contributions of this work include the following:    The rest of the paper is organized as follows: We summarize related work in \S.  \S introduces the data collection method and some statistics of our proposed multi-turn dialog sticker selection dataset. We then formulate our research problem in \S  and elaborate our approach in \S.  \S gives the details of our experimental setup and \S presents the experimental results.  Finally, \S concludes the paper.   %    We outline related work on sticker recommendation, user modeling, visual question answering, visual dialog, and multi-turn response selection.    Most of the previous works emphasize the use of emojis instead of stickers. For example,  use a multimodal approach to recommend emojis based on the text and images in an Instagram post.  propose a MultiLabel-RandomForest algorithm to predict emojis based on the private instant messages.  conduct emoji prediction on social media text , and they tackle this task as ranking among all emojis. The total number of unique emojis in their dataset is 50, which is much smaller than the number of stickers.   What is more, emojis are limited in variety, while there exists an abundance of different stickers.  incorporates the emoji information into the dialog generation task, and they use the emoji classification as an auxiliary task to facilitate the dialog generation to produce utterance with proper emotion. The most similar work to ours is , where they generate recommended stickers by first predicting the next message the user is likely to send in the chat, and then substituting it with an appropriate sticker.  However, more often than not the implication of the stickers cannot be fully conveyed by text and, in this paper, we focus on directly generating sticker recommendations from dialog history.    User modeling is a hot research topic especially in recommendation task, which models the preference of user based on the user history interaction data. Specifically, in the e-commerce recommendation task, the user modeling systems use the purchase history or click records to model the user's intrinsic interest and temporal interest. Most of the research typically utilize user-item binary relations, and assume a flat preference distribution over items for each user. They neglect the hierarchical discrimination between user intentions and user preferences. \citet{Zhu2020Sequential} propose a novel key-array memory network with user-intention-item triadic relations, which takes both user intentions and preferences into account for the next-item recommendation. As for the user modeling in the news recommendation task, there are much side information can be used to obtain better user preference representation. \citet{Wu2019Neural} propose a neural news recommendation approach which can exploit heterogeneous user behaviors, including the search queries and the browsed webpages of the user.  However, to model the user preference of sticker selection, we should not only model the sticker selection history, and the dialog context of each selected sticker should also be considered when modeling the user preference.    The memory network proposed by \citet{Sukhbaatar2015EndToEndMN} generally consists of two components. The first one is a memory matrix to save information  and the second one is a neural network to read/write the memory slots. The memory network has shown better performance than traditional long-short term memory network in several tasks, such as question answering, machine translation, text summarization, dialog system and recommendation. The reason is that the memory network can store the information in a long time range and has more memory storage units than LSTM which has the single hidden state. Follow memory network, there are many variations of memory network have been proposed, \ie key-value memory network and dynamic memory network. Our method is mainly based on the key-value memory network, which employs the user history dialog contexts as the memory keys and the corresponding selected stickers the memory values. However, there are two main differences between our PESRS model and the previous key-value memory network. First, the user history data is in chronological order, we should consider the time information when storing them into the memory. To recommend more accurate stickers, the model should not only consider the user preference information stored in the memory, but also incorporates the matching result between current dialog context and candidate stickers. The second difference lies in that we propose a dynamic fusion layer that considers both the memory read output and the matching result of the current context. Compared with these methods, we not only implement a key-value memory network, but also provide a sticker selection framework that could incorporate the user's preference.   Sticker recommendation involves the representation of and interaction between images and text, which is related to the Visual Question Answering  task. Specifically, VQA takes an image and a corresponding natural language question as input and outputs the answer. It is a classification problem in which candidate answers are restricted to the most common answers appearing in the dataset and requires deep analysis and understanding of images and questions such as image recognition and object localization. Current models can be classified into three main categories: early fusion models, later fusion models, and external knowledge-based models. One state-of-the-art VQA model is , which proposes an architecture, positional self-attention with co-attention, that does not require a recurrent neural network  for video question answering.  proposes an image-question-answer synergistic network, where candidate answers are coarsely scored according to their relevance to the image and question pair in the first stage.  Then, answers with a high probability of being correct are re-ranked by synergizing with images and questions.  The difference between sticker selection and VQA task is that the sticker selection task focus more on multi-turn multimodal interaction between stickers and utterances.   Visual dialog extends the single turn dialog task in VQA to a multi-turn one, where later questions may be related to former question-answer pairs.   To solve this task,  transfers knowledge from a pre-trained discriminative network to a generative network with an RNN encoder, using a perceptual loss.  combines reinforcement learning and generative adversarial networks  to generate more human-like responses to questions, where the GAN helps overcome the relative paucity of training data, and the tendency of the typical maximum-likelihood-estimation-based approach to generate overly terse answers.  demonstrates a simple symmetric discriminative baseline that can be applied to both predicting an answer as well as predicting a question in the visual dialog.  Unlike visual dialog tasks, in a sticker recommendation system, the candidates are stickers rather than text.   Multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context. In our task, we also need to take previous multi-turn dialog into consideration. Previous works include , which uses an RNN to represent context and response, and measure their relevance. More recently,  matches a response with each utterance in the context on multiple levels of granularity, and the vectors are then combined through an RNN. The final matching score is calculated by the hidden states of the RNN.  extends this work by considering the matching with dependency information. More recently,  proposes a multi-representation fusion network where the representations can be fused into matching at an early stage, an intermediate stage, or at the last stage.  Traditional multi-turn response selection deals with pure natural language processing, while in our task, we also need to obtain a deep understanding of images.     In this paper, we propose a single-choice model for MMRC that consider the options separately. Experiments results demonstrate that our method achieves significantly improvements and by taking advantage of other MRC datasets, we achieve a new state-of-the-art performance. We plan to consider the difference between two methods and if we can combine them together in future study.            File emnlp2019.tex      Based on the style files for ACL 2019, which were    Based on the style files for EMNLP 2018, which were    Based on the style files for ACL 2018, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp-ijcnlp-2019} \usepackage{latexsym} \usepackage{times} \usepackage{soul} \usepackage{url} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{subfigure} \usepackage{amsmath} \usepackage{booktabs} \usepackage{natbib} \usepackage{xcolor} \urlstyle{same} \usepackage{fancyhdr,graphicx,amssymb} \usepackage[ruled,vlined]{algorithm2e} \usepackage{multirow} \usepackage{url}  \usepackage{tikz} \usepackage{subfigure} \usepackage{xcolor} \usepackage{tcolorbox}  \usepackage{helvet}   Required \usepackage{courier}   Required  \newcommand\BibTeX{B{\sc ib}\TeX} \newcommand\confname{EMNLP-IJCNLP 2019} \newcommand\conforg{SIGDAT}  \title{Multi-choice Machine Reading Comprehension with Two-stage Training}  \author{First Author \\   Affiliation / Address line 1 \\   Affiliation / Address line 2 \\   Affiliation / Address line 3 \\   email@domain \\\And   Second Author \\   Affiliation / Address line 1 \\   Affiliation / Address line 2 \\   Affiliation / Address line 3 \\   email@domain \\}  \date{}    
","  We outline related work on sticker recommendation, user modeling, visual question answering, visual dialog, and multi-turn response selection.    Most of the previous works emphasize the use of emojis instead of stickers. For example,  use a multimodal approach to recommend emojis based on the text and images in an Instagram post.  propose a MultiLabel-RandomForest algorithm to predict emojis based on the private instant messages.  conduct emoji prediction on social media text , and they tackle this task as ranking among all emojis. The total number of unique emojis in their dataset is 50, which is much smaller than the number of stickers.   What is more, emojis are limited in variety, while there exists an abundance of different stickers.  incorporates the emoji information into the dialog generation task, and they use the emoji classification as an auxiliary task to facilitate the dialog generation to produce utterance with proper emotion. The most similar work to ours is , where they generate recommended stickers by first predicting the next message the user is likely to send in the chat, and then substituting it with an appropriate sticker.  However, more often than not the implication of the stickers cannot be fully conveyed by text and, in this paper, we focus on directly generating sticker recommendations from dialog history.    User modeling is a hot research topic especially in recommendation task, which models the preference of user based on the user history interaction data. Specifically, in the e-commerce recommendation task, the user modeling systems use the purchase history or click records to model the user's intrinsic interest and temporal interest. Most of the research typically utilize user-item binary relations, and assume a flat preference distribution over items for each user. They neglect the hierarchical discrimination between user intentions and user preferences. \citet{Zhu2020Sequential} propose a novel key-array memory network with user-intention-item triadic relations, which takes both user intentions and preferences into account for the next-item recommendation. As for the user modeling in the news recommendation task, there are much side information can be used to obtain better user preference representation. \citet{Wu2019Neural} propose a neural news recommendation approach which can exploit heterogeneous user behaviors, including the search queries and the browsed webpages of the user.  However, to model the user preference of sticker selection, we should not only model the sticker selection history, and the dialog context of each selected sticker should also be considered when modeling the user preference.    The memory network proposed by \citet{Sukhbaatar2015EndToEndMN} generally consists of two components. The first one is a memory matrix to save information  and the second one is a neural network to read/write the memory slots. The memory network has shown better performance than traditional long-short term memory network in several tasks, such as question answering, machine translation, text summarization, dialog system and recommendation. The reason is that the memory network can store the information in a long time range and has more memory storage units than LSTM which has the single hidden state. Follow memory network, there are many variations of memory network have been proposed, \ie key-value memory network and dynamic memory network. Our method is mainly based on the key-value memory network, which employs the user history dialog contexts as the memory keys and the corresponding selected stickers the memory values. However, there are two main differences between our PESRS model and the previous key-value memory network. First, the user history data is in chronological order, we should consider the time information when storing them into the memory. To recommend more accurate stickers, the model should not only consider the user preference information stored in the memory, but also incorporates the matching result between current dialog context and candidate stickers. The second difference lies in that we propose a dynamic fusion layer that considers both the memory read output and the matching result of the current context. Compared with these methods, we not only implement a key-value memory network, but also provide a sticker selection framework that could incorporate the user's preference.   Sticker recommendation involves the representation of and interaction between images and text, which is related to the Visual Question Answering  task. Specifically, VQA takes an image and a corresponding natural language question as input and outputs the answer. It is a classification problem in which candidate answers are restricted to the most common answers appearing in the dataset and requires deep analysis and understanding of images and questions such as image recognition and object localization. Current models can be classified into three main categories: early fusion models, later fusion models, and external knowledge-based models. One state-of-the-art VQA model is , which proposes an architecture, positional self-attention with co-attention, that does not require a recurrent neural network  for video question answering.  proposes an image-question-answer synergistic network, where candidate answers are coarsely scored according to their relevance to the image and question pair in the first stage.  Then, answers with a high probability of being correct are re-ranked by synergizing with images and questions.  The difference between sticker selection and VQA task is that the sticker selection task focus more on multi-turn multimodal interaction between stickers and utterances.   Visual dialog extends the single turn dialog task in VQA to a multi-turn one, where later questions may be related to former question-answer pairs.   To solve this task,  transfers knowledge from a pre-trained discriminative network to a generative network with an RNN encoder, using a perceptual loss.  combines reinforcement learning and generative adversarial networks  to generate more human-like responses to questions, where the GAN helps overcome the relative paucity of training data, and the tendency of the typical maximum-likelihood-estimation-based approach to generate overly terse answers.  demonstrates a simple symmetric discriminative baseline that can be applied to both predicting an answer as well as predicting a question in the visual dialog.  Unlike visual dialog tasks, in a sticker recommendation system, the candidates are stickers rather than text.   Multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context. In our task, we also need to take previous multi-turn dialog into consideration. Previous works include , which uses an RNN to represent context and response, and measure their relevance. More recently,  matches a response with each utterance in the context on multiple levels of granularity, and the vectors are then combined through an RNN. The final matching score is calculated by the hidden states of the RNN.  extends this work by considering the matching with dependency information. More recently,  proposes a multi-representation fusion network where the representations can be fused into matching at an early stage, an intermediate stage, or at the last stage.  Traditional multi-turn response selection deals with pure natural language processing, while in our task, we also need to obtain a deep understanding of images.",156
" Task-oriented dialog systems are commonplace in automated systems that interact with end users, including digital assistants, technical support agents, and various website navigation helpers. An essential part in any task-oriented dialog system is natural language generation , which consumes data, typically fed in the form of a dialog act, and converts it into natural language output to be served to the end user. The natural language response of the NLG component should 1) contain all essential information, 2) be contextualized around the user request, and 3) be natural sounding. Such a system requires consideration for content planning, correctness, grammaticality, and naturalness.  NLG systems employed in commercial settings are typically based on template-based text generation techniques . In these, humans author a minimal set of responses templates with placeholder slot values. These slots are later filled at runtime, with the dialog input. Although template-based NLG modules are appealing due to their deterministic nature,  inherent correctness, and low latency, they have major drawbacks: First, separate templates need to be authored for different response variations; this behavior is unfavorable for scaling. Second, templates authored for a particular domain are commonly not reusable.  Lastly, no matter the complexity of the language instilled into templates, they form a strictly discrete set of responses, and therefore are bound to be limited in their response naturalness.  More recently, advances in neural-network-based  language generation prompted a new direction in NLG research . The process is typically split into two steps:  serialization of input data into a flattened meaning representation , and  using the neural generation model to generate a natural language response conditioned on the MR. The models are trained on data that includes MR, response pairs, and therefore they are able to not only generate desired responses for MRs in their training data, but they are also expected to form coherent responses for novel MRs, owing to the generalization ability of their machine learning  backbone.  However, deploying neural NLG systems in an industry setting is quite challenging. First, it is not trivial to train a model that reliably presents its input data with the high fidelity required from a user-serving dialog system. Second, the models require much high-quality human-annotated data, which is resource intensive. Consequently, data annotation is a major limiting factor for scaling model-based NLG across domains and languages.  In this work, we detail our approach to production-level neural NLG, with a focus on scalability and data efficiency. Adopting the tree-structured MR framework introduced in Balakrishnan et al.~\shortcite{Balakrishnan2019constrainednlg}, which allows better control over generated responses, we train sequence-to-sequence RNN models  that can produce high-fidelity responses. We then employ a multitude of techniques for reducing the amount of  required data, primarily powered by eliminating the ``hidden'' redundancy by grouping data points with similar semantics into buckets. We train models either on the reduced data, or after increasing the size of the dataset using a novel synthetic augmentation technique. We also employ large, pre-trained attention-based language models, fine-tuning them on the same datasets, and then using novel methods to distill their knowledge into smaller sequence-to-sequence models. Further, we train models on data from multiple domains, showing gains over models trained on individual domains when the domains are semantically close together. We conclude with a compiled list of best practices for production-level NLG model development based on our analyses, and we present it as a runbook.     NLG from structured data has been an active research area for decades, facilitated of late by datasets like the E2E Challenge, MultiWoz and Conversational Weather. Recently, Seq2Seq models, have become popular for their superior naturalness and simplicity. These models have achieved high performance on benchmarks like E2E challenge and WebNLG challenge. However, they require a lot of data making them resource-intensive to stand up and manage at scale.   Our work introduces an approach for bootstrapping data-efficient NLG models by auto-annotating unlabelled examples using a large pretrained sequence de-noiser model known as BART  fine-tuned on a small annotated dataset. Additionally, to increase data collection efficiency, we present several bucketing strategies, which enable a more uniform data collection process over the possible semantic space. We improve upon the BART auto-annotation technique by combining it with an innovative method of dynamic data-augmentation  and fine-tuning BART auto-annotation on a small subset of data sampled using a medium grained bucketing approach. We also carried out experiments to examine the effects of bucketing granularity combined with domain complexity.  In similar studies, pretrained GPT models  were used by Chen et al. and Peng et al., who fine-tune them on a small set of in-domain data, but they did not distill these models into ones suitable for production.  Interestingly, Wen et al. demonstrated that the structure of arguments in existing dialogues can be used to guide data collection for low-resource domain adaptation, which is similar to the bucketing strategies explored here.  Additionally, Shah et al. introduce a dialogue self-play method where templates are instantiated with database values to create synthetic utterances, similar to our dynamic data-augmentation method; however, their instantiated templates are then rewritten by crowd-workers, whereas in our DDA method, crowd-sourced utterances are delexicalized and then re-instantiated with random values. Kedzie \& McKeown \shortcite{kedzie-mckeown-2019-good} also make use of a similar technique in their work on self-training for neural NLG; by comparison, we experiment with DDA in a wider variety of training scenarios.     We tackle the task of prerequisite relation learning using a variety of systems that explore three set of features: handcrafted features based on complexity intuitions, embedding models from Wikipedia and Wikidata, and contextual embedding from Italian-BERT model. We examine the capabilities of our models in in-domain and cross-domain scenarios.    out of 4 domains, and for raw-text versus structured-information settings. Our models ranked first in all the subtask of the PRELEARN competition at EVALITA 2020. We found that although our Italian-BERT model outperformed the others, the simpler models show competitive results.    A limitation of our work is that we used all the possible domains in our experiments.  We plan to further examine the impact of using a combination of all possible domains as training set on the performance of our models. 
","  NLG from structured data has been an active research area for decades, facilitated of late by datasets like the E2E Challenge, MultiWoz and Conversational Weather. Recently, Seq2Seq models, have become popular for their superior naturalness and simplicity. These models have achieved high performance on benchmarks like E2E challenge and WebNLG challenge. However, they require a lot of data making them resource-intensive to stand up and manage at scale.   Our work introduces an approach for bootstrapping data-efficient NLG models by auto-annotating unlabelled examples using a large pretrained sequence de-noiser model known as BART  fine-tuned on a small annotated dataset. Additionally, to increase data collection efficiency, we present several bucketing strategies, which enable a more uniform data collection process over the possible semantic space. We improve upon the BART auto-annotation technique by combining it with an innovative method of dynamic data-augmentation  and fine-tuning BART auto-annotation on a small subset of data sampled using a medium grained bucketing approach. We also carried out experiments to examine the effects of bucketing granularity combined with domain complexity.  In similar studies, pretrained GPT models  were used by Chen et al. and Peng et al., who fine-tune them on a small set of in-domain data, but they did not distill these models into ones suitable for production.  Interestingly, Wen et al. demonstrated that the structure of arguments in existing dialogues can be used to guide data collection for low-resource domain adaptation, which is similar to the bucketing strategies explored here.  Additionally, Shah et al. introduce a dialogue self-play method where templates are instantiated with database values to create synthetic utterances, similar to our dynamic data-augmentation method; however, their instantiated templates are then rewritten by crowd-workers, whereas in our DDA method, crowd-sourced utterances are delexicalized and then re-instantiated with random values. Kedzie \& McKeown \shortcite{kedzie-mckeown-2019-good} also make use of a similar technique in their work on self-training for neural NLG; by comparison, we experiment with DDA in a wider variety of training scenarios.",157
" The emergence of online collaboration platforms has dramatically changed the dynamics of human teamwork, creating a veritable army of virtual teams, composed of workers in different physical locations. Software engineering requires a tremendous amount of collaborative problem solving, making it an excellent domain for team cognition researchers who seek to understand the manifestation of cognition applied to team tasks.  Mining data from social coding platforms such as GitHub can yield insights about the thought processes of virtual teams.  Previous work on issue comments has focused on emotional aspects of team communication, such as sentiment and politeness.  Our aim is to map issue comments to states in team cognition such as information gathering, knowledge building and problem solving.  To do this we employ dialogue act  classification, in order to identify the intent of the speaker.  Dialogue act classification has a broad range of natural language processing applications, including machine translation, dialogue systems and speech recognition.  Semantic-based classification of human utterances is a challenging task, and the lack of a large annotated corpus that represents class variations makes the job even harder. Compared to the examples of human utterances available in standard datasets like the Switchboard  corpus and the CSI Meeting Recorder Dialogue Act  corpus, GitHub utterances are more complex.   The primary purpose of our study is the DA classification of GitHub issue comments by harnessing the strength of transfer learning, using word and sentence level embedding models fine-tuned on our dataset.  For word-level transfer learning, we have used GLoVe vectors, and Universal Sentence Encoders and BERT models were used for sentence-level transfer. This paper presents a comparison of the performance of various  architectures on GitHub dialogues in a limited resource scenario.  A second contribution is our publicly available dataset of annotated issue comments. The dataset is available at \url{https://drive.google.com/drive/folders/1kLZvzfE80VeEYA1tqua_aj6nSiT57f83?usp=sharing}. In the field of computational collective intelligence,  where people collaborate and work in teams to achieve goals, dialogue act classification can play a vital role in understanding human teamwork.     Issue resolution has been viewed by many researchers as a rich source of information about the emotional health of the team and how it affects the software development process.  Kikas et al. demonstrated a model for predicting issue lifetime that included a single feature aggregating textual comment information.  Several studies have employed sentiment analysis and topic modeling to study GitHub issue comments. Ortu et al. conducted a large study on communication patterns in which they measured politeness and emotional affect in issue comments; their aim was to understand how contribution levels modulate communication patterns.  Murgia et al. demonstrated a  machine learning classifier for identifing love, joy or sadness in issue comments. An empirical study of issue comments conducted by Guzman et al. showed that the sentiment expressed in issue comments varies based on day of week, geographic dispersion of the team, and the programming language. Yang et al. addressed the more practical question of the relationship of issue comment sentiment and bug fixing speed.   In contrast, our aim is to study the team cognition aspects of collaborative problem solving using dialogue act classification.  Unlike topic modeling or sentiment analysis, dialogue act classification has not been extensively applied to GitHub data.  However, Saha et al. proposed a deep learning approach for the dialogue act  classification of Twitter data. A convolutional neural network was used to create the classifier, along with hand-crafted rules.  Seven classes were included: statement, expression, suggestion, request, question, threat, and other.  In contrast, our work is done using a transfer learning approach and a significantly larger set of classes.  Prior to deep learning, statistical approaches such as hidden Markov models, have been used for dialogue act classification. The HMM represents discourse structure, with dialogue acts as states.  Stolcke et al. demonstrated such a model that combined prosodic, lexical, and collocational cues.  Chen et al. proposed the CRF-Attentive Structured Network  framework to exploit the CRF-attentive structure dependencies along with end-to-end training.   This paper presents a transfer learning approach for dialogue act classification that is used to compensate for our small dataset.   To do this, we learn an embedding from a larger dataset. The next section surveys the state of the art in embedding models for natural language processing.  Embeddings are a mechanism for mapping a high-dimensional space to a low-dimensional one while only retaining the most effective structural representations. They can be used as part of the transfer learning process to mitigate the low availability of labeled language resources on various NLP tasks.  This paper presents transfer learning results using the following state of the art embedding methods: Global Vectors for Word Representation , Universal Sentence Encoding , and Bidirectional Encoding .  We compare these embedding models with the probabilistic technique proposed by Duran et al. on our GitHub issue comments dataset.  \subsubsection{Global Vectors for Word Representation } Pennington et al. proposed the GloVe model in 2014. It creates a word-level embedding that leverages both the local context window and global matrix factorization methods. GloVe employs a log-bilinear prediction-based technique that utilizes word-word co-occurrence statistics to identify a meaningful structure and generate word-level embeddings. We are using the GloVe model to illustrate the results of DA classification of GitHub data using word-level embedding. \subsubsection{Universal Sentence Encoders } In 2018, Google Research released a Universal Sentence Encoder  model for sentence-level transfer learning that  achieves consistent performance across multiple NLP tasks. There are two different variants of the model: 1) a transformer architecture, which gives high accuracy at the cost of high resource consumption and 2) a deep averaging network that requires few resources and makes small compromises for efficiency. The former uses attention-based, context-aware encoding sub-graphs for the transfer architecture. The model outputs a 512-dimensional vector. The deep averaging network works by averaging words and bigram embeddings to use as an input to a deep neural network. The models are trained on web news, Wikipedia, web question-answer pages, discussion forums, and the Stanford Natural Language Inference  corpus.  These models are freely available on TF Hub.  \subsubsection{Bidirectional Encoder Representations from Transformers } Also created at Google, BERT is the first model that was trained on both left and right contexts. To achieve pre-trained deep bidirectional representation, it uses the masked model, which follows the cloze deletion task. This model is trained on Books Corpus and English Wikipedia corpus. The code for BERT is available at . There are two available flavors of BERT: 1), and 2) .  has 12 transformer blocks, 768 hidden layers, 12 self-attention heads, and 110 million parameters. On the other hand,  uses a fairly large network, with 24 transformer blocks, 1024 hidden layers, 16 self-attention heads, and 340 million parameters.                 The latency results revealed that our incremental system based on the cascade of three modules worked successfully with relatively small delays. However, the quality results suggested the task difficulty due to error propagation from ISR to IMT and lack of in-domain corpora in English-Japanese MT. We show two translation examples in Table. The first one is a relatively good result, and the other one is a typical error propagation example. Tight module integration would be promising, such as lattice-to-sequence , but its extension to the simultaneous translation is not trivial.  Besides, we have no common evaluation metrics for simultaneous speech-to-speech translation other than module-level ones. We used two latency metrics in this work, but the objective measurement of content delivery by speech-to-speech translation is crucial for further evaluation.    \section{Conclusions}   In this paper, we presented our English-to-Japanese simultaneous speech-to-speech translation system with its evaluation using English TED talks. The system works fully-incremental with speech inputs, by the cascaded modules of incremental ASR, incremental MT, and incremental TTS. The latency evaluation revealed the module-level computation could be finished with about three seconds delay at maximum. However, the system suffers from speaking latency.  Our future work includes further improvement of the modules in accuracy and efficiency, and controlling speaking duration to decrease the speaking latency.  \section{Acknowledgments} Part of this work was supported by JSPS KAKENHI Grant Numbers JP17H06101.    References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   -------------------------------------------------------------------------    
"," Issue resolution has been viewed by many researchers as a rich source of information about the emotional health of the team and how it affects the software development process.  Kikas et al. demonstrated a model for predicting issue lifetime that included a single feature aggregating textual comment information.  Several studies have employed sentiment analysis and topic modeling to study GitHub issue comments. Ortu et al. conducted a large study on communication patterns in which they measured politeness and emotional affect in issue comments; their aim was to understand how contribution levels modulate communication patterns.  Murgia et al. demonstrated a  machine learning classifier for identifing love, joy or sadness in issue comments. An empirical study of issue comments conducted by Guzman et al. showed that the sentiment expressed in issue comments varies based on day of week, geographic dispersion of the team, and the programming language. Yang et al. addressed the more practical question of the relationship of issue comment sentiment and bug fixing speed.   In contrast, our aim is to study the team cognition aspects of collaborative problem solving using dialogue act classification.  Unlike topic modeling or sentiment analysis, dialogue act classification has not been extensively applied to GitHub data.  However, Saha et al. proposed a deep learning approach for the dialogue act  classification of Twitter data. A convolutional neural network was used to create the classifier, along with hand-crafted rules.  Seven classes were included: statement, expression, suggestion, request, question, threat, and other.  In contrast, our work is done using a transfer learning approach and a significantly larger set of classes.  Prior to deep learning, statistical approaches such as hidden Markov models, have been used for dialogue act classification. The HMM represents discourse structure, with dialogue acts as states.  Stolcke et al. demonstrated such a model that combined prosodic, lexical, and collocational cues.  Chen et al. proposed the CRF-Attentive Structured Network  framework to exploit the CRF-attentive structure dependencies along with end-to-end training.   This paper presents a transfer learning approach for dialogue act classification that is used to compensate for our small dataset.   To do this, we learn an embedding from a larger dataset. The next section surveys the state of the art in embedding models for natural language processing.  Embeddings are a mechanism for mapping a high-dimensional space to a low-dimensional one while only retaining the most effective structural representations. They can be used as part of the transfer learning process to mitigate the low availability of labeled language resources on various NLP tasks.  This paper presents transfer learning results using the following state of the art embedding methods: Global Vectors for Word Representation , Universal Sentence Encoding , and Bidirectional Encoding .  We compare these embedding models with the probabilistic technique proposed by Duran et al. on our GitHub issue comments dataset.  \subsubsection{Global Vectors for Word Representation } Pennington et al. proposed the GloVe model in 2014. It creates a word-level embedding that leverages both the local context window and global matrix factorization methods. GloVe employs a log-bilinear prediction-based technique that utilizes word-word co-occurrence statistics to identify a meaningful structure and generate word-level embeddings. We are using the GloVe model to illustrate the results of DA classification of GitHub data using word-level embedding. \subsubsection{Universal Sentence Encoders } In 2018, Google Research released a Universal Sentence Encoder  model for sentence-level transfer learning that  achieves consistent performance across multiple NLP tasks. There are two different variants of the model: 1) a transformer architecture, which gives high accuracy at the cost of high resource consumption and 2) a deep averaging network that requires few resources and makes small compromises for efficiency. The former uses attention-based, context-aware encoding sub-graphs for the transfer architecture. The model outputs a 512-dimensional vector. The deep averaging network works by averaging words and bigram embeddings to use as an input to a deep neural network. The models are trained on web news, Wikipedia, web question-answer pages, discussion forums, and the Stanford Natural Language Inference  corpus.  These models are freely available on TF Hub.  \subsubsection{Bidirectional Encoder Representations from Transformers } Also created at Google, BERT is the first model that was trained on both left and right contexts. To achieve pre-trained deep bidirectional representation, it uses the masked model, which follows the cloze deletion task. This model is trained on Books Corpus and English Wikipedia corpus. The code for BERT is available at . There are two available flavors of BERT: 1), and 2) .  has 12 transformer blocks, 768 hidden layers, 12 self-attention heads, and 110 million parameters. On the other hand,  uses a fairly large network, with 24 transformer blocks, 1024 hidden layers, 16 self-attention heads, and 340 million parameters.",158
" Large, densely-labeled datasets are a critical requirement for the creation of effective supervised learning models. The pressing need for high quantities of labeled data has led many researchers to collect data from social media platforms and online forums . Due to the presence of noise and the lack of structure that exist in these data sources, manual quality analysis  is necessary to extract structured labels, filter irrelevant examples, standardize language, and perform other preprocessing tasks before the data can be used. However, obtaining dataset annotations in this manner is a time-consuming and expensive process that is often prone to errors.   In this work, we develop automated data cleaning and verification mechanisms for extracting high-quality data from social media platforms\footnote{All code is available at \url{https://github.com/rachel-1/qa_plausibility}.}. We specifically focus on the creation of question-answer datasets, in which each data instance consists of a question about a topic and the corresponding answer. In order to filter noise and improve data quality,  we propose the task of question-answer  plausibility, which includes the following three steps:   Because we assume social media users generally answer questions in good faith , we can assume plausible answers are correct ones . Necessarily, if this property were not satisfied, then any adequate solutions would require the very domain knowledge of interest. Therefore, we look to apply this approach toward data with this property.  In this study, we demonstrate an application of QA plausibility in the context of visual question answering , a well-studied problem in the field of computer vision . We assemble a large VQA dataset with images collected from an image-sharing social network, machine-generated questions related to the content of the image, and responses from social media users. We then train a multitask BERT-based model and evaluate the ability of the model to perform the three subtasks associated with QA plausibility. The methods presented in this work hold potential for reducing the need for manual quality analysis of crowdsourced data as well as enabling the use of question-answer data from unstructured environments such as social media platforms.      Prior studies on the automated labeling task for datasets derived from social media typically focus on the generation of noisy labels; models trained on such datasets often rely on weak supervision to learn relevant patterns. However, approaches for noisy label generation, such as Snorkel  and CurriculumNet , often use functions or other heuristics to generate labels. One such example is the Sentiment140 dataset, which consists of 1.6 million tweets labeled with corresponding sentiments based on the emojis present in the tweet . In this case, the presence of just three category labels  simplifies the labeling task and reduces the effects of incorrect labels on trained models; however, this problem becomes increasingly more complex and difficult to automate as the number of annotation categories increases.  Previous researchers have studied question relevance by reasoning explicitly about the information available to answer the question. Several VQA studies have explicitly extracted premises, or assumptions made by questions, to determine if the original question is relevant to the provided image . A number of machine comprehension models have been devised to determine the answerability of a question given a passage of text . In contrast, we are able to leverage the user's freeform response to determine if the original question was valid. Our model is also tasked with supporting machine-generated questions, which may be unanswerable and lead to noisy user-generated responses.  While the concept of answer plausibility in user responses has also been previously explored, existing approaches use hand-crafted rules and knowledge sources . By using a learned approach, we give our system the flexibility to adapt with the data and cover a wider variety of cases.    This paper demonstrates a dialogue act classification system for GitHub issue comments. Due to the lack of publicly available training sets of formal teamwork dialogues, we formulated the problem as a transfer learning task, using both sentence-level and word-level embedding models to leverage information from the SwDA dataset. A significant contribution of our work is identifying the embedding model that performs best after fine-tuning on issue comments. We used GloVe, probabilistic representation, USE, and BERT embedding to train five different models. USE showed the best performance with an accuracy of 50.71\ . The low accuracy of USE on DA classification as compared to its accuracy on other state-of-the-art NLP tasks shows the complex nature of the dialogue act classification.  We evaluated many different settings for learning rates, epochs, and batch size; even though minor accuracy improvements were achievable, the performance of the embedding models remained fairly stable.    Our aim is to map issue comments to cognitive states in the Macrocognition in Teams Model .  Drawing from research on externalized cognition, team cognition, group communication and problem solving, and collaborative learning and adaptation, MITM provides a coherent theoretically based conceptualization for understanding complex team processes and how these emerge and change over time. MITM consists of five components: Team Problem-Solving Outcomes, Externalized Team Knowledge, Internalized Knowledge, Team Knowledge Building, and Individual Knowledge Building. It captures the parallel and iterative processes engaged by teams as they synthesize these components in service of team cognitive processes such as problem solving, decision making and planning. MITM has been applied to other team problem solving scenarios in military logistics and business planning but has never been used to analyze software engineering teams. Its usage in the domain of software engineering would be a major research contribution to the field of team cognition.    Although it is possible to directly label issue comments using an MITM code book, that type of labeling would be less compatible with existing dialogue act datasets.  Instead we are constructing a mapping that relates the DAMSL tagset to these cognitive states.  For instance, the question tags in DAMSL clearly relate to information gathering processes. Also many of the DAMSL classes are less relevant to the team cognition process and could be ignored.  The most commonly occurring classes in the GitHub issue comments  are all relevant to the Macrocognition in Teams Model, and we plan to tune our dialogue act classifiers to bolster the performance on these classes. In future work, we continue to improve the size and quality of our publicly-released dataset by recruiting more annotators to help with the labeling task and also more systematically studying inter-coder reliability.   
","  Prior studies on the automated labeling task for datasets derived from social media typically focus on the generation of noisy labels; models trained on such datasets often rely on weak supervision to learn relevant patterns. However, approaches for noisy label generation, such as Snorkel  and CurriculumNet , often use functions or other heuristics to generate labels. One such example is the Sentiment140 dataset, which consists of 1.6 million tweets labeled with corresponding sentiments based on the emojis present in the tweet . In this case, the presence of just three category labels  simplifies the labeling task and reduces the effects of incorrect labels on trained models; however, this problem becomes increasingly more complex and difficult to automate as the number of annotation categories increases.  Previous researchers have studied question relevance by reasoning explicitly about the information available to answer the question. Several VQA studies have explicitly extracted premises, or assumptions made by questions, to determine if the original question is relevant to the provided image . A number of machine comprehension models have been devised to determine the answerability of a question given a passage of text . In contrast, we are able to leverage the user's freeform response to determine if the original question was valid. Our model is also tasked with supporting machine-generated questions, which may be unanswerable and lead to noisy user-generated responses.  While the concept of answer plausibility in user responses has also been previously explored, existing approaches use hand-crafted rules and knowledge sources . By using a learned approach, we give our system the flexibility to adapt with the data and cover a wider variety of cases.",159
"  Language modelling is the task of transforming individual words into vector representations based on the context they appear in. Hence, distant term dependencies are an inherited issue within the task. Language models always seek for smart approaches towards incorporating context from longer distances as it allows for better representations compared to their limited context counterparts. Intuitively, imagine attempting to start reading a novel series from the second book onward, with no information about the first. The amount of information previously missed is something that cannot be acquired. However, this is the case with most language models. While an understanding of the words is present due to the contextual information at each word's occurrence, entity information that are in distant text are lost or not transferred.   Until recently, Recurrent Neural Networks , and specifically  Long Short-Term Memory  networks, have been the core of all the state-of-the-art approaches . Thanks to the Transformers architecture , through the use of attention mechanisms, models such as XLNet , GPT  and BERT  can account for even longer sequences. However, the computational limitations of the multi-head attention in the architecture make it hard to increase the contextual information in such models . As a result, research has been focused on introducing variations to the transformer architecture, with focus on the multi-head attention mechanism, in order to alleviate part of the computational cost and increase the contextual information available to models.   In this paper we present a novel approach, that makes use of coreference information during training a language model via our Entity-Transformer architecture, which extends the original Transformer block in Transformer-Based language models. To that end, we incorporate the important entity information that would otherwise be unreachable for the model. As a result, we effectively boost the representations of the entity mentions, where entity information is present, without hindering the performance of the language model where entities are not present.   In our experiments, we extend the GPT2 architecture to formulate our model, named GPT2E and train it on the CoNLL-2012 dataset  using the annotated coreference information. We evaluate the model's performance in terms of Perplexity on the ConLL 2012 and the LAMBADA  datasets and showcase the effects of such training on the word representations as well as on the downstream task of Named Entity Recognition  using the CoNLL 2012 dataset. To that end, we compare GPT2E's performance to a base model  when trained on the same data, to highlight the effects of coreference information when paird with our Entity-Transformer architecture.      In the last decade, the field of Neural Language Modelling has witnessed enormous changes.  With pretrained neural language models being the current go-to approach in all NLP reserach, a variety of methods models have been developed. We distinguish two major categories:  \paragraph{General purpose language models.} Steady improvements have been achieved to this field with the use of deep RNNs and pre-training on a large number of training data . With Transformers, language models have been able to capture longer linguistic structures without the use of RNNs and surpass their RNN counterparts by a big margin . Recent research has focused on ways of taking advantage of more context  and introducing effective methodologies to scale up the models and train them .   \paragraph{Language modelling with entity decisions.} YangLM  was the first to incorporate entity decisions to a language model by introducing learnable entity embeddings. Alternative entity handling mechanisms are introduced in both EntityNLM  and SetLM  in addition to a length variable for EntityNLM. All of the aforementioned approaches are RNN-based and hence their performance is expected to be sub-par to Transformer based models. Furthermore,  concludes that language models handling entity decisions do not improve in performance with the addition of more hidden units and that the source data is of limited number and of specific genre which do not highlight the benefits of explicit entity information. , through attention head probing, experimentally proves that BERT does model anaphoric phenomenon in the form of antecedent selection, with attention heads directly attending to the respective mention's antecedent. However, these information are not explicitly used to further enhance the model. Furthermore, ERNIE , which uses knowledge graphs to infuse entity information to the model, only does so for named entities, completely ignoring pronouns and nominal mentions.    Our study constitutes the first attempt of modeling automatic translation for the extremely low-resource language of Bambara. We identified challenges for future work, such as the development of alignment tools for small-scale datasets, and the need for a general domain evaluation set. The current limitation of processing written text as input might furthermore benefit from the integration of spoken resources through speech recognition or speech translation, since Bambara is primarily spoken and the lack of standardization in writing complicates the creation of clean reference sets and consistent evaluation.     
","  In the last decade, the field of Neural Language Modelling has witnessed enormous changes.  With pretrained neural language models being the current go-to approach in all NLP reserach, a variety of methods models have been developed. We distinguish two major categories:  \paragraph{General purpose language models.} Steady improvements have been achieved to this field with the use of deep RNNs and pre-training on a large number of training data . With Transformers, language models have been able to capture longer linguistic structures without the use of RNNs and surpass their RNN counterparts by a big margin . Recent research has focused on ways of taking advantage of more context  and introducing effective methodologies to scale up the models and train them .   \paragraph{Language modelling with entity decisions.} YangLM  was the first to incorporate entity decisions to a language model by introducing learnable entity embeddings. Alternative entity handling mechanisms are introduced in both EntityNLM  and SetLM  in addition to a length variable for EntityNLM. All of the aforementioned approaches are RNN-based and hence their performance is expected to be sub-par to Transformer based models. Furthermore,  concludes that language models handling entity decisions do not improve in performance with the addition of more hidden units and that the source data is of limited number and of specific genre which do not highlight the benefits of explicit entity information. , through attention head probing, experimentally proves that BERT does model anaphoric phenomenon in the form of antecedent selection, with attention heads directly attending to the respective mention's antecedent. However, these information are not explicitly used to further enhance the model. Furthermore, ERNIE , which uses knowledge graphs to infuse entity information to the model, only does so for named entities, completely ignoring pronouns and nominal mentions.",160
"  Sequence labeling tasks are essential in web mining, such as named entity recognition , event extraction, and relation identification. For example, the NER models assign the predefined labels to tag tokens in the input sequences to indicate both the entity boundaries and types. In some web services, such as question answering, sequence labeling also plays a critical role, where it reads a passage in a Web page as the context and answers a given question by extracting a text span inside the given passage. This process is often called machine reading comprehension . MRC is also regarded as a sequence labeling task, since it predicts whether each token is the start, end, or none for the answer span.  There is a rich literature for sequence labeling. Classical methods include Hidden Markov models , maximum entropy Markov models , and conditional random field . Recently, combining neural networks as the representation layer with CRF models has further boosted the state-of-the-art performance. However, such statistical models require large amounts of training data. Consequently, they only show good performance in languages with rich training data, such as English. Sequence labeling on low-resource languages is still very challenging, mainly due to very limited training data available.  To tackle the challenge of sequence labeling in low-resource languages, some early works transfer the knowledge from rich-source languages to low-resource ones by information alignment through manually built bilingual parallel corpora,  or language-independent features. In recent years, multilingual pre-trained language models, such as Unicoder, mBERT, and XLM-Roberta , are developed for model transferring. For example, Wu et al.  fine-tune mBERT on a pseudo training set by a meta-learning method. To better leverage the unlabeled data in the target language, a teacher-student framework is proposed to distill knowledge from weighted teacher models. Inspired by back translation in neural machine translation , DualBERT is developed to learn source language and target language features simultaneously. Although these multilingual sequence labeling models can effectively locate target spans, they often fail to give the precise boundaries of the spans in the target languages.  %when predicting text spans in the target languages.  %that is, pairs of sentences with similar meanings but in different languages, %\jp{What is the conclusion we can draw from this paragraph?}  %The previous multilingual sequence labeling models can roughly identify the correct target spans, but often fail to give the precise boundaries when predicting text spans in the target languages.  We conduct an empirical study to quantitatively assess the challenge. In Figure , we categorize the mismatches between the predicted span and the ground truth span into four types:  the predicted answer is a super span of the ground truth;  the predicted answer is sub span of the ground truth;  the predicted answer both miss some terms in the ground truth and add extra terms not in the ground truth , and   the predicted answer is adjacent to the ground truth but contains no common sub-span with it .  We further show in Table the statistics of the error cases in the cross-lingual NER task using the XLM-R model, where the boundary errors, including super span, sub span, drifted span, and adjacent span, contribute to a large portion of all error cases as shown in the last column. The other errors cases are mainly entity type detection errors. This observation motivates us to tackle the bottleneck of boundary detection in sequence labeling models.           % \end{center}      \end{table}  Accurately detecting answer boundaries becomes a bottleneck in sequence labeling.  To tackle the challenge, in this paper, we propose a separate model for boundary calibration based on the output of a base model. Intuitively, the base model captures the global context of the whole input sequence and roughly locates the region for answers. Then, the calibration model conducts finer search within the detected region and the neighborhood, and focuses on the local context to refine the boundary. This is analogous to the human perception and cognition process, which first locates the target, sets up the local context, and finally zooms into details.  Our design is novel for sequence labeling, and is orthogonal and complements to all existing approaches.  Using a second model to focus on detecting answer boundaries accurately is an intuitive and nice idea.  However, how to construct high-quality training data for the calibration model remains challenging. One straightforward method is to transform the original training data of sequence labeling task into a new training set for calibration model. However, the data collected in this way is still quite limited, especially for low-resource languages. To address this challenge, we strategically propose a novel phrase boundary recovery  task to pre-train the model on large-scale augmented datasets synthesized from Wikipedia documents in multiple languages. The new pre-training approach dramatically improves the capability of the calibration module to determine answer boundaries accurately.  % Besides the design of employing two models, we further equip the calibration model with a pre-training process by emphasizing on the capability of recovering meaningful phrases from noisy input.  Our approach is shown in Figure. CalibreNet consists of two modules, a base module and a calibration module. The base module can take any model of sequence labeling. The predicted answers by the base module are combined with the input sequence to form the input to the calibration module. The calibration module considers both the initial results by the base module and the whole passage to refine the span boundaries. In particular, the calibration module is pre-trained with the PBR task on large-scale multilingual synthesized data from Wikipedia-derived corpus.  We make the following technical contributions in this paper. First, we propose the CalibreNet framework for the task of cross-lingual sequence labeling to improve the accuracy of labeled answers. Second, we propose a novel phrase boundary recovery task and a weakly supervised pre-training method using Wikipedia data. This approach effectively enhances the model sensitivity to phrase boundaries. Last but not least, we conduct extensive experiments on zero-shot cross-lingual NER and improve the SOTA results. In addition, the experiments on the MRC tasks also show consistent improvement over strong baseline methods.  The rest of the paper is organized as follows. We first review the related work in Section. We then present our approach in Section. We report the extensive experimental results in Sections.  We conduct further analysis in Section, and conclude the paper in Section.     To tackle the challenge of having only very limited training data in low-source languages for sequence labeling tasks, two major approaches are explored.  The first approach is to transfer knowledge from a source language with rich labeled data to a target language with little or even no labeled data. This is called cross-lingual sequence labeling. The other approach is to look for additional data sources and conduct weakly supervised learning.   In this paper, we focus on two multilingual sequence labeling tasks, NER and MRC.   \paragraph{Named Entity Recognition}.    The previous methods for cross-lingual named entity recognition  can be divided into data transfer methods and model transfer methods, according to the knowledge transfer mechanisms. Data transfer methods generate annotations in target languages that are used for model training. For example, \citeauthor{wang2014cross} employ bilingual parallel corpora and transfer labels through word alignment. Since parallel data may not be available, some other methods apply machine translation as a substitution for parallel data. For example, phrase alignment or word alignment can be used to transfer labels. However, the performance of these methods is limited by the translation quality, especially for low-resource languages.   The idea of model transfer is to generate language-independent features for NER models. By training models on source language, those methods automatically scale out to other languages through language-independent features. Some representative methods leverage gazetteers and Wikifier features, and align word representations. Most recent studies make great improvements by multilingual pre-trained language models. To reduce the dependency on training data in source languages, Wu et al. propose a knowledge distillation method, which distills knowledge from a multilingual teacher model to a student model through unlabeled data in target languages. Wang et al. develop another knowledge distillation method to transfer the structural knowledge from several monolingual models into a single multilingual model. Their motivation is to reduce the gap between a universal model for multiple languages with individual monolingual models. Our method also belongs to the model transfer category. We tackle the inaccurate boundaries of predicted entities, an issue overlooked by the previous methods. Therefore, all previous methods can be adopted as the base module in our approach.  As for knowledge distillation,   Different from previous methods, which transfer knowledge from source language, Cao et al. propose to generate weakly labeled data from Wikipedia pages in low-resource languages. Our method also uses Wikipedia anchor text as ground truth entities, but we do not directly use them for training a NER model. Instead, we synthesize initial answers and pre-train the model for entity boundary detection.   \paragraph{Machine Reading Comprehension}.  Machine reading comprehension  in rich-resource languages, such as English and Chinese, has been intensively studied in the past years. However, there are only few studies on cross-lingual MRC, partially due to the lack of data sets until most recently. Artetxe et al. extract instances from SQuAD v1.1 and translate them into ten languages in total by experts. \citeauthor{lewis2019mlqa} release MLQA where the parallel  sentences containing the answers are extracted with surrounding text from Wikipedia articles and human experts are employed to translate the questions. The availability of benchmark datasets facilitates the development of cross-lingual MRC methods. Hsu et al. demonstrate the feasibility of applying multilingual pre-trained language models for zero-shot MRC. Cui et al. propose a Dual-BERT architecture to encode question, passage pairs in source and target languages together and use modified multi-head attention to enhance the information fusion. Similar to the previous NER methods, the existing MRC methods cannot handle the boundaries of answers well. Our approach provides a general framework for sequence labeling tasks, in which MRC fits well through a simple extension to the setting for NER.        In this paper, we investigate several potential functions for neural CRF models. The proposed potential functions not only integrate the emission and transition functions, but also take into consideration representations of additional neighboring words. Our experiments show that D-Quadrilinear achieves the best overall performance. Our proposed approaches are simple and effective and could facilitate future research in neural sequence labeling.  
","  To tackle the challenge of having only very limited training data in low-source languages for sequence labeling tasks, two major approaches are explored.  The first approach is to transfer knowledge from a source language with rich labeled data to a target language with little or even no labeled data. This is called cross-lingual sequence labeling. The other approach is to look for additional data sources and conduct weakly supervised learning.   In this paper, we focus on two multilingual sequence labeling tasks, NER and MRC.   \paragraph{Named Entity Recognition}.    The previous methods for cross-lingual named entity recognition  can be divided into data transfer methods and model transfer methods, according to the knowledge transfer mechanisms. Data transfer methods generate annotations in target languages that are used for model training. For example, \citeauthor{wang2014cross} employ bilingual parallel corpora and transfer labels through word alignment. Since parallel data may not be available, some other methods apply machine translation as a substitution for parallel data. For example, phrase alignment or word alignment can be used to transfer labels. However, the performance of these methods is limited by the translation quality, especially for low-resource languages.   The idea of model transfer is to generate language-independent features for NER models. By training models on source language, those methods automatically scale out to other languages through language-independent features. Some representative methods leverage gazetteers and Wikifier features, and align word representations. Most recent studies make great improvements by multilingual pre-trained language models. To reduce the dependency on training data in source languages, Wu et al. propose a knowledge distillation method, which distills knowledge from a multilingual teacher model to a student model through unlabeled data in target languages. Wang et al. develop another knowledge distillation method to transfer the structural knowledge from several monolingual models into a single multilingual model. Their motivation is to reduce the gap between a universal model for multiple languages with individual monolingual models. Our method also belongs to the model transfer category. We tackle the inaccurate boundaries of predicted entities, an issue overlooked by the previous methods. Therefore, all previous methods can be adopted as the base module in our approach.  As for knowledge distillation,   Different from previous methods, which transfer knowledge from source language, Cao et al. propose to generate weakly labeled data from Wikipedia pages in low-resource languages. Our method also uses Wikipedia anchor text as ground truth entities, but we do not directly use them for training a NER model. Instead, we synthesize initial answers and pre-train the model for entity boundary detection.   \paragraph{Machine Reading Comprehension}.  Machine reading comprehension  in rich-resource languages, such as English and Chinese, has been intensively studied in the past years. However, there are only few studies on cross-lingual MRC, partially due to the lack of data sets until most recently. Artetxe et al. extract instances from SQuAD v1.1 and translate them into ten languages in total by experts. \citeauthor{lewis2019mlqa} release MLQA where the parallel  sentences containing the answers are extracted with surrounding text from Wikipedia articles and human experts are employed to translate the questions. The availability of benchmark datasets facilitates the development of cross-lingual MRC methods. Hsu et al. demonstrate the feasibility of applying multilingual pre-trained language models for zero-shot MRC. Cui et al. propose a Dual-BERT architecture to encode question, passage pairs in source and target languages together and use modified multi-head attention to enhance the information fusion. Similar to the previous NER methods, the existing MRC methods cannot handle the boundaries of answers well. Our approach provides a general framework for sequence labeling tasks, in which MRC fits well through a simple extension to the setting for NER.",161
"  The Text-to-SQL task aims to translate natural language texts into SQL queries. Users who do not understand SQL grammars can benefit from this task and acquire information from databases by just inputting natural language texts. Previous works  focus on context-independent text-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task.   Recently, context-dependent text-to-SQL task has attracted more attention. \citet{suhr2018learning} conduct experiments on ATIS dataset . Besides, two cross-domain context-dependent datasets SParC  and CoSQL  are released. Cross-domain means databases in test set differ from that in training set, which is more challenging.   EditSQL  is the previous state-of-the-art model on SParC and CoSQL datasets and it focuses on taking advantages of previous utterance texts and previously predicted query to predict the query for current turn. Table  shows the user inputs, ground truth queries and predicted queries of EditSQL for an interaction. In the second turn, EditSQL views ``Kacey"" as the name of a dog owner. However, since the context of the interaction is about dogs, ``Kacey"" should be the name of a dog. This example shows that a model using only historical information of user inputs may fail to keep context consistency and maintain thematic relations.   According to  and , to maintain thematic relations, users may change constraints, ask for different attributes for the same topic when they ask the next questions. Thus, database schema items  in current turn should have relation with items in previous turn. For example, in Table , the second question  adds a constraint of the name and asks for the age of a dog instead of the numbers of all dogs. The corresponding database schema items Dogs.age and Dogs.name in   belong to the same table as Dogs.* in previous query . Therefore, we propose to take historical information about database schema items into consideration.  % %     %     %\end{table}   In particular, we first construct a graph based on corresponding database, where graph nodes are database schema items and graph edges are primary-foreign keys and column affiliation. Short distance between graph nodes appearing in previous query and current query can reveal the context consistency since there is usually an edge between the different attributes of the same topic. We then propose a database schema interaction graph encoder to model database schema items together with historical items. Empirical results on two large cross-domain context-dependent text-to-SQL datasets - SParC and CoSQL show that our schema interaction graph encoder contributes to modeling context consistency and our proposed model with database schema interaction graph encoder substantially outperforms the state-of-the-art model.                \end{table}  Our main contributions are summarized as follows:        Many studies have focused on context-independent text-to-SQL task. \citet{DBLP:journals/corr/abs-1709-00103} split the vocabulary and use reinforcement learning. \citet{DBLP:journals/corr/abs-1711-04436} propose a sketched-based model, which decomposes the token prediction process into SELECT-clause prediction and WHERE-clause prediction, aiming at taking previous predictions into consideration. \citet{yu2018syntaxsqlnet} further employ a tree-based SQL decoder so as to decode SQL queries with the help of SQL grammar. In order to encode database schemas, schemas are regarded as graphs and graph neural networks have been applied . \citet{guo2019towards} design an intermediate representation to bridge the gap between natural language texts and SQL queries. \citet{choi2020ryansql} utilize a sketch-based slot filling approach to synthesize SQL queries. \citet{wang2019ratsql} attempt to align the database columns and their mentions in user inputs by using a relation-aware self attention.   Recently, context-dependent text-to-SQL task has drawn people's attention. In-domain context-dependent benchmarks ATIS  have been proposed. For ATIS, \citet{suhr2018learning} utilize a sequence to sequence framework. Besides, they introduce an interaction-level encoder for incorporating historical user inputs and a segment copy mechanism to reduce the length of generation. Later, two large and complex cross-domain context-dependent dataset SParC  and CoSQL  are proposed. In order to tackle cross-domain context-dependent text-to-SQL task, \citet{zhang-emnlp19} propose the EditSQL model in order to capture features from historical user inputs, variant database schemas and previously predicted SQL query. \citet{liu2020far} further evaluate context modeling methods and apply a grammar-based decoder. EditSQL achieves the state-of-the-art performance on the two cross-domain datasets.  Compared to EditSQL, our work further explore a new way to employ historical information of database schemas.         In this paper, we tackle the challenge of detecting span boundaries more precisely for sequence labeling tasks in low-resource languages. We propose the CalibreNet architecture as well as a novel Phrase Boundary Recovery task for more accurate boundary detection. Extensive experimental results verify the effectiveness of our approach and the generalization capability for multiple languages. As future works, we plan to introduce entity type prediction in the pre-training task, and also develop better methods for question generation for the MRC task.        The acknowledgments section is defined using the ""acks"" environment    . This ensures the proper    identification of the section in the article metadata, and the    consistent spelling of the heading.         The next two lines define the bibliography style to be used, and    the bibliography file.          If your work has an appendix, this is the place to put it.       
","  Many studies have focused on context-independent text-to-SQL task. \citet{DBLP:journals/corr/abs-1709-00103} split the vocabulary and use reinforcement learning. \citet{DBLP:journals/corr/abs-1711-04436} propose a sketched-based model, which decomposes the token prediction process into SELECT-clause prediction and WHERE-clause prediction, aiming at taking previous predictions into consideration. \citet{yu2018syntaxsqlnet} further employ a tree-based SQL decoder so as to decode SQL queries with the help of SQL grammar. In order to encode database schemas, schemas are regarded as graphs and graph neural networks have been applied . \citet{guo2019towards} design an intermediate representation to bridge the gap between natural language texts and SQL queries. \citet{choi2020ryansql} utilize a sketch-based slot filling approach to synthesize SQL queries. \citet{wang2019ratsql} attempt to align the database columns and their mentions in user inputs by using a relation-aware self attention.   Recently, context-dependent text-to-SQL task has drawn people's attention. In-domain context-dependent benchmarks ATIS  have been proposed. For ATIS, \citet{suhr2018learning} utilize a sequence to sequence framework. Besides, they introduce an interaction-level encoder for incorporating historical user inputs and a segment copy mechanism to reduce the length of generation. Later, two large and complex cross-domain context-dependent dataset SParC  and CoSQL  are proposed. In order to tackle cross-domain context-dependent text-to-SQL task, \citet{zhang-emnlp19} propose the EditSQL model in order to capture features from historical user inputs, variant database schemas and previously predicted SQL query. \citet{liu2020far} further evaluate context modeling methods and apply a grammar-based decoder. EditSQL achieves the state-of-the-art performance on the two cross-domain datasets.  Compared to EditSQL, our work further explore a new way to employ historical information of database schemas.",162
" The recent survey conducted by WHO shows that a total  million people in the world are living with depression.  % This has increased by 18.4\% between  and .  At its most severe, depression can lead to suicide and is responsible for  deaths every year  . Early detection and appropriate treatment can encourage remission and prevent relapse . However, the stigma coupled with the depression makes patients reluctant to seek support or provide truthful answers to physicians .  Additionally, clinical diagnosis is dependent on the self-reports of the patient閳ユ獨 behavior, which requires them to reflect and recall from the past, that may have obscured over time. In contrast, social media offers unique platform for people to share their experiences in the moment, express emotions and stress in their raw intensity, and seek social and emotional support for resilience. As such, the depression studies based on social media offer unique advantages over scheduled surveys or interviews . Social media self-narratives contain large amounts of implicit and reliable information expressed in real-time, that are  essential for practitioners to glean and understand user閳ユ獨 behavior outside of the controlled clinical environment. % \indent Several studies in the literature have explored various linguistic and visual cues to effectively detect user depression from the postings on social media platform like Twitter  and Reddit . Majority of these existing studies have formulated the social media depression detection task as a binary classification problem  and therefore are limited to only identifying the depressive users. \\ \indent To assist healthcare professionals  intervene in a  timely manner such as with an automatic triaging, it is necessary to develop an intelligent decision support system that provides HPs fine-grained depression related symptoms. The triage process is a critical step in giving care to the patients because, by prioritizing patients at different triage levels based on the severity of their clinical condition, one can enhance the utilization of healthcare facilities and the efficacy of healthcare interventions. There have been a few efforts to create datasets for capturing depression severity, however they are limited to  only clinical interviews  and questionnaires , and  individuals who voluntarily participate in the study .  \\ \indent In this work, we exploit the Twitter data to identify the indications  of depression. We developed a high quality dataset consisting of total  tweets, with  tweets posted by  self-reported depressed users over  weeks time, which were manually annotated using  questionnaire  based symptoms categories. In Table-, we provide sample tweets associated with the nine item  depression symptoms. % The  is a self-reported questionnaire, based on the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition  guidelines, for screening, diagnosing, and measuring severity of depression.  % The overall  scores range between  and , with a score of  or more linked to major depressive disorders. Our research hypothesis is that depressed individuals discuss their symptoms on Twitter that can be tracked reliably.  }   \end{table*} % Advancement in Natural Language Processing  is one of the most promising avenues for discovering vital mental health information from user-generated data. Nonetheless, user social-media post offer unique challenges as discussed below:   To account for this creative linguistic device widely observed in utterances of depressive users, we propose a Figurative Language enabled Multi-Task Learning framework  that works on the concept of task sharing mechanism . In this work, we improve the performance and robustness of the  for the primary task of `symptom identification' combined with the supervisory task `figurative usage detection' in a multi-task learning setting.  We introduce a mechanism  named `co-task aware attention' which enables the layer-specific soft sharing of the parameters for the tasks of interest. The proposed attention mechanism is parameterized with the task-specific scaling factor for BERT  layers. BERT enables even the low-resource tasks to benefit from deep bi-directional architectures and the unsupervised training framework to obtain the context-aware encoded representation. The virtue of this model is its ability to learn the task-specific representation of the input tweet by coordinating among the layers and between the tasks. \\ Contributions:   %%%%%%%%%%%%%%%%%%%%%  % % According to Word Health Organization \footnote{http://www.who.int/news-room/fact-sheets/detail/mental-disorders}, ``depressive disorder is characterized by sadness, loss of interest or pleasure, feelings of guilt or low self worth, disturbed sleep or appetite, feelings of tiredness, and poor concentration"".  % Major depressive disorder  has a world-wide impact on society with each year causing almost one million deaths. % The recent survey conducted by WHO shows that total  million people in the world are living with depression. This has increased by 18.4\% between  and . At its most severe depression can lead to suicide and is responsible for  deaths every year  . Early detection and appropriate treatment can encourage remission and prevent relapse . However, the stigma coupled with the depression makes patients reluctant to seek support. Also, the associated cognitive biases, inhibits the patients to provide truthful answer to physicians which further add limitation .\\ % \indent Additionally, clinical diagnosis is dependent on the hypothetical self-reports of patients behaviour, requiring patients to reflect on what they were doing and thinking sometime in the past, which may have become obscured over time. In contrast, social media offers unique platform for people to share their experiences, exhaust emotion and stress, and seek social and emotional support. As such, the depression studies based on social media offers several advantage . These self-narrative contains large amount of the implicit information, which are highly essential for practitioner to understand users behaviour outside the controlled clinical environment and in real-time.\\ % \indent Several studies in literature have explored various linguistic and visual cues to effectively detect depression from the social media platform like Twitter and Reddit. Majority of these existing studies have formulated the social media depression detection task as a binary classification problem  and therefore is only limited to identify the depressive users. \\ % \indent However, to assist healthcare professional  in making timely intervention, it is required to develop an intelligent decision support system that could provide HPs with more fine-grained depression related symptoms and automatic triaging techniques. The triage process is the first critical step in giving care to the patients by prioritizing patients at different triage levels based on the severity of their clinical conditions that could have the potential to enhance the efficacy of healthcare interventions. In literature, there has been few efforts to create dataset for capturing depression severity, however they are limited to  only clinical interview  and questionnaire , and  individuals who voluntary participated in the study.  \\ % \indent In this work, we exploit the Twitter data to identify the indications  of depression and finally assign  based severity labels: `None', `Mild', `Moderate', `Moderately Severe', and `Severe' for triaging. We developed a new dataset consisting  tweets posted by  self-reported depressed users over  weeks time, which are manually annotated into  symptom categories. In Table-, we provide the samples tweets associated with nine item  depression symptoms. % The  is a self-report questionnaire based on the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition  guidelines for screening, diagnosing, and measuring severity of depression. The overall  scoring ranges between  and , with  or more highly linked to major depressive disorders. % Our research hypothesis is that depressed individuals discuss their symptoms on Twitter.\\ % %This work aim to develop an intelligent decision support system in the context of major depressive disorder by providing the healthcare professionals  with more fine-grained depression related symptoms and automatic triaging technique that is required by HPs to make timely intervention. The triage process is the first critical step in giving care to the patients by prioritizing patients at different triage levels based on the severity of their clinical conditions that could have the potential to enhance the efficacy of healthcare interventions.\\ %  % } %  %  % \end{table*} % Advancement in Natural Language Processing  technology is one of the most promising avenues for discovering vital mental health information from user-generated data. Nonetheless, these texts offers some inherently distinct challenges discussed as follows: %  % Furthermore, previous studies utilizing social media data in biomedical natural language processing task reported prediction error when drug or symptom names are utilized in figurative sense. To account for this creative linguistic devices widely observed in utterances of depressive users, we proposed a multitask learning  framework that works on the concept of task sharing mechanism. Multi-task learning has been proven to the a useful instruments to improve the generalization performance of the primary task with related auxiliary tasks. In this work, we focused to improve the performance and generalization ability of the proposed model for the primary task of `symptom identification' in companionship with the supervisory task `figurative language detection'. We introduce a mechanism  named `co-task aware attention' which enables the layer-specific soft sharing of the parameters for the task at interest. The proposed attention mechanism is parameterize with the task-specific scaling factor for BERT layers. To the virtue of the this, the model is able to learn the task-specific representation of the input tweet by coordinating among the layers and between the tasks. \\ % Contributions: %    Depending upon the data modalities and depressive markers, we categorize the existing literature as follows:     Depending upon the data modalities and depressive markers, we categorize the existing literature as follows:      ##########################################################    In this paper, we focus on context-dependent cross-domain SQL generation task. We find that previous state-of-the-art model only takes historical user inputs and previously predicted query into consideration, but ignores the historical information of database schema items. Thus we propose a model named IGSQL to model database schema items in a conversational scenario. Empirical results demonstrate the efficacy of our model. We also conduct ablation experiments to reveal the significance of our database schema interaction graph encoder. For future work, we  will explore methods attempting to solve hard and extra hard questions.   \section*{Acknowledgments}  This work was supported by National Natural Science Foundation of China , Beijing Academy of Artificial Intelligence  and Key Laboratory of Science, Technology and Standard in Press Industry . We appreciate the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.  
"," Depending upon the data modalities and depressive markers, we categorize the existing literature as follows:     Depending upon the data modalities and depressive markers, we categorize the existing literature as follows:      ##########################################################",163
"  Early detection of dementia is important for improving clinical outcomes and management of dementia, as well as for lifestyle, financial, and future planning for patients and their caregivers . Yet, dementia is not formally diagnosed or coded in claims for over 50\% of older adults living with probable dementia . Tools that screen medical records for warning signs and present the digested information to providers may prove to be an important step for early intervention.  In this study, we aim to use NLP to detect signs of cognitive dysfunction from clinician notes in electronic health records  by applying deep learning techniques that have not been hitherto applied to this problem. We present an attention-based transformer model that allows for long text sequences to reveal signs of cognitive concerns and compare its performance to baseline models.      Prior works have used pattern based text-analytics to detect dementia in electronic health records. Dementia detection using NLP on provider notes has been applied to a cohort of acute-care EHRs of hip and stroke fracture patients ; a sample of patients enrolled in the UCLA Alzheimer's and Dementia Care  program ; and a group of patients who had a formal cognitive evaluation by the Mayo Clinic Study of Aging and were hospitalized at their institution . These studies demonstrated that the incorporation of NLP on EHR notes improves sensitivity of dementia detection. The current work uses deep learning based NLP, which has achieved numerous breakthroughs when applied to general text thanks to the use of word embeddings and attention-based models , while it has had limited application on healthcare data. Our work builds upon the idea in , where a deep-learning based model was applied to classify the outcome for treatment of depression, a phenotype also not captured well by structured data or term-based NLP, therefore motivating the use of deep learning techniques.       Our evaluation experiments on two coherence datasets reveal that RNN- or EGrid-based coherence models are able to detect syntactic alterations that undermine coherence, but are less effecient at detecting semantic ones even after fine-tuning on the latter.  We furthermore find that they particularly struggle with recognizing minor lexical changes even if they result in implausible meaning and resolving pronominal references.  On the other hand, these models are particularly good at detecting cases where a prefix is inserted or the subject pronoun is substituted with a lexical item, suggesting that they are capable of capturing the relevant syntactic patterns and do not solely rely on positional features.   We find that the best performing model overall is LCD which does not use an RNN sentence encoder but rather builds sentence representations by averaging BERT embeddings then utilizes a number of linear transformations over adjacent sentences to facilitate learning richer representations.    Our probing experiments reveal that models are better at encoding information regarding subject and object number  followed by verb number . These probing tasks align with Centering theory as they probe for subject and object relevant information. The task that tests for knowledge on coordination inversion is the lowest performing one overall, suggesting that there is little capacity at capturing information related to intra-sentential coherence. Excluding LCD, MTL is the best performing model; nevertheless, there is still scope for substantial improvement across all probing tasks and particularly on CoordInv and CorruptAgr.      \section{Conclusion}  We systematically studied how well current models of coherence can capture aspects of text implicated in discourse organisation. We devised datasets of various kinds of incoherence and examined model susceptibility to syntactic and semantic alterations. Our results demonstrate the models are robust with respect to corrupted syntactic patterns, prefix insertions and lexical substitutions. However, they fall short in capturing rhetorical and semantic corruptions, lexical perturbations and corrupt pronouns. We furthermore find that discourse embedding space encodes subject and object relevant information; however, there is scope for substantial improvement in terms of encoding linguistic properties relevant to discourse coherence. Experiments on coordination inversion further suggest that current models have little capacity at encoding information related to intra-sentential coherence.  We hope this study shall provide further insight into how to frame the task of coherence modeling and improve model performance further. Finally, we make our datasets publicly available for researchers to use to test coherence models.        
","  Prior works have used pattern based text-analytics to detect dementia in electronic health records. Dementia detection using NLP on provider notes has been applied to a cohort of acute-care EHRs of hip and stroke fracture patients ; a sample of patients enrolled in the UCLA Alzheimer's and Dementia Care  program ; and a group of patients who had a formal cognitive evaluation by the Mayo Clinic Study of Aging and were hospitalized at their institution . These studies demonstrated that the incorporation of NLP on EHR notes improves sensitivity of dementia detection. The current work uses deep learning based NLP, which has achieved numerous breakthroughs when applied to general text thanks to the use of word embeddings and attention-based models , while it has had limited application on healthcare data. Our work builds upon the idea in , where a deep-learning based model was applied to classify the outcome for treatment of depression, a phenotype also not captured well by structured data or term-based NLP, therefore motivating the use of deep learning techniques.",164
" We introduce \diagnnose, an open source library for analysing deep neural networks. The \diagnnose library allows researchers to gain better insights into the internal representations of such networks, providing a broad set of tools of state-of-the-art analysis techniques. The library supports a wide range of model types, with a main focus on NLP architectures based on LSTMs  and Transformers .  Open-source libraries have been quintessential in the progress and democratisation of NLP. Popular packages include HuggingFace's   -- allowing easy access to pre-trained Transformer models; % AllenNLP  -- providing useful abstractions over components in the NLP pipeline,   -- focusing on multitask and transfer learning within NLP;   -- providing a range of feature attribution methods; and   -- a platform for visualising and understanding model behaviour. We contribute to the open-source community by incorporating several \mbox{interpretability} techniques that have not been present in these packages.  Recent years have seen a considerable interest in improving the understanding of how deep neural networks operate . The high-dimensional nature of these models makes it notoriously challenging to untangle their inner dynamics. This has given rise to a novel subfield within AI that focuses on interpretability, providing us a peak inside the black box. \diagnnose aims to unify several of these techniques into one library, allowing interpretability research to be conducted in a more streamlined and accessible manner.  \diagnnose's main focus lies on techniques that aid in uncovering linguistic knowledge that is encoded within a model's representations. The library provides abstractions that allow recurrent models to be investigated in the same way as Transformer models, in a modular fashion. It contains an extensive activation extraction module that allows for the extraction of  model activations on a corpus. The analysis techniques that are currently implemented include:   % <design principles> ?  In this paper we present both an overview of the library, as well as a case study on subject-verb agreement within language models. We first present a brief overview of interpretability within NLP and a background to the analysis techniques that are part of the library . We then provide an overview of \diagnnose and expand briefly on its individual modules . % Next, we provide a more extensive background on the feature attributions that are part of the library . We conclude with a case study on subject-verb agreement, demonstrating several of \diagnnose's features in an experimental setup .     The increasing capacities of language models  have led to a rich field of research that aims to gain a better understanding of how these models operate. Approaches in this research area are often interdisciplinary in nature, borrowing concepts from fields such as psycho-linguistics, information theory, and game theory. \diagnnose provides support for several influential analysis techniques, for which we provide a brief background here.   Language models have stood at the basis of many successes within NLP in recent years . These models are trained on the objective of predicting the probability of an upcoming  token. In order to succeed in this task, these models need to possess a notion of many different linguistic aspects, such as syntax, semantics, and general domain knowledge. One popular line of research that tries to uncover a model's linguistic capacities does this via so-called {targeted syntactic evaluations} . This type of analysis compares a model's output on minimally different pairs of grammatical and ungrammatical constructions. If it assigns a higher probability to the grammatical construction, the model is said to possess a notion of the underlying linguistic principles, such as subject-verb agreement or NPI licensing:           \ex  \diagnnose supports a wide range of syntactic tasks, as well as an interface that allows new tasks to be added without effort.     Targeted syntactic evaluations assess a model's notion of linguistics in a behavioural fashion, based on the default output behaviour of the model. A second line of work tries to assess a model's understanding of linguistic properties -- such as part-of-speech tags or number information -- by directly training diagnostic classifiers on top of its representations . This type of analysis, also referred to as probing, has led to numerous insights into the inner workings of language models . The activations diagnostic classifiers are trained on are not restricted to just the hidden states of a language model at their top layer: this can, for instance, also be done on the individual gate activations to reveal patterns at the cell-level of a model .  Recently, it has been a topic of discussion to what extent a high accuracy of a diagnostic classifier signifies that that property is actively being encoded by the model. Several solutions to assess this have been proposed, such as training a diagnostic classifier on a baseline of random labels , or based on the minimum description length of the classifier, a concept from information theory . \diagnnose currently facilitates the training of diagnostic classifiers, as well as training control tasks alongside them.   Although probing allows us to uncover specific properties that are embedded within the model representations, it is unable to explain how a model transforms its input features into a successful prediction. This question can be addressed by computing the input feature contributions to a subsequent output. This is a challenging task, as the high-dimensional, non-linear nature of deep learning models prevents us from expressing these contributions directly on the basis of the model parameters.  Feature attributions can be computed in different ways. One common approach to this task is based on a concept that stems from cooperative game theory, called the Shapley value . A Shapley value expresses the contribution of a player  to the outcome of game . Computing Shapley values is computationally expensive, and several approximation algorithms have therefore been proposed, such as SHAP , and Integrated Gradients . \diagnnose currently facilitates the computation of feature attributions using a technique called Contextual Decomposition , and its generalisation as proposed by \citet{jumelet-etal-2019-analysing}.       We have presented a multi-task learning framework to enable the training of one universal incremental model with four tasks of disfluency detection, language modelling, part-of-speech tagging and utterance segmentation. We have observed that these tasks produce favorable inductive biases to each other, with utterance segmentation and disfluency detection getting the most benefits. We note that each task's optimal weighting relies heavily on the severity of the noise from the task. We showed that word timing information helps utterance segmentation and disfluency detection in an online setting, and adding new tasks with the exception of language modelling does not have a remarkable negative effect on the incremental metrics.   The results show that our framework can be suitable for online conversational systems, such as conversational agents in the mental health domain. In future work, we intend to analyze the interactions between different tasks as they occur in real time. Monitoring the interaction after each word could help highlight informative moments that contribute more to optimisation of our models. Furthermore, we intend to use raw acoustic features as the input for a strongly time-linear model.   include your own bib file like this:  
","  The increasing capacities of language models  have led to a rich field of research that aims to gain a better understanding of how these models operate. Approaches in this research area are often interdisciplinary in nature, borrowing concepts from fields such as psycho-linguistics, information theory, and game theory. \diagnnose provides support for several influential analysis techniques, for which we provide a brief background here.   Language models have stood at the basis of many successes within NLP in recent years . These models are trained on the objective of predicting the probability of an upcoming  token. In order to succeed in this task, these models need to possess a notion of many different linguistic aspects, such as syntax, semantics, and general domain knowledge. One popular line of research that tries to uncover a model's linguistic capacities does this via so-called {targeted syntactic evaluations} . This type of analysis compares a model's output on minimally different pairs of grammatical and ungrammatical constructions. If it assigns a higher probability to the grammatical construction, the model is said to possess a notion of the underlying linguistic principles, such as subject-verb agreement or NPI licensing:           \ex  \diagnnose supports a wide range of syntactic tasks, as well as an interface that allows new tasks to be added without effort.     Targeted syntactic evaluations assess a model's notion of linguistics in a behavioural fashion, based on the default output behaviour of the model. A second line of work tries to assess a model's understanding of linguistic properties -- such as part-of-speech tags or number information -- by directly training diagnostic classifiers on top of its representations . This type of analysis, also referred to as probing, has led to numerous insights into the inner workings of language models . The activations diagnostic classifiers are trained on are not restricted to just the hidden states of a language model at their top layer: this can, for instance, also be done on the individual gate activations to reveal patterns at the cell-level of a model .  Recently, it has been a topic of discussion to what extent a high accuracy of a diagnostic classifier signifies that that property is actively being encoded by the model. Several solutions to assess this have been proposed, such as training a diagnostic classifier on a baseline of random labels , or based on the minimum description length of the classifier, a concept from information theory . \diagnnose currently facilitates the training of diagnostic classifiers, as well as training control tasks alongside them.   Although probing allows us to uncover specific properties that are embedded within the model representations, it is unable to explain how a model transforms its input features into a successful prediction. This question can be addressed by computing the input feature contributions to a subsequent output. This is a challenging task, as the high-dimensional, non-linear nature of deep learning models prevents us from expressing these contributions directly on the basis of the model parameters.  Feature attributions can be computed in different ways. One common approach to this task is based on a concept that stems from cooperative game theory, called the Shapley value . A Shapley value expresses the contribution of a player  to the outcome of game . Computing Shapley values is computationally expensive, and several approximation algorithms have therefore been proposed, such as SHAP , and Integrated Gradients . \diagnnose currently facilitates the computation of feature attributions using a technique called Contextual Decomposition , and its generalisation as proposed by \citet{jumelet-etal-2019-analysing}.",165
" The goal of relation extraction is to extract relationships between two entities from plain text. Supervised learning methods for relation extraction have been widely used to extract relations based on training labeled data. Distant supervision or crowdsourcing have been used to collect more examples with labels and train the model for relation extraction. However, these methods are limited by the quantity  and quality  of the training data because manually labeling the data is time-consuming and labor-intensive and data labeled by distant-supervision is noisy. To overcome the problem of insufficient high-quality data, few-shot learning have been designed to require only few labeled sentences for training. A lot of research has been done on few-shot learning for computer vision~, and some work also includes few-shot learning methods for relation extraction~. Although these works only require few instances for training, they still do not work in many scenarios in which no training instances are available.  Some work on open information extraction  discovers new relationships in open-domain corpora without labeling the data. OpenIE aims to extract relation phrases directly from text. However, this technique can not effectively select meaningful relation patterns and discard irrelevant information. In addition, this technique can not discover relations if the relation's name does not appear in the given sentence. For example, OpenIE can not identify the relation of the sentence shown in Figure.  To address the aforementioned limitations, we focus on relation extraction in the context of zero-shot learning. Zero-shot learning  is similar to the way humans learn and recognize new concepts. It is a novel learning technique that does not use any exemplars of the unseen categories during training. We propose a zero-shot learning model for relation extraction , which focuses on recognizing new relations that have no corresponding labeled data available for training. ZSLRE is modified on prototypical networks utilizing side  information.  We construct side information from labels and its synonyms, hypernyms of two name entities and keywords from training sentences. The ZSL-based model can recognize new relations based on the side information available for it instead of using a collection of labeled sentences. We incorporate side information to enable our model to extract relations that never appear in the training datasets. We also build an automatic hypernym extraction framework to help us acquire hypernyms of different entities directly from web. Details of side information construction are described in Section Side Information Extraction.     Figure shows an example of how side information can be used for extract relations. Different side information are given for different relations. The query sentence in the example has a relation of classmate\_of, but the word classmate never appears in the sentence. We first get the two name entities Nell Newman and Mayday Parker of the sentence and extract the hypernyms of the name entities person and person based on our proposed hypernym extraction module in Section Hypernyms Extraction. In this example, relationship capital\_of is eliminated because the hypernyms of capital\_of should be location and location. Then we extract the keywords course and school from the query sentence and compare the distance with the keywords in side information box.  In this way, relationship children\_of is eliminated.  To make relation extraction effective in real-world scenarios, we design our models with the ability that it can extract both relations with training instances and the relations without any training instances.  We modify the vanilla prototypical networks to deal with both scenarios and compare the distance between the query sentence and the prototype. If the exponential of the minus distance is above a threshold, we consider the query sentence has a new relation. For new relations extraction, we take the side information embedding from the query sentence and compare the distance of it with the side information embedding of new relations. We conduct different experiments on both a noisy and a clean dataset and adding different percentages of new relations to evaluate the effectiveness and robustness of our proposed model. Besides, we also evaluate our proposed model in supervised learning, few-shot learning and zero-shot learning scenarios and the results show that our proposed model outperforms other existing models in all three scenarios. The contributions of this paper can be summarized as follows:    The rest of this paper is organized as follows. Section Related Work reviews work on supervised relation extraction, open relation extraction and zero-shot learning.  Section Methodology describes the proposed ZSLRE model. Section Experiments presents the experiments and compares the performance of our model with other different models on two public datasets. Section Conclusion and Future Work includes a discussion of conclusion and promising future work.    Supervised Relation Extraction. Relation Extraction  aims to extract relations between entities. Many existing relation extraction methods are based on supervised learning, where neural networks are used to automatically extract semantic features from text. For example, convolutional neural networks  are used to learn textual patterns. Recurrent neural networks  are used to better capture the sequential information present in the input data . Graph neural networks  are used to find dependencies and capture long-range relations between words . Although these traditional RE methods have achieved promising results by taking advantage of supervised or distantly-supervised data, they exhibit a key limitation since they all need large quantities of labeled training data.  Open Relation Extraction. Many existing approaches focus on discovering new relationships in open-domain corpora. This is because traditional supervised RE can not find new relation types due to their limited ability to only classify predefined relation types. Open RE or Open information extraction  aims to extract relation phrases directly from text. For example, tagging-based methods and clustering-based methods are used to discover new relation types. Other work proposed Relational Siamese Networks to transfer relational knowledge from supervised OpenRE data to calculate similarity  of unlabeled sentences for open relation clustering . However, OpenRE can not effectively select meaningful relation patterns and discard irrelevant information. In real-word scenarios, methods that rely on predefined relation types are always known to lack of training data.  Zero-shot Learning. Zero-shot learning has been widely applied in computer vision. Similar to zero-shot learning, few shot learning is well-studied in the field of relation extraction. However, compared with zero-shot learning for computer vision and few-shot learning explored in relation classification, there exists little work towards zero-shot learning in the domain of natural language processing. Some existing work uses a transferable architecture to jointly represent and map event types in order to detect unseen event types. Other work proposed a zero-shot learning method for relation extraction from webpages with unseen templates . However, this method only predicts relation types in unseen structures of webpages instead of new relation types.  The most related work to zero-shot learning for relation extraction uses zero-shot learning to extract unseen relation types by listing questions that define the relation閳ユ獨 slot values. However, this method requires external help such as a question-answering dataset annotated by human. In addition, this methods assumes that  a good reading comprehension model is learned and that  all values extracted from this model are correct.  In contrast, our proposed ZSLRE model can extract new relation types without training sentences and does not need to rely on other models. We construct side information to help us train the model without labeled training sentences. For example, some previous works use side information from knowledge graph or labels to lower the noise and improve performance in distantly-supervised relation extraction.     We proposed a multi-source embedding model, MW2V, aimed at dealing with general language variations.   Each slice obtained from the sources can represent time, geography, or field, among other dimensions.  To demonstrate its feasibility, we applied the MW2V to three newspaper datasets: The New York Times and The Guardian to study temporal variations, and a combination of both datasets to model cultural variations.  We performed an exhaustive evaluation of the method in text analysis tasks finding good quantitative and qualitative results compared to the state of the art, even for the temporal case, when the MW2V does not specifically model the time direction.  Future work includes the analysis of other applications, oriented to the exploitation of datasets, and also the possible implications of the use of a regularization parameter dependent on the slices and words,  instead of a constant one. Moreover, some more insight is needed to answer open questions raised by for this proposal, namely, to try a broader scope of languages and to evaluate its robustness.   
"," Supervised Relation Extraction. Relation Extraction  aims to extract relations between entities. Many existing relation extraction methods are based on supervised learning, where neural networks are used to automatically extract semantic features from text. For example, convolutional neural networks  are used to learn textual patterns. Recurrent neural networks  are used to better capture the sequential information present in the input data . Graph neural networks  are used to find dependencies and capture long-range relations between words . Although these traditional RE methods have achieved promising results by taking advantage of supervised or distantly-supervised data, they exhibit a key limitation since they all need large quantities of labeled training data.  Open Relation Extraction. Many existing approaches focus on discovering new relationships in open-domain corpora. This is because traditional supervised RE can not find new relation types due to their limited ability to only classify predefined relation types. Open RE or Open information extraction  aims to extract relation phrases directly from text. For example, tagging-based methods and clustering-based methods are used to discover new relation types. Other work proposed Relational Siamese Networks to transfer relational knowledge from supervised OpenRE data to calculate similarity  of unlabeled sentences for open relation clustering . However, OpenRE can not effectively select meaningful relation patterns and discard irrelevant information. In real-word scenarios, methods that rely on predefined relation types are always known to lack of training data.  Zero-shot Learning. Zero-shot learning has been widely applied in computer vision. Similar to zero-shot learning, few shot learning is well-studied in the field of relation extraction. However, compared with zero-shot learning for computer vision and few-shot learning explored in relation classification, there exists little work towards zero-shot learning in the domain of natural language processing. Some existing work uses a transferable architecture to jointly represent and map event types in order to detect unseen event types. Other work proposed a zero-shot learning method for relation extraction from webpages with unseen templates . However, this method only predicts relation types in unseen structures of webpages instead of new relation types.  The most related work to zero-shot learning for relation extraction uses zero-shot learning to extract unseen relation types by listing questions that define the relation闁炽儲鐛 slot values. However, this method requires external help such as a question-answering dataset annotated by human. In addition, this methods assumes that  a good reading comprehension model is learned and that  all values extracted from this model are correct.  In contrast, our proposed ZSLRE model can extract new relation types without training sentences and does not need to rely on other models. We construct side information to help us train the model without labeled training sentences. For example, some previous works use side information from knowledge graph or labels to lower the noise and improve performance in distantly-supervised relation extraction.",166
"  % Sentiment analysis is a text classification technique that analyses a given text and returns the nature of the underlying opinion. Therefore, sentiment analysis is widely used for tasks such as brand monitoring, political research analysis, product analysis, workforce analysis and many more. Sentiment analysis techniques could be fundamentally sub divided into two categories as lexicon-based approach and machine learning based approach. Recently introduced deep learning based sentiment analysis techniques have outperformed the lexicon based approaches and traditional machine learning approaches.  With the development of deep learning techniques such as Convolutional Neural Networks , Recurrent Neural Networks  and language independent features, the domain of sentiment analysis has reported impressive results. Over the years, many of these variants and combinations of deep learning techniques and feature representations have been used for high resourced languages such as English. There also exist certain advancements in sentiment analysis for languages such as Chinese, Arabic, Spanish and some Indic languages.   Sinhala, which is a morphologically rich Indo-Aryan language, has not experienced these advancements due to its insular and under-resourced nature. One of the main challenges is not having large enough annotated corpora. The data set from~\citet{liyanage2018sentiment} is the only publicly  available annotated data set for sentiment analysis. However it includes only 5010 comments extracted from one news source, and contains only POSITIVE and NEGATIVE samples.  %Work of~\citet{medagoda2017framework} is an example of simple solutions for Sinhala sentiment analysis. Under these approaches, rule-based techniques, lexicon based techniques, supervised and semi-supervised machine learning techniques were employed with traditional language dependent features.   The 閾夸购st experiment on using deep learning techniques for Sinhala sentiment analysis was conducted by~\citet{liyanage2018sentiment}. Under this research, basic deep learning techniques such as Long Short-Term Memory  network and CNN were used to categorize news comments as POSITIVE and NEGATIVE. %The LSTM trained with fastText embeddings outperformed traditional machine learning techniques such as Decision Tree, SVM, and Na\""ive Bayes. ~\citet{DemotteSLSTM2020Sinhala} conducted an experiment with the same data set using Sentence-State LSTM , which is a rather advanced technique where the analysis was further improved considering the n-gram features of text with word embeddings.  In this paper, we present a more comprehensive empirical study on the use of deep learning techniques for document-level sentiment analysis for Sinhala with respect to four sentiment categories as POSITIVE, NEGATIVE, NEUTRAL and CONFLICT. The experiments were conducted with the commonly used sequence models such as RNN, LSTM, Bi-LSTM, various improvements on these vanilla models such as stacking and regularization,  as well as more recent ones such as hierarchical attention hybrid neural networks and capsule networks. % for multi-class sentiment analysis using word embeddings as language independent features. These langauge independent features were able to outperform the usage of traditional language dependent features such as part of speech tagging and lexical resources.  ~Furthermore, we present a data set of 15059 comments, annotated with these four classes to be used for sentiment analysis, based on Sinhala news comments extracted from online newspapers namely GossipLanka and Lankadeepa. This is the only publicly available multi-class, multi-source dataset for Sinhala sentiment analysis.  Our code implementation, word embedding models, and annotated data set are publicly available.       %       Recent advancements of Natural Language Processing  tasks were a direct result of using deep learning techniques. In these techniques, text is treated as sequences or spatial patterns, which allowed the modeling of higher level NLP concepts beyond the boundaries of the meaning of words in natural language. CNNs and LSTMs were the proper representatives under this paradigm.   With respect to sentiment analysis, linguistic knowledge such as sentiment lexicons and POS tags has been utilized as auxiliary input for the deep learning models, to capture deeper levels of language specific features for greater success. However, formulating language specific linguistic knowledge needs considerable human effort. Another approach is to experiment different combinations and variation of deep learning techniques, as an end to end solution, considering both the sequential nature and local n-gram information of text.   CNNs and RNNs were basically experimented as deep learning strategies and many variants and combinations of these neural networks have been used for sentiment analysis. More specifically, these superior performances were achieved without using any language dependent features.    CNNs were initially applied towards the image processing tasks and later in the NLP domain. Mainly for text classification tasks, CNNs achieved comprehensive results compared to the existing methodologies. proposed an approach for sentence classi閾夸恭ation using a CNN with pre-trained word embeddings. The task specific architecture which was obtained through fine tuning, resulted in further improvement in performances under this experiment. conducted an experimental analysis on the sensitivity of the hyperparameters for sentence classification using CNNs. The comprehensive analysis on empirical findings and the discussion on  guidelines for practitioners for CNN based text classification tasks were the foremost contents within the research.    LSTM is the most commonly used sequence model for sentiment classification. experimented with an LSTM for document level sentiment analysis where the suggested approach includes improvements for LSTM architecture to capture overall semantic information in long text.   The optimal performance on the text classification tasks including sentiment analysis were reported based on the different variations and combinations of CNN and sequence architectures. The combination of BiLSTM-CRF and CNN based model proposed by was used for sentiment classification where the target expression extraction process for sentence categories was carried out by  the BiLSTM-CRF model and the one-dimensional CNN is capable of producing the sentiment detection task. introduced a model based on the combination of CNN and RNN. The focus of the proposed architecture is to use coarse-grained features extracted by CNN with the long-term dependencies learnt through the RNN for sentiment analysis task.  More recent research exploits the attention mechanism in sentiment classification. argued that different parts of a document have no similar or relevant information, thus special attention should be given to some parts of a document to identify the overall sentiment correctly. They proposed Hierarchical Attention Hybrid Neural Networks , which combines convolutional layers, Gated Recurrent units , LSTM units and attention mechanism to implement a better document classification model. It accordingly pays more or less attention to individual words and sentences when it constructs document representation with the two levels of attention mechanisms as word-level attention and sentence-level attention.    Word-level attention mechanism elevates words that are important to the meaning of the sentence while the sentence-level attention mechanism gives more attention to sentences that contributes more to represent the overall meaning of the document. Further, HAHNN improves the performance of document classification and sentiment analysis tasks by incorporating the document structure in the model and applying CNNs for the extraction of more coarse-grained features.  The capsule network, which was initially introduced by as an improvement to the CNN strategy, was implemented to be used in NLP tasks including sentiment analysis. implemented different variations of capsule architectures as capsule-A and capsule-B for binary and multi-level sentiment analysis with a dynamic routing process. The key feature of the capsule architecture is the ability to capture context level information with the exact order or pose of the information with the vector representation of the capsules. The dynamic routing process of the proposed architecture could eliminate the disadvantages of CNNs such as high computational cost and loss of information due to the max pooling strategy widely used in CNNs.  Moreover, the transformer networks built solely upon the attention mechanism while neglecting recurrence and convolutions tend to produce promising results in the domain of NLP. In particular, Bidirectional Encoder Representations  base and BERT large models have produced state-of-the-art performance for fine-grained sentiment analysis. However, the high computational cost to build models for low resource languages  hinder the use of BERT towards the NLP tasks more frequently. Another drawback of using BERT for under resource languages, is not having enough text to comprehensively learn contextual information as opposed to English, which has billions of words.      Related approaches for sentiment analysis in Indic languages were comprehensively investigated by. According to the authors, Indic languages such as Hindi, Bengali, Tamil, Malayalam, Urdu and Kannada are the languages with major research work for sentiment analysis.  Lexicons based, rule based and machine learning based statistical algorithms were initially experimented for sentiment analysis and deep learning based techniques were recently employed in this field to obtain better outcomes.   conducted the first experiment on deep learning based sentiment analysis for Hindi language. There they used a CNN with multi-objective optimization for both sentence-level and aspect-level sentiment analysis. also introduced deep learning techniques for sentiment analysis for Bengali language. There they used an LSTM with different variations of loss functions and regularization techniques. The RNN based approach for Bengali and Tamil tweets sentiment analysis proposed by further illustrated the advancements of deep learning techniques in sentiment analysis for Indic languages. also conducted an experiment on Malayalam tweets using CNN and LSTM. The comprehensive study conducted by includes the experiments based on many deep learning techniques such as CNN, RNN, Bi-LSTM, GRU on Malayalam tweet sentiment analysis.    Sinhala is a morphologically rich, but less resourced Indic language when compared to languages such as English or even other major Indic languages, in the perspective of sentiment analysis, as well as in NLP in general. Consequently, not many sentiment annotated corpora or sentiment lexicons are publicly available for Sinhala.   \citet{medagoda2016sentiment} conducted the 閾夸购st experiment on sentiment analysis for Sinhala. A simple feed forward neural network was used with document term frequencies.  experimented with three new techniques to enhance the sentiment classi閾夸恭ation process. The 閾夸购st methodology extracts cross linguistic features related to sentiment of Sinhala language based on a bilingual dictionary of English and Sinhala. A further analysis introduced the linguistic features speci閾夸恭 for Sinhala sentiment analysis. This research then mainly focused on statistical machine learning algorithms such as Support Vector Machines  and Na\""ive Bayes, where the generated lexicons in previous steps were used for sentiment analysis. also presented a technique based on corpus-based sentiment analysis. The proposed method could be introduced as a semi-automated method based on sentiment lexicon generation for sentiment analysis.   The strategy has two major parts: lexicon generation and sentiment analysis based on the generated sentiment lexicon. For the purpose of sentiment analysis, Na\""ive Bayes, SVM and decision trees were used. Experiment results suggest that the Na绡撹皨ve Bayes approach surpasses the rest of the techniques.   Work of can be considered as the 閾夸购st to experiment with deep learning techniques for binary sentiment analysis task in Sinhala language. These techniques include LSTM and CNN+SVM models for a rather small data set with POSITIVE and NEGATIVE sentiment categories. These models were trained using Sinhala word embedding models, thus no language-speci閾夸恭 features were used. The same features were used to train statistical machine learning algorithms, which included Na\""ive Bayes, logistic regression, decision trees, random forests and SVM. Although these classi閾夸躬rs showed a much superior performance with word embedding features as opposed to sparse features such as TF-IDF, their results were inferior to that of LSTM. This research carried out a comprehensive study on using different models, with respect to the dimensionality of the embeddings, and the effect of punctuation marks.   proposed  a strategy for sentiment detection of Sinhala news comments for the same data set used by based on S-LSTM. This is a rather advanced technique where the sentiment classification process was further improved considering the n-gram features with Word2Vec and fastText embeddings. The word level state and sentence level state with recurrent information exchange between each state of the S-LSTM network have proven to be able to capture long term dependencies and outperform the traditional LSTM architecture used by.      We introduced a number of approximations which greatly speed up noisy channel modeling for neural sequence to sequence models.  This includes using channel models which are a fraction of the size of commonly used sequence to sequence models, pruning most of the channel model output vocabulary, and reducing the number of beam candidates scored by the channel model.  Our approximations are simple, yet, highly effective and enable comparable inference speed to ensembles of direct models while delivering higher accuracy. Our experiments show that noisy channel modeling can outperform pre-training approaches by being able to better exploit wider beams. Moreover, this is achieved while using a smaller amount of monolingual data.       \clearpage     
","     Recent advancements of Natural Language Processing  tasks were a direct result of using deep learning techniques. In these techniques, text is treated as sequences or spatial patterns, which allowed the modeling of higher level NLP concepts beyond the boundaries of the meaning of words in natural language. CNNs and LSTMs were the proper representatives under this paradigm.   With respect to sentiment analysis, linguistic knowledge such as sentiment lexicons and POS tags has been utilized as auxiliary input for the deep learning models, to capture deeper levels of language specific features for greater success. However, formulating language specific linguistic knowledge needs considerable human effort. Another approach is to experiment different combinations and variation of deep learning techniques, as an end to end solution, considering both the sequential nature and local n-gram information of text.   CNNs and RNNs were basically experimented as deep learning strategies and many variants and combinations of these neural networks have been used for sentiment analysis. More specifically, these superior performances were achieved without using any language dependent features.    CNNs were initially applied towards the image processing tasks and later in the NLP domain. Mainly for text classification tasks, CNNs achieved comprehensive results compared to the existing methodologies. proposed an approach for sentence classi闁惧じ鎭璦tion using a CNN with pre-trained word embeddings. The task specific architecture which was obtained through fine tuning, resulted in further improvement in performances under this experiment. conducted an experimental analysis on the sensitivity of the hyperparameters for sentence classification using CNNs. The comprehensive analysis on empirical findings and the discussion on  guidelines for practitioners for CNN based text classification tasks were the foremost contents within the research.    LSTM is the most commonly used sequence model for sentiment classification. experimented with an LSTM for document level sentiment analysis where the suggested approach includes improvements for LSTM architecture to capture overall semantic information in long text.   The optimal performance on the text classification tasks including sentiment analysis were reported based on the different variations and combinations of CNN and sequence architectures. The combination of BiLSTM-CRF and CNN based model proposed by was used for sentiment classification where the target expression extraction process for sentence categories was carried out by  the BiLSTM-CRF model and the one-dimensional CNN is capable of producing the sentiment detection task. introduced a model based on the combination of CNN and RNN. The focus of the proposed architecture is to use coarse-grained features extracted by CNN with the long-term dependencies learnt through the RNN for sentiment analysis task.  More recent research exploits the attention mechanism in sentiment classification. argued that different parts of a document have no similar or relevant information, thus special attention should be given to some parts of a document to identify the overall sentiment correctly. They proposed Hierarchical Attention Hybrid Neural Networks , which combines convolutional layers, Gated Recurrent units , LSTM units and attention mechanism to implement a better document classification model. It accordingly pays more or less attention to individual words and sentences when it constructs document representation with the two levels of attention mechanisms as word-level attention and sentence-level attention.    Word-level attention mechanism elevates words that are important to the meaning of the sentence while the sentence-level attention mechanism gives more attention to sentences that contributes more to represent the overall meaning of the document. Further, HAHNN improves the performance of document classification and sentiment analysis tasks by incorporating the document structure in the model and applying CNNs for the extraction of more coarse-grained features.  The capsule network, which was initially introduced by as an improvement to the CNN strategy, was implemented to be used in NLP tasks including sentiment analysis. implemented different variations of capsule architectures as capsule-A and capsule-B for binary and multi-level sentiment analysis with a dynamic routing process. The key feature of the capsule architecture is the ability to capture context level information with the exact order or pose of the information with the vector representation of the capsules. The dynamic routing process of the proposed architecture could eliminate the disadvantages of CNNs such as high computational cost and loss of information due to the max pooling strategy widely used in CNNs.  Moreover, the transformer networks built solely upon the attention mechanism while neglecting recurrence and convolutions tend to produce promising results in the domain of NLP. In particular, Bidirectional Encoder Representations  base and BERT large models have produced state-of-the-art performance for fine-grained sentiment analysis. However, the high computational cost to build models for low resource languages  hinder the use of BERT towards the NLP tasks more frequently. Another drawback of using BERT for under resource languages, is not having enough text to comprehensively learn contextual information as opposed to English, which has billions of words.      Related approaches for sentiment analysis in Indic languages were comprehensively investigated by. According to the authors, Indic languages such as Hindi, Bengali, Tamil, Malayalam, Urdu and Kannada are the languages with major research work for sentiment analysis.  Lexicons based, rule based and machine learning based statistical algorithms were initially experimented for sentiment analysis and deep learning based techniques were recently employed in this field to obtain better outcomes.   conducted the first experiment on deep learning based sentiment analysis for Hindi language. There they used a CNN with multi-objective optimization for both sentence-level and aspect-level sentiment analysis. also introduced deep learning techniques for sentiment analysis for Bengali language. There they used an LSTM with different variations of loss functions and regularization techniques. The RNN based approach for Bengali and Tamil tweets sentiment analysis proposed by further illustrated the advancements of deep learning techniques in sentiment analysis for Indic languages. also conducted an experiment on Malayalam tweets using CNN and LSTM. The comprehensive study conducted by includes the experiments based on many deep learning techniques such as CNN, RNN, Bi-LSTM, GRU on Malayalam tweet sentiment analysis.    Sinhala is a morphologically rich, but less resourced Indic language when compared to languages such as English or even other major Indic languages, in the perspective of sentiment analysis, as well as in NLP in general. Consequently, not many sentiment annotated corpora or sentiment lexicons are publicly available for Sinhala.   \citet{medagoda2016sentiment} conducted the 闁惧じ璐璼t experiment on sentiment analysis for Sinhala. A simple feed forward neural network was used with document term frequencies.  experimented with three new techniques to enhance the sentiment classi闁惧じ鎭璦tion process. The 闁惧じ璐璼t methodology extracts cross linguistic features related to sentiment of Sinhala language based on a bilingual dictionary of English and Sinhala. A further analysis introduced the linguistic features speci闁惧じ鎭 for Sinhala sentiment analysis. This research then mainly focused on statistical machine learning algorithms such as Support Vector Machines  and Na\""ive Bayes, where the generated lexicons in previous steps were used for sentiment analysis. also presented a technique based on corpus-based sentiment analysis. The proposed method could be introduced as a semi-automated method based on sentiment lexicon generation for sentiment analysis.   The strategy has two major parts: lexicon generation and sentiment analysis based on the generated sentiment lexicon. For the purpose of sentiment analysis, Na\""ive Bayes, SVM and decision trees were used. Experiment results suggest that the Na缁℃捁鐨╲e Bayes approach surpasses the rest of the techniques.   Work of can be considered as the 闁惧じ璐璼t to experiment with deep learning techniques for binary sentiment analysis task in Sinhala language. These techniques include LSTM and CNN+SVM models for a rather small data set with POSITIVE and NEGATIVE sentiment categories. These models were trained using Sinhala word embedding models, thus no language-speci闁惧じ鎭 features were used. The same features were used to train statistical machine learning algorithms, which included Na\""ive Bayes, logistic regression, decision trees, random forests and SVM. Although these classi闁惧じ韬瑀s showed a much superior performance with word embedding features as opposed to sparse features such as TF-IDF, their results were inferior to that of LSTM. This research carried out a comprehensive study on using different models, with respect to the dimensionality of the embeddings, and the effect of punctuation marks.   proposed  a strategy for sentiment detection of Sinhala news comments for the same data set used by based on S-LSTM. This is a rather advanced technique where the sentiment classification process was further improved considering the n-gram features with Word2Vec and fastText embeddings. The word level state and sentence level state with recurrent information exchange between each state of the S-LSTM network have proven to be able to capture long term dependencies and outperform the traditional LSTM architecture used by.",167
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. % % form to use if the first word consists of a single letter: % \IAENGPARstart{A}{demo} file is .... % % form to use if you need the single drop letter followed by % normal text : % \IAENGPARstart{A}{}demo file is .... % % Some journals put the first two words in caps: % \IAENGPARstart{T}{his demo} file is .... % % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word.  \IAENGPARstart{T}{he} Neural Machine Translation   has been used to model state-of-the-art translation systems for many high-resource languages . For many language pairs though, the amount and/or quality of parallel data is not enough to train an NMT model whose accuracy can reach an acceptable standard . This category of language pairs is known as low resource. Many works have explored how to use of the easier-to-get monolingual data to improve the quality of translation models in this category of languages -- and even high resource languages -- .  The back-translation has so far been one of the most successful methods , involving the use of the translations of the target language monolingual data to increase the amount of the training data . The additional parallel data consists of authentic sentences in the target language and their translations -- synthetic sentences in the source language -- generated using a reverse  model that is trained on the available parallel data -- see the procedure in Algorithm 1. The approach has proven to be successful at improving the quality of translations in high, middle and low resourced languages . Many studies have shown that the quality of the backward system influences the performance of the ultimate NMT model . In low resource conditions, the available parallel data may not be able to train a standard backward model and the quality of the additional data generated using this model may hurt the quality of the final model. Despite this, the aim of standard back-translation has always been to improve the performance of the target NMT model by providing sufficient training data.  Some previous works have proposed various methods to improve the performance of the backward model during training. These methods include iterative back-translation , transfer learning , self-training  and the training of a bi-directional translation model for both backward and forward translations . Others have tried to mask the deficiencies of the backward model either during inference through generating multiple translations of the same target sentence using sampling to average-out the errors in individual translations  and noising the output of beam search ; or reducing the effects of the errors in the synthetic data when training the forward model through methods such as tagged back-translation  and pre-training and fine-tuning .  We present a hybrid approach that utilizes the monolingual target data to improve both the forward and backward models in back-translation. In this approach, we used the synthetic data to enhance the backward model through self-learning and the standard back-translation for improving the forward model. The approach was preliminary investigated in  and it was shown to achieve positive results. Earlier use of stand-alone self-training in machine translation proposed extra methods of either using quality estimation  or freezing of the decoder weights  when training on the synthetic side of the training data. It was suggested that the mistakes in the synthetic data will hurt the performance of the self-trained model . Instead,  showed that self-training is capable of improving the quality of the backward model even without using either of the specialized approaches. It was shown that using all of the synthetic data generated by the backward model to help in re-training the backward model improved its performance. The work, though, did not show the benefits or otherwise of using any of the specialized approach in cleaning the data, especially in low resource languages. It also did not investigate if the model can continue to learn from its output through iterating the self-learning process.   \end{table}  This work, therefore, investigates the effects of synthetic data cleaning using automatic quality estimation when training the backward model. We observed that while the approach may improve the backward model, selecting only a subset of the synthetic data may result in a superior but less generic model. We then investigated the use of iterative self-training with quality estimation as proposed in , enabling the backward model to be trained on all the monolingual data. For low resource languages, readily available quality estimation systems or the data to train such systems may not be available. This may limit the implementation of the approach.  We, therefore, proposed a novel iterative approach that relies only on all the available monolingual target data to improve the backward model before finally generating a much improved synthetic data for the forward model's training. Experimental results show that our approach is superior over the standard back-translation and the approach proposed in ; and that our iterative approach is superior to the iterative back-translation while also requiring less number of models to be trained.  We thus make the following contributions in this paper: \renewcommand{\labelitemi}{\textbullet}   The remainder of this paper is organized as follows: In Section , we reviewed the related works. We presented the proposed methods in Section . We reported the experiments and results in Section . We discussed the results and findings of the research work in Sections  and  respectively and, finally, the paper was concluded and directions for future work were proposed in Section .     This section presents prior work on back-translation, iterative back-translation, forward translation and self-training and automatic quality estimation for self-training.    Back-translation is an approach of augmenting the available parallel data with the back-translations of monolingual sentences in the target language to increase the amount of data for training improved translation models. In NMT,  proposed using this approach to solve the problem that the architecture struggles with 閳 lack of adequate training data. The approach has since been used for training state-of-the-art translation models for many language pairs . The success of the approach has been shown to rely on three factors: the quality of the synthetic data which depends on the quality of the backward model  and the synthetic data generation method used ; the ratio of the synthetic to authentic parallel data used ; and the architecture of the NMT used .  Many studies have shown that the quality of the backward system influences the performance of the ultimate NMT model more than other factors . In low resource conditions, the available parallel data may not be able to train a standard backward model and the quality of the additional data generated using this model may hurt the quality of the final model. Various research works have proposed methods aimed at training a standard backward model no matter the amount of authentic data available and, therefore, improving the quality of the synthetic data.  and  investigated pre-training the backward model on a high-resource language pair  and fine-tuning the model on the low resource language pair  through transfer learning .  posits that selecting the appropriate synthetic data generation method can offset the deficiencies of the backward model.  investigated the application of the self-training approach to improve the backward model in low resource NMT.    Back-translation relies on the parallel data between the two languages and the target monolingual data to improve the forward model only. The iterative back-translation, on the other hand, relies on the monolingual source and target data to improve the backward and forward models respectively. The backward model generates synthetic data to improve the forward model and the forward model does the same for the backward model. The process is repeated until a set amount of iterations or desired quality is reached.  The iterative back-translation was proposed in the work of  to enable the backward and forward models to be improved over several iterations. The work hypothesized that if the back-translation approach has shown to be successful at improving the standard of machine translation models, repeating the procedure will further enhance the quality of the translations produced. Whereas the synthetic data generated by the backward model is mainly used to enhance the forward model in standard back-translation, iterative back-translation enables the backward model, also, to be improved on the synthetic data generated by the forward model. The approach has been shown to successfully improve both the backward and forward models .  Standard back-translation is used throughout the process but it relies also on the monolingual source data, deviating from the requirements of traditional back-translation. Also, where the monolingual source data is not available, the procedure cannot be applied. The approach, implemented with standard back-translation, was shown to reduce the performance of English-Romanian machine translation, improving the performance only after applying a tagging approach .     Self-learning is an approach where a model learns from its own output. In machine translation, a model is trained on the available parallel data and is then used to translate a given set of monolingual source sentences to generate a pseudo-parallel  training data. This data is then used to train a better model than the generating model. Ueffing  first used the approach to improve phrased-based statistical machine translation systems. The work uses an existing system to generate the translations of a new set of source data. The confidence score of each of the translated sentences is estimated and based on these scores, translations that are considered reliable are extracted and used as additional training data for improving the existing system.  In neural machine translation,  explored the use of monolingual source data to improve the accuracy of a translation model through forward translation. A baseline NMT model was trained and used to generate synthetic parallel data -- authentic source + synthetic target data. Since the NMT architecture consists of an encoder-decoder architecture and the encoder is trained on the source data while the decoder learns representation based on the representations of the encoder and the target data, the use of synthetic data -- which often contains mistakes -- to train any or both of the encoder and decoder may deteriorate their performance. To mitigate this problem, the authors proposed freezing the parameters of the decoder when training on the synthetic target data. By this, the useful representations learned on the authentic data will not be unlearned when training on the additional data.  In an approach similar to ,  used self-training on phrased-based and rule-based statistical machine translation systems to better their performance. Their work investigated the use of iterative self-training with quality estimation in a process that utilizes all of the monolingual data to improve the generating models. First, a baseline translation model is trained and is used to translate a given set of monolingual sentences. The quality of the synthetic  sentences are determined using a quality estimation system. The best sentences are selected and added to the training data. This new  training data is used to retrain the translation model. the rest of the sentences whose translations were not selected are translated again using the improved model and the procedure continues until all of the monolingual sentences are exhausted.  The work of  investigated the feasibility of using the self-learning approach only -- without quality estimation and/or freezing of any parameters -- to train a better NMT model. It was determined that not only is it simpler than the other methods, it is also capable of improving baseline translation models. The authors used the approach to enhance the performance of a backward model in the back-translation approach. The quality of the model was improved, resulting in a better target NMT model. The work, though, only showed the applicability of the self-training approach but did not investigate the extent -- through iterative training -- to which the backward model will continue to be improved if it is trained on the synthetic data it generates. It also did not show the benefits or otherwise of data selection on the quality of the backward model. This work, therefore, proposes the iterative self-learning approach to improve the quality of the backward model. It also extends the work in  to compare the use or otherwise of quality estimation in determining the best synthetic data for self-training the backward model.   Quality Estimation  is a method used for Translation Quality Assessment. The method was introduced to help in determining the quality of translations when reference  text is not available . When such texts are available, traditional metrics such as the Bi-Lingual Evaluation Understudy   and the METEOR  are majorly used. The technique is particularly useful for indicating the reliability of translations especially to users who cannot read the source language , for deciding the translations that are fit for publishing and those requiring human or automatic revision  . This technique has been shown to reduce the time taken for post-editing .  Work on the technique was started in early 2000s  with works such as  and has been gaining prominence especially with the ever increasing reliance on machine translation models. Recent works such as  have achieved tremendous improvements in translation quality estimation, mostly using neural models to train better systems that are capable of estimating the quality of translations produced.  The QE technique was used in self-learning in works such as  to determine the translations that are considered fit for retraining the generating model and has been shown to improve the performance of statistical machine translation systems. Instead of relying on the translated sentences, which can be good or bad, the QE system enables the training of better translation models only on data that is estimated to be good.       For experiments which were conducted to identify effect of punctuation marks and dimension of word embeddings towards the sentiment analysis task, different preprocessing techniques, word embedding models, and several neural network setups were used.    For these experiments, the data-set was splitted into train and validation sets, with a ratio of .    First, different preprocessing techniques were evaluated for a multi-level sentiment analysis task in Sinhala language with baseline models. For that, an analysis was conducted with punctuation marks, without any punctuation marks and without punctuation marks except question mark.    Next, Different dimensions for both Word2vec and fastText models were experimented with baseline LSTM model and identified that fastText with 300 dimensions could beat other word embedding models as per results in Table. Therefore, The word embedding model of fastText, and dimension size of 300 were fixed for our succeeding experiments.     The experiments which were conducted with different baseline models to identify best models for further improvements suggested that BiLSTM was the optimal architecture as the primary baseline. As per results in Table with 10-fold cross validation, BiLSTM achieved the best weighted accuracy of 63.81\  and a weighted F1 score of 57.71\ , beating Vanilla RNN, LSTM, and GRU. Therefore LSTM and BiLSTM were selected for further improvements. After that, two strategies were followed to improve the selected approaches. First strategy is combining CNN with baseline models. Even though it is expected to increase the weighted accuracy and F1 score of sentiment analysis process by following the improved model architecture based on CNN, the results do not suggest a noticeable enhancement. One reason might be not having enough data to learn trainable parameters as a complex model due to the CNN integration. Results of these models are listed in Table along with results from other improvements to the baseline models.    As the final improvement to the baseline approaches stacking was implemented. As per results in Table with 10-fold cross validation, 'Stacked BiLSTM 3' model reached a weighted accuracy of 63.13\  and weighted F1 socre of 59.42\  by outperforming all other approaches. This could be justified as the ability of the stacked BiLSTM to capture the context level information in both left and right direction while considering substantial amount of neural representation for language modeling based on stacking strategy.    The capsule-B architecture went beyond all the other experimented models producing weighted accuracy of 63.23\  and weighted F1-score of 59.11\  with 10-fold cross validation. This observation could be elaborated based on the motivation behind the capsule strategy to represent the neural architecture based on vectors which further improve language representation considering the exact order or pose of the information. Furthermore, capsule-B outperformed capsule-A due to its sophisticated architecture which is designed to capture more n-grams features compared to capsule-A. The HAHNN does not illustrate greater performance as expected. This could be due to the shorter length of comments to learn deeper neural representation with the attention mechanism.  also employed under the HAHNN.  The weighted accuracy of each experiment was bounded below the value of 65\  as per the inter-annotator agreement value. This is a direct result of the high volume of noise in the dataset. As illustrated in Table, the CONFLICT and the NEUTRAL classes seem to be considerably mis-classified as NEGATIVE comments, due to the impact of a large number of NEGATIVE comments with respect to the number of CONFLICT and NEUTRAL comments in the training set. Figure shows few comments where the model was confused while classifying. The first example illustrates a comment that is negatively classified but truly a CONFLICT comment. When considering the interpretation of the comment, the sentence includes two negative sentences with a positive sentence, which indicates some bias towards the NEGATIVE sentiment. The second and third comments include NEGATIVE and NEUTRAL comments, which are classified as POSITIVE and CONFLICT, respectively.     The observation of the second example could be justified as the effect of the positive word ``{\iskool 鍠藉懅绌柧鎺勭П鍠惧啛绋搣'' , which greatly affects the final sentiment of comment, than the negative word ``{\iskool 鍠借剢绌╁柧鏀'' . The third example has both  negative and positive words ``{\iskool 鍠借啅鍔ㄥ柧鎷''  and ``{\iskool 鍠借剢绐夊柦绉界獕鍠芥磥绐倉'' , respectively. Therefore the comment is classified as CONFLICT, even though the overall sentiment of the comment is neutral.             }      -    m鑶 vidiya宄佺挦 par澧╧宄侇枾宄佸樅 vala宄佺挦 ad鑶﹍a v鑶﹔t鑶 pram鑶ヾav澧╩a tulin v蹇檙adikaru nid蹇檒l鑶 innav鑶. m鑶縱鑶 tamayi pra鑹a adhikara宄佸樅ya宄佺挦 t澧╪duvak ganna b蹇檙i. mun宄佺挦 da宀ｅ扯vam d澧╩a it鑶 hoda t澧╪duvak    \\     {\iskool 鍠借剢绌╁柧 鍠藉懅绌柧鎺勭П鍠惧啛绋 鍠借啅渚楀柧鏂风┈鍠惧ɑ绌柦浜ㄧ獋.} - pavu ahi宄佷够aka manussay鑶. \\      {\iskool 鍠借剢绌靛柦绉界獕鍠戒酣渚楀柧濞昏懀鍠 鍠芥盀绐夊柦浜ㄤ緱鍠惧ɑ渚 鍠筋垥绐撳柦浜ㄧ〒鍠 鍠芥洉渚楀柧鎴夐櫋 鍠界紪闄″柧 鍠芥柗绌柧婧归櫋 鍠芥盀绐夊柦浜ㄤ緱鍠惧ɑ渚.  鍠芥柗钁ｅ柦 鍠藉璁 鍠借啅鍔ㄥ柧 鍠界紪绐夊柧鍐熺獋 鍠藉懅娲炲柧 鍠筋垥绐呭柦姘炵┑鍠芥盀绐 鍠藉绌柧濞荤┈鍠界Ы绌 鍠借剢绐傚柦绉界獢 鍠惧啛绐撳柦缂栫 鍠借剢绐夊柦绉界獕鍠芥磥绐 鍠藉渚楀柧濞讳緱鍠惧柧.} - priyanta宄佺挦 kiyanna deyak 鑹抧鐗祄a nam ohoma kiyanna.otana i宀ｅ硽 madi nis鑶 api d蹇檏k鑶 issaraha p鑶﹔鑶 senaga piril鑶 innav鑶.    \\      
","  This section presents prior work on back-translation, iterative back-translation, forward translation and self-training and automatic quality estimation for self-training.    Back-translation is an approach of augmenting the available parallel data with the back-translations of monolingual sentences in the target language to increase the amount of data for training improved translation models. In NMT,  proposed using this approach to solve the problem that the architecture struggles with 闁 lack of adequate training data. The approach has since been used for training state-of-the-art translation models for many language pairs . The success of the approach has been shown to rely on three factors: the quality of the synthetic data which depends on the quality of the backward model  and the synthetic data generation method used ; the ratio of the synthetic to authentic parallel data used ; and the architecture of the NMT used .  Many studies have shown that the quality of the backward system influences the performance of the ultimate NMT model more than other factors . In low resource conditions, the available parallel data may not be able to train a standard backward model and the quality of the additional data generated using this model may hurt the quality of the final model. Various research works have proposed methods aimed at training a standard backward model no matter the amount of authentic data available and, therefore, improving the quality of the synthetic data.  and  investigated pre-training the backward model on a high-resource language pair  and fine-tuning the model on the low resource language pair  through transfer learning .  posits that selecting the appropriate synthetic data generation method can offset the deficiencies of the backward model.  investigated the application of the self-training approach to improve the backward model in low resource NMT.    Back-translation relies on the parallel data between the two languages and the target monolingual data to improve the forward model only. The iterative back-translation, on the other hand, relies on the monolingual source and target data to improve the backward and forward models respectively. The backward model generates synthetic data to improve the forward model and the forward model does the same for the backward model. The process is repeated until a set amount of iterations or desired quality is reached.  The iterative back-translation was proposed in the work of  to enable the backward and forward models to be improved over several iterations. The work hypothesized that if the back-translation approach has shown to be successful at improving the standard of machine translation models, repeating the procedure will further enhance the quality of the translations produced. Whereas the synthetic data generated by the backward model is mainly used to enhance the forward model in standard back-translation, iterative back-translation enables the backward model, also, to be improved on the synthetic data generated by the forward model. The approach has been shown to successfully improve both the backward and forward models .  Standard back-translation is used throughout the process but it relies also on the monolingual source data, deviating from the requirements of traditional back-translation. Also, where the monolingual source data is not available, the procedure cannot be applied. The approach, implemented with standard back-translation, was shown to reduce the performance of English-Romanian machine translation, improving the performance only after applying a tagging approach .     Self-learning is an approach where a model learns from its own output. In machine translation, a model is trained on the available parallel data and is then used to translate a given set of monolingual source sentences to generate a pseudo-parallel  training data. This data is then used to train a better model than the generating model. Ueffing  first used the approach to improve phrased-based statistical machine translation systems. The work uses an existing system to generate the translations of a new set of source data. The confidence score of each of the translated sentences is estimated and based on these scores, translations that are considered reliable are extracted and used as additional training data for improving the existing system.  In neural machine translation,  explored the use of monolingual source data to improve the accuracy of a translation model through forward translation. A baseline NMT model was trained and used to generate synthetic parallel data -- authentic source + synthetic target data. Since the NMT architecture consists of an encoder-decoder architecture and the encoder is trained on the source data while the decoder learns representation based on the representations of the encoder and the target data, the use of synthetic data -- which often contains mistakes -- to train any or both of the encoder and decoder may deteriorate their performance. To mitigate this problem, the authors proposed freezing the parameters of the decoder when training on the synthetic target data. By this, the useful representations learned on the authentic data will not be unlearned when training on the additional data.  In an approach similar to ,  used self-training on phrased-based and rule-based statistical machine translation systems to better their performance. Their work investigated the use of iterative self-training with quality estimation in a process that utilizes all of the monolingual data to improve the generating models. First, a baseline translation model is trained and is used to translate a given set of monolingual sentences. The quality of the synthetic  sentences are determined using a quality estimation system. The best sentences are selected and added to the training data. This new  training data is used to retrain the translation model. the rest of the sentences whose translations were not selected are translated again using the improved model and the procedure continues until all of the monolingual sentences are exhausted.  The work of  investigated the feasibility of using the self-learning approach only -- without quality estimation and/or freezing of any parameters -- to train a better NMT model. It was determined that not only is it simpler than the other methods, it is also capable of improving baseline translation models. The authors used the approach to enhance the performance of a backward model in the back-translation approach. The quality of the model was improved, resulting in a better target NMT model. The work, though, only showed the applicability of the self-training approach but did not investigate the extent -- through iterative training -- to which the backward model will continue to be improved if it is trained on the synthetic data it generates. It also did not show the benefits or otherwise of data selection on the quality of the backward model. This work, therefore, proposes the iterative self-learning approach to improve the quality of the backward model. It also extends the work in  to compare the use or otherwise of quality estimation in determining the best synthetic data for self-training the backward model.   Quality Estimation  is a method used for Translation Quality Assessment. The method was introduced to help in determining the quality of translations when reference  text is not available . When such texts are available, traditional metrics such as the Bi-Lingual Evaluation Understudy   and the METEOR  are majorly used. The technique is particularly useful for indicating the reliability of translations especially to users who cannot read the source language , for deciding the translations that are fit for publishing and those requiring human or automatic revision  . This technique has been shown to reduce the time taken for post-editing .  Work on the technique was started in early 2000s  with works such as  and has been gaining prominence especially with the ever increasing reliance on machine translation models. Recent works such as  have achieved tremendous improvements in translation quality estimation, mostly using neural models to train better systems that are capable of estimating the quality of translations produced.  The QE technique was used in self-learning in works such as  to determine the translations that are considered fit for retraining the generating model and has been shown to improve the performance of statistical machine translation systems. Instead of relying on the translated sentences, which can be good or bad, the QE system enables the training of better translation models only on data that is estimated to be good.",168
"  Because of the fact that obtaining   supervised training labels is costly and time-intensive,   and that   unlabeled data is relatively easy to obtain,   semi-supervised learning  , which  utilizes  in-domain  unlabeled data  to improve models trained on the labeled dataset , is of growing interest.  Under the context of large-scale of language model pretraining ,  where a language model is pretrained on an extremely large, open-domain dataset ,   how we can make the best use of the in-domain unlabeled dataset     is poorly understood.   There are basically two ways to take advantages of  the unlabeled, in-domain dataset :   {\bf in-domain pretraining}\footnote{To note,    the pretraining on the in-domain dataset  is distinguished from the pretraining on the large-scale, open-domain dataset largeU.  The model for in-domain pretraining can be randomly initialized or   taking a pretrained model based on the open-domain dataset largeU .}, where a language model is pretrained on   the in-domain dataset  , and then   fine-tuned on ;  {\bf pseudo-label} based approach , where unlabeled data points are assigned with labels predicted by the model trained  on  , forming a new dataset .  A new model  is trained for final predictions by considering .   Many important questions regarding the behavior of semi-supervised learning models under the context of large-scale LM pretraining   remain unanswered:    Is semi-supervised training  still beneficial with the presence of large scale pretraining on largeU?    Should  be used for in-domain LM pretraining or pseudo-label generation?  How should  pseudo-label based semi-supervised models    be  implemented? How different semi-supervised strategies  affect performances regarding  of different sizes, and  of different sizes, etc.    In this paper, we conduct comprehensive studies on the behavior of semi-supervised learning in NLP  with the presence of large-scale language model pretraining.   We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks. Our work sheds important lights on the behavior of semi-supervised learning models:  we find that   with the presence of  in-domain pretraining LM on , open-domain LM pretraining   is unnecessary, and we are able to achieve better performance with pretraining on  the in-domain dataset ;    both the in-domain pretraining strategy and the pseudo-label based strategy  lead to significant performance boosts, with the former performing better with larger , the latter performing better with smaller , and the  combination   of both performing the best;  for pseudo-label based strategies,  self-training  yields better performances when  is small, while joint training on the combination of   and  yields better performances when  is large.   Using semi-supervised learning models, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6 with the full dataset.  More importantly, our work marks an initial step toward understanding the behavior of semi-supervised learning models in the context of large-scale pretraining.    The rest of this paper is organized as follows: related work is detailed in Section 2.  Different strategies for training semi-supervised models are shown in Section 3.  Experimental results and findings are shown in Section 4, followed by a brief conclusion in Section 5.       Different from fully supervised learning which makes use of labeled data,  The goal of semi-supervised learning  is to use massive amount of   unlabeled data to improve the models trained  on labeled data.   Semi-supervised learning has been widely studied in literature, especially in the field of computer vision .  One widely-used type of semi-supervised method is  the pseudo-label  based method  , where unlabeled data points are assigned with labels predicted by a trained model,  forming a large pseudo labeled dataset to train a model.  {\bf Self-training}  is a specific type of pseudo-label  based method that is of growing interest.  Self-training involves training two models: a ``teacher'' used to label unlabeled data, which is  used as an augmented labeled dataset.  Then  a ``student'' is trained on the newly augmented dataset.  This process can be iterated to further boost performances. Self-training has been successfully applied in  different fields such as  computer vision , automatic speech recognition  .  In Vision, \citet{yalniz2019billion} adopted the self-training paradigm in image classification, and achieves the state-of-the-art top-1 result on ImageNet benchmark; \citet{xie2020self} proposed the strategy of Noisy Student Training, a variant of self-training built on top of EfficientNet . In ASR, \citet{parthasarathi2019lessons} trained an ASR model on one million hours of unlabeled speech data  using a teacher-student self-training model. \citet{park2020improved} proposed the concept of normalized filtering score that filters out low-confident utterance-transcript pairs generated by the teacher to mitigate the noise introduced by the teacher model.         and a combination of  different methods .     fused the probability that a pretrained language model produces into the process of generating pseudo labels by a teacher sequence-to-sequence acoustic model. Then a student acoustic is trained on the combination of the golden labeled data and the pseudo labeled data. The approach yields notable improvements over strong end-to-end ASR baselines.     The first is  which assumes slight noise injected into the original input would not change its target label as well as the model prediction.   Under this assumption, data points can be crafted from the original data by adding small noise to the input without changing the output label.  The second         In the context of natural language processing , the concept of semi-supervised learning has been adopted in different NLP tasks such as    machine translation , information extraction , text classification , and text generation . Particularly, \citet{he2019revisiting} studied the efficacy of self-training on sequence generation tasks and  found that self-training can significantly boost performances, particularly when labeled data is scarce. Besides, they also pointed out that the core of self-training for sequence generation tasks is the noise injected into the neural model, which can be interpreted to smooth the latent sequence space.  Word vector models  and language modeling pretraining     can also be viewed as a specific type of semi-supervised learning model, by leveraging the information of data in the general domain.  Other strategies for training semi-supervised models   involve co-training ,  low-density separation .      Except CV and ASR, many NLP tasks, particularly neural machine translation , has heavily adopted the idea of self-training . For example,    \citet{sennrich2016back-translation,edunov2018understanding} studied the effectiveness of back-translation, a technique used to synthesizing training data by leveraging two NMT models with opposite directions to translate monolingual target data into the source language.  In a followup, \citet{NIPS2016_6469} proposed the concept of dual learning, which builds interactions between two agents learning their own languages. Dual learning later motivated the development of unsupervised machine translation, where a common latent semantics space is learned through denoising and back-translation using only monolingual data .    Data augmentation  aims at   increasing the amount of training data by adding slightly modified copies of  existing data points or  created new synthetic data based on existing data points .  The concept consistency training  is widely used as a regulation to force the label of modified copies to the same as the original label.     Among different data augmentation strategies, data mixing is of growing interest :   \citet{zhang2017mixup} proposed Mixup, a method that crafted new data by linearly interpolating between pairs of random examples along with their labels. By doing so, the model is able to smoothly model linear properties amid training on augmented data. In a followup, substantial efforts have been paid to improve this method.   In NLP,   modified copies of  existing data points are generated usually by   synonym replacement and text editing , back-translation , noise injection , mixup , generation .   Data augmentation has introduced significant performance boost  especially in low-resource scenarios .        We introduced a framework for creating general purpose NLP systems that can solve tasks from natural language descriptions, synthesizing and extending previous work in zero-shot learning. To make progress toward this goal, we create a dataset, \dataset{}, that rigorously evaluates how well a model truly understands each task. The dataset is designed to test models' ability to systematically generalize across four different areas. State-of-the-art performance on \dataset is \finalscore\ , leaving much room for future improvement.  While we have been focused on zero shot learning from task descriptions, our framework also permits few-shot scenarios where a task description is given along with a handful of examples, making meta-learning approaches applicable.  This is an interesting avenue for future work, for which \dataset{} should also be useful. To facilitate future work, we make our models, code, and data available at .  
","   Different from fully supervised learning which makes use of labeled data,  The goal of semi-supervised learning  is to use massive amount of   unlabeled data to improve the models trained  on labeled data.   Semi-supervised learning has been widely studied in literature, especially in the field of computer vision .  One widely-used type of semi-supervised method is  the pseudo-label  based method  , where unlabeled data points are assigned with labels predicted by a trained model,  forming a large pseudo labeled dataset to train a model.  {\bf Self-training}  is a specific type of pseudo-label  based method that is of growing interest.  Self-training involves training two models: a ``teacher'' used to label unlabeled data, which is  used as an augmented labeled dataset.  Then  a ``student'' is trained on the newly augmented dataset.  This process can be iterated to further boost performances. Self-training has been successfully applied in  different fields such as  computer vision , automatic speech recognition  .  In Vision, \citet{yalniz2019billion} adopted the self-training paradigm in image classification, and achieves the state-of-the-art top-1 result on ImageNet benchmark; \citet{xie2020self} proposed the strategy of Noisy Student Training, a variant of self-training built on top of EfficientNet . In ASR, \citet{parthasarathi2019lessons} trained an ASR model on one million hours of unlabeled speech data  using a teacher-student self-training model. \citet{park2020improved} proposed the concept of normalized filtering score that filters out low-confident utterance-transcript pairs generated by the teacher to mitigate the noise introduced by the teacher model.         and a combination of  different methods .     fused the probability that a pretrained language model produces into the process of generating pseudo labels by a teacher sequence-to-sequence acoustic model. Then a student acoustic is trained on the combination of the golden labeled data and the pseudo labeled data. The approach yields notable improvements over strong end-to-end ASR baselines.     The first is  which assumes slight noise injected into the original input would not change its target label as well as the model prediction.   Under this assumption, data points can be crafted from the original data by adding small noise to the input without changing the output label.  The second         In the context of natural language processing , the concept of semi-supervised learning has been adopted in different NLP tasks such as    machine translation , information extraction , text classification , and text generation . Particularly, \citet{he2019revisiting} studied the efficacy of self-training on sequence generation tasks and  found that self-training can significantly boost performances, particularly when labeled data is scarce. Besides, they also pointed out that the core of self-training for sequence generation tasks is the noise injected into the neural model, which can be interpreted to smooth the latent sequence space.  Word vector models  and language modeling pretraining     can also be viewed as a specific type of semi-supervised learning model, by leveraging the information of data in the general domain.  Other strategies for training semi-supervised models   involve co-training ,  low-density separation .      Except CV and ASR, many NLP tasks, particularly neural machine translation , has heavily adopted the idea of self-training . For example,    \citet{sennrich2016back-translation,edunov2018understanding} studied the effectiveness of back-translation, a technique used to synthesizing training data by leveraging two NMT models with opposite directions to translate monolingual target data into the source language.  In a followup, \citet{NIPS2016_6469} proposed the concept of dual learning, which builds interactions between two agents learning their own languages. Dual learning later motivated the development of unsupervised machine translation, where a common latent semantics space is learned through denoising and back-translation using only monolingual data .    Data augmentation  aims at   increasing the amount of training data by adding slightly modified copies of  existing data points or  created new synthetic data based on existing data points .  The concept consistency training  is widely used as a regulation to force the label of modified copies to the same as the original label.     Among different data augmentation strategies, data mixing is of growing interest :   \citet{zhang2017mixup} proposed Mixup, a method that crafted new data by linearly interpolating between pairs of random examples along with their labels. By doing so, the model is able to smoothly model linear properties amid training on augmented data. In a followup, substantial efforts have been paid to improve this method.   In NLP,   modified copies of  existing data points are generated usually by   synonym replacement and text editing , back-translation , noise injection , mixup , generation .   Data augmentation has introduced significant performance boost  especially in low-resource scenarios .",169
"  \todo{Completely rewrite - emphasize that many methods have been proposed for learning embeddings  learn representations of the entities in a knowledge base  typically based on the text of each entity's Wikipedia article or the surrounding local context for mentions of each entity . %  \clm{I would have ""context surrounding mentions of each entity -- otherwise it looks like you're being just redundant and not making it clear this is what you will be calling it henceforth, tho this is just stylistic} context surrounding mentions of each entity Recent advances in neural EL have involved methods for pretraining entity embeddings using the link graph of Wikipedia to learn related entities and words . Similar to word embeddings, past work has shown that these embeddings reside in a high-dimensional pseudo-semantic space, with entities that are close in the space being semantically similar . % \glarionov{""with entities close in the space being...} However, little work has been done to understand what information different entity embeddings capture about the underlying entities and how that information affects downstream performance.  Our goal in this work is to identify semantic information in entity representations and determine how that information is linked to performance on downstream EL tasks. For this, we develop a series of probing tasks, which have previously been used to examine lexical and syntactic properties of neural model layers such as sentence encoders and decoders for neural machine translation systems .  % \glarionov{I would group these two citations at the end for readability} % \ees{for lexical and syntactic properties [this is too split, move this info to before citations]}. We extract structured data about entities using DBpedia and context words from Wikipedia anchor links to create probing tasks designed to evaluate the knowledge-based and distributional semantic contents of different entity embedding models.  We compare five entity embedding methods, first by them on two downstream EL tasks. We then probe the learned embeddings to evaluate what semantic information is important for the downstream tasks and how it is represented by the different models. % \ees{We show a strong relationship between probing task performance and performance on the downstream EL tasks. [too long, break up]} We find that pretrained entity embedding methods are generally more effective at representing distributional and knowledge-based semantic information than models that generate embeddings as a byproduct of training on an EL task. These improved representations lead to better performance on the EL tasks, with the best model showing high performance on both distributional and knowledge-based semantic tasks. We further find that entity embeddings trained to predict related words and entities in a skipgram-like model are able to learn fine-grained entity type information and specific relationship types between entities without explicitly providing this information.  Our primary contributions with this work are to:   % 1) describe methods for evaluating the semantic information learned by these methods and 2) to\clm{either move the first ""to"" after ""1)"", or delete this one} empirically demonstrate the importance of this information in creating models of entities for use in downstream tasks.\clm{I agree  with Liz you should bullet point this, because you want to highlight your contributions -- easier for reviewers} % \ees{maybe bullet point these two or put 1) .. 2) to make it mad easy to scan and get} Our hope is that this information can provide guidance in developing architectures that better combine explicit structured information with text to improve methods for representing entities that can be used in a variety of downstream tasks, similar to existing word embeddings. Our methods can additionally be used to potentially detect deficiencies in new representation methods and biases of learned attributes through other probing tasks. % and biases of current methods by probing .\clm{You might want to briefly address the means by which it detects bias, otherwise that is a question that could feel unanswered in the reader's head}      \ees{a la graham's suggestion to us, consider moving related work to right before conclusion. you've just said most of this at higher level in the intro. will let you get right into the meat of it quicker}   \clm{I did read Liz' commented out comment and I'd say go with Graham over me, but this does feel awkward.}   \glarionov{seconding claytons comment, why go against convention? Up to you tho, don't think it'll affect anyone's opinion of the paper}     \todo{Cut this whole first paragraph? Can I assume someone reading this would know the background of the task well enough to skip?} Entity Linking refers to the task of identifying the true referent  of a mention , where  is an external knowledge base containing thousands or millions of entities .   After selecting a set of candidates in a candidate retrieval phase , each candidate is typically scored and ranked with respect to mention , with the highest scoring candidate being selected.   Candidate selection can be performed in a local setting where each mention is disambiguated separately from the rest , or using a global approach, which jointly disambiguates all of the mentions, incorporating coherence between candidate choices to improve decisions . Initial neural EL models often learned representations that maximize the similarity between the KB candidate's text and the mention's context . Recently, approaches based on skip-gram and CBOW models  jointly train word and entity embeddings that have produced state of the art results on EL  as well as tasks like named entity recognition  and question answering .  Prior to neural EL systems, structured semantic information such as the candidate entity's type infobox attributes and other entities mentioned in the candidate's Wikipedia document or in the candidate document  were used as features to help rank candidates . Some neural EL systems have extended this work, including entity type information during disambiguation  or representing the entity entirely as a binary vector of Wikipedia types .   Huang et al. \shortcite{huang2015kg} present an early neural EL method for creating semantic vector representations of candidates using one-hot encoded vectors of entity types and relations labeled in the candidate document.   Gupta et al. \shortcite{gupta2017joint} jointly trained an EL system to predict the fine-grained entity types of the mention and the candidates and achieved significant improvements over their baseline.   Raiman and Raiman \shortcite{raiman2018deeptype} create symbolic  representations of entities using a learned type subset of Wikipedia categories and use it for EL by predicting types of entities to narrow down candidates.    Methods for interpreting neural representations have drawn increasing attention recently . We particularly focus on methods for detecting specific attributes in learned representations, referred to as auxiliary prediction tasks  or probing tasks . In these tasks, a model's weights are frozen after it is trained and used to output the internal representations for external analysis. There have been some limited explorations of probing entity representations published concurrent with this work . Our work takes a broader view, evaluating a range of architectures for creating entity representations for differences in how they entity type, relationship, and context information.     In this paper, we conduct comprehensive  analysis on semi-supervised learning in NLP under the context of  large-scale language model pretraining.  We find that even with the presence of large-scale LM pretraining, both the in-domain pretraining strategy and the pseudo-label based strategy introduce  additional  significant performance boost,  with the former performing better with larger ,  the latter performing better with smaller , and the combination leading to the best performance.  Using semi-supervised learning models, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6 with the full dataset.  Our work  sheds light on   the behavior of semi-supervised learning models in the context of large-scale pretraining.      
","   \ees{a la graham's suggestion to us, consider moving related work to right before conclusion. you've just said most of this at higher level in the intro. will let you get right into the meat of it quicker}   \clm{I did read Liz' commented out comment and I'd say go with Graham over me, but this does feel awkward.}   \glarionov{seconding claytons comment, why go against convention? Up to you tho, don't think it'll affect anyone's opinion of the paper}     \todo{Cut this whole first paragraph? Can I assume someone reading this would know the background of the task well enough to skip?} Entity Linking refers to the task of identifying the true referent  of a mention , where  is an external knowledge base containing thousands or millions of entities .   After selecting a set of candidates in a candidate retrieval phase , each candidate is typically scored and ranked with respect to mention , with the highest scoring candidate being selected.   Candidate selection can be performed in a local setting where each mention is disambiguated separately from the rest , or using a global approach, which jointly disambiguates all of the mentions, incorporating coherence between candidate choices to improve decisions . Initial neural EL models often learned representations that maximize the similarity between the KB candidate's text and the mention's context . Recently, approaches based on skip-gram and CBOW models  jointly train word and entity embeddings that have produced state of the art results on EL  as well as tasks like named entity recognition  and question answering .  Prior to neural EL systems, structured semantic information such as the candidate entity's type infobox attributes and other entities mentioned in the candidate's Wikipedia document or in the candidate document  were used as features to help rank candidates . Some neural EL systems have extended this work, including entity type information during disambiguation  or representing the entity entirely as a binary vector of Wikipedia types .   Huang et al. \shortcite{huang2015kg} present an early neural EL method for creating semantic vector representations of candidates using one-hot encoded vectors of entity types and relations labeled in the candidate document.   Gupta et al. \shortcite{gupta2017joint} jointly trained an EL system to predict the fine-grained entity types of the mention and the candidates and achieved significant improvements over their baseline.   Raiman and Raiman \shortcite{raiman2018deeptype} create symbolic  representations of entities using a learned type subset of Wikipedia categories and use it for EL by predicting types of entities to narrow down candidates.    Methods for interpreting neural representations have drawn increasing attention recently . We particularly focus on methods for detecting specific attributes in learned representations, referred to as auxiliary prediction tasks  or probing tasks . In these tasks, a model's weights are frozen after it is trained and used to output the internal representations for external analysis. There have been some limited explorations of probing entity representations published concurrent with this work . Our work takes a broader view, evaluating a range of architectures for creating entity representations for differences in how they entity type, relationship, and context information.",170
"   The vast amounts of scientific literature can provide a significant source of information for biomedical research. Using this literature to identify relations between entities is an important task in various applications .  Existing approaches to biomedical relation extraction usually fall into one of two categories. Mention-level extraction aims to classify the relation between a pair of entities within a short span of text . In contrast, pair-level extraction aims to classify the relation between a pair of entities across an entire paragraph, document or corpus.  For both mention-level and pair-level relation extraction, recent work has been focused on representation learning. This is considered to be one of the major steps towards making progress in artificial intelligence . Representations of relations which understand their context are particularly important in biomedical research, where identifying fruitful targets is crucial due to the high costs of experimentation. Learning such representations is likely to require large amounts of unsupervised data due to the scarcity of labelled data in this domain.  Recent mention-level methods have been based on using large unsupervised models with Transformer networks  to learn representations of sentences containing pairs of entities. These representations are then used as the inputs to much smaller models, which perform supervised relation classification .  Recent pair-level methods have been based on encoding each mention of a pair of entities, and designing a mechanism to pool these encodings  into a single representation. This representation is then used to classify the relation between the entity pair .  However, representation learning methods for both mention-level and pair-level extraction typically use a point estimate for each representation. As a result, they may struggle to capture the nature of the true, potentially complex relations between each pair of entities. For example, Figure  shows sentences for two entity pairs which demonstrate that relation statements can be very different, typically depending on biological circumstances . Such nuanced relations can be difficult to capture with a single point estimate.  We hypothesise that there is a true underlying relation for each entity pair, and that this relation can be multimodal . The sentences containing each pair are textual observations of these underlying relations.  We therefore propose a probabilistic model which uses a continuous latent variable to represent the true relation between each entity pair. The distribution of a sentence containing that pair is then conditioned on this latent variable. In order to be able to model the complex relations between each entity pair, we use an infinite mixture distribution for the latent representation.  Our model provides a unified architecture for learning representations of relations between entity pairs both at mention and pair level. We show that  the posterior distribution of the latent variable can be used for mention-level relation classification. We also demonstrate that the prior distribution from the same model can be used for pair-level classification. On both tasks, we achieve results competitive with strong baselines with a model which has fewer parameters and is significantly faster to train.  The code is released at \url{ https://github.com/BenevolentAI/RELVM} %.        Mention-level relation extraction is typically performed using supervised learning. In the general domain, \citet{tacred2017} combine an LSTM with a position-aware attention mechanism to perform multiclass relation extraction. \citet{SFLK2019} fine-tune the BERT  architecture to relation extraction tasks by enforcing similarity between representations of sentences containing the same pair of entities across a corpus.  construct a teacher model to generate soft labels which guide the optimisation of a student network via knowledge distillation. In the biomedical and scientific domains, BioBERT  and SciBERT  train the BERT architecture on domain-specific corpora, achieving state of the art results on mention-level relation extraction tasks. \citet{zhang-hybrid-2018} combine an RNN over the sentence's words and a CNN over its dependency graph to classify drug-drug and protein-protein interactions.  Pair-level relation extraction usually relies on distant supervision . In the general domain, \citet{multir2011} develop a latent variable model to perform multi-instance learning while handling overlapping relations. \citet{LSLLS2016} use an attention mechanism to pool the representations of sentences containing a given pair into a single representation, which is then used as the input to a classifier. \citet{QuirkPoon16} capture relations across sentences by linking dependency graphs between sentences. Other pair-level methods build representations using unsupervised models. \citet{ijcai2019relationvectors} use a latent variable model to learn a point-estimate representation from the unigram distribution of tokens co-occurring in sentences with the given pair. \citet{pair2vec2019} learn representations of pairs of entities by maximising their pointwise mutual information  with the contexts that the entities appear in. In the biomedical domain, \citet{BRAN2018} build a paragraph-level representation using a modified Transformer network, and aggregate over mentions using a softmax function. \citet{knowledge-graph-cascade-2019} combine knowledge embeddings and graph embeddings using a cascade learning framework to predict links in biochemical networks. \citet{ebc-altman-2015} use a distributional semantics approach to cluster together drug-gene pairs which are related in similar ways.  Contrary to our work, there does not appear to be prior research performing both mention-level and pair-level relation extraction with a unified model.           In this paper, we introduced the novel task of collaborative storytelling, where humans and AI agents work together to make stories. We presented a collaborative storytelling system that tunes a large-scale neural LM on storytelling data and uses a sampling-and-ranking approach to select more human-preferred story continuations. Quantitative evaluation of our system found that tuning and ranking both greatly contribute to its capability to generate story continuations that human evaluators prefer and consider acceptable. Qualitative evaluation of human evaluator preferences showed that humans found tuned+ranked more preferable than tuned and tuned more preferable than untuned in terms of engagingness, interestingness, and humanness metrics, as well as overall story quality preferences. Finally, we identified areas for potential future work, including evaluation of stories produced by humans and our system, integration of our system into intelligent agents such as robots and avatars, and improvement of generated story continuation quality by allowing genres or moods to be targeted.        The next two lines define the bibliography style to be used, and    the bibliography file.  \clearpage          If your work has an appendix, this is the place to put it.    \clearpage         
","   Mention-level relation extraction is typically performed using supervised learning. In the general domain, \citet{tacred2017} combine an LSTM with a position-aware attention mechanism to perform multiclass relation extraction. \citet{SFLK2019} fine-tune the BERT  architecture to relation extraction tasks by enforcing similarity between representations of sentences containing the same pair of entities across a corpus.  construct a teacher model to generate soft labels which guide the optimisation of a student network via knowledge distillation. In the biomedical and scientific domains, BioBERT  and SciBERT  train the BERT architecture on domain-specific corpora, achieving state of the art results on mention-level relation extraction tasks. \citet{zhang-hybrid-2018} combine an RNN over the sentence's words and a CNN over its dependency graph to classify drug-drug and protein-protein interactions.  Pair-level relation extraction usually relies on distant supervision . In the general domain, \citet{multir2011} develop a latent variable model to perform multi-instance learning while handling overlapping relations. \citet{LSLLS2016} use an attention mechanism to pool the representations of sentences containing a given pair into a single representation, which is then used as the input to a classifier. \citet{QuirkPoon16} capture relations across sentences by linking dependency graphs between sentences. Other pair-level methods build representations using unsupervised models. \citet{ijcai2019relationvectors} use a latent variable model to learn a point-estimate representation from the unigram distribution of tokens co-occurring in sentences with the given pair. \citet{pair2vec2019} learn representations of pairs of entities by maximising their pointwise mutual information  with the contexts that the entities appear in. In the biomedical domain, \citet{BRAN2018} build a paragraph-level representation using a modified Transformer network, and aggregate over mentions using a softmax function. \citet{knowledge-graph-cascade-2019} combine knowledge embeddings and graph embeddings using a cascade learning framework to predict links in biochemical networks. \citet{ebc-altman-2015} use a distributional semantics approach to cluster together drug-gene pairs which are related in similar ways.  Contrary to our work, there does not appear to be prior research performing both mention-level and pair-level relation extraction with a unified model.",171
"   Duplicate question detection  is an important application in information retrieval and NLP . It allows systems to recognize when two questions share an answer. This is significant for community forums, such as StackExchange\footnote{https://stackexchange.com/}   to increase their effectiveness in avoiding redundant questions and displaying relevant answers to search questions. It is also important for FAQ retrieval question answering systems .  To learn DQD models for \stackexchange{}, question pairs are usually annotated with duplication information that is extracted from community-provided meta-data. Such annotations are sparse for most domains, e.g., a new \stackexchange{} forum providing support for a new product.  Therefore, leveraging other training signals either from unsupervised data or supervised data from other domains is important .  Pre-trained language models  like BERT  and RoBERTA  are  great unsupervised textual representations. Several recent efforts adapt PLMs for the domains of interest  by  self-supervised fine-tuning on unsupervised domain data, which has shown  to be promising in several scenarios  .  We follow that and tune BERT on \stackexchange{} domains to obtain richer representations for the task of DQD.   Recently,  -nearest neighbors  is applied on the PLM representations for language modeling  and dialogue . We extend this line of study and apply \cdknn{} for  cross-domain generalization in DQD, where the models are trained on data from a source domain, and applied on data from a target domain.  To do so, we represent pairs from source and target in a common representation space and then score target pairs using nearest neighbors in the source pairs. \figref{knnprocess} shows an illustration of this procedure.   % The specific properties of \stackexchange{} DQD % is important to make this approach effective.  Our study on AskUbuntu as target and source datasets of , which include several domains of \stackexchange{} and also Quora and Sprint, reveals that \cdknn{} is more effective compared to cross-entropy classification if  the pair representation space from PLMs is rich for the target domain, i.e., adapted on the unsupervised data  from target or similar domains; or   source and target domains have large distributional shifts.   We make the following contributions:   We present the first study of combining strengths of \cdknn{} and      neural representations for cross-domain generalization in a sentence matching task, i.e., DQD.   Our experimental results  on cross-domain DQD demonstrate that \cdknn{} on      rich question-pair representations advances the results of      cross-entropy classification, especially when shifts in source to target domains is substantial.     Sparsity in DQD labeled examples in the \stackexchange{} domains is   tackled by leveraging the unsupervised data , the supervised data from other domains, or both  . We follow these approaches and learn representations from unsupervised data and  apply them for better generalization when external supervised data in other domains is used.    Large PLMs  learn high-quality representations from raw text through self-supervised objectives. This ability has been leveraged outside of DQD to provide better cross-domain generalization .  A combination of \cdknn{} with neural representations is the subject of several earlier work, mostly in image classification   \citet{papernot2018deep} show that \cdknn{} is more robust to out-of-distribution examples.  More related to our work, \citet{khandelwal2019generalization} apply \cdknn{} on neural representations computed from and applied on language modeling task and interpolates its scores with Softmax. They validate that this is effective in different scenarios, including domain adaptation.  Here we do not compute representations for \cdknn{} on the same task as the one we apply; the representations are computed by language modeling and applied on DQD.       We present \modelabbrevname{}, a transparent neural-symbolic reasoning framework for visual question answering, that incorporates [look, read, think and answer] steps to provide a human-readable form of justification at each step. The modular design of our methodology enables the whole framework to be trainable end-to-end. Our experiments on GQA dataset show that \modelabbrevname{} achieves high accuracy on full answer generation task, outperforming the state-of-the-art LXMERT results by a noticeable 15\  absolute margin. In addition, \modelabbrevname{} performance drops significantly more than LXMERT, when object attributes and relationships are masked, hence indicating that \modelabbrevname{} makes a step forward, towards truly understanding the question, rather than making a smart guess based on superficial data correlations. In the validation study, we have shown that when provided with an oracle scene graph, \modelabbrevname{} is able to achieve a high accuracy on both short answers  and full answers , nearing the theoretical bound 96\  on short answers. These observations indicate that better scene graph prediction methods offer a great potential in further improving \modelabbrevname{} performance on both short-answer and full-answer tasks.   
"," Sparsity in DQD labeled examples in the \stackexchange{} domains is   tackled by leveraging the unsupervised data , the supervised data from other domains, or both  . We follow these approaches and learn representations from unsupervised data and  apply them for better generalization when external supervised data in other domains is used.    Large PLMs  learn high-quality representations from raw text through self-supervised objectives. This ability has been leveraged outside of DQD to provide better cross-domain generalization .  A combination of \cdknn{} with neural representations is the subject of several earlier work, mostly in image classification   \citet{papernot2018deep} show that \cdknn{} is more robust to out-of-distribution examples.  More related to our work, \citet{khandelwal2019generalization} apply \cdknn{} on neural representations computed from and applied on language modeling task and interpolates its scores with Softmax. They validate that this is effective in different scenarios, including domain adaptation.  Here we do not compute representations for \cdknn{} on the same task as the one we apply; the representations are computed by language modeling and applied on DQD.",172
" Speaking and listening are the most common ways in which humans convey and understand each other in daily conversations. Nowadays, the speech interface has also been widely integrated into many applications/devices like Siri, Google Assistant, and Alexa . These applications use speech recognition-based approaches  to understand the spoken user queries. Like speech, the text is also a widely used medium in which people converse. Recent advances in language modeling and representation learning using deep learning approaches  have proven to be very promising in understanding the actual meanings of the textual data, by capturing semantical, syntactical, and contextual relationships between the textual words in their corresponding learned fixed-size vector representations.   Such computational language modeling is difficult in the case of speech for spoken language understanding because unlike textual words,  spoken words can have different meanings of the same word when spoken in different tones/expressions ,  it is difficult to identify sub-word units in speech because of the variable-length spacing and overlapping between the spoke-words , and  use of stress/emphasis on few syllables of a multi-syllabic word can increase the variability of speech production . Although the textual word representations capture the semantical, syntactical, and contextual properties, they fail to capture the tone/expression. Using only speech/audio data for training spoken-word representations results in semantically and syntactically poor representations.   So in this paper, we propose a novel spoken-word representation learning approach called STEPs-RL that uses speech and text entanglement for learning phonetically sound spoken-word representations, which not only captures the acoustic and contextual features but also are semantically, syntactically, and phonetically sound. STEPs-RL is trained in a supervised manner such that the learned representations can capture the phonetic structure of the spoken-words along with their inter-word semantic, syntactic, and contextual relationships. We validated the proposed model by  evaluating semantical and syntactical relationships between the learned spoken-word representations on four widely used word similarity benchmark datasets, and comparing its performance with the textual word representations learned by Word2Vec \& FastTexT , and  investigating the phonetical soundness of the generated vector space.     Earlier, speech processing was done using feature learning-based models like deep neural networks  . The DNN models were able to capture contextual and temporal information from the speech-based data after the introduction of sequential neural networks like RNNs , LSTMs , Bi-LSTMs , and GRUs .   Recent research by \citet{tera} has presented the use of a transformer-based self-supervised speech representation learning approach called TERA that uses multi-target auxiliary tasks. TERA is trained by generating acoustic frame reconstructions; \citet{Schneider2019} introduced wav2vec which is a CNN based model pre-trained in a unsupervised manner using contrastive loss to learn raw audio representations; \citet{khurana2020convolutional} explored the use of black-box variational inference for linguistic representation learning of speech using an unsupervised generative model; \citet{DBLP:journals/corr/abs-1807-03748} proposed Contrastive Predictive Coding  for extracting representations from high dimension data by predicting future in latent space, using autoregressive models; \citet{pmlr-v48-amodei16} presented an end-to-end deep learning model to recognize speech in two vastly different languages ; \citet{hsu2017learning} proposed a novel variational autoencoder based model that learns disentangled and interpretable latent representations of sequential data in an unsupervised manner; \citet{Ling2020} used BERT encoder for learning phonetically aware contextual speech representation vectors; \citet{8736337} proposed a Word2Vec type sequence-to-sequence autoencoder model for embedding variable-length audio segments. Other works on learning fixed-length spoken-word vector representations that use multi-task learning include , , ,  and .      In this paper, we discussed the methodological basis and realization of a tool allowing the learner to systematically learn the lexical material needed to be able to read a book they are interested in. Automatically structuring the lexical space and sequencing the learning is achieved through distributional semantic methods, the automatic identification of word families, and concepts from network analysis. The graph-based domain model that is automatically derived from the given book serves as the foundation of a learner model supporting the selection of an efficient learning path through the lexical space to be acquired. Multi-gap activities are automatically generated from the targeted book and used for practice and testing activities.     The application is also well suited to be a dedicated vocabulary   learning application as indicated earlier. The teachers can guide the   students to master vocabulary from the books of renowned authors where   they also exposed to a the intriguing language usage.  In addition to self-guided learning for people interested in reading specific books, which may be particularly useful in the context of so-called intensive reading programs, the approach is particularly well-suited for the English for Specific Purposes context, where both the language and the particular content domain are of direct importance.  Given this kind of integration of language and content learning, a similar affinity exists to so-called Content and Language Integrated Learning .    listhe thely auisd a basis for  is aab limitation that we should mention and point to an option for overcoming it:   This application can also be upgraded to learn domain knowledge.   Since the distribution semantic space is defined by a pre-trained   vector space model.  Some of the domain specific proper nouns are   missing. This could be overcame by training a custom vector space for   the chosen text. This leverage this application to facilitate domain   knowledge learning/revising like jargon, scientific names,   geographical names etc...    Additional supporting materials could be explored to scaffold the   learning apart from the usage in the chose text, dictionary reference   and translation of the word into learner's native language which are   used currently.    Though the learn model is further pruned to improve the visualisation.   The connectivity are potentially overwhelming. There could be a   considerable improvement in reporting global and local progress in the   structured space.  Or a simplified approach of visual thesaurus could   be adopted.    This application provides a lot of scope for gamification because of   its exploratory objective of vocabulary space provided with a graph   based framework to maximise the coverage. Which could be themed around   the goal of being reaching/trained for the actual task with more   engaging activities.    
"," Earlier, speech processing was done using feature learning-based models like deep neural networks  . The DNN models were able to capture contextual and temporal information from the speech-based data after the introduction of sequential neural networks like RNNs , LSTMs , Bi-LSTMs , and GRUs .   Recent research by \citet{tera} has presented the use of a transformer-based self-supervised speech representation learning approach called TERA that uses multi-target auxiliary tasks. TERA is trained by generating acoustic frame reconstructions; \citet{Schneider2019} introduced wav2vec which is a CNN based model pre-trained in a unsupervised manner using contrastive loss to learn raw audio representations; \citet{khurana2020convolutional} explored the use of black-box variational inference for linguistic representation learning of speech using an unsupervised generative model; \citet{DBLP:journals/corr/abs-1807-03748} proposed Contrastive Predictive Coding  for extracting representations from high dimension data by predicting future in latent space, using autoregressive models; \citet{pmlr-v48-amodei16} presented an end-to-end deep learning model to recognize speech in two vastly different languages ; \citet{hsu2017learning} proposed a novel variational autoencoder based model that learns disentangled and interpretable latent representations of sequential data in an unsupervised manner; \citet{Ling2020} used BERT encoder for learning phonetically aware contextual speech representation vectors; \citet{8736337} proposed a Word2Vec type sequence-to-sequence autoencoder model for embedding variable-length audio segments. Other works on learning fixed-length spoken-word vector representations that use multi-task learning include , , ,  and .",173
"  Recent decades have brought about an increase in the use of computer-based tools in practically every  field of human endeavor. The field of education is no exception. Such tools can be used to augment or  even completely replace traditional face-to-face teaching methods. The emergence of online learning platforms has necessitated the development of means to enable learning activities, such as  group discussions, to be performed through the use of technology. One such example of a learning  platform is the IMapBook software suite aimed at increasing the literacy and reading  comprehension skills of elementary school-aged children through the use of web-based eBooks,  embedded games related to their contents, as well as moderated group discussions. Keeping these discussions constructive and relevant can be difficult and usually requires a  discussion moderator to be present at all times. This can limit the opportunities for such discussions to take place. Leveraging the methods and insights  from the fields of artificial intelligence and machine learning, we can attempt to develop systems to automatically classify messages into  different categories and detect when the discussion has veered off course and necessitates intervention. Our research tackles this problem using a  compilation of discussions obtained during pilot studies testing the effectiveness of using the IMapBook software suite in 4th-grade classrooms.  The studies were performed in 8 different Slovene primary schools and, in total, included 342 students.  The discussions consist of 3541 messages along with annotations specifying their relevance to the  book discussion, type, category, and broad category. The ID of the book being discussed and the time  of posting are also included, as are the poster's school, cohort, user ID, and username.  Each message was also manually translated into English to aid non-Slovene-speaking researchers.  The use of the Slovene language presents unique challenges in applying standard language  processing methods, many of which are not as readily available as for other, more widely spoken languages.  Given a sequence of one or more newly observed messages, we want to estimate the relevance of  each message to the actual topic of discussion. Namely, we want to assign messages into two categories 閳 relevant to the book being discussed or not.  Additionally, we want to predict whether the message is a question, an answer, or a statement which we call the type of the message. Finally, we want to  assign a category label to each message where the possible labels can be either 'chatting', 'switching', 'discussion', 'moderating', or 'identity'.  Building a predictive model capable of performing such predictions with acceptable performance would allow us to experiment with including this new  level of automation in the IMapBook software suite as well as in any related products. The research insights are also applicable to areas such as  online user comments and content moderation.     The objective of our research is closely related to tasks concerning online  content moderation which has been the subject of much research in recent years. Perhaps one of the earliest studies done on this subject is the 2009  study by Yin et al.  in which the authors used sentiment/contextual features  in tandem with the TF-IDF approach to detect online harassment. An earlier study by Mclaren et al. specifically focuses on the use of machine learning techniques to support the mediation of student  online discussions in a uniquely constrained network-like environment that differs significantly from  ours . A 2016 study by Kadunc focuses on using machine learning methods to analyze  the sentiment of Slovene online comments and provides an important contribution in the form of an  opinion lexicon . However, the specifics and unique challenges presented by the  problem of classifying short Slovene text produced by this age group remains an area with little to  no research currently done.    In this paper, we introduced STEPs-RL for learning phonetically sound spoken-word representations using speech and text entanglement. Our approach achieved an accuracy of 89.47\  in predicting phonetic sequences when both gender and dialect of the speaker are used in the auxiliary information. We also compared its performance using different configurations and observed that the performance of the proposed model improved by  increasing the spoken word latent representation size, and  the addition of auxiliary information like gender and dialect. We were not only able to validate the capability of the learned representations to capture the semantical and syntactical relationships between the spoken-words but were also able to illustrate soundness in the phonetic structure of the generated vector space. For future work, we plan to  extend the model to use attention mechanisms,  improve performance by using transformer-based architecture, and  experimenting on larger datasets, and  using features other than MFCCs.     \comment{ 
","  The objective of our research is closely related to tasks concerning online  content moderation which has been the subject of much research in recent years. Perhaps one of the earliest studies done on this subject is the 2009  study by Yin et al.  in which the authors used sentiment/contextual features  in tandem with the TF-IDF approach to detect online harassment. An earlier study by Mclaren et al. specifically focuses on the use of machine learning techniques to support the mediation of student  online discussions in a uniquely constrained network-like environment that differs significantly from  ours . A 2016 study by Kadunc focuses on using machine learning methods to analyze  the sentiment of Slovene online comments and provides an important contribution in the form of an  opinion lexicon . However, the specifics and unique challenges presented by the  problem of classifying short Slovene text produced by this age group remains an area with little to  no research currently done.",174
" The Winograd Schema Challenge\/  was proposed by  as a means to test whether a  machine has human-like intelligence. It is an alternative to the well known Turing Test\/  and has been designed with the motivation of reducing certain problematic aspects that affect the TT. Specifically, while the TT is subjective in nature, the WSC provides a purely objective evaluation; and whereas passing the TT requires a machine to behave in a deceptive way, the WSC takes the form of a positive demonstration of intelligent capability.  The core problem of the WSC is to resolve the reference of pronouns occurring in natural language sentences.  To reduce the possibility that the task can be accomplished by procedures based on superficial or statistical characteristics, rather than `understanding' of the sentence, they specify that the test sentences used in the WSC, should be constructed in pairs, which have similar structure and differ only in some key word or phrase, and such that the correct referent of the pronoun is different in the two cases. This sentence pair, together with an indication of which pronoun is to be resolved and a pair of two possible candidates, is called a Winograd Schema.   The following is an example of the Winograd schemas from the original WSC273 data set :    \item The trophy doesn't fit in the brown suitcase because {\bf it} is too small\/.  \end{enumerate}   design Winograd schemas to require background knowledge to resolve a pronoun, which can be an evidence of thinking\/. Therefore, they exclude the sentences that can be resolved by a statistical association within a sentence.   In this paper, we introduce a keyword method to define domains in Winograd schemas. To our best knowledge, this is the first work to use keywords for defining domains in WSC and explore high-level patterns in them. To use the domain-specific high-level patterns, we also develop an advanced high-level knowledge-based reasoning method by modifying the method of . Furthermore, we suggest a simple ensemble method that combines knowledge-based reasoning and machine learning. By the experiments on the domain-specific data set, the ensemble method gives a better performance than each single method. Lastly, we also propose a `robust' accuracy  measure that is more objective by improving the switching method of .     Knowledge-based reasoning and machine learning are the two main approaches to resolve Winograd schemas.   	 It is advantageous that knowledge-based reasoning methods can give logical explanations for the answers of Winograd schemas by reasoning. \citet[p.18]{bailey:2015} use the ``correlation calculus'' for the reasoning by using different levels of background knowledge principles. In addition,  automates graphical representations of a sentence and the reasoning by using K-Parser  and Answer Set Programming  .  However, knowledge-based reasoning methods also have limitations on automation and building a knowledge base that covers this general domain\/.  do not give an automatic method to transform a natural language sentence into the form of first-order logic that they use. Though  have an automation method to extract background knowledge, their method is based on using a search engine, which cannot guarantee acquiring necessary knowledge.    Section 3    The best results were achieved by using the Feature stacking method model built on the complete  feature subset. The results indicate the performance to be sufficient for the methods to be used  in real-world tools and platforms. A significant portion of the information needed for  correct classifications is hidden in the strong temporal interdependence of the messages which  our developed methods exploited only marginally.  
"," Knowledge-based reasoning and machine learning are the two main approaches to resolve Winograd schemas.   	 It is advantageous that knowledge-based reasoning methods can give logical explanations for the answers of Winograd schemas by reasoning. \citet[p.18]{bailey:2015} use the ``correlation calculus'' for the reasoning by using different levels of background knowledge principles. In addition,  automates graphical representations of a sentence and the reasoning by using K-Parser  and Answer Set Programming  .  However, knowledge-based reasoning methods also have limitations on automation and building a knowledge base that covers this general domain\/.  do not give an automatic method to transform a natural language sentence into the form of first-order logic that they use. Though  have an automation method to extract background knowledge, their method is based on using a search engine, which cannot guarantee acquiring necessary knowledge.    Section 3",175
" KR\&R systems work well for certain knowledge-rich domains that typically involve a  set of axioms or rules, use structured queries and datasets, and have a need for precise logical inference with explanations. Formal logic-based reasoning engines such as Cyc  and Ergo  have been successfully deployed in domains such as legal, healthcare and finance.  One of the main advantages of using such systems is transparency 閳 the underlying reasoning of the system is well-understood and can be justified to end-users.   However, there are several known drawbacks of logic-based approaches. For one, the inference procedures are highly brittle in that they require precise matching/unification of logical terms and formulae in order to construct a complete explanation. Secondly, traditional reasoners don閳ユ獩 deal with uncertainty well , whereas rules in real-world applications are often probabilistic and contextual. Thirdly, all such systems suffer from the knowledge acquisition problem . Often, the rules are hand-coded, an approach which doesn閳ユ獩 scale in general.  Our problem domain is Natural Language Understanding , an area where all the issues mentioned above come into play 閳 the need to acquire and use implicit background knowledge to understand text, the application of rules differently based on the context, and the use of imperfect/fuzzy alignment of concepts and relations when doing reasoning.   To address these issues, we devise a novel FOL-based reasoner, called Braid. Braid includes a backward and forward chainer, assumption based reasoner and a constraint solver. This paper only refers to the backward chaining component, which we refer to as Braid-BC.    Braid-BC supports rules with confidences, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoning engines.  The custom-unifiers can be based on any statistical techniques, as long as they can propose and score mappings between the terms of two logical propositions . For example, we use neural matching functions as unifiers. Their purpose is to help the reasoner find proofs even when  goals, rule conditions and/or facts do not align perfectly.     The dynamic rule-generator  is given a target proposition  and a knowledge base  as input, and outputs a scored list of hypothesized rules that could be used to prove that proposition. The purpose of rule-generation is to connect the dots when the knowledge required for an inference is missing from the static KB. We describe two DRG implementations - one using a neural  rule generation model that was fine-tuned on a dataset of crowd-sourced causal rules, known as GLUCOSE , and the second that uses a rule-template based technique.     We describe the reasoning algorithms used in Braid-BC, and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query in a highly scalable manner. Our approach shares some similarities with the RETE framework  for matching production rules  but makes several novel extensions: we primarily do backward chaining via a heuristic best-first search , leverage a Master-Worker architecture where the Master builds the main proof graph while Workers make local inferential updates, and define general functions for Unifiers and Provers that lets us plug in various reasoning strategies combining standard reasoning  with statistical approaches .        State-of-the-art logical reasoning systems such as Cyc, Ergo, Pellet , Vampire , SPASS  etc. have a rich set of features beyond what Braid currently supports such as higher order logics , more sophisticated identity and negation reasoning, disjunction support and in some cases, explicit defeasible rule theories. However, they do not support rules with confidences, custom unification functions or dynamic rule-generation, features we believe are needed to overcome the brittleness of standard deductive reasoning.  Also, our distributed task-based reasoning framework which can scale to multiple cores/machines on a cluster has several novel aspects. The design shares some similarities with the RETE algorithm, in that propositional atoms in rules are nodes in an inference graph , we have join-support nodes similar to beta nodes in the RETE network to compute joint solutions across conjunctions, nodes are shared across rules and are associated with bindings that flow based on the RETE network graph. However, the RETE algorithm is typically used for forward inference, whereas our reasoning paradigm is goal-driven backward chaining. Moreover, we extend the approach to include a heuristic search strategy to deal with confidences associated with fuzzy unifications and rules, support for existential quantifications, and dynamic rules.  Our overall design has several benefits: reusing solutions for previously solved goals , minimizing communication overhead by ensuring locality of inferential computations on Workers, only passing new solutions between nodes to limit updates, potential for efficient belief-revision / truth maintenance by storing a global proof support graph across all queries, and updating results incrementally when the KB changes.    We contrast Braid with Markov Logic Networks  , a prominent SRL framework. An MLN KB is a collection of FOL rules with weights. The KB itself acts as a template for generating a Markov Network  which is created by grounding the variables using the constants in the KB. In the final network, each node  is a ground literal from a rule in the KB, and each rule forms a clique, that has a weight w. The basic idea is that given a particular configuration of truth values for the variables , if a rule containing those nodes is satisfied and its weight is high, the probability of the world goes up, and vice versa . Thus, rules thus act as soft constraints on the likelihood of the world. Given a KB of true facts along with the weighted rules, we can perform MAP inference to answer FOL queries .   The main issue with MLNs is scalability due to the grounding step which is exponential in the size of the KB. Additionally, MLN閳ユ獨 work with a static KB and do not support the dynamic addition of rules during the inference process.    In recent times, there has been a growing interest in exploring neural-symbolic approaches such as Logic Tensor Networks  , which use distributed representations for logical symbols. For example, in LTNs, each constant is mapped to a real valued vector, each n-ary function is defined as a linear transform over its argument vectors, and predicates and clauses are associated with a 0-1 score , computed by using a tensor network . The representations are learned by optimizing for approximate satisfiability of the clauses in the KB. We share the same objective with such approaches, namely, overcoming the brittleness of standard reasoning algorithms using statistical methods and fuzzy reasoning.  However, one concern with the LTN model is scalability, as each clause is trained using a separate tensor model. It is also unclear how much training data is needed to learn reliable representations. Finally, it is not clear how such a model can produce precise explanations/proofs for its final decisions, as the reasoning process becomes opaque.     We take an alternate approach to combining neural-symbolic information. Instead of using distributed representations for FOL term/formulae and learning how to compose them using neural models, we use an FOL-resolution styled model as the underlying reasoning paradigm which lets us preserve explicability of the final results, while allowing for statistical methods  to be used for fuzzy matching/unifications and hypothesizing missing rules. Also, we believe our implementation based on a distributed task-framework and message passing is more scalable.      A more directly related neuro-symbolic approach is that taken by NL-Prolog  in which the authors use a Prolog-like system to do back-chaining from a query to find proofs, though the entities/predicates have distributed representations , and the inference rules are learned during training by specializing generic templates like \texttt{P1 :- P2, P3}. On the surface, the notions of weak unification and dynamic rule induction in NL-Prolog seem very similar to that in Braid. However, there are some important fundamental differences: NL-Prolog is at its core an E2E differentiable system whose explanations are not fully transparent . Also, the embeddings for entities/predicates once learned during training are fixed at test time and independent of the local context. Finally, there are various hyperparameter choices such as number of rule template instances to create which can impact the final result and it is unclear how to set these appropriately.        On the other hand, Braid at its core is a symbolic reasoning engine that produces transparent logical explanations but uses distributed representations for doing fuzzy unification and rule hypothesizing/scoring. The entity/predicate embeddings used in fuzzy unification come from a transformer-based deep learning model which looks at the local context in which the entities/predicates appear . Also, the dynamically induced rules are fully interpretable , and their confidence scores are estimated via pre-trained language models.     In this work, we investigated the integration of structural information from a constituent tree in a neural model for Frame-semantic parsing. Constituent representations are learned through a GCN,   to learn encoded representations of syntactic constituents, which is trained with the specific task objective.  and used to build constituency path features to be added to every word representation in a sequence.  Each word in a sequence is enriched with syntactic information by summing all the constituent learned encodings on the path between the word and a task-specific node in the tree, e.g. the target word of a predicate.  We tested our approach on all the Frame-semantic parsing sub-tasks, namely Target Identification, Frame Identification, and Semantic Role Labeling, showing that such features contribute mainly on the TI and the SRL tasks.   Constituency path features can be applied  Future work will cover the application of the proposed constituency path features  to other sequence labelling based tasks, e.g. Named-Entity Recognition. Moreover, other modifications of GCNs have to be tested in this same framework, e.g. to assess whether Attention-based GCN may learn more refined constituent representations. Finally, these representations may be used in a node-classification approach, inspired by seminal works , in an attempt to move away from the well-used sequence labelling model of   recent years.        \clearpage      
","     State-of-the-art logical reasoning systems such as Cyc, Ergo, Pellet , Vampire , SPASS  etc. have a rich set of features beyond what Braid currently supports such as higher order logics , more sophisticated identity and negation reasoning, disjunction support and in some cases, explicit defeasible rule theories. However, they do not support rules with confidences, custom unification functions or dynamic rule-generation, features we believe are needed to overcome the brittleness of standard deductive reasoning.  Also, our distributed task-based reasoning framework which can scale to multiple cores/machines on a cluster has several novel aspects. The design shares some similarities with the RETE algorithm, in that propositional atoms in rules are nodes in an inference graph , we have join-support nodes similar to beta nodes in the RETE network to compute joint solutions across conjunctions, nodes are shared across rules and are associated with bindings that flow based on the RETE network graph. However, the RETE algorithm is typically used for forward inference, whereas our reasoning paradigm is goal-driven backward chaining. Moreover, we extend the approach to include a heuristic search strategy to deal with confidences associated with fuzzy unifications and rules, support for existential quantifications, and dynamic rules.  Our overall design has several benefits: reusing solutions for previously solved goals , minimizing communication overhead by ensuring locality of inferential computations on Workers, only passing new solutions between nodes to limit updates, potential for efficient belief-revision / truth maintenance by storing a global proof support graph across all queries, and updating results incrementally when the KB changes.    We contrast Braid with Markov Logic Networks  , a prominent SRL framework. An MLN KB is a collection of FOL rules with weights. The KB itself acts as a template for generating a Markov Network  which is created by grounding the variables using the constants in the KB. In the final network, each node  is a ground literal from a rule in the KB, and each rule forms a clique, that has a weight w. The basic idea is that given a particular configuration of truth values for the variables , if a rule containing those nodes is satisfied and its weight is high, the probability of the world goes up, and vice versa . Thus, rules thus act as soft constraints on the likelihood of the world. Given a KB of true facts along with the weighted rules, we can perform MAP inference to answer FOL queries .   The main issue with MLNs is scalability due to the grounding step which is exponential in the size of the KB. Additionally, MLN闁炽儲鐛 work with a static KB and do not support the dynamic addition of rules during the inference process.    In recent times, there has been a growing interest in exploring neural-symbolic approaches such as Logic Tensor Networks  , which use distributed representations for logical symbols. For example, in LTNs, each constant is mapped to a real valued vector, each n-ary function is defined as a linear transform over its argument vectors, and predicates and clauses are associated with a 0-1 score , computed by using a tensor network . The representations are learned by optimizing for approximate satisfiability of the clauses in the KB. We share the same objective with such approaches, namely, overcoming the brittleness of standard reasoning algorithms using statistical methods and fuzzy reasoning.  However, one concern with the LTN model is scalability, as each clause is trained using a separate tensor model. It is also unclear how much training data is needed to learn reliable representations. Finally, it is not clear how such a model can produce precise explanations/proofs for its final decisions, as the reasoning process becomes opaque.     We take an alternate approach to combining neural-symbolic information. Instead of using distributed representations for FOL term/formulae and learning how to compose them using neural models, we use an FOL-resolution styled model as the underlying reasoning paradigm which lets us preserve explicability of the final results, while allowing for statistical methods  to be used for fuzzy matching/unifications and hypothesizing missing rules. Also, we believe our implementation based on a distributed task-framework and message passing is more scalable.      A more directly related neuro-symbolic approach is that taken by NL-Prolog  in which the authors use a Prolog-like system to do back-chaining from a query to find proofs, though the entities/predicates have distributed representations , and the inference rules are learned during training by specializing generic templates like \texttt{P1 :- P2, P3}. On the surface, the notions of weak unification and dynamic rule induction in NL-Prolog seem very similar to that in Braid. However, there are some important fundamental differences: NL-Prolog is at its core an E2E differentiable system whose explanations are not fully transparent . Also, the embeddings for entities/predicates once learned during training are fixed at test time and independent of the local context. Finally, there are various hyperparameter choices such as number of rule template instances to create which can impact the final result and it is unclear how to set these appropriately.        On the other hand, Braid at its core is a symbolic reasoning engine that produces transparent logical explanations but uses distributed representations for doing fuzzy unification and rule hypothesizing/scoring. The entity/predicate embeddings used in fuzzy unification come from a transformer-based deep learning model which looks at the local context in which the entities/predicates appear . Also, the dynamically induced rules are fully interpretable , and their confidence scores are estimated via pre-trained language models.",176
"  % Understanding how BERT works is important. % the presence of blackbox nlp  is an indication that the research community values the ability to understand the internals of deep neural networks. Pre-trained transformer models such as BERT  are currently ubiquitous within natural language processing  research and have demonstrated improvements in topics from sentiment analysis to semantic parsing . The widespread development and use of such models has led to an increased effort to interpret such models' decisions . % * understanding models is important in society % * BERT is used all over, so important to understand BERT As defined in \citet{doshivelez2017rigorous}, model interpretability is ``the ability [of a model] to explain or present in understandable terms to a human''.  Intuitively, a more interpretable model is easier to understand, debug and improve.  % It's hard to understand BERT because  % * it's a neural model with many, many parameters % * pre-training + fine-tuning is newer than just training from scratch  -> read literature introductions/motivations Interpreting modern pre-trained transformer models is difficult. First, modern deep learning models have hundreds of millions of parameters, and scale only continues to increase . Understanding the impact of a single parameter is nearly impossible because these models are densely connected. Combined with the sheer number of parameters, manual analysis is infeasible. Secondly, while both pre-training and fine-tuning are required for state-of-the-art performance, effort has focused on alternative pre-training methods . % Understanding the impacts of fine-tuning is still not well understood. \todo{do I need a citation here?}   % Previous work attempted to use attention Previous work uses BERT's self-attention mechanism to interpret the model's predictions . However, a body of work  shows that models' attention mechanisms cannot be interpreted on single-sequence classification tasks.  % We apply bert to a sequence classification task We apply BERT and two BERT-based models  to an existing sentence classification task proposed in \citet{aesw}. We compare BERT-based models' performances with previous baselines and then use methods presented in \citet{vashishth2019attention} and \citet{deyoung-etal-2020-eraser} to evaluate BERT's interpretability in single-sequence classification tasks. We find that fine-tuning can teach BERT to recognize previously unknown patterns in natural language and that BERT is more interpretable than the attention-based models analyzed in \citet{jain-wallace-2019-attention} and \citet{vashishth2019attention}. To summarize, the key contributions of this paper are:   % this is nice because  % * BERT hasn't been applied to it % * professional data set, we have a baseline, all human-annotated % * it has marked spans of edits before and after % To the best of our knowledge, BERT has not been applied to the Automatic Evaluation of Scientific Writing  task.       work showing that attention is not interpretable   * jain & wallace   * vashishth on single sequence classification  There is a body of work demonstrating that attention mechanisms are not faithful explanations of a model's prediction , especially in single-sequence classification tasks . These studies focus on attention mechanisms in LSTMs and hierarchical attention networks.    work analyzing BERT   * syntactic structure    * visualizations    * vashishth on switching attention heads   * effects of fine-tuning  However, BERT is based on the transformer's self-attention mechanism . \citet{vashishth2019attention} found that ``altering weights in self-attention based models does have a substantial [negative] effect on the performance.'' Indeed, there are multiple studies demonstrating that BERT's attention maps are interpretable. \citet{clark-etal-2019-bert} found that particular attention heads within BERT learn syntactic relations such as ``direct objects of verbs, determiners of nouns, objects of prepositions and objects of possessive pronouns.'' \citet{vig-belinkov-2019-analyzing} inspected attention maps from both BERT and GPT-2  and found that no probing is necessary to identify dependency relations.    work on interpretable models   * new dataset/metric   Besides interpreting BERT's attention for error analysis and model understanding, there is increased interest in NLP models with interpretability as a goal. \citet{deyoung-etal-2020-eraser} introduced a new data set and evaluation metric aimed at helping researchers evaluate models' explanations for predictions. BERT-based models are evaluated, but directly interpreting BERT's attention is not considered.     In conclusion, we have proposed a general approach to learn relation prototypes from unlabeled texts. The prototype learning method can be applied in current models for better relation extraction by transferring knowledge from relations with sufficient training data to long-tail relations. We have conducted extensive experiments to verify the effectiveness of the proposed method on two publicly available datasets and compared them with eight state-of-the-art baselines. The results present significant improvements, especially in long-tail settings. Further ablation study and case study also demonstrate the effectiveness of our proposed method and the generalization ability to current RE models from both quantitative and qualitative perspectives. In the future, we are interested in enhancing entity embeddings with KG including structure and attribute information.   investigating more advanced entity embedding models, such as Graph Attention Networks , to improve the implicit mutual relation representation as well as relation prototypes. Also, other side information  can be incorporated to enrich the entity co-occurrence graph for better modeling.  
","    work showing that attention is not interpretable   * jain & wallace   * vashishth on single sequence classification  There is a body of work demonstrating that attention mechanisms are not faithful explanations of a model's prediction , especially in single-sequence classification tasks . These studies focus on attention mechanisms in LSTMs and hierarchical attention networks.    work analyzing BERT   * syntactic structure    * visualizations    * vashishth on switching attention heads   * effects of fine-tuning  However, BERT is based on the transformer's self-attention mechanism . \citet{vashishth2019attention} found that ``altering weights in self-attention based models does have a substantial [negative] effect on the performance.'' Indeed, there are multiple studies demonstrating that BERT's attention maps are interpretable. \citet{clark-etal-2019-bert} found that particular attention heads within BERT learn syntactic relations such as ``direct objects of verbs, determiners of nouns, objects of prepositions and objects of possessive pronouns.'' \citet{vig-belinkov-2019-analyzing} inspected attention maps from both BERT and GPT-2  and found that no probing is necessary to identify dependency relations.    work on interpretable models   * new dataset/metric   Besides interpreting BERT's attention for error analysis and model understanding, there is increased interest in NLP models with interpretability as a goal. \citet{deyoung-etal-2020-eraser} introduced a new data set and evaluation metric aimed at helping researchers evaluate models' explanations for predictions. BERT-based models are evaluated, but directly interpreting BERT's attention is not considered.",177
"  .          % % final paper: en-us version           %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }    Recently, neural machine translation  has demonstrated impressive performance improvements and became the de-facto standard .    However, like other neural methods, NMT is data-hungry.   This makes it challenging when we train such a model in low-resource scenarios .   Researchers have developed promising approaches to low-resource NMT.   Among these are data augmentation , transfer learning , and pre-trained models .   But these approaches rely on external data other than bi-text.   To date, it is rare to see work on the effective use of bilingual data for low-resource NMT.    In general, the way of feeding samples plays an important role in training neural models.   A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems.   More systematic studies on this issue can be found in recent papers .   For example,  have pointed out that deep neural networks tend to prioritize learning ``easy'' samples first.   This agrees with the idea of curriculum learning  in that an easy-to-hard learning strategy can yield better convergence for training.    In NMT, curriculum learning is not new.   Several research groups have applied it to large-scale translation tasks although few of them discuss the issue in a low-resource setup .   The first question here is how to define the ``difficulty'' of a training sample.   Previous work resorts to functions that produce a difficulty score for each training sample.   This score is then used to reorder samples before training.   But the methods of this type enforce a static scoring strategy and somehow disagrees with the fact that the sample difficulty might be changing when the model is updated during training.   Another assumption behind curriculum learning is that the difficulty of a sample should fit the competence of the model we are training.   Researchers have implicitly modeled this issue by hand-crafted curriculum schedules  or simple functions , whereas there has no in-depth discussion on it yet.    In this paper, we continue the line of research on curriculum learning in low-resource NMT.   We propose a dynamic curriculum learning  method to address the problems discussed above.   The novelty of DCL is two-fold.   First, we define the difficulty of a sample to be the decline of loss .   In this way, we can measure how hard a sentence can be translated via the real objective used in training.   Apart from this, the DCL method explicitly estimates the model competence once the model is updated, so that one can select samples that the newly-updated model has enough competence to learn.      DCL is general and applicable to any NMT system.   In this work, we test it in a Transformer-based system on three low-resource MT benchmarks and different sized data selected from the WMT'16 En-De task.   Experimental results show that our system outperforms the strong baselines and several curriculum learning-based counterparts.            show that NMT systems result in worse translation performance in low-resource scenarios.   Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance.   Data augmentation  alleviates this problem by generating pseudo parallel data.    A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair .   Pre-trained language models trained with a large amount of monolingual data  improve the quality of NMT model significantly .    However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data.     demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-resource scenarios without any external resources.   This is consistent with our motivation.   The difference lies in that they focus on the model settings, and we explore the training strategy which utilizes bilingual data effectively for low-resource NMT.        Curriculum learning  is motivated by the learning strategy of biological organisms which orders the training samples in an easy-to-hard manner .   Benefited from organized training, the neural network explores harder samples effectively utilizing the previous knowledge learned from easier samples.    demonstrate curriculum learning speeds up the learning process, especially at the beginning of training.   Curriculum learning has been applied to several tasks, including language modeling , image classification , and human attribute analysis .    Curriculum learning has recently shown to train large-scale translation tasks efficiently and effectively by controlling the way of feeding samples.    construct mini-batch contains sentences similar in length and linguistic phenomena, then organize the order by increased complexity in one epoch.    group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules.    propose competence-based curriculum learning that select training samples based on sample difficulty and model competence.    use reinforcement learning to learn the curriculum automatically.    propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system.    propose uncertainty-aware curriculum learning.   To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup.    On the other hand, curriculum learning is similar to data selection and data sampling methods.   More similar work is that  propose a dynamic sampling method that calculates the decline of loss during training to improve the NMT training efficiency.   They start training from the full training set and then gradually decrease.    This is contrary to the idea of curriculum learning.      In addition, the spirit of curriculum learning has been widely used in the NMT, such as data selection , non-autoregressive NMT , domain adaptation .       future work and applications   might use edited versions as negative cases   seq2seq model?   compare with non-bert attention models? In this paper, we apply three BERT-based models to a sentence classification task, then quantify their interpretability through a small-scale manual study before expanding to a larger-scale automated study. We find that BERT's final attention layer is clearly interpretable by both human annotators and simple automated metrics.  Future work might expand the subset of examples that can be automatically annotated in order to further understand BERT's interpretability on different classes of edits. Additionally, more work is needed to understand the impacts of in-domain pre-training on model interpretability.     
","       show that NMT systems result in worse translation performance in low-resource scenarios.   Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance.   Data augmentation  alleviates this problem by generating pseudo parallel data.    A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair .   Pre-trained language models trained with a large amount of monolingual data  improve the quality of NMT model significantly .    However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data.     demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-resource scenarios without any external resources.   This is consistent with our motivation.   The difference lies in that they focus on the model settings, and we explore the training strategy which utilizes bilingual data effectively for low-resource NMT.        Curriculum learning  is motivated by the learning strategy of biological organisms which orders the training samples in an easy-to-hard manner .   Benefited from organized training, the neural network explores harder samples effectively utilizing the previous knowledge learned from easier samples.    demonstrate curriculum learning speeds up the learning process, especially at the beginning of training.   Curriculum learning has been applied to several tasks, including language modeling , image classification , and human attribute analysis .    Curriculum learning has recently shown to train large-scale translation tasks efficiently and effectively by controlling the way of feeding samples.    construct mini-batch contains sentences similar in length and linguistic phenomena, then organize the order by increased complexity in one epoch.    group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules.    propose competence-based curriculum learning that select training samples based on sample difficulty and model competence.    use reinforcement learning to learn the curriculum automatically.    propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system.    propose uncertainty-aware curriculum learning.   To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup.    On the other hand, curriculum learning is similar to data selection and data sampling methods.   More similar work is that  propose a dynamic sampling method that calculates the decline of loss during training to improve the NMT training efficiency.   They start training from the full training set and then gradually decrease.    This is contrary to the idea of curriculum learning.      In addition, the spirit of curriculum learning has been widely used in the NMT, such as data selection , non-autoregressive NMT , domain adaptation .",178
" Searching for code fragments is a very common activity in software development. The advent of large code repositories like GitHub\footnote{https://github.com/} and StackOverflow\footnote{https://stackoverflow.com/} has only increased the number of developers to rely on these repositories to search and reuse existing code . Traditional Information Retrieval techniques  do not work well for code search and retrieval tasks due to limited shared vocabulary between the source code and the natural language search text . Often, developers who are new to a programming language, search for code snippets in a context-free natural language. The choice of words used to search may not overlap with the code snippets leading to failure of traditional information retrieval systems. Therefore, there is a need to gain a deeper understanding of code and text in order to find semantically relevant code snippet.  Consider an example where a developer has a functional requirement to validate if age is always lesser than  and alert otherwise. The developer is tasked to enforce this check in Java. A naive Java developer who is not familiar with the language might make a query based on the requirement as: java check condition correctness. The top 10 results\footnote{As of December 9, 2019} in StackOverflow do not discuss the assert keyword. A more programming friendly query such as java boolean check or the assert keyword itself results in code snippets demonstrating the steps as the top result in StackOverflow.  Use of deep neural network models have shown tremendous improvements in many tasks across domains including language tasks . This success can be largely attributed, in part, to their ability to learn meaningful relationships among words in documents efficiently and represent them in a way such that semantically equivalent words tend to have similar representations . One such family of models that are popular for determining text similarity are Siamese networks. First introduced by , a typical Siamese network consists of two identical sub networks that share weights. They work in tandem on different inputs and the output of both the networks are evaluated by a distance measure that also acts as a scoring function. This has been successfully applied in many similarity tasks in image domain  and recently in text domain as well . Another useful property of these models is their capability to learn from fewer data examples . Since code can be treated as a special kind of text data, one possible way to approach the problem of Semantic Code Search  is to treat it as a similarity task where the objective is to bring semantically equivalent code snippets and their natural language descriptions closer. Therefore, we study the application of Siamese networks to code and corresponding text descriptions for semantic code search.  We apply multiple variations of the base Siamese network model on two different datasets for semantic code search and study its efficacy.  We further take the state of the art baselines -  and  on these datasets and observe that Siamese networks can improve over the baseline results invariably . Finally, we present our analysis on the  performance of different Siamese network architectures explored and identify the conditions for improved performance.  The rest of the paper is organized as follows. We introduce some relevant prior art in section . Next, in section , we provide some background on Siamese networks and semantic code search and introduce terminology. In section , we describe our approach and the different architectures investigated. In section , we describe our experiments and present the results. Finally in section , we perform a detailed analysis of our observations, followed by conclusions in section .  % \tikz \draw[]  rectangle  node[pos=.2]{Answer Here:};      Traditionally solutions to code search were based on information retrieval techniques and natural language processing comprising of query expansion and reformulation .  expanded the query with synonyms from wordnet to search for code snippets.  API documentation was leveraged for query expansion for code snippets retrieval by the CodeHow tool proposed by .  The fundamental drawback of the above techniques is that there is a disparity in word overlap, including their synonyms, between the intent expressed in natural language and the low-level implementation details in the source code. There is a need to semantically relate the words between the two domains.   Traditionally code search is based on information retrieval techniques.  expanded the query with synonyms from wordnet to search for code snippets.  expanded the query with relevant API documentation and further applied the extended boolean model to retrieve the code snippets.  Lately deep learning techniques have been administered for understanding code semantics and structure . approaches the task of code summarization through an attention-based recurrent framework. The model generates a summary for the given code snippet which is then used to rank the relevance of the code to queries.   Deep Code Search   follows a slightly different approach. DCS takes three aspects of code namely the a) method name, b) API invocation sequence, and c) the tokens and in parallel also takes the code descriptions as inputs to a different network and learns corresponding embeddings. Similarity between the embeddings are measured using cosine similarity. However, learning between embeddings is not explicitly shared. In one of our experiments we apply a Siamese network on top of DCS to combine the embedding learning framework with a Siamese style of sharing parameters between the two sub networks.  Neural Code Search   is an unsupervised model that proposed a way to aggregate vector representation of the source code using TF-IDF weighting to form document embeddings. It uses FastText  for learning the embeddings for these bags of words.  Further, the similarity between these embeddings are used for retrieval. Embedding Unification   is an extension over NCS, applying attention on the code tokens. Again, both these works treat code and text as independent learning modules and project them in a common high dimension space.   CoaCor  proposed an Reinforcement Learning framework to generate code annotations and show the improvements on code retrieval  when combined with existing CR models like DCS . Though the objective is similar, we study Siamese for joint learning on top of DCS without generation.  Siamese Networks are popular for tasks that involve finding similarity or a relationship between two comparable artifacts. Introduced first in images , it has been applied in text domain to score relevance between a question and an answer candidate  and for learning text similarity . But to the best of our knowledge, we are not aware of any work on applying Siamese networks for semantic code search.     Our contribution:               \item Siamese based learning                The task of code retrieval entails finding and indexing germane code fragments given a query. Retrieving code with natural language query is a challenging due to the inherent difference in code and natural language semantics.          formulated a joined embedding for understanding both code and natural languages. They dub this model CodeDescriptionEmbeddin Neural Network .The embedding was learned by minimising the distance of relevant code - description pairs. The code snippets are pre-processed to extract three major components: method name as a sequence, API sequence  and bag-of-words representation. These abstractions are shown to capture relevant semantic structure present in the code. These components are individually processed through separate DNNs. Another DNN is now used to combine these representations to get a combined semantic understanding of code. A language model is additionally used to learn the semantic meaning of the description. Both learned representation of the code and language are in the same dimension. CODEnn uses cosine loss to learn and retrieve similar pairs. Cosine similarity is used as retrieval and training metric.      NCS   Cambronero et al.  capture code semantics as informal intents through continuous representations learned by Neural Code Search  model. NCS learns code objectives in an informal sense contrary to its mathematical sense. In a learned continuous vector space, related pairs of objects can be mapped closer than unrelated pairs. Token level embedding are created for both code and description using fastText, which learns embeddings in an unsupervised manner. Standard information retrieval techniques of TF-IDF is used for code search. Both code and natural language have the same embedding matrix. NCS creates combined embedding of the code bag-of-word token embedding by TF-IDF weighted averaging. Token level embedding for natural language description are combine into a single by simple averaging. TF-IDF assigns weights of code token determine their importance based on their frequency in a single document vs frequency across documents. Cosine similarity is used to rank and retrieve code.     UNIF   Cambronero et al.  improve NCS with supervision. The new model is called Embedding Unification . They introduce supervision for creation of embedding matrix for code and queries. Code and queries are represented with bag of work tokens respectively. Two separate embedding matrices are created for code and query. For combined representation of query, simple averaging is performed. Whereas, for combined representation of code an attention based model is used for creating weighted average. Cosine similrity is used for retrieval and ranking.     SCS     Another model for code seach is Semantic Code Seach  by GitHub     CoaCor    Yao et al.  approach the problem of code retrieval with a different perspective. They elucidate the shortcomings of simple code retrieval based approach and motivate the need for understanding code semantics better. For this they talk about using code-annotation models to to generate code summaries as another view to understand the code. An markov-decision-process  is used to generate token level summary of code-snippet. The MDP is optimized using a reinforcement learning algorithm. To learn relevant transforms the training is guided by scores from code-retrieval process, this optimizes code-annotation for retrieval. CoaCor uses two separate code-retrieval models for making a joined decision. Indexing and retrieval happens on code-query pairs as well as generated summary-query pairs as well. The author's show that this additional view gives a different perspective for understanding code-semantics.  In this paper, we show that using non-binary constituency trees can be beneficial, especially in semantic similarity tasks. Moreover, we highlight the need of powerful composition function to exploit such a rich representation. To this end, we have introduced a new Tree-LSTM model which leverages tensor canonical decomposition and weight sharing to process non-binary trees without adding new parameters.  Such results pave the way to the definition of new tensor models which leverage suitable tensor decomposition to take advantage of non-binary constituency trees. To this end, the next step would be the application of other tensor decompositions. Among the others, the tensor train decomposition seems to be promising to define new composition functions which are sensitive to child nodes order.  Ultimately, we would like to test multiple tensor-based models on different NLP tasks, studying the relation between the bias introduced by each different tensor decomposition and the intrinsic property of the task.  
","   Traditionally solutions to code search were based on information retrieval techniques and natural language processing comprising of query expansion and reformulation .  expanded the query with synonyms from wordnet to search for code snippets.  API documentation was leveraged for query expansion for code snippets retrieval by the CodeHow tool proposed by .  The fundamental drawback of the above techniques is that there is a disparity in word overlap, including their synonyms, between the intent expressed in natural language and the low-level implementation details in the source code. There is a need to semantically relate the words between the two domains.   Traditionally code search is based on information retrieval techniques.  expanded the query with synonyms from wordnet to search for code snippets.  expanded the query with relevant API documentation and further applied the extended boolean model to retrieve the code snippets.  Lately deep learning techniques have been administered for understanding code semantics and structure . approaches the task of code summarization through an attention-based recurrent framework. The model generates a summary for the given code snippet which is then used to rank the relevance of the code to queries.   Deep Code Search   follows a slightly different approach. DCS takes three aspects of code namely the a) method name, b) API invocation sequence, and c) the tokens and in parallel also takes the code descriptions as inputs to a different network and learns corresponding embeddings. Similarity between the embeddings are measured using cosine similarity. However, learning between embeddings is not explicitly shared. In one of our experiments we apply a Siamese network on top of DCS to combine the embedding learning framework with a Siamese style of sharing parameters between the two sub networks.  Neural Code Search   is an unsupervised model that proposed a way to aggregate vector representation of the source code using TF-IDF weighting to form document embeddings. It uses FastText  for learning the embeddings for these bags of words.  Further, the similarity between these embeddings are used for retrieval. Embedding Unification   is an extension over NCS, applying attention on the code tokens. Again, both these works treat code and text as independent learning modules and project them in a common high dimension space.   CoaCor  proposed an Reinforcement Learning framework to generate code annotations and show the improvements on code retrieval  when combined with existing CR models like DCS . Though the objective is similar, we study Siamese for joint learning on top of DCS without generation.  Siamese Networks are popular for tasks that involve finding similarity or a relationship between two comparable artifacts. Introduced first in images , it has been applied in text domain to score relevance between a question and an answer candidate  and for learning text similarity . But to the best of our knowledge, we are not aware of any work on applying Siamese networks for semantic code search.     Our contribution:               \item Siamese based learning                The task of code retrieval entails finding and indexing germane code fragments given a query. Retrieving code with natural language query is a challenging due to the inherent difference in code and natural language semantics.          formulated a joined embedding for understanding both code and natural languages. They dub this model CodeDescriptionEmbeddin Neural Network .The embedding was learned by minimising the distance of relevant code - description pairs. The code snippets are pre-processed to extract three major components: method name as a sequence, API sequence  and bag-of-words representation. These abstractions are shown to capture relevant semantic structure present in the code. These components are individually processed through separate DNNs. Another DNN is now used to combine these representations to get a combined semantic understanding of code. A language model is additionally used to learn the semantic meaning of the description. Both learned representation of the code and language are in the same dimension. CODEnn uses cosine loss to learn and retrieve similar pairs. Cosine similarity is used as retrieval and training metric.      NCS   Cambronero et al.  capture code semantics as informal intents through continuous representations learned by Neural Code Search  model. NCS learns code objectives in an informal sense contrary to its mathematical sense. In a learned continuous vector space, related pairs of objects can be mapped closer than unrelated pairs. Token level embedding are created for both code and description using fastText, which learns embeddings in an unsupervised manner. Standard information retrieval techniques of TF-IDF is used for code search. Both code and natural language have the same embedding matrix. NCS creates combined embedding of the code bag-of-word token embedding by TF-IDF weighted averaging. Token level embedding for natural language description are combine into a single by simple averaging. TF-IDF assigns weights of code token determine their importance based on their frequency in a single document vs frequency across documents. Cosine similarity is used to rank and retrieve code.     UNIF   Cambronero et al.  improve NCS with supervision. The new model is called Embedding Unification . They introduce supervision for creation of embedding matrix for code and queries. Code and queries are represented with bag of work tokens respectively. Two separate embedding matrices are created for code and query. For combined representation of query, simple averaging is performed. Whereas, for combined representation of code an attention based model is used for creating weighted average. Cosine similrity is used for retrieval and ranking.     SCS     Another model for code seach is Semantic Code Seach  by GitHub     CoaCor    Yao et al.  approach the problem of code retrieval with a different perspective. They elucidate the shortcomings of simple code retrieval based approach and motivate the need for understanding code semantics better. For this they talk about using code-annotation models to to generate code summaries as another view to understand the code. An markov-decision-process  is used to generate token level summary of code-snippet. The MDP is optimized using a reinforcement learning algorithm. To learn relevant transforms the training is guided by scores from code-retrieval process, this optimizes code-annotation for retrieval. CoaCor uses two separate code-retrieval models for making a joined decision. Indexing and retrieval happens on code-query pairs as well as generated summary-query pairs as well. The author's show that this additional view gives a different perspective for understanding code-semantics.",179
"  We are motivated by the problem of labelling a dataset for word sense disambiguation, where we want to use a limited budget to collect annotations for a reasonable number of examples of each sense for each word.  This task can be thought of as an active learning problem , but with two nonstandard challenges. First, for any given word we can get a set of candidate labels from a knowledge base such as WordNet . However, this label set is not necessarily representative of what occurs in the data: there may exist labels in the knowledge base that do not occur in the corpus because the sense is rare in modern English;  conversely, there may also exist true labels that do not exist in our knowledge base. For example, consider the word ``bass.'' It is frequently used as a noun or modifier, e.g., ``the bass and alto are good singers'', or ``I play the bass guitar''. It is also commonly used to refer to a type of fish, but because music is so widely discussed online, the fish sense of the word is orders of magnitude less common than the low-frequency sound sense in internet text. The Oxford dictionary  also notes that bass  once referred to a fibrous material used in matting or chords, but that sense is not common in modern English. We want a method that collects balanced labels for the common senses, ``bass frequencies'' and ``bass fish'', and ignores sufficiently rare senses, such as ``fibrous material''. Second, the empirical distribution of the true labels may exhibit extreme skew: word sense usage is often power-law distributed  with frequent senses occurring orders of magnitudes more often than rare senses.    When considered individually, neither of these constraints is incompatible with existing active learning approaches:  incomplete label sets do not pose a problem for any method that relies on classifier uncertainty for exploration ; and extreme skew in label distributions has been studied under the guided learning framework wherein annotators are asked to explicitly search for examples of rare classes rather than simply label examples presented by the system .  But taken together, these constraints make standard approaches impractical. Search-based ideas from guided learning are far more sample efficient with a skewed label distribution, but they require both a mechanism through which annotators can search for examples and a correct label set because it is undesirable to ask annotators to find examples that do not actually occur in a corpus.    Our approach is as follows. We introduce a frequency threshold, , below which a sense will be deemed to be ``sufficiently rare'' % to be ignored  = p_y < \thresholdp_y\hat{p}_y$ by using importance-weighted samples. Once we have found examples of common classes, we switch to more standard active learning methods to find additional examples to reduce classifier uncertainty.  Overall, this paper makes two key contributions. First, we present an Exemplar Guided Active Learning  algorithm that offers strong empirical performance under extremely skewed label distributions by leveraging exemplar embeddings. Second, we identify a stopping rule that makes EGAL robust to misspecified label sets and prove that this robustness only imposes a logarithmic cost over a hypothetical approach that knows the correct label set.  Beyond these key contributions, we also present a new Reddit word sense disambiguation dataset, which is designed to evaluate active learning methods for highly skewed label distributions.     \paragraph{Active learning under class imbalance} The decision-boundary-seeking behavior of standard active learning methods which are driven by classifier uncertainty has a class balancing effect under moderately skew data . But, under extreme class imbalance, these methods may exhaust their labeling budgets before they ever encounter a single example of the rare classes. This issue is caused by an epistemic problem: the methods are driven by classifier uncertainty, but standard classifiers cannot be uncertain about classes that they have never observed. Guided learning methods  address this by assuming that annotators can explicitly search for rare classes using a search engine . Search may be more expensive than annotation, but the tradeoff is worthwhile under sufficient class imbalance. However, explicit search is not realistic in our setting: search engines do not provide a mechanism for searching for a particular sense of a word and we care about recovering all classes that occur in our dataset with frequency above , so searching by sampling uniformly at random would require labelling  samples\footnote{For the probability of seeing at least one example to exceed , we need at least  samples. See Lemma  for details.} to find all such classes with high probability.   Active learning with extreme class imbalance has also been studied under the ``active search'' paradigm  that seeks to find as many examples of the rare class as possible in a finite budget of time, rather than minimizing classifier uncertainty. Our approach instead separates explicit search from uncertainty minimization in two different phases of the algorithm.  \paragraph{Active learning for word sense disambiguation}  Many authors have showed that active learning is a useful tool for collecting annotated examples for the word sense disambiguation task.  \citet{chen2006empirical} showed that entropy and margin-based methods offer significant improvements over random sampling. To our knowledge, \citet{zhu-hovy-2007-active} were the first to discuss the practical aspects of highly skewed sense distributions and their effect on the active learning problems. They studied over- and under-sampling techniques which are useful once one has examples, but did not address the problem of finding initial points under extremely skewed distributions.  \citet{zhu-etal-2008-active} and \citet{dligach-palmer-2011-good} respectively share the two key observations of our paper: good initializations lead to good active learning performance and language models are useful for providing a good initialization. Our work modernizes  these earlier papers by leveraging recent advances in self-supervised learning. The strong generalization provided by large-scale pre-trained embeddings allow us to guide the initial search for rare classes with exemplar sentences which are not drawn from the training set. We also provide  stopping rules that allow our approach to be run without the need to carefully select the target label set, which makes it practical run in an automated fashion.  \citet{yuan2016semi}  also leverage embeddings but they use label propagation to nearest neighbors in embedding space. This approach is similar to ours in that it also uses self-supervised learning, but we have access to ground truth through the labelling oracle which offers some protection against the possibility that senses are poorly clustered in embedding space.    \paragraph{Pre-trained representations for downstream NLP tasks}  There are a large number of recent papers showing that combining extremely large datasets with large Transformer models  and training them on simple sequence prediction objectives leads to contextual embeddings that are very useful for a variety of downstream tasks. In this paper we use contextual embeddings from BERT  but because the only property we leverage is the fact that the contextual embeddings provide a useful notion of distance between word senses, the techniques described are compatible with any of the recent contextual models \citep[e.g.][]{radford2019language, 2019t5}.                                              In this section, we analyze the results obtained above to understand the behavior of the DCS-Siamese network. We focus on this architecture since it outperforms all other architectures and baseline models considered. Specifically, we would like to analyze three observations:  A. Regularization effect of the DCS-Siamese model over the original DCS architecture  We visualize the embeddings learnt by the DCS network  and the output of the DCS extraction network, after using the Siamese network, for the text descriptions in the StaQC SQL dataset using t-SNE  in Figure . We consider the SQL dataset for visualization since the raw queries and code snippets are not available for the Java dataset.   A quick examination reveals that the embedding space has sharp, distinct clusters for the DCS-Siamese network , whereas the clusters in the original DCS network  are relatively smaller and more scattered. Further, we manually examined some of the clusters and evaluated the questions that are mapped to those clusters. Few samples are listed in Table . For the query groups DATE and JOIN, the clusters are scattered in different regions for the original DCS network. Also, the cluster corresponding to the MAX query group is adjacent to the DATE cluster. Comparatively, for the same query groups, the clusters for the DCS-Siamese network are well separated and coherent. This highlights the role of Siamese network as a regularizer when applied on top of the DCS network. The Siamese network seemingly helps in rearranging the embedding space leading to more meaningful representations, bringing similar inputs  closer in the embedding space. This effect is further reflected in the better retrieval MRR of the DCS-Siamese network.     We observe this result for both the datasets. In our experiments, the difference between the results of  and  variesd but we observe a clear trend in favor of . To understand the superior performance of , we visualize the embeddings learnt by the DCS-Siamese network at the DCS layer for two architectures with  and  respectively as shown in Figure . We consider the SQL dataset for visualization since the raw queries and code snippets are not available for the Java dataset. We use tSNE to plot the DCS embeddings for . A quick examination reveals that the embedding space has sharp, distinct clusters when , whereas the clusters when  are relatively diffused. Further, we manually examined some of the clusters and evaluated the questions that are mapped to those clusters. Some samples are listed in Table . Apart from the fact that the clusters in the right figure are smaller for the four sets of queries we examined, the ones on the left blend in with the other points in the figure, implying that the network has done a poor job at learning to distinguish between different queries at the DCS layer when . Its is unsurprising that we achieve better MRR when Code retrieval is performed at the DCS layer for .    This hints at the possibility that the narrow funnel in the network caused by having  output units at the top of the Siamese network act as a regualarizer that forces the lower layers to learn more meaningful embeddings, which in turn helps the overall task when using those embeddings for retrieval. We did observe 1 exception to this when we evaluated on the Siamese layer output with . However, the difference in performance was only marginal. This visualization, coupled with the results in Table  clearly establishes the value of the  DCS-Siamese network.     We have also observed the inverse of this regularization effect for other values of  and the model performance gradually degrades as the value of  increases.     This also explains why the clusters corresponding to a given set of similar queries are extremely well defined for the embeddings at the DCS layer than the embeddings at the top layer of the network. The restriction to compact information into 2 dimensions has led to a loss of information in the 2-d embedding space for a given set of queries, but this has led the DCS layer to learn a rich set of embeddings.    figure out how to show and pitch the scatter plot of the correct-wrong pairs of points, if at all needed    If this argument indeed holds, we would see a gradual loss of information as we look at the embeddings at the other intermediate layers of the network, upto the final layer. We visualize the embeddings of the layer between the DCS output and the final layer using tSNE for the same set of queries in table X. Indeed, we see that the cluster representing the queries in the embedding space of the intermediate layer is somewhat scattered, but not as much as that of the final layer.       B. For the DCS-Siamese network, retrieval on the output of DCS layer achieves higher MRR  We now focus on the actual embeddings learnt at the different layers of the DCS-Siamese network with .  Figure  shows the embedding plots of the final Siamese layer  and the output of the DCS layer . We focus on two specific set of queries shown in Table . We believe having  output units results in a much stronger regularization effect leading to these two sets of questions being mapped to well-defined regions in the embedding space.    As discussed earlier, the regularization effect of having  output units results in much better embedding at the DCS layer resulting in these  sets of questions being mapped to well-defined regions in the embedding space.  However, due to the low dimensionality at the final layer, there is a tremendous loss of information that deprives the layer of any meaning to its representations. The purpose of the representations is to simply reduce the loss function and guide the gradient forcing the lower layers to learn a much more meaningful embedding. The actual meaning to representations of code and text is hence obtained at a lower layer.  This effect is also consistent for embeddings of code, as observed in Figure , where we generated   a plot of the embeddings of the SQL queries corresponding to the questions in Table . In comparison, to the DCS layer embedding of the DSC-Siamese network , we observed more than one clusters for the sorting questions .      This is due to the fact that any question that involves a deletion would more-or-less be always translated to a 'DELETE FROM' clause in code . However, there could be several questions that might not explicitly ask about sorting, but still require sorting as an intermediate step in the answer. The code corresponding to such answers would have an 'ORDER BY' clause, but depending on the actual question, might involve other SQL clauses.  To summarize,  has a far stronger regularizing effect on the network as compared to larger values. However, due to this effect, there seems to be a loss of information in the final layers of the Siamese network and hence, the embeddings learned by the lower layers of the network contain richer information for the Code retrieval task.  C. The DCS extraction network greatly outperforms the other extraction networks when combined with Siamese networks  We selected the DCS setup as an extraction network because it leveraged a variety of features from code. Although we believe these features are collectively responsible for the impressive performance of the DCS-Siamese model, when considered individually with a Siamese network, they are unable to provide enough information during training, leading to an extremely poor model. This hints at the possibility that providing code as input to a deep learning network may not be straightforward and although the DCS features worked well, there are possibly other features that need to be discovered. Also, some code features might be useful for certain datasets, but not for all.     once you hav explained select and find/get/fetch are far apart, give a solution how to fix that   This result is surprising since [togther we stand paper] has reported impressive results using siamese networks with simple preprocessing and a conv-pooling-relu-FC network. However, when applied to code retrieval, siamese networks with different model architectures and embeddings sizes do not perform as well as other models such as [dcs][coacor] that rely on extracting multiple features from code. We hypothesize that this is due to different vocabularies as well as different mearnings of the same terms in code and text. For instance, the question and answer pairs of [together we stanbd paper] come from the same language , even though the distributions of these terms in questions and answers might be different.    actually, we need to think more on this and come up with more convincing experiemnts and results      Using an ablation study, we identify that API sequence tokens provide the highest performance of all the features used by the DCS model also mention this in training/model details of the simple models. \section{Conclusion and Future Work} Siamese networks can achieve impressive performance on Code Retrieval tasks by learning a meaningful embedding of code and it's description text. This performance is heavily reliant on an appropriate representation of code and we have observed that the DCS architecture can achieve this reasonably well. However, while we have some understanding of the regularization provided by the Siamese network, we would like to study this effect in more detail as future work. We would also like to validate our observations on more datasets and other tasks involving code and natural language text such as Code Summarization and Code Synthesis.    File acl2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{acl2020} \usepackage{times} \usepackage{latexsym} \usepackage{tikz} \usepackage{multirow} \renewcommand{\UrlFont}{\ttfamily\small}    This is not strictly necessary, and may be commented out,   but it will improve the layout of the manuscript,   and will typically save some space. \usepackage{microtype}   \aclfinalcopy   Uncomment this line for the final submission   \def\aclpaperid{***}    Enter the acl Paper ID here   \setlength\titlebox{5cm}   You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.  \newcommand\BibTeX{B\TeX}  \title{Evaluation of Siamese Networks for Semantic Code Search}     \author{}  \author{Raunak Sinha \\   IBM Research \\   \texttt{rsinha05@in.ibm.com} \\\And   Utkarsh Desai \\   IBM Research \\   \texttt{udesai26@in.ibm.com} \\\And   Srikanth Tamilselvam \\   IBM Research \\   \texttt{srikanth.tamilselvam@in.ibm.com} \\\And   Senthil Mani  \date{}                   
","  \paragraph{Active learning under class imbalance} The decision-boundary-seeking behavior of standard active learning methods which are driven by classifier uncertainty has a class balancing effect under moderately skew data . But, under extreme class imbalance, these methods may exhaust their labeling budgets before they ever encounter a single example of the rare classes. This issue is caused by an epistemic problem: the methods are driven by classifier uncertainty, but standard classifiers cannot be uncertain about classes that they have never observed. Guided learning methods  address this by assuming that annotators can explicitly search for rare classes using a search engine . Search may be more expensive than annotation, but the tradeoff is worthwhile under sufficient class imbalance. However, explicit search is not realistic in our setting: search engines do not provide a mechanism for searching for a particular sense of a word and we care about recovering all classes that occur in our dataset with frequency above , so searching by sampling uniformly at random would require labelling  samples\footnote{For the probability of seeing at least one example to exceed , we need at least  samples. See Lemma  for details.} to find all such classes with high probability.   Active learning with extreme class imbalance has also been studied under the ``active search'' paradigm  that seeks to find as many examples of the rare class as possible in a finite budget of time, rather than minimizing classifier uncertainty. Our approach instead separates explicit search from uncertainty minimization in two different phases of the algorithm.  \paragraph{Active learning for word sense disambiguation}  Many authors have showed that active learning is a useful tool for collecting annotated examples for the word sense disambiguation task.  \citet{chen2006empirical} showed that entropy and margin-based methods offer significant improvements over random sampling. To our knowledge, \citet{zhu-hovy-2007-active} were the first to discuss the practical aspects of highly skewed sense distributions and their effect on the active learning problems. They studied over- and under-sampling techniques which are useful once one has examples, but did not address the problem of finding initial points under extremely skewed distributions.  \citet{zhu-etal-2008-active} and \citet{dligach-palmer-2011-good} respectively share the two key observations of our paper: good initializations lead to good active learning performance and language models are useful for providing a good initialization. Our work modernizes  these earlier papers by leveraging recent advances in self-supervised learning. The strong generalization provided by large-scale pre-trained embeddings allow us to guide the initial search for rare classes with exemplar sentences which are not drawn from the training set. We also provide  stopping rules that allow our approach to be run without the need to carefully select the target label set, which makes it practical run in an automated fashion.  \citet{yuan2016semi}  also leverage embeddings but they use label propagation to nearest neighbors in embedding space. This approach is similar to ours in that it also uses self-supervised learning, but we have access to ground truth through the labelling oracle which offers some protection against the possibility that senses are poorly clustered in embedding space.    \paragraph{Pre-trained representations for downstream NLP tasks}  There are a large number of recent papers showing that combining extremely large datasets with large Transformer models  and training them on simple sequence prediction objectives leads to contextual embeddings that are very useful for a variety of downstream tasks. In this paper we use contextual embeddings from BERT  but because the only property we leverage is the fact that the contextual embeddings provide a useful notion of distance between word senses, the techniques described are compatible with any of the recent contextual models \citep[e.g.][]{radford2019language, 2019t5}.",180
"  {M}{usic} composition is a human creative process that requires a wide range of strong musical knowledge and expertise to create soothing music which continues to remain in our heart forever. Given the vast majority of music lovers and the limited availability of professional music composers, there is a strong need for machines to assist human creativity. Recent advancement in the software based music creation technology helped the professional and amateur music creators to produce music with great joy and ease of production in masses to be consumed by the music consumers with personal computers and hand-held devices. %The software applications such as Ableton Live, FL Studio, Logic Pro X, Garageband are the few examples which changed the way the music is produced in the past.  Though there exists a plenty of machine assistance to create high quality music with relative ease of production, the process of songwriting that is automatically generating lyrics, composing melody corresponding to the generated lyrics and synthesizing singing voice corresponding to the generated melody and lyrics remained as mutually exclusive tasks. Till date, the construction of novel/original songs is limited to the individuals who possess the following skills: the ability to create lyrics, compose melody and combine lyrics and melody to create a rational, relevant and soothing final complete songs. %Though by remixing technology, we can create new music to some extent which satisfies some music lovers, there is a need for creating truly novel songs under multiple constraints on remaking existing works.                       % -------------------------------------------------------------------------------------------------------------             In literature, we can find considerable amount of research work published on automatic music generation . Early machine assisted music generation is mostly based on music theory and expert domain knowledge to create novel works. With the advent of data driven approaches and exploded public music collections in the internet, data driven methods such as Hidden Markov models, graphic models and deep learning models showed a potential for music creation. Though there exists substantial amount of research on unconditional music generation, there exists considerably less amount of work done so far on generating melody from lyrics given in the form of text, which we call conditional melody/song generation from lyrics. The primary reasons for substantially less research on conditional melody generation can be attributed to i) the non-availability of the direct source for lyrics-melody pair dataset to train the data driven models, ii) a lyrics composition can have multiple melodic representations, which makes it hard to learn the correlation between the lyrics and melodies, and iii) it is hard to evaluate the generated melodies by objective measures.  This paper focuses on the most challenging aspect of algorithmic songwriting process which enables the human community to discover original lyrics, and  melodies suitable for the generated lyrics. To the best our knowledge, the proposed AutoNLMC is the first attempt to make the whole process of songwriting automatic using artificial neural networks. We also present the lyrics to vector model which is trained on a large dataset of popular English songs to obtain the dense representation of lyrics at syllables, words and sentence levels. The proposed AutoNLMC is an attention based encoder-decoder sequential recurrent neural network model consists of a lyric generator, lyric encoder and melody decoders trained end-to-end. We train several encoder-decoder models on various dense representations of the lyric tokens to learn the correlation between lyrics and corresponding melodies. Further, we prove the importance of dense representation of lyrics by various qualitative and quantitative measures. AutoNLMC is designed in such a way that it can generate both lyrics and corresponding melodies automatically for an amateur or a person without music knowledge by accepting a small piece of initial seed lyrics as input. It can also take lyrics from professional lyrics writer to generate the matching meaningful melodies.      Here, we briefly present closely related work on lyrics-conditional music generation frameworks developed by researchers in  past years. Orpheus is a dynamic programming based melody composition algorithm for Japanese lyrics. Orpheus is designed as an optimal melody search problem for the given lyrics under the prosodic constraints of the Japanese lyrics. The authors design two individual models such as rhythm and probabilistic pitch inference model to generate melodies from the given lyrics. A Finish song generating system called Sucus-Apparatusf is developed by Toivanen et al.. Sucus-Apparatusf is designed to randomly choose rhythm from the rhythm patterns actually found in the Finish art songs. Further, a second order Markov model is designed to generate the chord progression for the given lyrics. The pitches are generated from the joint probabilistic distribution of previously generated notes and the chords. In, the authors propose a system for automatically generating melodic accompaniments from a given lyrical text. The system is designed to generate the pitches modeled as n-gram models from the melodies of songs with similar style. Further, the rhythm for the melodic accompaniment is derived from the cadence information present in the text. The proposed method generated hundreds of melodies by giving random options driven by set of rules, followed by selecting among the generated options with an objective measure that incorporates expert music knowledge. Both melody and rhythm are generated by the same process for a given lyrics. Rhythm suggestion from lyrics by using set of rules is studied by Nichols while Oliveira proposed the inverse process of lyrics generation from the rhythm of melody.  A songwriting system called SMUG is proposed by Scirea et al. It uses academic papers to compose both lyrics and the corresponding melodies. SMUG utilizes Markov models to generate lyrics and melodies. ALYSIA is the first fully data driven model based on random forests to generate melody from the lyrics. ALYSIA is trained on a large set of features manually extracted from Music-XML files. ALYSIA is designed to suggest multiple melodies as output for a given lyrical piece thus giving user the ability to choose more pleasing melody for the given lyrics. ALYSIA consists of two independent melody prediction models to predict duration of the note and scale of the note independently. An encoder-decoder based RNN sequential model for lyrics-conditional melody generation for Chinese pop songs is presented in. The sequential model called Songwriter consists of two encoders and one hierarchical decoder. The encoders are designed to encode the lyric syllables and context melody of the prior generated melody. The hierarchical decoder is designed to decode the note attributes such as pitch, duration and syllable-note alignment labels since most of the syllables in Chinese songs had more than one note.         -------------------------------------------------------------------------------------------------------------     In this paper, we propose a joint enhancement and speech transformer training method with gated recurrent fusion for robust end-to-end speech recognition. The joint training compositional scheme is used to simultaneously optimize the enhancement and speech recognition. In addition, in order to address the speech distortion problem and extract more robust features for end-to-end ASR, we apply the gated recurrent fusion algorithm to combine the noisy and enhanced features. Experiments on Mandarin AISHELL-1 demonstrate that our proposed method is effective for the robust end-to-end ASR and can solve the speech distortion problem very well. In future, we will explore the time domain speech enhancement to acquire a better enhanced speech and obtain greater performance improvement for our proposed method.  In this paper, we propose a jointly traning of enhancement and speech transformer to imporove robustness of end-to-end systems. We use a jointly compositional scheme of enhancement and recognition. In addition, in order to alleviate the speech distortion problem and extract more robust features for ASR, we propose the deep attention fusion algorithm to combine the noisy and enhanced features.  Experiments on AISHELL-1 demonstrate effectiveness of our proposed method. In future, we will explore the time domain speech enhancement to acquire a better enhanced speech and obtain greater performance improvement for our proposed method.    
","   Here, we briefly present closely related work on lyrics-conditional music generation frameworks developed by researchers in  past years. Orpheus is a dynamic programming based melody composition algorithm for Japanese lyrics. Orpheus is designed as an optimal melody search problem for the given lyrics under the prosodic constraints of the Japanese lyrics. The authors design two individual models such as rhythm and probabilistic pitch inference model to generate melodies from the given lyrics. A Finish song generating system called Sucus-Apparatusf is developed by Toivanen et al.. Sucus-Apparatusf is designed to randomly choose rhythm from the rhythm patterns actually found in the Finish art songs. Further, a second order Markov model is designed to generate the chord progression for the given lyrics. The pitches are generated from the joint probabilistic distribution of previously generated notes and the chords. In, the authors propose a system for automatically generating melodic accompaniments from a given lyrical text. The system is designed to generate the pitches modeled as n-gram models from the melodies of songs with similar style. Further, the rhythm for the melodic accompaniment is derived from the cadence information present in the text. The proposed method generated hundreds of melodies by giving random options driven by set of rules, followed by selecting among the generated options with an objective measure that incorporates expert music knowledge. Both melody and rhythm are generated by the same process for a given lyrics. Rhythm suggestion from lyrics by using set of rules is studied by Nichols while Oliveira proposed the inverse process of lyrics generation from the rhythm of melody.  A songwriting system called SMUG is proposed by Scirea et al. It uses academic papers to compose both lyrics and the corresponding melodies. SMUG utilizes Markov models to generate lyrics and melodies. ALYSIA is the first fully data driven model based on random forests to generate melody from the lyrics. ALYSIA is trained on a large set of features manually extracted from Music-XML files. ALYSIA is designed to suggest multiple melodies as output for a given lyrical piece thus giving user the ability to choose more pleasing melody for the given lyrics. ALYSIA consists of two independent melody prediction models to predict duration of the note and scale of the note independently. An encoder-decoder based RNN sequential model for lyrics-conditional melody generation for Chinese pop songs is presented in. The sequential model called Songwriter consists of two encoders and one hierarchical decoder. The encoders are designed to encode the lyric syllables and context melody of the prior generated melody. The hierarchical decoder is designed to decode the note attributes such as pitch, duration and syllable-note alignment labels since most of the syllables in Chinese songs had more than one note.         -------------------------------------------------------------------------------------------------------------",181
"   Deep Neural Networks  are the current state-of-the-art models in many speech related tasks. From a computational neuroscience perspective, DNNs can be seen as rate coding based models, in the sense that if a neuron is responsive to a given stimulus, then if we augment the stimulus intensity, the neuron output intensity will also increase. Temporal coding based models try to also take into account information carried by the temporal structure of the stimulus. In the case of Spiking Neural Networks , spike timing and delays between spikes is important in order to retrieve patterns in the spike sequences given as input to a model. %https://en.wikipedia.org/wiki/Neural_coding  There is a growing interest for SNNs applied to speech recognition tasks, from isolated word and phone recognition,to large-vocabulary automatic speech recognition  very recently. Reasons are that the audio speech signal is particularly suited to event-driven models such as SNNs, SNNs are also more biologically realistic than DNNs, hardware friendly and energy efficient models, if implemented on dedicated energy-efficient neuromorphic chips. Furthermore, it has been shown recently that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. This new approach allows to train SNNs as one would do for DNNs.  In this work, we propose to use supervised SNNs for speech command  recognition. We explore the Leaky Integrate-and-Fire  neuron model for this task, and show that convolutional SNNs can reach an accuracy very close to the one obtained with state-of-the-art DNNs, for this task. Our main contributions are the following: i) we propose to use dilated convolution spiking layers, ii) we define a new regularization term to penalize the averaged number of spikes to keep the spiking neuron activity as sparse as possible, iii) we show that the leaky variant of the neuron model outperforms the non-leaky one , used in.  In order to facilitate reproducibility, our code using PyTorch is available online\footnote{https://github.com/romainzimmer/s2net}.     In, the authors based their SNNs on a non-leaky variant of IF models. In this case, solving Eq. and with our notations, Eq. boils down to:    We will report results using this NLIF formulation in Section.    In this work, we applied IRM to a toxicity classification task in order to demonstrate that Domain Generalization can serve as an important framework for building fair machine learning classifiers. Our findings show that IRM outperforms ERM with respect to both generalization accuracy and group fairness by learning invariant but likely non-causal predictors of toxicity. We hope that these results are first steps for future explorations of the relationship between robustness and fairness in machine learning.    {\small     }    
","  In, the authors based their SNNs on a non-leaky variant of IF models. In this case, solving Eq. and with our notations, Eq. boils down to:    We will report results using this NLIF formulation in Section.",182
" Books have been the one of the most important mediums for recording information and imparting knowledge in human history. Books can be classified into different categories based on their physical formats, contents, languages, and so on. In this paper, we focus on the task of book classification by its genre using the information provided just by the cover. Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Figure  presents some sample book covers. The information provided by a cover includes visual and textual information . For instance, in Figure 1, the background picture contains different food items and cookware which give the readers a visual impression about the book, while the texts shown on the cover states that it is a book about the ``authentic recipes from Malaysia"". Both the visual and textual information are shown in the cover and they together indicate that its genre is ``Cookbooks, Food \& Wine"". It is worth to mention that having only the visual information often makes the task extremely hard without textual information. For instance, in Figure 1 , without reading the texts on the cover, someone may classify the book as ``Cookbooks, Food \& Wine"" as well solely based on the visual information we get from the cover that includes food items on a table in a dining room setting. Therefore, it is sometimes essential to consider both visual information and textual information extracted from the cover when we conduct book genre classification. The automatic classification of books based on only covers without human intervention would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task.     The challenges of this task are the following. First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, varies in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc . To overcome these difficulties, we present a deep learning framework involving two moralities: one for visual information and the other for textual information extracted from the covers.   Recently, deep learning approaches have reached high performances across a wide variety of problems . In particular, some deep convolutional neural networks can achieve a satisfactory level of performance on many visual recognition and categorization tasks, exceeding human performances. One of the most attractive qualities of these techniques is that they can perform well without any external hand-designed resources or task-specific feature engineering.  The theoretical foundations of deep learning are well rooted in the classical neural network  literature. It involves many hidden neurons and layers as an architectural advantage in addition to the input and output layers . A deep convolutional neural network is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough .  The main contributions of this paper are fourfold:   The rest of the paper is structured as follows. Section 2 presents related works about book cover classification. Section 3 elaborates on the details of the proposed multi-modal architectures. In section 4, we discuss the experimental results. The last section concludes the paper and discusses future work.    In recent years, there has been an increasing interest in automated genre classification based on images by leveraging the strength of the deep neural network. Traditional machine learning methods often focus on feature engineering which extracts features using domain knowledge which are then used in the designed learning algorithm, while deep learning attempts to automatically learn features in large datasets through adjusting internal parameters using a backpropagation algorithm . There are multiple attempts to classify movie genre based on its poster with deep neural network . Convolutional neural networks have been used to categorize the genre of paintings and artworks . Similar works have been done in music genre classification as well .  In book genre classification, Chiang et al.  is the first attempt aimed to tackle this particular problem to the best of our knowledge. They implemented transfer learning with convolutional neural networks on the cover image along with natural language processing on the title text. A data set consisting of 6000 book covers from five genres obtained from OpenLibrary.org were utilized for their study. Iwana et al.  attempted to conduct book genre classification using only the visual clues provided by its cover. To solve this task, they created a large dataset consisting of 57,000 samples from 30 genres and adapted AlexNet  pre-trained on ImageNet . They achieved an overall accuracy rate of 24.7\  and 40.3\  for Top 1 and Top 3 predictions respectively.  Buczkowski et al.  created another dataset consisting of 160k book covers crawled from GoodReads.com from over 500 genres, from which they picked the top 13 most popular genres and grouped all the remaining books under a 14th class called ``Others"".  The authors used two different convolutional neural networks to predict book genres. One is relatively simple and shallow, with three convolutional layers, each followed directly by a   max-pooling layer with non-overlapping windows. The other one adopted a more sophisticated architecture similar to the VGG network , with blocks of consecutive convolutional layers together with dropout layers. The authors achieved an accuracy of 67\  using the more complex network and an accuracy of 74\  using the simpler network.  It is worth to note that the accuracy measure used in this study is not the traditional multi-class classification accuracy but a weighted score calculated based on the top 3 predicted genres. In , the authors utilized a multinomial logistic regression model to classify book genres based on extracted image features and title features. Their approach consists of three stages: image feature extraction using the Xception model , title feature extraction using the GloVe Model , and classification based on the combined extracted features. Their study was based on a dataset consisting of 6076 samples from Amazon.com belonging to five genres. The authors achieved an overall accuracy of 87\  when they used the combined image and title features. Lucieri et al.  aimed to benchmark deep learning models for classification of book covers. The authors used the same dataset introduced by Iwana et al. . They provided a detailed evaluation of the state-of-the-art classification models for the task of book cover classification in an attempt to establish a benchmark on this problem. They employed the most powerful image recognition models such as NASNet, Inception ResNet v2, ResNet-50, etc. They provided a thorough analysis of the dataset and proposed a cleansed dataset by removing the book in the genre Reference and merging the genre Christian Books \& Bibles with the genre Religion \& Spirituality, by which they obtained a 28-category subset consisting of 55,100 samples. The authors also incorporated the title information available from the dataset and the text-image model yields the highest accuracy of 55.7\ .      In this work, we explored the LIF neuron model to define dilated convolution spiking layers for a spoken command recognition application. Contrarily to most works using SNNs applied to speech tasks, in which special mechanisms, usually non-trainable, are needed to first encode the speech input features into some type of neural encoding  as a first step to then use SNNs , our approach is unified in the sense that the first convolution layer applied to real-valued speech features is trainable and shares the same definition and implementation than the ones processing spike trains as input. Our proposed SNN, trained with back-propagation through time with surrogate gradient, achieved results competitive with standard deep convolutional neural networks.     We defined a regularization term to penalize the averaged number of spikes to keep the spiking neuron activity as sparse as possible, which is a desirable property both from a biological point of view and for a future potential implementation on low-energy dedicated chips.  Finally, we conducted ablation studies in order to estimate the impact of different components of our approach. In particular, an interesting result is that the LIF neuron model outperformed the simpler non-leaky one , used in for ASR.   Another experiment showed that learning the values for the thresholds and leak coefficients during training does not bring accuracy improvements over using defaults constant values.  In future work, we will try to confirm these results in acoustic modeling for speech recognition.   We also would like to explore the possibility to design a variant, in which a layer sends its output spikes to the next layer as soon as they are produced, in a single time loop used for the whole model. This would be more efficient in terms of computation load. It would also eventually allow to take classification decisions faster, for audio streaming applications in particular.    Below is an example of how to insert images. Delete the ``\vspace'' line,   uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''   with a suitable PostScript file name.   -------------------------------------------------------------------------        To start a new column  and help balance the last-page   column length use \vfill\pagebreak.   -------------------------------------------------------------------------  \vfill  \pagebreak    
"," In recent years, there has been an increasing interest in automated genre classification based on images by leveraging the strength of the deep neural network. Traditional machine learning methods often focus on feature engineering which extracts features using domain knowledge which are then used in the designed learning algorithm, while deep learning attempts to automatically learn features in large datasets through adjusting internal parameters using a backpropagation algorithm . There are multiple attempts to classify movie genre based on its poster with deep neural network . Convolutional neural networks have been used to categorize the genre of paintings and artworks . Similar works have been done in music genre classification as well .  In book genre classification, Chiang et al.  is the first attempt aimed to tackle this particular problem to the best of our knowledge. They implemented transfer learning with convolutional neural networks on the cover image along with natural language processing on the title text. A data set consisting of 6000 book covers from five genres obtained from OpenLibrary.org were utilized for their study. Iwana et al.  attempted to conduct book genre classification using only the visual clues provided by its cover. To solve this task, they created a large dataset consisting of 57,000 samples from 30 genres and adapted AlexNet  pre-trained on ImageNet . They achieved an overall accuracy rate of 24.7\  and 40.3\  for Top 1 and Top 3 predictions respectively.  Buczkowski et al.  created another dataset consisting of 160k book covers crawled from GoodReads.com from over 500 genres, from which they picked the top 13 most popular genres and grouped all the remaining books under a 14th class called ``Others"".  The authors used two different convolutional neural networks to predict book genres. One is relatively simple and shallow, with three convolutional layers, each followed directly by a   max-pooling layer with non-overlapping windows. The other one adopted a more sophisticated architecture similar to the VGG network , with blocks of consecutive convolutional layers together with dropout layers. The authors achieved an accuracy of 67\  using the more complex network and an accuracy of 74\  using the simpler network.  It is worth to note that the accuracy measure used in this study is not the traditional multi-class classification accuracy but a weighted score calculated based on the top 3 predicted genres. In , the authors utilized a multinomial logistic regression model to classify book genres based on extracted image features and title features. Their approach consists of three stages: image feature extraction using the Xception model , title feature extraction using the GloVe Model , and classification based on the combined extracted features. Their study was based on a dataset consisting of 6076 samples from Amazon.com belonging to five genres. The authors achieved an overall accuracy of 87\  when they used the combined image and title features. Lucieri et al.  aimed to benchmark deep learning models for classification of book covers. The authors used the same dataset introduced by Iwana et al. . They provided a detailed evaluation of the state-of-the-art classification models for the task of book cover classification in an attempt to establish a benchmark on this problem. They employed the most powerful image recognition models such as NASNet, Inception ResNet v2, ResNet-50, etc. They provided a thorough analysis of the dataset and proposed a cleansed dataset by removing the book in the genre Reference and merging the genre Christian Books \& Bibles with the genre Religion \& Spirituality, by which they obtained a 28-category subset consisting of 55,100 samples. The authors also incorporated the title information available from the dataset and the text-image model yields the highest accuracy of 55.7\ .",183
"  In grounded language theory, the semantics of language are given by how symbols connect to the underlying real world---the so-called ``symbol grounding problem''. For example, we want a robotic system that sees an eggplant  to ground the recognition object to a canonical symbol for `eggplant.' When a user asks ""Please grab me the eggplant,"" the robot should ground the natural language word ""eggplant"" to the same symbol that denotes the relevant visual percepts. Once both language and vision successfully ground to the same symbol, it becomes feasible for the robot to complete the task. We learn this connection by using physical sensors in conjunction with language learning: paired language and perceptual data are used to train a joint model of how linguistic constructs apply to the perceivable world.   Machine learning of grounded language often demands large-scale natural language annotations of things in the world, which can be expensive and impractical to obtain. It is not feasible to build a dataset that encompasses every object and possible linguistic description. Novel environments will require symbol grounding to occur in real time, based on inputs from a human interactor. Learning the meanings of language from unstructured communication with people is an attractive approach, but requires fast, accurate learning of new concepts, as people are unlikely to spend hours manually annotating even a few hundred samples, let alone the thousands or millions commonly required for machine learning.  % Active learning, in which a system queries for specific training data, has the potential to improve learning efficiency and reduce the number of labels required to learn a grounded language model.  In this work we study active learning, in which a system deliberately seeks information that will lead to improved understanding with less data, to minimize the number of samples/human interactions required. The field of active learning typically assumes that a pool of unlabeled samples is available, and the model can request specific example that it would like to obtain a label for. By having the model select the most informative data points for labeling, the number of samples that need to be labeled is reduced. This maps to the goal of human-robot learning with minimum training data provided by the human. Furthermore, active learning can be part of a pipeline with other few-shot learning methods.   However, active learning is not a magic bullet. When not carefully applied, it does not outperform sequential or random sampling baselines. Thoughtful selection of suitable approaches for problems is required. While active learning has been used for language grounding %, , to the best of our knowledge, we present the first broad exploration of the best methods for active learning for grounding vision-language pairs. %  In this paper, our focus is on developing guidelines by which active learning methods might be appropriately selected and applied to vision-language grounding problems. We test different active learning approaches on grounded language problems of varying linguistic and sensory complexity, and use our results to drive a discussion of how to select active learning methods for different grounded language data acquisition problems in an informed way.  We consider the grounded language task of learning novel language about previously unseen object types and characteristics. Our emphasis is on determining what methods can reduce the amount of training data needed to achieve performance consistent with human evaluation. Primarily, we address five relevant questions concerning characteristic-based grounded language learning:  % We make conclusions with respect to these questions in \cref{sec:results}. % In addition to addressing the above research questions, we verify how generalizable these learning techniques are beyond characteristic-based grounding.    We find that a right ordering of training data makes it possible to learn successfully from significantly fewer descriptions in most cases, but also that the active learning methodology chosen is specific to the nature of the learning problem. Our main contribution is a principled analysis of using active learning methods as unsupervised data sampling techniques in language grounding with a discussion of what aspects of those problems are relevant to approach selection. While our contributions are primarily analytic rather than algorithmic, we argue they address a critical need within grounded language understanding, an active research area in which questions of efficiency and data collection are widespread, and have the potential to support additional algorithmic developments.     Grounded language learning has been successful in learning to follow directions, generating referring expressions, visual storytelling, video grounding and understanding commands, among others. Parsing can be grounded in a robot's world and action models, taking into account perceptual and grounding uncertainty.  or language ambiguity.  The problem space considered in this paper assumes that there are no pre-existing models of language or objects in the world---an agent is learning from novel language about previously unseen objects, making the evaluation more broadly applicable.   Active learning has been applied successfully to a number of problems, providing performance improvements in areas as diverse as learning from demonstration, following directions, and learning about object characteristics. A well-chosen active learning approach can reduce the number of labels required for grounded language learning, but raises questions of what queries to ask and when to ask them.   Advances in active learning techniques have improved the ability to find the most useful data points. Unsupervised learning techniques, such as subspace clustering, have been shown to find influential points from a cluster. A hybrid method that connects active learning and data programming has shown improvements in the reduction of noisy data in large scale workspaces. Similar to our work, active learning approaches have been effective while training biased and highly varied datasets. Similarly, researchers have put effort into utilizing different active learning methods depends on the complexity of the problem. Also, traditional active learning methods have helped to improve performances in other tasks, such as data fault or fake news detection. Though we consider efficiency over time complexity, researchers have studied methods that are time efficient, especially in large scale applications. Similar to our research,~ also compares two traditional active learning algorithms for selecting important points from a pool of training data. But we also consider distinct machine learning approaches with small scale and large scale datasets in our comparisons. Various Bayesian techniques have been used in selecting diverse points as the most influential is widely popular, and we use different variants of  DPP to select distinct data points as our active learning technique in batch sample selection.  In this work, our goal is to perform a principled exploration of selecting what data to query for labeling, using informativeness and uncertainty metrics in grounded language problems of varying complexity. We draw on existing techniques, particularly pool-based learning, uncertainty sampling   .  , and probabilistic sample selection.  We take advantage of that body of research to select our set of experimental approaches, which include sample selection via Gaussian mixture models and Determinantal Point Processes , which have proven effective in modeling diversity. Using supervised learners as the active learning techniques are not suitable for our current study since we concentrate on building a language model without prior knowledge.   Our work is most closely related to that of Thomason et al.~, who incorporate `opportunistic' active learning in a system that learns language in an unstructured environment. However, that work focuses on opportunistically querying for labels whenever annotators are present; this work, in contrast, is focused on exploring the best way of selecting good choices from a large range of possible queries, reflecting the assumption that opportunities to query users will often be severely limited.           \vskip 0.15in  As demonstrated by the competitive results obtained on the 5 data sets evaluated, especially those in Tables 1 and 3 for the deep neural network CNN and Tables 4 and 5 for the shallow neural network FC-NN, the hyper-sinh is deemed a suitable activation function that scales from shallow to deep neural networks. \\ In fact, its accuracy and reliability was high across both sets of benchmark image- and text-based data sets, as quantified via appropriate metrics in sub-section 2.4, and better than some gold standard functions, e.g., considering Table 1 with the accuracy and the F1-score of the CNN using hyper-sinh being 0.70 and 0.69 respectively on the CIFAR-10 image-based data set, as opposed to that of the same CNN but using sigmoid being 0.10 and 0.02 respectively. Moreover, its accuracy and reliability were comparable to the FC-NN using ReLU , with higher reliability than the same FC-NN when leveraging the sigmoid function on the 'Reuters' text-based data set . The proposed hyper-sinh also led to increased precision on the 'IMDB' text-based data set  as opposed to sigmoid and tanh , when using the same FC-NN as that leveraged to classify the 'Reuters' data set. \\ Therefore, the hyper-sinh demonstrates that it is possible to extend the m-arcsinh to generalise across both shallow and deep neural networks for image and text classification tasks, and that the mathematical formulation of this extended function does not have to be complex at all. As an accurate and reliable activation function, the hyper-sinh is thus deemed a new gold standard activation function for both shallow and deep neural networks, freely available in TensorFlow and Keras.                   Conclusion section                        \section{Conclusion}  hyper-sinh was proven an accurate and robust activation function for shallow and deep neural networks for image and text classification, thus being a new gold standard that scales well for FC-NN and CNN.  Since it is made freely available, open source, on the Python, TensorFlow and Keras ecosystems, it adds to the selection of activation functions that both not-for-profit and for-profit organisations can have when tackling image and text classification tasks with data sets of various sizes.  Importantly, the proposed algorithm, being accurate and reliable, and written in a high-level programming language , can be leveraged as a part of ML-based pipelines in specific use cases, wherein high accuracy and reliability need to be achieved, such as in the healthcare sector , from small to large clinics with its suitability from shallow to deep neural networks. Future work involves further improving this function to reduce its computational cost.     Acknowledgements should go at the end, before appendices and references  \acks{This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.}    Manual newpage inserted to improve layout of sample file - not   needed in general before appendices/bibliography.      
","  Grounded language learning has been successful in learning to follow directions, generating referring expressions, visual storytelling, video grounding and understanding commands, among others. Parsing can be grounded in a robot's world and action models, taking into account perceptual and grounding uncertainty.  or language ambiguity.  The problem space considered in this paper assumes that there are no pre-existing models of language or objects in the world---an agent is learning from novel language about previously unseen objects, making the evaluation more broadly applicable.   Active learning has been applied successfully to a number of problems, providing performance improvements in areas as diverse as learning from demonstration, following directions, and learning about object characteristics. A well-chosen active learning approach can reduce the number of labels required for grounded language learning, but raises questions of what queries to ask and when to ask them.   Advances in active learning techniques have improved the ability to find the most useful data points. Unsupervised learning techniques, such as subspace clustering, have been shown to find influential points from a cluster. A hybrid method that connects active learning and data programming has shown improvements in the reduction of noisy data in large scale workspaces. Similar to our work, active learning approaches have been effective while training biased and highly varied datasets. Similarly, researchers have put effort into utilizing different active learning methods depends on the complexity of the problem. Also, traditional active learning methods have helped to improve performances in other tasks, such as data fault or fake news detection. Though we consider efficiency over time complexity, researchers have studied methods that are time efficient, especially in large scale applications. Similar to our research,~ also compares two traditional active learning algorithms for selecting important points from a pool of training data. But we also consider distinct machine learning approaches with small scale and large scale datasets in our comparisons. Various Bayesian techniques have been used in selecting diverse points as the most influential is widely popular, and we use different variants of  DPP to select distinct data points as our active learning technique in batch sample selection.  In this work, our goal is to perform a principled exploration of selecting what data to query for labeling, using informativeness and uncertainty metrics in grounded language problems of varying complexity. We draw on existing techniques, particularly pool-based learning, uncertainty sampling   .  , and probabilistic sample selection.  We take advantage of that body of research to select our set of experimental approaches, which include sample selection via Gaussian mixture models and Determinantal Point Processes , which have proven effective in modeling diversity. Using supervised learners as the active learning techniques are not suitable for our current study since we concentrate on building a language model without prior knowledge.   Our work is most closely related to that of Thomason et al.~, who incorporate `opportunistic' active learning in a system that learns language in an unstructured environment. However, that work focuses on opportunistically querying for labels whenever annotators are present; this work, in contrast, is focused on exploring the best way of selecting good choices from a large range of possible queries, reflecting the assumption that opportunities to query users will often be severely limited.",184
" Deep neural networks are powerful and have been widely applied in natural language processing. However, recent studies demonstrate that these models are vulnerable to adversarial examples, which are malicious inputs intentionally crafted to fool the models. % The introduction of the adversarial example ushered in a new era to understand and improve neural the network-based models.  % Adversarial attacks and defenses against these attacks have drawn significant attention in recent years . Although generating adversarial examples for texts has proven to be a more challenging task than for images due to their discrete nature, a number of methods have been proposed to generate adversarial text examples and reveal the vulnerability of deep neural networks in natural language processing  tasks including reading comprehension , text classification , machine translation , dialogue systems , and dependency parsing . These methods attack text examples by replacing, scrambling, and erasing characters or words or other language units.  To settle the susceptible attack direction, they require a large number of queries to the target model for the predictions of given inputs. Thus the adversarial examples are typically generated for a specific model.  This motivates the main questions we aim to answer in this paper: Are there universal adversarial examples that can fool almost every neural network-based model? And are there universal attack rules for constructing such universal adversarial examples? %are there universal adversarial examples that can transfer to any neural network-based models?  It is well known that adversarial examples exhibit black-box transferability, meaning that adversarial examples generated for one model can fool another model .  Transfer attackers launch white-box attacks on local models to find candidate adversarial examples that may transfer to the target model. % In the white-box setting, an adversary can access the model's architecture, parameters and input feature representations while not in the black-box one. % However, adversarial examples are typically overfitted to the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models.  However, which factors most affect the transferability of adversarial examples is still unclear, especially for NLP models. In this study, we quantitatively investigate how adversarial transferability is impacted by several critical factors, including the network architecture, input form, word embedding type, and model capacity.  Based on the understanding of transferability among various neural models, we study whether it is possible to craft universal, model-agnostic text adversarial examples for almost all existing models.  Universal adversarial examples have at least two advantages. First, the adversaries do not need white-box access to the target models. They launch the attacks by their own models trained on similar data, which can transfer across models .  Second, universal adversarial examples are a useful analysis tool because, unlike typical attacks, they are model-agnostic.  Thus, they highlight general input-output patterns learned by a model. We can leverage this to study the influence of dataset biases and to identify those biases that are learned by models.   \end{center}  \end{table*}   In this study, we first systematically investigated a few critical factors of neural models, including network architectures , input forms , embedding types , and model capacities  and how they impact the transferability of text adversarial examples through extensive experiments on two datasets of text classification.  We vary one factor at a time while fixing all others to see which factor is more significant, and found that the input form has the greatest influence on the adversarial transferability, following by network architecture, embedding type, and model capacity. Then, we propose a genetic algorithm to find an optimal ensemble with minimum number of members on the basis of our understanding of the adversarial transferability among neural models.  The adversarial examples generated by attacking the ensemble found by our algorithm strongly transfer to other models, and for some models, they exhibit better transferability than those generated by attacking models with different random initialization. Finally, we generalize the adversarial examples constructed by the ensemble method into universal semantics-preserving word replacement rules that can induce adversaries on any text input strongly transferring to any neural network-based NLP model . Since those rules are model-agnostic, they provide an analysis of global model behavior, and help us to identify dataset biases and to diagnose heuristics learned by the models.        Observing that adversarial examples often transfer across different models , the attackers run standard white-box attacks on local surrogate models to find adversarial examples that are expected to transfer to the target models. Unfortunately, such a straightforward strategy often suffers from overfitting to specific weaknesses of local models and transfer-based attacks typically have much lower success rates than optimization-based attacks directly launched on the target models. To answer this problem, many methods have been proposed to improve the transfer success rate of adversarial examples on the target models by perturbing mid-layer activations , adding regularization terms to the example generation process , or ensembling multiple local models .    However, adversarial examples are typically overfitted to the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models.     Intermediate-layer perturbation.    attempted to    maximize the distances between natural images and adversarial examples in feature space to increase transferability rates.    introduce\orange{d} an intermediate level attack  that enhances black-box adversarial transferability by increasing the perturbation on a specific layer of a model.    tried to craft input-agnostic   adversarial perturbations by perturbing intermediate-layer activations that generalize not only across multiple CNN architectures but also across diverse computer vision tasks.     proposed a feature distribution attack  that leverages both class-wise and layer-wise deep feature distributions of a substitute DNN to generate adversarial examples that are highly transferable to a black-box target DNN.    use\orange{d} attention-weighed features to guide the search of adversarial examples, which prioritizes the corruption of critical features that are likely to be adopted by diverse architectures.    Introducing regularization.       integrate\orange{d} a momentum term into the iterative process of adversarial example generation, which can stabilize update directions and escape from poor local maxima, resulting in more transferable adversarial examples.    injects\orange{ed} two regularization terms into the training loss function to guide the search of adversarial manipulations, which alleviates the issue of vanishing gradient and reduces the variations of resultant adversarial samples.   They show that by introducing regularizers into the optimization process of adversarial samples, the performance of transfer-based black-box attacks can be improved significantly.    Ensemble method. Ensemble-based methods are most related to this study.  hypothesized that if an adversarial example remains adversarial for multiple models, then it is more likely to transfer to other models as well.   Following this hypothesis, they improved transferability rates by using an ensemble of local models. Ensemble-based methods have been used to generate transferable adversarial examples both in computer vision  and NLP .  found that the local non-smoothness of loss surface harms the transferability of generated adversarial examples, and proposed a variance-reduced attack to enhance the transferability by applying the locally averaged gradient to reduce the local oscillation of the loss surface. Unlike the methods that ensemble the predictions of different models, more transferable adversarial examples are generated by optimizing a perturbation over an ensemble of transformed images so that the generated examples are less sensitive to the local source models .    The methods mentioned above are designed to yield the best performance only on the model they are tuned to attack; often, the generated adversarial examples do not transfer to other models. In contrast,  proposed the Model-based Ensembling Attack that transfers better by avoiding dependence on any specific model.    In the text domain,  searched for universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. They focused on model-specific concatenated tokens generated using gradients under the white-box setting, and the founded universal triggers  are usually human unreadable.  presented semantic-preserving perturbations that cause models to change their predictions by the paraphrases generated via back-translation, and generalized these perturbations into universal replacement rules that induce adversaries on many text instances. They use the word ``universal'' to mean that their replacement rules can be used to any input text if some rules are matched with the input, but those rules were still generalized for some specific models. In contrast, we want to find the universal adversarial replacement rules by which the crafted adversarial examples can fool almost all existing models. Besides, the number of their replacement rules is quite small, and many texts do not meet the condition specified by their rules, while our adversarial word replacement rules can be applied to most texts, leading to higher success rates on various neural NLP models.   In addition, we search for an optimal ensemble with minimum number of models based on the understanding of the transferability among neural models, and the optimal ensemble can be used to generate adversarial examples that strongly transfer across neural network-based NLP models.          In this work, we present a thorough exploration of different active learning approaches to grounding unconstrained natural language in real-world sensor data. We demonstrate that active learning has the potential to reduce the amount of data necessary to ground language about objects, an active area of research in both NLP and robotics as well as machine learning from sparse data generally. We additionally provide suggestions for what approach may be suitable given the perceptual and linguistic complexity of a problem. Given our analysis of the causes of performance for different algorithms and cases, we believe these results will prove to generalize beyond the relatively simple data seen here, making it possible for these guidelines to apply to more complicated language grounding tasks in future.                                  
","    Observing that adversarial examples often transfer across different models , the attackers run standard white-box attacks on local surrogate models to find adversarial examples that are expected to transfer to the target models. Unfortunately, such a straightforward strategy often suffers from overfitting to specific weaknesses of local models and transfer-based attacks typically have much lower success rates than optimization-based attacks directly launched on the target models. To answer this problem, many methods have been proposed to improve the transfer success rate of adversarial examples on the target models by perturbing mid-layer activations , adding regularization terms to the example generation process , or ensembling multiple local models .    However, adversarial examples are typically overfitted to the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models.     Intermediate-layer perturbation.    attempted to    maximize the distances between natural images and adversarial examples in feature space to increase transferability rates.    introduce\orange{d} an intermediate level attack  that enhances black-box adversarial transferability by increasing the perturbation on a specific layer of a model.    tried to craft input-agnostic   adversarial perturbations by perturbing intermediate-layer activations that generalize not only across multiple CNN architectures but also across diverse computer vision tasks.     proposed a feature distribution attack  that leverages both class-wise and layer-wise deep feature distributions of a substitute DNN to generate adversarial examples that are highly transferable to a black-box target DNN.    use\orange{d} attention-weighed features to guide the search of adversarial examples, which prioritizes the corruption of critical features that are likely to be adopted by diverse architectures.    Introducing regularization.       integrate\orange{d} a momentum term into the iterative process of adversarial example generation, which can stabilize update directions and escape from poor local maxima, resulting in more transferable adversarial examples.    injects\orange{ed} two regularization terms into the training loss function to guide the search of adversarial manipulations, which alleviates the issue of vanishing gradient and reduces the variations of resultant adversarial samples.   They show that by introducing regularizers into the optimization process of adversarial samples, the performance of transfer-based black-box attacks can be improved significantly.    Ensemble method. Ensemble-based methods are most related to this study.  hypothesized that if an adversarial example remains adversarial for multiple models, then it is more likely to transfer to other models as well.   Following this hypothesis, they improved transferability rates by using an ensemble of local models. Ensemble-based methods have been used to generate transferable adversarial examples both in computer vision  and NLP .  found that the local non-smoothness of loss surface harms the transferability of generated adversarial examples, and proposed a variance-reduced attack to enhance the transferability by applying the locally averaged gradient to reduce the local oscillation of the loss surface. Unlike the methods that ensemble the predictions of different models, more transferable adversarial examples are generated by optimizing a perturbation over an ensemble of transformed images so that the generated examples are less sensitive to the local source models .    The methods mentioned above are designed to yield the best performance only on the model they are tuned to attack; often, the generated adversarial examples do not transfer to other models. In contrast,  proposed the Model-based Ensembling Attack that transfers better by avoiding dependence on any specific model.    In the text domain,  searched for universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. They focused on model-specific concatenated tokens generated using gradients under the white-box setting, and the founded universal triggers  are usually human unreadable.  presented semantic-preserving perturbations that cause models to change their predictions by the paraphrases generated via back-translation, and generalized these perturbations into universal replacement rules that induce adversaries on many text instances. They use the word ``universal'' to mean that their replacement rules can be used to any input text if some rules are matched with the input, but those rules were still generalized for some specific models. In contrast, we want to find the universal adversarial replacement rules by which the crafted adversarial examples can fool almost all existing models. Besides, the number of their replacement rules is quite small, and many texts do not meet the condition specified by their rules, while our adversarial word replacement rules can be applied to most texts, leading to higher success rates on various neural NLP models.   In addition, we search for an optimal ensemble with minimum number of models based on the understanding of the transferability among neural models, and the optimal ensemble can be used to generate adversarial examples that strongly transfer across neural network-based NLP models.",185
" Recent works have shown that NN models that are trained solely to maximize prediction performance are often vulnerable to adversarial attacks . Even though several works have been proposed to defend NN models against such attacks, only a few of them focus on the NLP domain . Since many recent NLP models are shown to be vulnerable to adversarial attacks--e.g., fake news detection  and dialog system , the investigation of robust defense methods for textual NN models has become necessary. To defend against adversarial texts, one can use either adversarial detection or model enhancement . Adversarial texts are often generated by replacing or inserting critical words  or characters  in a sentence, that are usually exhibiting grammatical errors. Hence, many detection methods have focused on recognizing and correcting such misspellings from texts--e.g., ScRNN  and DISP . While misspelling-based methods are model-independent and require neither re-training nor modifying the models, they only work well on character-based attacks. In contrast, model enhancement approaches perform well under both character and word-based attacks.  %Such generalization to a variety of attacks is critical since one might not know what type of adversarial techniques will be employed by adversaries.  Particularly, most of the model enhancement methods enrich NN models by training them with adversarial data augmented via known attack strategies such as in adversarial training , or with external information such as knowledge graphs . Nevertheless, these augmentations usually induce overhead costs in training. Therefore, we are in search of defense algorithms that directly enhance the models' structures  while achieving higher extendability without acquiring additional data.  %While developing these solutions is more challenging and still under exploration .   Fortunately, recent literature in computer vision shows that ensemble NNs achieve high adversarial robustness . In theory, by directly extending a single NN model to an ensemble of multiple diverse sub-models, we challenge adversaries to attack not only one but a set of very different models . This makes any attacks significantly more difficult. However, applying such an idea from computer vision to the NLP domain faces one main challenge. Current ensemble methods require simultaneous training of several NN sub-models . This introduces impractical computational overhead during both training and inference, especially when one wants to maximize prediction accuracy by utilizing complex state-of-the-art  sub-models such as BERT  and ROBERTA , both of which have more than 100M parameters. Furthermore, applying current ensemble or other defensive approaches that directly enhance a model's architecture to a large-scale NN model would usually require re-training everything from scratch, which may not be practical in many settings.  % Second, current ensemble approaches aim to promote the diversity of sub-models at either feature-level  or at class-level .  % Second, current ensemble approaches promote the diversity of sub-models by maximizing their differences among either prediction output vectors  or gradient vectors w.r.t an input image . However, most of NLP tasks, classification particularly, have much less labels than those of computer vision, which results in a much smaller degree-of-freedom when directly regularizing the differences of prediciton probability vectors.   % On the other hand, forcing the sub-models to focus on different tokens of an input text by directly regularizing their gradient vectors is not straightforward because text are discrete in nature. This can be easily resolved by regularizing the gradients w.r.t the continuous word-embedding vectors of the sentence instead. However, since every sub-model contributes equally to an input, there will be many overlaps among the key features, i.e., words or phrases, of each of the sub-models when the input text is short.   To address these challenges, we are borrowing ideas from Software Engineering, by first introducing the notion of {\bf Neural Patching} to improve the adversarial robustness of NN models by ``patching"" only parts of models . Next, we develop a novel neural patching algorithm, {\mymethod}, that patches only the last layer of an already deployed textual NN model of diverse architectures  and transforms it into an ensemble of multi-experts with enhanced adversarial robustness.  By patching only the last layer of a model, {\mymethod} introduces a lightweight computational overhead and requires no additional training data. %requires low construction overheads without compromising much computational complexity or additional training data.  Distinguished from current ensemble methods, each sub-model trained by {\mymethod} is specialized not only in a specific subset of features of the same input, i.e., an expert at feature-level , or a sub-set of labels, i.e., an expert at class-level , but also in texts of a distinguished topic, i.e., an expert at instance-level, during prediction. Such diversity at all of the feature-, class-, and instance-level expertise makes it challenging for adversaries to exploit multiple sub-models at the same time. In summary, our contributions in this paper are as follows:   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   {\mymethod} is mainly motivated by previous ensemble-based defense methods, most of which are designed for computer vision. From these works, works such as  further introduce the theoretical analysis of the relationship between the diversity of sub-models, i.e., heads, of an ensemble NN model and its adversarial robustness. Though these works focus on the image domain, we can also derive a similar analysis on the advantage of ensemble diversity for defending against adversarial texts. The key idea is to force attackers to exploit not one but multiple NN models whose behaviors are very different. We call such distinguished behaviors as expertises of these sub-models.   On a feature-level expertise, if the sub-models focus on different words of a sentence to make a prediction, replacing a few critical words in the text might not necessarily fool their combination . This diversity can be obtained by regularizing the differences among salient maps of sub-models via gradient vectors. However, previous works  only focus on minimizing the cosine-similarity, i.e., direction differences, among the gradient vectors. In this work, we find that the differences in length among them also correlate with adversarial robustness . In contrast, sub-models with class-level expertise might focus on a diverse sub-sets of labels. This can be enabled by regularizing prediction probabilities output by the sub-models . Since the probabilities are limited to , directly regularizing the differences in the prediction probabilities might not be effective. To improve this, we propose to regularize the prediction logits instead of probability on the class-level. For example, Table B shows that Head \#1 and \#2 specialize in only one label at a time, outputting logit scores for the positive and negative classes, respectively. In this work, we also propose an instance-level expertise where each sub-model is assigned with sentences of a distinguished topic . This is an extreme case in our method where one sub-model might not be at all involved in a specific prediction .   Instance-level diversity is achieved by utilizing Neural Architecture Search  to further search for  non-identical    incongruent but optimal diverse architectures for sub-models.   Distinguished from previous ensemble methods, {\mymethod} takes into account all of the three levels of expertise of sub-models. Moreover, applying current ensemble methods to the NLP domain faces a practical challenge where training multiple SOTA sub-models such as BERT or ROBERTA can be very costly in terms of space and time complexities. Thus, {\mymethod} also enables to ``hot-fix"" a complex NN model with our Neural Patching mechanism, removing the need for training the entire model from scratch.       \caption{Expertise of Heads on Clickbait Dataset}           \caption{Expertise among Heads in Detecting Clickbait   \lee{Terms like ""head"" or ""expertise"" are not properly explained. Predictions for Head #1-3 are unclear--what's the labels to predict?} }        In this study, we investigated four critical factors of NLP neural models, including network architectures, input forms, embedding types, and model capacities and how they impact the transferability of text adversarial examples with  different models. Based on the understanding of the transferability among those models, we proposed a genetic algorithm to find an optimal ensemble of very few models that can be used to generate adversarial examples that transfer well to all the other models. We also described a algorithm to discover universal adversarial word replacement rules that can be applied to craft adversarial examples with strong transferability across various neural models without access to any of them. Finally, since those adversarial examples are model-agnostic, they provide an analysis of global model behavior and help to identify dataset biases.  {\small   }   
"," {\mymethod} is mainly motivated by previous ensemble-based defense methods, most of which are designed for computer vision. From these works, works such as  further introduce the theoretical analysis of the relationship between the diversity of sub-models, i.e., heads, of an ensemble NN model and its adversarial robustness. Though these works focus on the image domain, we can also derive a similar analysis on the advantage of ensemble diversity for defending against adversarial texts. The key idea is to force attackers to exploit not one but multiple NN models whose behaviors are very different. We call such distinguished behaviors as expertises of these sub-models.   On a feature-level expertise, if the sub-models focus on different words of a sentence to make a prediction, replacing a few critical words in the text might not necessarily fool their combination . This diversity can be obtained by regularizing the differences among salient maps of sub-models via gradient vectors. However, previous works  only focus on minimizing the cosine-similarity, i.e., direction differences, among the gradient vectors. In this work, we find that the differences in length among them also correlate with adversarial robustness . In contrast, sub-models with class-level expertise might focus on a diverse sub-sets of labels. This can be enabled by regularizing prediction probabilities output by the sub-models . Since the probabilities are limited to , directly regularizing the differences in the prediction probabilities might not be effective. To improve this, we propose to regularize the prediction logits instead of probability on the class-level. For example, Table B shows that Head \#1 and \#2 specialize in only one label at a time, outputting logit scores for the positive and negative classes, respectively. In this work, we also propose an instance-level expertise where each sub-model is assigned with sentences of a distinguished topic . This is an extreme case in our method where one sub-model might not be at all involved in a specific prediction .   Instance-level diversity is achieved by utilizing Neural Architecture Search  to further search for  non-identical    incongruent but optimal diverse architectures for sub-models.   Distinguished from previous ensemble methods, {\mymethod} takes into account all of the three levels of expertise of sub-models. Moreover, applying current ensemble methods to the NLP domain faces a practical challenge where training multiple SOTA sub-models such as BERT or ROBERTA can be very costly in terms of space and time complexities. Thus, {\mymethod} also enables to ``hot-fix"" a complex NN model with our Neural Patching mechanism, removing the need for training the entire model from scratch.       \caption{Expertise of Heads on Clickbait Dataset}           \caption{Expertise among Heads in Detecting Clickbait   \lee{Terms like ""head"" or ""expertise"" are not properly explained. Predictions for Head #1-3 are unclear--what's the labels to predict?} }",186
"  As the growth of robots interacting with humans, different levels of environment understanding is required by the robot. A robot acting in an environment has to deal with many open questions, thus needs different levels of reasoning to do a task. Usually, robots rely on their initial knowledge, perception and their cognitive abilities to be able to understand and do reasoning in their situated environment. A recently hooked topic to a better Knowledge-Based cognition is dialogic interaction between a human and a robot, where the robot captures fresh information about the environment from a user through Natural Language. Information comes from Natural Language together with visually perceived information, and a Knowledge Base  lets a cognitive agent reach different levels of understanding in the environment.   The first level of understanding can be seen as classification and detection on sensory inputs, e.g. detection of objects in visual perception, or role tagging of lexical in a sentence. The second level of understanding concerns finding relations between different sensory inputs, e.g. finding common attributes in language and vision. Some famous problems such as symbol grounding  and anchoring  concern finding correspondences in different sensory input modalities. A higher and abstract level of understanding can be thought to find relations between the entities in an environment. e.g. in a scene with a desk and a book on top, some of the relationships between these are their relative physical position and their semantics that shows how entities  are similar.   Understanding relationships between physical entities can also be extended to the attributes of entities. Indeed the same definition of the relationship between entities can be found for the attributes. For example, when a user declares freshness attribute of 'apple-1' is 'spoiled', as well as 'apple-2' and 'apple-3', but 'orange-1' and 'banana-1' are 'fresh', a relation between the values of freshness attribute exists which connects semantic of entities; In this example, is that all apples are 'spoiled', and the rest of fruits are 'fresh', with closed world assumption.   Relation and rules for attributes of entities can help a robot that is interacting with a human in many applications. For example when a user utters ""bring me a fruit"", using the rules obtained for freshness attribute, the robot notices which fruits are spoiled and which are fresh to eat. Such logical rules between attributes let the robot realize that apples are spoiled, apples should be thrown out, and added to the shopping list. Moreover, the obtained rule for attributes can be used in a robot's low-level sensory input processing. Consider an utterance where the user of our example is declaring that a physical entity is spoiled, but the robot's visual perception has doubt whether the perceived object is apple or pear. As the robot already found that all apples are spoiled and other fruits are fresh, so the perceptual detection refines the recognized object as the apple.  %Different attributes can represent characteristics of an entity, where some are computed from visual perception and some from Natural Language through interaction with a user. In this work, we deal with nine different attributes, as a category, color, label, functionality, owner, size, weight, restriction, and location of entities in a scene; where the first two are computed from visual perception and the rest are obtained from Natural Language.  %%It is worth emphasis on the importance of attributes that come from Natural Language. Such information is almost impossible to obtain from visual perception, e.g. the information that a user can give about owner of an entity, cannot be obtained from the camera. Also, an initial knowledge base only gives information about the category of an entity, and not about a particular entity , and some of the assignments might be temporary. On the other hand, information about size, weight, and location, may be used for refinement of knowledge base and camera, or just a shortcut to obtaining such information from the user.   In this work, we propose a framework for learning logical rules that represent relations between attributes in a semantic model of the robot's environment. Such logical rules help the robot to find which attributes  entail a specific attribute. A distinctive novelty of our work is to generalize rules from a semantic model built via Human-Robot Interaction , through the integration of visual and linguistic cues. Our framework goes all the way from sensory input data to abstract First-Order Logic formulas that describe abstract relationship between attributes of entities in a scene. %Our approach differs from other works as our system is able to capture more attributes from Natural Language in addition to attributes from computer vision.  %Our proposed framework compute First-Order Logic formulas, which is useful for general reasoning upon entities that have common attributes.  We focus on latent rules, which the robot can capture implicitly when a human describes objects to the robot. In other words, we do not require the user to give rules explicitly to the robot, but rather we let the robot find rules and do further reasoning based on self-computed rules for improving its interaction with the user.    This paper continues with the review of related work, and then in Section  the proposed framework is described, followed by an implementation to demonstrate the viability of the proposed framework in Section . In Section  results of a test scenario are given, followed by the discussion about the applicability of the framework. In the end, conclusions of this work are drawn.         Our framework combines insights and techniques from semantic scene understanding; from situated Natural Language dialogue; and from Inductive Logic. Accordingly, in this section we review some of the related works. Firstly, we review some works around understanding a scene alongside a sentence. Thereafter, we review works in the joint fields of Natural Language and computer vision, with the focus on their Natural Language understanding module. Lastly, we discuss works that concern reasoning and Inductive Logic.   As Neural Network advancements achieved astonishing results, a new trend started to understand a scene via objects and their attributes in images. This trend aims to name and detect objects, and describe the attributes and their relationship in a scene . The work described in  focus on the state and transformation of objects in a scene for understanding an image. Sadeghi et al. in  point to depicted interactions for understanding an image. Some works, for example the work described in , solely focus on detection of the attributes from image, including learning visual relations between objects in the image. Alongside this trend, researchers advised to shift the task of object recognition by names to recognition by descriptions ; which transforms the problems of finding attributes and symbol grounding to the problem of Referential Expression Grounding . Some of the most promising works in this field are  , , , which try to find the referred object, given a phrase that describes object via attributes and interactions showed in the image. Despite the fact that all of these works do their task based on attributes of objects, they treat them as visual features for describing depicted objects, which bound their domain of attributes to visually perceivable attributes; In this work we extend the domain of attributes by including attributes captured from dialog.  Linguistics have different points of view to attributes. Some works extract the logical form of a sentence, through semantic parser ,  , , . The logical form that is computed from a language may be used for obtaining predicates from language, or obtaining attributes of a symbol in a sentence.  defining the problem of understanding attributes as the problem of semantic parsing. Their goal is to find the relation between attributes and the corresponding symbol from a phrase.  In particular, in the medium of text, the problem of finding attributes of an object in the text requires a symbolic detection system, even though neuro-systems may be deployed in detection of relationship and the role of words.  Some works use the combination of Natural Language and computer vision. The work presented in  captures semantic attributes from Natural Language, focused on category of objects inferred from a visual classifier. Pronobis et al. in  use attributes for resolving ambiguity in semantic mapping, which the two attributes from  Natural Language are category and position of objects that are extracted by grammar parser. In , a framework for capturing spatial relationships between objects and locations, inherited from the dialog, for learning a human-centered model is presented. Works that are dealing with maps are bounded to understanding verbal position and category of objects. Also, works that aim to capture more verbal attributes are using a shallow grammar parser, inheriting from the dependency tree of the sentence; they cannot capture attributes from different linguistic expressions, where the dependency of words is not so reliable, in most realistic cases. To overcome these shortcomings, we follow our previous work , which can capture seven different attributes from different linguistic expressions.   Focus on attributes in knowledge representation is not on capturing attributes from different input modalities, nor grounding them to physical environment, but rather on the relationship between attributes, and reasoning over attributes. Rules are widely used as a way to express the relationship between attributes. Inductive logic programming, born at the intersection of machine learning and logic programming, is widely used as a relational learning approach . Inductive Logic programming learns rules from positive and negative examples, supported by background knowledge. The resulting rules should entail as many as positive examples, regarding background knowledge, and as few negative examples as possible , . In most realistic applications there is not a particular rule that includes all positive examples and avoids all negatives at the same time: to address this problem, Raedt et al. integrated probabilities with logic programming, both in deductive  and inductive  reasoning. While these works assume an initial knowledge base, in our work we use these methods in a case where background knowledge, negative examples and positive examples are created incrementally from vision and dialog.   Some works combined knowledge engineering and Natural Language. Combinatorial Categorial Grammar   is one of the most famous language formalisms for semantic parsing. The work proposed in  use CDG for obtaining logical form of a sentence, more precisely they compute Lambda formula out of a sentence using CDG, for learning visual attributes by demonstration and spoken interaction. Some researchers focus on end-to-end models of language and logical formula. For example, work described in  shows an approach for transforming Natural Language into a logical formula, using end-to-end Neural Networks. Interacting with such system corresponds to giving the logical rules explicitly to the system, whereas in this work, we are proposing a system that can calculate the spatial logical relation implicitly from in the interaction with the user.     It is seldom the case data in the wild has a balanced distribution. In realistic settings, there is a limitation of acquiring relatively balanced data through choices of balanced data sources. Handling data skewness is a crucial problem because learning from imbalanced data inevitably brings bias toward frequently observed classes. Data-level manipulation tries to under-sample the majority classes or over-sample the minority classes. But these methods tend to discard valuable information from observations of majority classes or overfit to a sparse representation of minority classes, especially as the imbalance level gets higher. Moreover, recent methods such as SMOTE cannot be applied directly to the text data.   We propose ST, which effectively circumvents these issues by simply decomposing the data into k splits and sequentially training a learner in the decreasing order of KL divergence with the target distribution, which in the case of data imbalance problem is the discrete uniform distribution. Through extensive experiments, we show our architecture proves to be compatible with previous methods and outperforms existing methods when validated on simulated as well as real-application tasks. Our model shows superiority in performance because it enables more focus to be put on minority instances while not forgetting about majority instances. We believe that our work makes a meaningful step towards handling data skewness in text classification and the application of incremental learning methods focused on the data imbalance problem.  For future work, ensemble methods can be used by varying the  ratio to train multiple weak learners. Moreover, since ST can be applied simultaneously with algorithm-level methods, proven methods such as focal loss  and cost-sensitive deep neural network  could be implemented together to increase optimal performance.  
","  Our framework combines insights and techniques from semantic scene understanding; from situated Natural Language dialogue; and from Inductive Logic. Accordingly, in this section we review some of the related works. Firstly, we review some works around understanding a scene alongside a sentence. Thereafter, we review works in the joint fields of Natural Language and computer vision, with the focus on their Natural Language understanding module. Lastly, we discuss works that concern reasoning and Inductive Logic.   As Neural Network advancements achieved astonishing results, a new trend started to understand a scene via objects and their attributes in images. This trend aims to name and detect objects, and describe the attributes and their relationship in a scene . The work described in  focus on the state and transformation of objects in a scene for understanding an image. Sadeghi et al. in  point to depicted interactions for understanding an image. Some works, for example the work described in , solely focus on detection of the attributes from image, including learning visual relations between objects in the image. Alongside this trend, researchers advised to shift the task of object recognition by names to recognition by descriptions ; which transforms the problems of finding attributes and symbol grounding to the problem of Referential Expression Grounding . Some of the most promising works in this field are  , , , which try to find the referred object, given a phrase that describes object via attributes and interactions showed in the image. Despite the fact that all of these works do their task based on attributes of objects, they treat them as visual features for describing depicted objects, which bound their domain of attributes to visually perceivable attributes; In this work we extend the domain of attributes by including attributes captured from dialog.  Linguistics have different points of view to attributes. Some works extract the logical form of a sentence, through semantic parser ,  , , . The logical form that is computed from a language may be used for obtaining predicates from language, or obtaining attributes of a symbol in a sentence.  defining the problem of understanding attributes as the problem of semantic parsing. Their goal is to find the relation between attributes and the corresponding symbol from a phrase.  In particular, in the medium of text, the problem of finding attributes of an object in the text requires a symbolic detection system, even though neuro-systems may be deployed in detection of relationship and the role of words.  Some works use the combination of Natural Language and computer vision. The work presented in  captures semantic attributes from Natural Language, focused on category of objects inferred from a visual classifier. Pronobis et al. in  use attributes for resolving ambiguity in semantic mapping, which the two attributes from  Natural Language are category and position of objects that are extracted by grammar parser. In , a framework for capturing spatial relationships between objects and locations, inherited from the dialog, for learning a human-centered model is presented. Works that are dealing with maps are bounded to understanding verbal position and category of objects. Also, works that aim to capture more verbal attributes are using a shallow grammar parser, inheriting from the dependency tree of the sentence; they cannot capture attributes from different linguistic expressions, where the dependency of words is not so reliable, in most realistic cases. To overcome these shortcomings, we follow our previous work , which can capture seven different attributes from different linguistic expressions.   Focus on attributes in knowledge representation is not on capturing attributes from different input modalities, nor grounding them to physical environment, but rather on the relationship between attributes, and reasoning over attributes. Rules are widely used as a way to express the relationship between attributes. Inductive logic programming, born at the intersection of machine learning and logic programming, is widely used as a relational learning approach . Inductive Logic programming learns rules from positive and negative examples, supported by background knowledge. The resulting rules should entail as many as positive examples, regarding background knowledge, and as few negative examples as possible , . In most realistic applications there is not a particular rule that includes all positive examples and avoids all negatives at the same time: to address this problem, Raedt et al. integrated probabilities with logic programming, both in deductive  and inductive  reasoning. While these works assume an initial knowledge base, in our work we use these methods in a case where background knowledge, negative examples and positive examples are created incrementally from vision and dialog.   Some works combined knowledge engineering and Natural Language. Combinatorial Categorial Grammar   is one of the most famous language formalisms for semantic parsing. The work proposed in  use CDG for obtaining logical form of a sentence, more precisely they compute Lambda formula out of a sentence using CDG, for learning visual attributes by demonstration and spoken interaction. Some researchers focus on end-to-end models of language and logical formula. For example, work described in  shows an approach for transforming Natural Language into a logical formula, using end-to-end Neural Networks. Interacting with such system corresponds to giving the logical rules explicitly to the system, whereas in this work, we are proposing a system that can calculate the spatial logical relation implicitly from in the interaction with the user.",187
" Due to the growing presence of AI-powered systems in our lives, affective computing has become an important part of human-computer interaction. Emotion plays a role in our thoughts and actions and is an integral part of the way we communicate . The ability to leverage context to understand emotions communicated both verbally and non-verbally is trivial for humans but remains difficult for machines . Emotional responses depend on both our psyche and physiology and are governed by our perception of situations, people and objects. They also depend on our mental state   . The way we exhibit and perceive emotion may also differ based on our age, gender, race, culture and accent . In addition to all of this, unlike targets in other classification tasks, the emotions we experience are rarely distinct: they often coexist without clear temporal boundaries, adding considerable complexity to the task .  Despite these difficulties, automated emotion recognition has social and commercial applications that make it worth pursuing. In the medical domain, it has exciting potential: to identify and diagnose depression and stress in individuals , to monitor and help people with bipolar disorder  and to assist the general public in maintaining mental health. Commercial applications include call center customer management, advertising through neuro-marketing and social media engagement . As intelligent chatbots and virtual assistants have become more widely used, emotion detection has become a vital component in the design, development and deployment of these conversational agents .  Early research in emotion detection focused on binary classification in a single modality, whether in text, speech , or images . Text-based classifiers used the n-gram vocabulary of sentences to predict their polarity and speech models modeled the vocal dynamics that characterize these emotions. These approaches are inherently limited: a binary granularity and cues from a single modality are far removed from the actual human process they're meant to model. As a result, joint approaches which leverage all available modalities  are promising.  While existing multi-modal emotion corpora like IEMOCAP  and Crem-D  have been critical for the progress in affective computing to date, they suffer from three issues that are the focus of our work. First, these corpora tend to be small due to the high costs of annotating for emotion. This precludes the use of deep neural models with high model complexity as they require many training samples to generalize well. This also compounds the second difficulty inherent to many emotion datasets: while there are usually many neutral, happy and sad training examples, there are often very few examples of rarer emotions like disgust making them difficult to classify. This issue is not easily solved by combining different corpora due to the third issue, their lack of mutual compatibility -- they differ in the emotions identified, the types of dialogue and number of speakers represented and the naturalness of the recordings . This severely restricts the generalizability of models trained on a single corpus.  Contemporary literature has dealt with these problems by dropping labels . Hard and scarce emotions like disgust are dropped from the corpus and the models are trained and evaluated on the trimmed corpus. This allows evaluating models on different corpora by using utterances exhibiting only the most common emotions. While this is a reasonable, the resulting performance is not a complete reflection of how these models perform once deployed to production. When emotion models are used in real-world applications, we can expect them to encounter utterances corresponding to dropped labels. For such cases, these models are likely to exhibit degraded performance by predicting one of the known, but incorrect labels.  In this work, we address the problem of data sparsity by transfer learning via the pretrain-then-finetune paradigm. Deep complex models can be trained on large datasets for an auxiliary but related task to learn network parameters that reflect abstract notions related to the target task. As the expression of emotions is highly dependent on the individual, we train a multilayer TDNN  on the task of speaker identification using the VoxCeleb corpus  and then fine-tune its final few layers on the task of emotion identification using the Crema-D corpus . Using this network, we extract speech embeddings for Crema-D from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a fine-tuned BERT model  and then train an LDA - pLDA  model on the resulting dense representations.  pLDA allows our model to more easily adapt to previously unseen classes and domains, a requirement for both evaluating against a different emotion corpus with an incompatible label set and performing well in the wild.  To understand the merits of each component, we exhaustively evaluate the predictive power of every permutation: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof. Our best variant, trained on only VoxCeleb and Crema-D and evaluated on IEMOCAP, achieves an Equal Error Rate  of \%. Including a portion of IEMOCAP during training produces a 5-fold averaged EER of \%.        In this work, we focus on two tasks related to emotions.  The first is emotion identification.  Given the audio and accompanying text for an utterance , emotion identification is the task of identifying the emotion  expressed in  from a fixed set of emotions .  This is the standard classification task found in the literature.  The second is emotion confirmation.  Given the audio and accompanying texts for two utterances  and , emotion confirmation is the task of identifying whether the two utterances express the same emotion.  This task can be thought of as analogous to either hypothesis testing in speaker recognition or a one-shot classification task.  It is motivated by the labeling mismatches among the various emotion corpora and is meant to better reflect the requirement that emotion detection systems be able to adapt to emotions unseen during training once deployed to production.     Early work on emotion detection in speech focused on the extraction of hand-crafted features for classification. \citet{liscombe_features} extracted a set of continuous features based on the fundamental frequency, amplitude and spectral tilt of speech and analyzed its correlation with different emotions. Contemporary literature has focused on deep neural networks with particular successes in transfer leraning. \citet{pappagari2020x} studied the transfer of embeddings from a ResNet-based speaker verification model using linear methods. Transfer learning with TDNNs has also been shown to be effective. \citet{zhou2020transfer} fine-tune a pre-trained ASR model and achieve strong results. An alternate approach seen in the literature is to train in a multi-task context on a useful auxiliary task .    Deep neural techniques have often been applied to integrate information from both the speech and text modalities. \citet{heusser2019bimodal} trained separate speech and text emotion classifiers and then jointly optimized them for multimodal emotion detection. An alternate method for creating multimodal classifiers is to use the embeddings from a hidden layer of the unimodal models for multimodal analysis. While \citet{chen2020multi} created an ensemble of classifiers using the unimodal embeddings individually and their multimodal concatenations, \citet{tripathi2019deep} fed the concatenation to a fully-connected neural layer and backpropagated the classification loss. Fusion using attention is yet another method for combining embeddings from different modalities . Even in the trimodal setting , the primary approaches for integrating modalities are concatenation and attentive fusion .  Contemporary results in emotion detection on IEMOCAP are shown in Table  with their accompanying modalities, supported emotion sets and performances.   &  \\ \citet{heusser2019bimodal} & happy, angry, sad, neutral &  &  \\ \citet{lian2019domain} & happy+exc, angry, sad, neutral & Weighted Accuracy &  \\ \citet{tripathi2019deep} & happy, angry, sad, neutral &  &  \\ \midrule \multicolumn{4}{c}{Trimodal - Speech, Text and Vision} \\ \midrule \citet{yoon2020attentive} &  &  &  \\ \citet{mittal2019m3er} & happy, angry, sad, neutral &  &  \\ \citet{tripathi2018multi} & happy+exc, angry, sad, neutral & Accuracy  &  \\ \bottomrule       		 		 		We presented Kathaka, an NTTS model trained using a novel two-stage training approach for generating speech with contextually appropriate prosody. In the first stage of training, we learnt a distribution of sentence-level prosodic representations. We then introduced a novel sampling mechanism of using trained samplers to sample from the learnt sentence-level prosodic distribution. We introduced two samplers, 1)~the BERT sampler which uses contextual word-piece embeddings from BERT and 2)~the Graph sampler where we interpret constituency parse trees as graphs and use a Message Passing based Graph Attention Network on them. We then combine both these samplers as the BERT+Graph sampler, which is used in Kathaka. We also modify the baseline duration model to incorporate the latent prosodic information. We conducted an ablation study of the samplers and showed a statistically significant improvement over the baseline in each case. Finally, we compared Kathaka against a baseline, and showed a statistically significant relative improvement of . 	 	
","    In this work, we focus on two tasks related to emotions.  The first is emotion identification.  Given the audio and accompanying text for an utterance , emotion identification is the task of identifying the emotion  expressed in  from a fixed set of emotions .  This is the standard classification task found in the literature.  The second is emotion confirmation.  Given the audio and accompanying texts for two utterances  and , emotion confirmation is the task of identifying whether the two utterances express the same emotion.  This task can be thought of as analogous to either hypothesis testing in speaker recognition or a one-shot classification task.  It is motivated by the labeling mismatches among the various emotion corpora and is meant to better reflect the requirement that emotion detection systems be able to adapt to emotions unseen during training once deployed to production.     Early work on emotion detection in speech focused on the extraction of hand-crafted features for classification. \citet{liscombe_features} extracted a set of continuous features based on the fundamental frequency, amplitude and spectral tilt of speech and analyzed its correlation with different emotions. Contemporary literature has focused on deep neural networks with particular successes in transfer leraning. \citet{pappagari2020x} studied the transfer of embeddings from a ResNet-based speaker verification model using linear methods. Transfer learning with TDNNs has also been shown to be effective. \citet{zhou2020transfer} fine-tune a pre-trained ASR model and achieve strong results. An alternate approach seen in the literature is to train in a multi-task context on a useful auxiliary task .    Deep neural techniques have often been applied to integrate information from both the speech and text modalities. \citet{heusser2019bimodal} trained separate speech and text emotion classifiers and then jointly optimized them for multimodal emotion detection. An alternate method for creating multimodal classifiers is to use the embeddings from a hidden layer of the unimodal models for multimodal analysis. While \citet{chen2020multi} created an ensemble of classifiers using the unimodal embeddings individually and their multimodal concatenations, \citet{tripathi2019deep} fed the concatenation to a fully-connected neural layer and backpropagated the classification loss. Fusion using attention is yet another method for combining embeddings from different modalities . Even in the trimodal setting , the primary approaches for integrating modalities are concatenation and attentive fusion .  Contemporary results in emotion detection on IEMOCAP are shown in Table  with their accompanying modalities, supported emotion sets and performances.   &  \\ \citet{heusser2019bimodal} & happy, angry, sad, neutral &  &  \\ \citet{lian2019domain} & happy+exc, angry, sad, neutral & Weighted Accuracy &  \\ \citet{tripathi2019deep} & happy, angry, sad, neutral &  &  \\ \midrule \multicolumn{4}{c}{Trimodal - Speech, Text and Vision} \\ \midrule \citet{yoon2020attentive} &  &  &  \\ \citet{mittal2019m3er} & happy, angry, sad, neutral &  &  \\ \citet{tripathi2018multi} & happy+exc, angry, sad, neutral & Accuracy  &  \\ \bottomrule",188
"   Vocoders were originally used for speech compression in the field of communication. Recently, vocoders have been utilized in various fields such as text-to-speech and voice conversion or speech-to-speech translation. Neural vocoders generate human-like voices using neural networks, instead of using traditional methods that contain audible artifacts .  Recently, it has been demonstrated that vocoders exhibit superior performances in generation speed and audio fidelity when trained with single speaker utterances. However, some models face difficulty when generating natural sounds in multiple domains such as speakers, language, or expressive utterances. The ability of these models can be evaluated by the sound quality when the model is trained on data of multiple speakers and the sound quality of the unseen domain . A vocoder that can generate high-fidelity audio in various domains,  regardless of whether the input has been encountered during training or has come from an out-of-domain source, is usually called a universal vocoder.  MelGAN is a vocoder based on generative adversarial networks . It is a lightweight and robust model for unseen speakers but yields lower fidelity than popularly employed models . MelGAN alleviates the metallic sound that occurs mainly in unvoiced and breathy speech segments through multi-scale discriminators that receive different scale waveforms as inputs. However, it has not been implemented efficiently for learning with multiple speakers for a universal vocoder.  In this study, we propose Universal MelGAN. The generated waveform of the original MelGAN with audible artifacts appears as an over-smoothing problem with a non-sharp spectrogram. We added multi-resolution spectrogram discriminators to the model to address this problem in the frequency domain. Our multi-scale discriminators enable fine-grained spectrogram prediction by discriminating waveforms and spectrograms. In particular, they alleviate the over-smoothing problem in the high frequency band of the large footprint model, enabling the generation of realistic multi-speaker waveforms.  To evaluate the performance of the proposed model, we compare with full-band MelGAN  as a baseline and two other vocoders: WaveGlow and WaveRNN. We designed experiments in both Korean and English for language independency. For evaluation, we prepared multiple speaker utterances that included unseen domain scenarios, such as new speakers, emotions, and languages.  The evaluation results indicate that the proposed model achieved the best mean opinion score  in most scenarios and efficiently preserved the fidelity in unseen speakers. In addition, the evaluations show that the model efficiently preserves the original speech, even in challenging domains such as expressive utterances and unseen languages. In multi-speaker text-to-speech scenarios, our model can generate high-fidelity waveforms with high MOS, and the model outperforms compared vocoders. This results without any external domain information suggest the possibility of the proposed model as a universal vocoder.     MelGAN is a GAN-based lightweight vocoder in which the generator comprises transposed convolution layers for upsampling and a stack of residual blocks for effective conversion. Multiple discriminators are trained with different scale waveforms to operate in different ranges. Recently, a modified architecture called FB-MelGAN with improved fidelity has been proposed.  WaveGlow can directly maximize the likelihood of data based on a normalizing flow. A chain of flows transforms simple distributions  into the desired data distribution. It has been shown that the speaker generalization of WaveGlow obtained high objective scores than other models.  WaveRNN is an autoregressive model that generates waveforms using recurrent neural network  layers. It has been demonstrated that WaveRNN preserves sound quality in unseen domains. The robustness of WaveRNN using speaker representations has been discussed.    \hfill           In this work, we present a multi-modal approach to emotion detection that first transfers learning from related tasks in speech and text to produce robust neural embeddings and then uses these embeddings to train a pLDA classifier that is able to adapt to previously unseen emotions and domains.  We show that:    In the future, we think there is promise in adapting learning from such fine-tuned emotion detection models to other emotions, domains and languages via one-shot classification with pLDA.  We are also interested in exploring the effectiveness of transferring from other auxiliary tasks like automated speech recognition.   
","  MelGAN is a GAN-based lightweight vocoder in which the generator comprises transposed convolution layers for upsampling and a stack of residual blocks for effective conversion. Multiple discriminators are trained with different scale waveforms to operate in different ranges. Recently, a modified architecture called FB-MelGAN with improved fidelity has been proposed.  WaveGlow can directly maximize the likelihood of data based on a normalizing flow. A chain of flows transforms simple distributions  into the desired data distribution. It has been shown that the speaker generalization of WaveGlow obtained high objective scores than other models.  WaveRNN is an autoregressive model that generates waveforms using recurrent neural network  layers. It has been demonstrated that WaveRNN preserves sound quality in unseen domains. The robustness of WaveRNN using speaker representations has been discussed.    \hfill",189
" %What is spoken term detection  Unsupervised speech modeling is the task of discovering and modeling speech units at various levels from audio recording without using any prior linguistic information. It is an interesting, challenging and impactful research problem as phonetic, lexical and even semantic information could be acquired without the process of transcribing and understanding the given speech data. The relevant technology is particularly important to facilitate data preparation especially in the scenarios where: 1) a large  amount of audio data are readily available online but they are untranscribed; 2) a large amount of audio recording is available for an unpopular language about which no structured linguistic knowledge or documentation can be found.  Spoken term discovery is a representative task of unsupervised speech modeling. It aims to discover repetitively occurred words and/or phrases from untranscribed audio.  The problem is commonly tackled with a two-stage approach. In the first stage, a set of subword units are automatically discovered from untranscribed speech data and these units in turn can be used to represent the speech data as a symbol sequence. In the second stage, variable-length sequence matching and clustering are performed on the subword sequence representations. One major drawback of this is that the subword decoding errors in the first stage would propagate to deteriorate the outcome of spoken term discovery in the second stage. The present study investigates the use of Siamese and Triplet networks in spoken term discovery. Siamese network has been commonly applied to pattern classification or matching problems when only weak labels are available. We propose to train a Siamese/Triplet network with a small dataset of matched and mismatched sequence pairs obtained and use the trained network to generate feature representations for unseen subword sequences. The training dataset is constructed based on hypothesized spoken term clusters from an baseline spoken term discovery system developed in our previous study. With the new feature representations learned by the Siamese/Triplet network, re-clustering of subword sequences is carried out to generate an improved set of discovered spoken terms.        Spoken term discovery aims to find and extract repetitively occurred sequential pattern from audio in an unsupervised manner.  There are different ways of implementation.  In general, a spoken term discovery system performs three tasks one after the other: segmentation, matching and clustering .   The repeated patterns are then clustered to form the spoken terms.  There are mainly two approaches to spoken term discovery. In the first approach, pattern discovery is done directly with acoustic features. Word-level speech segments are matched using sequence matching algorithms like segmental-DTW. The matching could be based on conventional frame-level features  or fixed-dimension segment representations . Another approach involves a two-stage process. Unsupervised subword modeling is first carried out with the untranscribed audio, resulting in a symbolic representation known as the pseudo transcription of speech. Sequential pattern discovery is then performed by local alignment or string matching and clustering of sequential patterns . The results of clustering could be corresponded to the discovered spoken terms in the given audio dataset.    Siamese neural network was proposed in . It consists of two identical sub-network components, which share the learnable parameters. Through the two sub-network components, Siamese neural network is trained to perform a designated classification task on a pair of data samples. The most common task is to determine whether the two input samples are from the same class or not. In other words, the exact class identities for individual training samples are not needed. The training of Siamese network requires relatively fewer training samples than conventional neural network classifiers . Siamese network is widely used in image processing. It is shown to have the ability of comparing samples from unseen classes in the problem of one shot classification .  Triplet network  is an extension of Siamese network. It consists of three identical sub-networks, which process 3 input samples in parallel, including one reference sample, one matched and one mismatched samples. The network is trained to capture the similarity between the matched sample and the reference and the dissimilarity between the mismatched sample and the reference.     Siamese network has been used in learning speech embeddings and it has shown to be able to learn effective subword units and term units representations . It has been shown that Siamese network is able to learn new representations from audio signals, which facilitate spoken word classification .  It is also able to generate effective representations for spoken term detection . While existing work assumes matched pair and mismatched pairs for training the Siamese network are available, one challenge in unsupervised spoken term discovery is that no information is given to the system except the recording only. In order to apply Siamese network in learning segment representations, reliable matched and mismatched pairs are required for training the network.   Relatively less work is done on unsupervised generation of matched and mismatched training pairs. There is work that identifies frame-level training samples. After segmentation, frames from same segments are treated as matched pairs, frames from adjacent segments are treated as mismatched pairs . There is also work that extracts training examples from available spoken term discovery system, with sampling based on distributions of speakers and matched/mismatched pairs .     閳ユ矾nsupervised Acoustic Segmentation and Clustering using Siamese Network Embeddings閳   Pairs of frames from the same segment are considered as matched pairs, while pairs of frames from adjacent segments are considered as mismatched pairs.  frame level  ---> match: from same segments, mismatch: adjacent segments   閳ユ藩ampling strategies in Siamese Networks for unsupervised speech representation learning閳   We use the orthographic transcription from word-level annotations to determine same and different pairs to train the siamese networks. In the fully unsupervised setting, we obtain pairs of same and different words from the Track 2 baseline of the 2015 ZeroSpeech challenge [3]: the Spoken Term Discovery system from [13]. We use both the original files from the baseline, and a rerun of the algorithm with systematic variations on its similarity threshold parameter   they only use one line to mention the similarity filter --> maybe can eleborate on it      In this study, we propose Universal MelGAN, a robust neural vocoder for high-fidelity synthesis in multiple domains. We solved the over-smoothing problem that causes a metallic sound, by attaching multi-resolution spectrogram discriminators to the model. Our model is stable while generating waveforms with fine-grained spectrograms in large footprint models. The evaluation results indicate that the proposed model achieved the highest MOS in most seen and unseen domain scenarios. The result demonstrates the universality of the proposed model. For more general use of the model, we will study a lightweight model in the future and apply the multi-band strategy to reduce the complexity while preserving the sound quality.    
","    Spoken term discovery aims to find and extract repetitively occurred sequential pattern from audio in an unsupervised manner.  There are different ways of implementation.  In general, a spoken term discovery system performs three tasks one after the other: segmentation, matching and clustering .   The repeated patterns are then clustered to form the spoken terms.  There are mainly two approaches to spoken term discovery. In the first approach, pattern discovery is done directly with acoustic features. Word-level speech segments are matched using sequence matching algorithms like segmental-DTW. The matching could be based on conventional frame-level features  or fixed-dimension segment representations . Another approach involves a two-stage process. Unsupervised subword modeling is first carried out with the untranscribed audio, resulting in a symbolic representation known as the pseudo transcription of speech. Sequential pattern discovery is then performed by local alignment or string matching and clustering of sequential patterns . The results of clustering could be corresponded to the discovered spoken terms in the given audio dataset.    Siamese neural network was proposed in . It consists of two identical sub-network components, which share the learnable parameters. Through the two sub-network components, Siamese neural network is trained to perform a designated classification task on a pair of data samples. The most common task is to determine whether the two input samples are from the same class or not. In other words, the exact class identities for individual training samples are not needed. The training of Siamese network requires relatively fewer training samples than conventional neural network classifiers . Siamese network is widely used in image processing. It is shown to have the ability of comparing samples from unseen classes in the problem of one shot classification .  Triplet network  is an extension of Siamese network. It consists of three identical sub-networks, which process 3 input samples in parallel, including one reference sample, one matched and one mismatched samples. The network is trained to capture the similarity between the matched sample and the reference and the dissimilarity between the mismatched sample and the reference.     Siamese network has been used in learning speech embeddings and it has shown to be able to learn effective subword units and term units representations . It has been shown that Siamese network is able to learn new representations from audio signals, which facilitate spoken word classification .  It is also able to generate effective representations for spoken term detection . While existing work assumes matched pair and mismatched pairs for training the Siamese network are available, one challenge in unsupervised spoken term discovery is that no information is given to the system except the recording only. In order to apply Siamese network in learning segment representations, reliable matched and mismatched pairs are required for training the network.   Relatively less work is done on unsupervised generation of matched and mismatched training pairs. There is work that identifies frame-level training samples. After segmentation, frames from same segments are treated as matched pairs, frames from adjacent segments are treated as mismatched pairs . There is also work that extracts training examples from available spoken term discovery system, with sampling based on distributions of speakers and matched/mismatched pairs .     闁炽儲鐭緉supervised Acoustic Segmentation and Clustering using Siamese Network Embeddings闁   Pairs of frames from the same segment are considered as matched pairs, while pairs of frames from adjacent segments are considered as mismatched pairs.  frame level  ---> match: from same segments, mismatch: adjacent segments   闁炽儲钘゛mpling strategies in Siamese Networks for unsupervised speech representation learning闁   We use the orthographic transcription from word-level annotations to determine same and different pairs to train the siamese networks. In the fully unsupervised setting, we obtain pairs of same and different words from the Track 2 baseline of the 2015 ZeroSpeech challenge [3]: the Spoken Term Discovery system from [13]. We use both the original files from the baseline, and a rerun of the algorithm with systematic variations on its similarity threshold parameter   they only use one line to mention the similarity filter --> maybe can eleborate on it",190
"  The natural language processing community has made tremendous progress  in using pre-trained language models to improve predictive accuracy . Models have now surpassed human performance on language understanding benchmarks such as SuperGLUE . However, studies have shown that these results are partially driven by these models detecting superficial cues that correlate well with labels but which may not be useful for the intended underlying task . This brittleness leads to overestimating model performance on the artificially constructed tasks and poor performance in out-of-distribution or adversarial examples.  A well-studied example of this phenomenon is the natural language inference dataset MNLI . The generation of this dataset led to spurious surface patterns that correlate noticeably with the labels.  highlight that negation words  are often associated with the contradiction label.  show that a model trained solely on the hypothesis, completely ignoring the intended signal, reaches strong performance. We refer to these surface patterns as dataset biases since the conditional distribution of the labels given such biased features is likely to change in examples outside the training data distribution .  A major challenge in representation learning for NLP is to produce models that are robust to these dataset biases. Previous work  has targeted removing dataset biases by explicitly factoring them out of models. These works explicitly construct a biased model, for instance, a hypothesis-only model for NLI experiments, and use it to improve the robustness of the main model. The core idea is to encourage the main model to find a different explanation where the biased model is wrong. During training, products-of-experts ensembling  is used to factor out the biased model.   While these works show promising results, the assumption of knowledge of the underlying dataset bias is quite restrictive. Finding dataset biases in established datasets is a costly and time-consuming process, and may require access to private details about the annotation procedure, while actively reducing surface correlations in the collection process of new datasets is challenging given the number of potential biases .  In this work, we explore methods for learning from biased datasets which do not require such an explicit formulation of the dataset biases. We first show how a model with limited capacity, which we call a weak learner, trained with a standard cross-entropy loss learns to exploit biases in the dataset. We then investigate the biases on which this weak learner relies and show that they match several previously manually identified biases. Based on this observation, we leverage such limited capacity models in a product of experts ensemble to train a more robust model and evaluate our approach in various settings ranging from toy datasets up to large crowd-sourced benchmarks: controlled synthetic bias setup , natural language inference  and extractive question answering .  Our contributions are the following:  we show that weak learners are prone to relying on shallow heuristics and highlight how they rediscover previously human-identified dataset biases;  we demonstrate that we do not need to explicitly know or model dataset biases to train more robust models that generalize better to out-of-distribution examples;  we discuss the design choices for weak learners and show trade-offs between higher out-of-distribution performance at the expense of the in-distribution performance.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%     Many studies have reported dataset biases in various settings. Examples include visual question answering , story completion , and reading comprehension . Towards better evaluation methods, researchers have proposed to collect ``challenge'' datasets that account for surface correlations a model might adopt . Standard models without specific robust training methods often drop in performance when evaluated on these challenge sets.  While these works have focused on data collection, another approach is to develop methods allowing models to ignore dataset biases during training. Several active areas of research tackle this challenge by adversarial training , example forgetting  and dynamic loss adjustment . Previous work  has shown the effectiveness of product of experts to train un-biased models.  In our work, we show that we do not need to explicitly model biases to apply these de-biasing methods and can use a more general setup than previously presented.  Orthogonal to these evaluation and optimization efforts, data augmentation has attracted interest as a way to reduce model biases by explicitly modifying the dataset distribution , either by leveraging human knowledge about dataset biases such as swapping male and female entities  or by developing dynamic data collection and benchmarking . Our work is mostly orthogonal to these efforts and alleviates the need for a human-in-the-loop setup which is common to such data-augmentation approaches.   Large pre-trained language models have contributed to improved out-of-distribution generalization . However, in practice, that remains a challenge in natural language processing  and our work aims at out-of-distribution robustness without significantly compromising in-distribution performance.  Finally, as we were preparing this manuscript for submission, we became aware of a parallel work  which presents a related de-biasing method leveraging shallow models's mistakes without the need to explicitly model dataset biases. Our approach is different in several ways, in particular we advocate for using limited capacity weak learner while  uses the same architecture as the robust model trained on a few thousands examples. We investigated the trade-off between learner's capacity and resulting performances as well as the resulting few-shot learning regime in the limit of a high capacity weak model.                                       \subfile{sections/conclusions}   
","   Many studies have reported dataset biases in various settings. Examples include visual question answering , story completion , and reading comprehension . Towards better evaluation methods, researchers have proposed to collect ``challenge'' datasets that account for surface correlations a model might adopt . Standard models without specific robust training methods often drop in performance when evaluated on these challenge sets.  While these works have focused on data collection, another approach is to develop methods allowing models to ignore dataset biases during training. Several active areas of research tackle this challenge by adversarial training , example forgetting  and dynamic loss adjustment . Previous work  has shown the effectiveness of product of experts to train un-biased models.  In our work, we show that we do not need to explicitly model biases to apply these de-biasing methods and can use a more general setup than previously presented.  Orthogonal to these evaluation and optimization efforts, data augmentation has attracted interest as a way to reduce model biases by explicitly modifying the dataset distribution , either by leveraging human knowledge about dataset biases such as swapping male and female entities  or by developing dynamic data collection and benchmarking . Our work is mostly orthogonal to these efforts and alleviates the need for a human-in-the-loop setup which is common to such data-augmentation approaches.   Large pre-trained language models have contributed to improved out-of-distribution generalization . However, in practice, that remains a challenge in natural language processing  and our work aims at out-of-distribution robustness without significantly compromising in-distribution performance.  Finally, as we were preparing this manuscript for submission, we became aware of a parallel work  which presents a related de-biasing method leveraging shallow models's mistakes without the need to explicitly model dataset biases. Our approach is different in several ways, in particular we advocate for using limited capacity weak learner while  uses the same architecture as the robust model trained on a few thousands examples. We investigated the trade-off between learner's capacity and resulting performances as well as the resulting few-shot learning regime in the limit of a high capacity weak model.",191
" Topic models  have been popularly used to extract abstract topics which occur commonly across documents in a corpus in the field of Natural Language Processing. Each topic is a group of semantically coherent words that represent a common concept. In addition to gaining insights from unstructured texts, topic models have been used in several tasks of practical importance such as learning text representations for document classification , keyphrase extraction , review understanding for recommendations in e-commerce domain , semantic similarity detection between texts  etc.  % in order to make topic sampling distribution converge to the desired posterior distribution  Early popular works on topic discovery include statistical methods such as Latent Dirichlet Allocation   which approximates each topic as a probability distribution over word vocabulary and performs approximate inference over document-topic and topic-word distributions through Variational Bayes . This was followed by a modified inference algorithm - Collapsed Gibbs sampling  that follows Markov Chain Monte Carlo  . However, these methods require an expensive iterative inference step which has to be performed for each document. This was circumvented through introduction of deep neural networks  and emergence of Variational Autoencoders   in particular, where variational inference can be performed in single forward pass.  % while estimating the posterior distribution. % Laplace approximation of  % The re-parameterisation trick of VAEs allows to perform variational inference in a differentiable manner while training the neural network.  Such neural variational inference based topic models  outperformed the traditional probabilistic sampling methods. Broadly, they model a document as Bag-of-Words  determined on the basis of frequency count of each vocabulary token in the given document. The BoW input is processed through an MLP followed by variational inference  which samples a latent document-topic vector. A decoder network then reconstructs original BoW using latent document-topic vector which allows it to capture relationship between document-topic and topic-word distributions. VAE family of neural topic models can be categorised on the basis of prior enforced on latent document-topic distribution. Methods such as NVDM , NTM-R , NVDM-GSM  use Gaussian prior. NVLDA and ProdLDA  use Dirichlet prior approximation which enables model to capture that a document stems from sparse set of topics.  % and perform better by providing more coherent topics compared to Gaussian prior.  % in order to capture latent document-topic distribution,  % The context vector obtained as a result of attention is used to perform variational inference %  and capture semantics effectively  % which can further help in inferring latent document-topic vector % as carried  in usual VAE based topic models  % using the final LSTM state and the outputs corresponding  While the main focus of previous neural topic models has been to enforce suitable priors, little effort has been spent on explicitly improving the document encoding framework in order to capture document semantics better. In this work, we build upon VAE based topic model using laplace approximation to Dirichlet prior and propose a novel framework where we model the input document as a sequence of tokens. The sequence is processed through an LSTM  that allows it to encode the sequential order which does not remain preserved in BoW. To allow the model to focus on specific parts in the document, we use an attention mechanism  to attend at different document tokens. We hypothesise that topic-word distribution being learned by the model can be factored in the attention mechanism to enable the model to attend on tokens which convey topic related information and cues. We validate our hypothesis and propose TAN-NTM: Topic Attention Networks for Neural Topic Modeling which performs attention efficiently in a topic guided manner. We perform separate attention for each topic using its corresponding word probability distribution and obtain topic-wise context vectors. The context vectors are then composed using topic weights which represent proportion of each topic present in a given document. These topic weights are obtained using the learned token embedding and topic-word distribution. The final composed context vector is then used to perform variational inference followed by BoW decoding. We perform extensive ablations to compare different ways of composing topic-wise context vectors.  % and averages the coherence score over the topics % generated by the model   In order to evaluate our approach, we estimate commonly used NPMI coherence  which measures the extent to which most probable words in a topic are semantically related to each other. Using this metric, we compare our TAN-NTM model with several previous state-of-the-art topic models  outperforming them significantly over 4 benchmark datasets of varying scale and complexity - 20NewsGroup  , Yelp Review Polarity, DBpedia  and AGNews . We demonstrate the efficacy of our model in learning better document feature representations and latent document-topic vectors by achieving higher document classification accuracy over baseline topics models. Furthermore, topic models have previously been used to improve supervised keyphrase generation . We show that our proposed framework can be adapted to modify the topic model and further improve keyphrase generation achieving SOTA performance on StackExchange and Weibo datasets. Our contributions can be summarised as:       %   Early topic models include Bayesian methods such as Latent Semantic Analysis   and Latent Dirichlet Allocation  . Recent development of neural networks paved path for Variational Autoencoders  . The VAE-based methods use a prior distribution to approximate the posterior for latent space and compute the Evidence Lower Bound  using the reparametrization trick. Neural Variational Document Model   employ the multivariate gaussian as the prior while Gaussian Softmax Model   provides parameterizable distributions. NTM-R  uses pre-trained word embeddings for coherence regularization.    Since gaussian prior seemed inappropriate for modeling document distribution, NVLDA and ProdLDA  introduce dirichlet prior through laplace approximation, using logistic-normal distribution. ProdLDA uses product of experts on individual words distribution while NVLDA doesn't    instead of actual topic model coherence in the loss function.   A new approach,   based new topic modeling method     The authors propose a Generative Adversarial Network   and Maximum Mean Discrepancy  based training for distribution matching.    With the introduction of   an alternative to the VAE where  were used to prove the superiority of AAE over VAE    using dirichlet prior   uses a dirichlet prior and  With introduction of Wasserstein Autoencoders   that proved superiority of Adversarial Autoencoders   over VAEs, non-variational inference based topic model Wasserstein-LDA   was introduced. This is an MMD  based WAE framework that uses information diffusion kernel  and Dirichlet prior. However, WAE-MMD topic models rely heavily on choice of kernel function. Adversarial Topic Model   was proposed based on GAN  but it cannot infer document-topic distribution. A major advantage of W-LDA over ATM is distribution matching in the document-topic space, hence we use W-LDA as one of our baselines. Bidirectional Adversarial Topic model   employs a bilateral transformation between document-word and document-topic distribution. The input format is BoW so to incorporate relation between words, word embedding is used by discriminator.    and gaussian      rather than dirichlet in LDA-based models   as mentioned in a new topic model named GNB-NTM     Traditional LDA based models are challenged by  pre-trained word embeddings combined with weighted clustering and followed by reranking in .    the distributed dependencies between variational inference framework and mixed counting models    future problem to be addressed.  Some works have attempted to use other distributions such as beta . iDocNade proposed by  is a neural autoregressive topic model for sparse data and short text utilizing the pre-trained embeddings as distributional prior. However, iDocNade topic model performs inferior than ProdLDA in terms of coherence. Gamma Negative Binomial-Neural Topic Model   is one of the most recent neural variational topic model involving reparameterization of Gamma distribution and Gaussian approximation of Poisson distribution. GNB-NTM combines mixed counting models and neural variational inference. However, effectively merging them still remains a challenge. Recently, Reinforcement Learning  has also been employed for topic modeling. VTMRL  incorporates topic coherence as a reward signal to guide learning of a VAE-based model.     A prominent advantage of RL based methods is the automatic separation of background words, thus eliminating the pre-processing step of filtering words for building vocabulary.        in the context of unsupervised neural topic models. Variational Topic Model with Reinforcement Learning     mentioned VAE and AAE based methods     Sequence models like recurrent neural network  has seen a massive surge in research, and thus, attention mechanisms are widely used in different research problems in the literature.    the order of tokens in the input sequence    One of the machine translation methods, Bahdanau Attention  is quite successful in attending important key-words. We use this as an inspiration to propose our self-attention mechanism for the topic model encoder.   However, both papers did not compare their model with other topic models in terms of coherence.  .  proposed    as introduced to integrate the merits of RNNs and topic models, thus capturing the semantic meaning relating words in a document    and shows better topic clustering ability than traditional topic models  Despite substantial progress in topic modeling, very little importance has been given to learning improved document encoding and feature representations. All the previous methods use BoW as input. To the best of our knowledge, our proposed TAN-NTM is the first framework leveraging the sequence of tokens as input for topic modeling to perform topic guided attention on tokens in an efficient manner to learn document-topic distribution in latent space effectively.    However, sparsity in social media language limits their direct application.    architecture can be divided into two modules:   with BoW input  TopicRNN  employs conventional BoW topic model to assist a sequence RNN based model extracting improved text features for performing sentiment analysis and word prediction tasks. LSTM-Topic Matrix Factorization   model combines features from text processed sequentially through an LSTM with BoW topic model for review recommendation. Another key application of topic models is supervised keyphrase generation. Some of the existing neural keyphrase generation methods include SEQ-TAG  based on sequence tagging, SEQ2SEQ-CORR  based on seq2seq model without copy mechanism and SEQ2SEQ-COPY  which additionally uses copy mechanism. Topic-Aware Keyphrase Generation   is a seq2seq based neural keyphrase generation framework for social media language. TAKG uses a neural topic model inspired by NVDM-GSM  and a keyphrase generation  module which is conditioned on latent document-topic vector from the topic model. We adapt our proposed topic model to TAKG to improve keyphrase generation and discuss it in detail later in Experiments section.     We shall briefly discuss the architecture of ProdLDA, a SOTA VAE   , which represents the mean of all possible document reconstructions given the sampled     We have presented an effective method for training models robust to dataset biases. Leveraging a weak learner with limited capacity and a modified product of experts training setup, we show that dataset biases do not need to be explicitly known or modeled to be able to train models that can generalize significantly better to out-of-distribution examples. We discuss the design choices for such weak learner and investigate how using higher-capacity learners leads to higher out-of-distribution performance and a trade-off with in-distribution performance. We believe that such approaches capable of automatically identifying and mitigating datasets bias will be essential tools for future bias-discovery and mitigation techniques.                                                                            \clearpage  
"," Early topic models include Bayesian methods such as Latent Semantic Analysis   and Latent Dirichlet Allocation  . Recent development of neural networks paved path for Variational Autoencoders  . The VAE-based methods use a prior distribution to approximate the posterior for latent space and compute the Evidence Lower Bound  using the reparametrization trick. Neural Variational Document Model   employ the multivariate gaussian as the prior while Gaussian Softmax Model   provides parameterizable distributions. NTM-R  uses pre-trained word embeddings for coherence regularization.    Since gaussian prior seemed inappropriate for modeling document distribution, NVLDA and ProdLDA  introduce dirichlet prior through laplace approximation, using logistic-normal distribution. ProdLDA uses product of experts on individual words distribution while NVLDA doesn't    instead of actual topic model coherence in the loss function.   A new approach,   based new topic modeling method     The authors propose a Generative Adversarial Network   and Maximum Mean Discrepancy  based training for distribution matching.    With the introduction of   an alternative to the VAE where  were used to prove the superiority of AAE over VAE    using dirichlet prior   uses a dirichlet prior and  With introduction of Wasserstein Autoencoders   that proved superiority of Adversarial Autoencoders   over VAEs, non-variational inference based topic model Wasserstein-LDA   was introduced. This is an MMD  based WAE framework that uses information diffusion kernel  and Dirichlet prior. However, WAE-MMD topic models rely heavily on choice of kernel function. Adversarial Topic Model   was proposed based on GAN  but it cannot infer document-topic distribution. A major advantage of W-LDA over ATM is distribution matching in the document-topic space, hence we use W-LDA as one of our baselines. Bidirectional Adversarial Topic model   employs a bilateral transformation between document-word and document-topic distribution. The input format is BoW so to incorporate relation between words, word embedding is used by discriminator.    and gaussian      rather than dirichlet in LDA-based models   as mentioned in a new topic model named GNB-NTM     Traditional LDA based models are challenged by  pre-trained word embeddings combined with weighted clustering and followed by reranking in .    the distributed dependencies between variational inference framework and mixed counting models    future problem to be addressed.  Some works have attempted to use other distributions such as beta . iDocNade proposed by  is a neural autoregressive topic model for sparse data and short text utilizing the pre-trained embeddings as distributional prior. However, iDocNade topic model performs inferior than ProdLDA in terms of coherence. Gamma Negative Binomial-Neural Topic Model   is one of the most recent neural variational topic model involving reparameterization of Gamma distribution and Gaussian approximation of Poisson distribution. GNB-NTM combines mixed counting models and neural variational inference. However, effectively merging them still remains a challenge. Recently, Reinforcement Learning  has also been employed for topic modeling. VTMRL  incorporates topic coherence as a reward signal to guide learning of a VAE-based model.     A prominent advantage of RL based methods is the automatic separation of background words, thus eliminating the pre-processing step of filtering words for building vocabulary.        in the context of unsupervised neural topic models. Variational Topic Model with Reinforcement Learning     mentioned VAE and AAE based methods     Sequence models like recurrent neural network  has seen a massive surge in research, and thus, attention mechanisms are widely used in different research problems in the literature.    the order of tokens in the input sequence    One of the machine translation methods, Bahdanau Attention  is quite successful in attending important key-words. We use this as an inspiration to propose our self-attention mechanism for the topic model encoder.   However, both papers did not compare their model with other topic models in terms of coherence.  .  proposed    as introduced to integrate the merits of RNNs and topic models, thus capturing the semantic meaning relating words in a document    and shows better topic clustering ability than traditional topic models  Despite substantial progress in topic modeling, very little importance has been given to learning improved document encoding and feature representations. All the previous methods use BoW as input. To the best of our knowledge, our proposed TAN-NTM is the first framework leveraging the sequence of tokens as input for topic modeling to perform topic guided attention on tokens in an efficient manner to learn document-topic distribution in latent space effectively.    However, sparsity in social media language limits their direct application.    architecture can be divided into two modules:   with BoW input  TopicRNN  employs conventional BoW topic model to assist a sequence RNN based model extracting improved text features for performing sentiment analysis and word prediction tasks. LSTM-Topic Matrix Factorization   model combines features from text processed sequentially through an LSTM with BoW topic model for review recommendation. Another key application of topic models is supervised keyphrase generation. Some of the existing neural keyphrase generation methods include SEQ-TAG  based on sequence tagging, SEQ2SEQ-CORR  based on seq2seq model without copy mechanism and SEQ2SEQ-COPY  which additionally uses copy mechanism. Topic-Aware Keyphrase Generation   is a seq2seq based neural keyphrase generation framework for social media language. TAKG uses a neural topic model inspired by NVDM-GSM  and a keyphrase generation  module which is conditioned on latent document-topic vector from the topic model. We adapt our proposed topic model to TAKG to improve keyphrase generation and discuss it in detail later in Experiments section.     We shall briefly discuss the architecture of ProdLDA, a SOTA VAE   , which represents the mean of all possible document reconstructions given the sampled",192
" In recent years, smart devices with built-in personal assistants like Google Assistant and Siri are becoming omnipresent. Behind these intelligent systems, a key question is how to identify the underlying intent of a user utterance, which has triggered a large amount of work on intent detection . Most existing intent detection systems are built on deep learning models trained on large-scale annotated data. However, as user demands and the functions of smart devices continue to grow, collecting supervised data for every new intent becomes time-consuming and labor-intensive.  To address this issue, some studies tackle intent detection in the zero-shot learning  manner, attempting to utilize the learned knowledge of seen classes to help detect unseen classes. The recent methods of zero-shot intent detection  can be roughly divided into two categories: The first category , referred to as the transformation-based methods, utilizes word embeddings of label names to establish a similarity matrix, which is then used to transfer the prediction space of seen intents to unseen intents. Another line of work is based on the compatibility-based methods , which aims to encode the label names and utterances into representations in the same semantic space and then calculate their similarity. In both kinds of methods, a critical problem is learning intent representations. However, most existing ZSID methods are class-inductive, which relies entirely on labeled data from seen intents in the training stage. Consequently, the representations of unseen intents cannot be learned, resulting in two limitations.  First, the ZSID methods are not good at modeling the relationship between seen and unseen intents. For the transformation-based methods, when the label names are given in the form of raw phrases or sentences, word embeddings of label names are inadequate to associate the connections between seen and unseen intents. For example, 閳ユ窂ookRestaurant閳 is similar to 閳ユ珐ateBook閳 when measured by word embeddings, as they share the word 閳ユ窂ook閳 . However, the meaning of these two intents are not that relevant. % As a result, the computed similarity matrix is inadequate in associating the connections between seen and unseen intents .  For the compatibility-based methods, they minimize the similarity between seen intent samples and seen label names in a shared semantic space, and directly transfer it to detect unseen intents. Since the unseen intent representations are not learned, they might be entangled with the representations of seen intents. This can severely hurt the accuracy of the predicted label-utterance similarity, especially when the expressions of utterances are diverse. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Second, the vanilla ZSL methods are not applicable to generalized intent detection . Compared with the ZSL setting , which assumes that the models are only presented with utterances from unseen classes at test time, GZSID requires the model to detect both seen and unseen intents. In GZSID, existing ZSL models usually suffer from the dubbed domain shift  problem, in which utterances from unseen intents are almost always mistakenly classified into seen intents.   Unlike the class-inductive methods, class-transductive ZSL uses semantic information about the unseen classes for model training . In the context of intent detection, the label name provides a proper sketch of the intent meaning. Motivated by this, we propose to utilize label names of the unseen intents to learn disentangled intent representations . Specifically, we include the unseen intents into the prediction space during training, with the label names serving as the pseudo utterances. This allows the model to learn the boundary of each seen and unseen class in the semantic space. Under this framework, we introduce an assistant task that forces the model to find the distinction between seen and unseen intents, thereby alleviating the domain-shift problem. On this basis, we refine the word embedding based similarity matrix by averaging the representations of all corresponding  utterances and  label names. As a result, we can better capture the intent meanings and the similarity matrix reflects more accurate intent connections.   In summary, our contribution is three-fold:  We believe that the potential of class-transductive ZSL  in intent detection is still not fully exploited, to encourage more related studies in the future, we will release our codes and data.      \paragraph{Intent Detection}   Intent detection belongs to the family of text classification.    Early work  tackles this task with the Support Vector Machine.  In recent years, deep neural network based methods are showing great success in intent detection . However, their success primarily relies on large amount of annotated data.   \paragraph{Zero-shot Learning}  Zero-shot learning, as proposed by \citet{Christoph}, requires the models to distinguish among unseen classes without annotated data. In computer vision, ZSL is a well-developed sub-field, where a common approach is to relate unseen classes with seen classes through visual attributes  or word2vec representations of the class names . Inspired by these methods, \citet{XiaZYCY18,liu2019reconstructing} calculate the inter-intent similarity based on the word embeddings of intent labels. The similarity scores are then used to transform the predictions from seen intents to unseen intents. Compatibility models  attempt to learn a shared semantic space for label names and utterances, and then perform intent detection by measuring the intent-utterance similarity in this space.    These ZSID methods break the bottleneck of traditional intent detection systems, while they are inadequate in associating the connection between seen and unseen intents.  There are also studies resorting to external knowledge, e.g., label ontologies  or human-defined attributes , which, however, are laborious to obtain.   \paragraph{Generalized Zero-shot Learning} \citet{frome2013devise,NorouziMBSSFCD13,mensink2012metric} extended ZSL by including seen classes into the prediction space at test time. However, the test samples still only come from unseen classes. In contrast, \citet{scheirer2012toward} allowed test sample to come from seen classes. CMT  proposed a two-stage procedure for GZSL, which first determines whether a test sample belongs to seen classes or unseen classes, and then apply the corresponding classifier.    This two-stage methods is not applicable to the case of the variety of unseen classes.  Similarly, \citet{zhang2016online} estimated the probability of each sample coming from an unseen class. In the task of intent detection, ReCapsNet-ZS  enhanced IntentCapsNet  in GZSID by modeling the correlation between the dimensions of word embeddings, which can find a more accurate connection between seen and unseen intents. On the basis of ReCapsNet-ZS, \citet{Guangfeng} proposed a two-stage GZSID method that combines unknown intent detection   and ZSID, which successfully resolves the domain-shift problem. However, this is at the cost of the performance in seen intents, which is the majority in real-world applications.    However, the unseen intent representations are still not learned by the aforementioned GZSL methods in training.  \paragraph{Class-transductive Zero-shot Learning} Class-transductive zero-shot learning utilizes semantic information  about the unseen classes in the training stage. In the CV field, class-transductive methods are used to infer the relationship between seen and unseen classes  or directly predict the parameters of unseen intent classifiers . In comparison, DIR uses the unseen label names as training instances to learn unseen intent representations, which takes advantage of the fact that intent label names and utterances both come from the textual domain.                                                                                                                      In this paper, we investigated two approaches for cross-lingual neural discourse parsing. Experimental results show that both utilizing cross-lingual representation and adopting segment-level translation contribute to obtaining state-of-the-art performance on various treebanks. Moreover, monolingual models can also benefit from cross-lingual training by introducing data from more domains. For future work, we consider conducting domain adaption via few-shot learning to make our approach more generalizable.  
"," \paragraph{Intent Detection}   Intent detection belongs to the family of text classification.    Early work  tackles this task with the Support Vector Machine.  In recent years, deep neural network based methods are showing great success in intent detection . However, their success primarily relies on large amount of annotated data.   \paragraph{Zero-shot Learning}  Zero-shot learning, as proposed by \citet{Christoph}, requires the models to distinguish among unseen classes without annotated data. In computer vision, ZSL is a well-developed sub-field, where a common approach is to relate unseen classes with seen classes through visual attributes  or word2vec representations of the class names . Inspired by these methods, \citet{XiaZYCY18,liu2019reconstructing} calculate the inter-intent similarity based on the word embeddings of intent labels. The similarity scores are then used to transform the predictions from seen intents to unseen intents. Compatibility models  attempt to learn a shared semantic space for label names and utterances, and then perform intent detection by measuring the intent-utterance similarity in this space.    These ZSID methods break the bottleneck of traditional intent detection systems, while they are inadequate in associating the connection between seen and unseen intents.  There are also studies resorting to external knowledge, e.g., label ontologies  or human-defined attributes , which, however, are laborious to obtain.   \paragraph{Generalized Zero-shot Learning} \citet{frome2013devise,NorouziMBSSFCD13,mensink2012metric} extended ZSL by including seen classes into the prediction space at test time. However, the test samples still only come from unseen classes. In contrast, \citet{scheirer2012toward} allowed test sample to come from seen classes. CMT  proposed a two-stage procedure for GZSL, which first determines whether a test sample belongs to seen classes or unseen classes, and then apply the corresponding classifier.    This two-stage methods is not applicable to the case of the variety of unseen classes.  Similarly, \citet{zhang2016online} estimated the probability of each sample coming from an unseen class. In the task of intent detection, ReCapsNet-ZS  enhanced IntentCapsNet  in GZSID by modeling the correlation between the dimensions of word embeddings, which can find a more accurate connection between seen and unseen intents. On the basis of ReCapsNet-ZS, \citet{Guangfeng} proposed a two-stage GZSID method that combines unknown intent detection   and ZSID, which successfully resolves the domain-shift problem. However, this is at the cost of the performance in seen intents, which is the majority in real-world applications.    However, the unseen intent representations are still not learned by the aforementioned GZSL methods in training.  \paragraph{Class-transductive Zero-shot Learning} Class-transductive zero-shot learning utilizes semantic information  about the unseen classes in the training stage. In the CV field, class-transductive methods are used to infer the relationship between seen and unseen classes  or directly predict the parameters of unseen intent classifiers . In comparison, DIR uses the unseen label names as training instances to learn unseen intent representations, which takes advantage of the fact that intent label names and utterances both come from the textual domain.",193
" Multi-turn open-domain dialogue modeling is an active research topic in the field of natural language processing.  However, generating a coherent and informative response for a given dialogue context remains a challenge. % However, it is still challenging for dialogue models to generate a coherent and informative response for a given dialogue context. %Research in this domain mainly addresses the following two questions: 1) How can we learn to represent the context? 2) In the presence of context representation, how can we infer the distribution of the response?  A critical challenge is the learning of rich and robust context representations of dialogue utterances~, namely the challenge of encoding a dialogue context into a vector that adequately captures the semantics . % A major challenge in this domain is to learn rich and robust context representations of dialogue utterances~, namely the challenge of encoding a dialogue context into a vector that adequately captures the semantics .  Large-scale pre-training language models using Transformer-based architectures have recently achieved remarkable successes in a variety of NLP tasks~. % Recently, large-scale pre-training language models using Transformer-based architectures have achieved remarkable successes in a variety of NLP tasks~.  As such, there are increasingly work that aims to use pre-training language models for conversation modeling~. For example, DialoGPT~ extends the GPT-2~ to generate conversation responses on large-scale dialogue corpus. Meena~ trains a sequence-to-sequence model~ with the Evolved Transformer~ on large-scale multi-turn conversations.  Blender, developed by Facebook, provides recipes for building open-domain chatbots that perform well in human evaluations~.  However, existing pre-training conversation models usually view the dialogue context as a linear sequence of tokens and learns to generate the next word through token-level self-attention.  One issue of this approach is that the high-level relationships between utterances are harder to capture using word-level semantics. % One issue of this approach is that the relationships between utterances are scattered into individual words, hindering the capturing of discourse-level coherence.  For example, the discourse-level relationship between the utterances ``coffee please'' and ``here you are''  is apparent, but word-level comparisons, such as coffee, you and please, are, obscures the high-level relationship. % For example, the utterance ``coffee please'' and ``here you are'' in Figure have a strong certain relationship, by contrast, pairs of individual words in these two utterances such as coffee, you and please, are have obscure correlations. Furthermore, this full pairwise attention is inefficient since it requires each word in the context and the decoder to interact with all other words regardless of their distances and semantic units. % Furthermore, the full pairwise attention is inefficient since it requires each word in the context and the decoder to interact with all other words regardless of their distances and semantic units.  To alleviate the issues above, we present DialogBERT, a novel conversational response generation model.  % To alleviate the aforementioned issues, we present DialogBERT, a novel conversational response generation model.  DialogBERT employs a hierarchical Transformer architecture to represent the dialogue context.  It first encodes dialogue utterances through a Transformer encoder and then encodes the resulting utterance vectors using a discourse-level Transformer to obtain a representation of the entire dialogue context.  To efficiently capture discourse-level coherence among utterances, we propose two training objectives in analogy to the original BERT training: 1) masked context regression, which masks a randomly-selected utterance and predicts the encoding vector for the masked utterance directly; and 2) distributed utterance order ranking, which %reconstructs the order of utterances that belong to the same dialog context   organizes randomly shuffled utterances of a conversation into a coherent dialogue context  through a Learning-to-Rank~ neural network.  We evaluate DialogBERT on popular multi-turn conversation datasets, namely Weibo, MultiWOZ and DailyDialog.  Results show that DialogBERT outperforms baselines in terms of perplexity, BLEU, and NIST. Human evaluation supports the superiority of our approach in capturing discourse-level semantics and generating more plausible dialogue responses.  %Our contributions can be summarized as follows: %     This work is closely related to  pre-trained language models,  pre-trained models for conversations, and  adopting auxiliary multi-task objectives for improving pre-trained language models.  Pre-trained Language Models.  The current paradigm has gradually evolved from word vectors~ and contextualized word embedding models~. Recent works have explored various architecture choices and training objectives for large-scale pre-trained language models~ based on Transformers~.  A recent work proposed using the denoising autoencoder framework with composite corruption schemes~.  Pre-trained Models for Dialogue Generation.  Recent advances in pre-trained language models have spurred success in dialogue response generation. Specifically, \citet{budzianowski2019hello} explored the use of pre-trained language Transformers for task-oriented dialogues. \citet{wolf2019transfertransfo} also proposed adopting auxiliary unsupervised objectives for pre-training dialogue language models. Extracting language representations from pre-trained transformers for dialogue tasks has been explored in \citet{henderson2019convert}.  Another important line of work pertains to designing specific Transformer-based architectures that captures dialogue structures and directly pre-training these architectures on dialogue corpora. \citet{mehri2019pretraining} proposed a Transformer-based hierarchical model and various unsupervised objectives for pre-training contextual semantics of dialogue utterances. DialogBERT differs from the methods proposed by \citet{mehri2019pretraining} in both the architecture and objectives.  DialogBERT contains a context encoder that models the discourse coherence, while \citet{mehri2019pretraining} proposed optimizing the utterance encoder directly.   Lastly, an emerging trend in dialogue generation explores the feasibility of directly pre-training Transformer-based language modeling architectures on large-scale dialogue corpora. Recent works, such as DialoGPT, Meena, and Blender, demonstrate strong generation performances attainable from training Transformer-based language generators on open-domain discourses.   Multi-task Learning for Pre-training. Our work is also profoundly related to auxiliary multi-task learning, which augments the pre-training of language models. The common theme is to guide the language modeling Transformers with complementing objectives.  One way is to augment language modeling with annotation rationales~. ERNIE2~ is a continual multi-task learning framework for language understanding that combines unsupervised and supervised objectives. HIBERT~ has been proposed to model documents using a hierarchical BERT architecture trained with the masked sentence decoding scheme, where the goal is to predict the entire erased sentence. Our work differs from HIBERT in that  we directly match the context sensitive sentence representations with the real utterance encoding and  we propose a novel objective that trains the model to predict utterance orders. ELECTRA~ explores combining discriminative and generative objectives for learning language modeling.     @KMY: I will check this section tomorrow.    In this paper, we propose a class-transductive framework to overcome the limitations of existing ZSID models. The framework learns disentangled representations for unseen intents by including them into the prediction space during training. Under the DIR framework, we present a multi-task learning objective in the training stage to encourages the model to learn the distinctions between unseen and seen intents. In the inference stage, we develop a similarity scorer, which can better associate the inter-intent connections based on the learned representations. Experiments on two benchmarks show that DIR is effective and robust, which can bring considerable improvement to ZSID systems with different zero-shot learning strategies and backbone networks.    
"," This work is closely related to  pre-trained language models,  pre-trained models for conversations, and  adopting auxiliary multi-task objectives for improving pre-trained language models.  Pre-trained Language Models.  The current paradigm has gradually evolved from word vectors~ and contextualized word embedding models~. Recent works have explored various architecture choices and training objectives for large-scale pre-trained language models~ based on Transformers~.  A recent work proposed using the denoising autoencoder framework with composite corruption schemes~.  Pre-trained Models for Dialogue Generation.  Recent advances in pre-trained language models have spurred success in dialogue response generation. Specifically, \citet{budzianowski2019hello} explored the use of pre-trained language Transformers for task-oriented dialogues. \citet{wolf2019transfertransfo} also proposed adopting auxiliary unsupervised objectives for pre-training dialogue language models. Extracting language representations from pre-trained transformers for dialogue tasks has been explored in \citet{henderson2019convert}.  Another important line of work pertains to designing specific Transformer-based architectures that captures dialogue structures and directly pre-training these architectures on dialogue corpora. \citet{mehri2019pretraining} proposed a Transformer-based hierarchical model and various unsupervised objectives for pre-training contextual semantics of dialogue utterances. DialogBERT differs from the methods proposed by \citet{mehri2019pretraining} in both the architecture and objectives.  DialogBERT contains a context encoder that models the discourse coherence, while \citet{mehri2019pretraining} proposed optimizing the utterance encoder directly.   Lastly, an emerging trend in dialogue generation explores the feasibility of directly pre-training Transformer-based language modeling architectures on large-scale dialogue corpora. Recent works, such as DialoGPT, Meena, and Blender, demonstrate strong generation performances attainable from training Transformer-based language generators on open-domain discourses.   Multi-task Learning for Pre-training. Our work is also profoundly related to auxiliary multi-task learning, which augments the pre-training of language models. The common theme is to guide the language modeling Transformers with complementing objectives.  One way is to augment language modeling with annotation rationales~. ERNIE2~ is a continual multi-task learning framework for language understanding that combines unsupervised and supervised objectives. HIBERT~ has been proposed to model documents using a hierarchical BERT architecture trained with the masked sentence decoding scheme, where the goal is to predict the entire erased sentence. Our work differs from HIBERT in that  we directly match the context sensitive sentence representations with the real utterance encoding and  we propose a novel objective that trains the model to predict utterance orders. ELECTRA~ explores combining discriminative and generative objectives for learning language modeling.     @KMY: I will check this section tomorrow.",194
" % \rev{@Ileana: this is an example on how to indicate changes in the text, based on the revision.} % \todo[inline]{we need to add color bars on figures, as promised to the reviewers}    Given enough computational power, the scalability of the attention mechanism~ will allow for building ever larger Natural Language Processing  models with billions of parameters . While impressive, these advances also pose a responsibility to the NLP community to interpret the behavior of the hundreds of attention heads in a single model, and potentially to reduce the number of computations. Responding to this challenge, previous work has taken pioneering steps to discover and to explain the sparseness in the attention patters. Here, we argue that as the number of heads grows in the range of thousands, automatic measures would be needed to discover and to impose sparseness to such models.  We introduce a simple task-agnostic data-informed pruning method for attention mechanisms: Attention Pruning. We train Transformer-based models and we analyze global observed attention patterns, averaged over all input sequences in the train set, in order to identify and to remove weak connections between the input tokens. Following \citet{lottery}, we then retrain these models, enforcing sparseness through masking, and we demonstrate that attention mechanisms incorporate extraneous connections between the input tokens: we obtain comparable  % \question{or even marginally better performance} performance while using sparse attention patterns for NLP tasks such as language and sequence-to-sequence  modelling, as well as %Natural Language  Inference . \rev{prediction on GLUE tasks. Figure summarizes the impact of using our pruning method on standard NLP tasks.}     These global sparseness patterns could help improve both interpretability and inference-time computational efficiency for widely-used attention models. Our contributions are as follows:    % The rest of the paper is organized as follows: In Section, we present related work. In Section, we introduce the details behind our attention pruning method. In Section, we apply AP to experiments with language modelling. In Section, we apply AP for seq2seq modelling on machine translation tasks. In Section, we extend our machine translation experiments to demonstrate that AP is compatible with -entmax regularization~, which is another promising sparseness technique. In Section, we study the effect of AP with BERT on the GLUE benchmark. % % Section. In Section we discuss theoretically how our pruned Transformers could yield speedups in terms of MACs.  % In Section, we discuss the hardware efficiency of AP and its promise for speeding up modelling for really long sequences. In Section, we conclude and we point to promising directions for future work.      There are several fruitful directions for research focused on improving the computational efficiency and the interpretability of the attention mechanism. Sparseness plays a central role in all of them, as simple attention mechanisms inherently scale quadratically with sequence length and assign non-zero correlation between any two input tokens.   One line of research is that of incorporating sparseness by restricting attention patterns to pre-defined ones. \citet{Child2019GeneratingLS} introduced two sparse matrix factorizations, which reduce the computational complexity from  to   and successfully applied this method to language modelling tasks. \citet{reformer} leveraged algorithmic insights to create a sparse attention mechanism with  computational complexity: they used local sensitivity hashing to cluster tokens that should attend to each other and only compute attention within tokens from the same chunks. More directly related to our work is that of \citet{longformer} who looked directly at sparsifying attention patterns rather than at the underlying  matrix factorization and reduced the computational complexity of attention from  to  using GPU kernels .  One key difference between these approaches and ours is that we do not impose any a priori restrictions on the type of attention patterns we can generate, and we show theoretical computational gains with specialized GPU implementations rather than reduce the computational complexity of our model.    \citet{martins_adapt_sparse} and \citet{martins_sparse-seq2seq} explored directly incorporating sparseness into Transformer models through the choice of activation functions and introduced the -entmax functions. This encompasses both softmax for  and sparsemax  for . For any  -entmax is sparse. \citet{martins_sparse-seq2seq} provided an efficient implementation and experimented with . \rev{We leverage global attention masks, rather than creating a sparse attention pattern for each key-value pair, and we manage both to provide quantifiable speed guarantees and to achieve higher sparseness in practice.}      extend the lottery ticket in the context of NLP and prune network parameters.   There has been a lot of research on understanding overparameterization and developing methods that make BERT models faster and more lightweight~.  found that different attention heads encode similar patterns and hence these heads are not always all necessary. They obtain good performance by removing entire attention heads at test time. \citet{Sajjad2020PoorMB} prune whole Transformer layers at a time and again obtain good performance while removing a large percentage of the model's parameters. Our pruning method takes a more fine-grained approach and prunes individual connections rather than whole heads or layers. We speculate that our method could be used in conjuction with the above-mentioned and leave this for future work.     In this paper, we propose a novel architecture, label enhanced heterogeneous graph attention networks model ,  for Chinese ED. To fully exploit information between characters and words, we formulate characters and words as different types of nodes, and connect them with richly functional edges. The heterogeneous graph attention networks is utilized to enable adequate information propagation. Besides, we utilize the semantic clues from event labels to guide the detection of event triggers. Experiment results show that L-HGAT consistently achieves superior performance over previous competing approaches. In the future, we would like to adapt L-HGAT for other information extraction tasks, such as named entity recognition and aspect extraction.    \def\year{2021}\relax  File: formatting-instructions-latex-2021.tex  release 2021.1 \documentclass[letterpaper]{article}   DO NOT CHANGE THIS \usepackage{aaai21}    DO NOT CHANGE THIS \usepackage{times}    DO NOT CHANGE THIS \usepackage{helvet}   DO NOT CHANGE THIS \usepackage{courier}    DO NOT CHANGE THIS \usepackage[hyphens]{url}    DO NOT CHANGE THIS \usepackage{graphicx}   DO NOT CHANGE THIS \urlstyle{rm}   DO NOT CHANGE THIS \def\UrlFont{\rm}    DO NOT CHANGE THIS \usepackage{natbib}    DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption}   DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing    DO NOT CHANGE THIS \setlength{\pdfpagewidth}{8.5in}    DO NOT CHANGE THIS \setlength{\pdfpageheight}{11in}    DO NOT CHANGE THIS  \usepackage{multirow} \usepackage{amssymb}  \usepackage{booktabs} \usepackage{bm} \usepackage{CJKutf8} \usepackage[switch]{lineno} \newcommand{\tabincell}[2]{} 閺鎯ф躬鐎佃壈鈻堥崠  \nocopyright  PDF Info Is REQUIRED.   For /Author, add all authors within the parentheses, separated by commas. No accents or commands.   For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses. \pdfinfo{ /Title  /Author  /TemplateVersion  }  Leave this   /Title    Put your actual complete title  within the parentheses in mixed case   Leave the space between \Title and the beginning parenthesis alone   /Author    Put your actual complete list of authors  within the parentheses in mixed case.   Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,   remove them.    DISALLOWED PACKAGES   \usepackage{authblk} -- This package is specifically forbidden   \usepackage{balance} -- This package is specifically forbidden   \usepackage{color    \usepackage{CJK} -- This package is specifically forbidden   \usepackage{float} -- This package is specifically forbidden   \usepackage{flushend} -- This package is specifically forbidden   \usepackage{fontenc} -- This package is specifically forbidden   \usepackage{fullpage} -- This package is specifically forbidden   \usepackage{geometry} -- This package is specifically forbidden   \usepackage{grffile} -- This package is specifically forbidden   \usepackage{hyperref} -- This package is specifically forbidden   \usepackage{navigator} -- This package is specifically forbidden      \indentfirst} -- This package is specifically forbidden   \layout} -- This package is specifically forbidden   \multicol} -- This package is specifically forbidden   \nameref} -- This package is specifically forbidden   \usepackage{savetrees} -- This package is specifically forbidden   \usepackage{setspace} -- This package is specifically forbidden   \usepackage{stfloats} -- This package is specifically forbidden   \usepackage{tabu} -- This package is specifically forbidden   \usepackage{titlesec} -- This package is specifically forbidden   \usepackage{tocbibind} -- This package is specifically forbidden   \usepackage{ulem} -- This package is specifically forbidden   \usepackage{wrapfig} -- This package is specifically forbidden   DISALLOWED COMMANDS   \nocopyright -- Your paper will not be published if you use this command   \addtolength -- This command may not be used   \balance -- This command may not be used   \baselinestretch -- Your paper will not be published if you use this command   \clearpage -- No page breaks of any kind may be used for the final version of your paper   \columnsep -- This command may not be used    -- No page breaks of any kind may be used for the final version of your paper   \pagebreak -- No page breaks of any kind may be used for the final version of your paperr   \pagestyle -- This command may not be used   \tiny -- This is not an acceptable font size.   {0}  May be changed to 1 or 2 if section numbers are desired.    The file aaai21.sty is the style file for AAAI Press   proceedings, working notes, and technical reports.      Title    Your title must be in mixed case, not sentence case.   That means all verbs ,   nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while   articles, conjunctions, and prepositions are lower case unless they   directly follow a colon or long dash  \title{Appendix}  \begin{document}  \maketitle  
","   There are several fruitful directions for research focused on improving the computational efficiency and the interpretability of the attention mechanism. Sparseness plays a central role in all of them, as simple attention mechanisms inherently scale quadratically with sequence length and assign non-zero correlation between any two input tokens.   One line of research is that of incorporating sparseness by restricting attention patterns to pre-defined ones. \citet{Child2019GeneratingLS} introduced two sparse matrix factorizations, which reduce the computational complexity from  to   and successfully applied this method to language modelling tasks. \citet{reformer} leveraged algorithmic insights to create a sparse attention mechanism with  computational complexity: they used local sensitivity hashing to cluster tokens that should attend to each other and only compute attention within tokens from the same chunks. More directly related to our work is that of \citet{longformer} who looked directly at sparsifying attention patterns rather than at the underlying  matrix factorization and reduced the computational complexity of attention from  to  using GPU kernels .  One key difference between these approaches and ours is that we do not impose any a priori restrictions on the type of attention patterns we can generate, and we show theoretical computational gains with specialized GPU implementations rather than reduce the computational complexity of our model.    \citet{martins_adapt_sparse} and \citet{martins_sparse-seq2seq} explored directly incorporating sparseness into Transformer models through the choice of activation functions and introduced the -entmax functions. This encompasses both softmax for  and sparsemax  for . For any  -entmax is sparse. \citet{martins_sparse-seq2seq} provided an efficient implementation and experimented with . \rev{We leverage global attention masks, rather than creating a sparse attention pattern for each key-value pair, and we manage both to provide quantifiable speed guarantees and to achieve higher sparseness in practice.}      extend the lottery ticket in the context of NLP and prune network parameters.   There has been a lot of research on understanding overparameterization and developing methods that make BERT models faster and more lightweight~.  found that different attention heads encode similar patterns and hence these heads are not always all necessary. They obtain good performance by removing entire attention heads at test time. \citet{Sajjad2020PoorMB} prune whole Transformer layers at a time and again obtain good performance while removing a large percentage of the model's parameters. Our pruning method takes a more fine-grained approach and prunes individual connections rather than whole heads or layers. We speculate that our method could be used in conjuction with the above-mentioned and leave this for future work.",195
" .     %     % % final paper: en-us version     %     %   % space normally used by the marker     % This work is licensed under a Creative Commons     % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } Pre-trained language models have received great interest in the natural language processing  community in the last recent years . These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence . Then, transfer learning  can be used to leverage the learned knowledge for a down-stream task, such as text-classification .  \citet{devlin_bert:_2019} introduced the  ``Bidirectional Encoder Representations from Transformers'' , a pre-trained language model based on the Transformer architecture . BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context . The fact is, BERT has achieved state of the art results on the ``General Language Understanding Evaluation''  benchmark  by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis , relation extraction  and word sense disambiguation , as well as its adaptability to languages other than English . However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios .  In this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques , i.e. reducing the parameter space, impact model training convergence with fewer data points?  To answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty  for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model. To the best of our knowledge, the work presented in this paper is the first demonstration of combining modern transfer learning using pre-trained Transformer-based language model such as the BERT model with active learning to improve performance in low-resource scenarios. Furthermore, we explore the effect of trainable parameters reduction on model performance and training stability by analyzing the layer-wise change of model parameters to reason about the selection of layers excluded from training. %Furthermore, we explore whether a more sophisticated decoder architecture, i.e. convolutional neural networks  can improve the overall performance or if the added complexity hinders a fast model adaption with such little training data.  The main findings of our work are summarized as follows: a) we found that the model's classification uncertainty on unseen data can be approximated by using Bayesian approximations and therefore, used to efficiently select data for manual labeling in an active learning setting; b) by analyzing layer-wise change of model parameters, we found that the active learning strategy specifically selects data points that train the first and thus more general natural language understanding layers of the BERT model rather than the later and thus more task-specific layers.      Low-resource NLP Previous work in low-resource NLP tasks includes feature-engineering  which requires a recurring effort when adapting to a new data set. Another approach is to transfer knowledge across domains to increase the amount of data that is available for training . One of these approaches relied on adversarial training  to learn a domain adaptive classifier  in another domain or language where training data was plentiful while ensuring that the model generalizes to the low-resource domain . However, these approaches have not used a pre-trained generic language model, but perform pre-training for each task individually.   Adapting pre-trained models The effectiveness of transfer learning in low-resource settings was previously demonstrated for machine translation , sequence tagging  and sentiment classification . However, all this prior work does not use general LMs but transfers knowledge from a pre-trained model to a close to target corpus. Some previous work has analyzed the performance behavior of the BERT model in different scenarios. \citet{hao_visualizing_2019} showed that a classifier that fine-tunes a pre-trained BERT model generally has wider optima on the training loss curves in comparison to models trained for the same task from scratch, indicating a more general classifier . \citet{peters_tune_2019} examine the adaption phase of LM based classifiers by comparing fine-tuning and feature extraction where the LMs parameters are fixed. In contrast, we focus on the low-resource setting where less than 1,000 data points are available for fine-tuning.   Layer freezing in deep Transformers Experiments by \citet{yosinski_how_2014} indicated that the first layers of a LM capture a more general language understanding, while later layers capture more task-specific knowledge. With this motivation, \citet{howard_universal_2018} introduced gradual unfreezing of the Transformer layers during each epoch, beginning with the last layer. \citet{hao_visualizing_2019} analyzed the loss surfaces in dependency of the model parameters before and after training and came to the same conclusion that lower layers contain more transferable features. However, none of the work has considered the training set size as a dependent parameter as our experiments presented in this paper.   Active learning in NLP There is some prior work regarding active learning for NLP tasks using deep neural networks. \citet{zhang_active_2017} explored pool-based active learning for text classification using a model similar to our setting. However, they used word-level embeddings  and focus on representation learning, querying pool points that are expected to maximize the gradient changes in the embedding layer. \citet{shen_deep_2017} used active learning for named entity recognition tasks. They proposed a acquisition strategy named Maximum Normalized Log-Probability which is a normalized form of the Constrained Forward-Backward confidence estimation . Using this strategy, they achieved on-par performance in comparison to a model using the BALD acquisition function and MC Dropout without needing multiple forward passes. However, this approach is not suitable for any arbitrary model architecture but requires conditional random fields  for the approximation of model uncertainty.    This work proposed an evolutionary deep learning approach to discover performant char-CNN architectures. This goal was achieved through the implementation of a genetic programming-based algorithm  coupled with a reduced cellular encoding scheme and the backpropogation algorithm. The SurDG-EC algorithm located, on average, higher accuracy models than those located by SurDG-Random. The fittest evolved phenotype defeated one of the state-of-the-art char-CNN models and achieved comparable results to the state-of-the-art VDCNN-29 architecture. The evolved model also generalised favourably across most unseen datasets. There is clear evidence that width may potentially add to the efficacy of char-CNNs.This does not mean that width will always result in increased accuracy, as also observed in the results. There are many other factors to consider. It is not known how much of the efficacy of the evolved phenotypes are due to increased width or some other unknown variable or combination of variables. There are, however, clear indications that the importance of width should be further researched. The SurDG-EC algorithm also revealed two interesting properties of char-CNNs. Building a rich tapestry of feature representations at the early stages of the network potentially aids in improving the accuracy of the networks as they grow deeper - in turn constructing a hierarchy of relations from this rich feature tapestry. The evolutionary crossover operation also revealed that combing the widths of two phenotypes produced a wider phenotype with greater validation accuracy. This is a further clue that there may be value in making char-CNNs with increased width.   ------------------------------------------------------------------------------ 
","   Low-resource NLP Previous work in low-resource NLP tasks includes feature-engineering  which requires a recurring effort when adapting to a new data set. Another approach is to transfer knowledge across domains to increase the amount of data that is available for training . One of these approaches relied on adversarial training  to learn a domain adaptive classifier  in another domain or language where training data was plentiful while ensuring that the model generalizes to the low-resource domain . However, these approaches have not used a pre-trained generic language model, but perform pre-training for each task individually.   Adapting pre-trained models The effectiveness of transfer learning in low-resource settings was previously demonstrated for machine translation , sequence tagging  and sentiment classification . However, all this prior work does not use general LMs but transfers knowledge from a pre-trained model to a close to target corpus. Some previous work has analyzed the performance behavior of the BERT model in different scenarios. \citet{hao_visualizing_2019} showed that a classifier that fine-tunes a pre-trained BERT model generally has wider optima on the training loss curves in comparison to models trained for the same task from scratch, indicating a more general classifier . \citet{peters_tune_2019} examine the adaption phase of LM based classifiers by comparing fine-tuning and feature extraction where the LMs parameters are fixed. In contrast, we focus on the low-resource setting where less than 1,000 data points are available for fine-tuning.   Layer freezing in deep Transformers Experiments by \citet{yosinski_how_2014} indicated that the first layers of a LM capture a more general language understanding, while later layers capture more task-specific knowledge. With this motivation, \citet{howard_universal_2018} introduced gradual unfreezing of the Transformer layers during each epoch, beginning with the last layer. \citet{hao_visualizing_2019} analyzed the loss surfaces in dependency of the model parameters before and after training and came to the same conclusion that lower layers contain more transferable features. However, none of the work has considered the training set size as a dependent parameter as our experiments presented in this paper.   Active learning in NLP There is some prior work regarding active learning for NLP tasks using deep neural networks. \citet{zhang_active_2017} explored pool-based active learning for text classification using a model similar to our setting. However, they used word-level embeddings  and focus on representation learning, querying pool points that are expected to maximize the gradient changes in the embedding layer. \citet{shen_deep_2017} used active learning for named entity recognition tasks. They proposed a acquisition strategy named Maximum Normalized Log-Probability which is a normalized form of the Constrained Forward-Backward confidence estimation . Using this strategy, they achieved on-par performance in comparison to a model using the BALD acquisition function and MC Dropout without needing multiple forward passes. However, this approach is not suitable for any arbitrary model architecture but requires conditional random fields  for the approximation of model uncertainty.",196
" Domain shift is common in language applications. One is more likely to find ""internet"" or ""PC"" in reviews on electronics than those on books, while he or she is more likely to find ""writing"" or ""B.C."" in reviews on books than those on electronics. This proposes a fundamental challenge to NLP in that many computational models fail to maintain comparable level of performance across domains. Formally, a distribution shift happens when a model is trained on data from one distribution , but the goal is to make good predictions on some other distribution  that shares the label space with the source.   We study unsupervised domain adaptation in this work, where we have fully-labeled data on source domain but no labeled data on target domain. The most prevailing methods in this field aim to learn domain-invariant feature by aligning the source and target domains in the feature space. The pioneering works in this field try to bridge domain gap with discrepancy-based approach.  first introduce MMD to measure domain discrepancy in feature space and use its variant MK-MMD as an objective to minimize domain shift. Another line of work introduces a domain classifier and adversarial training to induce domain invariant feature, followed by works using generative models to enhance adversarial training. However, note that both MMD-based approach and adversarial training formulates with a minimax optimization procedure that is widely known as hard to converge to a satisfactory local optimum. Moreover, some recent works have discovered that both of them don't guarantee good adaptation and will introduce inevitable error on target domain under label distribution shift because they may render incorrect distribution matching. For example, thinking of a binary classification task, the source domain has 50\% of positive samples and 50\% of negative samples while the target domain has 30\% postive and 70\% negative. Successfully aligning these distributions in representation space requires the classifier to predict the same fraction of positive and negative on source and target. If one achieves 100\% accuracy on the source, then target accuracy will be at most 80\%, that is 20\% error at best.   % Self-supervised learning is prominent in feature representation learning. Recent works have approached unsupervised domain adaptation for computer vision with SSL[][]. [] adopted rotation prediction, flip prediction and patch location prediction to induce domain-invarint feature and find that some auxiliary tasks involving fine-grained semantics like pixel reconstruction may force the model to focus on domain-specific feature, further widening the domain gap.   Self-supervised representation learning could be a good workaround for this problem because it enforces predictive behaviour matching instead of distribution matching. The main idea is to learn discriminative representation that is able to genenralize across domains.  use sentiment-indicating pivot prediction as their auxiliary task for cross-domain sentiment analysis. The method proposed in this paper adopts contrastive learning to extract generalizable discriminative feature. Contrastive learning is a subclass of self-supervised learning that is gaining popularity thanks to recent progress. It utilizes positive and negative samples to form contrast against the queried sample on pretext tasks in order to learn meaningful representations. However, the pretext tasks must be carefully chosen. shows with experiments on computer vision tasks that the transfer performance will suffer under improper pretext tasks like pixel reconstruction.  % Recent developments in contrastive learning obtained promising results both on representation learning benchmarks for CV[][][] and for NLP[][][]. % Like with self-supervised learning[], joint learning of pretext tasks in contrastive learning is able to align domain in the feature space, as illustrated in figure.  There are a group of works adopting it for domain adaptation for CV[][][]. However, these method cannot be easily adopted to NLP due to the inherent signal difference between the to domains. .   Therefore, in this paper we explore two classic data augmentation methods in natural language processing閳ユ敃ynonym substitution and back translation to define our pretext task. Experiments on two cross-domain sentiment classification benchmarks show the efficacy of the proposed method. We also examine whether in-domain contrastive learning and entropy minimization helps cross-domain sentiment classification under varied label distribution settings. Our main contributions in this work  are summarized as follows:      \paragraph{Cross-Domain Sentiment Classification} The most prevailing methods for unsupervised domain adaptation for sentiment classification aims to learn domain-invariant feature by aligning the source and target domains in feature space. One line of work in this field derives from, using MMD and its variants to measure and minimize domain discrepancy. Another line of work follows , using a domain classifier and adversarial training to induce domain invariant feature. However, these methods fail to take care of label shift across domains. This can cause undesired performance degradation on target domain according to our analysis in the introduction section. Another important line of work follows Structure Correspondence Learning. They use pivot prediction as an auxiliary task to help extract domain-invariant knowledge. Since transformer-based language models catch on, designs novel self-supervised ""post-training"" tasks for BERT along with domain adversarial training to help domain transfer. Please note that although we also use BERT as feature extractor, we're different from it in multiple aspects: We not only use contrastive learning instead of BERT-pretraining-style self-supervised learning as is in , but we also get rid of distribution matching with domain adversarial training. Our competitive performance on benchmarks further illustrates the efficacy of our model. \paragraph{Contrastive Learning} Recent developments on contrastive learning have achieved promising results on the standard representation learning benchmarks on computer vision tasks. Although there have been several works applying contrastive learning to NLP, most of them concentrate on single-domain tasks like image caption retrieval, machine translation and those on the GLUE benchmark. To the best of our knowledge, we first adopt contrastive learning as an approach to facilitate domain adaptation in natural language processing.      We present an event-guided denoising approach for relation extraction corpus creation that, when used with the current state-of-the-art training procedure, achieves comparable results in English under a low-resource regime for only a fraction of the training cost. It also performs well in Spanish, demonstrating its adaptability to resource-constrained relation extraction tasks in non-English languages.   Our technique affords the broader research community the ability to approximate the current state-of-the-art in relation extraction by significantly lowering its associated training costs.  However, it requires a fairly large date-marked news corpus which may not be available in low resource languages. We leave an exploration of broader language coverage and minimal required corpus size for future work.   One promising direction for expanding language coverage is cross-lingual learning via ``codeswitched"" examples and other language modeling losses .  We hypothesize that such methods could help knowledge transfer among languages and improve results on downstream tasks.  Finally, we note that since our approach extracts relation statements from news corpora, it is likely that the resulting distribution of underlying relation types is different than the distribution found in Wikipedia. For example, Wikipedia may contain more expressions of standard ontological relations  characteristic of factoids. Despite this hypothesized difference, our approach performs well on both FewRel and SemEval 2010 Task 8, both of which include a subset of such relation types.  In the future we intend to investigate these differences and their implications more closely.    Acknowledgments: AIDA, partially supported by 
"," \paragraph{Cross-Domain Sentiment Classification} The most prevailing methods for unsupervised domain adaptation for sentiment classification aims to learn domain-invariant feature by aligning the source and target domains in feature space. One line of work in this field derives from, using MMD and its variants to measure and minimize domain discrepancy. Another line of work follows , using a domain classifier and adversarial training to induce domain invariant feature. However, these methods fail to take care of label shift across domains. This can cause undesired performance degradation on target domain according to our analysis in the introduction section. Another important line of work follows Structure Correspondence Learning. They use pivot prediction as an auxiliary task to help extract domain-invariant knowledge. Since transformer-based language models catch on, designs novel self-supervised ""post-training"" tasks for BERT along with domain adversarial training to help domain transfer. Please note that although we also use BERT as feature extractor, we're different from it in multiple aspects: We not only use contrastive learning instead of BERT-pretraining-style self-supervised learning as is in , but we also get rid of distribution matching with domain adversarial training. Our competitive performance on benchmarks further illustrates the efficacy of our model. \paragraph{Contrastive Learning} Recent developments on contrastive learning have achieved promising results on the standard representation learning benchmarks on computer vision tasks. Although there have been several works applying contrastive learning to NLP, most of them concentrate on single-domain tasks like image caption retrieval, machine translation and those on the GLUE benchmark. To the best of our knowledge, we first adopt contrastive learning as an approach to facilitate domain adaptation in natural language processing.",197
"   %Recently, Neural machine translation~ has achieved great success and reached satisfactory translation performances for several language pairs~. %These NMT models are sequence-to-sequence models trained on large parallel data.  % Ensemble learning, which aggregates multiple diverse models during inference, has attracted huge interest in both academia and industry communities thanks to its effectiveness in a variety of computational intelligence problems such as classification, prediction and function approximation. So far, many aggregating approaches have been developed such as bagging and boosting to improve the practical performance.  % % Ensemble learning is primarily used to improve the classification task or reduce the likelihood of a poorly learned model.  % Recently, ensemble of different neural networks  has greatly improved the accuracy of neural machine translation , making it a vital widely used technique in state-of-the-art Neural NMT systems. In the scenario of NMT, a common implementation is to average the probability of each token computed by different individual models and then decode with the averaged probabilities. Previous studies show that the performance of ensemble method heavily depends on both the accuracy and diversity of base models, which are typically obtained through independent training on different sets of attributes.  % % Ensemble learning, which aggregates multiple models during inference, is an   % Despite its success in various tasks and applications, in practice there are a few common challenges of ensemble methods, which prevent its wide usage: 1) High computational cost. For ensemble learning, all individual models have to conduct encoding and decoding, which is prohibitively time and memory consuming. It gets even worse in the context of NMT due to the large size of state-of-the-art networks like transformer. 2) Absence of monolingual data. Ensemble exploit the independence cannot make full use of the large scale monolingual data from source side.  Recently, self-training method has shown remarkable success in image recognition.  % Taking advantage of unlabeled data, Trained on noisy augmented data, an EfficientNet model finetuned with self-training can achieve 87.4\% top-1 accuracy on ImageNet, which is 1.0\% better than the state-of-the-art model that requires 3.5B weakly labeled images. Typically, in self-training we first train a base model on the labeled data, and then utilize the learned model to label unannotated data. Finally, both labeled and pseudo data are combined as the training set to yield the next level model. In the context of natural language processing, many works have successfully applied self-training technique including word sense disambiguation and parsing. %  Nevertheless, the performance gains achieved through self-training are still limited for structured prediction tasks such as Neural Machine Translation~ where the target space is vast. Originally designed for classification problems, previous work suggests that self-training can be effective only when the predictions on unlabeled samples are good enough, and otherwise it will suffer from the notorious reinforced mistakes. However, this problem is common in NMT scenario, where the hypotheses generated from a single model are often far away from the ground-truth target due to the compositionality of the target space. \citet{zhang2016exploiting} found that training on this biased pseudo data may accumulate the mistakes at each time step and enlarge the error, and thus they propose to freeze the decoder parameters when training on the pseudo parallel data which may negatively impact the decoder model of NMT.  % We argue that the performance drop of self-training for NMT mainly comes from the reinforced mistakes.  To overcome this issue, in this paper we borrow the reciprocal teaching concept from the educational field and revisit the core idea of classic ensemble approaches. Ensemble is built upon the assumption that different models have different inductive biases and better predictions can be made by majority voting. We propose to replace the self-supervision with Reciprocal-Supervision in NMT, leading to a novel co-EM  scheme named \method. In \method, we use multiple separately learned models to provide diverse proper pseudo data, allowing us to enjoy the independence between different models and dramatically reduce the error through strategic aggregation. %Most of these NMT works use only one type of neural network model such as ConvS2S~ and Transformer~. %Usually, different neural models have different performances and they may also catch minor different patterns in the sequences. More specifically, we first learn multiple different models on the parallel data. Then in the E-step all individual models are used to translate the monolingual data. And in the M-step the generated pseudo data produced by different models are combined to tune all student models. %To combine these advantages and diversities, the intuitive method is ensemble, in which several models are trained and every model will be used during inference, then the output of these models are combined for a better prediction.  \method is inspired by the success of ensemble method. However, ensemble is resource-demanding during inference, which prevents its wide usage. Besides, it cannot make use of the large scale monolingual data from source side. %The teacher-student framework can be used to make one model learn from others. Some works have been done to explore the assistance from right-to-left decoding model to usual left-to-right model~. These works shown that a regular NMT model can learn from a right-to-left decoding model and obtain better performance. However, to our best knowledge, there are no such work exploring the assistance from several different models. So in this work, we try to utilize multiple different models as teachers, and train a student model to learn from them. Through this procedure, the student model can have better performance. %Following this procedure, we have another advantage that monolingual data of source side language can be easily utilized to extend the training method to our self-training framework with diverse teachers. %Similar to teacher-student framework for zero-shot NMT~, the student model can also learn from teachers by monolingual data. \method is also related to the data augmentation approaches for NMT.  While most of previous works concentrate on monolingual data of target side such as back-translation~, we pay more attention to the source side. Knowledge distillation  is another relevant research topic. However, KD is preliminary designed to improve a weak student model with a much stronger teacher model. By contrast, \method boosts the performance of base models through reciprocal-supervision from other just comparable or even weaker learners. % Unsupervised machine translation~ can also be seen as utilizing target side monolingual data. To the best of our knowledge, we are the first self-training framework with reciprocal-supervision, which can correct the bias of each model and fully utilize the monolingual data of source side language. More precisely, the advantages of \method % our cooperative-supervised framework with diverse parameterized networks  can be summarized as follows:  Through extensive experiments, \method achieves significant gains on several standard translation tasks including En\{Ro, De\}. Surprisingly, we also have found that \method with other much weaker learners could even outperform a strong BERT enhanced NMT model with big margins.   Our work is highly related to several important research directions of NMT.       \zhouhan{The NMT formulation should be moved out from related works section. The knowledge distillation part could be moved in here. }       Improving NMT with Monolingual Data. NMT heavily relies on a large amount of bilingual data with parallel sentence pairs, which is expensive to collect. To overcome this obstacle, many works have been proposed to leverage the rich monolingual data to help the training in the semi-supervised setting.   , which can be unified into two directions. One idea is to  \citet{gulcehre2015using,gulcehre2017integrating} incorporate external language models  into the NMT model, which improves the fluency in target language. \citet{sennrich-etal-acl2016-improvingwithmono} use the back translation  approach to exploit target side monolingual data. They back translate the target side monolingual data to source side through an additional target-to-source NMT model learned on the bilingual dataset. Then    the generated data will be paired with monolingual data as  the original bilingual data will be augmented with the synthetic parallel corpus for further training the source-to-target NMT model.  learns from non-parallel data in a round-trip game via dual learning, where the source sentence is first forward translated to the target space and then back translated to the source space. The reconstruction loss is used to benefit the training. \citet{zhang2018joint} propose to jointly train the source-to-target and target-to-source NMT models, where two models can provide back-translated pseudo data for each other.  While target side monolingual data has been extensively studied, there exist few attempts to use the source side data. \citet{ueffing2006using} and \citet{zhang2016exploiting} explored self-training  in statistical and neural machine translation respectively, albeit with limited gains. Recently, \citet{he2019revisiting}  shows that the perturbation on the input and hidden states is critical for self-training on NMT. However, this study is conducted on relatively small-scale monolingual data, and therefore ST remains unclear in the large-scale setting.   Ensemble and KD for NMT. Among various model aggregating methods in machine learning, the most effective and widely adopted methods for NMT is the token-level ensemble.   Let  and  denote the source and target language spaces respectively, and  denotes the given  source-to-target translation models. Let  denote the vocabulary of the target language. In this approach, given the source sentences, a group of individually learned models cooperate together to generate the target sentence step by step. More specifically, the token-level ensemble method generates the target sequence by averaging the predicted probabilities of each token.  A commonly used KD approach is ensemble KD, where each individual NMT model distill the knowledge from an ensemble model. More recently, multi-agent learning is proposed to distill the knowledge from a dynamic ensemble teacher model during training. By contrast, there is no teacher model and no ensemble procedure in \method, and we just improve individual models together through reciprocal learning. Note that, for \citet{bi2019multi}, models such as conventional Statistical MT or NMT with right2left decoding direction cannot be aggregated with common models with token-level ensemble method, which means \method is much more flexible.      We have proposed a powerful and easy to deploy approach to augment text data through conditional generation. By leveraging an off-the-shelf language model , we successfully guide the generation towards a specified direction , with the help of reinforcement learning. We find that Data Boost improves the performance of classification tasks, is classifier-agnostic, and that it surpasses several prior augmentation methods in three diverse classification tasks.   In the future, we plan to implement a more sophisticated guidance for the augmentation by adding syntactic and position features to the reward function, to enable augmentation of more diverse types of text data. The code will be made available upon request.   
","  Our work is highly related to several important research directions of NMT.       \zhouhan{The NMT formulation should be moved out from related works section. The knowledge distillation part could be moved in here. }       Improving NMT with Monolingual Data. NMT heavily relies on a large amount of bilingual data with parallel sentence pairs, which is expensive to collect. To overcome this obstacle, many works have been proposed to leverage the rich monolingual data to help the training in the semi-supervised setting.   , which can be unified into two directions. One idea is to  \citet{gulcehre2015using,gulcehre2017integrating} incorporate external language models  into the NMT model, which improves the fluency in target language. \citet{sennrich-etal-acl2016-improvingwithmono} use the back translation  approach to exploit target side monolingual data. They back translate the target side monolingual data to source side through an additional target-to-source NMT model learned on the bilingual dataset. Then    the generated data will be paired with monolingual data as  the original bilingual data will be augmented with the synthetic parallel corpus for further training the source-to-target NMT model.  learns from non-parallel data in a round-trip game via dual learning, where the source sentence is first forward translated to the target space and then back translated to the source space. The reconstruction loss is used to benefit the training. \citet{zhang2018joint} propose to jointly train the source-to-target and target-to-source NMT models, where two models can provide back-translated pseudo data for each other.  While target side monolingual data has been extensively studied, there exist few attempts to use the source side data. \citet{ueffing2006using} and \citet{zhang2016exploiting} explored self-training  in statistical and neural machine translation respectively, albeit with limited gains. Recently, \citet{he2019revisiting}  shows that the perturbation on the input and hidden states is critical for self-training on NMT. However, this study is conducted on relatively small-scale monolingual data, and therefore ST remains unclear in the large-scale setting.   Ensemble and KD for NMT. Among various model aggregating methods in machine learning, the most effective and widely adopted methods for NMT is the token-level ensemble.   Let  and  denote the source and target language spaces respectively, and  denotes the given  source-to-target translation models. Let  denote the vocabulary of the target language. In this approach, given the source sentences, a group of individually learned models cooperate together to generate the target sentence step by step. More specifically, the token-level ensemble method generates the target sequence by averaging the predicted probabilities of each token.  A commonly used KD approach is ensemble KD, where each individual NMT model distill the knowledge from an ensemble model. More recently, multi-agent learning is proposed to distill the knowledge from a dynamic ensemble teacher model during training. By contrast, there is no teacher model and no ensemble procedure in \method, and we just improve individual models together through reciprocal learning. Note that, for \citet{bi2019multi}, models such as conventional Statistical MT or NMT with right2left decoding direction cannot be aggregated with common models with token-level ensemble method, which means \method is much more flexible.",198
"  Hypernym, sometimes also known as hyperonym, is the term in linguistics referring to a word or a phrase whose semantic field covers that of its hyponym. The most common relationship between a hypernym and a hyponym is an ``is-a'' relationship. For example, ``red is a color'' provides the relationship between ``red'' and ``color'', where ``color'' is the hypernym of ``red''.   The hypernym-hyponym relation is an essential element in the semantic network and corresponding tasks related to semantic network analysis . The hypernym graph built on a collection of hyponym-hypernym relations can enhance the accuracy of taxonomy induction . The linkage between the hyponym and the hypernym can be used to improve the performance of link prediction and network completion in the knowledge graph or semantic network . In natural language processing , the hyponym-hypernym relation can help the named entity recognition , and the question-answering tasks for ``what is'' or ``is a'' . The data mining, information search and retrieval can also benefit from the hyponym-hypernym relation .   Given the role and application of the hypernym-hyponym relation, it is essential to explore an automatic method to extract such the relation between two entities, which presents an important task in knowledge-driven NLP . Following the landmark work focusing on lexico-syntactic patterns , several pattern-based methods are developed for hypernym extraction . Then the feature-based classification methods are introduced , which applies machine learning tools to enhance the recall rate. Recently, distributional methods and hybrid distributional models are successfully applied to learn the embedding of words, based on which the hypernym-hyponym relation can be inferred . The deep learning approach is also effective in many sequence labeling tasks including hypernym extraction .    While the extraction of hyponym-hypernym relation can be done in many different environments, in this work we focus on the hypernym extraction from definitions. More specifically, the definition refers to a short statement or description of a word. Take the word ``red'' as an example, whose definition on Wikipedia  is ``Red is the color at the end of the visible spectrum of light, next to orange and opposite violet.'' The aim is to identify the word ``color'' as the hypernym of ``red'' from all the nouns in the definition. Intuitively, this task can be solved by general resources such as WordNet dictionary  or Wikipedia. But given a word's different meanings in different contexts, these resources can not sufficiently complete this task. As an example, the term ``LDA'' in Wikipedia denotes ``Linear Discriminant Analysis'' in machine learning, ``Low dose allergens'' in medicine, and ``Landing distance available'' in aviation. The combination of general resources and context identification would also fail in some domain-specific applications where the general resources do not cover the special or technical terms in that area. Moreover, existing technical approaches also demonstrate certain limitations in the task of hypernym extraction from definitions, which we summarize as follows:   To briefly illustrate the difficulty, let us consider a definition from the Stack-Overflow with an irregular format: ``fetch-api: the fetch API is an improved replacement for XHR''. The term ``fetch-api'' is not included in any common dictionary. While the definition has the ``is an'' pattern, it does not connect to the hypernym. The definition is very short and every distinct word in this definition appears just once, which makes it difficult to accurately learn the word representation. Overall, it is challenging to find a method that would accurately identify ``API'' as the correct hypernym.   The definition of a word represents a certain type of knowledge extracted and collected from disordered data. Indeed, there are tools capable of extracting definitions from the corpora with good accuracy . Nevertheless, tools to extract hypernym from definitions remain limited.  % To cope with this issue, we propose a recurrent network method using syntactic features. Because the definition directly points to a noun, the hyponym is already given. Therefore, the hypernym extraction is to identify the correct hypernym from all words in the definition sentence. This task can be considered as a binary classification, in which the classifier judges if a candidate noun is a hypernym or not. In order to better learn the syntactic feature, we transfer the definition sentence into the part of speech  sequence after labeling the PoS of each word by a standard tool . The syntactic structure surrounding the candidate is learned by a bidirectional gated recurrent units  based model. To further fine tune the results, we use a set of features including the centrality of the word in the hypernym co-occurrence network. We use two corpora to evaluate our method. One is Wikipedia, featuring definitions with canonical syntax structure and intensively used by previous studies. The other is from Stack-Overflow, whose definition is domain-specific and usually with the irregular format. Our method is compared with several existing ones. Overall, it outperforms all others in both corpora, which demonstrates the advantage of combing both the tool of RNN and the PoS information in the task of hypernym extraction.    This paper is organized as follows. We review related works in Section  and introduce details of the method in Section . Experiments and evaluations of the proposed model are presented in Section . After that, we draw a conclusion about this research in Section .       The existing methods in hypernym extraction generally fall into one of the following four categories: pattern-based method, feature-based classification method, distributional method and deep learning method.      The pattern-based method directly uses the syntactic patterns in definitions, such as ``is-a'', ``is called'', ``is defined as'' and more. This method is commonly applied in early works due to its simplicity and intuitiveness. The majority of these approaches apply the symbolic method that depends on lexico-syntactic patterns or features , which are manually crafted or semi-automatically learned. However, because only a small fraction of syntactic patterns can be included, these methods usually have a low recall value. In order to cover more patterns,  considers PoS tags instead of simple word sequences, which raises the recall rate. To improve the generalization of the pattern-based method,  starts to model the pattern matching as a probabilistic process that generates token sequences. Moreover,  proposes the three-step use of directed acyclic graphs, called Word-Class Lattices , to classify definitions on Wikipedia. To better cluster definition sentences, the low-frequency words are replaced by their PoS. For a simple example, definitions that ``Red is a color'' and ``English is a language'' are in the same class that is characterized by a pattern ``noun is a noun"". In this way, more patterns can be characterized to identify the hypernym. In recent years, much research pay attention to extracting hypernyms from larger data resources via the high precise of pattern-based methods.  extract hypernymy relations from the CommonCrawl web corpus using lexico-syntactic patterns. In order to address the low recall of pattern-based method in large data resources,  integrate distributional methods and patterns to detect hypernym relations from several existing datasets.  Nevertheless, the pure pattern-based approaches are generally inefficient, given the fact that syntactic patterns are either noisy by nature or domain-specific. It is very difficult to further improve the performance in this direction.      To overcome the issue of generalization in the pattern-based method, the feature-based classification method is introduced.  proposes a method to learn the generalized lexico-syntactic pattern and assign scores to candidate hypernyms. The scores are used to identify the true hypernym out of others.  uses conditional random fields to identify scientific terms and their accompanying definitions. Moreover,  uses the role of syntactic dependencies as the input feature for a support vector machine  based classifier.  explores the features in the dependency tree analysis.  These feature-based classification approaches heavily rely on manually specified features. Patterns learned from sentences or features analyzed from the NLP tools may not fully represent the syntactic structure. In addition, the NLP tools like dependency tree analysis are often time-consuming, and error at early steps may propagate which eventually leads to inaccurate final results.     The distributional method is based on the Distributional Inclusion Hypothesis which suggests that a hypernym tends to have a broader context than its hyponyms . If the similarity between two words can be accurately measured, then a hypernym should be associated with a similar but larger set of words than its hyponyms , tests the Distributional Inclusion Hypothesis and find that hypothesis only holds when it is applied to relevant dimensions. Because word embedding can reflect the corresponding semantic relationship,  constructs semantic hierarchies based on the notion of word embedding.  uses linear classifiers to represent the target words by two vectors concatenation.  introduces a simple-to-implement unsupervised method to discover hypernym via per-word non-negative vector embeddings.  proposes a novel representation learning framework, which generates a term pair feature vectors based on bidirectional residuals of projections, reaches a state of the art performance in general resources.  Nevertheless, the application of the distributional method relies on a very large corpus to learn the word representation. Moreover, the Distributional Inclusion Hypothesis may not be always hold. In the task discussed in this paper, because many terminologies occur infrequently and the length of a definition is usually short, it can be very inefficient to learn word representation.         The recurrent neural networks   have been applied to handle many sequential prediction tasks. By taking a sentence as a sequence of tokens, RNN also works in a variety of NLP problems, such as spoken language understanding and machine translation. It is applied in hypernym extraction as well.  converts the task of definition extraction to sequence labeling. Using a top-N strategy , the infrequently appeared words are replaced by their corresponding PoS. The sequence mixed with words and PoS elements is fed to the long short-term memory   RNN to predict the definition.   More recently,  proposes a two-phase neural network model with yields an enhanced performance compared with . The first phase is constructed by a bi-directional LSTM to learn the sequence information. Then a CRF and a logistic regression are used to refine the classification results.    Both of the two works focus on words. Although  considers the PoS information, the purpose is only to reduce the total number of words by grouping less frequent words together according to their PoS property. While they demonstrate improved performance compared with other methods, they are only tested in Wikipedia corpus, where the definition usually has a very regular format. The performance on other irregular definitions remains unknown.     The computational level of analysis  allows us to contemplate what problem a cognitive phenomenon solves. For example, learning word meanings via cross-situational statistics can be formulated as finding mappings between words and referents that are most consistent with our observations. On the other hand, modeling cognition at the algorithmic level  plays an important role in providing insight about cognitive mechanisms; it requires specifying the details of algorithms and representations which in turn enables us to study their role and interaction at different stages of learning.     Previous research has studied word learning at both algorithmic and computational levels \citep[\eg,][]{siskind.1996,yu.ballard.2007,frank.etal.2007,fazly.etal.2010.csj}. We proposed a framework for modeling cross-situational word learning at the computational level that unifies some of previous work in this domain -- approaches that formulate word learning as a translation problem.   We also show that instantiating this framework results in different word learning models at the algorithmic level: each model has specific inductive biases that define how words and referents compete for association strength given a word/referent.  More specifically, we examine how competition among words or referents plays a role in:  learning from a given observation or in-the-moment learning,  overall learning of word meanings, and  comprehension of a given word. Moreover, we investigate how these assumptions change the performance of a model in the face of uncertainty.   Our results show that models that implement the two complementary types of referent and word competition perform the best. Each competition type addresses a specific type of uncertainty -- word and referent competitions address linguistic and referential certainty, respectively; because the word learning input has both uncertainties, it is important for a model to implement the two competitions.   These models are the most robust and learn successfully from few examples.   Moreover, we find that the best model implements competition during in-the-moment learning and comprehension, but not a global competition over word meaning representations.  By avoiding an overall word meaning competition, the model is able to successfully learn multiple meanings of ambiguous words, given sufficient evidence.    \section{The Derivation of the FAS model}  The FAS model assumes that referents are generated independently given an utterance ; instead of calculating  as in \Equation{eq:cll}, the likelihood is defined over the conditional probability of referents given an utterance:      Here, the alignment variable defines the mappings between words to a given referent. More specifically, the value of the alignment variable  selects the word in the utterance   that is mapped to a given referent .    where  returns the association of  to the referent  given the learned distribution . Note that this corresponds to the Expectation step of the EM algorithm, and \Equation{eq:emfasalign} is an instantiation of \Equation{eq:emalign}.  \break \break In the Maximization step, the new value of  is calculated by finding  that maximizes the model likelihood:  where  and  are a set of all scenes and utterances, respectively:      where   is the word mapped to , and  is the number of times  and  have co-occurred in the corpus .  The FAS model assumes that  is a conditional probability, . This means that there is an additional dependence assumption on the learned representations: each word is a distribution over features, and thus given a word, the features compete to be associated with that word.   To impose this new assumption on the representation, a constraint is added to the expectation defined in \Equation{eq:mstepfas}:    Note that the Lagrange multipliers  ensures that the new constraint on  -- a distribution over referents for each word -- is satisfied.   To find  that maximizes the expectation in \Equation{eq:mstepfas}, the derivative of objective function  is calculated and equated to zero:       Given , we calculate :      Using the above  and \Equation{eq:emfasalign} to calculate the alignment probabilities, we have:    We can approximate  by adding the current alignment probability, , to the sum of all the previously calculated ones . This approach is an approximation of  because the value of the alignment probability changes after processing each - pair, but it can be calculated incrementally; FAS defined an association score,  which is updated as the model process - pairs,   where  and the initial value of   is zero.     Intuitively, this score shows the overall association strength of a word and a referent and it captures how strongly the word-referent pair are associated in each observation, -. \documentclass[12pt]{article} \usepackage[group-separator={,}]{siunitx} \usepackage[natbibapa]{apacite}  \usepackage[american]{babel} \usepackage{newtxtext} \usepackage{newtxmath} \usepackage[utf8]{inputenc} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{capt-of} \usepackage{csquotes} \usepackage{appendix} \usepackage{graphicx} \usepackage{caption} \usepackage{subcaption} \usepackage[usenames, dvipsnames]{xcolor} \usepackage[flushleft]{threeparttable} \usepackage{array} \usepackage{tabularx} \usepackage{enumitem} \usepackage{kantlipsum} \usepackage{multirow} \usepackage{bm} \usepackage{amsmath} \usepackage{setspace} \usepackage{siunitx} \usepackage{etoolbox} \usepackage{makecell} \usepackage[normalem]{ulem} \usepackage{numprint} \npthousandsep{,}    FOR COGSCI SUBMISSION \renewcommand{\baselinestretch}{1.5} \usepackage[margin=1in]{geometry} \renewcommand{\figurename}{Fig.} \documentclass[alpha-refs]{wiley-article}   \documentclass[blind,num-refs]{wiley-article}    Add additional packages here if required \usepackage{siunitx}    Update article type if known \papertype{Original Article}   Include section in journal if known, otherwise delete \paperfield{Journal Section}    \usepackage[round,authoryear]{natbib} \usepackage{hyperref} \renewcommand{\ref}[1]{\hyperref{#1}} \usepackage{setspace} \doublespacing  \usepackage{url} \usepackage{graphicx} \usepackage{csquotes}  \usepackage{pslatex} \usepackage{latexsym} \usepackage{caption} \usepackage{subcaption} \usepackage{xcolor}  \usepackage{amsmath}    Latin phrase short forms \newcommand\etal{et~al.\ } \newcommand\ie{i.e.} \newcommand\eg{e.g.} \newcommand\cf{cf.\ }    Command to not display content \newcommand{\ignore}[1]{}    Content-specific labels \newcommand{\Table}[1]{Table} \newcommand{\Algorithm}[1]{Algorithm} \newcommand{\Example}[1]{Ex.} \newcommand{\Figure}[1]{Figure} \newcommand{\Equation}[1]{Eqn.~} \newcommand{\Section}[1]{Section} \newcommand{\Appendix}[1]{Appendix}    Author note commands \newcommand\scream[1]{{#1}} \newcommand\oldtext[1]{{#1}} \newcommand\an[1]{{#1}} \newcommand\sxs[1]{{SS: #1}} \newcommand\zs[1]{{ZS: #1}} \newcommand\tlg[1]{{TLG: #1}} \newcommand{\replace}[2]{{OLD:} \st{#1} {NEW: #2}} \newcommand\todo[1]{{#1}}  \newcommand\fas{FAS} \newcommand\simf{\mathrm{sim}}  \newcommand{\rep}[3]{\mathrm{\theta}^{#1}_{#2#3}}  \DeclareMathOperator*{\argmax}{arg\,max}  HERE \graphicspath{{plots/zero_unseen_prob/}} \graphicspath{{plots/}}  \author[1\authfn{1}]{Aida Nematzadeh} \author[2\authfn{2}]{Zahra Shekarchi} \author[3\authfn{3}]{Thomas L. Griffiths} \author[4\authfn{4}]{Suzanne Stevenson}  \contrib[1\authfn{1}, 2\authfn{2}]{Equally contributing authors.}  Work was done prior to joining DeepMind.}     Include full affiliation details for all authors \affil[1]{DeepMind} \affil[2]{University of Toronto} \affil[3]{Princeton University} \affil[4]{University of Toronto}   \corraddress{Author One PhD, Department, Institution, City, State or Province, Postal Code, Country} \corremail{nematzadeh@google.com}    Include the name of the author that should appear in the running header \runningauthor{Nematzadeh et al.}   \clearpage  \tableofcontents \clearpage              \break  
","   The existing methods in hypernym extraction generally fall into one of the following four categories: pattern-based method, feature-based classification method, distributional method and deep learning method.      The pattern-based method directly uses the syntactic patterns in definitions, such as ``is-a'', ``is called'', ``is defined as'' and more. This method is commonly applied in early works due to its simplicity and intuitiveness. The majority of these approaches apply the symbolic method that depends on lexico-syntactic patterns or features , which are manually crafted or semi-automatically learned. However, because only a small fraction of syntactic patterns can be included, these methods usually have a low recall value. In order to cover more patterns,  considers PoS tags instead of simple word sequences, which raises the recall rate. To improve the generalization of the pattern-based method,  starts to model the pattern matching as a probabilistic process that generates token sequences. Moreover,  proposes the three-step use of directed acyclic graphs, called Word-Class Lattices , to classify definitions on Wikipedia. To better cluster definition sentences, the low-frequency words are replaced by their PoS. For a simple example, definitions that ``Red is a color'' and ``English is a language'' are in the same class that is characterized by a pattern ``noun is a noun"". In this way, more patterns can be characterized to identify the hypernym. In recent years, much research pay attention to extracting hypernyms from larger data resources via the high precise of pattern-based methods.  extract hypernymy relations from the CommonCrawl web corpus using lexico-syntactic patterns. In order to address the low recall of pattern-based method in large data resources,  integrate distributional methods and patterns to detect hypernym relations from several existing datasets.  Nevertheless, the pure pattern-based approaches are generally inefficient, given the fact that syntactic patterns are either noisy by nature or domain-specific. It is very difficult to further improve the performance in this direction.      To overcome the issue of generalization in the pattern-based method, the feature-based classification method is introduced.  proposes a method to learn the generalized lexico-syntactic pattern and assign scores to candidate hypernyms. The scores are used to identify the true hypernym out of others.  uses conditional random fields to identify scientific terms and their accompanying definitions. Moreover,  uses the role of syntactic dependencies as the input feature for a support vector machine  based classifier.  explores the features in the dependency tree analysis.  These feature-based classification approaches heavily rely on manually specified features. Patterns learned from sentences or features analyzed from the NLP tools may not fully represent the syntactic structure. In addition, the NLP tools like dependency tree analysis are often time-consuming, and error at early steps may propagate which eventually leads to inaccurate final results.     The distributional method is based on the Distributional Inclusion Hypothesis which suggests that a hypernym tends to have a broader context than its hyponyms . If the similarity between two words can be accurately measured, then a hypernym should be associated with a similar but larger set of words than its hyponyms , tests the Distributional Inclusion Hypothesis and find that hypothesis only holds when it is applied to relevant dimensions. Because word embedding can reflect the corresponding semantic relationship,  constructs semantic hierarchies based on the notion of word embedding.  uses linear classifiers to represent the target words by two vectors concatenation.  introduces a simple-to-implement unsupervised method to discover hypernym via per-word non-negative vector embeddings.  proposes a novel representation learning framework, which generates a term pair feature vectors based on bidirectional residuals of projections, reaches a state of the art performance in general resources.  Nevertheless, the application of the distributional method relies on a very large corpus to learn the word representation. Moreover, the Distributional Inclusion Hypothesis may not be always hold. In the task discussed in this paper, because many terminologies occur infrequently and the length of a definition is usually short, it can be very inefficient to learn word representation.         The recurrent neural networks   have been applied to handle many sequential prediction tasks. By taking a sentence as a sequence of tokens, RNN also works in a variety of NLP problems, such as spoken language understanding and machine translation. It is applied in hypernym extraction as well.  converts the task of definition extraction to sequence labeling. Using a top-N strategy , the infrequently appeared words are replaced by their corresponding PoS. The sequence mixed with words and PoS elements is fed to the long short-term memory   RNN to predict the definition.   More recently,  proposes a two-phase neural network model with yields an enhanced performance compared with . The first phase is constructed by a bi-directional LSTM to learn the sequence information. Then a CRF and a logistic regression are used to refine the classification results.    Both of the two works focus on words. Although  considers the PoS information, the purpose is only to reduce the total number of words by grouping less frequent words together according to their PoS property. While they demonstrate improved performance compared with other methods, they are only tested in Wikipedia corpus, where the definition usually has a very regular format. The performance on other irregular definitions remains unknown.",199
"  Although neural machine translation  has achieved great success on sentence-level translation tasks, many studies pointed out that  translation mistakes become more noticeable at the document-level. They proved that these mistakes can be alleviated by feeding the inter-sentential contexts into context-agnostic NMT models.  Previous works have explored various methods to integrate context information into NMT models. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks  or extra context encoders . Different from representation-based approaches, ~\citeauthor{tu2018learning}~\shortcite{tu2018learning} and ~\citeauthor{kuang-etal-2018-modeling}~\shortcite{kuang-etal-2018-modeling} propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is usually updated when new translations are generated. Therefore, long-distance contexts would likely to be erased.  How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence  and using memory and hierarchical structures , are proposed to take global contexts into consideration. However, \citeauthor{kim2019and}~\shortcite{kim2019and} point out that not all the words in a document are beneficial to context integration, suggesting that it is essential for each word to focus on its own relevant context.    \footnotetext{Dependency and coreference relations are from Stanford CoreNLP .}  To address this problem, we suppose to build a document graph for a document, where each word is connected to those words which have a  direct influence on its translation. Figure  shows an example of a document graph. Explicitly, a document graph %for a document  is defined as a directed graph where:  each node represents a word in the document;  each edge represents one of the following relations between words:  adjacency;  syntactic dependency;  lexical consistency; or  coreference.   We apply a Graph Convolutional Network  on the document graph to obtain a document-level contextual representation for each word,  fed to the conventional Transformer model  by additional attention and gating mechanisms. We evaluate our model on four translation benchmarks, IWSLT English--French  and Chinese--English , Opensubtitle English--Russian , and WMT English--German . Experimental results demonstrate that our approach is consistently superior to previous works  on all the language pairs.   The contributions of this work are summarized as:      In recent years, a variety of studies work on improving document-level machine translation with contextual information. Most of them focus on using a limited number of previous sentences.  One typical approach is to equip conventional sentence-level NMT with an additional encoder to learn context representations, which are then integrated into encoder and/or decoder ~.  and adopted hierarchical mechanisms to integrate contexts into NMT models.   and used cache-base methods to memorize historical translations which are then used in following decoding steps.  Recently, several studies have endeavoured to consider the full document context.  averaged the word embeddings of a document to serve as the global context directly.   applied a memory network to remember hidden states of the document, which are then attended by a decoder. first selected relevant sentences as contexts and then attended to words in these sentences. learned global context-aware representations by firstly using a sentence encoder followed by a document encoder.  considered the global context by merely concatenating all the sentences in a document.  Unlike previous approaches, we represent document-level global context in source and target graphs encoded by graph encoders and integrated into conventional NMT via attention and gating mechanisms.    However, most of these techniques mainly focus on the contextual information from the source side. Only few of them considered the context from the target side and based on the RNNSearch model Find a other one. In this work, we propose a method which consider each docuemtn as a whole and leverage the context from both source and target side.        The hyponym-hypernym relationship plays an important role in many NLP tasks. Despite intensive studies on this topic, tools that can accurately extract hypernym from a definition is limited. The definition, representing a special type of summarized knowledge, is commonly observed, not only because some corpora such as Wikipedia or GitHub directly give the definition of a term, but also because there are tools capable of extracting definitions with good accuracy. Hence, it is useful to develop a capable tool for this task. Here we construct a bidirectional GRU model for patterns learning. We use the PoS tags of words surrounding the hypernym as the feature. Our model outperforms existing methods in both the general corpus  and the domain-specific corpus . It also demonstrates a good balance between the performance and complexity, if compared with the kernels by Transformer or Bert. More importantly, by the feature and kernel ablation, we show that the PoS feature is indeed the key element that guarantees the final performance.   The application of the tool we proposed in Stack-Overflow would help us understand the evolution of technology, group users for social network study, and build the semantic network in the domain of computer science. The performance of the tool is limited by the accuracy of PoS tagging. Hence, it would be useful to try or develop other methods other than the Stanford-NLP tool. The use of PoS feature may also have potential in other text sequence labeling tasks, which may have advantages over the word embedding. All these problems will be addressed in future studies.   
"," In recent years, a variety of studies work on improving document-level machine translation with contextual information. Most of them focus on using a limited number of previous sentences.  One typical approach is to equip conventional sentence-level NMT with an additional encoder to learn context representations, which are then integrated into encoder and/or decoder ~.  and adopted hierarchical mechanisms to integrate contexts into NMT models.   and used cache-base methods to memorize historical translations which are then used in following decoding steps.  Recently, several studies have endeavoured to consider the full document context.  averaged the word embeddings of a document to serve as the global context directly.   applied a memory network to remember hidden states of the document, which are then attended by a decoder. first selected relevant sentences as contexts and then attended to words in these sentences. learned global context-aware representations by firstly using a sentence encoder followed by a document encoder.  considered the global context by merely concatenating all the sentences in a document.  Unlike previous approaches, we represent document-level global context in source and target graphs encoded by graph encoders and integrated into conventional NMT via attention and gating mechanisms.    However, most of these techniques mainly focus on the contextual information from the source side. Only few of them considered the context from the target side and based on the RNNSearch model Find a other one. In this work, we propose a method which consider each docuemtn as a whole and leverage the context from both source and target side.",200
"  Automatic summarization is a fundamental task in natural language generation and computational linguistics. It is crucial to help the user quickly read and understand daily events, and has been continuously studied for decades. . In this paper, we focus on meeting summarization, which is an extensively studied task in the field of automatic summarization. Given multiple speakers and corresponding utterances in text, the task calls for generating a shorter transcript, covering salient information of the entire meeting. An example is shown in Figure , which includes 3 speakers and their utterances , and , as well as a human-written summary.  Meeting summarization is typically regarded as a kind of abstractive summarization problem in the literature. The majority of existing studies build summarization systems based on the sequence-to-sequence model, which adopts a sequence modeling strategy for encoding utterances . Despite the effectiveness of these approaches, they typically only use sequential text information while ignoring the important influences of dialogue structure. We claim that dialogue-specific structural information is important for meeting summarization. For example, dialogue discourse is an effective structural feature. As shown in Figure , ``Contrast閳, ``Question-Answer閳 and ``Continuation閳 are three dialogue discourse relations, which can provide more precise semantic relationships between each utterance. Specifically, we can see that the existing sequence modeling method is unable to generate correct summary results ), which can be attributed to the system not knowing the  and  are opposed to the 閳ユ獨 proposal. Differently, the dialogue discourse can provide this key information via labeling the 閳ユ窅ontrast閳 relationship, as shown in Figure . Accordingly, how to effectively integrate the discourse relationship into the existing summarization model become a crucial step in meeting summarization.  In this paper, we propose Dialogue Discourse-Aware Graph Convolutional Networks  to address this problem. In detail, we first convert the entire meeting with dialogue discourse labeling into a discourse graph, which represents both utterances and discourse relationships as vertices. Afterwards, we additionally design six types of directed edges and one global vertex in the discourse graph to facilitate information flow. Finally, we employ a graph convolutional network  to encode the graph and pass the semantic representation to the RNN decoder. Besides, we further use the question-answer discourse relationship to construct a pseudo-summarization corpus for pre-training DDA-GCN. In a conversation, a question often sparks a discussion, so naturally, the question can be used as a pseudo-summary for subsequent discussions.  We conduct experiments on the widely used AMI benchmark . Our approach outperforms various baselines. Moreover, we analyze the effectiveness of dialogue discourse and pseudo-summarization corpus. In the end, we give a brief summary of our contributions:  To the best of our knowledge, we are the first to apply dialogue discourse to model the structure of a meeting for meeting summarization;  We design a discourse-aware graph model to encode the entire meeting;  Our model achieves a new SOTA on the AMI dataset.        Meeting Summarization Previous works focused on extractive meeting summarization . A recent study shows that for meeting summarization, people prefer abstract summaries to extracted ones .  proposed a unified framework for fully unsupervised abstractive meeting summarization.  incorporated dialogue acts which indicate the effect of utterances.  used CRF to tag each utterance a discourse label and then remove utterances that do not contribute to the meeting based on rules.  incorporated topic information which serves as a coarse-grained structure of the meeting.  proposed a hierarchical model under the multi-modal setting by incorporating vision features.  generated a summary in a two-stage manner by first producing a sequence of keywords and then a full summary.  made use of large-scale news datasets to first pretrain the model and then fine-tune it it on meeting dataset.  revealed domain terminology has a substantial impact on meeting summarization performance. In this paper, we first propose to transform the utterances of a meeting into a graph via dialogue discourse.    Graph-to-Sequence Generation Recent research efforts for text generation consider utilizing Graph Neural Networks  to better model structured data, such as AMR , SQL , and knowledge graph . Additionally, there are many works employed GNN in non-structural scenarios, such as summarization  and comment generation , by transforming the input into a meaningful graph. We propose the discourse graph to facilitate information flow over the graph.    In this paper, we propose a graph-based approach for document-level translation, which leverages both source and target contexts. Graphs are constructed according to inter-sentential and intra-sentential relations. We employ a GCN-based graph encoder to learn the graph representations, which are then fed into the NMT model via attention and gating mechanisms.  Experiments on four translation tasks show the proposed approach consistently improves translation quality across different language pairs. Further analyses demonstrate the effectiveness of graphs and the capability of leveraging long-distance context. In the future, we would like to enrich the types of relations to cover more document phenomena.        
","   Meeting Summarization Previous works focused on extractive meeting summarization . A recent study shows that for meeting summarization, people prefer abstract summaries to extracted ones .  proposed a unified framework for fully unsupervised abstractive meeting summarization.  incorporated dialogue acts which indicate the effect of utterances.  used CRF to tag each utterance a discourse label and then remove utterances that do not contribute to the meeting based on rules.  incorporated topic information which serves as a coarse-grained structure of the meeting.  proposed a hierarchical model under the multi-modal setting by incorporating vision features.  generated a summary in a two-stage manner by first producing a sequence of keywords and then a full summary.  made use of large-scale news datasets to first pretrain the model and then fine-tune it it on meeting dataset.  revealed domain terminology has a substantial impact on meeting summarization performance. In this paper, we first propose to transform the utterances of a meeting into a graph via dialogue discourse.    Graph-to-Sequence Generation Recent research efforts for text generation consider utilizing Graph Neural Networks  to better model structured data, such as AMR , SQL , and knowledge graph . Additionally, there are many works employed GNN in non-structural scenarios, such as summarization  and comment generation , by transforming the input into a meaningful graph. We propose the discourse graph to facilitate information flow over the graph.",201
"  Pre-trained language models such as BERT or RoBERTa learn contextualized word representations on large-scale text corpus through self-supervised learning, and obtain new state-of-the-art results on many downstream NLP tasks . Recently, researchers have observed that pre-trained language models can internalize real-word knowledge into their model parameters. For example, pre-trained language models are able to answer the questions such as ``the sky is }'' or ``Beethoven was born in }'' with moderate accuracy. To further explore their potential, researchers have proposed various approaches to guide the pre-training of the language models by injecting different forms of knowledge into them, such as structured knowledge graph or linguistic knowledge  .      	 \end{table*}  Table lists some of the previous knowledge-guided pre-trained language models with their training methods. We group them into two categories: generative tasks and discriminative tasks. Generative tasks are often formulated as predicting the masked tokens given the context. By particularly masking out the words that contain certain types of knowledge  in generative pre-training, the model can be more adept in memorizing and completing such knowledge. While discriminative tasks are often formulated as a classification problem with respect to the sentence or the tokens. By training on the positive and negative examples constructed according to the external knowledge, the discriminator can be more capable of verifying the true or false knowledge in natural language. Existing research has demonstrated that generative and discriminative training have their advantages: the former has a large negative sample space so that the model can learn fine-grained knowledge, while the latter avoids the ``'' tokens in pre-training, and is therefore more consistent with fine-tuning. On the other hand, generative and discriminative capture the different aspects of data distribution and could be complementary to each other in knowledge consolidation. However, to the best of our knowledge, there is not previous work in combining the two approaches in a systematic way. Inspired by the recent success on the generative-discriminative pre-trained model named ELECTRA, we propose to learn the generator and discriminator jointly in the knowledge-guided pre-training, which we call the KgPLM model.  In this paper, we design masked span prediction as the generative knowledge completion task, and span replacement checking as the discriminative knowledge verification task. Hybrid knowledge, including link structure of Wikipedia and structured knowledge graph in Wikidata, is used to guide the both tasks. The spans covering the factual knowledge are more likely to be selected for masking or replacement, and the choices of their replacements are also related to the proximity to the original span in the knowledge space. Figure shows an example of the span masking and replacement tasks. To further explore effective ways to the joint training of the two tasks, we design two learning schemes, which we called two-tower scheme and pipeline scheme. Basically, the generator and discriminator are trained in parallel with the shared parameters in the two-tower scheme. While in the pipeline scheme, the output of generator is input to the successive discriminative training. The generator and discriminator in our KgPLM model are both pre-trained based on RoBERTa. They have some additional benefits: 1) the model can be readily extended to much larger pre-training corpus, which keeps some potential room for further improvement; 2) the model retains the same amount of parameters as RoBERTa, and does not require any modifications in fine-tuning for the downstream tasks.  We evaluate the model performance on LAMA~, which consists of several zero-shot knowledge completion tasks, and MRQA shared tasks~, which include several benchmark question answering datasets. The experiments show the proposed KgPLM, especially that trained with the pipeline scheme, achieves the state-of-the-art performance, and significantly outperform several strong baselines  on some of the tasks. The results indicate that the knowledge-guided generative and discriminative pre-training provides an effective way to incorporate external knowledge and achieve competitive performance on the knowledge intensive NLP tasks.     Most knowledge-guided pre-training methods can be categorized into groups according to their pre-training objectives  \footnote{There are other methods that follow other knowledge injection approaches, such as pre-training via knowledge embeddings.}:   generative objectives, and  discriminative objectives.  For masked language models, different masking mechanisms are always used to design the generative objectives. ERNIE-Baidu introduces new masking units such as phrases and entities to learn knowledge information in these masking units. As a reward, syntactic and semantic information from phrases and entities is implicitly integrated into the language model. SpanBERT extends subword-level masking to span-level masking, and selects random spans of full words to predict. In addition to the MLM objective, a span boundary objective is designed to predict each subword within a span using subword representations at the boundaries. \citet{joshi2020contextualized} utilize retrieved background sentences for phrases to extend the input text, and combine the TEK-enriched context and span masking to pre-train the language model. Besides, \citet{rosset2020knowledge} introduce entity information into an autoregressive language model, called KALM, which identifies the entity surface in a text sequence and maps word n-grams into entities to obtain an entity sequence for knowledge-aware language model pre-training.   \citet{ye2019align} propose a discriminative pre-training approach for incorporating commonsense knowledge into the language model, in which the question is concatenated with different candidates to construct a multi-choice question answering sample, and each choice is used to predict whether the candidate is the correct answer. KEPLER unifies knowledge representation learning and language modeling objectives, which builds a bridge between text representation and knowledge embeddings by encoding entity descriptions, and can better integrate factual knowledge into the pre-trained language model. \citet{xiong2019pretrained} introduce entity replacement checking task into the pre-trained language model, which greatly enhances the modeling of entity information. \citet{wang2020k} propose a plug-in way to infuse knowledge into language models, and different discriminative objectives are used to keep different kinds of knowledge in different adapters.     To the best of our knowledge, this work is the first to explore knowledge-guided pre-training that considers the generative and discriminative approaches simultaneously. Besides, our model does not involve any additional cost to downstream tasks in terms of parameters and computations.   SSPT  finetune    In this paper, we apply the dialogue discourse to model the structure of a meeting for meeting summarization. We first transform the entire meeting text and corresponding dialogue discourse relations into a discourse graph. Specifically, both the utterances and discourse relations are constructed as vertices, and we design six types of edge and a global vertex to facilitate the information flow. Moreover, we develop a Dialogue Discourse-Aware Graph Convolutional Networks  which consists of an utterance encoder, a graph encoder, and a pointer decoder. In addition, we construct a pseudo-summarization corpus by utilizing the question-answer discourse relation, which can be used to pre-train our model.  Experiments on the AMI dataset show the effectiveness of our model which can achieve the SOTA performance.       
","  Most knowledge-guided pre-training methods can be categorized into groups according to their pre-training objectives  \footnote{There are other methods that follow other knowledge injection approaches, such as pre-training via knowledge embeddings.}:   generative objectives, and  discriminative objectives.  For masked language models, different masking mechanisms are always used to design the generative objectives. ERNIE-Baidu introduces new masking units such as phrases and entities to learn knowledge information in these masking units. As a reward, syntactic and semantic information from phrases and entities is implicitly integrated into the language model. SpanBERT extends subword-level masking to span-level masking, and selects random spans of full words to predict. In addition to the MLM objective, a span boundary objective is designed to predict each subword within a span using subword representations at the boundaries. \citet{joshi2020contextualized} utilize retrieved background sentences for phrases to extend the input text, and combine the TEK-enriched context and span masking to pre-train the language model. Besides, \citet{rosset2020knowledge} introduce entity information into an autoregressive language model, called KALM, which identifies the entity surface in a text sequence and maps word n-grams into entities to obtain an entity sequence for knowledge-aware language model pre-training.   \citet{ye2019align} propose a discriminative pre-training approach for incorporating commonsense knowledge into the language model, in which the question is concatenated with different candidates to construct a multi-choice question answering sample, and each choice is used to predict whether the candidate is the correct answer. KEPLER unifies knowledge representation learning and language modeling objectives, which builds a bridge between text representation and knowledge embeddings by encoding entity descriptions, and can better integrate factual knowledge into the pre-trained language model. \citet{xiong2019pretrained} introduce entity replacement checking task into the pre-trained language model, which greatly enhances the modeling of entity information. \citet{wang2020k} propose a plug-in way to infuse knowledge into language models, and different discriminative objectives are used to keep different kinds of knowledge in different adapters.     To the best of our knowledge, this work is the first to explore knowledge-guided pre-training that considers the generative and discriminative approaches simultaneously. Besides, our model does not involve any additional cost to downstream tasks in terms of parameters and computations.   SSPT  finetune",202
" 	 	Knowledge graphs , such as WordNet , Freebase  and Wikidata , aggregate a large amount of human knowledge and express in a structured way. 	% are representative of existing KGs, in which knowledge is formalized as triples. 	%, such as  where  is the head entity,  is the tail entity, and  is the relation between these two entities. 	The large number of triples in these KGs have constructed a complex knowledge network, but it is far from complete. 	In recent years, knowledge graph completion  tasks have attracted great attention. 	 	 	 	Despite new state-of-the-art  models  emerge constently, most methods ignore the topological structure information of the KGs.  	Relation paths are the most common topological structure in KGs, and Figure shows some relation path instances.  	 is a relation triple, while  is a two-step relation path. 	Similar to word context in language models , relation paths can be considered as one kind of contextual information in KGs. 	We call it ``graph contextual information''. 	And Harris's famous distributional hypothesis   can also be extend to knowledge graphs: you shall know an entity by the relationships it involves. 	Although these two kinds of contextual information are similar, the latter has its own specialities. 	In knowledge graphs, not all relation paths are meaningful. 	For example,  is a valid relation path, but this does not indicate that there must be a relationship between  and . 	Unreliable relation paths are common in knowledge graphs, and  \citet{lin2015modeling} found that it is necessary to select reliable relation paths for knowledge representation learning. 	%In this work, a path-constraint resource allocation algorithm is proposed to measure the weights of inference patterns. 	They learn inference patterns between relations and paths to utilize knowledge contained in relation paths. 	%Despite its success, the modeling objects are more limited to the inference patterns between relations and paths. 	%Recently, \citeauthor{wang2019coke} \shortcite{wang2019coke} propose a method to model the contextual nature of triples and relation paths, and they explore the benefits of graph contextual information for link prediction tasks on two specific datasets. 	%However, simply adding graph contextual information  into the training pool is not always effective, and this operation may reduce the performance of the original model. 	Instead of relying on inference patterns, we propose PPKE, a path-based pre-training approach that integrates  graph contextual information contained in relation paths into the model parameters. 	We think this is a more general way to develop the unexploited graph contextual information. 	During the path-based pre-training procedure,  two-step relation paths are extracted from the knowledge graph and fed into the pre-training module with original triples. 	Then, the pre-trained model can be finetuned for downstream KGC tasks, such as link prediction and relation prediction. 	Our contributions are as follows: 	     In the literature,  relation paths have been utilized to improve the performance of KG-related tasks. One of the representative models is PtransE  , which learns inference patterns from relation paths to improve the knowledge base completion tasks. In this work, a path-constraint resource allocation algorithm is proposed to measure the weights of inference patterns, and semantic composition of relation embeddings is utilized to represent relation paths. Despite its success, the modeling objects are limited to the inference patterns between relations and paths, and it did not model  the contextual information that implicited in paths.  Our work aims to model the graph contextual information contained in relation paths and use it to improve task performance.  Recently, several KRL methods have attempted to introduce more contextual information into knowledge representations. Relational Graph Convolutional Networks   is proposed to learn entity embeddings from their incoming neighbors, which greatly enhances the information interaction between related triples. \citeauthor{nathani-etal-2019-learning} \shortcite{nathani-etal-2019-learning} further extend the information flow from 1-hop in-entities to n-hop during the learning process of entity representations.  , but a re-evaluation study shows that this method is not always effective .  KG-BERT Moreover, contextualized knowledge representation learning method  is proposed to model the contextual nature of triples and relation paths . However, these methods do not experiment with benchmark datasets to verify that graph context information can improve the model performance . We believe that the information contained in knowledge graphs has not been sufficiently exploited. In this study, we develop a knowledge graph pre-training model to integrate more graph contextual information, and utilize this model to benefit KG-related tasks through a finetuning procedure. 	 	   We have proposed a pre-training method by cooperatively modeling the generative and discriminative knowledge injecting approaches. Our model can be easily extended to larger pre-training corpus and does not introduce any modifications for downstream tasks during finetuning. Experiments show our model consistently outperforms all \texttt{BASE} models on a variety of question answering datasets, demonstrating that our KgPLM is a preferred choice for the knowledge intensive NLP tasks.   Our method uses two-tower and pipeline frameworks to integrate knowledge span masking with knowledge span checking for pre-training.   add TEK into pre-training and finetuning.    train from scratch   
","  In the literature,  relation paths have been utilized to improve the performance of KG-related tasks. One of the representative models is PtransE  , which learns inference patterns from relation paths to improve the knowledge base completion tasks. In this work, a path-constraint resource allocation algorithm is proposed to measure the weights of inference patterns, and semantic composition of relation embeddings is utilized to represent relation paths. Despite its success, the modeling objects are limited to the inference patterns between relations and paths, and it did not model  the contextual information that implicited in paths.  Our work aims to model the graph contextual information contained in relation paths and use it to improve task performance.  Recently, several KRL methods have attempted to introduce more contextual information into knowledge representations. Relational Graph Convolutional Networks   is proposed to learn entity embeddings from their incoming neighbors, which greatly enhances the information interaction between related triples. \citeauthor{nathani-etal-2019-learning} \shortcite{nathani-etal-2019-learning} further extend the information flow from 1-hop in-entities to n-hop during the learning process of entity representations.  , but a re-evaluation study shows that this method is not always effective .  KG-BERT Moreover, contextualized knowledge representation learning method  is proposed to model the contextual nature of triples and relation paths . However, these methods do not experiment with benchmark datasets to verify that graph context information can improve the model performance . We believe that the information contained in knowledge graphs has not been sufficiently exploited. In this study, we develop a knowledge graph pre-training model to integrate more graph contextual information, and utilize this model to benefit KG-related tasks through a finetuning procedure.",203
"   Data collection is an essential part of the field of spoken dialogue systems and conversational AI. %, and requires developers to make difficult decisions and budget accordingly.   In particular, designing a dialogue system for a completely new domain is still a very challenging task.  Data collection options include running lab-based experiments, crowd-sourced tasks  or gathering data from social media platforms, such as Reddit or Twitter. Ambitious large scale data collections across multiple domains have resulted in widely used datasets, such as MultiWOZ . % and collected from various platforms .% to create representations of dialogues in the vector space.   However, starting off in a new domain from scratch still has its challenges. Difficult and costly decisions have to be made as to how and where to collect the data.    A large majority of recent dialogue corpora has been collected using crowd-sourcing either by pairing workers and letting them chat, often about a given topic , or by asking them to add the next utterance to the dialogue given a set of conditions . Other studies have recruited subjects to play the role of the system, i.e., to act as a wizard or user . Each of these approaches has its own advantages and disadvantages, depending on if the dialogue is task-oriented or not. By letting users type in an unrestricted way, the richness of the dialogue increases, which is a positive feature for chit-chat. On the other hand, too much variability could be a problem for a high stakes, task-oriented dialogues, such as in the medical domain. Letting multiple users contribute with one utterance per dialogue , speeds up the data collection, however, dialogues may lack coherence and severely diverge from real dialogues. On the other hand, hiring and training subjects to chat or perform the wizard role results in a more controlled data collection but dramatically increases the cost of the data collection and makes it less scalable.     The quality of such datasets has been often assessed according to the degree of variability  observed  or the lexical complexity of the utterances collected . %, however to the best of our knowledge, there is no work assessing the impact of the different methods directly on training dialogue models.   %This paper aims at addressing this issue by investigating the impact of two different data collection methods on the performance of the model. Furthermore, most of the above-mentioned datasets focus on increasing the size of the dataset available for dialogue research, rather than investigating the impact of the data collection strategies on the performance of the models trained. The work presented in this paper aims at highlighting the pros and cons, using a methodology to quickly leverage a robust dialogue system, minimising the cost and effort involved in the data collection process. Analyses comparing different strategies for the data collection process across various platforms have been done in the past , but we are not aware of a similar study for dialogue data.  The data used in this study was collected in the scope of an emergency response system to be used on an off-shore energy platform as part of the EPSRC ORCA Hub programme . One of the collections was done using crowd-sourcing  and the second one was done in a lab using a Wizard-of-Oz setting, where participants were interacting either with a social robot or a smart speaker. Both datasets were used to train a dialogue model using an implementation of a Hybrid Code Network  and here we compare the results achieved by models trained on data collected by either method. To validate the use of crowd-sourced data to bootstrap a dialogue system for situated interaction, we ran experiments where we train the model on the crowd-sourced data and test it on the lab data, in order to verify if it %This will result in an estimate of the number of dialogues needed to  %varied the amount of crowd-sourced dialogues during training to estimate the necessary amount of crowd-sourced data needed to  achieves comparable performances with the models trained only with the lab data.   The contributions of this paper are as follows: 1) a comparison of models trained with two datasets collected in different ways but on the same task, 2) evidence that suggests that specialised dialogue tasks, such as our emergency response task, are not well covered by current pre-trained dialogue models, and 3) a set of recommendations regarding the data collection for dialogue research.\footnote{Please find code and data in: \href{https://github.com/zedavid/TheLabVsTheCrowd}{}.}  The paper is organised as follows. Section  will cover previous work related to this problem. Our experimental set-up will be introduced in Section , followed by the results in Section . The paper concludes with the discussion in Section  and future work and conclusions in Section .            In this section, we first discuss a number of previously used methods for dialogue data collection and then describe studies where researchers have compared different approaches in data collection and their impact the model performance.   Talk about different behaviour between on-line and lab participants, the degree of control.    Due to the high data demands of training dialogue models, running laboratory data collections at scale for this purpose has became impractical. One of the earliest large datasets for dialogue research was collected with the Let's Go system . This dataset includes 171,128 interactions of real-customers with a telephone-based agent who provided bus schedule information in Pittsburgh. A similar paradigm was used in \citet{williams2016dialog}, who collected 15K dialogues, however instead of real bus passengers, crowd-workers were paid to interact with systems over the phone. Nowadays, many data collections involve crowd-workers acting either as the wizard, as in the Wizard-of-Oz paradigm , or the user. However, the way they contribute might be different depending on the design of the data collection. In \citet{budzianowski2018multiwoz,wen-etal-2017-network,lee2019multi-domain,eric-etal-2017-key}, crowd-workers are shown the history of the dialogue and they are asked to add a coherent response. They are typically given some constraints, which they should take into account when acting as the wizard or agent. In addition, workers may also have to perform annotation on the previous user input, according to a set of instructions given. Although this method was mostly used for task-oriented data collections, it could also be used to collect a diverse chit-chat corpus . A variation on this data collection method is described in \citet{andreas2020dialogflow}, where the framework would generate a set candidates for the wizards to select from, in a similar fashion as to what was done in , for the data used in this paper.   While crowd-sourcing could be a good solution for creating large amounts of data, this method has an impact on the quality of the data, due to the rapid nature of the tasks. To increase the quality of the dataset in \citet{ElAsri2017}, instead of crowd-workers, a limited set of subjects were hired. Subjects contributed to the data collection both as the user and the wizard. They swapped roles during the data collection, which made them aware the agent was played by another human. This approach is not very scalable and therefore, to perform larger collections, a hybrid model where crowd-workers are paired with trained subjects to act as the wizard has been used by \citet{peskov2019multi}.   Hiring subjects to act as wizards is a strategy followed to improve not only the quality and consistency of the dialogues, but also the quality of the annotations, necessary for downstream tasks. This paradigm was also used in \citet{Byrne2019}, who collected a multi-domain corpus. In addition, authors have also collected a similar amount of data using self-authored dialogues, where a single crowd-worker authored both the agent and the user utterances. Unlike what was found by \citet{jonell2019crowdsourcing} for chit-chat dialogues, authors found that self-authored dialogues were richer in content and variability. A different approach has been used in a dataset where a simulator, a probabilistic automaton, was designed to create dialogues with a variety of trajectories . Once this structured representation of dialogue semantics is created, the crowd-workers' task is to paraphrase them into natural language.  While datasets such as these are widely used for research, they have a few limitations. Firstly, for research in spoken dialogue, their usage is limited since the datasets are mostly written, not spoken. The exception is \citet{Byrne2019}, where crowd-workers could listen to the agent response using TTS. However, even in this case, the timing aspect of spoken interaction is removed. Secondly, there is some overlap in the domains covered by these datasets . This raises questions about their possible usage for training models in totally new domains .   To mitigate the first limitation, there have been some collections in the wild, such as the one presented by \citet{siegert-2020-alexa}, where an Amazon Alexa was part of an itinerant exhibition, where visitors could interact with it using voice. This dataset contains more than 7,000 dialogues in German.  , which is an impressive number for an in-the-wild data collection, but still below the 10,000 dialogues collected in MultiWOZ.     Given that these datasets cover a variety of different domains, studies have been conducted investigating how a model trained on a source domain performs when tested on a different target domain . Another line of research has tried to find the minimal number of dialogues from the target domain that should be used during training to achieve a competitive performance .  However, since there is a fair amount of overlap between datasets, comparisons between models trained with different datasets are starting to emerge. The Schema Guided Dialogue  dataset collected by  \citet{rastogi2019towards}  was used to train a model, which was then tested in  MutilWOZ 2.1. Both datasets were crowd-sourced, but the methodology followed was slightly different. Despite this fact, the model trained with the SGD dataset outperformed the state-of-the art at the time, in terms of joint goal accuracy in MultiWOZ 2.1.   Recently, advances in Natural Language Processing have been made by training neural models with very large datasets and applying those models to a variety of downstream tasks. These pre-trained dialogue models have been released for general use , and are trained on a variety of sources, including Twitter and Reddit . While these models can address common-sense questions and perform multi-turn reasoning, they might fail to capture specific aspects of the task in task-oriented dialogues.  \citet{wu2020tod} attempted to address this with their pre-trained model, which was trained with an ensemble of several task-oriented dialogue datasets publicly available. This BERT-based model was successfully tested in different downstream tasks including dialogue act prediction, entity extraction in dialogue and response generation.   The examples describe attempt to create powerful general purpose neural models which can be used for a variety of tasks in dialogue. Instead of Reddit data, this model was trained using dialogues for a variety of domains such as restaurant booking, ordering food or asking for a taxi to give a few examples. While there has been shown that this strategy works quite well for a variety of downstream tasks, there might be cases where the domain is completely different to those used in dialogue research this far, which may result at this models falling short. There might be cases where rather than fine-tuning a pre-trained model, a new model has to be created from scratch. Therefore, a data collection has to be done. Access to data from human-human conversations in that domain could be a good starting point, however this is not always available.  In both \citet{wu2020tod} and \citet{rastogi2019towards}, models were trained and tested on data collected with different methods. However, their focus was rather on the overall performance of the models rather than in optimising the data collection.    trained a models for the restaurant domain using both artificial data and real data. The performance in the real data dropped significantly.     To alleviate the challenge of knowledge role missing in multi-choice MRC, this work makes the first attempt to integrating external knowledge based on span extraction into MRC modeling, presenting 	extbf{Reference Knowledgeable Network }, which can simulate the human strategy of reading comprehension and quote external knowledge for multi-choice MRC tasks.  RekNet helps achieve significantly performance improvement on two multi-choice MRC benchmarks RACE and DREAM, which passed the significance test. In the future, we will apply RekNet to other forms of MRC tasks.       
","    In this section, we first discuss a number of previously used methods for dialogue data collection and then describe studies where researchers have compared different approaches in data collection and their impact the model performance.   Talk about different behaviour between on-line and lab participants, the degree of control.    Due to the high data demands of training dialogue models, running laboratory data collections at scale for this purpose has became impractical. One of the earliest large datasets for dialogue research was collected with the Let's Go system . This dataset includes 171,128 interactions of real-customers with a telephone-based agent who provided bus schedule information in Pittsburgh. A similar paradigm was used in \citet{williams2016dialog}, who collected 15K dialogues, however instead of real bus passengers, crowd-workers were paid to interact with systems over the phone. Nowadays, many data collections involve crowd-workers acting either as the wizard, as in the Wizard-of-Oz paradigm , or the user. However, the way they contribute might be different depending on the design of the data collection. In \citet{budzianowski2018multiwoz,wen-etal-2017-network,lee2019multi-domain,eric-etal-2017-key}, crowd-workers are shown the history of the dialogue and they are asked to add a coherent response. They are typically given some constraints, which they should take into account when acting as the wizard or agent. In addition, workers may also have to perform annotation on the previous user input, according to a set of instructions given. Although this method was mostly used for task-oriented data collections, it could also be used to collect a diverse chit-chat corpus . A variation on this data collection method is described in \citet{andreas2020dialogflow}, where the framework would generate a set candidates for the wizards to select from, in a similar fashion as to what was done in , for the data used in this paper.   While crowd-sourcing could be a good solution for creating large amounts of data, this method has an impact on the quality of the data, due to the rapid nature of the tasks. To increase the quality of the dataset in \citet{ElAsri2017}, instead of crowd-workers, a limited set of subjects were hired. Subjects contributed to the data collection both as the user and the wizard. They swapped roles during the data collection, which made them aware the agent was played by another human. This approach is not very scalable and therefore, to perform larger collections, a hybrid model where crowd-workers are paired with trained subjects to act as the wizard has been used by \citet{peskov2019multi}.   Hiring subjects to act as wizards is a strategy followed to improve not only the quality and consistency of the dialogues, but also the quality of the annotations, necessary for downstream tasks. This paradigm was also used in \citet{Byrne2019}, who collected a multi-domain corpus. In addition, authors have also collected a similar amount of data using self-authored dialogues, where a single crowd-worker authored both the agent and the user utterances. Unlike what was found by \citet{jonell2019crowdsourcing} for chit-chat dialogues, authors found that self-authored dialogues were richer in content and variability. A different approach has been used in a dataset where a simulator, a probabilistic automaton, was designed to create dialogues with a variety of trajectories . Once this structured representation of dialogue semantics is created, the crowd-workers' task is to paraphrase them into natural language.  While datasets such as these are widely used for research, they have a few limitations. Firstly, for research in spoken dialogue, their usage is limited since the datasets are mostly written, not spoken. The exception is \citet{Byrne2019}, where crowd-workers could listen to the agent response using TTS. However, even in this case, the timing aspect of spoken interaction is removed. Secondly, there is some overlap in the domains covered by these datasets . This raises questions about their possible usage for training models in totally new domains .   To mitigate the first limitation, there have been some collections in the wild, such as the one presented by \citet{siegert-2020-alexa}, where an Amazon Alexa was part of an itinerant exhibition, where visitors could interact with it using voice. This dataset contains more than 7,000 dialogues in German.  , which is an impressive number for an in-the-wild data collection, but still below the 10,000 dialogues collected in MultiWOZ.     Given that these datasets cover a variety of different domains, studies have been conducted investigating how a model trained on a source domain performs when tested on a different target domain . Another line of research has tried to find the minimal number of dialogues from the target domain that should be used during training to achieve a competitive performance .  However, since there is a fair amount of overlap between datasets, comparisons between models trained with different datasets are starting to emerge. The Schema Guided Dialogue  dataset collected by  \citet{rastogi2019towards}  was used to train a model, which was then tested in  MutilWOZ 2.1. Both datasets were crowd-sourced, but the methodology followed was slightly different. Despite this fact, the model trained with the SGD dataset outperformed the state-of-the art at the time, in terms of joint goal accuracy in MultiWOZ 2.1.   Recently, advances in Natural Language Processing have been made by training neural models with very large datasets and applying those models to a variety of downstream tasks. These pre-trained dialogue models have been released for general use , and are trained on a variety of sources, including Twitter and Reddit . While these models can address common-sense questions and perform multi-turn reasoning, they might fail to capture specific aspects of the task in task-oriented dialogues.  \citet{wu2020tod} attempted to address this with their pre-trained model, which was trained with an ensemble of several task-oriented dialogue datasets publicly available. This BERT-based model was successfully tested in different downstream tasks including dialogue act prediction, entity extraction in dialogue and response generation.   The examples describe attempt to create powerful general purpose neural models which can be used for a variety of tasks in dialogue. Instead of Reddit data, this model was trained using dialogues for a variety of domains such as restaurant booking, ordering food or asking for a taxi to give a few examples. While there has been shown that this strategy works quite well for a variety of downstream tasks, there might be cases where the domain is completely different to those used in dialogue research this far, which may result at this models falling short. There might be cases where rather than fine-tuning a pre-trained model, a new model has to be created from scratch. Therefore, a data collection has to be done. Access to data from human-human conversations in that domain could be a good starting point, however this is not always available.  In both \citet{wu2020tod} and \citet{rastogi2019towards}, models were trained and tested on data collected with different methods. However, their focus was rather on the overall performance of the models rather than in optimising the data collection.    trained a models for the restaurant domain using both artificial data and real data. The performance in the real data dropped significantly.",204
"  . 	%  	% % final paper: en-us version  	% 	%	   % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/}. }  The widespread dissemination of fake news has lead to a significant influence on personal fame, public trust, and security. For example, spreading misinformation, such as ``Asians are more vulnerable to novel coronavirus''~\footnote{https://www.thestar.com.my/news/regional/2020/03/11/myth-busters-10-common-rumours-about-covid-19} about COVID-19 has very serious repercussions, making people ignore the harmfulness of the virus and directly affecting public health. Research has shown that misinformation spreads faster, farther, deeper, and more widely than true information. Therefore, fake news detection on social media has attracted tremendous attention recently in both research and industrial fields.   Early research on fake news detection mainly focused on the design of effective features from various sources, including textual content, user profiling data, and news diffusion patterns. Linguistic features, such as writing styles and sensational headlines, lexical and syntactic analysis, have been explored to separate fake news from true news. Apart from linguistic features, some studies also proposed a series of user-based features, and temporal features about the news diffusion. However, these feature-based methods are very time-consuming, biased, and require a lot of labor to design. Besides, these features are easily manipulated by users.   To solve the above problems, many recent studies apply various neural networks to automatically learn high-level representations for fake news detection. For example, recurrent neural network , convolutional neural network , matrix factorization and graph neural network are applied to learn the representation of content and diffusion graph of news. These methods only apply more types of information for fake news detection, but paying little attention to early detection. Moreover, these models can only detect fake news in consideration of all or a fixed proportion of repost information, while in practice they cannot detect fake news in the early stage of news propagation. Some studies explore to detect fake news early by relying on a minimum number of posts. The main limitation of these methods is that they ignore the importance of publishers' and users' credibility for the early detection of fake news.   When we humans see a piece of breaking news, we firstly may use common sense to judge whether there are factual errors in it. At the same time, we will also consider the reputation of the publishers and reposted users. People tend to believe the news from a trusted and authoritative source or the news shared by lots of users with a good reputation. If the publisher is reliable, we tend to believe this news. On the other hand, if the news is reposted by many low-reputation users in a short period, it may be that some spammers tried to heat up on the news, resulting in lower credibility of the news.   Inspired by the above observation, we explicitly take the credibility of publishers and users as supervised information, and model fake news detection as a multi-task classification task. We can annotate a small part of publishers and users by their historical publishing and reposting behaviors. Although the credibility of publishers and users does not always provide correct information, they are necessary complementary supervised information for fake news detection. To make the credibility information generalized to other unannotated users, we construct a heterogeneous graph to build the connections of publishers, news, and users. Through a graph-based encoding algorithm, every node in the graph will be influenced by the credibility of publishers and users.    In this paper, we address the following challenges:  How to fully encode the heterogeneous graph structure and news content; and  How to explicitly utilize the credibility of publishers and users for facilitating early detection of fake news. To tackle the above challenges, we propose a novel structure-aware multi-head attention network for the early detection of fake news. Firstly, we design a structure-aware multi-head attention module to learn the structure of the publishing graph and produce the publisher representations for the credibility prediction of publishers. Then, we apply the structure-aware multi-head attention module to encode the diffusion graph of the news among users and generate user representations for the credibility prediction of users. Finally, we apply a convolutional neural network to map the news text from word embedding to semantic space and utilize the fusion attention module to combine the news, publisher, and user representations for early fake news detection.   The contributions of this paper can be summarized as follows:            Early studies in fake news detection concentrate on designing some good features for separating fake news from true news. These features are mainly extracted from text content or users' profile information. Linguistic patterns, such as special characters and keywords, writing styles and sensational headlines, lexical and syntactic features, temporal-linguistic features, have been explored to detect fake news. Apart from linguistic features, some studies also proposed a series of user-based features, e.g. the number of fans, registration age, and genders to find clues for fake news detection.  However, the language used in social media is highly informal and ungrammatical, which makes traditional natural language processing techniques hard to effectively learn semantic information from news content. Second, designing effective functions is often time-consuming and relies heavily on expert knowledge in specific fields. Some features are often unavailable or inadequate in the early stage of news propagation.       Recurrent neural network , convolutional neural network  and graph neural network have been imported to learn the representations from news content or diffusion graph. Some studies also combine news content and users' response, such as conflicting viewpoints, topics, or stance, to find clues by neural networks for fake news detection. These methods only apply more types of information for fake news detection, but paying little attention to early detection.   Recently, some studies have proposed some methods to detect fake news at the early stage of propagation. However, these methods ignored the importance of publishers' and users' credibility for the early detection of fake news. Different from these studies, our method explicitly takes the credibility of publishers and users as weakly supervised information for facilitating fake news detection. We propose a novel deep learning model to simultaneously optimize the fake news detection task and users' credibility prediction task.      In this paper, we propose a framework for building general multilingual NLU models, which can be used across different marketplaces and languages.  To choose the model with the best performance, we use language-specific test sets to evaluate the candidate models and their corresponding baseline models  along four metrics, domain accuracy, intent accuracy, slot F1, and frame accuracy. The models which win in most of the evaluation metrics are the final picks. We find that models built from a simple multi-task biLSTM-CRF model setup are comparable to standard production models in terms of latency constraints required for on-the-fly voice assistant conversational models.  We observe performance improvements in all models with the introduction of transfer learning. Encoder transfer produced the greatest improvements whereas the transfer of the decoder did not bring much change when compared to the baseline model performance, except when tested on an English test set, when the transfer learning is performed from the model trained on English data. This is due to the fact that the target non-English language contains slots or intents which are not included in the pre-trained model, thus the decoder fails to predict correct classes simply because they are missing in the vocabulary. To mitigate this effect, a decoder with default initialization gives better performance because it now can embrace all available slots and intents in the target language realm.  Furthermore, we find that a model pre-trained in a multilingual setup performs better than the one trained on a monolingual data set. This confirms that a multilingual model built based on lexically and orthographically similar languages may provide more beneficial context information to any similar target language. Experimental result on Hindi show that such a multilingual model can work even for non-alike languages with the same or better performance improvement. This confirms that a common multilingual model can be used to support multiple language with better results than a set of monolingual models.  With a single general multilingual NLU model, bootstrapping new languages can be faster as we can use cross-lingual contextual information from all existing high-resource languages. At the same time, maintaining only one model requires much less effort in terms of regular model updates.     
","     Early studies in fake news detection concentrate on designing some good features for separating fake news from true news. These features are mainly extracted from text content or users' profile information. Linguistic patterns, such as special characters and keywords, writing styles and sensational headlines, lexical and syntactic features, temporal-linguistic features, have been explored to detect fake news. Apart from linguistic features, some studies also proposed a series of user-based features, e.g. the number of fans, registration age, and genders to find clues for fake news detection.  However, the language used in social media is highly informal and ungrammatical, which makes traditional natural language processing techniques hard to effectively learn semantic information from news content. Second, designing effective functions is often time-consuming and relies heavily on expert knowledge in specific fields. Some features are often unavailable or inadequate in the early stage of news propagation.       Recurrent neural network , convolutional neural network  and graph neural network have been imported to learn the representations from news content or diffusion graph. Some studies also combine news content and users' response, such as conflicting viewpoints, topics, or stance, to find clues by neural networks for fake news detection. These methods only apply more types of information for fake news detection, but paying little attention to early detection.   Recently, some studies have proposed some methods to detect fake news at the early stage of propagation. However, these methods ignored the importance of publishers' and users' credibility for the early detection of fake news. Different from these studies, our method explicitly takes the credibility of publishers and users as weakly supervised information for facilitating fake news detection. We propose a novel deep learning model to simultaneously optimize the fake news detection task and users' credibility prediction task.",205
"  Infusing emotions into conversation systems can substantially improve its usability and promote customers' satisfaction. Moreover, perceiving emotions sufficiently is the core premise of expressing emotions. In real-life scenarios, humans can instinctively perceive complex or subtle emotions from multiple aspects, including the emotion flow of dialogue history, facial expressions and personalities of speakers, and then express suitable emotions for feedback. Figure shows the organization of multi-source information in a dialogue graph and the relationship between them.       \paragraph{Dialog Systems.} In the past few years, previous studies mainly focus on improving the content quality of dialogue, and only a little work pays attention to improving the emotion quality of dialogue. Researchers firstly set user-input emotions directly for response generation , such as emotion-controllable conversation systems, which contain some representative work.  To automatically learn emotions , researchers track the emotion flow of dialog history and generate emotion-rich responses, where they incorporate Valence, Arousal, and Dominance embeddings into their models to provide additional affective knowledge. Besides,  multi-modal studies also have attracted much attentions in conversation systems such as video-grounded dialogue system and visual question-answering, aiming to answer human queries grounded a video or image.    閸欙箒鎹ｆ稉濞 瀹割喖绱撶佃鐦敍  The differences between our model and those in  and  are: they only perceive emotions from text and ignore other source knowledge, e.g., the facial expressions and speakers' personalities, where the speaker information has been shown helpful for conversation systems. The deep difference from  is: we focus on automatically learning emotions and then expressing it rather than emotion-controllable conversation systems .  Furthermore, we expect to construct a practical emotional conversation system, i.e. infusing multi-source knowledge with heterogeneous graph neural network for improving emotional conversation, so we employ a simple and general decoder. Obviously, a powerful emotion-aware decoder can be integrated into our framework, such as, which will be considered in our future work to further improve the emotional performance.   we design a heterogeneous graph-based network to perceive emotions from multiple sources for emotional conversation generation. In contrast to tasks in , we mainly utilize multi-modal information for emotion perception and expression in response generation.     Heterogeneous Graph for NLP.\quad Heterogeneous graph neural networks can deal with various types of nodes and edges and have more advantages than homogeneous graph neural networks. Its superiority has been verified in many natural language processing  tasks, such as graph representation learning, reading comprehension, text classification and extractive document summarization. Inspired by the success of heterogeneous graph neural network, we first introduce it to emotional conversation generation to gain a better understanding of content and fully perceive emotions from multi-source knowledge, and then produce a satisfactory response.     We presented an application of Soft Patterns -- a finite state automaton parameterized by a neural network -- as the encoder of a sequence-to-sequence model. We show that it is competitive with the popular LSTM encoder on character-level copy and morphological tagging, while providing interpretable patterns.  We analyzed the behavior of \sopa encoders on \morphana, \lemmatization and \copytask by computing the average Jaccard similarity between the patterns extracted from the source side. We found two trends that coincide with linguistic intuition. One is that \lemmatization and morphological analysis require patterns that match less similar subwords than the other two task pairs. The other one is that \copytask and morphological analysis are more similar in languages with rich inflectional morphology.  
"," \paragraph{Dialog Systems.} In the past few years, previous studies mainly focus on improving the content quality of dialogue, and only a little work pays attention to improving the emotion quality of dialogue. Researchers firstly set user-input emotions directly for response generation , such as emotion-controllable conversation systems, which contain some representative work.  To automatically learn emotions , researchers track the emotion flow of dialog history and generate emotion-rich responses, where they incorporate Valence, Arousal, and Dominance embeddings into their models to provide additional affective knowledge. Besides,  multi-modal studies also have attracted much attentions in conversation systems such as video-grounded dialogue system and visual question-answering, aiming to answer human queries grounded a video or image.    闁告瑱绠掗幑锝嗙▔婵 鐎瑰壊鍠栫槐鎾朵絻顫夐惁顕鏁  The differences between our model and those in  and  are: they only perceive emotions from text and ignore other source knowledge, e.g., the facial expressions and speakers' personalities, where the speaker information has been shown helpful for conversation systems. The deep difference from  is: we focus on automatically learning emotions and then expressing it rather than emotion-controllable conversation systems .  Furthermore, we expect to construct a practical emotional conversation system, i.e. infusing multi-source knowledge with heterogeneous graph neural network for improving emotional conversation, so we employ a simple and general decoder. Obviously, a powerful emotion-aware decoder can be integrated into our framework, such as, which will be considered in our future work to further improve the emotional performance.   we design a heterogeneous graph-based network to perceive emotions from multiple sources for emotional conversation generation. In contrast to tasks in , we mainly utilize multi-modal information for emotion perception and expression in response generation.     Heterogeneous Graph for NLP.\quad Heterogeneous graph neural networks can deal with various types of nodes and edges and have more advantages than homogeneous graph neural networks. Its superiority has been verified in many natural language processing  tasks, such as graph representation learning, reading comprehension, text classification and extractive document summarization. Inspired by the success of heterogeneous graph neural network, we first introduce it to emotional conversation generation to gain a better understanding of content and fully perceive emotions from multi-source knowledge, and then produce a satisfactory response.",206
" Text classification is one of the fundamental tasks in natural language processing  with wide applications such as sentiment analysis, news filtering, spam detection and intent recognition. Plenty of algorithms, especially deep learning-based methods, have been applied successfully in text classification, including recurrent neural networks ,  convolutional networks   . More recently, large pre-training language models such as ELMO , BERT , Xlnet  and so on have also shown their outstanding performance in all kinds of NLP tasks, including text classification.   Although numerous deep learning models have shown their success in text classification problems, they all share the same learning paradigm: a deep model for text representation, a simple classifier to predict the label distribution and a cross-entropy loss between the predicted probability distribution and the one-hot label vector. However, this learning paradigm have at least two problems:  In general text classification tasks, one-hot label representation is based on the assumption that all categories are independent with each other. But in real scenarios, labels are often not completely independent and instances may relate to multiple labels, especially for the confused datasets that have similar labels. As a result, simply representing the true label by a one-hot vector fails to take the relations between instances and labels into account, which further limits the learning ability of current deep learning models.  The success of deep learning models heavily relies on large annotated data, noisy data with labeling errors will severely diminish the classification performance, but it is inevitable in human-annotated datasets. Training with one-hot label representation is particularly vulnerable to mislabeled samples as full probability is assigned to a wrong category. In brief, the limitation of current learning paradigm will lead to  confusion in prediction that the model is hard to distinguish some labels, which we refer as label confusion problem . A label smoothing  method is proposed to remedy the inefficiency of one-hot vector labeling , however, it still fails to capture the realistic relation among labels, therefore not enough the solve the problem.      In this work, we propose a novel Label Confusion Model  as an enhancement component to current deep learning text classification models and make the model stronger to cope with label confusion problem. In particular, LCM learns the representations of labels and calculates their semantic similarity with input text representations to estimate their dependency, which is then transferred to a label confusion distribution . After that, the original one-hot label vector is added to the LCD  with a controlling parameter and normalized by a softmax function to generate a simulated label distribution . We use the obtained SLD to replace the one-hot label vector and supervise the training of model training. With the help of LCM, a deep model not only capture s the relations between instances and labels, but also learns the overlaps among different labels, thus, performs better in text classification tasks. We conclude our contributions as follows:      Deep learning models have been widely use in natural language processing, including text classification problems. The studies of deep text representations can be categorized into two groups. One is focusing on the word embeddings. Another group mainly study the deep learning structures that can learn better text representations. Typical deep structures include recurrent neural networks  based long short-term memory  , convolutional neural networks   and context-dependent language models like BERT .The reason why deep learning methods have become so popular is their ability to learn sophisticated semantic representations from text, which are much richer than hand-crafted features.    Label smoothing  is first proposed in image classification tasks as a regularization technique to prevent the model from predicting the training examples too confidently, and has been used in many state-of-the-art models, including image classification , language translation  and speech recognition . LS improves model accuracy by computing loss not with the ``hard"" one-hot targets, but with a weighted mixture of these targets with a uniform noise distribution.  Nevertheless, the label distribution generated form LS cannot reflect the true label distribution for each training sample, since it is obtained by simply adding some noise. The true label distribution should reveal the semantic relation between the instance and each label, and similar labels should have similar degree in the distribution. In nature, label smoothing encourages the model to learn less, rather than learn more accurately of the knowledge in training samples, which may have the risk of underfitting.   Label embedding is to learn the embeddings of the labels in classification tasks and has been proven to be effective.  convert labels into semantic vectors and thereby convert the classification problem into vector matching tasks. Then attention mechanisms are used to jointly learn the embedding of words and labels .  use label embedding in a sequence generation model for multi-label classification which captures the co-relation between labels. In our work, we also use jointly learn the label embeddings, which can be used to further capture the semantic relation between text and labels.   Label Distribution Learning   is a novel machine learning paradigm for applications where the overall distribution of labels matters. A label distribution covers a certain number of labels, representing the degree to which each label describes the instance. LDL is proposed for problems where the distribution of labels matters.  gives out several algorithms for this kind of tasks. However, the true label distribution is hard to obtain for many existing classification tasks such as 20NG  and MNIST   where we only have a unique label for each sample. In this kind of classification tasks, LDL is not applicable.      }        We propose a heterogeneous graph-based framework to understand dialogue content and fully perceive complex and subtle emotions from multi-source knowledge to generate coherent and emotional response. Experimental results and analysis demonstrate the effectiveness and generalizability of our model, which can be easily adapted to different number of knowledge sources. In the future, we would like to infuse knowledge from more sources and further investigate various relations between them to further improve the quality of responses.  
","  Deep learning models have been widely use in natural language processing, including text classification problems. The studies of deep text representations can be categorized into two groups. One is focusing on the word embeddings. Another group mainly study the deep learning structures that can learn better text representations. Typical deep structures include recurrent neural networks  based long short-term memory  , convolutional neural networks   and context-dependent language models like BERT .The reason why deep learning methods have become so popular is their ability to learn sophisticated semantic representations from text, which are much richer than hand-crafted features.    Label smoothing  is first proposed in image classification tasks as a regularization technique to prevent the model from predicting the training examples too confidently, and has been used in many state-of-the-art models, including image classification , language translation  and speech recognition . LS improves model accuracy by computing loss not with the ``hard"" one-hot targets, but with a weighted mixture of these targets with a uniform noise distribution.  Nevertheless, the label distribution generated form LS cannot reflect the true label distribution for each training sample, since it is obtained by simply adding some noise. The true label distribution should reveal the semantic relation between the instance and each label, and similar labels should have similar degree in the distribution. In nature, label smoothing encourages the model to learn less, rather than learn more accurately of the knowledge in training samples, which may have the risk of underfitting.   Label embedding is to learn the embeddings of the labels in classification tasks and has been proven to be effective.  convert labels into semantic vectors and thereby convert the classification problem into vector matching tasks. Then attention mechanisms are used to jointly learn the embedding of words and labels .  use label embedding in a sequence generation model for multi-label classification which captures the co-relation between labels. In our work, we also use jointly learn the label embeddings, which can be used to further capture the semantic relation between text and labels.   Label Distribution Learning   is a novel machine learning paradigm for applications where the overall distribution of labels matters. A label distribution covers a certain number of labels, representing the degree to which each label describes the instance. LDL is proposed for problems where the distribution of labels matters.  gives out several algorithms for this kind of tasks. However, the true label distribution is hard to obtain for many existing classification tasks such as 20NG  and MNIST   where we only have a unique label for each sample. In this kind of classification tasks, LDL is not applicable.      }",207
" Over recent years, various task-oriented conversational agents, such as Amazon Alexa, Apple閳ユ獨 Siri, Google Assistant, and Microsoft閳ユ獨 Cortana, have become more popular in people閳ユ獨 everyday life and are expected to be highly intelligent. For the NLU component, this means that we expect models to perform recognition of the actions and entities within a user閳ユ獨 request with high accuracy. When first training an NLU model on a new language , there is a strong requirement for high quality annotated data that would support the most common user requests across a range of domains. As the modeling space expands to support new features and additional languages, NLU models are regularly re-trained on updated data sets to ensure support for these new functions. The major bottleneck in both of these processes is the labor and cost associated with collecting and annotating new training utterances for every new feature or language.   Recent advances in machine learning methods, including the use of techniques such as transfer learning~ and active learning, can lead to more efficient data usage by NLU models and therefore decrease the need for annotated training data. Additionally, data augmentation models are being widely explored. The advantage of data augmentation is that once synthetic data is generated, it can be ingested into subsequent models without additional effort, allowing for faster experimentation.   NLU models in dialog systems can perform a variety of tasks. In this study, we will focus on three of them: Domain classification  -- identify the domain that the user request belongs to , Intent classification  -- extract actions requested by users , and Named Entity Recognition  -- identify and extract entities  from user requests.  For each utterance we expect our NLU model to output a domain, intent, and set of extracted entities with corresponding tags. For example, if a user requests ``play Bohemian Rhapsody by Queen'', we expect the NLU model to return \{domain: music, intent: play\_song, named\_entities: [, ]\}. We call this output annotation, and the utterance along with annotation is called an annotated utterance. Named entities with corresponding labels are called slots.  For our NLU model to perform well on real-time user requests, we need to train it on a large dataset of diverse annotated utterances. However, there could be some areas of functionality where large datasets for training are not available. To boost model performance in situations where training data is limited, we use synthetic data generated from a small set of unique utterances that cover the basic functionality of the user experience, called Golden utterances. We leverage a Sequence Generative Adversarial Networks  introduced by~\citet{Yu2016SeqGANSG} to generate new utterances from this ``seed'' set, and use these generated utterances to augment training data and evaluate the performance of the classification and recognition tasks. We also investigate how the metrics that we use to evaluate the quality of the generated synthetic data links to the performance boost in the underlying tasks.     NLU model boosting through training data augmentation has been an active area of research over the last few years, with more sophisticated techniques and models being developed. Some of these techniques include data resampling, the use of Variational Autoencoders  and GANs. \citet{Xie2017DataNA} generalize resampling methods by proposing noising schemes that are designed to smooth input data by randomly changing the word tokens in a sentence. First described by, VAEs learn distributed representations of latent variables, and decode random samples to generate data that have similar characteristics to those that the network was trained on. GAN model proposed by includes two competing neural networks: a generator that creates fake data, and a discriminator that is trained to distinguish between fake and real data. The generator is trained on the results of its success in fooling the discriminator and this contest results in synthetic data that is progressively more similar to real data.  Synthetic data have shown to be useful for IC model boosting. For example, \citet{Malandrakis2019ControlledTG} explored a set of encoder-decoder models and proposed the use of conditional VAEs  to generate phrase templates, called carrier phrases. Authors used CVAEs to control the domain, intent, and slot types to generate desirable outputs that resulted in a higher F1 score on the intent classification task.  \citet{Kumar2019ACL} focused on a few-shot IC problem where new categories with limited training data are introduced into an existing system with mature categories. They compared different techniques that were designed to augment training data, including upsampling, random perturbation, extrapolation, CVAEs, and delta-encoders, and combined feature space augmentation with popular BERT pre-training to provide better performance.  The use of GANs has been previously explored for text data augmentation in language modeling  and sentiment classification. However, discrete text sequence generation brings about several challenges: first, one needs to generate a set of discrete tokens from a random sample of real-valued continuous data, and second, GANs are designed to give feedback on entire sequences, whereas generators need guidance for each subsequent token. The SeqGAN model developed by attempts to resolve these issues by applying reinforcement algorithms for the GAN objective with a policy gradient that evaluates current state-action value using Monte Carlo  search. In this work, we adopt a SeqGAN model to boost DC, IC, and NER tasks in NLU models that suffer from sparse data limitations.    In this work, we propose Label Confusion Model  as an enhancement component to current text classification models to improve their performance. LCM can capture the relations between instances and labels as well as the dependency among labels. Experiments on five benchmark datasets proved LCM's enhancement on several popular deep learning models such as LSTM, CNN and BERT.  Our future work include the following directions:  Designing a better LCM structure for computer vision tasks and conducting more experiments on image classification.  Generalizing the LCM method to multi-label classification problems and label distribution prediction.   
"," NLU model boosting through training data augmentation has been an active area of research over the last few years, with more sophisticated techniques and models being developed. Some of these techniques include data resampling, the use of Variational Autoencoders  and GANs. \citet{Xie2017DataNA} generalize resampling methods by proposing noising schemes that are designed to smooth input data by randomly changing the word tokens in a sentence. First described by, VAEs learn distributed representations of latent variables, and decode random samples to generate data that have similar characteristics to those that the network was trained on. GAN model proposed by includes two competing neural networks: a generator that creates fake data, and a discriminator that is trained to distinguish between fake and real data. The generator is trained on the results of its success in fooling the discriminator and this contest results in synthetic data that is progressively more similar to real data.  Synthetic data have shown to be useful for IC model boosting. For example, \citet{Malandrakis2019ControlledTG} explored a set of encoder-decoder models and proposed the use of conditional VAEs  to generate phrase templates, called carrier phrases. Authors used CVAEs to control the domain, intent, and slot types to generate desirable outputs that resulted in a higher F1 score on the intent classification task.  \citet{Kumar2019ACL} focused on a few-shot IC problem where new categories with limited training data are introduced into an existing system with mature categories. They compared different techniques that were designed to augment training data, including upsampling, random perturbation, extrapolation, CVAEs, and delta-encoders, and combined feature space augmentation with popular BERT pre-training to provide better performance.  The use of GANs has been previously explored for text data augmentation in language modeling  and sentiment classification. However, discrete text sequence generation brings about several challenges: first, one needs to generate a set of discrete tokens from a random sample of real-valued continuous data, and second, GANs are designed to give feedback on entire sequences, whereas generators need guidance for each subsequent token. The SeqGAN model developed by attempts to resolve these issues by applying reinforcement algorithms for the GAN objective with a policy gradient that evaluates current state-action value using Monte Carlo  search. In this work, we adopt a SeqGAN model to boost DC, IC, and NER tasks in NLU models that suffer from sparse data limitations.",208
"  	Encoder-decoder architecture~ has been extensively used in neural machine translation ~. Given a source sentence, an encoder firstly converts it into hidden representations, which are then conditioned by a decoder to generate the target sentence. Attention mechanism~ is very effective in learning the alignment between a source sentence and a target sentence. Hence, attention mechanism is usually used in the architecture to improve its capability, such as capturing long-distance dependencies.  	Similar to traditional machine learning efforts~, some recent approaches in deep learning attempt to improve encoder-decoder architecture with multiple passes of decoding~. NMT refers this to polish mechanism~. Under this scheme, more than one translations are generated for a source sentence and, except for the first translation, each of them is based on the translation from the previous decoding pass. While these methods have achieved promising results, they lack a proper termination policy to the multi-turn process. \citet{xia2017deliberation,zhang2018asynchronous} adopt a fixed number of decoding passes that can be inflexible in deciding the optimal number of decoding passes. \citet{geng2018adaptive} use reinforcement learning ~ to automatically decide the optimal number of decoding passes. However, RL is unstable due to its high variance of gradient estimation and objective instability~. Since these methods may have premature termination or over translation, their potential can be limited.  	 	 To address this problem, we propose a novel framework, Rewriter-Evaluator, in this paper. It consists of a rewriter and an evaluator. The translation process involves multiple passes. Given a source sentence, at every pass, the rewriter generates a new target sequence aiming at improving the translation from prior passes, and the evaluator measures the translation quality to determine whether to terminate the rewriting process. We also propose a prioritized gradient descent  method that facilitates training the rewriter and the evaluator jointly. The essential idea is using a priority queue to improve sampling efficiency by collecting the translation cases that yield low scores from the evaluator for next-pass rewriting. The size of the queue is a few times larger than the batch size. Although Rewriter-Evaluator involves multiple decoding passes, training time using PGD method is comparable to that of training an encoder-decoder~ that doesn't have multiple decoding passes.  	  	 We apply Rewriter-Evaluator to improve the widely used NMT models,  RNNSearch~ and Transformer~. Extensive experiments have been conducted on two translation tasks, Chinese-English and English-German, to verify the proposed method. The results demonstrate that the proposed framework notably improves the performance of NMT models and significantly outperforms prior methods.     	 		 			 		 	   	Our work is closely related to recent efforts in end-to-end multi-pass decoding~. The models generate multiple target sentences for a source sentence and, except for the first one, each of them is based on the sentence generated in previous turn. For example, \citet{xia2017deliberation} propose deliberation network that uses a second decoder to polish the raw sequence produced by the first-pass decoder. While these methods have achieved promising results, they lack proper termination policy to the multi-pass translation process. \citet{zhang2018asynchronous} adopt a predefined number of decoding passes, which is not flexible. \citet{geng2018adaptive} incorporate post-editing mechanism into NMT model via RL. However, RL is notoriously unstable for training because of the high variance of gradient estimation.  	An alternative line of research focuses on computer-assisted translation ~ that collaborates existing machine translation technologies with human translators.  In such a situation, quality estimation  and automatic post-editing  play important roles in reducing human efforts. Word-level QE~ assigns a label of OK or BAD to every word in the translation. For example, work in \citet{basu2018keep} measures the similarity of the source context of the target word with the context for which the word is retained. APE corrects typical and repetitive mistakes found in the generated sequence. For instance, work in \citet{vu2018automatic} interleaves generating and executing the edit actions to rectify errors. Most recently, some works explore multi-task learning of QE and APE~, which is analogous to us. While the translation quality has indeed been improved, these approaches heavily rely on extra handcraft annotation that is expensive and domain-specific. Moreover, they essentially serve as the post-process modules in a pipeline-based translation system, instead of directly empowering machine translation models with mechanisms to estimate and improve the translation quality.    In this paper, we evaluate the use of the SeqGAN model for synthetic annotated data generation to boost NLU model performance. We have shown that adding synthetic data to bolster our Goldens can significantly improve DNN model performance in intent classification and named entity recognition tasks. We propose a token-level reward with Monte Carlo search rollout to guide the generator model, that showed better performance when compared with a regular token-level reward implementation, sentence-level reward implementations both with and without Monte Carlo tree search, and with a pure upsampling strategy. We also show that using SeqGAN together with embeddings pre-trained on high-resource domains to generate synthetic data can significantly improve the performance of low-resource domains. Embeddings pre-trained on different tasks can carry over the information they have learned and that can be especially useful in low-resource model building scenarios.      \onecolumn   
","  	 		 			 		 	   	Our work is closely related to recent efforts in end-to-end multi-pass decoding~. The models generate multiple target sentences for a source sentence and, except for the first one, each of them is based on the sentence generated in previous turn. For example, \citet{xia2017deliberation} propose deliberation network that uses a second decoder to polish the raw sequence produced by the first-pass decoder. While these methods have achieved promising results, they lack proper termination policy to the multi-pass translation process. \citet{zhang2018asynchronous} adopt a predefined number of decoding passes, which is not flexible. \citet{geng2018adaptive} incorporate post-editing mechanism into NMT model via RL. However, RL is notoriously unstable for training because of the high variance of gradient estimation.  	An alternative line of research focuses on computer-assisted translation ~ that collaborates existing machine translation technologies with human translators.  In such a situation, quality estimation  and automatic post-editing  play important roles in reducing human efforts. Word-level QE~ assigns a label of OK or BAD to every word in the translation. For example, work in \citet{basu2018keep} measures the similarity of the source context of the target word with the context for which the word is retained. APE corrects typical and repetitive mistakes found in the generated sequence. For instance, work in \citet{vu2018automatic} interleaves generating and executing the edit actions to rectify errors. Most recently, some works explore multi-task learning of QE and APE~, which is analogous to us. While the translation quality has indeed been improved, these approaches heavily rely on extra handcraft annotation that is expensive and domain-specific. Moreover, they essentially serve as the post-process modules in a pipeline-based translation system, instead of directly empowering machine translation models with mechanisms to estimate and improve the translation quality.",209
"   % \subsection{Problem Statement and Motivation}  Researchers' ability to automate natural language processing has grown exponentially over the past few years, particularly with the advent of the Transformer architecture . Despite the fact that recent machine learning methods achieve impressive and almost human-level performance on tasks such as dialogue modeling  and natural language generation , many intelligent voice assistants still rely on rule-based architectures and cached responses in open domain dialogue . This is primarily due to the lack of controls in deep learning architectures for producing specific phrases, tones, or topics, which makes these models inherently unpredictable and therefore too risky for most entities - corporate or otherwise - who wish to deploy public-facing intelligent agents. For example, it is often desirable for a conversational agent to maintain a specific identity  throughout an exchange of dialogue and it is currently impossible to condition deep learning algorithms to maintain a coherent identity across dialogue without training them on highly specialized  data sets. Fine-tuning on these specialized data sets comes with an additional, significant cost: it can lead to catastrophic forgetting of the language model . Despite this aspect of fine-tuning, current state-of-the-art methods  require fine-tuning  of the entire network when their original data set proves unsuitable for a given task , even if the language being modeled is the same across tasks. Furthermore, models produced by current methods are almost entirely uninterpretable and therefore generally difficult to test for egregious failure cases.  % \subsection{Solution Overview}  In this paper, we address both the issue of content control as well as that of catastrophic forgetting induced by fine-tuning. We define `content control' as being able to command a network to either incorporate or eschew an exact word, phrase, topic, style, or sentiment in its output, and therefore attempt a more granular level of control than the purely topic/style-level control that has been published in recent literature . We also introduce an alternative to fine-tuning neural language models and demonstrate through experimentation that the high-cost of overwriting model weights through fine-tuning  often fails to induce the desired behavior in generalized settings.  %is inspired by the ``No Free Lunch"" theorems introduced by Wolpert \& Macready  in that we seek to avoid training a neural network to simultaneously model language and act on explicit commands.  Instead,  we recast the problem of control in natural language generation as one of combining separate models - one of the natural language itself and one of high-level command responses - to produce desired linguistic output. In doing so, we develop a framework for interpreting and subsequently controlling the hidden activations of a pretrained neural network without any adjustments being made to the pretrained model. This framework is biologically consistent with the findings of Knutson et al., who discovered that neural pathways in humans are inhibited by other neuron clusters , and has applications to other neural network architectures and questions outside the domain of controllable text generation.        Recently, several high-profile publications in controllable natural language generation have reported impressive results. Particularly interesting is the work regarding the CTRL  and Meena  neural networks, however, these projects represent single neural networks that model both natural language as well as input commands simultaneously. Our work differs from CTRL  and Meena  in that we seek to  achieve content control and  separate the language model from the control model to avoid fine-tuning the language model. The recently published GPT-3  language model is capable of learning new linguistic modeling tasks without any fine-tuning but, as the authors state in their paper, it is a predictive model and therefore not ideal for goal-oriented text generation. Recently, several publications in controllable natural language generation have reported impressive results. Particularly interesting is the approach taken by Plug and Play Language Models  , which bears significant resemblance to our own despite the two approaches having been developed completely independently . Our work differs from PPLM in multiple ways, foremost among them being our distinct uses of neural networks to produce perturbations in the pretrained model. Rather than using summed discriminator gradients to produce perturbations as with PPLM's attribute models, we train a neural network  to produce perturbations adversarially against a discriminator; this makes our approach compatible with arbitrary discriminator algorithms, including any which may not provide gradients to the perturbation model  during training . Our novel approach also enables us to avoid adopting a greedy approach to textual control , whereas the PPLM paper is focused primarily on greedy control. The differences in our methodology carry over to our data sets, where our novel data curation approach  does not require pre-labeled text data exemplifying target behavior and which could be generalized to non-textual inputs . In Section 4 we present experimental results comparing the NPI and PPLM approaches which demonstrate the relative strengths of both algorithms. , and discuss the merits of each algorithm.  The work regarding the CTRL , Meena , and GPT-3  neural networks also demonstrates significant progress in controllable natural language generation, however, these projects represent single neural networks that model both natural language as well as input commands simultaneously. Our work differs from CTRL  and Meena  in that we seek to  achieve content control and  separate the language model from the control model to avoid fine-tuning the language model. The recently published GPT-3  language model is capable of learning new linguistic modeling tasks without any fine-tuning, but, as the authors state in their paper, it is a predictive model and therefore not ideal for goal-oriented text generation.  While there has been work which suggests neural architectures can learn to multi-task , multi-tasking networks are typically trained to model various highly-correlated tasks . We argue that modeling natural language and responding to high-level commands are two fundamentally different tasks, and that a neural network can therefore not optimally model both tasks, as suggested by the ``No Free Lunch""  theorems by Wolpert \& Macready .   Thus, the theoretical ideas which form the foundation for our approach to controllable natural language generation are the ``No Free Lunch"" theorems by Wolpert \& Macready , which state that ``what an algorithm gains in performance on one class of problems is necessarily offset by its performance on the remaining problems"". We therefore contend that a network that is optimized to model natural language cannot also be optimal for the significantly different task of producing controlled text in accordance with given commands, and vice versa. While there has been work which suggests neural architectures can learn to multi-task , multi-tasking networks are typically trained to model various highly-correlated tasks . Indeed, our own approach leverages a network that models a sequence of controls in parallel, similar to the multiple time-step modeling problem addressed in Feng Jin \& Shiliang Sun's ""Neural network multitask learning for traffic flow forecasting"" . However, we hypothesize that the ``No Free Lunch"" theorems  still apply for tasks that differ in domain and scope as significantly as modeling natural language differs with high-level command response.  Rather than model these tasks jointly, our proposed method of targeted text generation makes no permanent changes to a pretrained language model - which is assumed to be an  optimal model of natural language - and instead leverages an additional neural network  trained on a separate objective to manipulate the inner workings of the pretrained language model in response to high-level commands.  We focus our experiments on controlling the output of the autoregressive GPT-2 language model , though other autoregressive transformers such as Transformer-XL , XLNet , and GPT-3  could theoretically be controlled in a similar manner, and we propose further research be done into controlling these models. We use several variations on vanilla feed-forward neural network architectures  in various capacities, as well as an adversarial GAN-style setup , for training our NPI network. The NPI loss function was inspired by the Style-Transfer loss function introduced by Gatys, Ecker, \& Bethge . Much of our code was adapted from the Hugging Face Transformers GitHub repository  and is available online .  Our methodology, which seeks to alter the functionality of pretrained networks, has significant crossover with adversarial attacks on neural networks .  as well as the field of ablation testing for neural networks .  Our work differs from that of adversarial perturbations of seq-2-seq models  in that we formulate our Style-Transfer loss function such that it encourages the perturbed outputs to be as close to the original outputs as possible while still introducing the desired content, whereas related work does not incorporate this condition .     In this work, we highlight open challenges in the existing multilingual approach by  and . Specifically, we show that large pre-trained multi-lingual LMs are not enough for this task. We produce  several novel strategies for multilingual QA that go beyond zero-shot training and outshine the previous baseline built on top of \mbert{}. We present a translation model that has 14 times more training data. Further, our AT and LAF strategies utilize translation as data augmentation to bring the language-specific embeddings of the LM closer to each other. These approaches help us significantly improve the cross-lingual transfer. Empirically, our models demonstrate strong results and all approaches improve over the previous ZS strategy. We hope these techniques spur further research in the field such as exploring other multilingual LMs and invoking additional networks on top of large LMs for multilingual NLP. 
","     Recently, several high-profile publications in controllable natural language generation have reported impressive results. Particularly interesting is the work regarding the CTRL  and Meena  neural networks, however, these projects represent single neural networks that model both natural language as well as input commands simultaneously. Our work differs from CTRL  and Meena  in that we seek to  achieve content control and  separate the language model from the control model to avoid fine-tuning the language model. The recently published GPT-3  language model is capable of learning new linguistic modeling tasks without any fine-tuning but, as the authors state in their paper, it is a predictive model and therefore not ideal for goal-oriented text generation. Recently, several publications in controllable natural language generation have reported impressive results. Particularly interesting is the approach taken by Plug and Play Language Models  , which bears significant resemblance to our own despite the two approaches having been developed completely independently . Our work differs from PPLM in multiple ways, foremost among them being our distinct uses of neural networks to produce perturbations in the pretrained model. Rather than using summed discriminator gradients to produce perturbations as with PPLM's attribute models, we train a neural network  to produce perturbations adversarially against a discriminator; this makes our approach compatible with arbitrary discriminator algorithms, including any which may not provide gradients to the perturbation model  during training . Our novel approach also enables us to avoid adopting a greedy approach to textual control , whereas the PPLM paper is focused primarily on greedy control. The differences in our methodology carry over to our data sets, where our novel data curation approach  does not require pre-labeled text data exemplifying target behavior and which could be generalized to non-textual inputs . In Section 4 we present experimental results comparing the NPI and PPLM approaches which demonstrate the relative strengths of both algorithms. , and discuss the merits of each algorithm.  The work regarding the CTRL , Meena , and GPT-3  neural networks also demonstrates significant progress in controllable natural language generation, however, these projects represent single neural networks that model both natural language as well as input commands simultaneously. Our work differs from CTRL  and Meena  in that we seek to  achieve content control and  separate the language model from the control model to avoid fine-tuning the language model. The recently published GPT-3  language model is capable of learning new linguistic modeling tasks without any fine-tuning, but, as the authors state in their paper, it is a predictive model and therefore not ideal for goal-oriented text generation.  While there has been work which suggests neural architectures can learn to multi-task , multi-tasking networks are typically trained to model various highly-correlated tasks . We argue that modeling natural language and responding to high-level commands are two fundamentally different tasks, and that a neural network can therefore not optimally model both tasks, as suggested by the ``No Free Lunch""  theorems by Wolpert \& Macready .   Thus, the theoretical ideas which form the foundation for our approach to controllable natural language generation are the ``No Free Lunch"" theorems by Wolpert \& Macready , which state that ``what an algorithm gains in performance on one class of problems is necessarily offset by its performance on the remaining problems"". We therefore contend that a network that is optimized to model natural language cannot also be optimal for the significantly different task of producing controlled text in accordance with given commands, and vice versa. While there has been work which suggests neural architectures can learn to multi-task , multi-tasking networks are typically trained to model various highly-correlated tasks . Indeed, our own approach leverages a network that models a sequence of controls in parallel, similar to the multiple time-step modeling problem addressed in Feng Jin \& Shiliang Sun's ""Neural network multitask learning for traffic flow forecasting"" . However, we hypothesize that the ``No Free Lunch"" theorems  still apply for tasks that differ in domain and scope as significantly as modeling natural language differs with high-level command response.  Rather than model these tasks jointly, our proposed method of targeted text generation makes no permanent changes to a pretrained language model - which is assumed to be an  optimal model of natural language - and instead leverages an additional neural network  trained on a separate objective to manipulate the inner workings of the pretrained language model in response to high-level commands.  We focus our experiments on controlling the output of the autoregressive GPT-2 language model , though other autoregressive transformers such as Transformer-XL , XLNet , and GPT-3  could theoretically be controlled in a similar manner, and we propose further research be done into controlling these models. We use several variations on vanilla feed-forward neural network architectures  in various capacities, as well as an adversarial GAN-style setup , for training our NPI network. The NPI loss function was inspired by the Style-Transfer loss function introduced by Gatys, Ecker, \& Bethge . Much of our code was adapted from the Hugging Face Transformers GitHub repository  and is available online .  Our methodology, which seeks to alter the functionality of pretrained networks, has significant crossover with adversarial attacks on neural networks .  as well as the field of ablation testing for neural networks .  Our work differs from that of adversarial perturbations of seq-2-seq models  in that we formulate our Style-Transfer loss function such that it encourages the perturbed outputs to be as close to the original outputs as possible while still introducing the desired content, whereas related work does not incorporate this condition .",210
"  Emotion analysis of user-generated content  available on the web provides insights toward making meaningful decisions. Micro-blog platforms such as Twitter has gained profuse popularity for textual content holding people's opinions. The past decade has seen the active growth in emotion analysis models in many domains. Recently there has been an increasing interest in analysis of emotions of informal short texts such as tweets. In this paper, we introduce and analyze a system to accurately identify the emotions of the individual tweets with the associated intensities~\footnote{Intensity refers to the degree or amount of an emotion}.  % explain why it is important to analyze emotions  Analyzing emotions in social media such as twitter benefits society in a number of ways. Policymakers can use emotional information in social media to accurately identify concerns of people when making decisions. Monitoring social media for health issues benefits not only public health but also government decision makers. Furthermore, organizations can monitor opinion of the public on their products and services to provide better service to the society. Once emotions are recognized, emotion intensity can be used to prioritize the major concerns.  Studies in emotion analysis have often focused on emotion classification. However, emotions may exhibit varying levels of intensities. Here, emotion intensity can be defined as the degree or the intensity of particular emotion felt by the speaker. Additionally, we may observe multiple emotions simultaneously in the same tweet with varying intensities.   One purpose of this study is to develop a model to accurately identify the emotions and associated emotion intensities for a given tweet. In this paper, we propose a transfer learning approach backed by a neural network classifier and a regressor. Although the proposed neural network alone is inadequate to beat the benchmark, we show that features learned when training the above neural networks can be used to improve the overall performance when combined with other features.  Another purpose of this study is to explain how the input word level features affect the features extracted by the neural network.  % [complete the actual findings here] The findings should make an important contribution in understanding how features are used in a neural network and to effectively select features to improve the effectiveness of extracted features.   Our main contributions of this study:   \pagebreak  Major challenge in using deep learning to train emotion intensity prediction models is the lack of large labeled datasets. More recently, emoji and hashtags were used in studies to create large naturally labeled datasets. However, it is not possible to use a similar technique to obtain the intensity of emotions. Furthermore, creating a large dataset manually is time consuming and expensive.  are some existing datasets for emotional intensity prediction. Due to the limited amount of task-specific training data the previous researches have opted for transfer learning approaches~\citet{baziotis2018ntua, duppada2018seernet} and traditional machine learning. However, in this paper we argue that even with reasonable size dataset we can train a neural network to obtain good performance provided that there is proper regularization. Additionally, we show that features learned when training the neural network can be combined with other features to improve the overall performance of emotion intensity prediction.   % [explain methodology in brief]  % } \end{table}  In \S, we outline related works on sentiment and emotion mining. Next, in \S we will discuss the datasets used in this study. After, we introduce the background and our methodology in \S and \S accordingly. Then, in \S we will discuss the evaluation results. Finally, we will conclude this paper in \S.        [Emotion classification] Sentiment Analysis has become an important area, particularly when trying to analyze social media. Early examples of research into sentiment analysis involve in polarity classification of the textual input. In recent years, there has been an increasing amount of literature on algorithms for emotion analysis which are closely aligned with our work. \citet{eisner2016emoji2vec} has introduced emoji2vec, a method to obtain emoji embedding from existing pre-trained word2vec using emoji definitions. They have shown the importance of their emoji embedding by using it in sentiment analysis task. DeepMoji is a neural network trained on large twitter corpus naturally labeled for emoji. They have used a deep neural network with hidden bi-directional long short term memory layers  and an attention layer. This study has shown that transfer learning from DeepMoji can improve the performance of emotion and sentiment classification.    [discuss on SemEval/ WASSA papers] Several studies have investigated the approaches to predict the intensity of emotions in tweets. \citet{rosenthal2015semeval, kiritchenko2016semeval} have laid the groundwork for determining sentiment intensity of English phrases by introducing it as a shared task. Following the work of \citet{kiritchenko2016semeval}, \citet{mohammad2017emotion}~and have introduced datasets for emotion intensity prediction in tweets. Various studies have been carried out on creating models for predicting emotion intensities of tweets. The model presented by \citet{duppada2018seernet} has achieved a 79.9\  Pearson correlation score in emotion intensity dataset presented in. Their model composed of a stacked ensemble of xgboost regressors and random forest regressors via a meta-regressor and currently holds the benchmark results. In this approach, each base regressor is trained with a specific set of features transferred from a pre-trained model. This prohibits the ability to combine transfer features from multiple pre-trained models at the initial levels.   A number of published studies try to utilize Deep Neural Networks  to analyze emotions. Up to now, far too little attention has been paid to explain those models. \citet{mohammad2018semeval} has studied how a number of systems perform under different biases. However, there are no insights on how input features are being used to make the prediction. \citet{baziotis2018ntua} has developed a methodology based on a deep attentive RNNs and transfer learning to analyze emotions while presenting the weight given by the self-attention mechanism as a viable solution to visualizing the word level importance. However, this does not provide holistic view of entire model.       The key contribution and insight of this paper is the use of a small, independently trained neural network called a Neural Programming Interface  to influence the behavior of a large pretrained model. In contrast to fine-tuning, this approach retains the linguistic breadth and versatility of the original model, allowing the possibility to control for multiple factors either in sequence or simultaneously, and to induce behavior in the language model contrary to the patterns baked into linguistic training data . We have demonstrated that this approach can be used to produce specific words within a GPT-2 model's output text, to pivot away from a specific word, and to create a linguistic aversion to offensive speech. We believe that future avenues for this research include investigations of the use for NPI models in network interpretability, regulation, and bias mitigation.  
","     [Emotion classification] Sentiment Analysis has become an important area, particularly when trying to analyze social media. Early examples of research into sentiment analysis involve in polarity classification of the textual input. In recent years, there has been an increasing amount of literature on algorithms for emotion analysis which are closely aligned with our work. \citet{eisner2016emoji2vec} has introduced emoji2vec, a method to obtain emoji embedding from existing pre-trained word2vec using emoji definitions. They have shown the importance of their emoji embedding by using it in sentiment analysis task. DeepMoji is a neural network trained on large twitter corpus naturally labeled for emoji. They have used a deep neural network with hidden bi-directional long short term memory layers  and an attention layer. This study has shown that transfer learning from DeepMoji can improve the performance of emotion and sentiment classification.    [discuss on SemEval/ WASSA papers] Several studies have investigated the approaches to predict the intensity of emotions in tweets. \citet{rosenthal2015semeval, kiritchenko2016semeval} have laid the groundwork for determining sentiment intensity of English phrases by introducing it as a shared task. Following the work of \citet{kiritchenko2016semeval}, \citet{mohammad2017emotion}~and have introduced datasets for emotion intensity prediction in tweets. Various studies have been carried out on creating models for predicting emotion intensities of tweets. The model presented by \citet{duppada2018seernet} has achieved a 79.9\  Pearson correlation score in emotion intensity dataset presented in. Their model composed of a stacked ensemble of xgboost regressors and random forest regressors via a meta-regressor and currently holds the benchmark results. In this approach, each base regressor is trained with a specific set of features transferred from a pre-trained model. This prohibits the ability to combine transfer features from multiple pre-trained models at the initial levels.   A number of published studies try to utilize Deep Neural Networks  to analyze emotions. Up to now, far too little attention has been paid to explain those models. \citet{mohammad2018semeval} has studied how a number of systems perform under different biases. However, there are no insights on how input features are being used to make the prediction. \citet{baziotis2018ntua} has developed a methodology based on a deep attentive RNNs and transfer learning to analyze emotions while presenting the weight given by the self-attention mechanism as a viable solution to visualizing the word level importance. However, this does not provide holistic view of entire model.",211
"  Online reviewing for businesses becomes more and more important nowadays, where customers can publish their reviews for businesses, and other potential customers or shop owners can view them. Positive feedback from customers may prosper the store businesses, while negative one could have opposite consequences. Yelp, one of the largest company founded in 2004 for publishing crowd-sourced reviews about businesses, provides one open dataset, Yelp Open Dataset , which has tremendously many data about businesses, reviews, and users. Such dataset has been proven to be a good material for personal, educational, and academic purposes.  Among multiple tasks on the Yelp Open Dataset, predicting ratings for restaurants based their reviews is one of fundamental and important tasks. This task can help Yelp classify reviews into proper groups for its recommendation system, detect anomaly reviews to protect businesses from malicious competitions, and assign rating to texts automatically.  Yelp review rating prediction can be done in multiple ways, such as sentiment analysis and 5-star rating classification. In this paper, we will focus on rating prediction for restaurants based only on their review texts. This task can be viewed as a multiclass classification problem, where the input is the textual data , and output is the predicted class . We will apply both machine learning and deep learning models. After analyzing data distribution, splitting datasets, and extracting features, we will use four machine learning methods, including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine  . Then we will focus on four transformer-based models, including BERT , DistilBERT , RoBERTa , and XLNet , where several different architectures will be tried with hyperparameter tuning. This project is done on \href{https://colab.research.google.com/}{Google Colab}, where multi-processors and GPUs are available. The code is publicly available at GitHub .      In past years, many projects have been done on the Yelp Open Dataset. The data distribution has been analyzed thoroughly in . The review rating prediction has also been performed. Multiple feature generation methods and several machine learning models including Naive Bayes, Logistic Regression, Support Vector Machine , and Gaussian Discriminant Analysis have been used for this classification task in . One reported best accuracy for the 5-star classification is 64\  on the testing set in . Some deep learning model such as Neural Network, Recurrent Neural Network , Long Short-Term Memory , and Bidirectional Encoder Representations  were also applied in .   -------------------------------------------------------------------------------    In this study, we propose a simple yet effective model for emotion classification and emotion intensity prediction in Tweets while suggesting a method to explain and visualize a trained DNN. We utilized a neural network with LSTM layer followed by a convolution layer with max-pooling for emotion category classification as well as emotion intensity prediction. We extend this work by transferring features from above models and two state-of-the-art models trained for different tasks to a XGBoost regressor to predict the emotion intensity in Tweets more accurately. Moreover, we suggest a technique to visualize and interpret the feature importance of trained DNNs for emotion intensity prediction. In the future, we plan on experimenting with using attentive mechanisms to improve the emotion intensity prediction further. Our models outperformed existing state-of-the-art models for emotion classification and in predicting fear and anger emotion intensities, while maintaining a competitive results in predicting other emotions.  
","  In past years, many projects have been done on the Yelp Open Dataset. The data distribution has been analyzed thoroughly in . The review rating prediction has also been performed. Multiple feature generation methods and several machine learning models including Naive Bayes, Logistic Regression, Support Vector Machine , and Gaussian Discriminant Analysis have been used for this classification task in . One reported best accuracy for the 5-star classification is 64\  on the testing set in . Some deep learning model such as Neural Network, Recurrent Neural Network , Long Short-Term Memory , and Bidirectional Encoder Representations  were also applied in .   -------------------------------------------------------------------------------",212
"  Language processing requires tracking information over multiple timescales. To be able to predict the final word ``timescales"" in the previous sentence, one must consider both the short-range context  and the long-range context . How do humans and neural language models encode such multi-scale context information? Neuroscientists have developed methods to study how the human brain encodes information over multiple timescales during sequence processing. By parametrically varying the timescale of intact context, and measuring the resultant changes in the neural response, a series of studies  showed that higher-order regions are more sensitive to long-range context change than lower-order sensory regions. These studies indicate the existence of a ``hierarchy of processing timescales"" in the human brain. More recently, \citet{chien2020constructing} used a time-resolved method to investigate how the brain builds a shared representation, when two groups of people processed the same narrative segment preceded by different contexts. By directly mapping the time required for individual brain regions to converge on a shared representation in response to shared input, we confirmed that higher-order regions take longer to build a shared representation. Altogether, these and other lines of investigation suggest that sequence processing in the brain is supported by a distributed and hierarchical structure: sensory regions have short processing timescales and are primarily influenced by the current input and its short-range context, while higher-order cortical regions have longer timescales and track longer-range dependencies .  How are processing timescales organized within recurrent neural networks  trained to perform natural language processing? Long short-term memory networks   have been widely investigated in terms of their ability to successfully solve sequential prediction tasks. However, long-range dependencies have usually been studied with respect to a particular linguistic function , and there has been less attention on the broader question of how sensitivity to prior context -- broadly construed --  is functionally organized within these RNNs. Therefore, drawing on prior work in the neuroscience literature, here we demonstrate a model-free approach to mapping processing timescale in RNNs. We focused on existing language models that were trained to predict upcoming tokens at the word level  and at the character level . The timescale organization of these two models both revealed that the higher layers of LSTM language models contained a small subset of units which exhibit long-range sequence dependencies; this subset includes previously reported units  as well as previously unreported units.  After mapping the timescales of individual units, we asked: does the processing timescales of each unit in the network relate to its functional role, as measured by its connectivity? The question is motivated by neuroscience studies which have shown that in the human brain, higher-degree nodes tend to exhibit slower dynamics and longer context dependence than lower-degree nodes . More generally, the primate brain exhibits a core periphery structure in which a relatively small number of ``higher order閳 and high-degree regions  maintain a large number of connections with one another, and exert a powerful influence over large-scale cortical dynamics . Inspired by the relationships between timescales and network structure in the brain, we set out to test corresponding hypotheses in RNNs:  Do units with longer-timescales tend to have higher degree in neural language models? and  Do neural language models also exhibit a ``core network"" composed of functionally influential high-degree units? Using an exploratory network-theoretic approach, we found that units with longer timescales tend to have more projections to other units. Furthermore, we identified a set of medium-to-long timescale ``controller"" units which exhibit distinct and strong projections to control the state of other units, and a set of long-timescale ``integrator units"" which showed influence on predicting words where the long context is relevant. In summary, these findings advance our understanding of the timescale distribution and functional organization of LSTM language models, and provide a method for identifying important units representing long-range contextual information in RNNs.     Linguistic Context in LSTMs. How do LSTMs encode linguistic context at multiple timescales? Prior work suggested that the units sensitive to information that requires long-range dependencies are sparse. By ablating one unit at a time, \citet{lakretz2019emergence} found two units that encode information required for processing long-range subject-verb number agreement . They further identified several long-range ``syntax units"" whose activation was associated with syntactic tree-depth. Overall, \citet{lakretz2019emergence} suggests that a sparse subset of units tracks long-range dependencies related to subject-verb agreement and syntax. If this pattern is general -- i.e.\ if there are very few nodes tracking long-range dependencies in general -- this may limit the capacity of the models to process long sentences with high complexity, for reasons similar to those that may limit human sentence processing . To test whether long-range nodes are sparse in general, we require a model-free approach for mapping the context dependencies of every unit in the language network.   Whole-network context dependence. Previous work by \citet{khandelwal2018sharp} investigated the duration of prior context that LSTM language models use to support word prediction. Context-dependence was measured by permuting the order of words preceding the preserved context, and observing the increase in model perplexity when the preserved context gets shorter. \citet{khandelwal2018sharp} found that up to 200 word-tokens of prior context were relevant to the model perplexity, but that the precise ordering of words only mattered within the most recent 50 tokens. The token-based context-permutation method employed in this study was analogous to the approach used to measure context-dependence in human brain responses to movies  and to auditory narratives .  Inspired by the findings of \citet{khandelwal2018sharp} and \citet{lakretz2019emergence}, in the present study we set out to map the context-dependence across all of the individual units in the LSTM model. This enabled us to relate the timescales to the effects of node-specific ablation and the network architecture itself. In addition, our context manipulations included both context-swapping  and context-shuffling , which allowed us to better understand how individual words and syntactically structured word-sequences contribute to the the context representation of individual hidden units.     In this paper, we predicted ratings from Yelp review texts. Yelp Open Dataset was used. The imbalanced data distribution was presented, and a balanced training dataset was built. Four machine learning models including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine were used based on numerical features from tf-idf vectorizer. Four transformer-based models including BERT, DistilBERT, RoBERTa, and XLNet were also trained and tested on the textual data. Comparisons between models and hyperparameters were done, and 64\  accuracy score for the machine learning model and 70\  accuracy score for the transformer-based one were achieved on the testing set.   Transformer-based models were summarized and experimented. Cased, large BERT models were found giving better performances than the uncased, base ones. DistilBERT has a faster computation speed with a bit lower metrics, while RoBERTa and XLNet give higher evaluation metrics with more computational resources required.  We hope our work could give some inspirations and insights for further work in Yelp review rating prediction based on machine learning and deep learning models.   ----------------------------------------------------------- {\small   } 
","  Linguistic Context in LSTMs. How do LSTMs encode linguistic context at multiple timescales? Prior work suggested that the units sensitive to information that requires long-range dependencies are sparse. By ablating one unit at a time, \citet{lakretz2019emergence} found two units that encode information required for processing long-range subject-verb number agreement . They further identified several long-range ``syntax units"" whose activation was associated with syntactic tree-depth. Overall, \citet{lakretz2019emergence} suggests that a sparse subset of units tracks long-range dependencies related to subject-verb agreement and syntax. If this pattern is general -- i.e.\ if there are very few nodes tracking long-range dependencies in general -- this may limit the capacity of the models to process long sentences with high complexity, for reasons similar to those that may limit human sentence processing . To test whether long-range nodes are sparse in general, we require a model-free approach for mapping the context dependencies of every unit in the language network.   Whole-network context dependence. Previous work by \citet{khandelwal2018sharp} investigated the duration of prior context that LSTM language models use to support word prediction. Context-dependence was measured by permuting the order of words preceding the preserved context, and observing the increase in model perplexity when the preserved context gets shorter. \citet{khandelwal2018sharp} found that up to 200 word-tokens of prior context were relevant to the model perplexity, but that the precise ordering of words only mattered within the most recent 50 tokens. The token-based context-permutation method employed in this study was analogous to the approach used to measure context-dependence in human brain responses to movies  and to auditory narratives .  Inspired by the findings of \citet{khandelwal2018sharp} and \citet{lakretz2019emergence}, in the present study we set out to map the context-dependence across all of the individual units in the LSTM model. This enabled us to relate the timescales to the effects of node-specific ablation and the network architecture itself. In addition, our context manipulations included both context-swapping  and context-shuffling , which allowed us to better understand how individual words and syntactically structured word-sequences contribute to the the context representation of individual hidden units.",213
"    We summarize our contribution as follows:        Because of the significance of keyphrase, producing the succinct and accurate keyphrase can effectively help downstream tasks. Existing methods are mainly categorized into keyphrase extraction and keyphrase generation. \paragraph{Keyphrase Generation} Traditional keyphrase production methods directly extract significant spans as keyphrases through candidates extracting  and ranking . However, these traditional methods can only extract keyphrases that appear in the text with a sequential order. To produce absent keyphrases, which  do not appear as any continuous subsequence of text, sequence-to-sequence based models become a popular trend.  Techniques include  CopyRNN , CorrRNN  with coverage mechaism and reviewer mechanism, TG-Net, and reinforcement learning. However, these works do not consider the meta-sentence information or the logical structure of the document, which provides important clues for generating keyphrases.  \paragraph{Incorporating Document Structure} Documents such as science literatures and news articles are well structured. The hierarchical structure of the documents are used in various NLP tasks to boost the performance. \citet{yang2016hierarchical} incorporate the knowledge of the hierarchical structure to text classification model. \citet{alzahrani2012using} utilizes the structural information to detect significant plagiarism problems. \citet{frermann2019inducing} induce latent document structure for generating aspect-based summarization. And in Keyphrase Extraction area, \citet{nguyen2010wingnus} propose to add document structure to hand-crafted features to help identify candidates, and \citet{chen2019guided} treat the document title as a dominant role to the overall document and let it guide the generation process. And in our work, we consider the logical structure of the whole documents , and propose a sentence selection module to determine which part of the document is important. However it is difficult for the module to learn the binary indicator from scratch, and hence, we adopt weakly supervised learning in our training process.    \paragraph{Weakly Supervised Learning} For lacking enough annotated data, weakly supervised learning has been an active research direction in various NLP applications.         and they mainly contain two steps: candidates extracting and ranking. The first step produces several candidate phrases with heuristic methods such as Part-of-Speech tags or n-grams. Then the candidate phrase will be sorted according to the probability of becoming the keyphrase with supervised method or unsupervised method . However, these traditional extraction methods cannot produce absent keyphrases.     To solve the problem that the extraction model cannot produce absent keyphrase,  first proposes a seq-to-seq model named CopyRNN, which is enhanced with an attention mechanism and copy mechanism. Since then, various of mechanisms   and methods are applied to this framework to improve performance.   CorrRNN introduces the Coverage mechanism and proposes the review mechanism to ease coverage and duplication issues.     TG-Net makes use of title information and regards the title as extra information to guide the source encoding, they think previous approaches treat the title and main body equally, and they use beam search to generate multiple keyphrases. \citet{yuan2018one} proposes two methods catSeq and catSeqD that can generate multiple keyphrases one time. Recently,  introduces   reinforcement learning  with an adaptive reward to encourage the model to generate more accurate and proposes a new evaluation method which can accurately deal with the problem of synonym.     We follow the \citet{yuan2018one} and train our model to generate a diverse number of keyphrases. Similar to TG-net, our model also focuses on significant information in the document. But to be different from TG-net,  our model pays more attention to the all significant sentences.   \newcommand{\tabincell}[2]{ }        \caption{Case of three kinds of keyphrase, A letter stands for a word. Semi-present keyphrase is that all words in this keyphrase exist in a particular sentence. }              We demonstrated a new method for mapping the timescale organization in recurrent neural language models. Using this method, we mapped the timescale distributions of units within word-level and character-level LSTM language models, and identified a small set of units with long timescales. We then used network analyses to understand the relationship between the timescale of a unit and its connectivity profile, and we distinguished two subsets of long-timescale units with seemingly distinctive functions. Altogether, we proposed methods combining timescale and connectivity analyses for discovering timescale and functional organization in language models.  The units with longer processing timescales included some units whose role in long-range language dependencies had already been established , but almost all of the long timescale units are of unknown function. The timescale mapping procedure described here provides a model-free method for identifying nodes necessary for long-range linguistic and discursive processes . Future studies of these neural language models could focus on the specific linguistic information tracked by the long-timescale units, especially the ``controller'' units which control the information flow of other units in the network.   The current study measured unit timescales using a simple token distance, and so the method may be applied to understanding recurrent neural nets beyond language models. It will be insightful for future studies to investigate whether the processing timescales characterized via token distance are comparable to those measured using functional measures, such as syntactic distance. Relatedly, while we explored the timescale variance under several context conditions, a more thorough investigation will be needed to examine how the timescales of individual units may vary at different positions within a sentence, both in terms of token location and syntactic location.  Processing timescales may exhibit an analogous hierarchical organization in LSTMs and in the human cerebral cortex: in both cases, a subset of nodes with high degree and high inter-connectivity express unusually long timescales. More detailed testing of this apparent correspondence is required, however, because units within an LSTM layer are not spatially embedded and constrained as in biological brains, and thus the LSTM units do not express a spatially graded timescale topography.  \subsubsection*{Acknowledgments} C.J.H and H-Y.S.C gratefully acknowledge the support of the National Institutes of Mental Health         \section{Appendix}  \subsection {Units excluded from timescale analysis}   We excluded 1 unit in the WLSTM model and 5 units in CLSTM model which were not properly fit using the logistic function; we further excluded 14 units in the WLSTM model and 7 units in the CLSTM model which either did not show a non-zero activation difference before the shared segment started, or whose activation differences increased when started to process the shared segment. After these exclusions, 635 units remained in the WLSTM and 1012 units remained in the CLSTM for further analysis.  \subsection {Timescale analyses across different datasets and context conditions}  \subsubsection {Wikipedia test dataset}   The Anna Karenina corpus used in the current study has a different linguistic structure from the Wikipedia corpus on which the WLSTM and CLSTM models were trained. Although we analyzed only the Anna Karenina sentences with low perplexity, it was important to test the robustness of our results across datasets. Thus, we mapped the timescale of each unit using the Wikipedia test set, as used by \citet{gulordava2018colorless}. Specifically, we sampled 500 long sentences containing ``, and"" for the Intact Context condition. As before, we generated sentences by preceding the ``shared input'' segment  with either the original prior context segment, or a randomly chosen prior context segment. Same as the original analysis, we then replaced the context segment with 30 context segments randomly sampled from other parts of the test set for generating the Random Context condition. The mapped timescales using the Wikipedia test set were highly correlated with the novel corpus, suggesting the robustness of unit timescales .   \subsubsection {Timescales measured in the middle of a sentence}   To examine how the timescales of individual units may vary across different positions in a sentence, we varied the location of the segmentation point. Instead of using the conjunction  as the segmentation point, we chose an arbitrary segmentation point: the 15th token of a long sentence, to separate context segment and shared input segment. In the Random Context condition, we replaced the context segment with the first 15 tokens from other sentences of the corpus. We found that the unit timescales were highly correlated with the condition where we used the conjunction as the segmentation point with several units shift their timescales to either directions .  This analysis was conducted using Wikipedia test set.   \subsubsection {Timescale reset at the beginning of a sentence}   To examine if the timescales of individual units can flexibly reset at the beginning of a sentence, we conducted the same timescale analysis but using a ``full stop"" as the segmentation point instead of the conjunction ``, and"". Thus, if the original test string was ``The girl kicked the call, and the boy caught it"", then the full-stop version of the test string would be ``The girl kicked the ball. The boy caught it."" In this setting, the context segment and shared input segment in the Intact Context condition are two consecutive sentences. To ensure the temporal dependence between the context segment and shared input segment, we sampled 100 consecutive sentence pairs from the Anna Karenina corpus. Note that this is not possible using the Wikipedia test set from \citet{gulordava2018colorless}, because that set is composed of unrelated sentences. The Random Context condition was generated by replacing the first sentence with randomly sampled sentences from other parts of the novel. We found that when using ``full stop"" to segment context and shared input, most units in the network showed timescale near 0, indicating near-zero dependence on the linguistic context from the text preceding the full stop . This suggests that the units in LSTM tend to ``reset"" their context representation at the beginning of a sentence.   \subsubsection {Context representation shaped by individual words }   Inspired by the token-shuffling procedure of \citet{khandelwal2018sharp}, we explored whether the context representations of individual units in the LSTM were shaped by individual words, rather than coherent sequences of words. For this analysis, instead of replacing the context with syntactically structured segments from other part of the corpus, we generated the ``random context"" by shuffling the order of words within the context segment. We then mapped the unit timescales as before, by examining the unit activation difference as a function of the distance from the onset of shared input. Intriguingly, we found that most of the units showed similar timescales across the context-replacement and context-shuffling procedures . This suggests that the context representations in LSTMs largely depend on the presence of individual words in the context, rather than their appearance within coherent linguistic sequences. However, we did observe a subset of units  whose timescales were longer when context was replaced rather than shuffled. For this subset of units, the ability to maintain a representation of prior context over many tokens depends on that prior context being a coherent linguistic sequence. This subset of units are a promising target for future studies of syntactic representations in LSTMs.  \subsection {Identifying strong hidden-to-gate projections}  First, for each hidden unit, we concatenated the corresponding rows in the  and  matrices, to generate a single ``hidden-to-gate"" projection vector for that hidden unit. Next we we z-scored the vector to get standardized projection values from that unit to all other units in the network. Using z-score 5 as criterion, we identified a total of 258 ``strong projections"" from all hidden units to the input gate and forget gate in the WLSTM. The projection strength of each unit was then calculated based on its number of ""strong projections"" . Although the criterion z-score was selected to better visualize the results in Figure , different criteria did not change the results that units with longer timescales have more strong projections. For example, using z-score 3 as threshold we obtained corr = 0.30, p0.001; z-score 4 we obtained corr = 0.35, p0.001.  Next, we identified the edges corresponding to the top 258 magnitude weight-values within the combined  and  matrices. Together, these edges formed a  ""strong-projection network"". Finally, we used k-core analysis to identify the main core of the strong-projection network. This main core composed our ""controller units"" .   Using the same criteria and method, we identified a total of 390 ``strong projections"" from all hidden units to the input gate and forget gate in the CLSTM. We then extracted the top 390 weight values from the weight matrices to construct a ``strong-projection network"" and again identified the main core network, composed the ``controller units"" for the CLSTM model    \subsection {Ablation analyses on putative controller and integrator units}  To examine the non-trivial roles of the controller and integrator units identified in the word-level LSTM model, we performed a preliminary group ablation analysis to look at how ablating the controller units influences model performance on predicting the next token, relative to the ablation of a random set of units. Specifically, since long-timescale integrator units should have most effect predicting tokens at the later part of the sentences , we examined the model performance on predicting tokens at two different positions:  all the tokens regardless of their positions in the sentences , and  the last tokens of sentences .   We evaluated the effects of ablation on model performance by measuring the differences of probabilities  assigned to the target words . Ablation effects for controller units  and integrator units  were compared against a baseline of ablating the same number of randomly-selected units from layer 2 of the LSTM . We used the test corpus used by \citet{gulordava2018colorless} and measured the average performance of each model across 100 text-batches, randomly sampled from the Wikipedia test dataset. Each text-batch was composed of 1000 tokens that start at the beginning of a sentence.  In the ``All tokens"" condition, we calculated the P for every token in the tested text, while in the ``Final tokens"" condition, we calculated P only at the last token of every sentence . We then average the P in both conditions across text-batches to get a mean performance difference between the ablated model and the intact model.   Ablating controller units reduced the probabilities assigned to the target words, more so than ablating random units . In contrast, ablating integrator units reduced the probabilities less than ablating random units . We hypothesized that that the integrator units mostly influence the model's prediction performance for tokens where long-range information is especially relevant, such as in the later portions of clauses and sentences. Consistent with this, we found that, when we examined the ablation effects only for tokens in the final position of a sentence, ablating integrator units reduced the probabilities more than ablating random units . Interestingly, ablating controller units reduced the probability of sentence-final targets less than random units .  In summary, these ablation results indicate a non-trivial functional role for the controller and integrator units, despite the fact that each subset of units is composed of only 10 amongst 650 total hidden units. Also, the putative controller and integrator sets appear to have distinctive roles within the WLSTM, with the controllers supporting accurate predictions overall, while the integrator units appear to boost accurate predictions at the end of sentences.  \subsection {Mapping the timescale organization in a GRU language model}   \subsubsection {Training} To explore whether the timescale mapping methods, and our findings, may generalize to other model architectures, we trained and studied a word-level GRU language model .  As far as possible, we applied similar parameters in the GRU as were used for the LSTM by \citet{gulordava2018colorless}: the same Wikipedia training corpus, the same loss function , and the same hyperparameters except for a learning rate initialized to 0.1, which we found more optimal to train the GRU. The GRU model also had two layers, with 650 hidden units in each layer.  We trained the GRU model for 30 epochs, at which point the GRU converged to a validation perplexity of 118.36. Note that since we adapted similar training settings as were used for training the LSTM model by Gulordava et al. without model-specific optimization, the perplexity is higher than that of the LSTM model reported in \citet{gulordava2018colorless} . We then analyzed the timescale of its hidden units using the same method as was used for analyzing the LSTMs, and using the test data derived from the training Wikipedia corpus.  \subsubsection {Timescale organization of a GRU model} Similar to the LSTM model of Gulordova et al, the majority of the units in the GRU also showed shorter timescales. More specifically, we found:  the second layer of the GRU model was more sensitive to prior context than the first layer, as in the LSTM ;  the distribution of timescales across units was similar in the GRU and LSTM, although the GRU showed a more right-skewed distribution with a larger proportion of short-timescale units .   \subsubsection {Timescale versus network connectivity in a GRU model} We also performed the timescale vs. network connectivity analyses on the GRU model. Because the update of hidden states in GRU are controlled by the reset and update gate, we measured the projection patterns of hidden units by analyzing the matrix of combined hidden-to-update-gate and hidden-to-reset-gate weights. In contrast to the LSTM models, hidden units in the GRU that we trained did not show a relationship between longer timescales and stronger hidden-to-gate projections . Moreover, when using k-core analysis to identify subunits of interconnected high-degree units, the core network in the GRU contained many units with long to short timescales. Interestingly, when we visualized the position of the k-core units in the MDS space, they tended to locate at the edge of the space, similar to what we found in LSTM. This indicates that, as in the LSTM, the core units in the GRU have distinctive profiles, distant from one another and from other units in the network . However, we did not observe the pattern of ``integrator units"" in the GRU as in the LSTM.  These apparent similarities and differences between LSTM and GRU are intriguing, but we emphasize that  the perplexity of this GRU model is much higher than the LSTM, due to the sub-optimal parameter settings, and that  comparing the LSTM and GRU connection patterns is not straightforward, as the overall distribution of weights is different. Further work will be required to determine comparable thresholds for 閳ユ笩trong閳 projections and 閳ユ笁igh-degree units閳 in each case. As we noted in the manuscript and above, the connectivity results are exploratory; however, we believe that the GRU analysis demonstrates how these methods can be extended to map and compare the functional organization of language models of different architecture.   Finally, we note that when conducting the timescale analysis on an incompletely trained GRU model , the timescale distribution was more right-skewed  than the better-trained GRU . Altogether, these results suggest that the long-timescale units in GRU were gradually formed during the training process.    \subsection {Mapping the timescale organization in a word-level LSTM with different hidden size}   To examine whether the number of hidden units in the model would affect the timescale organization in an LSTM, we trained another 2-layer word-level LSTM model with the same Wikipedia corpus and similar parameter settings as in \citet{gulordava2018colorless}, but with only 100 hidden units in each layer. We called this model LSTM-100. We trained the model for 56 epochs until the model converged to a validation perplexity 98.75, and conducted the same analysis as described in the main text to map the timescales of LSTM-100. Because LSTM-100 have overall less weight connections, we use z-score 3 as criteria to determine the ``strong"" hidden-to-gate projections for connectivity analyses.  Regarding the timescale distribution in LSTM-100, we found that the results were similar to the 650-unit word-level LSTM model, in that:  the second layer of LSTM-100 showed more context sensitivity than the first layer, and  although it was difficult to quantitatively compare the unit-level timescale distribution between the LSTM-100 model and the LSTM with 650 units, they both contain a similarly small subset of long-timescale units. .  We did not observe a significant correlation between the unit timescale and number of strong projections generated by each unit in the LSTM-100 model: the long-timescale units in the LSTM-100 did not have more connections than short-timescale units. When visualizing the MDS space of connectivity similarity of LSTM-100, the ``controller units"" identified using the k-core analysis were located in the edge of the space, similar to the 650-unit LSTM model. Interestingly, we observed a subset of long-timescale units in the center of the MDS space, analogous to the ``integrator units"" found in the 650-unit LSTM model. Altogether, the pattern of ``integrator units"" might be a commonly evolved feature that is shared between LSTM model architectures, but not with GRU architectures.     \counterwithin{figure}{section} \renewcommand{\thefigure}{A.\arabic{figure}} \setcounter{figure}{0}       \                    
","   Because of the significance of keyphrase, producing the succinct and accurate keyphrase can effectively help downstream tasks. Existing methods are mainly categorized into keyphrase extraction and keyphrase generation. \paragraph{Keyphrase Generation} Traditional keyphrase production methods directly extract significant spans as keyphrases through candidates extracting  and ranking . However, these traditional methods can only extract keyphrases that appear in the text with a sequential order. To produce absent keyphrases, which  do not appear as any continuous subsequence of text, sequence-to-sequence based models become a popular trend.  Techniques include  CopyRNN , CorrRNN  with coverage mechaism and reviewer mechanism, TG-Net, and reinforcement learning. However, these works do not consider the meta-sentence information or the logical structure of the document, which provides important clues for generating keyphrases.  \paragraph{Incorporating Document Structure} Documents such as science literatures and news articles are well structured. The hierarchical structure of the documents are used in various NLP tasks to boost the performance. \citet{yang2016hierarchical} incorporate the knowledge of the hierarchical structure to text classification model. \citet{alzahrani2012using} utilizes the structural information to detect significant plagiarism problems. \citet{frermann2019inducing} induce latent document structure for generating aspect-based summarization. And in Keyphrase Extraction area, \citet{nguyen2010wingnus} propose to add document structure to hand-crafted features to help identify candidates, and \citet{chen2019guided} treat the document title as a dominant role to the overall document and let it guide the generation process. And in our work, we consider the logical structure of the whole documents , and propose a sentence selection module to determine which part of the document is important. However it is difficult for the module to learn the binary indicator from scratch, and hence, we adopt weakly supervised learning in our training process.    \paragraph{Weakly Supervised Learning} For lacking enough annotated data, weakly supervised learning has been an active research direction in various NLP applications.         and they mainly contain two steps: candidates extracting and ranking. The first step produces several candidate phrases with heuristic methods such as Part-of-Speech tags or n-grams. Then the candidate phrase will be sorted according to the probability of becoming the keyphrase with supervised method or unsupervised method . However, these traditional extraction methods cannot produce absent keyphrases.     To solve the problem that the extraction model cannot produce absent keyphrase,  first proposes a seq-to-seq model named CopyRNN, which is enhanced with an attention mechanism and copy mechanism. Since then, various of mechanisms   and methods are applied to this framework to improve performance.   CorrRNN introduces the Coverage mechanism and proposes the review mechanism to ease coverage and duplication issues.     TG-Net makes use of title information and regards the title as extra information to guide the source encoding, they think previous approaches treat the title and main body equally, and they use beam search to generate multiple keyphrases. \citet{yuan2018one} proposes two methods catSeq and catSeqD that can generate multiple keyphrases one time. Recently,  introduces   reinforcement learning  with an adaptive reward to encourage the model to generate more accurate and proposes a new evaluation method which can accurately deal with the problem of synonym.     We follow the \citet{yuan2018one} and train our model to generate a diverse number of keyphrases. Similar to TG-net, our model also focuses on significant information in the document. But to be different from TG-net,  our model pays more attention to the all significant sentences.   \newcommand{\tabincell}[2]{ }        \caption{Case of three kinds of keyphrase, A letter stands for a word. Semi-present keyphrase is that all words in this keyphrase exist in a particular sentence. }",214
" Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Many semantic parsing methods are based on the principle of semantic compositionality ~, of which the main idea is to put together the meanings of utterances by combining the meanings of the parts~. However, these methods suffer from heavy dependence on handcrafted grammars, lexicons, and features.  To overcome this problem, many neural semantic parsers have been proposed and achieved promising results~. %\textcolor{red}{However, compared to compositional semantic parsers, neural semantic parsers are not aware of the compositional structure of utterances, which often limits their generalization between various compound-complex utterances: However, due to the lack of capturing compositional structures in utterances, neural semantic parsers usually have poor generalization ability to handle unseen compositions of semantics~. For example, a parser trained on ``How many rivers run through oklahoma?'' and ``Show me states bordering colorado?'' may not perform well on ``How many rivers run through the states bordering colorado?''.     \end{table}     \end{table*}  In this paper, we propose a novel framework to boost neural semantic parsers with the principle of compositionality~. It iterates between segmenting a span from the utterance and parsing it into a partial meaning representation. Table  shows an example. Given an utterance ``How many rivers run through the states bordering colorado?'', we parse it through three iterations:  we segment a span ``the states bordering colorado'' from the utterance, and parse it into ;  as the utterance is reduced to ``How many rivers run through \?'', we segment a span ``rivers run through \'' from it, and parse it into ;  the utterance is further reduced to ``How many \?'', and we parse it into . We compose these partial meaning representations into the final result.  Our framework consists of two neural modules: an utterance segmentation model  and a base parser . The former is in charge of segmenting a span from an utterance, and the latter is in charge of parsing the span into its meaning representation. These two modules work together to parse complex input utterances in a divide-and-conquer fashion.  One key advantage of this framework is that it does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the base parser provides pseudo supervision to the utterance segmentation model. Specifically: we train a preliminary base parser on the original train data; then, for each train sample , we use this preliminary base parser to check whether spans in  can be parsed to \textcolor{red}{be} a part of  \textcolor{red}{or not}. If true, we leverage these spans as pseudo supervision signals for training the utterance segmentation model, and thereby do not require any handcraft templates or additional labeled data.} %The key to implement this framework is to address the challenge of lacking labeled data for utterance segmentation. %We achieve this through cooperative training of the segmentation model and the base parser: %leverage pre-trained base parser to derive synthetic supervision signals for training the segmentation model, then leverage the segmentation model to derive synthetic supervision signals for updating the base parser.  % \textcolor{green}{Moreover, considering that there are usually no labeled data for utterance segmentation, we propose to search for reasonable segmentation points from utterances via the base parser, and use them as a distant supervision. This improves the domain adaptability of our framework.}  % While lacking the direct supervision for segmentation model, we seek to address this challenge in a distantly supervised way. % shaped like  %\textcolor{red}{ %Firstly, we train the base parser, and use it to search for and evaluate all viable ways to segment training utterances. %Then, these segmentations are leveraged as distant supervision for training the utterance segmentation model and fine-tuning the base neural semantic parser.}  In summary, our proposed framework has four advantages:  the base parser learns to parse simpler spans instead of whole complex utterances, thus alleviating the training difficulties and improving the compositional generalization ability;  our framework is flexible to incorporate various popular encoder-decoder models as the base parser;  our framework does not require any handcraft templates or additional labeled data for utterance segmentation; % our framework addresses the challenge of lacking labeled data for utterance segmentation through cooperative training.  our framework improves the interpretability of neural semantic parsing by providing explicit alignment between spans and partial meaning representations.  We conduct experiments on three datasets: Geo~, ComplexWebQuestions~, and Formulas . They use different forms of meaning representations: FunQL, SPARQL, and Spreadsheet Formula. Experimental results show that our framework consistently improves the performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gain: Geo , Formulas , ComplexWebQuestions .        There are two major paradigms of semantic parsing: compositional semantic parsing~, and neural semantic parsing~. Our work aims to combine their respective advantages.  In neural semantic parsing, various efforts have been made to leverage the syntax of meaning representations  to enhance decoders~. In these works, encoders treat input utterances as sequential tokens, without considering their compositional structures. On the other hand, some researchers focus on exploring data augmentation techniques to provide a compositional inductive bias in models. However, they rely on exact matching of spans, which work well on word/phrase-level re-combination or simple domains , but is not suitable for more complex scenarios . Therefore, the lack of compositional generalization ability is still a challenging problem in neural semantic parsing~.     Utterance segmentation intersects with many NLP tasks such as question answering and semantic parsing.  In Question Answering, question segmentation has been successfully applied to help answer questions requiring multi-hop reasoning~. A key challenge in these works is to derive supervision for question segmentation. \citet{kalyanpur2012fact} segments questions based on predominantly lexico-syntactic features. \citet{talmor-berant-2018-web} leverages simple questions to derive distant supervision. \citet{min-etal-2019-multi} uses additional labeled data to fine-tune a BERT-based model; \citet{qi-etal-2019-answering} utilizes the longest common strings/sequences between utterances and their supporting context documents for segmentation.  utilizes the longest common strings/sequences algorithm between utterances and their supporting context documents for segmentation.  In Semantic Parsing, \citet{zhang-etal-2019-complex} proposes HSP, a novel hierarchical semantic parsing method, which utilizes the decompositionality of complex utterances for semantic parsing. This method requires  instances for training. \citet{pasupat2019span} proposes a span-based hierarchical semantic parsing method for task-oriented dialog. This method requires span-based annotations for training.      In this study, we investigate a syntactic representation learning method to automatically utilize the syntactic structure information for neural network based TTS. Nuclear-norm maximization loss is introduced to enhance the discriminability and diversity of synthsized speech prosody. Experimental results demonstrate the effectiveness of our proposed approach. For sentences with multiple syntactic parse trees, prosodic difference can be clearly observed from the synthesized speeches.    To start a new column  and help balance the last-page   column length use \vfill\pagebreak.   -------------------------------------------------------------------------  \vfill  \pagebreak   \vfill\pagebreak    References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   ------------------------------------------------------------------------- 
","   There are two major paradigms of semantic parsing: compositional semantic parsing~, and neural semantic parsing~. Our work aims to combine their respective advantages.  In neural semantic parsing, various efforts have been made to leverage the syntax of meaning representations  to enhance decoders~. In these works, encoders treat input utterances as sequential tokens, without considering their compositional structures. On the other hand, some researchers focus on exploring data augmentation techniques to provide a compositional inductive bias in models. However, they rely on exact matching of spans, which work well on word/phrase-level re-combination or simple domains , but is not suitable for more complex scenarios . Therefore, the lack of compositional generalization ability is still a challenging problem in neural semantic parsing~.     Utterance segmentation intersects with many NLP tasks such as question answering and semantic parsing.  In Question Answering, question segmentation has been successfully applied to help answer questions requiring multi-hop reasoning~. A key challenge in these works is to derive supervision for question segmentation. \citet{kalyanpur2012fact} segments questions based on predominantly lexico-syntactic features. \citet{talmor-berant-2018-web} leverages simple questions to derive distant supervision. \citet{min-etal-2019-multi} uses additional labeled data to fine-tune a BERT-based model; \citet{qi-etal-2019-answering} utilizes the longest common strings/sequences between utterances and their supporting context documents for segmentation.  utilizes the longest common strings/sequences algorithm between utterances and their supporting context documents for segmentation.  In Semantic Parsing, \citet{zhang-etal-2019-complex} proposes HSP, a novel hierarchical semantic parsing method, which utilizes the decompositionality of complex utterances for semantic parsing. This method requires  instances for training. \citet{pasupat2019span} proposes a span-based hierarchical semantic parsing method for task-oriented dialog. This method requires span-based annotations for training.",215
"     Word alignment is a task of finding the corresponding words in a sentence pair  and used to be a key component of statistical machine translation . Although word alignment is no longer explicitly modeled in neural machine translation , it is often leveraged to interpret and analyze NMT models . Word alignment is also used in many other scenarios, such as imposing lexical constraints on the decoding process , improving automatic post-editing  and providing guidance for translators in computer-aided translation .  Recently, unsupervised neural alignment methods have been studied and outperformed GIZA++  on many alignment datasets . However, these methods are trained with a translation objective, which computes the probability of each target token conditioned on source tokens and previous target tokens. This will bring noisy alignments when the prediction is ambiguous . To alleviate this problem, previous studies modify Transformer  by adding alignment modules to re-predict the target token , or computing an additional alignment loss on the full target sequence . Moreover, \citet{chen2020accurate} propose an extraction method that induces alignment when the to-be-aligned target token is the decoder input.   Although these methods have demonstrated their effectiveness, they have two drawbacks. First, they retain the translation objective which is not tailored for word alignment. Consider the example in Figure . When predicting target token ``Tokyo'', the translation model may wrongly generate ``1968'' as it only considers the previous context, which will result in an incorrect alignment link . A better modeling is needed for obtaining more accurate alignments. Second, they need an additional guided alignment loss  to outperform GIZA++, which requires inducing alignments for entire training corpus.  In this paper, we propose a self-supervised model specifically designed for the word alignment task, namely Mask-Align. Our model masks each target token and recovers it with the source and the rest of the target tokens. For example, as shown in Figure , the target token ``Tokyo'' is masked and re-predicted. During this process, our model can identify that only the source token ``Tokio'' has not been translated yet, so the to-be-predicted target token ``Tokyo'' is aligned to ``Tokio''. Comparing with the translation model, this masked modeling method is highly related to word alignment, and based on that our model generates more accurate predictions and alignments.  % We model the target token conditioned on all other tokens in both source and target, which will disambiguate the prediction and thus lead to an accurate alignment ). As the vanilla transformer architecture requires sequential time to model this probability, we modify the attention in the decoder by separating the queries from keys and values and  % updating only the former in each layer. This allows our model to predict all target tokens in a single forward pass without information leakage. Besides, we also propose a variant of attention called leaky attention that allieviates the unexpected high attention weights on some specific tokens such as periods, which is helpful for the alignment extraction from attention matrix. Finally, we leverage the attention weights from the models in two directions by incorporating an agreement loss in the training process.  % Experiments on four public datasets show that our model significantly outperforms all existing statistical and neural methods without using guided alignment loss.  To summarize, the main contributions of our work are listed as follows:        Neural Alignment Model Some neural alignment models use gold-standard alignment data. \citet{stengel2019discriminative} introduce a discriminative model using the dot-product distance measure between source and target representations to predict the alignment labels. \citet{nagata2020supervised} first transform the task of word alignment into a question answering task and then use a multilingual BERT to solve it. This line of research suffers from the lack of human-annotated alignment data. Therefore, many studies focus on alignment extraction without gold data . \citet{alkhouli2016alignment} present neural translation and alignment models trained by using silver-standard alignments obtained from GIZA++. \citet{peter2017generating} propose a target foresight approach and use silver-standard alignments to perform guided alignment training . These methods are not satisfactory in terms of alignment results. Recently, there are plenty of studies that induce alignments from an NMT model. \citet{garg2019jointly} apply the guided alignment loss on a single attention head with silver-standard alignments from GIZA++. \citet{zenkel2019adding, zenkel2020end} introduce an additional alignment module on top of the NMT model and also use guided training. \citet{chen2020accurate} come up with an extraction method that induce alignments when the to-be-aligned target token is the decoder input. However, all previous methods adopt a translation objective in the training process. Also, they outperform GIZA++ only with guided training, which requires inducing alignment for entire training set. Our method is fully self-supervised with a masked modeling objective and exceed all these unsupervised methods.   Masked Language Model  Pre-trained masked language models  have been successfully applied to many NLP tasks such as natural language understanding  and text generation . Its idea has also been adopted in many advanced NLP models. \citet{ghazvininejad2019mask} introduce a conditional masked language model  to perform parallel decoding for non-autoregressive machine translation. The CMLM can leverage both previous and future context on the target side for sequence-to-sequence tasks with the masking mechanism. \citet{kasai2020disco} extend it with a disentangled context Transformer that predicts every target token instead of a subset conditioned on arbitrary context. Our masked modeling method is inspired by CMLMs, as such a masking and predicting process is highly related to word alignment. To the best of our knowledge, this is the first work that incorporates a CMLM objective into alignment models.    In this paper, we propose a novel framework for boosting neural semantic parsers via iterative utterance segmentation. The insight is a bottom-up divide-and-conquer mechanism, which significantly improves the compositional generalization ability and interpretability of neural semantic parsers. Considering the usual absence of labeled data for utterance segmentation, we propose a cooperative training method to tackle this problem. Experimental results show that our framework consistently improves the performance of different neural semantic parsers across tasks.  In the future, we plan to improve the robustness of our framework for various complex language phenomena. We also plan to apply this framework to more semantic parsing tasks such as text-to-SQL and text-to-code.     
","   Neural Alignment Model Some neural alignment models use gold-standard alignment data. \citet{stengel2019discriminative} introduce a discriminative model using the dot-product distance measure between source and target representations to predict the alignment labels. \citet{nagata2020supervised} first transform the task of word alignment into a question answering task and then use a multilingual BERT to solve it. This line of research suffers from the lack of human-annotated alignment data. Therefore, many studies focus on alignment extraction without gold data . \citet{alkhouli2016alignment} present neural translation and alignment models trained by using silver-standard alignments obtained from GIZA++. \citet{peter2017generating} propose a target foresight approach and use silver-standard alignments to perform guided alignment training . These methods are not satisfactory in terms of alignment results. Recently, there are plenty of studies that induce alignments from an NMT model. \citet{garg2019jointly} apply the guided alignment loss on a single attention head with silver-standard alignments from GIZA++. \citet{zenkel2019adding, zenkel2020end} introduce an additional alignment module on top of the NMT model and also use guided training. \citet{chen2020accurate} come up with an extraction method that induce alignments when the to-be-aligned target token is the decoder input. However, all previous methods adopt a translation objective in the training process. Also, they outperform GIZA++ only with guided training, which requires inducing alignment for entire training set. Our method is fully self-supervised with a masked modeling objective and exceed all these unsupervised methods.   Masked Language Model  Pre-trained masked language models  have been successfully applied to many NLP tasks such as natural language understanding  and text generation . Its idea has also been adopted in many advanced NLP models. \citet{ghazvininejad2019mask} introduce a conditional masked language model  to perform parallel decoding for non-autoregressive machine translation. The CMLM can leverage both previous and future context on the target side for sequence-to-sequence tasks with the masking mechanism. \citet{kasai2020disco} extend it with a disentangled context Transformer that predicts every target token instead of a subset conditioned on arbitrary context. Our masked modeling method is inspired by CMLMs, as such a masking and predicting process is highly related to word alignment. To the best of our knowledge, this is the first work that incorporates a CMLM objective into alignment models.",216
" The sequence-to-sequence  models~, which learn to map an arbitrary-length input sequence to another arbitrary-length output sequence, have successfully tackled a wide range of language generation tasks. % including machine translation, text summarization, question generation, to name a few.  Early seq2seq models have used recurrent neural networks to encode and decode sequences, leveraging attention mechanism  that allows the decoder to attend to a specific token in the input sequence to capture long-term dependencies between the source and target sequences. Recently, the Transformer~, which is an all-attention model that effectively captures long-term relationships between tokens in the input sequence as well as across input and output sequences, has become the de facto standard for most of the text generation tasks due to its impressive performance. Moreover, Transformer-based language models trained on large text corpora  have shown to significantly improve the model performance on text generation tasks. %Seq2seq tasks are becoming increasingly more important, as  show that most of text-based language problems can be cast into sequence-to-sequence problems.  However, a crucial limitation of seq2seq models is that they are mostly trained only with teacher forcing, where ground truth is provided at each time step and thus  never exposed to incorrectly generated tokens during training ), which hurts its generalization. This problem is known as the ``exposure bias"" problem  and often results in the generation of low-quality texts on unseen inputs. Several prior works tackle the problem, such as using reinforcement learning  to maximize non-differentiable reward . %  --- BLEU or Rouge.   Another approach is to use RL or gumbel softmax  to match the distribution of generated sentences to that of the ground truth, in which case the reward is the discriminator output from a Generative Adversarial Network  . Although the aforementioned approaches improve the performance of the seq2seq models on text generation tasks, they either require a vast amount of effort in tuning hyperparameters or stabilize training. %Moreover,  show that RL methods for machine translation often do not optimize the expected reward and the performance gain is attributed to the side effects, such as increasing the peakiness of the output distribution.      In this work, we propose to mitigate the exposure bias problem with a simple yet effective approach, in which we contrast a positive pair of input and output sequence to negative pairs, to expose the model to various valid or incorrect sentences. Na鑼倂ely, we can construct negative pairs by simply using random non-target sequences from the batch~. However, such a na鑼倂e construction yields meaningless negative examples that are already well-discriminated in the embedding space ), which we highlight as the reason why existing methods~ require large batch size. This is clearly shown in Fig., where a large portion of positive-negative pairs can be easily discriminated without any training, which gets worse as the batch size decreases as it will reduce the chance to have meaningfully difficult examples in the batch. Moreover, discriminating positive and na鑼倂e negative pairs becomes even more easier for models pretrained on large text corpora.   To resolve this issue, we propose principled approaches to automatically generate negative and positive pairs for constrastive learning, which we refer to as Contrastive Learning with Adversarial Perturbation for Seq2seq learning . Specifically, we generate a negative example by adding a small perturbation to the hidden representation of the target sequence, such that its conditional likelihood is minimized ). Conversely, we construct an additional positive example ) by adding a large amount of perturbation to the hidden representation of target sequence such that the perturbed sample is far away from the source sequence in the embedding space, while enforcing it to have high conditional likelihood by minimizing Kullback-Leibler  divergence between the original conditional distribution and perturbed conditional distribution. This will yield a negative example that is very close to the original representation of target sequence in the embedding space but is largely dissimilar in the semantics, while the generated positive example is far away from the original input sequence but has the same semantic as the target sequence. This will generate difficult examples that the model fails to correctly discriminate , Fig.2), helping it learn with more meaningful pairs.  To verify the efficacy of our method, we empirically show that it significantly improves the performance of seq2seq model on three conditional text generation tasks, namely machine translation, text summarization and question generation. Our contribution in this work is threefold:         Conditional Sequence Generation    introduce an encoder-decoder architecture with recurrent neural networks for conditional text generation. Attention  which learns to focus on certain tokens of an input sentence significantly improves machine translation and becomes core component of seq2seq model. Furthermore, Transformer  is introduced to tackle long term dependency with self-attention. After pretrained language models , show impressive performance in several discriminative tasks, various pretraining methods  are introduced to target language generation and improve the performance of several generation tasks.    Exposure Bias There are several prior works to tackle the exposure bias .  introduce scheduled sampling where the model is initially guided with the true previous tokens but uses the tokens generated by the seq2seq model as the conditional input for the next token, as training goes on.   However, it is fundamentally inconsistent cannot fundamentally tackle the problem .  leverage RL to maximize non-differentiable rewards, so it enables to penalize the model for incorrectly generated sentences. Another works  train GANs to match the distribution of generated sequences to that of ground truth. Since sampling tokens from the generator is not differentiable, they resort RL or gumbel-softmax to train the networks in end-to-end fashion. However, they require either a large amount of effort to tune hyperparameters or stabilize training. However,  show that RL for machine translation does not optimize the expected reward and the performance gain is attributed to the unrelated effects such as increasing the peakiness of the output distribution. Moreover,   show that by tuning the temperature parameter, the language models trained with MLE can be tuned to  outperform GAN-based text generation models.     Adversarial Perturbation Many existing works, such as , address the robustness of neural networks to adversarial examples, which are generated by applying a small perturbations to the input samples. While adversarial robustness has been mostly explored in image domains,  adopted adversarial training to text domains. However instead of targeting robustness to perturbed samples, they utilize the adversarial examples as augmented data, and enforce consistency across the predictions across original unlabeled example and its perturbation, for semi-supervised learning. Recently  and  leverage adversarial training to induce the smoothness of text classifiers, to prevent overfitting to training samples. While they are relevant to ours, these methods do not have the notion of positive and negative examples as they do not consider contrastive learning, and only target text classification. Moreover, they are computationally prohibitive since they use PGD for adversarial training, which requires iterative optimization for each individual sample. Recently,  propose a simpler yet effective method based on Gaussian noise perturbation to regularize neural networks without expensive PGD steps, which is shown to outperform methods from and. Although our work is similar to these prior works in that we add perturbations to the text embeddings, note that we used the adversarially-generated samples as negative examples of our contrastive learning framework rather than trying to learn the model to be robust to them.     Contrastive Learning Contrastive learning has been widely used. It is to learn a representation by contrasting positive pairs and negative pairs.  \citet{triplet1, lmnn, triplet3} leverage a triplet loss to separate positive examples from negative examples in metric learning. \citet{simclr} show that contrastive learning can boost the performance of self-supervised and semi-supervised learning in computer vison tasks. In natural language processing , contrastive learning has been widely used. In Word2Vec , neighbouring words are predicted from context with noise-contrastive estimation . Beyond word representation,  sample two contiguous sentences for positive pairs and the sentences from other document as negative pairs. They constrast positive and negative pairs to learn sentence representation. Moreover, contrastive learning has been investigated in various NLP tasks --- language modeling , unsupervised word alignment , caption generation , and machine translation .      In this paper, we propose a self-supervised neural alignment model Mask-Align. Different from the NMT-based methods, our model adopts a novel masked modeling objective that is more suitable for word alignment tasks. Moreover, Mask-Align can alleviate the problem of high attention weights on special tokens by introducing leaky attention. Experiments show that Mask-Align achieves new state-of-the-art results without guided alignment loss. We leave it for future work to extend our model in a semi-supervised setting.     
","   Conditional Sequence Generation    introduce an encoder-decoder architecture with recurrent neural networks for conditional text generation. Attention  which learns to focus on certain tokens of an input sentence significantly improves machine translation and becomes core component of seq2seq model. Furthermore, Transformer  is introduced to tackle long term dependency with self-attention. After pretrained language models , show impressive performance in several discriminative tasks, various pretraining methods  are introduced to target language generation and improve the performance of several generation tasks.    Exposure Bias There are several prior works to tackle the exposure bias .  introduce scheduled sampling where the model is initially guided with the true previous tokens but uses the tokens generated by the seq2seq model as the conditional input for the next token, as training goes on.   However, it is fundamentally inconsistent cannot fundamentally tackle the problem .  leverage RL to maximize non-differentiable rewards, so it enables to penalize the model for incorrectly generated sentences. Another works  train GANs to match the distribution of generated sequences to that of ground truth. Since sampling tokens from the generator is not differentiable, they resort RL or gumbel-softmax to train the networks in end-to-end fashion. However, they require either a large amount of effort to tune hyperparameters or stabilize training. However,  show that RL for machine translation does not optimize the expected reward and the performance gain is attributed to the unrelated effects such as increasing the peakiness of the output distribution. Moreover,   show that by tuning the temperature parameter, the language models trained with MLE can be tuned to  outperform GAN-based text generation models.     Adversarial Perturbation Many existing works, such as , address the robustness of neural networks to adversarial examples, which are generated by applying a small perturbations to the input samples. While adversarial robustness has been mostly explored in image domains,  adopted adversarial training to text domains. However instead of targeting robustness to perturbed samples, they utilize the adversarial examples as augmented data, and enforce consistency across the predictions across original unlabeled example and its perturbation, for semi-supervised learning. Recently  and  leverage adversarial training to induce the smoothness of text classifiers, to prevent overfitting to training samples. While they are relevant to ours, these methods do not have the notion of positive and negative examples as they do not consider contrastive learning, and only target text classification. Moreover, they are computationally prohibitive since they use PGD for adversarial training, which requires iterative optimization for each individual sample. Recently,  propose a simpler yet effective method based on Gaussian noise perturbation to regularize neural networks without expensive PGD steps, which is shown to outperform methods from and. Although our work is similar to these prior works in that we add perturbations to the text embeddings, note that we used the adversarially-generated samples as negative examples of our contrastive learning framework rather than trying to learn the model to be robust to them.     Contrastive Learning Contrastive learning has been widely used. It is to learn a representation by contrasting positive pairs and negative pairs.  \citet{triplet1, lmnn, triplet3} leverage a triplet loss to separate positive examples from negative examples in metric learning. \citet{simclr} show that contrastive learning can boost the performance of self-supervised and semi-supervised learning in computer vison tasks. In natural language processing , contrastive learning has been widely used. In Word2Vec , neighbouring words are predicted from context with noise-contrastive estimation . Beyond word representation,  sample two contiguous sentences for positive pairs and the sentences from other document as negative pairs. They constrast positive and negative pairs to learn sentence representation. Moreover, contrastive learning has been investigated in various NLP tasks --- language modeling , unsupervised word alignment , caption generation , and machine translation .",217
"  Task-specific finetuning of pretrained deep networks has become the dominant paradigm in contemporary NLP, achieving state-of-the-art results across a suite of natural language understanding tasks . While straightforward and empirically effective, this approach is difficult to scale to multi-task, memory-constrained settings , as it requires shipping and storing a full set of model parameters for each task. Inasmuch as these models are learning generalizable, task-agnostic language representations through self-supervised pretraining, finetuning the entire model for each task seems especially profligate.   A popular approach to parameter-efficiency with pretrained models is to learn sparse models for each task where a subset of the final model parameters  are exactly zero~. Such approaches often face a steep sparsity/performance tradeoff, and a substantial portion of nonzero parameters  are still typically required to match the performance of the dense counterparts. An alternative is to use multi-task learning or feature-based transfer for more parameter-efficient transfer learning with pretrained models~. These methods learn only a small number of additional parameters  on top of a shared model. However, multi-task learning generally requires access to all tasks during training to prevent catastrophic forgetting~, while feature-based transfer learning  is typically outperformed by full finetuning~.    Adapters~ have recently emerged as a promising approach to parameter-efficient transfer learning within the pretrain-finetune paradigm~.  Adapter layers are smaller, task-specific modules that are inserted between layers of a pretrained model, which remains fixed and is shared across tasks.  These approaches do not require access to all tasks during training, making them attractive in settings where one hopes to obtain and share performant models as new tasks arrive in stream.  \citet{houlsby2019adapters} find that adapter layers trained on BERT can match the performance of fully finetuned BERT on the GLUE benchmark  while only requiring 3.6\% additional parameters  per task.   In this work, we consider a similar setting as adapters but propose a new diff pruning approach with the goal of even more parameter-efficient transfer learning.  Diff pruning views finetuning as learning a task-specific \underline{diff}erence  vector%\footnote{Similar to the  command in Unix operating systems.}    \ that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks.   In order to learn this vector, we reparameterize the task-specific model parameters as , where the pretrained parameter vector  is fixed  and  the task-specific diff vector  is finetuned. The diff vector is regularized with a differentiable approximation to the -norm penalty~ to encourage sparsity. This approach can become  parameter-efficient as the number of tasks increases as it only requires storing the nonzero positions and weights of the diff vector for each task. The cost of storing the shared pretrained model remains constant and is amortized across multiple tasks.  On the GLUE benchmark~, diff pruning can match the performance of the fully finetuned BERT baselines  while finetuning only  of the pretrained parameters per task, making it a potential alternative to adapters for parameter-efficient transfer learning.      \paragraph{Multi-task learning} Multi-task learning , broadly construed, aims to learn models and representations that can be utilized across a diverse range of tasks, and offers a natural approach to training parameter-efficient deep models.  Several works have shown that a single BERT model can obtain good performance  across multiple tasks when jointly trained . An alternative approach to multi-task learning that does not require access to all tasks during training involve training smaller task-specific layers that interact with a fixed pretrained model . In particular, adapter layers , which learn to read and write to layers of a shared model, have been applied to obtain parameter-efficient BERT models .  A related line of work targets extreme parameter-efficiency through task-agnostic sentence representations that can be used without finetuning for downstream tasks . These feature-based transfer learning methods are however generally outperformed by fully finetuned models .   \paragraph{Model compression} There has been much recent work on compressing  pretrained trained with self-supervision .  A particularly promising line of work focuses on obtaining smaller pretrained models  through weight pruning  and/or knowledge distillation . It would be interesting to see whether our approach can be applied on top of these smaller pretrained models to for even greater parameter-efficiency.  \paragraph{Learning to mask} Our work is closely related to the line of work on learning to mask parts of deep networks with differentiable relaxations of binary masks for model pruning and parameter sharing . While these works also enable parameter-efficient transfer learning, they generally apply the masks directly on the pretrained parameters instead of on the difference vector as in the present work.  \paragraph{Regularization towards pretrained models} Finally, diff pruning is also related to works which regularize the learning process towards pretrained/shared models  for continual learning , domain adaptation , and stable finetuning . These works typically do not utilize sparse regularizers and target a different goal than parameter-efficiency.      In this paper, we propose a Disentanglement-based Attractive Headline Generator  to generate an attractive headline.    Our model is built on the fact that the attractiveness of the headline comes from both style and content aspects.   Given the prototype document-headline pair, DAHG disentangles the attractive content and style space from the prototype attractive headline.    The headline generator generates attractive headlines under the guidance of both.    Our model achieves state-of-the-art results in terms of ROUGE scores and human evaluations by a large margin.   In near future, we aim to bring the model online.                 \clearpage  
","  \paragraph{Multi-task learning} Multi-task learning , broadly construed, aims to learn models and representations that can be utilized across a diverse range of tasks, and offers a natural approach to training parameter-efficient deep models.  Several works have shown that a single BERT model can obtain good performance  across multiple tasks when jointly trained . An alternative approach to multi-task learning that does not require access to all tasks during training involve training smaller task-specific layers that interact with a fixed pretrained model . In particular, adapter layers , which learn to read and write to layers of a shared model, have been applied to obtain parameter-efficient BERT models .  A related line of work targets extreme parameter-efficiency through task-agnostic sentence representations that can be used without finetuning for downstream tasks . These feature-based transfer learning methods are however generally outperformed by fully finetuned models .   \paragraph{Model compression} There has been much recent work on compressing  pretrained trained with self-supervision .  A particularly promising line of work focuses on obtaining smaller pretrained models  through weight pruning  and/or knowledge distillation . It would be interesting to see whether our approach can be applied on top of these smaller pretrained models to for even greater parameter-efficiency.  \paragraph{Learning to mask} Our work is closely related to the line of work on learning to mask parts of deep networks with differentiable relaxations of binary masks for model pruning and parameter sharing . While these works also enable parameter-efficient transfer learning, they generally apply the masks directly on the pretrained parameters instead of on the difference vector as in the present work.  \paragraph{Regularization towards pretrained models} Finally, diff pruning is also related to works which regularize the learning process towards pretrained/shared models  for continual learning , domain adaptation , and stable finetuning . These works typically do not utilize sparse regularizers and target a different goal than parameter-efficiency.",218
" Sentiment classification is the task of analyzing a piece of text to predict the orientation of the attitude towards an event or opinion. The sentiment of a text can be either positive or negative. Sometimes, a neutral perspective is also considered for classification. SA has many different applications, such as reducing the early age suicide rate by identifying cyberbullying , discouraging unwarranted activities towards a particular community through hate-speech detection , and monitoring public response towards a proposed government bill  among many others.    The task of SA has achieved superior improvement in other languages, i.e. English - about 97.1\% accuracy for 2-class  and 91.4\% accuracy for 3-class SA . But only a few research works have been published for the SA in Bengali. This is because we lack quality datasets in Bengali for training a computation model for the sentiment classification. However, in the last few years, we have seen the rise of Internet users in the Bengali domain mostly due to the development of wireless network infrastructure throughout South East Asia. This resulted in a massive increase in the total number of online social network users as well as newspaper readers. So it became comparatively easier to collect the public comments posted online on the Bengali news websites.    %  \end{table}  Thus we created two SA datasets for 2-class and 3-class SA in Bengali and trained a multi-lingual BERT model via transfer learning approach for sentiment classification in Bengali, referred as  in this paper.  achieves an accuracy of 71\% for the 2-class and 60\% for the 3-class manually tagged dataset. We further use this model to analyze the sentiment of 1,002 public comments collected from the online daily newspaper. Table  shows that in general, sentiment in public comments is positive for religious news articles, while that is negative for political or sports news articles. In this paper, we present the following contributions:    % \makeatletter % \patchcmd{\@makecaption} %   {\scshape} %   {} %   {} %   {} % \makeatletter % \patchcmd{\@makecaption} %   {\\} %   {.\ } %   {} %   {} % \makeatother % \def\tablename{Table}       Bidirectional Encoder Representations from Transformers, or BERT , is an unsupervised language representation model that had been pre-trained using large plain text corpus. BERT makes use of transformer, an attention mechanism to learn the contextual relations between words. BERT is fundamentally different from the context-free models such as Word2Vec or GloVe that generate a single word embedding representation for each word in the vocabulary . Instead, BERT takes into account the context for each occurrence of a given word in a sentence. For instance, the vector for ``running"" will have the same Word2Vec or GloVe vector representation for both of its occurrences in the sentences ``He is running a company"" and ``He is running a marathon."" But BERT will provide two contextualized embedding vectors based on the appearance of ""running"" in two different sentences.  BERT is very popular for aspect-based sentiment analysis by either fine-tuning BERT's pre-trained model  or using the benchmark dataset for question-answering . However, in the pre-BERT era, research works used other end-to-end deep network layers like LSTM, BiLSTM, CNN, etc. Lei et al.  integrated with three kinds of sentiment linguistic knowledge  into the deep neural network via attention mechanisms. In another research work, Baziotis et al.  used LSTM networks augmented with two kinds of attention mechanisms, on top of pre-trained word embedding for sentiment classification and achieved the rank   at the  SemEval-2017 Task 4 Subtask A .  In spite of such advances in English SA, only a few notable works were done on Bengali SA. Sharfuddin et al.  use term frequency閳ユ悆nverse document frequency  and BiLSTM to predict the sentiment of unseen sentences accurately and holds the current state-of-the-art performance on 2-class Bengali sentiment classification in a small balanced dataset. On the other hand, Karim et al.  focused primarily on building a Bengali word embedding which was incorporated into a Multichannel Convolutional LSTM  network for predicting different types of tasks including sentiment analysis.   The lack of quality datasets and complex linguistics feature of Bengali language make the task of SA very challenging. In this research work, we contribute two manually tagged datasets for 2-class and 3-class sentiment classification in Bengali. We trained our proposed model  as well as the model proposed by Sharfuddin et al.  on those datasets and compare the performance of both the models in the section 6.      Other handful quality work includes \citet{al2017sentiment}'s skillful use of word2vec to determine positive and negative label of a sentence and \citet{phani2016sentiment}閳ユ獨 proposed methods for feature extraction such as n-gram and thorough normalization to obtain best results across multiple languages at SAIL data-set         We introduce Sequence Mixup, a set of regularization and data augmentation techniques for RNNs. Our work can thought as extending both input mixup  and manifold mixup , which are originally porposed for feed-forward neural nets. For the case of manifold mixup, we propose two distinct variants called Pre-Output and Throgh-Time Mixup, respectively. An asymptotic theoretical analysis reveals that Pre-Output Mixup imposes  a locally linear behavior on the network's output generating section. In a classification task, this property leads to partitioning of the hidden representation space into a set of orthogonal affine subspaces, each of which corresponds to a unique class. Experimental results showed improvement on the loss and F-1 scores of both 1) a baseline and 2) state-of-the-art model on CoNLL-2003 NER task. We have studied the correlation of mixup coefficients through consecutive time-steps, and found out that using identical coefficients achieves better loss and F-1 on the NER task. However, at the same time, we conjecture that optimal correlation values for mixup coefficients across time may vary from task to task and thus requires experimental exploration to be adjusted. Lastly, the considerable reduction in the test loss achieved by sequence mixup methods  implies that employing sequence mixup methods for language models may lead to a substantial improvement on the test perplexity.       
","   Bidirectional Encoder Representations from Transformers, or BERT , is an unsupervised language representation model that had been pre-trained using large plain text corpus. BERT makes use of transformer, an attention mechanism to learn the contextual relations between words. BERT is fundamentally different from the context-free models such as Word2Vec or GloVe that generate a single word embedding representation for each word in the vocabulary . Instead, BERT takes into account the context for each occurrence of a given word in a sentence. For instance, the vector for ``running"" will have the same Word2Vec or GloVe vector representation for both of its occurrences in the sentences ``He is running a company"" and ``He is running a marathon."" But BERT will provide two contextualized embedding vectors based on the appearance of ""running"" in two different sentences.  BERT is very popular for aspect-based sentiment analysis by either fine-tuning BERT's pre-trained model  or using the benchmark dataset for question-answering . However, in the pre-BERT era, research works used other end-to-end deep network layers like LSTM, BiLSTM, CNN, etc. Lei et al.  integrated with three kinds of sentiment linguistic knowledge  into the deep neural network via attention mechanisms. In another research work, Baziotis et al.  used LSTM networks augmented with two kinds of attention mechanisms, on top of pre-trained word embedding for sentiment classification and achieved the rank   at the  SemEval-2017 Task 4 Subtask A .  In spite of such advances in English SA, only a few notable works were done on Bengali SA. Sharfuddin et al.  use term frequency闁炽儲鎮唍verse document frequency  and BiLSTM to predict the sentiment of unseen sentences accurately and holds the current state-of-the-art performance on 2-class Bengali sentiment classification in a small balanced dataset. On the other hand, Karim et al.  focused primarily on building a Bengali word embedding which was incorporated into a Multichannel Convolutional LSTM  network for predicting different types of tasks including sentiment analysis.   The lack of quality datasets and complex linguistics feature of Bengali language make the task of SA very challenging. In this research work, we contribute two manually tagged datasets for 2-class and 3-class sentiment classification in Bengali. We trained our proposed model  as well as the model proposed by Sharfuddin et al.  on those datasets and compare the performance of both the models in the section 6.      Other handful quality work includes \citet{al2017sentiment}'s skillful use of word2vec to determine positive and negative label of a sentence and \citet{phani2016sentiment}闁炽儲鐛 proposed methods for feature extraction such as n-gram and thorough normalization to obtain best results across multiple languages at SAIL data-set",219
" Content based websites such as Quora, Reddit, StackOverflow are primarily used for seeking genuine answers to questions. People from different domains put up their questions and educators or people knowledgeable in a certain field answer them. One major impediment to a plain sailing execution of information exchange is the proliferation of toxic comments. The key challenge is to weed out such toxic comments termed as Insincere Questions. An Insincere Question is designated as a comment intended to make a statement than to look for genuine answers.  An Insincere Question is characterised by:   This major class of problem pertains to Text classification which has been a benchmark problem of evaluating various research advancements in natural language processing. While traditional machine learning algorithms such as naive bayes, logistic regression and decision trees can be rightfully applied to this problem, they suffer with major impediments in their constructs. Vanilla RNNs, Gated Recurrent Unit and Long Short Term Memory Networks replaced their usage as the new state of the art. Even though LSTMs and GRUs performed well, they failed to capture the dependencies in long range sentences. Now with the advent of Transfer Learning, Language model pre-training has proven to be useful in learning universal language representations. Researchers in the field are developing new and better language models at an unprecedented speed. Applying these new state of the art models could improve current methods and replace manual labeling tasks for text classification, but also find widespread application in similar other fields, such as machine translation and question answering. In this paper, we test this by applying new transformer models from the BERT-family to improve the current method of binary text classification in the context of Insincere Questions Classification. We make use of the Quora Insincere Questions Classification dataset  for this purpose We find that all of our models achieve remarkable results in classifying the given  data , with BERT achieving the best results compared to RoBERTa, DistilBERT, and ALBERT. This indicates that the models are well equipped to take over tasks that researchers have previously solved in less optimal ways.      Detecting divisive and inappropriate content is a highly relevant task in Natural Language Processing today. Over the last few years, the class of problems pertaining to text classification have been dominated by deep learning based architectures. Kim , reports a series of experiments with CNN trained on top of pre trained word-vectors for sentence level classification tasks. Liu et al  develop what is called the XMLCNN built on top of KimCNN with modifications such as wider convolutional kernels, adaptive dynamic max-pooling, and an additional bottleneck layer to capture the features of large documents more efficiently.  Yang et al  propose the Hierarchical Attention Network model which consists of two levels of attention mechanism operating at the word as well as the sentence level thus focusing distinctively on more and less important content for deriving document representations. Adhikari et al  propose a simple Bidirectional LSTM with attention mechanism and appropriate regularization techniques to yield next to state-of-the-art results on document classification.  Alternatively, substantial work has been done to prove pre-trained models trained on a large corpus of data yield promising results in this domain, thus  avoiding training models from scratch. Mikolov et al  introduced a skip gram model with negative sampling termed word2vec and pennington et al  came up with the glove embeddings which leverages statistical information by training only on the nonzero elements in a word-word co occurrence matrix rather than their sparse representations. Peters et al  introduce Elmo which is a deep contextualized word representation which models complex syntax and semantics, trained with Bidirectional LSTMs on a large text corpus using a fixed embedding for each word,  looks at the entire sentence before assigning each word in it an embedding.  More recently the attention algorithm was introduced that completely changed the landscape in the NLP field. It was first introduced by the Google Brain team with the paper 閳ユ穾ttention is all you need閳  emphasizing the fact that their model does not use recurrent neural networks at all.  Attention had already been a known idea used in LSTMs, but this was the first time it completely took the place of the recurrence. In the following paragraphs we explore various models and advancements that are built on top of the aforementioned works.       Active transfer learning using amalgamation of results from multiple models is a novel and, as proved above, successful methodology for identifying causal sentences. This two class problem, whereby we aimed to correctly identify the causal sentences, shows very high and maintainable recall rates. While the performance of this methodology, in terms of accuracy and precision can be improved by incorporating additional active learning iterations, the results are still significant enough to be used for practically solving any two class textual mining problem. In future, we shall look towards the application of our methodology for solving real world problems, such as generation of patient summaries from clinical text.        Copyright 2007, 2008, 2009 Elsevier Ltd       This file is part of the 'Elsarticle Bundle'.    ---------------------------------------------       It may be distributed under the conditions of the LaTeX Project Public    License, either version 1.2 of this license or  any    later version.  The latest version of this license is in       http://www.latex-project.org/lppl.txt    and version 1.2 or later is part of all distributions of LaTeX    version 1999/12/01 or later.       The list of all files belonging to the 'Elsarticle Bundle' is    given in the file `manifest.txt'.        Template article for Elsevier's document class `elsarticle'    with numbered style bibliographic references    SP 2008/03/01                     \documentclass[preprint,12pt,3p]{elsarticle}     Use the option review to obtain double line spacing  \documentclass[preprint,review,12pt]{elsarticle}     Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn    for a journal layout:    \documentclass[final,1p,times]{elsarticle}    \documentclass[final,1p,times,twocolumn]{elsarticle}    \documentclass[final,3p,times]{elsarticle}    \documentclass[final,3p,times,twocolumn]{elsarticle}    \documentclass[final,5p,times]{elsarticle}    \documentclass[final,5p,times,twocolumn]{elsarticle}  \usepackage{float}     if you use PostScript figures in your article    use the graphics package for simple commands    \usepackage{graphics}    or use the graphicx package for more complicated commands \usepackage{amsmath,amssymb,amsfonts} \usepackage{algorithmic} \usepackage{algorithm2e} \usepackage{textcomp} \usepackage{float} \usepackage{longtable} \usepackage{xcolor}    or use the epsfig package if you prefer to use the old commands    \usepackage{epsfig}     The amssymb package provides various useful mathematical symbols \usepackage{amssymb}    The amsthm package provides extended theorem environments    \usepackage{amsthm}     The lineno packages adds line numbers. Start line numbering with    . Or switch it on    for the whole article with \linenumbers after .    \usepackage{lineno}     natbib.sty is loaded by default. However, natbib options can be    provided with \biboptions{...} command. Following options are    valid:       round  -  round parentheses are used       square -  square brackets are used   [option]      curly  -  curly braces are used      {option}      angle  -  angle brackets are used    <option>      semicolon  -  multiple citations separated by semi-colon      colon  - same as semicolon, an earlier confusion      comma  -  separated by comma      numbers-  selects numerical citations      super  -  numerical citations as superscripts      sort   -  sorts multiple citations according to order in ref. list      sort&compress   -  like sort, but also compresses numerical citations      compress - compresses without sorting       \biboptions{comma,round}    \biboptions{}   \journal{Journal of Biomedical Informatics}              Start line numbering here if you want      \linenumbers     main text    
"," Detecting divisive and inappropriate content is a highly relevant task in Natural Language Processing today. Over the last few years, the class of problems pertaining to text classification have been dominated by deep learning based architectures. Kim , reports a series of experiments with CNN trained on top of pre trained word-vectors for sentence level classification tasks. Liu et al  develop what is called the XMLCNN built on top of KimCNN with modifications such as wider convolutional kernels, adaptive dynamic max-pooling, and an additional bottleneck layer to capture the features of large documents more efficiently.  Yang et al  propose the Hierarchical Attention Network model which consists of two levels of attention mechanism operating at the word as well as the sentence level thus focusing distinctively on more and less important content for deriving document representations. Adhikari et al  propose a simple Bidirectional LSTM with attention mechanism and appropriate regularization techniques to yield next to state-of-the-art results on document classification.  Alternatively, substantial work has been done to prove pre-trained models trained on a large corpus of data yield promising results in this domain, thus  avoiding training models from scratch. Mikolov et al  introduced a skip gram model with negative sampling termed word2vec and pennington et al  came up with the glove embeddings which leverages statistical information by training only on the nonzero elements in a word-word co occurrence matrix rather than their sparse representations. Peters et al  introduce Elmo which is a deep contextualized word representation which models complex syntax and semantics, trained with Bidirectional LSTMs on a large text corpus using a fixed embedding for each word,  looks at the entire sentence before assigning each word in it an embedding.  More recently the attention algorithm was introduced that completely changed the landscape in the NLP field. It was first introduced by the Google Brain team with the paper 闁炽儲绌総tention is all you need闁  emphasizing the fact that their model does not use recurrent neural networks at all.  Attention had already been a known idea used in LSTMs, but this was the first time it completely took the place of the recurrence. In the following paragraphs we explore various models and advancements that are built on top of the aforementioned works.",220
"  A contract is a legally binding agreement that recognizes and governs the rights and duties of the parties to the agreement. Correctly composing contracts is crucial to ensure its legal validity. In many real-world scenarios, a standard contract is prepared by filling   blanks in a precompiled form. Due to carelessness, two blanks that should be filled with the same  content may be incorrectly filled with different  content. This will result in contract inconsistencies, which may severely impair the legal validity of the contract.  Contract review is widely used by companies to check contract inconsistencies. However, contract review is labor-intensive and costly. Big companies have to hire tens of thousands of lawyers to conduct contract review, and it is estimated that Fortune Global  and Fortune  companies spend about 35281299,62194.05\%90.90\%$.  Our contributions are summarized as follows:   We formulate the Contract Inconsistency Checking  problem. As far as we know, this problem has not yet been studied in the AI community.   We propose a novel Pair-wise Blank Resolution  framework to address the CIC problem. In PBR, we propose a  that extends the Transformer encoder architecture to efficiently model meaningless blanks.   We collected and labeled  a large-scale Chinese contract corpus for CIC. The experimental results show the promising performance of our PBR method.        Our work is mainly related to three lines of recent researches: automatic contract analysis, coreference resolution, and blank modeling.  Existing automatic contract analysis methods mainly assist legal professionals by information extraction. Early ML-based attempts performed sentence-level classification on clause patterns and service exceptions. Recent DL-based methods focus on fine-grained intra-document classification of contract elements, obligations and prohibitions, and insurance policies, and cross-document search of relevant clauses. Previous works do not perform a further comparison on the retrieved related sentences, leaving the detailed checking process to lawyers.   are restricted to finding related sentences of the one under review, leaving further reading comprehension to legal professionals.  In this paper, we automate the CIC process in a fully data-driven and end-to-end manner that significantly speeds up the manual review process.  Coreference resolution  aims to identify the words or phrases  that refer to the same real-world entity. Existing methods can be classified into three broad categories of mention-pair, entity-mention, and ranking models. Mention-pair models predict the coreference label for every two mentions. Entity-mention models directly model an entity by clustering mentions. Ranking models were further introduced to model the degree of coreference. CIC can be view as a modified CR task that aims to identify the blanks that refer to the same content. However, CIC is much more challenging since the blanks are meaningless empties that can not be addressed with CR methods.  Blank modeling has been investigated in Text Infilling and Zero Pronoun Resolution . In text infilling, a blank is usually treated as an out-of-vocabulary token and modeled with sequence models such as BiLSTM and Transformer. However, these methods encode a blank with all its context words that contain irrelevant noise descriptions.  Similar to CR, ZPR aims to identify words that co-refer with a gap . To encode the gap, Yin et al. designed a CenteredLSTM that focuses on the more related local words. In their later work, self-attention was further utilized to enhance the performance. Recently, Aloraini et al. adopted BERT to encode the gap with its nearest two words. Though able to avoid incorporating irrelevant descriptions, ZPR methods would negligent faraway relevant descriptions.   yield inadequate blank representations due to the negligence of       In this paper, we introduce a new task, Writing Polishment with Similes, and curate a large-scale Chinese simile dataset. Our experiments demonstrate the feasibility and potential of the task, which we consider as a first step towards figurative writing polishment in a real-world setting. We establish Locate\&Gen model and benchmark it on the developed dataset.  Future works include but not limited to:    Furthermore, from an AI writing assistant perspective, we surmise that assisting humans with writing polishment is more likely to develop the potentials of current AI models than just letting AIs write on the fly . Given that figurative language is an essential creative aspect of language use, we encourage the use of the CS dataset in various contexts and look forward to the emergence of intelligent writing assistant tools like magic\footnote[1]{We applied our Locate\&Gen model to generate this simile, which is ``婵″倸鎮撴鏃婀抽懜顒傛畱'' in Chinese before being translated to English.} in the future.      
","  Our work is mainly related to three lines of recent researches: automatic contract analysis, coreference resolution, and blank modeling.  Existing automatic contract analysis methods mainly assist legal professionals by information extraction. Early ML-based attempts performed sentence-level classification on clause patterns and service exceptions. Recent DL-based methods focus on fine-grained intra-document classification of contract elements, obligations and prohibitions, and insurance policies, and cross-document search of relevant clauses. Previous works do not perform a further comparison on the retrieved related sentences, leaving the detailed checking process to lawyers.   are restricted to finding related sentences of the one under review, leaving further reading comprehension to legal professionals.  In this paper, we automate the CIC process in a fully data-driven and end-to-end manner that significantly speeds up the manual review process.  Coreference resolution  aims to identify the words or phrases  that refer to the same real-world entity. Existing methods can be classified into three broad categories of mention-pair, entity-mention, and ranking models. Mention-pair models predict the coreference label for every two mentions. Entity-mention models directly model an entity by clustering mentions. Ranking models were further introduced to model the degree of coreference. CIC can be view as a modified CR task that aims to identify the blanks that refer to the same content. However, CIC is much more challenging since the blanks are meaningless empties that can not be addressed with CR methods.  Blank modeling has been investigated in Text Infilling and Zero Pronoun Resolution . In text infilling, a blank is usually treated as an out-of-vocabulary token and modeled with sequence models such as BiLSTM and Transformer. However, these methods encode a blank with all its context words that contain irrelevant noise descriptions.  Similar to CR, ZPR aims to identify words that co-refer with a gap . To encode the gap, Yin et al. designed a CenteredLSTM that focuses on the more related local words. In their later work, self-attention was further utilized to enhance the performance. Recently, Aloraini et al. adopted BERT to encode the gap with its nearest two words. Though able to avoid incorporating irrelevant descriptions, ZPR methods would negligent faraway relevant descriptions.   yield inadequate blank representations due to the negligence of",221
"  Building a human-like open-domain conversational agent  has been one of the milestones in artificial intelligence . Early conversational agents are primarily based on rules , e.g., Eliza , the first CA developed in 60's, simulates a Rogerian psychotherapist based on hand-crafted pattern matching rules. In recent years, with the advancement of data-driven neural networks, neural open-domain conversational models are becoming dominant .  Recent efforts in open-domain neural conversational models are primarily aiming to improve the response diversity  and endowing responses with knowledge , personality , emotion  and empathy .  All the efforts mentioned above are focusing on models that passively respond to user messages. However, in many real-world scenarios, e.g., conversational recommendation, psychotherapy and education, conversational agents are required to actively lead the conversation by smoothly changing the conversation topic to a designated one. For example, during a casual conversation, the agent may actively lead the user to a specific product or service that the agent wants to introduce and recommend.  In this paper, we follow the line of research in  and study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. As illustrated in Figure , given a target keyword ``juice"" and a random starting keyword ``comics"", the agent is required to converse with the user in multiple exchanges and lead the conversation to ``juice"". The challenge of this problem lies in how to balance the tradeoff between maximizing keyword transition smoothness and minimizing the number of turns taken to reach the target. On the one hand, passively responding to the user solely based on the conversation context would achieve high smoothness but may take many turns to reach the target, but on the other hand, directly jumping to the target word by ignoring the conversation context would minimize the number of turns but produce non-smooth keyword transitions.  \citet{tang2019target} proposed to break down the problem into two sub-problems: next-turn keyword selection and keyword-augmented response retrieval. \citet{tang2019target} proposed a next-turn keyword predictor and a rule-based keyword selection strategy to solve the first sub-problem, allowing the agent to know what is the next keyword to talk about given the conversation history and the target keyword. In addition, \citet{tang2019target} proposed a keyword-augmented response retrieval model to solve the second sub-problem, allowing the agent to produce a response that is relevant to the selected keyword.    However, there are two major limitations in existing studies . First, the training and evaluation datasets for next-turn keyword prediction are directly extracted from conversations without human annotations, thus, the majority of the ground-truth keyword transitions are noisy and have low correlations with human judgements. As illustrated in Figure , only a few keyword transitions in a conversation are considered relevant. In fact, in our human annotation studies of over 600 keyword transitions, we found that around 70\% of keyword transitions in the next-turn keyword prediction datasets are rated as not relevant, which renders the trained next-turn keyword predictor in existing studies less reliable.  Second, the rule-based keyword selection strategy primarily leverages the cosine similarity between word embeddings to select keywords that are closer to the target keyword. Word embeddings are trained based on the distributional hypothesis that words that have similar contexts have similar meanings, which may not reflect how humans relate words in conversational turn-taking.  In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs  for both next-turn keyword selection and keyword-augmented response retrieval. Humans rely on commonsense to reason, and commonsense reasoning plays an important role in the cognitive process of conversational turn-taking . Relying on a CKG for keyword transition would allow the agent to select a more target-related keyword for the next-turn.  Moreover, we leverage commonsense triplets from the CKG using Graph Neural Networks  for both next-turn keyword prediction and keyword-augmented response retrieval to achieve more accurate predictions.   In summary, our contributions are as follows:    In recent years, several studies proposed to build conversational agents that can actively lead a conversation to a designated target keyword/goal . Our work follows the task definition in , which has been discussed in Introduction. Very recently, \citet{qin2020dynamic} improved  in 1) next-turn keyword prediction by only considering keyword transitions that are present in the training dataset and 2) keyword-augmented response retrieval by constraining that the selected response must contain the predicted keyword or a keyword closer to the target keyword. As a result, \citet{qin2020dynamic} obtained the state-of-the-art performance on this task in terms of task success rate and transition smoothness.  Another line of research  focused on the specific movie domain and proposed to use factoid knowledge graph to proactively lead the conversation from a random entity to a given entity. Our work differs from  in that 1) we focus on open-domain conversations whereas they focus on movie domain; 2) we leverage commonsense knowledge graph for keyword transitions whereas they leverage factoid knowledge graph for entity transitions\footnote{In our work, a keyword can be a named entity, e.g., AAAI2021, or a generic content word, e.g., conference.}; and 3) we allow the target to be any arbitrary keyword whereas they constrain the target to be at most two-hop away from the starting entity. Following the line of research in , \citet{xu2020knowledge} proposed to use hierarchical reinforcement learning  to incorporate factoid knowledge graph for high-level topic selection and low-level in-depth topic-related conversation. \citet{xu2020conversational} proposed a framework to represent prior information as a conversation graph  and leverage policy learning to incorporate the CG into conversation generation.  Commonsense has been studied extensively in recent neural conversational models .  \citet{zhou2018commonsense} proposed graph attentions to statically incorporate one-hop knowledge triplets into conversation understanding and dynamically generate knowledge-aware responses. Recently, \citet{zhang2020grounded} extended  to multi-hop knowledge triplets by proposing an attention mechanism to incorporate outer triplets and a GNN model to aggregate central triplets. Different from existing studies that leverage commonsense to improve the diversity and informativeness of responses, we incorporate commonsense into our approach for more reasonable keyword transition and more accurate response retrieval.     In this work, we formulate the Contract Inconsistency Checking  problem, an automatic contract analysis task with significant practical importance, and we propose a novel end-to-end Pair-wise Blank Resolution  framework to predict the consistency relation for every two blanks with high accuracy. In PBR, we extend the Transformer encoder architecture and propose \texttt{BlankCoder}, an off-the-shelf effective blank modeling method that could easily generalize to other tasks such as text infilling. Extensive experiments show that our model can significantly and consistently outperform existing baselines, yielding a promising balanced accuracy of  and an F1 score of . In the future, we plan to consider more complex cases  and explore more complex consistency checking scenarios that require logical reasoning.    .     
"," In recent years, several studies proposed to build conversational agents that can actively lead a conversation to a designated target keyword/goal . Our work follows the task definition in , which has been discussed in Introduction. Very recently, \citet{qin2020dynamic} improved  in 1) next-turn keyword prediction by only considering keyword transitions that are present in the training dataset and 2) keyword-augmented response retrieval by constraining that the selected response must contain the predicted keyword or a keyword closer to the target keyword. As a result, \citet{qin2020dynamic} obtained the state-of-the-art performance on this task in terms of task success rate and transition smoothness.  Another line of research  focused on the specific movie domain and proposed to use factoid knowledge graph to proactively lead the conversation from a random entity to a given entity. Our work differs from  in that 1) we focus on open-domain conversations whereas they focus on movie domain; 2) we leverage commonsense knowledge graph for keyword transitions whereas they leverage factoid knowledge graph for entity transitions\footnote{In our work, a keyword can be a named entity, e.g., AAAI2021, or a generic content word, e.g., conference.}; and 3) we allow the target to be any arbitrary keyword whereas they constrain the target to be at most two-hop away from the starting entity. Following the line of research in , \citet{xu2020knowledge} proposed to use hierarchical reinforcement learning  to incorporate factoid knowledge graph for high-level topic selection and low-level in-depth topic-related conversation. \citet{xu2020conversational} proposed a framework to represent prior information as a conversation graph  and leverage policy learning to incorporate the CG into conversation generation.  Commonsense has been studied extensively in recent neural conversational models .  \citet{zhou2018commonsense} proposed graph attentions to statically incorporate one-hop knowledge triplets into conversation understanding and dynamically generate knowledge-aware responses. Recently, \citet{zhang2020grounded} extended  to multi-hop knowledge triplets by proposing an attention mechanism to incorporate outer triplets and a GNN model to aggregate central triplets. Different from existing studies that leverage commonsense to improve the diversity and informativeness of responses, we incorporate commonsense into our approach for more reasonable keyword transition and more accurate response retrieval.",222
" In recent years, there has been a dramatic surge in the adoption of voice assistants such as Amazon Alexa, Apple Siri, and Google Assistant. Customers use them for a variety of tasks such as playing music and online shopping.  These voice assistants are built on complex Spoken Language Understanding  systems that are typically too large to store on an edge device such as a mobile phone or a smart speaker. Hence, user traffic is routed through a cloud server to process requests. This has led to privacy concerns and fueled the push for tiny AI and edge processing, where the user requests are processed on the device itself.   Traditional SLU systems consist of a two-stage pipeline, an Automatic Speech Recognition  component that processes customer speech and generates a text transcription , followed by a Natural Language Understanding  component that maps the transcription to an actionable hypothesis consisting of intents and slots . An end-to-end  system that goes directly from speech to the hypothesis would help make the SLU system smaller and faster, allowing it to be stored on an edge device. It could potentially also be better optimized than a pipeline since it eliminates cascading errors.  However, E2E systems are not used in practice because they have some key issues. These systems are hard to build since they consist of large neural components such as transformers and require massive amounts of E2E training data. They also don't make use of the vastly available training data for the ASR and NLU components that could be used to enhance their performance, because the examples in these datasets may not be aligned to create an E2E training sample. Another issue is feature expansion, a scenario where a new domain, with new intents and slots, is added to the voice assistant's capabilities. Here, developers typically only have access to some synthetically generated text-hypothesis examples. Speech data isn't readily available and it is very expensive to collect. E2E models thus fail as they require lots of new audio and hypothesis data to learn this new domain.  In this work, we build an E2E model that mitigates these issues using transfer learning. We call it the Audio-Text All-Task  Model. AT-AT is an E2E transformer-based model that is jointly trained on multiple audio-to-text and text-to-text tasks. Examples of these tasks include speech recognition , hypothesis prediction from speech , masked LM prediction , and hypothesis prediction from text . Our model achieves this by converting data from all these tasks into a single audio-to-text or text-to-text format. Figure shows this joint training phase in detail. Our findings indicate that there is significant knowledge transfer taking place from multiple tasks, which in turn helps in downstream model performance. We see that the AT-AT pretrained model shows improved performance on SLU hypothesis prediction on internal data collected from Alexa traffic. We also report state-of-the-art results on two public datasets: FluentSpeech , and SNIPS Audio .   Furthermore, since our model contains a text encoder, it can consume both audio and text inputs to generate a target sequence. By jointly training on both audio-to-text and text-to-text tasks, we hypothesize that this model learns a shared representation for audio and text inputs. This allows us to simply train on new text-to-text data and get audio-to-text performance for free, giving us a way to do E2E hypothesis prediction in a zero-shot fashion during feature expansion. We test this approach on an internal dataset from Alexa traffic, and an external dataset, Facebook TOP . Since TOP consists of only text data, we collected speech data for the test split using an internal tool at Amazon. We will soon release this dataset.  In summary, our contributions are as follows.     The architecture of prior E2E SLU models is taken from neural speech recognition literature. Speech recognition was originally performed using hidden Markov models that predict acoustic features, followed by word-level language models . More recently, deep learning models have become more popular for this task . Deep learning models solve this task by posing it as a sequence-to-sequence problem . With the success of transformer-based sequence-to-sequence models on text based tasks , researchers have explored and shown success in applying them for speech recognition . Our architecture is based on these models.  Other end-to-end SLU models also closely resemble this sequence-to-sequence encoder-decoder framework . The slot-filling task for SLU is formulated as a target text sequence by wrapping the target English tokens with intent and slot tags, which was shown to achieve state of the art results . Our approach improves upon these models by introducing transfer learning. The transfer learning paradigm we adopt here is similar to prior efforts that use multiple tasks or pretraining to improve SLU performance . The audio-text shared training idea also has prior work. However, these efforts require parallel audio-text data , or are evaluated on a simpler classification task .  Zeroshot E2E SLU, where we only have text NLU training data but no audio has also been explored. Recently,  approached this task using speech synthesis. They generate synthetic speech from text using a Text to Speech  system and use the resultant audio to train their models. While this approach is simple and intuitive, its success greatly depends on access to a good TTS system. We propose a method that can perform this task, end-to-end, without any TTS system, and can also be used in conjunction with a TTS system to further improve performance.  Finally, an important part of all these models is the representation of audio. The raw audio waveform is typically converted into higher level features before being passed to the actual models. While Mel-Frequency Cepstral Coefficitents  have been the traditional choice for this conversion, Log-filterbank features  have become more popular recently . We use LFB features here.      In this paper, we have presented a novel framework composed of a homophone error detector and a SANMT model to cope with homophone noise. Experimental results show that our method not only achieves substantial improvement over previous robust NMT baselines both on the test sets with artificial or real-world noise, but also outperforms the NMT baseline on the clean test sets. We consider that future studies could modeling noise detection and NMT jointly.    References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   -------------------------------------------------------------------------  \clearpage 
"," The architecture of prior E2E SLU models is taken from neural speech recognition literature. Speech recognition was originally performed using hidden Markov models that predict acoustic features, followed by word-level language models . More recently, deep learning models have become more popular for this task . Deep learning models solve this task by posing it as a sequence-to-sequence problem . With the success of transformer-based sequence-to-sequence models on text based tasks , researchers have explored and shown success in applying them for speech recognition . Our architecture is based on these models.  Other end-to-end SLU models also closely resemble this sequence-to-sequence encoder-decoder framework . The slot-filling task for SLU is formulated as a target text sequence by wrapping the target English tokens with intent and slot tags, which was shown to achieve state of the art results . Our approach improves upon these models by introducing transfer learning. The transfer learning paradigm we adopt here is similar to prior efforts that use multiple tasks or pretraining to improve SLU performance . The audio-text shared training idea also has prior work. However, these efforts require parallel audio-text data , or are evaluated on a simpler classification task .  Zeroshot E2E SLU, where we only have text NLU training data but no audio has also been explored. Recently,  approached this task using speech synthesis. They generate synthetic speech from text using a Text to Speech  system and use the resultant audio to train their models. While this approach is simple and intuitive, its success greatly depends on access to a good TTS system. We propose a method that can perform this task, end-to-end, without any TTS system, and can also be used in conjunction with a TTS system to further improve performance.  Finally, an important part of all these models is the representation of audio. The raw audio waveform is typically converted into higher level features before being passed to the actual models. While Mel-Frequency Cepstral Coefficitents  have been the traditional choice for this conversion, Log-filterbank features  have become more popular recently . We use LFB features here.",223
"  Neural Machine Translation   has achieved state of the art in various MT systems, including rich and low resource language pairs . However, the quality of low-resource MT is quite unpretentious due to the lack of parallel data while it has achieved better results on systems of the available resource. Therefore, low-resource MT is one of the essential tasks investigated by many previous works .    Recently, some works present MT systems that have achieved remarkable results for low-resource language . Inspired by these works, we collect data from the TED Talks domain, then attempt to build multilingual MT systems from French, English-Vietnamese. Experiments demonstrate that both language pairs: French-Vietnamese and English-Vietnamese have achieved significant performance when joining the training. %  Although multilingual MT can reduce the sparse data in the shared space by using word segmentation, however, rare words still exist, evenly they are increased more if languages have a significant disparity in term vocabulary. Previous works suggested some strategies to reduce rare words such as using translation units at sub-word and character levels or generating a universal representation at the word and sentence levels . These help to downgrade the dissimilarity of tokens shared from various languages. However, these works require learning additional parameters in training, thus increasing the size of models.   Our paper presents two methods to augment the translation of rare words in the source space without modifying the architecture and model size of MT systems:  exploiting word similarity. This technique has been mentioned by previous works . They employ monolingual data or require supervised resources like a bilingual dictionary or WordNet, while we leverage relation from the multilingual space of MT systems.  Adding a scalar value to the rare word embedding in order to facilitate its translation in the training process.  %  Due to the fact that NMT tends to have bias in translating frequent words, so rare words  often have less opportunity to be considered. Our ideal is inspired by the works of .  and  proposed various solutions to urge for translation of rare words, including modification embedding in training. They only experimented with recurrent neural networks  while our work uses the state-of-the-art transformer architecture.  transforms the word embedding of a token into the universal space, and they learn plus parameters while our method does not.  We apply our strategies in our fine-tuning processes, and we show substantial improvements of the systems after some epochs only.    Monolingual data are widely used in NMT to augment data for low-resource NMT systems . Back-translation  is known as the most popular technique in exploiting target-side monolingual data to enhance the translation systems while the self-learning method  focuses on utilizing source-side monolingual data. Otherwise, the dual-learning strategy  also suggests using both source- and target-side monolingual data to tackle this problem. Our work investigates the self-learning method  on the low-resource multilingual NMT systems specifically related to Vietnamese. Besides, monolingual data are also leveraged in unsupervised or zero-shot translation.  % learn the lexical relative between one token on a source language and the other once from another source language without modifying the system architecture as well as the model size. We also do not use any additional resources in our systems.   The main contributions of our work are:    In section 2, we review the transformer architecture used for our experiments. The brief of multilingual translation is shown in section 3. Section 4 presents our methods to deal with rare words in multilingual translation scenarios. The exploitation of monolingual data for low-resource multilingual MT is discussed in section 5. Our results are described in section 6, and related work is shown in section 7. Finally, the paper ends with conclusions and future work. %     Due to the unavailability of the parallel data for low-resource language pairs or zero-shot translation, previous works focus on the task to have more data such as leveraging multilingual translation  or using monolingual data with back-translation, self-learning  or mix-source  technique.  For leveraging multilingual translation,  added language code and target forcing in order to learn the shared representations of the source words and specify the target words.  demonstrated a one-to-many multilingual MT with three different strategies which modify their architecture.  built many-to-one multilingual MT systems by adding a layer to transform the source embeddings and representation into a universal space to augment the translation of low resource language, which is similar to ours.  implemented a massive many-to-many multilingual system, employing many low-resource language pairs. All of the mentioned works have shown substantial improvements in low-resource translation, however, they are less correlative to our translation tasks.  Although multilingual MT equips a shared space with many advantages, rare word translation is still the issue that needs to be considered. The task of dealing with rare words has been mentioned in previous works.  copied words from source sentences by words from target sentences after the translation using a bilingual dictionary.  and  learned word similarity from monolingual data to improve their systems. Our approach is similar to these works, but we only learn similarity from the shared multilingual space of MT systems.  addressed the rare word problem by using the synonyms from WordNet.    and  presented different solutions to solve rare word situation by transforming the embeddings during the training of their RNN-based architecture. Those solutions cannot be applied to the transformer architecture. In , the embeddings of rare tokens and universal tokens are jointly learned through a plus parameter while we only add a scalar value to the embeddings.  Monolingual data is used to generate synthetic bilingual data in sparsity data issues.  proposed back-translation method that uses a backward model to get the source data from the monolingual target data.  In contrast,  shown the self-learning technique by employing a forward model to translate monolingual source data into the target data.  incorporated both mentioned techniques into their NMT systems. Monolingual data is also demonstrated its efficiency in unsupervised machine translation or in zero-shot multilingual NMT . In our work, we use the self-learning method to produce pseudo bilingual data, and it is then used to train our low-resource multilingual NMT systems.     Our evaluation clearly shows that there is a lot of knowledge transfer happening between various speech processing tasks. AT-AT when evaluated on downstream SLU tasks benefits significantly when it is pretrained with additional ASR data. This result holds when the ASR data is from the same domain  and also when the data is from a different domain . It also holds across different dataset sizes. We see that the pretraining is extremely helpful for datasets with training data size of about a 1000 such as SNIPS, and it remains helpful all the way to our limited internal music dataset  and the full music dataset . We believe this is because the decoder learns a good language model by seeing additional ASR data. We can also think of these additional pretraining tasks as good regularizers.    Our zeroshot results with AT-AT are even more interesting. We designed a way to train an end-to-end model on new data without using any corresponding audio data, real or synthetically generated, and our model's performance, while not matching an end-to-end model trained on real audio data, is still remarkable. Our approach can be adapted to make use of synthetic data if we have access to a TTS system to further improve performance. We managed to learn a shared audio-text model, not by explicitly enforcing a loss penalty to force the audio and text hidden states into the same space, but by constraining the decoder and forcing the model to learn jointly from different input sources.    On a closing note, we would like to remark that AT-AT somewhat mimics actual human learning. We typically read a lot more words than we hear. But when we hear a word for the first time, we transfer our knowledge of that word from when we read it. AT-AT similarly learns to understand and perform NLU tagging from text and then applies this knowledge when it is given speech.  \section{Conclusion} We propose the Audio-Text All-Task  model that uses transfer learning to improve the performance on end-to-end SLU. AT-AT beat the performance of E2E models on our internal music data, both in the full and low-resource settings. It also achieved state-of-the-art performance on the FluentSpeech  and SNIPS audio datasets  with significant improvements over prior models. AT-AT also demonstrated its ability to perform zeroshot E2E SLU, without access to a TTS system, and by learning a shared audio-text representation without any explicit loss penalty to force the audio and text hidden states into the same space. We also showed how AT-AT can work in conjunction with a TTS system to further improve E2E performance. It achieves a zeroshot E2E EM Accuracy of 70.60 on the TOP dataset.    We set this new benchmark and release the audio data for the TOP dataset for future research.   On a closing note, we would like to remark that AT-AT somewhat mimics actual human learning. We typically read a lot more words than we hear. But when we hear a word for the first time, we transfer our knowledge of that word from when we read it. AT-AT similarly learns to understand and perform NLU tagging from text and then applies this knowledge when it is given speech.    
","   Due to the unavailability of the parallel data for low-resource language pairs or zero-shot translation, previous works focus on the task to have more data such as leveraging multilingual translation  or using monolingual data with back-translation, self-learning  or mix-source  technique.  For leveraging multilingual translation,  added language code and target forcing in order to learn the shared representations of the source words and specify the target words.  demonstrated a one-to-many multilingual MT with three different strategies which modify their architecture.  built many-to-one multilingual MT systems by adding a layer to transform the source embeddings and representation into a universal space to augment the translation of low resource language, which is similar to ours.  implemented a massive many-to-many multilingual system, employing many low-resource language pairs. All of the mentioned works have shown substantial improvements in low-resource translation, however, they are less correlative to our translation tasks.  Although multilingual MT equips a shared space with many advantages, rare word translation is still the issue that needs to be considered. The task of dealing with rare words has been mentioned in previous works.  copied words from source sentences by words from target sentences after the translation using a bilingual dictionary.  and  learned word similarity from monolingual data to improve their systems. Our approach is similar to these works, but we only learn similarity from the shared multilingual space of MT systems.  addressed the rare word problem by using the synonyms from WordNet.    and  presented different solutions to solve rare word situation by transforming the embeddings during the training of their RNN-based architecture. Those solutions cannot be applied to the transformer architecture. In , the embeddings of rare tokens and universal tokens are jointly learned through a plus parameter while we only add a scalar value to the embeddings.  Monolingual data is used to generate synthetic bilingual data in sparsity data issues.  proposed back-translation method that uses a backward model to get the source data from the monolingual target data.  In contrast,  shown the self-learning technique by employing a forward model to translate monolingual source data into the target data.  incorporated both mentioned techniques into their NMT systems. Monolingual data is also demonstrated its efficiency in unsupervised machine translation or in zero-shot multilingual NMT . In our work, we use the self-learning method to produce pseudo bilingual data, and it is then used to train our low-resource multilingual NMT systems.",224
" % Fabian: Describing what it is Entity linking  is the task of mapping entity mentions in text documents to standard entities in a given knowledge base. For example, the word ``Paris'' is ambiguous: It can refer either to the capital of France or to a hero of Greek mythology. Now given the text ``Paris is the son of King Priam'', the goal is to determine that, in this sentence, the word refers to the Greek hero, and to link the word to the corresponding entity in a knowledge base such as YAGO  or DBpedia .  %Intriguingly, the Greek hero also goes by the name of ``Alexander''. Thus, the words ``Paris'' and ``Alexander'' are synonymous, and if they both refer to the Greek hero in some input text, they both have to be linked to the same entity in the knowledge base.  % Fabian: Describing why it's important In the biomedical domain, entity linking maps mentions of diseases, drugs, and measures to normalized entities in standard vocabularies. It is an important ingredient for automation in medical practice, research, and public health. Different names of the same entities in Hospital Information Systems seriously hinder the integration and use of medical data. If a medication appears with different names, researchers cannot study its impact, and patients may erroneously be prescribed the same medication twice.   % Fabian: Describing why it's difficult The particular challenge of biomedical entity linking is not the ambiguity: a word usually refers to only a single entity. Rather, the challenge is that the surface forms vary markedly, due to abbreviations, morphological variations, synonymous words, and different word orderings.  For example, ``Diabetes Mellitus, Type 2'' is also written as ``DM2'' and ``lung cancer'' is also known as ``lung neoplasm malignant''. In fact, the surface forms vary so much that all the possible expressions of an entity cannot be known upfront. This means that standard disambiguation systems cannot be applied in our scenario, because they assume that all forms of an entity are known. %, and thus they cannot be applied in our scenario.  One may think that variation in surface forms is not such a big problem, as long as all variations  of an entity are sufficiently close to its canonical form. Yet, this is not the case. For example, the phrase ""decreases in hemoglobin"" could refer to at least 4 different entities in MedDRA, which all look alike:  ""changes in hemoglobin"", ""increase in hematocrit"", ""haemoglobin decreased"", and ""decreases in platelets"". In addition, biomedical entity linking cannot rely on external resources such as  alias tables, entity descriptions, or entity co-occurrence, which are often used in classical entity linking settings.   % Fabian: what has been done For this reason, entity linking approaches have been developed particularly for biomedical entity linking. Many methods use deep learning: the work of \citet{li2017cnn} casts biomedical entity linking as a ranking problem,  leveraging convolutional neural networks .  More recently, the introduction of BERT has advanced the performance of many NLP tasks, including in the biomedical domain .  BERT creates rich pre-trained representations on unlabeled data and achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. However, considering the number of parameters of pre-trained BERT models,  the improvements brought by fine-tuning them come with a heavy computational cost and memory footprint.  This is a problem for energy efficiency, for smaller organizations, or in poorer countries.  In this paper, we introduce a very lightweight model that achieves a performance statistically indistinguishable from the state-of-the-art BERT-based models. The central idea is to use an alignment layer with an attention mechanism,  which can capture the similarity and difference of corresponding parts between candidate and mention names. Our model is 23x smaller and 6.4x faster than BERT-based models on average; and more than twice smaller and faster than the lightweight BERT models. Yet, as we show, our model achieves comparable performance on all standard benchmarks. Further, we can show that adding more complexity to our model is not necessary: the entity-mention priors, the context around the mention, or the coherence of extracted entities \cite[as used, e.g., in][]{hoffart2011robust} do not improve the results any further. \footnote{All data and code are available at  \url{https://github.com/tigerchen52/Biomedical-Entity-Linking}.}      In the biomedical domain, much early research focuses on capturing string similarity of mentions and entity names with rule-based systems. Rule-based systems are simple and transparent, but researchers need to define rules manually, and these are bound to an application.  To avoid manual rules, machine-learning approaches learn suitable similarity measures between mentions and entity names automatically from training sets.  However, one drawback of these methods is that they cannot recognize semantically related words.  For example, they cannot see that `cardiac' and `heart' are closely related.  Recently, deep learning methods have been successfully applied to different NLP tasks, based on  pre-trained word embeddings, such as word2vec  and Glove .  \citet{li2017cnn} and \citet{wright2019normco} introduce a CNN and RNN, respectively, with pre-trained word embeddings, which casts biomedical entity linking into a ranking problem.  However, traditional methods for learning word embeddings allow for only a single context-independent representation of each word.   Bidirectional Encoder Representations from Transformers  address this problem by pre-training deep bidirectional representations from unlabeled text, jointly conditioning on both the left and the right context in all layers. \citet{ji2020bert} proposed an biomedical entity normalization architecture by fine-tuning the pre-trained BERT / BioBERT / ClinicalBERT models .   Extensive experiments show that their model outperforms previous methods and advanced the state-of-the-art for biomedical entity linking. A shortcoming of BERT is that it needs high-performance machines.      We have built multilingual MT systems for two low-resource language pairs: English-Vietnamese and French-Vietnamese, and proposed two approaches to tackle rare word translation. We show that our approaches bring significant improvements to our MT systems. We find that the pseudo bilingual can furthermore enhance a multilingual NMT system in case of French  Vietnamese translation task.  In the future, we would like to use more language pairs in our systems and to combine proposed methods in order to evaluate the effectiveness of our MT systems.   
","  In the biomedical domain, much early research focuses on capturing string similarity of mentions and entity names with rule-based systems. Rule-based systems are simple and transparent, but researchers need to define rules manually, and these are bound to an application.  To avoid manual rules, machine-learning approaches learn suitable similarity measures between mentions and entity names automatically from training sets.  However, one drawback of these methods is that they cannot recognize semantically related words.  For example, they cannot see that `cardiac' and `heart' are closely related.  Recently, deep learning methods have been successfully applied to different NLP tasks, based on  pre-trained word embeddings, such as word2vec  and Glove .  \citet{li2017cnn} and \citet{wright2019normco} introduce a CNN and RNN, respectively, with pre-trained word embeddings, which casts biomedical entity linking into a ranking problem.  However, traditional methods for learning word embeddings allow for only a single context-independent representation of each word.   Bidirectional Encoder Representations from Transformers  address this problem by pre-training deep bidirectional representations from unlabeled text, jointly conditioning on both the left and the right context in all layers. \citet{ji2020bert} proposed an biomedical entity normalization architecture by fine-tuning the pre-trained BERT / BioBERT / ClinicalBERT models .   Extensive experiments show that their model outperforms previous methods and advanced the state-of-the-art for biomedical entity linking. A shortcoming of BERT is that it needs high-performance machines.",225
" % background Sentence semantic matching is a fundamental Natural Language Processing~ task that tries to infer the most suitable label for a given sentence pair. For example, Natural Language Inference~ targets at classifying the input sentence pair into one of the three relations~. Paraphrase Identification~ aims at identifying whether the input sentence pair expresses the same meaning. Figure gives some examples with different semantic relations from different datasets.    % Current state As a fundamental technology, sentence semantic matching has been applied successfully into many NLP fields, e.g., information retrieval, question answering, and dialog system.  Currently, most work leverages the advancement of representation learning techniques to tackle this task.  They focus on input sentences and design different architectures to explore sentence semantics comprehensively and precisely.  Among all these methods, BERT plays an important role.  It adopts multi-layer transformers to make full use of large corpus~ for the powerful pre-trained model.  Meanwhile, two self-supervised learning tasks~ are designed to better analyze sentence semantics and capture as much information as possible.  % more citation Based on BERT, plenty of work has made a big step in sentence semantic modeling.    In fact, since relations are the predicting targets of sentence semantic matching task, most methods do not pay enough attention to the relation learning.  They just leverage annotated labels to represent relations, which are formulated as one-hot vectors.  However, these independent and meaningless one-hot vectors cannot reveal the rich semantic information and guidance of relations, which will cause an information loss.  \citeauthor{gururangan2018annotation}~ has observed that different relations among sentence pairs imply specific semantic expressions.  Taking Figure as an example, most sentence pairs with ``contradiction'' relation contain negation words~.  ``entailment'' relation often leads to exact numbers being replaced with approximates~.  ``Neutral'' relation will import some correct but irrelevant information~.  Moreover, the expressions between sentence pairs with different relations are very different.  Therefore, the comparison and contrastive learning among different relations~ can help models to learn more about the semantic information implied in the relations, which in turn helps to strengthen the sentence analysis ability of models. They should be treated as more than just meaningless one-hot vectors.   One of the solutions for better relation utilization is the embedding method inspired by Word2Vec.  Some researchers try to jointly encode the input sentences and labels in the same embedding space for better relation utilization during sentence semantic modeling.  Despite the progress they have achieved, label embedding method requires more data and parameters to achieve better utilization of relation information.  It still cannot fully explore the potential of relations due to the small number of relation categories or the lack of explicit label embedding initialization.   To this end, in this paper, we propose a novel \fullname~approach to make full use of relation information in a simple but effective way.  In concrete details, we first utilize pre-trained BERT to model semantic meanings of the input words and sentences from a global perspective.  Then, we develop a CNN-based encoder to obtain partial information~ of sentences from a local perspective.  Next, inspired by self-supervised learning methods in BERT training processing, we propose a Relation of Relation~ classification task to enhance the learning ability of \shortname~for the implicit common features corresponding to different relations.    Moreover, a triplet loss is used to constrain the model, so that the intra-class and inter-class relations are analyzed better.   Along this line, input sentence pairs with the same relations will be represented much closer and vice versa further apart.  Relation information is properly integrated into sentence pair modeling processing, which is in favor of tackling the above challenges and improving the model performance.  Extensive evaluations of two sentence semantic matching tasks  demonstrate the effectiveness of our proposed \shortname~and its advantages over state-of-the-art sentence semantic matching baselines.      In this section, we mainly introduce the related work from two aspects: 1) Sentence Semantic Matching, and 2) Label Embedding for Text Classification.   With the development of various neural network technologies such as CNN, GRU, and the growing importance of the attention mechanism, plenty of methods have been exploited for sentence semantic matching on large datasets like SNLI, SciTail~, and Quora~.  Traditionally, researchers try to fully use neural network technologies to model semantic meanings of sentences in an end-to-end fashion.  Among them, CNNs focus on the local context extraction with different kernels, and RNNs are mainly utilized to capture the sequential information and semantic dependency.  For example, \citeauthor{mou2015natural}~ employed a tree-based CNN to capture the local context information in sentences.  \citeauthor{zhang2018ImageEnhance}~ combined CNN and GRU into a hybrid architecture, which utilizes the advantages of both networks. They used CNN to generate phrase-level semantic meanings and GRU to model the word sequence and dependency between sentences.    Recently, attention-based methods have shown very promising results on many NLP tasks, such as machine translation~, reading comprehension, and NLI.  Attention helps to extract the most important parts in sentences, capture semantic relations, and align the elements of two sentences properly. It has become an essential component for improving model performance and sentence understanding.  Early attempts focus on designing different attention methods that are suitable for specific tasks, like inner-attention, co-attention, and multi-head attention. To fully explore the potential of attention mechanism, \citeauthor{zhang2019drr}~ proposed a dynamic attention mechanism, which imitates human reading behaviors to select the most important word at each reading step. This method has achieved impressive performance.  Another direction is pre-trained methods. \citeauthor{devlin2018bert}~ used very large corpus and multi-layer transformers to obtain a powerful per-trained BERT.  This method leverages multi-head self-attention to encode sentences and achieves remarkable performances on various NLP tasks. With the powerful representation ability, pre-trained BERT  model has accelerated the NLP research.   However, most of these methods only focus on the input sentences and treat the labels as meaningless one-hot vectors, which ignores the potential of label information. There still remains plenty of space for further improvement on sentence semantic matching.     As an extremely important part of training data, labels contain much implicit information that needs to be explored.  In computer vision, researchers have proposed label embedding methods to make full use of label information.   However, research on explicit label utilization in NLP is still a relatively new domain.  One possible reason is that there are not that many labels in NLP tasks. Thus, label information utilization is only considered on the task with relatively a large number of labels or multi-task learning.  For example, \citeauthor{zhang-etal-2018-multi}~ proposed a multi-task label embedding method for better implicit correlations and common feature extraction among related tasks.  \citeauthor{du2019explicit}~ designed an explicit interaction model to analyze the fine-grained interaction between word representations and label embedding.  They have achieved impressive performance on text classification tasks.   In addition,  \citeauthor{wang-etal-2018-joint-embedding}~ and \citeauthor{pappers2019gile}~ transferred the text classification task to a label-word joint embedding problem.  They leveraged the semantic vectors of labels to guide models to select the important and relevant parts of input sentences for better performance.  The above work demonstrates the superiority of explicit label utilization and inspires us to make better use of label information.        In this work, we proposed a novel method to extract rationales for neural predictions. Our method uses an adversarial-based technique to make a selector-predictor model learn from a guider model. In addition, we proposed a novel regularizer based on language models, which makes the extracted rationales semantically fluent.  In this way, the ``guider"" model  tells the selector-predictor model what kind of information  remains unselected or over-selected.   We conducted experiments on a task of sentiment analysis and three tasks from the legal domain.  The experimental results showed that our method improves the selection of rationales by a large margin.   This regularizer  also gives priority to important adjacent word pairs when considering whether to select or unselect them simultaneously,  which further refines the rationales.   Finally, we have conducted experiments on two datasets to prove the effectiveness of our model.  We conducted experiments on two datasets to prove the effectiveness of our model.  As future work, the main architecture of our model can be directly applied to other domains, e.g., images or tabular data. However, it remains an open question what would be a good regularizer for these domains.   For example, variational autoencoders with discrete latent space, providing rationales to different kinds of deep learning applications.     
"," In this section, we mainly introduce the related work from two aspects: 1) Sentence Semantic Matching, and 2) Label Embedding for Text Classification.   With the development of various neural network technologies such as CNN, GRU, and the growing importance of the attention mechanism, plenty of methods have been exploited for sentence semantic matching on large datasets like SNLI, SciTail~, and Quora~.  Traditionally, researchers try to fully use neural network technologies to model semantic meanings of sentences in an end-to-end fashion.  Among them, CNNs focus on the local context extraction with different kernels, and RNNs are mainly utilized to capture the sequential information and semantic dependency.  For example, \citeauthor{mou2015natural}~ employed a tree-based CNN to capture the local context information in sentences.  \citeauthor{zhang2018ImageEnhance}~ combined CNN and GRU into a hybrid architecture, which utilizes the advantages of both networks. They used CNN to generate phrase-level semantic meanings and GRU to model the word sequence and dependency between sentences.    Recently, attention-based methods have shown very promising results on many NLP tasks, such as machine translation~, reading comprehension, and NLI.  Attention helps to extract the most important parts in sentences, capture semantic relations, and align the elements of two sentences properly. It has become an essential component for improving model performance and sentence understanding.  Early attempts focus on designing different attention methods that are suitable for specific tasks, like inner-attention, co-attention, and multi-head attention. To fully explore the potential of attention mechanism, \citeauthor{zhang2019drr}~ proposed a dynamic attention mechanism, which imitates human reading behaviors to select the most important word at each reading step. This method has achieved impressive performance.  Another direction is pre-trained methods. \citeauthor{devlin2018bert}~ used very large corpus and multi-layer transformers to obtain a powerful per-trained BERT.  This method leverages multi-head self-attention to encode sentences and achieves remarkable performances on various NLP tasks. With the powerful representation ability, pre-trained BERT  model has accelerated the NLP research.   However, most of these methods only focus on the input sentences and treat the labels as meaningless one-hot vectors, which ignores the potential of label information. There still remains plenty of space for further improvement on sentence semantic matching.     As an extremely important part of training data, labels contain much implicit information that needs to be explored.  In computer vision, researchers have proposed label embedding methods to make full use of label information.   However, research on explicit label utilization in NLP is still a relatively new domain.  One possible reason is that there are not that many labels in NLP tasks. Thus, label information utilization is only considered on the task with relatively a large number of labels or multi-task learning.  For example, \citeauthor{zhang-etal-2018-multi}~ proposed a multi-task label embedding method for better implicit correlations and common feature extraction among related tasks.  \citeauthor{du2019explicit}~ designed an explicit interaction model to analyze the fine-grained interaction between word representations and label embedding.  They have achieved impressive performance on text classification tasks.   In addition,  \citeauthor{wang-etal-2018-joint-embedding}~ and \citeauthor{pappers2019gile}~ transferred the text classification task to a label-word joint embedding problem.  They leveraged the semantic vectors of labels to guide models to select the important and relevant parts of input sentences for better performance.  The above work demonstrates the superiority of explicit label utilization and inspires us to make better use of label information.",226
" 	Discovering novel user intents is important to improve the service quality in dialogue systems. By analyzing the discovered new intents, we may find underlying user interests, which could provide business opportunities and guide the improvement direction.  	 	 	Intent discovery has attracted much attention in recent years. Many researchers regard it as an unsupervised clustering problem, and they manage to incorporate some weak supervised signals to guide the clustering process. For example,~\citet{hakkani-tr2013a} propose a hierarchical semantic clustering model and collect web page clicked information as implicit supervision for intent discovery.~\citet{hakkani2015clustering} utilize a semantic parsing graph as extra knowledge to mine novel intents during clustering.~\citet{Padmasundari2018} benefit from the consensus predictions of multiple clustering techniques to discover similar semantic intent-wise clusters.~\citet{haponchyk2018supervised} cluster questions into user intent categories under the supervision of structured outputs.~\citet{shi2018auto} extract intent features with an autoencoder and automatically label the intents with a hierarchical clustering method. 	  	However, all of the above methods fail to leverage the prior knowledge of known intents. These methods assume that the unlabeled samples are only composed of undiscovered new intents. A more common case is that some labeled data of known intents are accessible and the unlabeled data are mixed with both known and new intents. As illustrated in Figure, we may have a few labeled samples  of known intents in advance. The remaining known and new intent samples are all unlabeled. Our goal is to find known intents and discover new intents with the prior knowledge of limited labeled data. Our previous work CDAC+ directly tackles this problem. Nevertheless, it uses pairwise similarities as weak supervised signals, which are ambiguous to distinguish a mixture of unlabeled known and new intents. Thus, the performance drops with more new intents. 	 	To summarize, there are two main difficulties in our task. On the one hand, it is challenging to effectively transfer the prior knowledge from known intents to new intents with limited labeled data. On the other hand, it is hard to construct high-quality supervised signals to learn friendly representations for clustering both unlabeled known and new intents. 	 	To solve these problems, we propose an effective method to leverage the limited prior knowledge of known intents and provide high-quality supervised signals for feature learning.  As illustrated in Figure, we firstly use the pre-trained BERT model to extract deep intent features. Then, we pre-train the model with the limited labeled data under the supervision of the softmax loss. We retain the pre-trained parameters and use the learning information to obtain well-initialized intent representations. Next, we perform clustering on the extracted intent features and estimate the cluster number   by eliminating the low-confidence clusters. 	 	As most of the training samples are unlabeled, we propose an original alignment strategy to construct high-quality pseudo-labels as supervised signals for learning discriminative intent features. For each training epoch, we firstly perform k-means on the extracted intent features, and then use the produced cluster assignments as pseudo-labels for training the neural network. However, the inconsistent assigned labels cannot be directly used as supervised signals, so we use the cluster centroids as the targets to obtain the alignment mapping between pseudo-labels in consequent epochs. Finally, we perform k-means again for inference. Benefit from the relatively consistent aligned targets, our method can inherit the history learning information and boost the clustering performance. 	 	We summarize our contributions as follows. Firstly, we propose a simple and effective method that successfully generalizes to mass of new intents and estimate the number of novel classes with limited prior knowledge of known intents. Secondly, we propose an effective alignment strategy to obtain high-quality self-supervised signals by learning discriminative features to distinguish both known and new intents. Finally, extensive experiments on two benchmark datasets show our approach yields better and more robust results than the state-of-the-art methods.  	 	  	 	Many researchers try modeling user intents in dialogue systems in recent years. A line for these works is to enrich the intent information jointly with other tasks, such as sentiment classification, slot filling and so on. Another line is to  	leverage hidden semantic information to construct supervised signals for intent feature learning. In this work, we follow the second line to model intents.  	 	 	There are many classical unsupervised clustering methods, such as partition-based methods, hierarchical methods and density-based methods. However, the high-dimensional pattern representations suffer from high computational complexity and poor performance. Though some feature dimensionality reduction and data transformation methods have been proposed, these methods still can not capture high-level semantics of intent features.  	 	\subsubsection{Deep Clustering} 	With the development of deep learning, researchers adopt deep neural networks  to extract friendly features for clustering. The joint unsupervised learning  combines deep feature learning with hierarchical clustering but needs huge computational and memory cost on large-scale datasets. Deep Embedded Clustering  trains the autoencoder with the reconstruction loss and iteratively refines the cluster centers by optimizing KL-divergence with an auxiliary target distribution. Compared with DEC, Deep Clustering Network  further introduces a k-means loss as the penalty term to reconstruct the clustering loss. Deep Adaptive Image Clustering  utilizes the pairwise similarities as the learning targets and adopts an adaptive learning algorithm to select samples for training. However, all these clustering methods cannot provide specific supervised signals for representation learning. 	 	DeepCluster benefits from the structured outputs to boost the discriminative power of the convolutional neural network . It alternately performs k-means and representation learning. It considers the cluster assignments as pseudo-labels, which are explicit supervised signals for grouping each class. However, it needs to reinitialize the classifier parameters randomly before each training epoch. To deal with this issue, we propose an alignment strategy to produce aligned pseudo-labels for self-supervised learning without reinitialization. 	 	 	Although there are various unsupervised clustering methods, the performances of these methods are still limited without the prior knowledge for guiding the clustering process. Therefore, researchers perform semi-supervised clustering with the aid of some labeled data. 	 	Classical constrained clustering methods use the pairwise information as constraints for guiding the representation learning and clustering process. COP-KMeans uses instance-level constraints  and modifies k-means to satisfy these constraints. PCK-means presents a framework for pairwise constrained clustering, and it further selects informative pairwise constraints with an active learning method. MPCK-means incorporates the metric-learning approach into PCK-means and combined the centroid-based methods and metric-based methods into a unified framework. However, these methods need huge computational cost by enumerating pairwise conditions.  	 	KCL uses deep neural networks to perform pairwise constraint clustering. It firstly trains an extra network for binary similarity classification with a labeled auxiliary dataset. Then, it transfers the prior knowledge of pairwise similarity to the target dataset and uses KL-divergence to evaluate the pairwise distance. MCL uses the meta classification likelihood as the criterion to learn pairwise similarities. However, the domain adaptation methods are still limited in our task. CDAC+ is specifically designed for discovering new intents. It uses limited labeled data as a guide to learn pairwise similarities. However, it is limited in providing specific supervised signals and fails to estimate the number of novel classes. DTC is a method for discovering novel classes in computer vision. It improves the DEC algorithm and transfers the knowledge of labeled data to estimate the number of novel classes. However, the amount of the labeled data has a great influence on its performance. 	 	 		\caption{   Statistics of CLINC and BANKING datasets. \# indicates the total number of sentences. In each run of the experiment, we randomly select 75\  intents as known intents. Taking the CLINC dataset as an example, we randomly select 113 known intents and treat the remaining 37 intents as new intents. } 	 	 	 	  In this paper, we presented a simple but effective method named \shortname~for sentence semantic matching.  This method not only uses powerful BERT and CNN to encode sentences from global and local perspectives, but also makes full use of relation information for better performance enhancement.  Specifically, we design a R classification task to help \shortname~for learning the implicit common knowledge from the pairwise relation learning processing.  Moreover, a triplet loss is employed to constrain \shortname~for better triplet based relation learning and intra-class and inter-class information analyzing.  Extensive experiments on NLI and PI tasks demonstrate the superiority of \shortname. In the future, we plan to combine the advantages of label embedding method for better sentence semantic comprehension.    
"," 	 	Many researchers try modeling user intents in dialogue systems in recent years. A line for these works is to enrich the intent information jointly with other tasks, such as sentiment classification, slot filling and so on. Another line is to  	leverage hidden semantic information to construct supervised signals for intent feature learning. In this work, we follow the second line to model intents.  	 	 	There are many classical unsupervised clustering methods, such as partition-based methods, hierarchical methods and density-based methods. However, the high-dimensional pattern representations suffer from high computational complexity and poor performance. Though some feature dimensionality reduction and data transformation methods have been proposed, these methods still can not capture high-level semantics of intent features.  	 	\subsubsection{Deep Clustering} 	With the development of deep learning, researchers adopt deep neural networks  to extract friendly features for clustering. The joint unsupervised learning  combines deep feature learning with hierarchical clustering but needs huge computational and memory cost on large-scale datasets. Deep Embedded Clustering  trains the autoencoder with the reconstruction loss and iteratively refines the cluster centers by optimizing KL-divergence with an auxiliary target distribution. Compared with DEC, Deep Clustering Network  further introduces a k-means loss as the penalty term to reconstruct the clustering loss. Deep Adaptive Image Clustering  utilizes the pairwise similarities as the learning targets and adopts an adaptive learning algorithm to select samples for training. However, all these clustering methods cannot provide specific supervised signals for representation learning. 	 	DeepCluster benefits from the structured outputs to boost the discriminative power of the convolutional neural network . It alternately performs k-means and representation learning. It considers the cluster assignments as pseudo-labels, which are explicit supervised signals for grouping each class. However, it needs to reinitialize the classifier parameters randomly before each training epoch. To deal with this issue, we propose an alignment strategy to produce aligned pseudo-labels for self-supervised learning without reinitialization. 	 	 	Although there are various unsupervised clustering methods, the performances of these methods are still limited without the prior knowledge for guiding the clustering process. Therefore, researchers perform semi-supervised clustering with the aid of some labeled data. 	 	Classical constrained clustering methods use the pairwise information as constraints for guiding the representation learning and clustering process. COP-KMeans uses instance-level constraints  and modifies k-means to satisfy these constraints. PCK-means presents a framework for pairwise constrained clustering, and it further selects informative pairwise constraints with an active learning method. MPCK-means incorporates the metric-learning approach into PCK-means and combined the centroid-based methods and metric-based methods into a unified framework. However, these methods need huge computational cost by enumerating pairwise conditions.  	 	KCL uses deep neural networks to perform pairwise constraint clustering. It firstly trains an extra network for binary similarity classification with a labeled auxiliary dataset. Then, it transfers the prior knowledge of pairwise similarity to the target dataset and uses KL-divergence to evaluate the pairwise distance. MCL uses the meta classification likelihood as the criterion to learn pairwise similarities. However, the domain adaptation methods are still limited in our task. CDAC+ is specifically designed for discovering new intents. It uses limited labeled data as a guide to learn pairwise similarities. However, it is limited in providing specific supervised signals and fails to estimate the number of novel classes. DTC is a method for discovering novel classes in computer vision. It improves the DEC algorithm and transfers the knowledge of labeled data to estimate the number of novel classes. However, the amount of the labeled data has a great influence on its performance. 	 	 		\caption{   Statistics of CLINC and BANKING datasets. \# indicates the total number of sentences. In each run of the experiment, we randomly select 75\  intents as known intents. Taking the CLINC dataset as an example, we randomly select 113 known intents and treat the remaining 37 intents as new intents. }",227
" The U.S.~NIH's precision medicine  initiative calls for designing treatment and preventative interventions considering genetic, clinical, social, behavioral, and environmental exposure variability among patients. The initiative rests on the widely understood finding that considering individual variability is critical in tailoring healthcare interventions to achieve substantial progress in reducing disease burden worldwide. Cancer was chosen as its near term focus with the eventual aim of expanding to other conditions. As the biomedical research enterprise strives to fulfill the initiative's goals, computing needs are also on the rise in drug discovery, predictive modeling for disease onset and progression, and in building NLP tools to curate information from the evidence base being generated.  \subsection{TREC Precision Medicine Series}     \end{table}   In a dovetailing move, the U.S.~NIST's  TREC  has been running a PM track since 2017 with a focus on cancer. The goal of the TREC-PM task is to identify the most relevant biomedical articles and clinical trials for an input patient case. Each case is composed of    a disease name,   a gene name and genetic variation type, and  demographic information . Table shows two example cases from the 2019 track. So the search is ad hoc in the sense that we have a free text input in each facet but  the    facets themselves highlight the PM related attributes that ought to characterize the retrieved documents. We believe this style of faceted retrieval is going to be more common across medical IR tasks for many conditions as the PM initiative continues its mission.   \subsection{Vocabulary Mismatch and Neural IR}  The vocabulary mismatch problem is a prominent issue in medical IR given the large variation in the expression of medical concepts and events. For example, in the query ``What is a potential side effect for Tymlos?'' the drug is referred by its brand name. Relevant scientific literature may contain the generic name Abaloparatide more frequently. Traditional document search engines have clear limitations on resolving   mismatch issues. The IR community has extensively explored methods to address the vocabulary mismatch problem, including query expansion based on relevance feedback, query term re-weighting, or query reconstruction by optimizing the query syntax.  Several recent studies highlight exploiting neural network models for query refinement in document retrieval  settings. \citet{nogueira2017task}  address  this issue by generating a transformed query from the initial query using a neural model.  They use reinforcement  learning  to train it where an agent  learns to reformulate the initial query to maximize the expected return  through actions . In a different approach, \citet{narayan2018ranking}  use RL for sentence ranking for extractive summarization.  \subsection{Our Contributions}  In this paper, building on the BERT architecture, we focus on a different hybrid document scoring and reranking setup involving three components: .~a document relevance classification model, which predicts  whether a document is relevant to the given query ; .~a keyword extraction model which spots tokens in a document that are likely to be seen in PM related queries; and .~an abstractive document summarization model that generates a pseudo-query given the document context and a facet type  via the BERT encoder-decoder setup. The keywords ) and the pseudo-query ) are together compared with the original query to generate a score. The scores from all the components are combined to rerank top   documents returned with a basic Okapi BM25 retriever from a Solr index of the corpora. %This is critical because neural document-query matching and summarization are expensive operations that cannot practically scale to the full corpus.  Our main innovation is in pivoting from the focus on queries by previous methods to emphasis on transforming candidate documents into pseudo-queries via summarization. Additionally, while generating the pseudo-query, we also let the   decoder output concept codes from biomedical terminologies that capture disease and gene names. We do this by embedding both words and concepts in a common semantic space before letting the decoder generate summaries that include concepts. Our overall architecture was evaluated using the TREC-PM datasets  with the 2019 dataset used as the test set. The results show an absolute  improvement in P@10 compared to prior best approaches while obtaining a small  gain in R-Prec. Qualitative analyses also highlight how the summarization is able to focus on document segments that are highly relevant to patient cases.    The basic reranking architecture we begin with is  the Bidirectional Encoder Representations from Transformers  model. BERT is trained on a masked language modeling objective on a large text corpus such as Wikipedia and BooksCorpus. As a sequence modeling method, it has achieved state-of-the-art results in a wide range of natural language understanding  tasks, including machine translation and text summarization.  With an additional layer on top of a pretrained BERT model, we can fine-tune models for specific NLU tasks. In our study, we utilize this framework in all three components identified in Section by starting with a \texttt{bert-base-uncased} pretrained   HuggingFace model.   We plan to leverage both extractive and abstractive candidate document summarization in our framework. In terms of learning methodology, we view extractive summarization as a sentence  classification problem. Previously proposed models include the RNN-based sequence model, the attention-based neural encoder-decoder model, and the sequence model with a global learning objective  for ranking sentences optimized via RL. More recently,  graph convolutional neural networks  have also been adapted to allow the incorporation of global information in text summarization tasks. Abstractive summarization is typically cast as a sequence-to-sequence learning problem. The encoder of the framework reads a document and yields a sequence of continuous representations, and the decoder generates the target summary token-by-token. Both approaches have their own merits in generating comprehensive and novel summaries; hence most systems leverage these two different models in one framework. We use the extractive component to identify tokens in a candidate document that may be relevant from a PM perspective and use the abstractive component to identify potential terms that may not necessarily be in the document but nevertheless characterize it for PM purposes.   Most of the neural text summarization models, as described in the previous section, adopt the encoder-decoder framework that is popular in machine translation. As such the vocabulary on the decoding side does not have to be the same as that on the encoding side. We exploit this to design a summarization trick for PM where the decoder outputs both regular English tokens and also entity codes from a standardized biomedical terminology that captures semantic concepts discussed in the document. This can be trained easily by converting the textual queries in the training examples to their corresponding entity codes. This trick is to enhance our ability to handle vocabulary mismatch in a different way .  We created BioMedical Entity Tagged  embeddings\footnote{} for this purpose. BMET embeddings are trained on biomedical literature abstracts that were annotated with entity codes in the  Medical Subject Headings  terminology\footnote{}; codes are appended to the associated textual spans in the training examples. So regular tokens and the entity codes are thus embedded in the same semantic space via pretraining with the fastText architecture. Besides regular English tokens, the vocabulary of BMET thus includes 29,351 MeSH codes and a subset of supplementary concepts.  In the dictionary, MeSH codes are differentiated from the regular words by a unique prefix; for example, mesh\_d000123 for MeSH code D000123. With this, our summarization model can now translate a sequence of regular text tokens into a sequence of biomedical entity codes or vice versa. That is, we use MeSH as a new ``semantic'' facet besides those already provided by TREC-PM organizers. The expected output for the MeSH facet is the set of codes that capture entities in the disease and gene variation facets.    	In this work, we have introduced an effective method for discovering new intents. Our method successfully transfers the prior knowledge of limited known intents and estimates the number of intents by eliminating low-confidence clusters. Moreover, it provides more stable and concrete supervised signals to guide the clustering process. We conduct extensive experiments on two challenging benchmark datasets to evaluate the performance. Our method achieves significant improvements over the compared methods and obtains more accurate estimated cluster numbers with limited prior knowledge. In the future, we will try different clustering methods to produce supervised signals and explore more self-supervised methods for representation learning. 	 	 	
"," The basic reranking architecture we begin with is  the Bidirectional Encoder Representations from Transformers  model. BERT is trained on a masked language modeling objective on a large text corpus such as Wikipedia and BooksCorpus. As a sequence modeling method, it has achieved state-of-the-art results in a wide range of natural language understanding  tasks, including machine translation and text summarization.  With an additional layer on top of a pretrained BERT model, we can fine-tune models for specific NLU tasks. In our study, we utilize this framework in all three components identified in Section by starting with a \texttt{bert-base-uncased} pretrained   HuggingFace model.   We plan to leverage both extractive and abstractive candidate document summarization in our framework. In terms of learning methodology, we view extractive summarization as a sentence  classification problem. Previously proposed models include the RNN-based sequence model, the attention-based neural encoder-decoder model, and the sequence model with a global learning objective  for ranking sentences optimized via RL. More recently,  graph convolutional neural networks  have also been adapted to allow the incorporation of global information in text summarization tasks. Abstractive summarization is typically cast as a sequence-to-sequence learning problem. The encoder of the framework reads a document and yields a sequence of continuous representations, and the decoder generates the target summary token-by-token. Both approaches have their own merits in generating comprehensive and novel summaries; hence most systems leverage these two different models in one framework. We use the extractive component to identify tokens in a candidate document that may be relevant from a PM perspective and use the abstractive component to identify potential terms that may not necessarily be in the document but nevertheless characterize it for PM purposes.   Most of the neural text summarization models, as described in the previous section, adopt the encoder-decoder framework that is popular in machine translation. As such the vocabulary on the decoding side does not have to be the same as that on the encoding side. We exploit this to design a summarization trick for PM where the decoder outputs both regular English tokens and also entity codes from a standardized biomedical terminology that captures semantic concepts discussed in the document. This can be trained easily by converting the textual queries in the training examples to their corresponding entity codes. This trick is to enhance our ability to handle vocabulary mismatch in a different way .  We created BioMedical Entity Tagged  embeddings\footnote{} for this purpose. BMET embeddings are trained on biomedical literature abstracts that were annotated with entity codes in the  Medical Subject Headings  terminology\footnote{}; codes are appended to the associated textual spans in the training examples. So regular tokens and the entity codes are thus embedded in the same semantic space via pretraining with the fastText architecture. Besides regular English tokens, the vocabulary of BMET thus includes 29,351 MeSH codes and a subset of supplementary concepts.  In the dictionary, MeSH codes are differentiated from the regular words by a unique prefix; for example, mesh\_d000123 for MeSH code D000123. With this, our summarization model can now translate a sequence of regular text tokens into a sequence of biomedical entity codes or vice versa. That is, we use MeSH as a new ``semantic'' facet besides those already provided by TREC-PM organizers. The expected output for the MeSH facet is the set of codes that capture entities in the disease and gene variation facets.",228
" %Discourse Parsing is a key NLP %an important task, aiming to establish a better understanding of multi-sentential natural language. %, which is inherently ambiguous and intent-driven.  %Most research in the area thereby focuses on one of the two main discourse theories RST  or PDTB , both proposed over a decade ago. Discourse Parsing is a key Natural Language Processing  task for processing multi-sentential text. Most research in the area focuses on one of the two main discourse theories -- RST  or PDTB . The latter thereby postulates shallow discourse structures, combining adjacent sentences and mainly focuses on explicit and implicit discourse connectives. The RST discourse theory, on the other hand, proposes discourse trees over complete documents in a constituency-style manner, with tree leaves as so called Elementary Discourse Units , representing span-like sentence fragments. Internal tree-nodes encode discourse relations between sub-trees as a tuple of \{Nuclearity, Relation\}, where the nuclearity defines the sub-tree salience in the local context, and the relation further specifies the type of relationship between the binary child nodes   with automatically inferred discourse structures and nuclearity attributes from large-scale sentiment datasets already reached state-of-the-art  performance on the inter-domain discourse parsing task. Similarly, \citet{liu2018learning} infer latent discourse trees from the text classification task, and \citet{liu2019single} employ the downstream task of summarization using a transformer model to generate discourse trees. Outside the area of discourse parsing, syntactic trees have previously been inferred according to several strategies, e.g. \citet{socher2011semi, yogatama2016learning, choi2018learning, maillard2019jointly}. %including: Discrete decisions frameworks using a Gumbel-softmax component , applying a reinforcement approach to syntactic parsing , using the reconstruction error of adjacent spans as an indicator for syntactic coherence within a sentence  or by employing a CKY approach to select syntactic trees from a soft model .  In general, the approaches mentioned above  %to automatically annotate text with discourse structures or syntactic trees  have shown to capture valuable structural information. Some models outperform baselines trained on human-annotated datasets , others have proven to enhance diverse downstream tasks . However, despite these initial successes, one critical limitation that all aforementioned models share is the task-specificity, possibly only capturing downstream-task related information. %of discourse,  This potentially compromises the generality of the resulting trees, as for instance shown for the model using text classification data  in \citet{ferracane2019evaluating}.  %For instance, the approach by \citet{huber2019predicting} uses document-level sentiment information to inform the discourse tree generation, with others %have been  %using summarization data  or sentence-level sentiment cues  to achieve the results.  In order to alleviate this limitation of task-specificity, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending the latent tree induction framework proposed by \citet{choi2018learning} with an auto-encoding objective. %.  Our system thereby extracts important knowledge from natural text by optimizing both the underlying tree structures and the distributed representations. We believe that the resulting discourse structures effectively aggregate related and commonly appearing patterns in the data by merging coherent text spans into intermediate sub-tree encodings, similar to the intuition presented in \citet{drozdov2019unsupervised}. However, in contrast to the approach by \citet{drozdov2019unsupervised}, our model makes discrete structural decisions, rather than joining possible subtrees using a soft attention mechanism. We believe that our discrete tree structures allow the model to more efficiently achieve the autoencoder objective in reconstructing the inputs, directly learning how written language can be aggregated in the wild . In general, the proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and further problems outside of NLP, like tree-planning  and decision-tree generation . Yet, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to %complement task-specific models in  generate much larger and more diverse discourse treebanks.       Our paper is located at the intersection of autoencoders, unsupervised tree-style inference models, and the area of discourse parsing.  There is a large number of diverse autoencoders, such as variational autoencoders , sparse autoencoders  and de-noising autoencoders .  Within the last decade, general autoencoder frameworks have been frequently used to compress data, such as in \citet{srivastava2015unsupervised}. More recently, sequential autoencoders have been applied in the area of NLP , with many popular approaches, such as sequence-to-sequence learning models  having strong ties to sequential autoencoders.  Based on the promising results of the sequential autoencoder, researchers started to compress and reconstruct more general structures in tree-style models, such as \citet{chen2018tree} showing that with available gold-standard trees, the programming-language translation task  can be learned with a tree-to-tree style neural autoencoder network. Furthermore, variational autoencoders have been shown effective for the difficult task of grammar induction .   While both previously mentioned applications for tree-style autoencoder models require readily available tree structures to guide the aggregation process, another line of work by \citet{socher2011semi} overcomes this requirement by using the reconstruction error of an autoencoder applied to every two adjacent text spans as an indicator for syntactic correctness within a sentence. In their model, \citet{socher2011semi} combine the tree-inference objective with the autoencoder topology, training an unsupervised tree-structured model, which is subsequently fine-tuned on a small-scale supervised dataset. While their model is clearly comparable to our approach, there are three major differences:  They make sequential, local decisions on the aggregation of spans to generate a tree structure, rather than optimizing the complete process holistically.  Their model uses an unsupervised objective in the initial step but requires supervision in later stages and  The model has been only applied to syntactic parsing. In contrast, we apply our model to discourse parsing, which arguably introduces further difficulties, as we will discuss in section .   Recently, \citet{choi2018learning} showed a promising approach to infer tree structures in a holistic and parallelizable manner, generating task-depended trees solely relying on sentiment-related information. In their model, they make use of the Gumbel-Softmax   , allowing the neural network to make discrete decisions while still being able to use standard approaches like back-propagation to optimize the model. By combining a similar objective to \citet{socher2011semi} and \citet{chen2018tree}, we utilize the discrete decision-process in \citet{choi2018learning}, positioning our work at the intersection of these two lines of research.  The general task of tree inference has been mostly explored on sentence-level. For instance in \citet{choi2018learning} and \citet{socher2011semi} as described above, or by applying a reinforcement approach  or CKY methodology  to syntactic parsing. Our work employs a novel, fully differentiable approach to a similar problem in the area of discourse parsing.   In discourse parsing itself, there have been multiple attempts to overcome the aforementioned limitations of small-scale human annotated datasets. However, all previous models  use downstream tasks to infer discourse structures. While this is a valid strategy, shown to achieve SOTA results on the inter-domain discourse parsing task , as well as performance gains on downstream tasks , those discourse structures are likely task-depended and need to be either combined across multiple downstream tasks  to avoid task-specific structures  or can only be applied in similar domains.  While  Further work has been trying to infer RST-style discourse structures in a linguistically supervised manner  with the method proposed by  , showing  very promising results, reaching similar or even superior good performance when heavily exploiting syntactic markers in combination with general linguistic priors. Yet, the approach appears to be very specific to the data at hand -- News articles from the Wall Street Journal -- raising questions in regards to overfitting.  to such data.  compared to supervised approaches.   Yet,  \citet{nishida2020unsupervised} achieve this by heavily exploiting syntactic structures within and between EDUs, which appear to be specific to the data at hand -- News articles from the Wall Street Journal -- therefore arguably overfitting to such data.  . While this approach shows clear evidence for the benefits of syntactic structures in discourse parsing, the results are heavily skewed towards the available data at hand -- News articles from the Wall Street Journal.   In this work, we explore a purely unsupervised approach: instead of relying on domain specific syntactic features, we infer general discourse trees   , applicable to any domain,  by exploiting inherently available information from natural data , making our model similar to approaches in language modelling . More specifically, our proposal extends the previously proposed Gumbel-TreeLSTM method  by substituting the original downstream-task related objective with an autoencoder-style reconstruction.       We present an end-to-end model to resolve ambiguous questions in dialogue by clarifying them using label suggestions. We cast the question clarification problem as a collection partition problem. In order to improve the quality of the interactive labels as well as reduce the semantic overlap of the labels and the user's question, we propose a novel reward based on recall of potential intents and information gain. We establish its effectiveness in a series of experiments, which suggest that this novel notion of clarification may as well be adopted for other kinds of disambiguation problems.  Our experiments shows that the way of intent interaction is more effective in solving user problems than returning relevant results. At the same time, through the comparison of online ctr, it fully proves that the intents recommend by the policy model trained via our new reward is more helpful to users.         
","  Our paper is located at the intersection of autoencoders, unsupervised tree-style inference models, and the area of discourse parsing.  There is a large number of diverse autoencoders, such as variational autoencoders , sparse autoencoders  and de-noising autoencoders .  Within the last decade, general autoencoder frameworks have been frequently used to compress data, such as in \citet{srivastava2015unsupervised}. More recently, sequential autoencoders have been applied in the area of NLP , with many popular approaches, such as sequence-to-sequence learning models  having strong ties to sequential autoencoders.  Based on the promising results of the sequential autoencoder, researchers started to compress and reconstruct more general structures in tree-style models, such as \citet{chen2018tree} showing that with available gold-standard trees, the programming-language translation task  can be learned with a tree-to-tree style neural autoencoder network. Furthermore, variational autoencoders have been shown effective for the difficult task of grammar induction .   While both previously mentioned applications for tree-style autoencoder models require readily available tree structures to guide the aggregation process, another line of work by \citet{socher2011semi} overcomes this requirement by using the reconstruction error of an autoencoder applied to every two adjacent text spans as an indicator for syntactic correctness within a sentence. In their model, \citet{socher2011semi} combine the tree-inference objective with the autoencoder topology, training an unsupervised tree-structured model, which is subsequently fine-tuned on a small-scale supervised dataset. While their model is clearly comparable to our approach, there are three major differences:  They make sequential, local decisions on the aggregation of spans to generate a tree structure, rather than optimizing the complete process holistically.  Their model uses an unsupervised objective in the initial step but requires supervision in later stages and  The model has been only applied to syntactic parsing. In contrast, we apply our model to discourse parsing, which arguably introduces further difficulties, as we will discuss in section .   Recently, \citet{choi2018learning} showed a promising approach to infer tree structures in a holistic and parallelizable manner, generating task-depended trees solely relying on sentiment-related information. In their model, they make use of the Gumbel-Softmax   , allowing the neural network to make discrete decisions while still being able to use standard approaches like back-propagation to optimize the model. By combining a similar objective to \citet{socher2011semi} and \citet{chen2018tree}, we utilize the discrete decision-process in \citet{choi2018learning}, positioning our work at the intersection of these two lines of research.  The general task of tree inference has been mostly explored on sentence-level. For instance in \citet{choi2018learning} and \citet{socher2011semi} as described above, or by applying a reinforcement approach  or CKY methodology  to syntactic parsing. Our work employs a novel, fully differentiable approach to a similar problem in the area of discourse parsing.   In discourse parsing itself, there have been multiple attempts to overcome the aforementioned limitations of small-scale human annotated datasets. However, all previous models  use downstream tasks to infer discourse structures. While this is a valid strategy, shown to achieve SOTA results on the inter-domain discourse parsing task , as well as performance gains on downstream tasks , those discourse structures are likely task-depended and need to be either combined across multiple downstream tasks  to avoid task-specific structures  or can only be applied in similar domains.  While  Further work has been trying to infer RST-style discourse structures in a linguistically supervised manner  with the method proposed by  , showing  very promising results, reaching similar or even superior good performance when heavily exploiting syntactic markers in combination with general linguistic priors. Yet, the approach appears to be very specific to the data at hand -- News articles from the Wall Street Journal -- raising questions in regards to overfitting.  to such data.  compared to supervised approaches.   Yet,  \citet{nishida2020unsupervised} achieve this by heavily exploiting syntactic structures within and between EDUs, which appear to be specific to the data at hand -- News articles from the Wall Street Journal -- therefore arguably overfitting to such data.  . While this approach shows clear evidence for the benefits of syntactic structures in discourse parsing, the results are heavily skewed towards the available data at hand -- News articles from the Wall Street Journal.   In this work, we explore a purely unsupervised approach: instead of relying on domain specific syntactic features, we infer general discourse trees   , applicable to any domain,  by exploiting inherently available information from natural data , making our model similar to approaches in language modelling . More specifically, our proposal extends the previously proposed Gumbel-TreeLSTM method  by substituting the original downstream-task related objective with an autoencoder-style reconstruction.",229
"  Named Entity Recognition  is the task of identifying the span and the class of a Named Entity  in unstructured text. NEs typically include but are not limited to persons, companies, dates, and geographical locations .   Legal NER is a central task in language processing of legal documents, especially for extracting key information such as the name of the parties in a case, the court name or the case number, or references to laws or judgements, to name a few. The extracted NEs could be integrated in legal research workflows for functionalities such as search, document anonymization or case summarization  thereby enabling and expediting insights for legal professionals .  NER is commonly formalized as a sequence labeling task: each token of the document is assigned a single label that indicates whether the token belongs to an entity from a predefined set of categories . To create a training dataset in such a format the annotator is required to manually label each token in a sentence with the respective category. In this format, both the NE and the location of the NE in the source text are known. This format of training data is what we refer to hereafter as 閳ユ笀old standard閳 data. Obtaining the required voluminous gold standard data to train such models is, therefore, a laborious and costly task.    In this paper, we perform NER in filed lawsuits in US courts. Specifically, we aim to identify the party names in each case, i.e. the names of the plaintiffs and the defendants, in a large collection of publicly available cases from more than 200 courts in different US jurisdictions. The party names have been identified by legal annotators but their exact location in the text is unknown. In this respect, we do not have access to 閳ユ笀old standard閳 training data even though the target NEs are available. This feature of our dataset introduces a key difference of our task to most NER tasks.  One solution to this problem is to generate the 閳ユ笀old standard閳 training data by searching for the locations of the known NEs in the source text . By performing this additional transformation to our data, we would be able to train sequence labeling NER models. For the following reasons, this solution is nontrivial. First, as our source text is also extracted from scanned PDF files , it contains Optical Character Recognition  mistakes and/or typos which may not be present in the target NEs. Second, besides the potential OCR errors at the character level, the closely spaced, two-column page layouts that can be often found as headers in the filed cases, represent an additional challenge for the OCR, which tends to concatenate the text across columns . In such cases, the tokens that make up the NEs in the source text may be intertwined with other words and/or sentences. Third, variations of the names may be also present in the source text and in our human-generated labels, such as presence of first and/or middle names whole or as initials and, to a lesser extent, typos.        To address some of the challenges imposed by the format of our training data and inspired by the work in the field of abstractive summarization, we propose to reformulate the NER task, not as a sequence labeling problem, but as a text-to-text sequence generation problem with the use of a pointer generator network . With this reformulation, in contrast to sequence labeling, we do not require knowledge of the NE閳ユ獨 locations in the text as training labels. A recent study by \citet{Li2020} proposed a different formulation of the NER task as a question answering task and achieved state-of-the-art performance in a number of published NER datasets . In this study, we adopt a hybrid extractive-abstractive architecture, based on recurrent neural networks coupled with global  attention and copying  attention  mechanisms . The proposed architecture can be successfully used for abstractive summarization since it can copy words from the source text via pointing and can deal effectively with out-of-vocabulary  words 閳 words that have not been seen during training. Our approach is conceptually simple but empirically powerful and we show that the pointer generator outperforms the typical NER architectures in the case of noisy and lengthy inputs where the NE's location in the text is not known.   In addition, we examine how our approach can be used for the related NER task of case number extraction. The case number is a unique combination of letters, numbers and special characters as a single token and are, therefore, particularly challenging for NER models as they are often dealt with as OOV words by the model. As in the party names task discussed above, in the case number task we do not have 閳ユ笀old standard閳 labels of the case number閳ユ獨 location in the text. We show that a character level sequence generation network can dramatically increase our ability to extract case numbers from the source text, compared to a word level sequence generation network.  The rest of the paper is organized as follows. In Section 2, we discuss related work in the field of NER in the legal domain. In Section 3, we describe our proposal of NER as a text-to-text sequence generation task in the absence of gold standard data and formulate the task in two ways:  as a combination of automatically labeling the NE's location and then using the conventional sequence labeling method for NER, and  as a text-to-text sequence generation task where the NEs are directly generated as text. Section 4 presents our experimental design, results and analysis. Section 5 presents the case number case study. Finally, we conclude and discuss directions for future work.      There is a long history of research in the NER field ranging from statistical models such as Maximum Entropy Models , Hidden Markov Models  or Conditional Random Fields  , to the current state-of-the-art deep neural network approaches based on bidirectional recurrent neural network  architectures often combined with a final CRF layer  as well as Transformers .   The vast majority of the developed NER approaches have been trained and evaluated on English texts from the general or news domains . This makes them less efficient for legal documents given the intricacies of the legal language, such as long complex sentences and domain specific vocabulary. For example, embeddings trained on general web crawling derived corpora might encounter many OOV words when used for the analysis of legal texts .   Furthermore, most NER approaches operate at the sentence level which means that NER in long documents rely on effective sentence tokenizers. However, sentence boundary detection is challenging for legal text because of the variety of punctuation and syntax  and the presence of idiosyncratic text formatting  . This problem becomes especially exacerbated in the case of noisy, unstructured texts such as the ones produced by OCR systems where unavoidably mistakes create cascading effects to the downstream processing of the text . It is also worth noting that sentence level analysis prevents the model from benefiting from global information  . Figure 1 shows the first page of a representative complaint found in our dataset. The original PDF is displayed along with the extracted source text obtained with OCR. Here the name of the plaintiff is intertwined with the case number 閳ユ竷ase no閳 as a result of the OCR extraction of the double column format. Similarly, the words that make up the defendant name are interrupted by the token 閳14閳. Finally, the plaintiff name appears in multiple locations in the text.    In the recent years, sequence-to-sequence neural networks have achieved great success in a number of natural language processing tasks, including but not limited to Machine Translation  and Text Summarization . The backbone of these models is an encoder-decoder model , often coupled with an attention mechanism . The attention mechanism enables the decoder to use a dynamically changing context, in the form of a context vector, in addition to the encoded state. Another mechanism, that has been particularly successful in the field of extractive summarization, is the pointing mechanism . The pointing mechanism provides the model with a choice between generating tokens from the target vocabulary or copying where the decoder 閳ユ竷opies閳 tokens directly from the source sequence. The pointing mechanism allows the model to be particularly effective when dealing with OOV words, as the decoder can copy words from the source sequence that are not in the vocabulary . The pointer generator type network, and similar architectures such as CopyNet , combine the standard sequence to sequence model with a pointing ability.   To the best of our knowledge, a pointer generator sequence to sequence model has not been applied to the NER task before.        \iffalse For papers accepted to the main conference, we will invite authors to provide a translation  of the title and abstract and a 1-2 page synopsis of the paper in a second  language of the authors' choice. Appropriate languages include but are not  limited to authors' native languages, languages spoken in the authors' place  of affiliation, and languages that are the focus of the research presented. \fi  
","   There is a long history of research in the NER field ranging from statistical models such as Maximum Entropy Models , Hidden Markov Models  or Conditional Random Fields  , to the current state-of-the-art deep neural network approaches based on bidirectional recurrent neural network  architectures often combined with a final CRF layer  as well as Transformers .   The vast majority of the developed NER approaches have been trained and evaluated on English texts from the general or news domains . This makes them less efficient for legal documents given the intricacies of the legal language, such as long complex sentences and domain specific vocabulary. For example, embeddings trained on general web crawling derived corpora might encounter many OOV words when used for the analysis of legal texts .   Furthermore, most NER approaches operate at the sentence level which means that NER in long documents rely on effective sentence tokenizers. However, sentence boundary detection is challenging for legal text because of the variety of punctuation and syntax  and the presence of idiosyncratic text formatting  . This problem becomes especially exacerbated in the case of noisy, unstructured texts such as the ones produced by OCR systems where unavoidably mistakes create cascading effects to the downstream processing of the text . It is also worth noting that sentence level analysis prevents the model from benefiting from global information  . Figure 1 shows the first page of a representative complaint found in our dataset. The original PDF is displayed along with the extracted source text obtained with OCR. Here the name of the plaintiff is intertwined with the case number 闁炽儲绔穉se no闁 as a result of the OCR extraction of the double column format. Similarly, the words that make up the defendant name are interrupted by the token 闁14闁. Finally, the plaintiff name appears in multiple locations in the text.    In the recent years, sequence-to-sequence neural networks have achieved great success in a number of natural language processing tasks, including but not limited to Machine Translation  and Text Summarization . The backbone of these models is an encoder-decoder model , often coupled with an attention mechanism . The attention mechanism enables the decoder to use a dynamically changing context, in the form of a context vector, in addition to the encoded state. Another mechanism, that has been particularly successful in the field of extractive summarization, is the pointing mechanism . The pointing mechanism provides the model with a choice between generating tokens from the target vocabulary or copying where the decoder 闁炽儲绔穙pies闁 tokens directly from the source sequence. The pointing mechanism allows the model to be particularly effective when dealing with OOV words, as the decoder can copy words from the source sequence that are not in the vocabulary . The pointer generator type network, and similar architectures such as CopyNet , combine the standard sequence to sequence model with a pointing ability.   To the best of our knowledge, a pointer generator sequence to sequence model has not been applied to the NER task before.",230
" Query reformulation and paraphrase generation techniques are employed for a variety of purposes in natural language processing , such as dialogue generation , machine translation , and especially in question answering  systems . Generating coherent and clean texts can reduce potential errors in downstream systems. In the cases when users are at the receiving end of NLP pipelines, it is essential to show them fluent and human-like languages before they lose faith and recede into requiring human agents for the sake of better understanding and communication. In search or question answering systems, query reformulation aims to paraphrase or restructure original question sequences, transforming them into ones that are more interpretable with natural well-formedness in both grammar and semantics. Typically, users may not have the patience to input an entirely grammatical or coherent question, which can cause issues for the downstream components to understand and give accurate predictions or answers. When human representatives are present, an originally noisy query or question can be reiterated and rephrased to double-check with users what they are asking for. This is a costly operation if every convoluted question needs to be restated. By having an NLP model to reformulate input queries, reformulations are fed back to users to confirm their original intentions in an automated way. As a result, unnecessary errors are eliminated and noises are prevented from propagating in an NLP pipeline, which can contain a series of models such as intent classification, information retrieval and question answering.  Traditionally, rule-based and statistical methods have been studied for paraphrase and reformulation generation . The advent of sequence-to-sequence learning   made it feasible to train deep neural networks as a new paradigm. We investigate how to paraphrase and denoise queries and generate well-formed reformulations using Seq2Seq learning models such as LSTMs  and transformers . Following the framework from AQA , a Seq2Seq model is pre-trained on supervised tasks and further tuned using reinforcement learning  on a machine comprehension QA dataset SearchQA , learning from a pre-trained BiDAF  QA system that generates rewards. SearchQA is a suitable and challenging dataset as queries contain noisy phrases and the associated contexts are concatenated web text snippets from Google's search engine. Our goal is to obtain a model that can generate better-formed reformulations based on the original query sequences and achieve good QA performance with these reformulations. We use transfer learning  from pre-trained transformers with text-to-text task formulations . In our approach, pre-trained T5 models are first fine-tuned on paraphrase generation  and denoising  datasets to gain general paraphrasing capabilities. Then, reinforcement learning of downstream QA rewards is performed to further encouraged the model to produce task-specific reformulations. To our knowledge, this is a first attempt to fine-tune text-to-text transformers with RL, nudging the model to generate reward-acquiring query trajectories to get better answers. We show that fine-tuned text-to-text transformers are better starting points for RL as they are more sample efficient in achieving the same level of QA performance, acquiring rewards faster than the previous AQA approach that uses translation-based LSTMs. T5 models also generate reformulations with better readability and can generalize to out-of-sample data. We provide a new way to evaluate fluency on a sequence level using an trained metric on the well-formedness   dataset, which is based on real evaluations from humans, a more reliable source than widely-used algorithmic metrics based on overlapping n-grams.     Our work is related to the task of paraphrasing. This is to restate a given sequence while preserving the same meaning. To use pre-trained language model's representation capabilities to generate paraphrases of sequences, \citet{Witteveen_2019} focus on fine-tuning large language models with supervised datasets. USE , ROUGE-L  and BLEU  are measured to determine the best paraphrase. The models demonstrate the ability to generate paraphrases for out-of-sample sentences and paragraphs. Another common approach for paraphrasing is to leverage machine translation, \citet{Guo2019ZeroShotPG}'s work uses multilingual translation and pivoting for zero-shot paraphrases. Expensive human workers are employed in this process to evaluate fluency. \citet{Roy2019UnsupervisedPW} uses a VQ-VAE  to compare a monolingual paraphrasing method with unsupervised and supervised translation-based approaches. Other generative approaches have also been explored for paraphrasing . Metrics based on n-gram overlaps are used in most of these works, even though it has been shown that BLEU or ROUGE do not agree well with human judgements . These N-gram based metrics require reference gold sentences, which are not available in the paraphrasing or reformulation task since there are many ways to reformulate the same query. Our evaluation of the reformulation qualities do not rely on gold references or any related algorithmic metrics. Fluency scores are generated by a T5-base model fine-tuned on the QW dataset containing human judgements.   Our reinforcement learning framework is closely related to \citet{Buck2018AskTR}'s AQA approach by leveraging policy-based RL methods to generate question reformulations. These reformulations are treated as inputs to a BiDAF  question answering system that generates token-level F1 rewards. However, their GNMT  reformulation model depends on complex pre-training on multilingual translations and paraphrasing procedures that are not reproducible from its open-sourced project. Our approach leverages recent advances in Seq2Seq learning and transfer learning, enabling direct fine-tuning of a pre-trained text-to-text transformer with paraphrasing and denoising tasks. This gives flexibility in what starting point we can use before we enter the RL stage. The general linguistic and reformulation knowledge encoded in the model helps retain sequence-level fluency and well-formedness before and after RL training.  We leverage Text-to-text Transfer Transformers   as the foundation of our reformulation and well-formedness models. As a systematic ablation study on transfer learning in NLP, this work compares and contrasts different transfer learning schemes extensively. The resultant best-performing model is T5, which has an encoder-decoder architecture unlike single-stacked BERT  and its descendants . The largest T5 model can achieve state-of-the-art results on many NLP benchmarks including SuperGLUE , where it reaches near human-level performance. T5 formulates any text-based NLP task in an unified text-to-text format, a natural fit for generative tasks like query reformulation. Fine-tuning task descriptions can be directly specified as a prefix to the input. This provides flexibility of fine-tuning on different tasks without having to change the training pipeline. We leverage the general linguistic knowledge of the English language implicitly encoded in the parameters of the transformer-based T5 model pre-trained on unlabeled text. We also train T5 further with policy-based RL after supervised fine-tuning. To our knowledge, using RL to tune a T5 model has not been attempted to our knowledge.  showed T5's superior performance on reformulating questions within a conversational history. Their model uses both the question and the context as input, whereas in our approach, only the original noisy query are used as input to the reward-generating black-box QA model at the RL stage. Identifying well-formed questions  by training binary classification models have been studied using BERT and transfer learning with pre-trained models . Instead, we investigate more fine-grained 6-way classification using a fined-tuned T5 well-formedness model, leveraged as a proxy for evaluating sequence-level fluency of reformulators.  There has been a body of work in other domains to adopt RL for structured sequential prediction tasks. \citet{Keneshloo2020DeepRL} survey various reinforcement learning techniques for training sequence-to-sequence models. In the context of abstractive summarization \citet{Keneshloo2019DeepTR}, they choose to use self-critical training , which we leverage as an alternative method for RL. \citet{Stiennon2020LearningTS} geneerate summaries with proximal policy optimization  as the reinforcement learning algorithm with an KL-regularized reward signal. \citet{Angermller2020ModelbasedRL} studies biological sequence generation with RL and points out the difficulty of value-based methods like DQN  in the setting where rewards are episodic and delayed at the end of sequence generation, which is a similar setup to our task.     We introduce \method toolkit for easily building and training end-to-end speech translation models. We provide straightforward recipes for audio data pre-processing, training, and inference, which we believe is friendly with NLP researchers. Moreover, we report strong and reproducible benchmarks, which can be regarded as the reliable baselines for further research.  This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended. \pdfoutput=1   In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.  \documentclass[11pt]{article}    Remove the ""review"" option to generate the final version.   \usepackage[review]{naacl2021} \usepackage[]{naacl2021}    Standard package includes \usepackage{times} \usepackage{latexsym}    For proper rendering and hyphenation of words containing Latin characters  \usepackage[T1]{fontenc}   For Vietnamese characters   \usepackage[T5]{fontenc}   See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets    This assumes your files are encoded as UTF8 \usepackage[utf8]{inputenc}    This is not strictly necessary, and may be commented out,   but it will improve the layout of the manuscript,   and will typically save some space. \usepackage{microtype} \usepackage{multirow}    If the title and author information does not fit in the area allocated, uncomment the following    \setlength\titlebox{<dim>}     and set <dim> to something 5cm or larger.  \newcommand{\method}{NeurST\space} \newcommand{\red}[1]{{\color{red} #1}}  \title{NeurST: Neural Speech Translation Toolkit}    Author information can be set in various styles:   For several authors from the same institution:   \author{Author 1 \and ... \and Author n \\           Address line \\ ... \\ Address line}   if the names do not fit well on one line use           Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\   For authors from different institutions:   \author{Author 1 \\ Address line \\  ... \\ Address line           \And  ... \And           Author n \\ Address line \\ ... \\ Address line}   To start a seperate ``row'' of authors use \AND, as in   \author{Author 1 \\ Address line \\  ... \\ Address line           \AND           Author 2 \\ Address line \\ ... \\ Address line \And           Author 3 \\ Address line \\ ... \\ Address line}  \author{Chengqi Zhao, Mingxuan Wang \and Lei Li \\   ByteDance Inc. \\   \texttt{\{zhaochengqi.d, wangmingxuan.89, lileilab\}@bytedance.com} \\}  \begin{document} \maketitle               Entries for the entire Anthology, followed by custom entries  
"," Our work is related to the task of paraphrasing. This is to restate a given sequence while preserving the same meaning. To use pre-trained language model's representation capabilities to generate paraphrases of sequences, \citet{Witteveen_2019} focus on fine-tuning large language models with supervised datasets. USE , ROUGE-L  and BLEU  are measured to determine the best paraphrase. The models demonstrate the ability to generate paraphrases for out-of-sample sentences and paragraphs. Another common approach for paraphrasing is to leverage machine translation, \citet{Guo2019ZeroShotPG}'s work uses multilingual translation and pivoting for zero-shot paraphrases. Expensive human workers are employed in this process to evaluate fluency. \citet{Roy2019UnsupervisedPW} uses a VQ-VAE  to compare a monolingual paraphrasing method with unsupervised and supervised translation-based approaches. Other generative approaches have also been explored for paraphrasing . Metrics based on n-gram overlaps are used in most of these works, even though it has been shown that BLEU or ROUGE do not agree well with human judgements . These N-gram based metrics require reference gold sentences, which are not available in the paraphrasing or reformulation task since there are many ways to reformulate the same query. Our evaluation of the reformulation qualities do not rely on gold references or any related algorithmic metrics. Fluency scores are generated by a T5-base model fine-tuned on the QW dataset containing human judgements.   Our reinforcement learning framework is closely related to \citet{Buck2018AskTR}'s AQA approach by leveraging policy-based RL methods to generate question reformulations. These reformulations are treated as inputs to a BiDAF  question answering system that generates token-level F1 rewards. However, their GNMT  reformulation model depends on complex pre-training on multilingual translations and paraphrasing procedures that are not reproducible from its open-sourced project. Our approach leverages recent advances in Seq2Seq learning and transfer learning, enabling direct fine-tuning of a pre-trained text-to-text transformer with paraphrasing and denoising tasks. This gives flexibility in what starting point we can use before we enter the RL stage. The general linguistic and reformulation knowledge encoded in the model helps retain sequence-level fluency and well-formedness before and after RL training.  We leverage Text-to-text Transfer Transformers   as the foundation of our reformulation and well-formedness models. As a systematic ablation study on transfer learning in NLP, this work compares and contrasts different transfer learning schemes extensively. The resultant best-performing model is T5, which has an encoder-decoder architecture unlike single-stacked BERT  and its descendants . The largest T5 model can achieve state-of-the-art results on many NLP benchmarks including SuperGLUE , where it reaches near human-level performance. T5 formulates any text-based NLP task in an unified text-to-text format, a natural fit for generative tasks like query reformulation. Fine-tuning task descriptions can be directly specified as a prefix to the input. This provides flexibility of fine-tuning on different tasks without having to change the training pipeline. We leverage the general linguistic knowledge of the English language implicitly encoded in the parameters of the transformer-based T5 model pre-trained on unlabeled text. We also train T5 further with policy-based RL after supervised fine-tuning. To our knowledge, using RL to tune a T5 model has not been attempted to our knowledge.  showed T5's superior performance on reformulating questions within a conversational history. Their model uses both the question and the context as input, whereas in our approach, only the original noisy query are used as input to the reward-generating black-box QA model at the RL stage. Identifying well-formed questions  by training binary classification models have been studied using BERT and transfer learning with pre-trained models . Instead, we investigate more fine-grained 6-way classification using a fined-tuned T5 well-formedness model, leveraged as a proxy for evaluating sequence-level fluency of reformulators.  There has been a body of work in other domains to adopt RL for structured sequential prediction tasks. \citet{Keneshloo2020DeepRL} survey various reinforcement learning techniques for training sequence-to-sequence models. In the context of abstractive summarization \citet{Keneshloo2019DeepTR}, they choose to use self-critical training , which we leverage as an alternative method for RL. \citet{Stiennon2020LearningTS} geneerate summaries with proximal policy optimization  as the reinforcement learning algorithm with an KL-regularized reward signal. \citet{Angermller2020ModelbasedRL} studies biological sequence generation with RL and points out the difficulty of value-based methods like DQN  in the setting where rewards are episodic and delayed at the end of sequence generation, which is a similar setup to our task.",231
" 	Identifying the user's open intent plays a significant role in dialogue systems. As shown in Figure, we have two known intents for specific purposes, such as book flight and restaurant reservation. However, there are also utterances with irrelevant or unsupported intents that our system cannot handle. It is necessary to distinguish these utterances from the known intents as much as possible. On the one hand, effectively identifying the open intent can improve customer satisfaction by reducing false-positive error. On the other hand, we can use the open intent to discover potential user needs. 	 	We regard open intent classification as an -class classification task as suggested in, and group open classes into the  class . Our goal is to classify the n-class known intents into their corresponding classes correctly while identifying the  class open intent. To solve this problem,~\citet{scheirer2013toward} propose the concept of open space risk as the measure of open classification.~\citet{fei-liu-2016-breaking} reduce the open space risk by learning the closed boundary of each positive class in the similarity space. However, they fail to capture high-level semantic concepts with SVM.  	~\citet{bendale2016towards} manage to reduce the open space risk through deep neural networks , but need to sample open classes for selecting the core hyperparameters.~\citet{hendrycks17baseline} use the softmax probability as the confidence score, but also need to select the confidence threshold with negative samples.~\citet{Shu2017DOCDO} replace softmax with the sigmoid activation function, and calculate the confidence thresholds of each class based on statistics. However, the statistics-based thresholds can not learn the essential differences between known classes and the open class.~\citet{lin-xu-2019-deep} propose to learn the deep intent features with the margin loss and detect unknown intents with local outlier factor. However, it has no specific decision boundaries for distinguishing the open intent, and needs model architecture modification.  	 	Most of the existing methods need to design specific classifiers for identifying the open class and perform poorly with the common classifier. Moreover, the performance of open classification largely depends on the  decision conditions. Most of these methods need negative samples for determining the suitable decision conditions. It is also a complicated and time-consuming process to manually select the optimal decision condition, which is not applicable in real scenarios.  	 	To solve these problems, we use known intents as prior knowledge, and propose a novel post-processing method to learn the adaptive decision boundary  for open intent classification. As illustrated in Figure, we first extract intent representations from the BERT model. Then, we pre-train the model under the supervision of the softmax loss. We define centroids for each known class and suppose known intent features are constrained in the closed ball areas. Next, we aim to learn the radius of each ball area to obtain the decision boundaries. Specifically, we initialize the boundary parameters with standard normal distribution and use a learnable activation function as a projection to get the radius of each decision boundary.  	 	The suitable decision boundaries should satisfy two conditions. On the one hand, they should be broad enough to surround in-domain samples as much as possible. On the other hand, they need to be tight enough to prevent out-of-domain samples from being identified as in-domain samples. To address these issues, we propose a new loss function, which optimizes the boundary parameters by balancing both the open space risk and the empirical risk. The decision boundaries can automatically learn to adapt to the intent feature space until balance with the boundary loss. We find that our post-processing method can still learn discriminative decision boundaries to detect the open intent even without modifying the original model architecture. 	 	We summarize our contribution as follows. Firstly, we propose a novel post-processing method for open classification, with no need for prior knowledge of the open class. Secondly,  we propose a new loss function to automatically learn tight decision boundaries adaptive to the feature space. To the best of our knowledge, this is the first attempt to adopt deep neural networks to learn the adaptive decision boundary for open classification. Thirdly, extensive experiments conducted on three challenging datasets show that our approach obtains consistently better and more robust results compared with the state-of-the-art methods.  	 		 	\end{table*} 	  	  	There are many works for intent detection in dialogue systems in recent years. Nevertheless, they all make the assumption in a closed world without open intent. perform intent detection with a zero-shot learning  method. However, ZSL is different from our task because it only contains novel classes during testing.  	 	Unknown intent detection is a specific task to detect the unknown intent. propose an unsupervised approach to modeling intents,  but fail to utilize the prior knowledge of known intents. jointly train the in-domain classifier and out-of-domain detector but need to sample out-of-domain utterances. adopt adversarial learning to generate positive and negative samples for training the classifier. use a generative adversarial network  to train on the in-domain samples and detect the out-of-domain samples with the discriminator. However, it has been shown that deep generative models fail to capture high-level semantics on real-world data. Recent methods try to learn friendly features for unknown intent detection , but they need to modify the  model architecture, and fail to construct specific decision boundaries. 	 	 	At first, researchers use SVM to solve open set problems. One-class classifiers find the decision boundary based on the positive training data. For multi-class open classification, One-vs-all SVM trains the binary classifier for each class and treats the negative classified samples as open class. extend the method to computer vision and introduce the concept of open space risk. estimate the unnormalized posterior probability of inclusion for open set problems. It fits the probability distributions to statistical Extreme Value Theory  using a Weibull-calibrated multi-class SVM. propose a Compact Abating Probability  model, which further improves the performance of Weibull-calibrated SVM by truncating the abating probability. However, all these methods need negative samples for selecting the decision boundary or probability threshold, and SVM cannot capture more advanced semantic features of intents. 	 	Recently, researchers use deep neural networks for open classification. OpenMax fits Weibull distribution to the outputs of the penultimate layer, but still needs negative samples for selecting the best hyperparameters. MSP calculates the softmax probability of known samples and rejects the low confidence unknown samples with the threshold. ODIN uses temperature scaling and input preprocessing to enlarge the difference between known and unknown samples. However, both of them need unknown samples to artificially select the confidence threshold. DOC uses sigmoid functions and calculates the confidence threshold based on Gaussian statistics, but it performs worse when the output probabilities are not discriminative.  	 	 	 	 	 	 	 	 	   In this paper, we propose a novel regularized attentive capsule network for overlapped relation extraction. RA-CapNet embeds relation query multi-head attention into the capsule network and uses a novel disagreement regularization term to encourage the diversity among heads and capsules, making it capable of gathering salient information from diverse semantic spaces. Our model is resistant to the noise of distant supervision and achieves significant improvements on both standard and complex datasets.  In the future, we will experiment with different forms of regularization terms and their application to other components of our model.  
"," 	  	There are many works for intent detection in dialogue systems in recent years. Nevertheless, they all make the assumption in a closed world without open intent. perform intent detection with a zero-shot learning  method. However, ZSL is different from our task because it only contains novel classes during testing.  	 	Unknown intent detection is a specific task to detect the unknown intent. propose an unsupervised approach to modeling intents,  but fail to utilize the prior knowledge of known intents. jointly train the in-domain classifier and out-of-domain detector but need to sample out-of-domain utterances. adopt adversarial learning to generate positive and negative samples for training the classifier. use a generative adversarial network  to train on the in-domain samples and detect the out-of-domain samples with the discriminator. However, it has been shown that deep generative models fail to capture high-level semantics on real-world data. Recent methods try to learn friendly features for unknown intent detection , but they need to modify the  model architecture, and fail to construct specific decision boundaries. 	 	 	At first, researchers use SVM to solve open set problems. One-class classifiers find the decision boundary based on the positive training data. For multi-class open classification, One-vs-all SVM trains the binary classifier for each class and treats the negative classified samples as open class. extend the method to computer vision and introduce the concept of open space risk. estimate the unnormalized posterior probability of inclusion for open set problems. It fits the probability distributions to statistical Extreme Value Theory  using a Weibull-calibrated multi-class SVM. propose a Compact Abating Probability  model, which further improves the performance of Weibull-calibrated SVM by truncating the abating probability. However, all these methods need negative samples for selecting the decision boundary or probability threshold, and SVM cannot capture more advanced semantic features of intents. 	 	Recently, researchers use deep neural networks for open classification. OpenMax fits Weibull distribution to the outputs of the penultimate layer, but still needs negative samples for selecting the best hyperparameters. MSP calculates the softmax probability of known samples and rejects the low confidence unknown samples with the threshold. ODIN uses temperature scaling and input preprocessing to enlarge the difference between known and unknown samples. However, both of them need unknown samples to artificially select the confidence threshold. DOC uses sigmoid functions and calculates the confidence threshold based on Gaussian statistics, but it performs worse when the output probabilities are not discriminative.",232
"  Recently, deep contextual language models have shown their effective modeling ability for text, achieving state-of-the-art results in series of NLP tasks. These models capture the syntactic and semantic information of the input text, generating fine-grained contextual embeddings, which can be easily applied to downstream models. Despite the success of large scale pre-trained language models on various tasks,  it is less clear how to extend them to semantic parsing tasks such as text-to-SQL, which requires joint reasoning of the natural language utterance and structured database schema information. Recent work shows that with more powerful pre-trained language models, the highly domain-specific semantic parsers can be further improved, even though these language models are trained for pure text encoding.  %      %    \end{table}%  However, based on error analysis on the output of neural language model-based text-to-SQL systems, we observe that these models can be further enhanced if we could mitigate the following three pain points, which are also illustrated in Table.  The model is ineffective to match and detect column names in utterances. The model should learn to detect column names mentioned in utterances by matching utterance tokens with the schema, and use the matched columns in the generated SQL.  The error analysis indicates that, in some cases, models miss some columns when synthesizing the target SQL, while the column is mentioned explicitly in the utterance.   The model fails to infer the columns implicitly from cell values. This problem is trickier than the first one, because the model is expected to infer the column name based on some cell values mentioned in the utterance, instead of just matching the utterance tokens with the schema. This requires the model to have more domain knowledge. For example, as presented in the second section of Table, the model should know  is a .  The model should learn to compose complex queries. Besides the column selection, to generate a correct SQL, the model should learn to attach the selected columns to the correct clauses. This is a non-trivial task, especially when the target SQL is complex, e.g., when the query is nested. As shown in the last section of Table, the model should learn to use corresponding column  in the nested SQL, instead of using column .  Recent work has demonstrated that jointly pre-training on utterances and table contents  can benefit downstream tasks such as table parsing and semantic parsing . These models are pre-trained using the Masked Language Modeling  task by either masking tokens from the utterance input or tokens from the schema input. However, this learning objective can only model the alignment between the utterance and schema implicitly. We hypothesize that, in order to cope with the three pain points previously listed, it is necessary to use pre-training objectives that enforce the learning of contextual representations that better capture the alignment between utterances and schema/table contents.  In this work, we present a language model pre-training framework, Generation-Augmented Pre-training~, that exploits multiple learning objectives  and synthetic data generation to jointly learn contextual representations of natural language utterances and table schema. We propose the following three new learning objectives that not only enforce joint learning but also improve the ability of the model to grasp more domain knowledge, which is helpful in cross-domain scenarios:  column prediction task, which is a pre-training task that consists in giving a label for each column in the input schema to decide whether it is used in the input utterance or not. This task is intent to improve the column detection ability of the model.  column recovery task,  which consists in randomly replacing some of the column names with one of their cell values and asking the model to recover the original column name either based on the cell value itself or based on the contextual information of the utterance when the column is explicitly mentioned in the utterance. This learning objective is meant to enhance the column inferring ability of the model.  SQL generation, which consists in generating SQL queries given utterances and schema. This task can boost the ability of the model to compose complex queries by leveraging large scale SQL datasets from the Web.%, such as Github.  A key challenge to use the proposed pre-training tasks is training data. Although it is easy to obtain large scale datasets of crawled tables and SQL queries,  it is difficult to obtain high-quality utterances interrelated with the tables or logically consistent with crawled SQL queries. Recent work used the surrounding text of tables as a proxy of natural language utterances. However, this option is far from optimal because those texts are dissimilar to user utterances in terms of text length, composition and content. The surrounding text of a table is usually a paragraph, while natural language utterances in the downstream task are short sentences. Furthermore, the content of surrounding text of tables can be quite noisy because the text may be irrelevant to the table. In \modelname, we overcome the pre-training data challenge through the use of synthetic data. We propose two sequence-to-sequence  generative models, SQL-to-text and table-to-text, that can produce large scale datasets with enough quality for pre-training. We train our generative models by finetuning BART, a state-of-the-art pre-trained language model. Concurrently,~\citet{yu2020grappa} and~\citet{deng2020structure} utilized synthetic data generated from synchronized context-free grammar and existing data-to-text datasets for pre-training, respectively, which requires extra crowd and expert annotation efforts.  The outcome of \modelname is a pre-trained model that can be plugged into neural semantic parsers to compute contextual representations of utterances and schema. We apply \modelname to text-to-SQL semantic parsing datasets, and experimental results show that systems augmented with \modelname~outperform state-of-the-art semantic parsers on Spider and Criteria-to-SQL datasets. In summary, our work presents the following main contributions:       \smallskip  Semantic Parsing: The semantic parsing task is framed as mapping the natural language utterances to meaning representations. The meaning representations can be executed in a variety of environments such as data analysis by translating the natural language queries into database queries. Based on different meaning representations, the semantic parsing task can be classified into three regimes: logic based formalism such as -DCS, graph based formalism such as AMR and UCCA, and programming languages such as Python and SQL.  Recently, more interests are concentrated on the SQL-based semantic parsing, and most of the work try to solve the problem with general encoder-decoder architecture. Overall, they enhance the models based on following aspects:   Improving the decoding mechanism;  Improving the decoding target;  Improving the model encoding ability;  Reranking over the generated candidates to improve parses quality. \modelname advances the line of  by leveraging generation models and three novel learning objectives to enhance the utterance-schema representations.  \smallskip  Question Generation and Table-to-Text Generation: The question generation task is to generate grammatically and semantically correct questions. The generated questions are usually used for enhancing the question answering models.  The table-to-text generation task is to generate declarative sentences that describe the information provided by the table. Our Table-to-Text model is a combination of these two directions, focusing on generating questions from table, i.e., composing questions based on the sampled columns and cell values, without providing the detailed information about ``what to ask''.  \smallskip  Pre-training Models: Recent pre-training techniques exploit external knowledge~ into large-scale pretrained language models.  More recently, \citet{yin2020tabert}, \citet{herzig2020tapas}, leverage the semi-structured table data to enhance the representation ability of language models. Concurrently, \citet{yu2020grappa} and \citet{deng2020structure} leveraged synchronous context-free grammar to generate synthetic data and utilized existing high-quality data-to-text dataset for pre-training, respectively. Different from these work, we explore the direction of utilizing the generators to enhance the joint utterances and structured schema encoding ability of the pre-trained models.    	In this paper, we propose a novel post-processing method for open intent classification. After pre-training the model with labeled samples, our model can learn specific and tight decision boundaries adaptive to the known intent feature space. Our method has no require for open intent or model architecture modification. Extensive experiments on three benchmark datasets show that our method yields significant improvements over the compared baselines and is more robust with less labeled data and fewer known intents. 	 	
","   \smallskip  Semantic Parsing: The semantic parsing task is framed as mapping the natural language utterances to meaning representations. The meaning representations can be executed in a variety of environments such as data analysis by translating the natural language queries into database queries. Based on different meaning representations, the semantic parsing task can be classified into three regimes: logic based formalism such as -DCS, graph based formalism such as AMR and UCCA, and programming languages such as Python and SQL.  Recently, more interests are concentrated on the SQL-based semantic parsing, and most of the work try to solve the problem with general encoder-decoder architecture. Overall, they enhance the models based on following aspects:   Improving the decoding mechanism;  Improving the decoding target;  Improving the model encoding ability;  Reranking over the generated candidates to improve parses quality. \modelname advances the line of  by leveraging generation models and three novel learning objectives to enhance the utterance-schema representations.  \smallskip  Question Generation and Table-to-Text Generation: The question generation task is to generate grammatically and semantically correct questions. The generated questions are usually used for enhancing the question answering models.  The table-to-text generation task is to generate declarative sentences that describe the information provided by the table. Our Table-to-Text model is a combination of these two directions, focusing on generating questions from table, i.e., composing questions based on the sampled columns and cell values, without providing the detailed information about ``what to ask''.  \smallskip  Pre-training Models: Recent pre-training techniques exploit external knowledge~ into large-scale pretrained language models.  More recently, \citet{yin2020tabert}, \citet{herzig2020tapas}, leverage the semi-structured table data to enhance the representation ability of language models. Concurrently, \citet{yu2020grappa} and \citet{deng2020structure} leveraged synchronous context-free grammar to generate synthetic data and utilized existing high-quality data-to-text dataset for pre-training, respectively. Different from these work, we explore the direction of utilizing the generators to enhance the joint utterances and structured schema encoding ability of the pre-trained models.",233
"    Neural Machine Translation   yields  state-of-the-art translation performance when a large number of parallel sentences are available. However, only a few parallel corpora are available for the majority of language pairs and domains. It has been known that NMT does not perform well in the specific domains where the domain-specific corpora are limited, such as medical domain. As such, high-quality domain-specific machine translation  systems are in high demand whereas general purpose MT has limited applications.  There are many studies of domain adaptation for NMT, which can be mainly divided into two categories: data-centric and model fine-tuning.  Data-centric methods focus on  selecting or generating target domain data from general domain corpora, which is effective and well explored.  In this paper, we focus on the second approach. Fine-tuning is  very common in domain adaptation, which first trains a base model on the general domain data and then fine-tunes it on each target domain . However, unconstrained or full fine-tuning  requires very careful hyper-parameter tuning,  and is prone to over-fitting on the target domain as well as forgetting on the general domain. To tackle these problems, researchers have proposed several constructive approaches, with the view to limiting the size or plasticity of parameters in the fine-tuning stage, which can be roughly divided into two categories: regularization and partial-tuning strategy. Regularization methods often integrate extra training objectives to prevent parameters from large deviations, such as model output regularization , elastic weight consolidation  . Regularization methods, which impose arbitrary global constraints on parameter updates, may further restrict the adaptive process of the network, especially when domain-specific corpora are scarce. Partial-tuning methods either freeze several sub-layers of the network and fine-tune the others, or integrate domain-specific adapters into the network. By only fine-tuning the domain-specific part of the model, they can alleviate the over-fitting and forgetting problem in fine-tuning. However, the structure designed to adapting is usually hand-crafted, which relies on experienced experts and the adapter brings additional parameters. Therefore,  a more adaptive, scalable, and parameter-efficient approach for domain adaptation is very valuable and worth well studying.    In this paper, we propose \method, a novel domain adaptation method via adaptive structure pruning. Our motivation is inspired from Continual Learning  and the lottery hypothesis that a randomly-initialized, dense neural network contains a sub-network which  can match the test accuracy of the original network after training for at most the same number of iterations.   We therefore suppose that multiple  machine translation models for different domains can share different sparse subnetworks within a single neural network.   Specifically, we first apply a standard pruning technique to automatically uncover the subnetwork from a well-trained NMT model in the general domain.  The  subnetwork is capable of  reducing the parameter without compromising accuracy. Therefore, it has the potential to keep as much general information as possible.   Then we freeze this informative sparse network and leave the unnecessary  parameters unfixed for the target  domain, which enables our approach to be parameter efficient, and eases the scalability of the approach to more domains.  The capacity of these non-fixed parameters can be tuned to match the requirements of the target domain, while keeping the parameters of the general domain. Our method successfully circumvents catastrophic forgetting problem and retains the quality on the general domain.  As the benefits of the flexible design, \method can be easily extended to other transfer learning problems, such as multilingual machine translation.      We summarize our main contribution as follows:      % --------------------Background--------------------     Domain adaptation has been widely investigated in recent years.  In Machine Translation, the fine-tuning based approach is the most relevant to our work.  Fine-tune is the conventional way for domain adaptation . Many studies try to address the shortcoming of Fine-tune. \citet{thompson2018freezing} freeze selected modules of the general network. Adapters is introduced for parameter efficiency . \citet{khayrallah2018regularized} explore regularization techniques to avoid over-fitting. \citet{thompson2019overcoming} employ EWC  to alleviate the catastrophic forgetting problem in domain adaptation. \citet{zhang2020revisiting} re-initialize parameters from some layer for few-sample BERT fine-tuning. \citet{wuebker2018compact} introduce sparse offset from the general model parameters for every domain, sharing the similar idea of our proposed method. The key difference is that \method provides a dynamic parameter adaptation method, which is  parameter efficient and potentially makes the most of general domain information for the target  domain.   Another research line for domain adaptation is data selection and data mixing, both being concerned with how to sample examples to train an MT model with a strong focus on a specific domain, while \method focused on the training model which can complement with the data-driven methods perfectly.    The main idea of our approach is originated from the Continual Learning community , as they all try to alleviate the catastrophic forgetting problem. \citet{mallya2018packnet, mallya2018piggyback, hung2019compacting} learn separate subnetworks for multiple tasks in computer vision, which inspires us with \method for machine translation domain adaptation.    Our approach is also inspired by many studies of sparse networks .  \citet{frankle2018lottery, liu2018rethinking} reevaluate unstructured network pruning to highlight the importance of sparse network structure.  \citet{zhu2017prune} introduce advanced pruning technique to compress the model. \citet{sun2019learning} learn sparse sharing architecture for multi-task learning. \citet{hung2019compacting} introduce compact parameter subnetwork for continual learning.  Different from these work, \method aims at finding the best sparse structure for a specific domain  based on an NMT model trained on large scale general domain data. Model pruning is an effective method for our approach.       --------------------conclusion--------------------   In this work, we spot three pain points in the Text-to-SQL semantic parsing task,  and propose a generation-augmented pre-training framework to alleviate them, with four different learning objectives. Experimental results on  dataset and  dataset show the effectiveness of this framework, which achieves state-of-the-art performance on both datasets.      \clearpage   
","   Domain adaptation has been widely investigated in recent years.  In Machine Translation, the fine-tuning based approach is the most relevant to our work.  Fine-tune is the conventional way for domain adaptation . Many studies try to address the shortcoming of Fine-tune. \citet{thompson2018freezing} freeze selected modules of the general network. Adapters is introduced for parameter efficiency . \citet{khayrallah2018regularized} explore regularization techniques to avoid over-fitting. \citet{thompson2019overcoming} employ EWC  to alleviate the catastrophic forgetting problem in domain adaptation. \citet{zhang2020revisiting} re-initialize parameters from some layer for few-sample BERT fine-tuning. \citet{wuebker2018compact} introduce sparse offset from the general model parameters for every domain, sharing the similar idea of our proposed method. The key difference is that \method provides a dynamic parameter adaptation method, which is  parameter efficient and potentially makes the most of general domain information for the target  domain.   Another research line for domain adaptation is data selection and data mixing, both being concerned with how to sample examples to train an MT model with a strong focus on a specific domain, while \method focused on the training model which can complement with the data-driven methods perfectly.    The main idea of our approach is originated from the Continual Learning community , as they all try to alleviate the catastrophic forgetting problem. \citet{mallya2018packnet, mallya2018piggyback, hung2019compacting} learn separate subnetworks for multiple tasks in computer vision, which inspires us with \method for machine translation domain adaptation.    Our approach is also inspired by many studies of sparse networks .  \citet{frankle2018lottery, liu2018rethinking} reevaluate unstructured network pruning to highlight the importance of sparse network structure.  \citet{zhu2017prune} introduce advanced pruning technique to compress the model. \citet{sun2019learning} learn sparse sharing architecture for multi-task learning. \citet{hung2019compacting} introduce compact parameter subnetwork for continual learning.  Different from these work, \method aims at finding the best sparse structure for a specific domain  based on an NMT model trained on large scale general domain data. Model pruning is an effective method for our approach.       --------------------conclusion--------------------",234
" As an important task in a dialogue system, response selection aims to find the best matched response from a set of candidates given the context of a conversation. The retrieved responses usually have natural, fluent and diverse expressions with rich information owing to the abundant resources. Therefore, response selection has been widely used in industry and has attracted great attention in academia.  Most existing studies on this task pay more attention to the matching problem between utterances and responses, but with insufficient concern for the reasoning issue in multi-turn response selection. Just recently, MuTual, the first human-labeled reasoning-based dataset for multi-turn dialogue, has been released to promote this line of research. Reasoning is quite different from matching in the conversations. Specifically, matching focuses on capturing the relevance features between utterances and responses, while reasoning not only needs to identify key features , but also needs to conduct inference based on these clue words. The challenges of this new task include:  how to identify the clue words in utterances, which is fundamental for inference;  how to conduct inference according to the clue words in utterances. Figure illustrates a motivating example. To infer the current time, we must first identify the clue words `10:45' in  and `15 minutes' in . Then we must conduct a logical inference based on these clue words in  and .    To tackle these challenges, first, we need better contextual representation for identifying the clue words in conversations. This is because clue word identification inevitably relies on the context of a conversation. Although previous literature publications have achieved promising results in context modeling, there are still several limitations of these approaches. More concretely, the existing studies either concatenate the utterances to form context or process each utterance independently, leading to the loss of dependency relationships among utterances or important contextual information. It has been validated that the chronological dependency between utterances, as well as the semantical dependency between utterances, are crucial for multi-turn response selection. Thus, how to model the dependencies in utterances remains a challenging problem for context representation.  Second, we need to devise a new strategy to collect the clue words scattered in multiple utterances and need to reason according to these clue words. In recent years, we have witnessed great success in KBQA  and MRC  tasks. However, new obstacles emerge for transferring current reasoning approaches in KBQA and MRC to conversational reasoning.  A clear reasoning path based on entities in a well-structured knowledge base exists in KBQA, but there is no similar reasoning path in utterances.  Current approaches on MRC conduct inference based on graph while taking shared entities as nodes, while it is difficult to construct such graphs based on entities in short utterances, which usually suffer from greater coreference resolution, poor content and serious semantic omission problems in comparison with document text.  In this paper, we propose a new model named GRN  which can tackle both challenges in an end-to-end way. We first introduce two pre-training tasks called NUP  and UOP  which are specially designed for response selection. NUP endows GRN with context-aware ability for semantical dependency, and UOP facilitates GRN with the ability to capture the chronological dependency. These customized pre-training methods are beneficial for modeling dependencies contained in utterances to achieve better context representation. We perform task-adaptive pre-training with the combined NUP and UOP tasks based on the ALBERT model. To conduct reasoning based on clue words, we devise a graph neural network called UDG , which not only models the dependencies between utterances with each utterance as a node but also collects the clue words from different utterances. Reasoning is achieved by propagating the messages of clue words between nodes along various utterance paths on UDG, and this graph reasoning structure realizes the inference based on an utterance-level context vector with local perspective. On the other hand, we also implement a reasoning network by the output of the trained model and self-attention mechanism. This sequence reasoning structure realizes the inference based on the highly summarized context vector with global perspective. To summarize, we make the following contributions:      \subsubsection{Response Selection} Response selection aims to select the best matched response from a set of candidates, which can be categorized into single-turn and multi-turn dialogues. Early studies focused on the single-turn dialogues. Recently, researchers devote more attention to the multi-turn dialogues technology . Existing methods tend to use deep matching methods to model the relationships between utterances and candidate responses. These models generally use representation methods based on LSTM, attention mechanisms and hierarchical interaction techniques. These models are focused on matching not reasoning. The key problem of matching type models is how to extract better matching features.  In fact, however, the key problem of reasoning is how to conduct inference according to clue words from different utterances, which is more complicated. Existing multi-turn response selection methods are not suitable for the reasoning problems. \subsubsection{Graph Neural Network} The GNN  has achieved outstanding performance in reasoning tasks based on question answer . In this paper, we convert the sequence structure of utterances into a graph structure and realize reasoning by using the GCN . The graph structure network is adept at information collection, fusion and summarization by message passing along different node paths. The reasoning problem of dialogue is different from those of KBQA and MRC. There is a clear reasoning path based on entity in triples knowledge in KBQA. Currently, most applications of MRC construct the graph according to the shared entities. All of these approaches are difficult to perform for conversation. Previous works on GCN show the superior ability of GCN to integrate the features of local nodes, which inspires us to solve the inference issues in conversation with graph structure.   In this work, we propose \method, an effective way for adapting neural machine translation models which first generates an informative subnetwork for the general domain via gradual pruning and then fine-tunes the unnecessary parameters for the target domain. By doing so, \method is able to retain as much general information as possible and alleviate the catastrophic forgetting problems. Experiments show that the proposed \method outperforms fine-tuning and several strong baselines and it is shown to be much more robust compared to fine-tuning due to the complete retainment of the general information. Beyond that, \method can be extended to adapting multiple domains by iteratively pruning and tuning, which is naturally suitable for multi-lingual scenario. We leave the multi-lingual problem as our future work.    --------------------Acknowledgements-------------------- 
"," \subsubsection{Response Selection} Response selection aims to select the best matched response from a set of candidates, which can be categorized into single-turn and multi-turn dialogues. Early studies focused on the single-turn dialogues. Recently, researchers devote more attention to the multi-turn dialogues technology . Existing methods tend to use deep matching methods to model the relationships between utterances and candidate responses. These models generally use representation methods based on LSTM, attention mechanisms and hierarchical interaction techniques. These models are focused on matching not reasoning. The key problem of matching type models is how to extract better matching features.  In fact, however, the key problem of reasoning is how to conduct inference according to clue words from different utterances, which is more complicated. Existing multi-turn response selection methods are not suitable for the reasoning problems. \subsubsection{Graph Neural Network} The GNN  has achieved outstanding performance in reasoning tasks based on question answer . In this paper, we convert the sequence structure of utterances into a graph structure and realize reasoning by using the GCN . The graph structure network is adept at information collection, fusion and summarization by message passing along different node paths. The reasoning problem of dialogue is different from those of KBQA and MRC. There is a clear reasoning path based on entity in triples knowledge in KBQA. Currently, most applications of MRC construct the graph according to the shared entities. All of these approaches are difficult to perform for conversation. Previous works on GCN show the superior ability of GCN to integrate the features of local nodes, which inspires us to solve the inference issues in conversation with graph structure.",235
" As a fundamental task in natural language processing , coherence analysis can benefit various downstream tasks, such as sentiment analysis  and document summarization . Rhetorical Structure Theory   is one of the most influential theories of text coherence, under which a document is represented by a hierarchical discourse tree, which consists of a set of semantic units organized in the form of a dependency structure, labeled with their rhetorical relations. As shown in Figure , the leaf nodes of an RST discourse tree are basic text spans called Elementary Discourse Units , and the EDUs are iteratively connected by rhetorical relations  to form larger text spans until the entire document is included.  The rhetorical relations are further categorized to Nucleus and Satellite based on their relative importance, in which Nucleus corresponds to the core part while Satellite corresponds to the subordinate part. While manual coherence analysis under the RST theory is labor-intensive and requires specialized linguistic knowledge, a discourse parser serves to automatically transform a document into a discourse tree. Document-level discourse parsing consists of three sub-tasks: hierarchical span splitting, rhetorical nuclearity determination, and rhetorical relation classification.    Models for RST-style discourse parsing have made much progress in the past decade. While statistical methods utilize hand-crafted lexical and syntactic features , data-driven neural approaches reduce feature-engineering labor by effective representation learning, and are capable of characterizing implicit semantic information. Neural networks are first used as feature extractors along with traditional shift-reduce approaches  or dynamic programming approaches . Then, \citet{yu2018transition} bridges the gap between neural and traditional methods by an end-to-end transition-based neural parser via an encoder-decoder architecture. Recently, pointer networks are introduced to achieve linear-time complexity, and models with top-down parsing procedures achieve favorable results on sentence-level discourse analysis tasks .  However, there is still much space for improvement in document-level discourse parsing. First, compared to sentence-level parsing, document-level parsing is more challenging due to the deeper tree structures and longer dependencies among EDUs: in the benchmark dataset RST Discourse Tree Bank  , the average EDU number at the document level is 56, which is 20 times larger than that of sentence-level parsing. Thus modeling context information across a long span is essential, especially if considering a top-down parsing procedure where poor accuracy at the top of the tree will propagate toward the leaf nodes. Second, the three sub-tasks of discourse parsing strongly rely on nuanced semantic judgments, which require comprehensive contextual representation with various types of linguistic information. Take discourse relation classification for example, explicit relations are overtly signaled by a connective word such as ``although'' and ``because'', which can be determined by lexical and syntactic features. However, this approach can not be readily adapted to implicit discourse relations determination, as it requires high-order features with semantic information. Moreover, to compensate for the lack of large-scale corpora, prior work in neural modeling has leveraged inductive biases through syntactic features such as part-of-speech tagging to improve performance. However, such models still suffer from insufficient linguistics information from the lack of data, thus they are incapable of acquiring deeper and richer contextual representations useful for discourse processing.  In this paper, to tackle the aforementioned challenges, we propose a document-level neural discourse parser with robust representation modeling at both the EDU and document level, based on a top-down parsing procedure. To take advantage of widely-adopted vector representations that encode rich semantic information, we first exploit a large-scale pre-trained language model as a contextual representation backbone.  Then we incorporate boundary information with implicit semantic and syntactic features to the EDU representations, and introduce a hierarchical encoding architecture to more comprehensively characterize global information for long dependency modeling. To improve inference accuracy and alleviate the aforesaid error propagation problem, we present breadth-first span splitting to propose a layer-wise beam search algorithm.  We train and evaluate our proposed model on the benchmark corpus RST-DT\footnote{https://catalog.ldc.upenn.edu/LDC2002T07} , and achieve the state-of-the-art performance on all fronts, significantly surpassing previous models while approaching the upper bound of human performance. We also conduct extensive experiments to analyze the effectiveness of our proposed method.    RST discourse parsing has been in the spotlight since . Statistical models are dominant in initial studies. \citet{soricut2003sentence} proposed a learning-based probabilistic model to build the discourse trees, then the shift-reduce based framework was introduced to discourse parsing by \citet{sagae2009analysis}. Later greedy transition-based shift-reduce parsers achieved cutting-edge performances .  Different from shift-reduce parsers,  employed conditional random field approaches to seek globally optimal results, where two EDUs with the highest relational probability were merged into one span iteratively to generate the discourse tree. In addition, linguistic features have been demonstrated to be effective in RST discourse parsing. Classic features adopted by previous work  include:  -gram features, which is a lexical dictionary, describe the discourse cues ;  Syntactic features such as part-of-speech  tags;  Organizational features, which illustrate the textual organization including the number of EDUs and length of tokens, as well as distances of the units from the beginning and the end of text spans;  Dominance sets and lexical chains which show the dominance relation and indicate topic shifts respectively. Furthermore, \citet{li2014text} explored the benefits of dependency structures in discourse parsing.   Recently, neural networks have been making inroads into discourse analysis frameworks. \citet{li2016discourse} proposed an attention-based hierarchical neural network model, which employed a Bi-LSTM network for obtaining compositional semantic representations.  \citet{yu2018transition} improved the performance by integrating neural syntax features into the greedy transition-based parser. In addition, \citet{lin-etal-2019-unified} and their follow-up work \citet{liu2019hierarchical} successfully explored encoder-decoder neural architectures on sentence-level discourse analysis, with a top-down parsing procedure. \citet{zhang-etal-2020-top} evaluated the effectiveness of top-down parsing compared with a bottom-up approach. More recently, \citet{kobayashi-etal-2020-Top} proposed a multi-stage parsing process from document-level and paragraph-level to sentence-level. \citet{liu-etal-2020-multilingual-neural} investigated cross-lingual representations and EDU-level translation on multilingual RST discourse parsing.   In this work, we show how incorporating different levels of granularity of implicit linguistic features and hierarchically modeling the content from the EDU to the document level, can improve the performance on the more challenging document-level RST discourse parsing task.     In this paper, we have formulated the task of entity linking as a candidate ranking approach. Using a Triplet Network,  we learn high-quality representations of candidates, tailored to reveal relative distances between the disease mention and its positive and negative candidates. Furthermore,  we take a step towards eliminating the need to generate candidates based on hand-crafted rules and external knowledge resources. Though our method outperforms the existing systems by a strong margin, there is a scope for improvement in terms of attention-based disease similarity  . An intriguing course for future work is to further explore the robustness and scalability of this approach to other clinical datasets for entity normalization.  
"," RST discourse parsing has been in the spotlight since . Statistical models are dominant in initial studies. \citet{soricut2003sentence} proposed a learning-based probabilistic model to build the discourse trees, then the shift-reduce based framework was introduced to discourse parsing by \citet{sagae2009analysis}. Later greedy transition-based shift-reduce parsers achieved cutting-edge performances .  Different from shift-reduce parsers,  employed conditional random field approaches to seek globally optimal results, where two EDUs with the highest relational probability were merged into one span iteratively to generate the discourse tree. In addition, linguistic features have been demonstrated to be effective in RST discourse parsing. Classic features adopted by previous work  include:  -gram features, which is a lexical dictionary, describe the discourse cues ;  Syntactic features such as part-of-speech  tags;  Organizational features, which illustrate the textual organization including the number of EDUs and length of tokens, as well as distances of the units from the beginning and the end of text spans;  Dominance sets and lexical chains which show the dominance relation and indicate topic shifts respectively. Furthermore, \citet{li2014text} explored the benefits of dependency structures in discourse parsing.   Recently, neural networks have been making inroads into discourse analysis frameworks. \citet{li2016discourse} proposed an attention-based hierarchical neural network model, which employed a Bi-LSTM network for obtaining compositional semantic representations.  \citet{yu2018transition} improved the performance by integrating neural syntax features into the greedy transition-based parser. In addition, \citet{lin-etal-2019-unified} and their follow-up work \citet{liu2019hierarchical} successfully explored encoder-decoder neural architectures on sentence-level discourse analysis, with a top-down parsing procedure. \citet{zhang-etal-2020-top} evaluated the effectiveness of top-down parsing compared with a bottom-up approach. More recently, \citet{kobayashi-etal-2020-Top} proposed a multi-stage parsing process from document-level and paragraph-level to sentence-level. \citet{liu-etal-2020-multilingual-neural} investigated cross-lingual representations and EDU-level translation on multilingual RST discourse parsing.   In this work, we show how incorporating different levels of granularity of implicit linguistic features and hierarchically modeling the content from the EDU to the document level, can improve the performance on the more challenging document-level RST discourse parsing task.",236
" Due to the substantial growth and effortless access to the Internet in recent years, an enormous amount of unstructured textual contents have generated. It is a crucial task to organize or structure such a voluminous unstructured text in manually. Thus, automatic classification can be useful to manipulate a huge amount of texts, and extract meaningful insights which save a lot of time and money. Text categorization is a classical NLP problem which aims to categorize texts into organized groups. It has a wide range of applications like machine translation, question answering, summarization, and sentiment analysis. There are several approaches available to classify texts according to their labels. However, deep learning method outperforms the rule-based and machine learning-based models because of their ability to capture sequential and semantic information from texts . We propose a classifier using CNN , and BiLSTM  to classify technical texts in the computer science domain. Furthermore, by sequentially adding these networks, remarkable accuracy in several shared classification tasks can be obtained. The rest of the paper is organized as follows: related work given in section 2. Section 3 describes the dataset. The framework described in section 4. The findings presented in section 5.   %%%%%%%%%%%% Related Work %%%%%%%%%   CNN and LSTM have achieved great success in various NLP tasks such as sentence classification, document categorization, sentiment analysis, and summarization. \citet{kim2014convolutional} used convolution neural network to classify sentences. A method used contents and citations to classify scientific document . \citet{zhou2016text} used 2-D max pooling and bidirectional LSTM to classify texts. \citet{zhou2015c} combined CNN and LSTM to classify sentiment and question type. Their system achieved superior accuracy than CNN and LSTM individually. \citet{hossain2020sentilstm} used LSTM to classify sentiment of Bengali text documents. Their system got maximum accuracy with one layer of LSTM followed by three dense layers. \citet{ranjan2017document} proposed a document classification framework using LSTM and feature selection algorithms. \citet{ameur2020robust} combined CNN and RNN methods to categorize Arabic texts. They used dynamic, fine-tuned words embedding to get effective result on open-source Arabic dataset.      We proposed to exploit robust representations of multiple levels of granularity at the syntactic and semantic levels and in turn incorporated such representations in an end-to-end encoder-decoder neural architecture for resourceful discourse processing. Our document-level discourse parser compares favorably with the current state-of-the-art. Experimental results show that our document-based neural discourse parser benefits the most from incorporating boundary information at the EDU level and from modeling global information.   
"," CNN and LSTM have achieved great success in various NLP tasks such as sentence classification, document categorization, sentiment analysis, and summarization. \citet{kim2014convolutional} used convolution neural network to classify sentences. A method used contents and citations to classify scientific document . \citet{zhou2016text} used 2-D max pooling and bidirectional LSTM to classify texts. \citet{zhou2015c} combined CNN and LSTM to classify sentiment and question type. Their system achieved superior accuracy than CNN and LSTM individually. \citet{hossain2020sentilstm} used LSTM to classify sentiment of Bengali text documents. Their system got maximum accuracy with one layer of LSTM followed by three dense layers. \citet{ranjan2017document} proposed a document classification framework using LSTM and feature selection algorithms. \citet{ameur2020robust} combined CNN and RNN methods to categorize Arabic texts. They used dynamic, fine-tuned words embedding to get effective result on open-source Arabic dataset.",237
" The traditional task-oriented dialogue systems, which focuses on providing information and performing actions by the given databases or APIs, often meet the limitation that the DB/API can not cover enough necessary cases. A good enhance can be achieved with lots of relevant domain knowledge in the form of descriptions, FAQs and customer reviews, which we call unstructured knowledge. Track 1 of the 9th Dialogue System Technology Challenges , Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access, aims at generating a response based on dialogue history and unstructured knowledge access. The whole task can be divided into three subtasks, knowledge-seeking turn detection, knowledge selection and knowledge-grounded response. Test set of this track includes seen and unseen parts. The unseen test set are collected on different domains, entities, and locales, aiming to evaluate models' generalization ability.   Knowledge-seeking turn detection, as the first subtask, needs to determine whether the related knowledge is contained in the unstructured knowledge base. In other words, this subtask can be modeled as a binary classification problem. If the model predicts that there exists related knowledge, then subtask 2  will search for the most relevant knowledge snippets and then pass them to the generation process . If the model predicts that there is no related knowledge for the specific question, the remaining two subtasks will not be performed. In this paper, we first conduct an entity matching for each question and then add the domain label from matching results to the end of dialogue history as model input.  Knowledge selection is to retrieve the most relevant knowledge snippets from the database according to the dialogue history and provide information for the subsequent response generation. The dialogue history is a conversation between the human speaker and the machine. Close to the end of the conversation, the human speaker brings up a question about a certain place  or service . The given knowledge database consists of question-answer pairs involving diverse facts and is organized by different domains and entities. % Note that the knowledge-seeking turn detection model determines whether our dialog system needs to access the knowledge database before generating the response.  % We perform knowledge selection for the samples  that requires relevant knowledge in the database. The retrieved knowledge snippets provide information for the subsequent response generation.  % Information retrieval  techniques are widely applied to search for related candidates in retrieval-based knowledge-grounded system. Some researchers  compute the traditional tf-idf score to search the most relevant document to the user's query, while others leverage the power of neural networks to learn the ranking score directly through an end-to-end learning process. Recently, due to the significant improvements on numerous natural language processing tasks, large scale pre-trained language models have also been applied to better model the semantic relevance in knowledge selection.  In this paper, we first apply retrieval techniques to narrow down the searching space and then use a neural network initialized by a pre-trained model to formulate the ranking function. % We propose two base models for the knowledge selection, and the final ensemble model combines the predictions of different base models to improve the selection performance.  % The Retrieve \& Rank model first gathers the knowledge snippets of potentially relevant entities from the knowledge base, then a ranking model is trained to select the most plausible knowledge snippets from the retrieved candidates. % Different from the Retrieve \& Rank model, Three-step model divides the ranking model into three cascade parts to rank domain, entity and documents respectively in order to force the model to take the knowledge hierarchy into account. % We also ensemble these two models together and experiments show the ensemble model has a better performance than two base model separately.   % briefly introduce the three-step pipeline model.   Knowledge-grounded response generation requests to give a response automatically from the model using dialogue history and unstructured knowledge as input. There are two different types of dialogue systems, retrieval-based system, and generation-based system. Retrieval-based dialogue system, giving responses from a list of candidate sentences, only has fixed answer forms in candidate sets. To deal with our problem, which needs more flexible and natural responses, the generation-based model is a better choice. Dialogue generation requires an encoder to represent the input and a decoder to generate the response. The network often needs to minimize the cross-entropy loss between the output and the ground truth. In this paper, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism.  % Pre-trained language models make a great progress on dialogue generation. Note that bi-directional model is not designed for dialogue generation task, and thus PLATO and  PLATO-2 use uni- and bi-directional processing for pre-training. Moreover, large-scale Reddit and Twitter conversations are utilized to further pre-train the generation model to reduce data distribution gaps. Furthermore, a latent variable  is used to capture one-to-many relations of post-response pairs.  As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics. In the following sections, we will explain the details of our proposed model. Experiment results will be shown next with some analysis and conclusions.    All three subtasks use the pre-trained language model to represent sentences better and deal with the unseen test set. For subtask 1, we use ELECTRA as our baseline model, while for subtask 2 and subtask 3, we use RoBERTa as base encoder. ELECTRA and RoBERTa are BERT-like architecture with the bi-directional attention mechanism, while GPT only has uni-directional attention.  \subsubsection{Knowledge-seeking Turn Detection} To our best knowledge, the Knowledge-seeking Turn Detection is newly proposed by this contest.  Essentially, Knowledge-seeking Turn Detection is a general classification task that has been explored by NLP community for decades. There are some relevant research topics, such as sentiment analysis and natural language inference . Currently, the main stream pre-trained models can get state-of-the-art performance on these classification tasks. According to our experiments, ELECTRA reached the highest performance on subtask1, so we select ELECTRA as our baseline model on this subtask.  \subsubsection{Knowledge Selection}   \subsubsection{Knowledge Retrieval}  Information retrieval  techniques are widely applied to search for related candidates in retrieval-based knowledge-grounded system. Some researchers  compute the traditional tf-idf score to search the most relevant document to the user's query, while others leverage the power of neural networks to learn the ranking score directly through an end-to-end learning process. Recently, due to the significant improvements on numerous natural language processing tasks, large scale pre-trained language models have also been applied to better model the semantic relevance in knowledge selection.     In retrieval-based knowledge-grounded system, information retrieval techniques are widely applied to search for related candidates. Some researchers  compute the traditional tf-idf score to search the most relevant document to the user's query, while others leverage the power of neural networks to learn the ranking score directly through an end-to-end learning process. Recently, due to the  strong improvements on numerous natural language processing tasks, large scale pre-trained language models are also applied to better model the semantic relevance in knowledge selection.     \subsubsection{Dialogue Generation} Two architectures are often used in generation, sequence-to-sequence  and language model. Pre-trained language models make a great progress on dialogue generation. PLATO and PLATO-2 use uni- and bi-directional processing to further pre-train on large-scale Reddit and Twitter conversations dataset to reduce data distribution gaps. Moreover, a latent variable  is used to capture one-to-many relations of post-response pairs.  \subsubsection{Conditional Dialogue Generation} Being viewed as conditional dialogue response generation, this subtask is closed to persona-chat, which aims to generate responses within dialogue by given personality of speakers. use GPT-2 as their based model by concatenating the personality and dialogue history splitting by speaker tokens. They also use an auxiliary task, binary classification, to decide whether the response is true under given condition. Baseline given by organizer also using GPT-2 with concatenating the knowledge and dialogue history with speaker tokens.   \subsubsection{Pointer Network} Knowledge-based dialogue generation struggles with out-of-vocabulary  words since knowledge will be exact time or some strange proper nouns, which will not be seen by pre-trained language models. To deal with OOV problems, provided a method to generate words by adding the attention distribution into standard decoder output. It is also called the copy mechanism since the probability of input words would be copied as answers.    This paper presents a detail description of the proposed system and its evaluation for the technical texts classification in different languages. As the baseline method, we used CNN and BiLSTM, and compare these methods with the proposed model . Each model is trained, tuned and evaluated separately for subtasks 1 and 2. The proposed method showed better performance in terms of accuracy for subtasks  of task 1 and task 2a on development set. However, in the case of test set, the system performed better for the subtasks 1a, 1b, 1c, 1g and 2a. More dataset can be included for improved performance. In future, the attention mechanism may be explored to observe its effects on text classification tasks.   
"," All three subtasks use the pre-trained language model to represent sentences better and deal with the unseen test set. For subtask 1, we use ELECTRA as our baseline model, while for subtask 2 and subtask 3, we use RoBERTa as base encoder. ELECTRA and RoBERTa are BERT-like architecture with the bi-directional attention mechanism, while GPT only has uni-directional attention.  \subsubsection{Knowledge-seeking Turn Detection} To our best knowledge, the Knowledge-seeking Turn Detection is newly proposed by this contest.  Essentially, Knowledge-seeking Turn Detection is a general classification task that has been explored by NLP community for decades. There are some relevant research topics, such as sentiment analysis and natural language inference . Currently, the main stream pre-trained models can get state-of-the-art performance on these classification tasks. According to our experiments, ELECTRA reached the highest performance on subtask1, so we select ELECTRA as our baseline model on this subtask.  \subsubsection{Knowledge Selection}   \subsubsection{Knowledge Retrieval}  Information retrieval  techniques are widely applied to search for related candidates in retrieval-based knowledge-grounded system. Some researchers  compute the traditional tf-idf score to search the most relevant document to the user's query, while others leverage the power of neural networks to learn the ranking score directly through an end-to-end learning process. Recently, due to the significant improvements on numerous natural language processing tasks, large scale pre-trained language models have also been applied to better model the semantic relevance in knowledge selection.     In retrieval-based knowledge-grounded system, information retrieval techniques are widely applied to search for related candidates. Some researchers  compute the traditional tf-idf score to search the most relevant document to the user's query, while others leverage the power of neural networks to learn the ranking score directly through an end-to-end learning process. Recently, due to the  strong improvements on numerous natural language processing tasks, large scale pre-trained language models are also applied to better model the semantic relevance in knowledge selection.     \subsubsection{Dialogue Generation} Two architectures are often used in generation, sequence-to-sequence  and language model. Pre-trained language models make a great progress on dialogue generation. PLATO and PLATO-2 use uni- and bi-directional processing to further pre-train on large-scale Reddit and Twitter conversations dataset to reduce data distribution gaps. Moreover, a latent variable  is used to capture one-to-many relations of post-response pairs.  \subsubsection{Conditional Dialogue Generation} Being viewed as conditional dialogue response generation, this subtask is closed to persona-chat, which aims to generate responses within dialogue by given personality of speakers. use GPT-2 as their based model by concatenating the personality and dialogue history splitting by speaker tokens. They also use an auxiliary task, binary classification, to decide whether the response is true under given condition. Baseline given by organizer also using GPT-2 with concatenating the knowledge and dialogue history with speaker tokens.   \subsubsection{Pointer Network} Knowledge-based dialogue generation struggles with out-of-vocabulary  words since knowledge will be exact time or some strange proper nouns, which will not be seen by pre-trained language models. To deal with OOV problems, provided a method to generate words by adding the attention distribution into standard decoder output. It is also called the copy mechanism since the probability of input words would be copied as answers.",238
"  %    Recent years have witnessed the rapid advancement of online recruitment platforms. With the increasing amount of online recruitment data, more and more interview related studies have emerged such as person-job  fit and automatic analysis of asynchronous video interviews , which aim to enable automated job recommendation and candidate assessment. Among these studies, person-job fit is to casting the task as a supervised text match problem. Given a set of labeled data , it aims to predict the matching label between the candidate resumes and job description. More recently, deep learning has enhanced person-job fit methods by training more effective text match or text representations models. AVI is to determine whether the candidate is hirable by evaluating the answers of interview questions. In AVIs, an interview is usually considered as a sequence of questions and answers containing salient socials signals. To evaluate the candidates more comprehensively, AVI models will extract the features of video , text, and voice in the process of answering questions. In this work, we focus on the scoring of multiple QA pairs,  we only extract the features of text modality and define this task as the scoring competency of candidates rather than the score of whether or not to be employed. Based on the anatomy of the human interviewers' evaluation process, the solutions consist of two stages:  analyzing and evaluating individual QA pair one by one, then acquiring the evaluation status, and  grading the competency of the candidate based on the evaluation status of multiple QA pairs.      For the first stage, existing methods tend to employ text matching or attentional text matching algorithms to evaluate QA pairs, which feeds the concatenated representation of the question and the answer to the subsequent classifier. As we all know, questions in an asynchronous video interview are not limited to specific domains. That is to say, candidates can answer questions according to their work or study experience. In this way, the candidates' answers will be varied and it is difficult to evaluate the answer accurately only by text matching. Intuitively, it is more reasonable to evaluate QA pairs through the semantic interaction between questions and answers. A critical challenge along this line is how to reveal the latent relationships between each question and answer.  %Intuitively, experienced interviewers could discover the semantic-level correlation between interview questions and candidates' answers, then obtain a preliminary judgement on the answer to the current question, and finally give an assessment based on the judgements of several problems. Therefore,  %In this work, we propose a sentence-level reasoning GNN to assess the single QA pair at the semantic interaction level. Graph neural networks  can learn effective representation of nodes by encoding local graph structures and node attributes. Due to the compactness of model and the capability of inductive learning, GNNs are widely used in modeling relational data and logical reasoning. Recently, ~\citet{zhang2020efficient} proposed a GNN variant, Named ExpressGNN, to strike a nice balance between the representation power and the simplicity of the model in probabilistic logic reasoning.~\citet{ghosal2019dialoguegcn} constructed the DialogeGCN to address context propagation issues present in the RNN-based methods. Specifically, they leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Inspired by, we present a sentence-level relational GCN to represent the internal temporal and QA interaction dependency in the process of answering questions. %Recently, graph neural network or graph emebedding has attracted wide attention. Graph neural networks have been effective at tasks thought to have rich relational structure and can preserve global structure information of a graph in graph emebedding.   %In this work, we aim to address the task of automatically scoring the textual answer of candidates at the semantic interaction level.  %The automatic short answer scoring  is a task of estimating a score of a short text answer written as response to a given prompt on the basis of whether the answer satisfies the rubrics prepared by a human in advance. ASAS systems have mainly been constructed to markedly reduce the scoring cost of human rater.   %鐟欏嫬鍨鍫ユ閸掕泛鐣鹃敍灞芥礈濮濄倕顕梻顕顣介崪灞芥礀缁涙棃妫跨拠顓濈疅娴溿倓绨伴惃鍕閹烘ɑ妯夊妤佹纯閸旂娀鍣哥憰 %閸ョ偓膩閸ㄥ婀梻顕顣介幒銊ф倞娴犺濮熸稉濂僥ep learning has proven to be effective in long text NLP tasks. Due to the lack of information in the short sentence of the ASAS corpus, it seems not good enough in the ASAS task.  For the second stage of grading the candidate, based on the representation of QA pairs, exists methods prefer to encoder question-answer pairs as a sequence directly. However, this kind of approaches lead to insufficient interaction between the semantic information of question and answer pairs. Therefore, it is difficult to ensure the rationality and explainability of the evaluation. To mitigate this issue, in the first stage, we present a semantic-level graph attention network  to model the interaction states of each QA session.    %Automatic scoring of answer transcriptions in job interview aims to evaluate multiple question-answer pairs.  %To alleviate this limitation of previous approaches, To this end, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic scoring of answer transcriptions  in job interviews. Specifically, the proposed sentence-level relational graph convolutional neural network  is used to capture the contextual dependency, and the semantic-level Reasoning graph attention network  is applied to acquire the latent interaction states. And the contribution of our work can be summarized as follows:     {\bf Asynchronous Video Interviews} The asynchronous video interview is considered as one of the most essential tasks in talent recruitment, which forms a bridge between employers and candidates in fitting the eligible person for the right job.  developed a joint learning system to model job description, candidate resume, and interview assessment. It can effectively learn the representation perspectives of the different job interview process from the successful job interview records and then applied in person-job fit and interview question recommendation.  takes an interview process as a sequence of questions and answers and proposed a hierarchical attention model named HireNet to predict the hireability of the candidates. As far as we know, these approaches ignore the deep dependency between interview questions and answers.  With the development of neural networks, We argue that     {\bf Short Answer Scoring} Automatic short answer scoring  is a research subject of intelligent education, which is a hot field of natural language understanding. Methods for ASAS are driven with the help of deep learning techniques and domain-specific knowledge. Recently, have used InferSent and neural domain adaptation to obtain state-of-the-art results in the ASAS task.  proposed multiple data augmentation strategies to learn language representation and achieved a significant gain over benchmark models. It should be emphasized that, in ASAS tasks, the answer text is short and the domain is specific. For the ASAT task which contains several open-domain interview questions, the scoring of long-text answers is much more challenging.    {\bf Graph Neural Network} Graph neural networks have been successfully applied to several natural language processing tasks, such as text classification, machine translation, question generation and fact verification.  propose a graph-based evidence aggregating and reasoning  framework which enables information to transfer on a fully-connected evidence graph and then utilize different aggregators to collect multi evidence information. constructed a semantic-level graph for content selection and improved the performance over questions requiring reasoning over multiple facts. Inspired by previous researches, we proposed a hierarchical reasoning graph neural network to alleviate the issues of lacking interaction and semantic reasoning between questions and answers in the video job interview.    This paper describes our overall system that is evaluated in Track 1 of DSTC 9.  Pre-trained language models, ELECTRA and RoBERTa, are used as our base encoder, and task-specific components are applied to improve performance. In the released evaluation results, we rank second under objective metrics and rank fourth under human metrics. Considering the gap between validation and test set, it is worthwhile for us to further study how to generalize our model in a better way, that is, transferring our in-domain system to the out-of-domain scenario. 
"," {\bf Asynchronous Video Interviews} The asynchronous video interview is considered as one of the most essential tasks in talent recruitment, which forms a bridge between employers and candidates in fitting the eligible person for the right job.  developed a joint learning system to model job description, candidate resume, and interview assessment. It can effectively learn the representation perspectives of the different job interview process from the successful job interview records and then applied in person-job fit and interview question recommendation.  takes an interview process as a sequence of questions and answers and proposed a hierarchical attention model named HireNet to predict the hireability of the candidates. As far as we know, these approaches ignore the deep dependency between interview questions and answers.  With the development of neural networks, We argue that     {\bf Short Answer Scoring} Automatic short answer scoring  is a research subject of intelligent education, which is a hot field of natural language understanding. Methods for ASAS are driven with the help of deep learning techniques and domain-specific knowledge. Recently, have used InferSent and neural domain adaptation to obtain state-of-the-art results in the ASAS task.  proposed multiple data augmentation strategies to learn language representation and achieved a significant gain over benchmark models. It should be emphasized that, in ASAS tasks, the answer text is short and the domain is specific. For the ASAT task which contains several open-domain interview questions, the scoring of long-text answers is much more challenging.    {\bf Graph Neural Network} Graph neural networks have been successfully applied to several natural language processing tasks, such as text classification, machine translation, question generation and fact verification.  propose a graph-based evidence aggregating and reasoning  framework which enables information to transfer on a fully-connected evidence graph and then utilize different aggregators to collect multi evidence information. constructed a semantic-level graph for content selection and improved the performance over questions requiring reasoning over multiple facts. Inspired by previous researches, we proposed a hierarchical reasoning graph neural network to alleviate the issues of lacking interaction and semantic reasoning between questions and answers in the video job interview.",239
"  Social media is a unique source of information. On the one hand, their low cost, easy access and distribution speed make it possible to quickly share the news. On the other hand, the quality and reliability of social media news is difficult to verify . This is the source of a lot of false information that has a negative impact on society.   Over the past year, the world has been watching the situation developing around the novel coronavirus pandemic. The COVID-19 pandemic has become a significant newsworthy event of 2020. Therefore, news related to COVID-19 are actively discussed on social media and this topic generates a lot of misinformation. Fake news related to the pandemic have large-scale negative social consequences, they provoke huge public rumor spreading and misunderstanding about the COVID-19 and aggravate effects of the pandemic. Moreover, recent studies  show an increase in symptoms such as anxiety and depression in connection with the pandemic. This is closely related to the spread of misinformation, because fake news can be more successful when the population is experiencing a stressful psychological situation . The popularity of fake news on social media can rapidly increase, because the rebuttal is always published too late. In this regard, there is evidence that the development of tools for automatic COVID-19 fake news detection plays a crucial role in the regulation of information flows.  In this paper, we present our approach for the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in English  that attracted 433 participants on CodaLab. This approach achieved the weighted F1-score of 98.69  on the test set among 166 submitted teams in total.  The rest of the paper is organized as follows. A brief review of related work is given in Section 2. The definition of the task has been summarized in Section 3, followed by a brief description of the data used in Section 4. The proposed methods and experimental settings have been elaborated in Section 5. Section 6 contains the results and error analysis respectively. Section 7 is a conclusion.     In recent years, the task of detecting fake news and rumors is extremely relevant. False information spreading involves various research tasks, including: fact checking , topic credibility , fake news spreaders profiling , and manipulation techniques detection . Various technologies and approaches in this field range from traditional machine learning methods , to state-of-the-art transformers .  A overview of fake news detection approaches and challenges on social media has been discussed in . Many scholars have proposed their solutions to this problem in different subject areas . Up to now, a large number of studies in fake news detection have used supervised methods including models based on transformers-based architecture .   Some recent work have focused on detecting fake news about COVID-19. For example, the predictors of the sharing of false information about the pandemic are discussed in . In , a novel COVID-19 fact checking algorithm is proposed that retrieves the most relevant facts concerning user claims about particular facts. A number of studies have begun to examine COVID-19 fake news detection methods for non-English languages .  In addition, several competitions have been announced over the past year related to the analysis of posts about COVID-19 on social media .     In this paper, we propose a hierarchical reasoning graph neural network  for the automatic scoring of answer transcriptions  in the video job interview. The ASAT task is to score the competency of candidates based on several textual question-answer pairs. Unlike other matching based methods or frameworks, HRGNN can utilize the relational dependency of sentences in the questions and answers, and aggregate them in the semantic level with reasoning flow between different graph layers. Particularly, the proposed relational graph convolutional network  module constructs internal temporal dependency and question-answer interaction dependency to represent the relations between sentences in the question and the answer. And in the graph-based reasoning part, we propose a graph attention network to further aggregate semantic interactions between the question and the answer. Finally, we apply a GRU-based classifier to discriminate the candidate is competent or not. Empirical results with 10 random seeds show that our model achieves state-of-the-art on a Chinese real-world dataset .   We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the video job interview. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and then give the evaluation results combined with multiple interaction states. Recent studies tend to use text matching approaches to evaluate each QA pair roughly, which fails to take advantage of the semantic association between questions and answers. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level reasoning graph neural network to capture the latent semantic interaction of sentences in the question or the answer. Based on these graphs, we employ a semantic-level graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit with a global fusion mechanism to aggregates evidence of temporal QA pairs for the final score. Empirical results conducted on CHNAT  clearly validate that our proposed model significantly outperforms text-matching based benchmark models.   \begin{comment} 
","  In recent years, the task of detecting fake news and rumors is extremely relevant. False information spreading involves various research tasks, including: fact checking , topic credibility , fake news spreaders profiling , and manipulation techniques detection . Various technologies and approaches in this field range from traditional machine learning methods , to state-of-the-art transformers .  A overview of fake news detection approaches and challenges on social media has been discussed in . Many scholars have proposed their solutions to this problem in different subject areas . Up to now, a large number of studies in fake news detection have used supervised methods including models based on transformers-based architecture .   Some recent work have focused on detecting fake news about COVID-19. For example, the predictors of the sharing of false information about the pandemic are discussed in . In , a novel COVID-19 fact checking algorithm is proposed that retrieves the most relevant facts concerning user claims about particular facts. A number of studies have begun to examine COVID-19 fake news detection methods for non-English languages .  In addition, several competitions have been announced over the past year related to the analysis of posts about COVID-19 on social media .",240
"  	 	 	      	Medical dialogue system  aims to converse with patients to inquire additional symptoms beyond their self-reports and make a diagnosis automatically, which has gained increasing attention . 	It has a significant potential to simplify the diagnostic process and relieve the cost of collecting information from patients . Moreover, preliminary diagnosis reports generated by MDS may assist doctors to make a diagnosis more efficiently.   	Because of these considerable benefits, many researchers devote substantial efforts  to address critical sub-problems in  MDS, such as natural language understanding , dialogue policy learning, dialogue management, and make promising  progress to build a satisfactory MDS.   	 	 	Medical dialogue generation , which generates responses in natural language to request additional symptoms or make a diagnosis, is critical in MDS but rarely studied. 	Conventional generative dialogue models often employ neural sequence modeling and cannot be applied to the medical dialogue scenario directly in absence of medical knowledge. Recently, large-scale pre-training language models  over unsupervised corpora have achieved significant success.      However, fine-tuning such large language models in the medical domain requires sufficient task-specific data  so as to learn the correlations between diseases and symptoms. 	Unfortunately, as depicted in Fig., there are a large portion of diseases that only have a few instances in practice, which means that newly-coming diseases in the realistic diagnosis scenario are often under low-resource conditions. Therefore, it is highly desirable to transfer the diagnostic experience from high-resource diseases to others of data scarcity.  	Besides, existing knowledge-grounded approaches may fail to perform such transfer well, as they only learn one unified model for all diseases and ignore the specificity and relationships of different diseases. 	Finally, in practice, the disease-symptom relations of each disease may vary or evolve along with more cases, which is also not considered in prior works.    	Contributions.	To address the above challenges, we first propose an end-to-end  dialogue system for the low-resource medical dialogue generation. 	This model integrates three components seamlessly, a hierarchical context encoder,  a meta-knowledge graph reasoning  network and a graph-guided response generator. Among them, the context encoder encodes  the conversation into hierarchical representations. For MGR, it mainly contains a parameterized meta-knowledge graph, which is initialized by a prior commonsense graph and characterizes the correlations among diseases and symptoms.  When fed into the context information, MGR can adaptively evolve its meta-knowledge graph to reason the disease-symptom correlations and then predict related symptoms of the patient in the next response to further determine the disease. Finally, the response generator generates a response for symptoms request  under the guidance of the meta-knowledge graph.   	The second contribution is that we further develop a novel Graph-Evolving Meta-Learning  framework to  transfer the diagnostic experience in the low-resource scenario. Firstly, GEML trains the above medical dialogue model under the meta-learning framework. It regards generating responses to a handful of dialogues as a task and learns a meta-initialization for the above dialogue model that can fast adapt to each task of the new disease with limited dialogues. In this way, the learnt model initialization contains sufficient meta-knowledge\footnote{We name such knowledge as ``meta-knowledge"" since it is obtained through meta-training from different source diseases.} from all source diseases and can serve as a good model initialization to quickly transfer meta-knowledge to a new disease. More importantly, GEML also learns a good parameterized meta-knowledge  graph in the MGR module to characterize the disease-symptom relationships from source diseases. Concretely, under the meta learning framework, for each disease, GEML enriches the meta-knowledge graph via constructing a global-symptom graph from the online dialogue examples. In this way, the learnt meta-knowledge graph can bridge the gap between the commonsense medical graph and the real diagnostic dialogues and thus can be fast evolved for the new target disease. Thanks to graph evolving, the dialogue model can request patients for underlying symptoms more efficiently and thus improve the diagnostic accuracy. Besides,  GEML can also well address the real-world challenge that the disease-symptom correlations could vary along with more cases, since the meta-knowledge  graph is trainable based on collected dialogue examples.  	 	Finally, we  construct a large medical dialogue dataset, called Chunyu\footnote{Code and dataset are released at https://github.com/ha-lins/GEML-MDG.}.  	It covers 15  kinds of diseases and 12,842 dialogue examples totally, and  is much larger than the existing CMDD  medical dialogue dataset. The more challenging benchmark can better comprehensively evaluate the performance of medical dialogue systems.  Extensive experimental results on both datasets demonstrate the superiority of our method over the state-of-the-arts.    Medical Dialogue System .   	Recent research on MDS  mostly focus on the natural language understanding  or dialogue management  with the line of pipeline-based dialogue system. Various NLU problems have been studied to improve the MDS performance, \eg, ~entity inference , symptom extraction  and slot-filling . For medical dialogue management, most works focus on reinforcement learning   based task-oriented dialogue system.  	\citet{wei2018task} proposed to learn dialogue policy with RL to facilitate automatic diagnosis.  	\citet{xu2019end} incorporated the knowledge inference into dialogue management via RL. However,  no attention  has been paid  to the medical dialogue generation, which is a critical recipe in MDS. Differing from existing approaches, we investigate to build an end-to-end graph-guided medical dialogue generation model directly. 	     Knowledge-grounded Dialog Generation.  Recently, dialogue generation grounded on extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be derived from or open-domain knowledge graphs  or retrieved from unstructured documents . Different from them, our MDG model is built on the dedicated medical-domain knowledge graph and further require evolving it to satisfy the need for the real-world diagnosis. 	 	Meta-Learning.   By meta-training a model initialization from training tasks with the ability of fast adaptation to new tasks,  meta-learning has achieved promising results in many NLP areas, such as machine translation , task-oriented dialogues , and text classification \shortcite{obamuyide-vlachos-2019-model,wu-etal-2019-learning}.  But there is the few effort to devote meta-learning into MDS, which requires grounding on the external medical knowledge and reasoning for disease-symptom correlations. In this work, we employ the Reptile, one first-order model-agnostic meta learning approach,  because of its efficiency and effectiveness, and enhance it with the meta-knowledge graph reasoning and evolving. 	              In this work, we propose a simple but effective approach to COVID-19 fake news detection based on CT-BERT and ensembling learning. Our experiments confirmed that BERT-based models specialized in the subject area successfully cope with such tasks and perform high-quality binary classification.  The experimental results showed that our solution achieved 98.69\  of the weighted F1-score on test data and ranked in the first place in the Constraint@-AAAI2021 shared task. For future work, we can experiment with different training and data augmentation techniques. We can also apply and evaluate hybrid models combining BERT-based architectures with other methods of natural language processing .      ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.     
"," Medical Dialogue System .   	Recent research on MDS  mostly focus on the natural language understanding  or dialogue management  with the line of pipeline-based dialogue system. Various NLU problems have been studied to improve the MDS performance, \eg, ~entity inference , symptom extraction  and slot-filling . For medical dialogue management, most works focus on reinforcement learning   based task-oriented dialogue system.  	\citet{wei2018task} proposed to learn dialogue policy with RL to facilitate automatic diagnosis.  	\citet{xu2019end} incorporated the knowledge inference into dialogue management via RL. However,  no attention  has been paid  to the medical dialogue generation, which is a critical recipe in MDS. Differing from existing approaches, we investigate to build an end-to-end graph-guided medical dialogue generation model directly. 	     Knowledge-grounded Dialog Generation.  Recently, dialogue generation grounded on extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be derived from or open-domain knowledge graphs  or retrieved from unstructured documents . Different from them, our MDG model is built on the dedicated medical-domain knowledge graph and further require evolving it to satisfy the need for the real-world diagnosis. 	 	Meta-Learning.   By meta-training a model initialization from training tasks with the ability of fast adaptation to new tasks,  meta-learning has achieved promising results in many NLP areas, such as machine translation , task-oriented dialogues , and text classification \shortcite{obamuyide-vlachos-2019-model,wu-etal-2019-learning}.  But there is the few effort to devote meta-learning into MDS, which requires grounding on the external medical knowledge and reasoning for disease-symptom correlations. In this work, we employ the Reptile, one first-order model-agnostic meta learning approach,  because of its efficiency and effectiveness, and enhance it with the meta-knowledge graph reasoning and evolving.",241
" Commonsense question answering  is recently an attractive field in that it requires systems to understand the common sense information beyond words, which are normal to human beings but nontrivial for machines. There are plenty of datasets that are proposed for this purpose, for instance, CommonsenseQA , CosmosQA , WIQA . Different from traditional machine reading comprehension  tasks such as SQuAD  or NewsQA  that the key information for answering the questions is directly given by the context paragraph, solving commonsense questions requires a more comprehensive understanding of both the context and the relevant common knowledge, and further reasoning out the hidden logic between them. There are varieties of knowledge bases that meet the need, including text corpora like Wikipedia, and large-scale knowledge graphs .  Recent popular solution resorts to external supporting facts from such knowledge bases as evidence, to enhance the question with commonsense knowledge or the logic of reasoning . However, the quality of the supporting facts is not guaranteed, as some of them are weak in interpretability so that do not help the question answering. Specifically, current methods are mainly two-fold. The first group of methods  pre-train language models on those external supporting facts  so that the models could remember some of the common knowledge, which is empirically proven by Tandon et al. \shortcite{tandon2019wiqa} and Trinh and Le \shortcite{trinh2018do}. The second group of methods  incorporates the question with knowledge subgraphs or paths that carry information such as relation among concepts or show multi-hop reasoning process. The structured information is typically encoded via graph models such as GCN , and after which merged with the question features. Generally, current methods all handle evidence by brute force, without further selection or refinement according to the interpretability of the supporting facts. But as the example shown in Figure, some of the supporting facts do not interpret the question, regardless that they are semantically related. Thus, there is need for models that will further our processing of the evidence.  In this paper, we introduce a new recursive erasure memory network  that further refines the candidate supporting fact set. The REM-Net consists of three main components: a query encoder, an evidence generator, and a novel recursive erasure memory  module. Specifically, the query encoder is a pre-trained encoder that encodes the question. The evidence generator is a pre-trained generative model that produces candidate supporting facts based on the question. Compared with those retrieved supporting facts, the generated facts provides new question-specific information beyond the existing knowledge bases. The REM module refines the candidate supporting fact set by recursively matching the supporting facts and the question in feature space to estimate each fact's quality. This estimation helps both updating the question feature and the supporting fact set. The question feature is updated by a residual term, whereas the supporting fact set is updated by removing the low-quality facts. Compared with the standard attention mechanisms  that allocate weights to the supporting facts once, the multi-hop operation in REM module widens the gap of how much each supporting fact contributes to the question answering by the number of recursive steps their features are incorporated for the feature update. Therefore this procedure leads to a refined use of given supporting facts.  We conduct experiments on two commonsense QA benchmarks, WIQA  and CosmosQA . The experimental results demonstrate that REM-Net outperforms current methods, and the refined supporting facts are more qualified for the questions. Our contributions are mainly three-fold:       \paragraph{Commonsense Question Answering}  Similar to open-domain question answering tasks , commonsense question answering  requires open-domain information to support the answer prediction. But different from open-domain question answering tasks that the text comprehension is straightforward and the retrieved open-domain information is direct to the questions, in commonsense question answering tasks the open-domain information is more complicated in that they play a role as evidence to bridge the understanding gap in the commonsense questions. Current works leverage the open-domain information by whether incorporating external knowledge as evidence or training the models to generate evidence. \citet{lv2019graph} extracts knowledge from ConceptNet  and Wikipedia, and learns features with GCN  and graph attention .   \citet{zhong2019improving} retrieves ConceptNet   triplets and train two functions to measure direct and indirect connections between concepts. \citet{rajani2019explain} train a GPT  to generate reasonable evidence for the questions. During evaluation, the model generates evidence and predicts the multi-choice answers concurrently. \citet{ye2019align} automatically constructs a commonsense multi-choice dataset from ConceptNet triplets. However, the retrieved or generated evidence are usually not further refined, and some of them could be unnecessary or even confounding to answering the questions. The proposed model explores to refine the original evidence to discover those most supporting evidence to the commonsense questions and therefore provides stronger interpretations.  \paragraph{Memory Networks} Memory networks  are proposed to solve early reasoning problems such as bAbI ) that requires to locate useful information for answer prediction. The sentences are stored into memory slots and later selected for the question answering. Recently, multi-head attention memory networks  are proposed so that takes advantage of the transformer-based networks. Our proposed model is based on multi-head attention memory network that is modified with a recursive erasure manipulation to adapt to the commonsense question answering tasks for accurate evidence refinement.     figure 2      This paper shows that the MNMT architecture by itself has an impact on gender accuracy. Language-Specific outperforms Shared in two different language sets: English, German, Spanish, French and English, German, Spanish, French, and Russian. We observe that the difference in gender accuracy is higher in the language set including Russian.   Further interpretability analysis of the results shows that source embeddings in the Language-Specific architecture retain higher information on gender. Moreover, this architecture also keeps enough diversion in the attention, especially when including Russian. Both elements help in better inferring the correct gender.   Finally, a manual analysis shows that most of the errors are made assuming a masculine occupation instead of a feminine one. In contrast, the inverse error tends to come when there is a feminine version of that word with another meaning.    
"," \paragraph{Commonsense Question Answering}  Similar to open-domain question answering tasks , commonsense question answering  requires open-domain information to support the answer prediction. But different from open-domain question answering tasks that the text comprehension is straightforward and the retrieved open-domain information is direct to the questions, in commonsense question answering tasks the open-domain information is more complicated in that they play a role as evidence to bridge the understanding gap in the commonsense questions. Current works leverage the open-domain information by whether incorporating external knowledge as evidence or training the models to generate evidence. \citet{lv2019graph} extracts knowledge from ConceptNet  and Wikipedia, and learns features with GCN  and graph attention .   \citet{zhong2019improving} retrieves ConceptNet   triplets and train two functions to measure direct and indirect connections between concepts. \citet{rajani2019explain} train a GPT  to generate reasonable evidence for the questions. During evaluation, the model generates evidence and predicts the multi-choice answers concurrently. \citet{ye2019align} automatically constructs a commonsense multi-choice dataset from ConceptNet triplets. However, the retrieved or generated evidence are usually not further refined, and some of them could be unnecessary or even confounding to answering the questions. The proposed model explores to refine the original evidence to discover those most supporting evidence to the commonsense questions and therefore provides stronger interpretations.  \paragraph{Memory Networks} Memory networks  are proposed to solve early reasoning problems such as bAbI ) that requires to locate useful information for answer prediction. The sentences are stored into memory slots and later selected for the question answering. Recently, multi-head attention memory networks  are proposed so that takes advantage of the transformer-based networks. Our proposed model is based on multi-head attention memory network that is modified with a recursive erasure manipulation to adapt to the commonsense question answering tasks for accurate evidence refinement.     figure 2",242
"  Neural machine translation  has advanced significantly in recent years . In particular, the Transformer model has become popular for its well-designed architecture and the ability to capture the dependency among positions over the entire sequence . Early systems of this kind stack 4-8 layers on both the encoder and decoder sides , and the improvement often comes from the use of wider networks . More recently, researchers try to explore deeper models for Transformer. Encouraging results appeared in architecture improvements by creating direct pass from the low-level encoder layers to the decoder , and proper initialization strategies .  Despite promising improvements, problems still remain in deep NMT. Deep Transformer stacked by dozens of encoder layers always have a large number of parameters, which are computationally expensive and memory intensive. For example, a 48-layer Transformer is  larger than a 6-layer system and  slower for inference. It is difficult to deploy such models on resource-restricted devices, such as mobile phones. Therefore, it is crucial to compress such heavy systems into light-weight ones while keeping their performance.  Knowledge distillation is a promising method to address the issue. Although several studies  have attempted to compress the 12-layer BERT model through knowledge distillation, effectively compressing extremely deep Transformer NMT systems is still an open question in the MT community. In addition, these methods leverage sophisticated layer-wise distillation loss functions to minimize the distance between the teacher and the student models, which requires huge memory consumption and enormous training cost.  In this paper, we investigate simple and efficient compression strategies for deep Transformer. We propose a novel Transformer compression approach ) to transfer the knowledge from an extremely deep teacher model into a shallower student model. We disturb the computation order among each layer group during the teacher training phase, which is easy to implement and memory friendly. Moreover, to further enhance the performance of the teacher network, we introduce a vertical ``dropout''  into training by randomly omitting sub-layers to prevent co-adaptations of the over-parameterized teacher network. Although similar technique has been discussed in \citet{fan2019reducing}'s work, we believe that the finding here is complementary to theirs. Both Gpkd and regularization training methods can be well incorporated into the teacher training process, which is essential for obtaining a strong but light-weight student model.  \pgfdeclarepatternformonly{soft horizontal lines}{\pgfpointorigin}{\pgfqpoint{100pt}{1pt}}{\pgfqpoint{100pt}{3pt}}% {   \pgfsetstrokeopacity{0.3}   \pgfsetlinewidth{0.1pt}   \pgfpathmoveto{\pgfqpoint{0pt}{0.5pt}}   \pgfpathlineto{\pgfqpoint{100pt}{0.5pt}}   \pgfusepath{stroke} }  \pgfdeclarepatternformonly{soft crosshatch}{\pgfqpoint{-1pt}{-1pt}}{\pgfqpoint{6pt}{6pt}}{\pgfqpoint{5pt}{5pt}}% {   \pgfsetstrokeopacity{0.3}   \pgfsetlinewidth{0.4pt}   \pgfpathmoveto{\pgfqpoint{4.5pt}{0pt}}   \pgfpathlineto{\pgfqpoint{0pt}{4.5pt}}   \pgfpathmoveto{\pgfqpoint{0pt}{0pt}}   \pgfpathlineto{\pgfqpoint{4.5pt}{4.5pt}}   \pgfusepath{stroke} }  \definecolor{ugreen}{rgb}{0,0.5,0}    	%reoder 1 	\draw[line width=1pt,draw=red!30,fill=red!20] -- --  --  -- --  --  -- ; 	\draw[line width=1pt,draw=blue!35,fill=blue!20] -- --  --  -- --  --  -- ;  	%reoder 2 	\draw[line width=1pt,draw=red!30,fill=red!20] -- --  --  -- --  --  -- ; 	\draw[line width=1pt,draw=blue!35,fill=blue!20] -- --  --  -- --  --  -- ;  	%reoder 3 	\draw[line width=1pt,draw=red!30,fill=red!20] -- --  --  -- --  --  -- ; 	\draw[line width=1pt,draw=blue!35,fill=blue!20] -- --  --  -- --  --  -- ; 	\node[anchor=north,inner sep=0pt] at {}; 	\node[anchor=north,inner sep=0pt] at {};   	\node[anchor = south,font=;   	\node[anchor = south,font=;   	\node[anchor = south,font=;   	\node[anchor = south,font=\footnotesize]  at  {};   	\node[anchor = east,font=\footnotesize,rotate=-90]  at  {};   	\node[anchor = east,font=\footnotesize,rotate=-90]  at  {};   	\node[anchor = east,font=\footnotesize,rotate=-90]  at  {};    	\node[anchor = north,font=\scriptsize]  at {reorder};    	\node[anchor = west]   at  {};   	\node[anchor = west]  at  {};   	\node[anchor = west]  at  {};     %draw   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;    \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;    \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;    \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;      \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;   \draw[-latex',thick]  -- ;          \draw[-latex',very thick,red!40] ..controls + and +..;   \draw[-latex',very thick,blue!40] ..controls + and +..; 	\node[font=\tiny]  at  {sampling};    \node [auto,anchor=west,font=\footnotesize,rotate=-90] at {Teacher Training} ;    \node [auto,anchor=west,font=\footnotesize,rotate=-90] at {Student Training} ;       \node[auto,anchor=south,font=\footnotesize,inner sep=0pt] at  {Generate Skd-data} ;      \node[draw=gray!70,line width=1pt,fill=gray!10,single arrow,minimum height=2.2em,minimum width=4pt,single arrow head extend=3pt]  at {};    \node[draw=gray!70,line width=1pt,fill=gray!10,single arrow,minimum height=1.6em,minimum width=4pt,single arrow head extend=3pt,rotate=-90]  at {};    \end{tikzpicture}       \end{figure*}   We ran experiments on the WMT16 English-German, NIST OpenMT12 Chinese-English and WMT19 Chinese-English translation tasks. The Gpkd method compressed a 48-layer Transformer into a 6-layer system with almost no loss in BLEU. It outperformed the baseline with the same depth by + BLEU points. Through skipping sub-layer method, the teacher network achieved a BLEU score of  BLEU on the newstest2014 English-German task, and the student obtains additional improvements of  BLEU points.  % Moreover, we present a deep-encoder and shallow-decoder architecture  which achieves a speedup of  times with almost no loss in BLEU.         Deep neural networks play an important role in the resurgence of deep learning. It has been observed that increasing the depth of neural networks can drastically improve the performance of convolutional neural network-based systems . The machine translation communities follow this trend. For example, \citet{bapna-etal-2018-training} and \citet{wang-etal-2019-learning} shortened the path from upper-level layers to lower-level layers so as to avoid gradient vanishing/exploding. \citet{wu-etal-2019-depth} designed a two-stage approach with three specially designed components to build a 8-layer Transformer-Big system. \citet{zhang-etal-2019-improving} successfully trained a deep Post-Norm Transformer with carefully designed layer-wise initialization strategy. More attempts on initialization strategy emereged recently . Perhaps the most relevant work with us is \citet{fan2019reducing}'s work. They employed LayerDrop mechanism to train a 12-6 Transformer-Big and pruned sub-networks during inference without finetuning. Here we address a similar issue in deep Transformer, which has not been discussed yet. Beyond this, we present a new training strategy that can boost the deep system in a robust manner.  For model compression, there are many successful methods, such as quantization , knowledge distillation  , weight pruning  and efficient Transformer architecture . For Transformer models, \citet{sun-etal-2019-patient} proposed a novel approach to compressing a large BERT model into a shallow one via the Patient Knowledge Distillation method. \citet{jiao2019tinybert} achieved a better compression rate by richer supervision signals between the teacher network and the student network. However, these methods are not straightforwardly applicable to machine translation, they need simultaneously compute the logits of each layer in both the teacher and student networks, which consumes large GPU memory. In this work, we propose the  method to compress an extremely deep model into a baseline-like system, without any additional computation cost.     In this work, we curated a large-scale dialogue dataset, OSED, comprising of 1M emotional dialogues from movie subtitles. This dataset is more general-purpose, larger in size, and contains more fine-grained emotion categories and empathetic response intents than the existing emotional dialogue datasets. To facilitate annotation, we developed a dialogue emotion classifier capable of recognizing 32 fine-grained emotions and 9 empathetic response intents with significant accuracy. It was trained on movie dialogues initially annotated using human computation and extended using self-labeling, and sentence similarity approaches. As future work, we intend to extend the taxonomy of empathetic response intents using new labels discovered during this process and utilize the OSED dataset to develop a controllable neural chatbot capable of generating empathetic responses during social chitchat.       
","    Deep neural networks play an important role in the resurgence of deep learning. It has been observed that increasing the depth of neural networks can drastically improve the performance of convolutional neural network-based systems . The machine translation communities follow this trend. For example, \citet{bapna-etal-2018-training} and \citet{wang-etal-2019-learning} shortened the path from upper-level layers to lower-level layers so as to avoid gradient vanishing/exploding. \citet{wu-etal-2019-depth} designed a two-stage approach with three specially designed components to build a 8-layer Transformer-Big system. \citet{zhang-etal-2019-improving} successfully trained a deep Post-Norm Transformer with carefully designed layer-wise initialization strategy. More attempts on initialization strategy emereged recently . Perhaps the most relevant work with us is \citet{fan2019reducing}'s work. They employed LayerDrop mechanism to train a 12-6 Transformer-Big and pruned sub-networks during inference without finetuning. Here we address a similar issue in deep Transformer, which has not been discussed yet. Beyond this, we present a new training strategy that can boost the deep system in a robust manner.  For model compression, there are many successful methods, such as quantization , knowledge distillation  , weight pruning  and efficient Transformer architecture . For Transformer models, \citet{sun-etal-2019-patient} proposed a novel approach to compressing a large BERT model into a shallow one via the Patient Knowledge Distillation method. \citet{jiao2019tinybert} achieved a better compression rate by richer supervision signals between the teacher network and the student network. However, these methods are not straightforwardly applicable to machine translation, they need simultaneously compute the logits of each layer in both the teacher and student networks, which consumes large GPU memory. In this work, we propose the  method to compress an extremely deep model into a baseline-like system, without any additional computation cost.",243
"  {S}{emantic} role labeling , also known as shallow semantic parsing, conveys the meaning of a sentence by forming a predicate-argument structure for each predicate in a sentence, which is generally described as the answer to the question ""Who did what to whom, where and when?"". The relation between a specific predicate and its argument provides an extra layer of abstraction beyond syntactic dependencies  , such that the labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Given a sentence in Figure , SRL pipeline framework consists of 4 subtasks, including predicate identification , predicate disambiguation , arguments identification  and arguments classification . SRL is a core task of natural language processing  having wide range of applications such as neural machine translation , information extraction , question answering , emotion recognition from text , document summarization  etc.   Semantic role labeling can be categorized into two categories, span and dependency. Both types of SRL are useful for formal semantic representations but dependency based SRL is better for the convenience and effectiveness of semantic machine learning. Johansson and Nugues  concluded that the best dependency based SRL system outperforms the best span based SRL system through gold syntactic structure transformation. The same conclusion was also verified by Li et al.  through a solid empirical verification. Furthermore, since 2008, dependency based SRL has been more studied as compared to span based SRL. With this motivation, we focus on dependency based SRL, which is mainly popularized by CoNLL-2008 and CoNLL-2009 shared tasks .   The traditional approaches to SRL focus on feature engineering which struggles in apprehending discriminative information  while neural networks are proficient enough to extract features automatically . Specifically, since large scale empirical verification of Punyakanok et al. , syntactic information has been proven to be extremely beneficial for SRL task. Later works  achieve satisfactory performance for SRL with syntax-agnostic models which creates conflict with the long-held belief that syntax is essential for high-performance SRL .  The study of Li et al.  shows that the empirical results from neural models on the less importance of syntax indicate a potential challenge and despite the satisfactory performance of syntax-agnostic SRL systems, the reasons behind the absence of syntax in these models are three-fold. First, the effective incorporation of syntax in neural SRL models is quite challenging as compared to traditional approaches. Second, neural SRL models may cover partial syntactic clues more or less. Third, syntax has always been a complicated formalism in linguistics and its not easy to encode syntax for later usage. %Despite the satisfactory performance of syntax-agnostic SRL models, the reasons behind the absence of syntax in these models are two-fold. First, the effective incorporation of syntax information in neural SRL models is quite challenging. Second, the unreliability of syntactic parsers on account of the risk of erroneous syntactic input may lead to error proliferation. This has been proven by Li et al.  through a strong empirical verification. They show that the effective method of syntax incorporation and the high quality of syntax can promote SRL performance.%     Semantic role labeling was pioneered by Gildea and Jurafsky . In early days of SRL research, a substantial attention has been paid to featured engineering . Pradhan et al.  deploy the SVM classifier and combine features from different syntactic parses, while Zhao et al.  use sets of language-specific features for SRL task. Li et al.  integrate features driven from verbal SRL architecture. Bj{\""o}rkelund et al.  propose a beam search in the first stage of their system to label arguments, reranker in the second stage and then combine these scores in the third stage to label arguments for each predicate.  Yang and Zong  learn generalized feature vectors for arguments with a strong intuition that arguments occurring in the same syntactic positions bear the same syntactic roles. Che et al.  use a hybrid convolution tree kernel to learn link feature between argument and predicate and syntactic structure features to perform SRL task. Li and Zhou  present a unified framework for SRL for verbal and nominal predicates. Yang et al.  use Bi-directional Projection  method to perform bilingual semantic role labeling.  With the recent success of neural networks , a number of neural network based SRL systems have been proposed . Foland and Martin  use a convolutional and time-domain neural network to develop a semantic role labeler. FitzGerald et al.  present a neural network to jointly embed arguments and their semantic roles, akin to the work  which presents a tensor based approach to induce compact feature representation of the words and their corresponding relations.  Recently, many researchers proposed syntax agnostic models for SRL   and achieve favorable results without using syntax. Cai et al  use a biaffine attention model to propose a full end-to-end syntax agnostic model for SRL. While researchers have been able to produce satisfactory results without syntax, many efforts have been made to effectively integrate syntax in SRL systems. Roth and Lapata  modeled the syntactic information through dependency path embeddings to achieve notable success. Marcheggiani and Titov  deployed a graph convolutional neural network, while Qian et al.  used SA-LSTM to encode syntactic information in sentences.  Li et al  presented various ways of deploying syntactic information and concluded that the effective integration of syntax can boost SRL performance.  In this work, we follow Li et al  to integrate syntax information by using a modified version of Tree LSTM. Owing to the recent success of CNNs in NLP , we integrate adaptive convolution via a filter generation network in our SRL model. The ability of the filter generation network to produce filters conditioned on inputs allows the model to extract important syntactic features encoded by BiLSTM and Tree-LSTM encoder. We further study the effect of a hashing technique on the compression of a filter generation network in terms of trainable parameters.        Our contributions in this work are two folds.  We propose a  method to compress the deep model into a shallower one with minor performance sacrifice, which outperforms the  method by a large margin.  The proposed Skipping Sub-Layer method reduces the overfitting problem when training extremely deep encoder systems by randomly omitting sub-layers during training phase. The experimental results on three widely-used benchmarks validate the effectiveness of the proposed methods. After the incorporating of two methods, the strong but light-weight student models show competitive performance which is application friendly.  
"," Semantic role labeling was pioneered by Gildea and Jurafsky . In early days of SRL research, a substantial attention has been paid to featured engineering . Pradhan et al.  deploy the SVM classifier and combine features from different syntactic parses, while Zhao et al.  use sets of language-specific features for SRL task. Li et al.  integrate features driven from verbal SRL architecture. Bj{\""o}rkelund et al.  propose a beam search in the first stage of their system to label arguments, reranker in the second stage and then combine these scores in the third stage to label arguments for each predicate.  Yang and Zong  learn generalized feature vectors for arguments with a strong intuition that arguments occurring in the same syntactic positions bear the same syntactic roles. Che et al.  use a hybrid convolution tree kernel to learn link feature between argument and predicate and syntactic structure features to perform SRL task. Li and Zhou  present a unified framework for SRL for verbal and nominal predicates. Yang et al.  use Bi-directional Projection  method to perform bilingual semantic role labeling.  With the recent success of neural networks , a number of neural network based SRL systems have been proposed . Foland and Martin  use a convolutional and time-domain neural network to develop a semantic role labeler. FitzGerald et al.  present a neural network to jointly embed arguments and their semantic roles, akin to the work  which presents a tensor based approach to induce compact feature representation of the words and their corresponding relations.  Recently, many researchers proposed syntax agnostic models for SRL   and achieve favorable results without using syntax. Cai et al  use a biaffine attention model to propose a full end-to-end syntax agnostic model for SRL. While researchers have been able to produce satisfactory results without syntax, many efforts have been made to effectively integrate syntax in SRL systems. Roth and Lapata  modeled the syntactic information through dependency path embeddings to achieve notable success. Marcheggiani and Titov  deployed a graph convolutional neural network, while Qian et al.  used SA-LSTM to encode syntactic information in sentences.  Li et al  presented various ways of deploying syntactic information and concluded that the effective integration of syntax can boost SRL performance.  In this work, we follow Li et al  to integrate syntax information by using a modified version of Tree LSTM. Owing to the recent success of CNNs in NLP , we integrate adaptive convolution via a filter generation network in our SRL model. The ability of the filter generation network to produce filters conditioned on inputs allows the model to extract important syntactic features encoded by BiLSTM and Tree-LSTM encoder. We further study the effect of a hashing technique on the compression of a filter generation network in terms of trainable parameters.",244
" Exponential growths of micro-blogging sites and social media not only provide platforms for empowering freedom of expressions and individual voices, but also enables people to express anti-social behavior such as online harassment, cyberbullying, rumors, and spreading hatred statements. %In recent years, micro-blogging sites and social media sites have grown exponentially, enabling the users to express anti-social behavior, false political or religious rumor, and spreading hatred activities. Besides, abusive or threatening speech that expresses prejudice against a certain group, which religious, political, geopolitical, personal, and gender abuse are very common and on the basis of race, religion, and sexual orientation are getting pervasive. United Nations Strategy and Plan of Action on Hate Speech defines hate speech as ``any kind of communication in speech, writing or behaviour, that attacks or uses pejorative or discriminatory language with reference to a person or a group on the basis of who they are, in other words, based on their religion, ethnicity, nationality, race, colour, descent, gender or other identity factor''.  Bengali is spoken by 230 million people in Bangladesh and India, making it one of the major languages in the world. Although, a rich language with a lot of diversity, Bengali is severely low-resourced for natural language processing~, which is due to the scarcity of computational resources such as language models, labeled datasets, and efficient machine learning~ methods required for different NLP tasks. Similar to other major languages like English, the use of hate speech in Bengali is also getting rampant. This is mainly due to unrestricted access and use of social media and digitalization. Some examples of Bengali hate speech and their respective English translations are shown in \cref{cdec_wf3} that are either directed towards a specific person or entity or generalized towards a group. These examples signify how severe Bengali hateful statements could be. Nevertheless, there is a potential chance that these could lead to serious consequences such as hate crimes, regardless of languages, geographic locations, or ethnicity.    Automatic identification of hate speech and creating awareness among people is very challenging. However, manual reviewing and verification from a vast amount of online content is not only labor-intensive but also time-consuming. Nevertheless, accurate identification requires automated, robust, and efficient machine learning~ methods. Compared to traditional ML and neural network~-based approaches, state-of-the-art~ language models are becoming increasingly effective. On a serious drawback: a prediction made by many models can neither be traced back to the input, nor it is clear why the output is transformed in a certain way. This makes even the most efficient DNN models `black-box' methods. On the other hand, the General Data Protection Regulation~ by the European Parliament enforces the `right to explanation', which prohibits the use of ML for automated decisions unless a clear explanation of the logic used to make each decision is well explained. Therefore, how a prediction is made by an algorithm should be as transparent as possible in order to gain human trust.    %Recent research efforts from both the NLP and ML communities have proven to be very useful for well-resourced languages like English. %Nevertheless, accurate identification requires automated, robust, and efficient machine learning~ methods. As state-of-the-art language models becoming increasingly effective, their decisions should be made as transparent as possible in order to improve human trust. %Some of these techniques are based on the model閳ユ獨 local gradient information while other methods seek to redistribute the function閳ユ獨 value on the input variables, typically by reverse propagation in the neural network graph. Bach et al. proposed specific propagation rules for neural networks . These rules were shown to produce better explanations than e.g. gradient-based techniques not only for computer vision but also text data. To overcome the shortcomings of `black-box'-based methods and inspired by the outstanding success of transformer language models~, we propose an explainable approach for hate speech detection from under-resourced Bengali language. Our approach is based on the ensemble of several BERT variants, including monolingual Bangla BERT-base, m-BERT~, and XLM-RoBERTa. Further, we not only provide both global and local explanations of the predictions, in a post-hoc fashion but also provide the measure of explanations in terms of faithfulness.  The rest of the paper is structured as follows: \Cref{rw} reviews related work on hate speech and Bengali word embedding. \Cref{section:3} describes the data collection and annotation process. \Cref{nettwork} describes the process of Bengali neural embedding, network construction, and training. \Cref{er} illustrates experiment results, including a comparative analysis with baseline models on all datasets. \Cref{con} summarizes this research with potential limitations and points some possible outlook before concluding the paper.      \todo[inline]{this section will be written tomorrow} When it comes to major languages like English, numerous works have been proposed for accurate identification of hate speech that is based on ML and DNN-based approaches. Classic methods that rely on manual feature engineering, e.g., support vector machines~, Na{\""i}ve Bayes~, k-nearest neighbors~, logistic regression~, decision trees~, random forest~, and gradient boosted trees~. Approaches based on deep neural networks~ that learn multilayers of abstract features from raw texts, which are primarily based on convolutional~ or long short-term memory~ networks.  These approaches are mostly deep learning~ based, referring to the depth of the neural networks used. Nevertheless, despite the growth of studies and existing ML approaches, under-resourced languages such as Bengali lacks resources such as rich word embedding models and comparative evaluation on NLP tasks. These approaches, in comparison with DL-based methods, are rather incomparable because the efficiency of linear models at dealing with billions of such texts proven less accurate and unscalable.  , which is probably another primary reason.  CNN and LSTM are two popular DNN architectures: CNN is an effective feature extractor, whereas LSTM suitable for modeling orderly sequence learning problems.  We also observe the use of LSTM or GRU with pre-trained \texttt{fastText} embeddings to fed into a CNN with max-pooling to produce input vectors for a neural network.  , because, in the context of the text    classification, CNN extracts word or character combinations, e.g., n-grams, and LSTM learns a long-range word or character dependencies in texts.  In theory, \texttt{Conv-LSTM} is a robust architecture to capture long-term dependencies between features extracted by CNN and found more effective than structures solely based on CNN or LSTM, where the class of a word sequence depends on its preceding word sequence.   class. While each type of network has relative advantages, few works have explored combining both architectures into a single network. Except for a few restricted transfer learning settings that achieved higher classification accuracy than a single neural network.  However, accurate identification of hate speech in Bengali is still a challenging task. Consequently, only a few restrictive approaches have been proposed, so far. Romim et al. prepared a dataset of 30K comments, making it one of the largest datasets for identifying offensive and hateful statements. However, this dataset has several issues. Firstly, it is very imbalanced, where the ratio of hate speech and non-hate speech ratio is 10K:20K. Secondly, the majority of the hate speech is shorter in length and word count compared to non-hate statements. Besides, this study has several potential limitations. Firstly, their approach merely achieved moderately high accuracy at identifying offensive or hateful statements, giving an accuracy of 82\ . Secondly, their approach is `black-box' methods without showing how predictions are made.    Recently, Ismam S. et al. collected hateful comments from Facebook and annotated 5,126 hateful statements. Subsequently, they classified them into six classes閳 hate speech, communal attack, inciteful, religious hatred, political comments, and religious comments. Their approach based on GRU-based DNN achieved an accuracy of 70.10\  only, making it not up to the mark. In a recent approach, we provided classification benchmarks for document classification, sentiment analysis, and hate speech detection in Bengali. The approach, by combining fastText embeddings with multichannel Conv-LSTM network architecture, is probably the first work among a few other studies on hate speech detection. The Conv-LSTM architecture by combining fastText embeddings performed much better compared to Word2Vec and GloVe models, as fastText works well with rare words such that even if a word was not seen during the training, it can be broken down into n-grams to get its corresponding embeddings.  Unfortunately, all of these restrictive approaches are `black-box' methods without showing how a prediction is made by an algorithm. Interpretable methods put more emphasis on the transparency and traceability of opaque DNN models. For example, with layer-wise relevance propagation~ relevant parts of inputs to, and representations in, a neural network that caused a result, can be highlighted. To mitigate such opaqueness and to improve explainability in hate speech identification, Binny et al. proposed `HateXplain' and prepared a benchmark dataset for explainable hate speech detection. Based on SotA methods, they observe that high classification accuracy is not everything, but also high on explainability is desired. They measured the explainability of an NLP model in terms of plausibility and faithfulness that are based on human rationales for training. Inspired by their study and SotA interpretability techniques such as sensitivity analysis~ and LRP, we propose a novel approach called \texttt{DeepHateExplainer} for hate speech detection from Bengali language with more reliably and accurately.  In our approach, Bengali texts are first comprehensively preprocessed, before classifying them into political, personal, geopolitical, and religious hates, by employing a neural ensemble of different transformer-based neural architectures such as monolingual Bangla BERT-base, multilingual BERT~-cased/uncased, and XLM-RoBERTa. Then, we identify important terms with SA and LRP to provide human-interpretable explanations. Subsequently, we provide explanations of hate speech detection, covering both global and local explainability. Finally, we measure the explainability of the hate speech detection in terms of two metrics called comprehensiveness and sufficiency, i.e., faithfulness. Besides, we trained several ML~ and DNN~ baseline models. To the end, \texttt{DeepHateExplainer}  focus on algorithmic transparency and explainability, with the following assumptions:      We apply the sensitivity analysis~ and layer-wise relevance propagation~ methods to a bidirectional LSTM model~ to identify the most and least important terms that are responsible for attributing specific types of hates.    This paper formally introduces the task of universal representation learning and then presents a pre-trained language model for such a purpose to map different granular linguistic units into the same vector space where similar sequences have similar representations and enable unified vector operations among different language hierarchies.   In detail, we focus on the less concentrated language representation, seeking to learn a uniform vector form across different linguistic unit hierarchies. Far apart from learning either word only or sentence only representation, our method extends BERT's masking and training objective to a more general level, which leverage information from sequences of different lengths in a comprehensive way and effectively learns a universal representation from words, phrases to sentences.   Overall, our proposed BURT outperforms its baselines on a wide range of downstream tasks with regard to sequences of different lengths in both English and Chinese languages. We especially provide an universal analogy task, an insurance FAQ dataset and an NLG dataset for extensive evaluation, where our well-trained universal representation model holds the promise for demonstrating accurate vector arithmetic with regard to words, phrases and sentences and in real-world retrieval applications.       use section* for acknowledgment   \ifCLASSOPTIONcompsoc       The Computer Society usually uses the plural form     
","   \todo[inline]{this section will be written tomorrow} When it comes to major languages like English, numerous works have been proposed for accurate identification of hate speech that is based on ML and DNN-based approaches. Classic methods that rely on manual feature engineering, e.g., support vector machines~, Na{\""i}ve Bayes~, k-nearest neighbors~, logistic regression~, decision trees~, random forest~, and gradient boosted trees~. Approaches based on deep neural networks~ that learn multilayers of abstract features from raw texts, which are primarily based on convolutional~ or long short-term memory~ networks.  These approaches are mostly deep learning~ based, referring to the depth of the neural networks used. Nevertheless, despite the growth of studies and existing ML approaches, under-resourced languages such as Bengali lacks resources such as rich word embedding models and comparative evaluation on NLP tasks. These approaches, in comparison with DL-based methods, are rather incomparable because the efficiency of linear models at dealing with billions of such texts proven less accurate and unscalable.  , which is probably another primary reason.  CNN and LSTM are two popular DNN architectures: CNN is an effective feature extractor, whereas LSTM suitable for modeling orderly sequence learning problems.  We also observe the use of LSTM or GRU with pre-trained \texttt{fastText} embeddings to fed into a CNN with max-pooling to produce input vectors for a neural network.  , because, in the context of the text    classification, CNN extracts word or character combinations, e.g., n-grams, and LSTM learns a long-range word or character dependencies in texts.  In theory, \texttt{Conv-LSTM} is a robust architecture to capture long-term dependencies between features extracted by CNN and found more effective than structures solely based on CNN or LSTM, where the class of a word sequence depends on its preceding word sequence.   class. While each type of network has relative advantages, few works have explored combining both architectures into a single network. Except for a few restricted transfer learning settings that achieved higher classification accuracy than a single neural network.  However, accurate identification of hate speech in Bengali is still a challenging task. Consequently, only a few restrictive approaches have been proposed, so far. Romim et al. prepared a dataset of 30K comments, making it one of the largest datasets for identifying offensive and hateful statements. However, this dataset has several issues. Firstly, it is very imbalanced, where the ratio of hate speech and non-hate speech ratio is 10K:20K. Secondly, the majority of the hate speech is shorter in length and word count compared to non-hate statements. Besides, this study has several potential limitations. Firstly, their approach merely achieved moderately high accuracy at identifying offensive or hateful statements, giving an accuracy of 82\ . Secondly, their approach is `black-box' methods without showing how predictions are made.    Recently, Ismam S. et al. collected hateful comments from Facebook and annotated 5,126 hateful statements. Subsequently, they classified them into six classes闁 hate speech, communal attack, inciteful, religious hatred, political comments, and religious comments. Their approach based on GRU-based DNN achieved an accuracy of 70.10\  only, making it not up to the mark. In a recent approach, we provided classification benchmarks for document classification, sentiment analysis, and hate speech detection in Bengali. The approach, by combining fastText embeddings with multichannel Conv-LSTM network architecture, is probably the first work among a few other studies on hate speech detection. The Conv-LSTM architecture by combining fastText embeddings performed much better compared to Word2Vec and GloVe models, as fastText works well with rare words such that even if a word was not seen during the training, it can be broken down into n-grams to get its corresponding embeddings.  Unfortunately, all of these restrictive approaches are `black-box' methods without showing how a prediction is made by an algorithm. Interpretable methods put more emphasis on the transparency and traceability of opaque DNN models. For example, with layer-wise relevance propagation~ relevant parts of inputs to, and representations in, a neural network that caused a result, can be highlighted. To mitigate such opaqueness and to improve explainability in hate speech identification, Binny et al. proposed `HateXplain' and prepared a benchmark dataset for explainable hate speech detection. Based on SotA methods, they observe that high classification accuracy is not everything, but also high on explainability is desired. They measured the explainability of an NLP model in terms of plausibility and faithfulness that are based on human rationales for training. Inspired by their study and SotA interpretability techniques such as sensitivity analysis~ and LRP, we propose a novel approach called \texttt{DeepHateExplainer} for hate speech detection from Bengali language with more reliably and accurately.  In our approach, Bengali texts are first comprehensively preprocessed, before classifying them into political, personal, geopolitical, and religious hates, by employing a neural ensemble of different transformer-based neural architectures such as monolingual Bangla BERT-base, multilingual BERT~-cased/uncased, and XLM-RoBERTa. Then, we identify important terms with SA and LRP to provide human-interpretable explanations. Subsequently, we provide explanations of hate speech detection, covering both global and local explainability. Finally, we measure the explainability of the hate speech detection in terms of two metrics called comprehensiveness and sufficiency, i.e., faithfulness. Besides, we trained several ML~ and DNN~ baseline models. To the end, \texttt{DeepHateExplainer}  focus on algorithmic transparency and explainability, with the following assumptions:      We apply the sensitivity analysis~ and layer-wise relevance propagation~ methods to a bidirectional LSTM model~ to identify the most and least important terms that are responsible for attributing specific types of hates.",245
" Many seemingly convincing rumors such as ``Most humans only use 10 percent of their brain'' are widely spread, but ordinary people are not able to rigorously verify them by searching for scientific literature. In fact, it is not a trivial task to verify a scientific claim by providing supporting or refuting evidence rationales, even for domain experts.  %Such The situation worsens as misinformation is proliferated  %by the  on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes more and more crucial for combating  %against  the spread of misinformation.  %There are many existing datasets and %the corresponding  %systems for fact-verification tasks %, emphasizing on  %in various domains, such as Wikipedia , social media , and politics . These tasks  %are  The existing fact-verification tasks usually consist of three sub-tasks: document retrieval, rationale sentence extraction, and fact-verification. However, due to the nature of scientific literature that requires domain knowledge, it is challenging to collect a large scale scientific fact-verification dataset, and further, to perform fact-verification under a low-resource setting with limited training data. \citet{Wadden2020FactOF} collected a scientific claim-verification dataset, SciFact, and proposed a scientific claim-verification task: given a scientific claim, find evidence sentences that support or refute  %such the claim  %from  in a corpus of scientific paper abstracts. \citet{Wadden2020FactOF} also proposed a simple, pipeline-based, sentence-level model, VeriSci, as a baseline solution based on \citet{deyoung2019eraser}.  %Despite the simplicity of VeriSci ,  VeriSci is a pipeline model that runs modules for abstract retrieval, rationale sentence selection, and stance prediction sequentially, and thus the error generated from  %the an upstream module may propagate to the downstream modules. To overcome this drawback, we hypothesize that a module jointly optimized on multiple sub-tasks may mitigate the error-propagation problem to improve the overall performance.  %On the other hand,  In addition, we observe that a complete set of rationale sentences usually contains multiple inter-related sentences from the same paragraph. Therefore, we propose a novel, paragraph-level, multi-task learning model for the SciFact task.  In this work, we employ compact paragraph encoding, a novel strategy of computing sentence representations using BERT-family models. We directly feed an entire paragraph as a single sequence to BERT, so that the encoded sentence representations are already contextualized on the neighbor sentences by taking advantage of the attention mechanisms in BERT. In addition, we jointly train the modules for rationale selection and stance prediction as multi-task learning  by leveraging the confidence score of rationale selection as the attention weight of the stance prediction module. Furthermore, we compare two methods of transfer learning that mitigate the low-resource issue: pre-training and domain adaptation . Our experiments show that: % the compact paragraph encoding method is beneficial over separately computing sentence embeddings, and  with negative sampling, the joint training of rationale selection and stance prediction is beneficial over the pipeline solution. %\todo{you may want to create a list of contribution. -Violet}      Fact-verification has been widely studied. There are many datasets available on various domains , among which the most influential one is FEVER shared task , which aims to develop systems to check the veracity of human-generated claims by extracting evidences from Wikipedia. Most existing systems  leverages a three-step pipeline approach by building modules for each of the step: document retrieval, rationale selection and fact verification. Many of them focus on the claim verification step , such as KGAT , one of the top models on FEVER leader board. On the other hand, there are some attempts on jointly optimizing rationale selection and stance prediction. TwoWingOS  leverages attentive CNN  to inter-wire two modules, while \citet{hidey2020deseption} used a single pointer network  for both sub-tasks. We propose another variation that directly links two modules by a dynamic attention mechanism.  Because   is a scientific version of FEVER , systems designed for FEVER can be applied to  in principle. However, as a fact-verification task in scientific domain,  task has inherited the common issue of lacking sufficient data, which can be mitigated with transfer learning by leveraging language models and introducing external dataset. The baseline model by \citet{Wadden2020FactOF} leverages Roberta-large  fine-tuned on FEVER dataset , while   leverages T5  and fine-tuned on MS MARCO dataset . In this work, in addition to fine-tuning Roberta-large on FEVER, we also explore domain adaptation  to mitigate the low resource issue.    We present a novel sentence representation learning method Conditional Masked Language Modeling  for training on large scale unlabeled corpus. CMLM outperforms the previous state-of-the-art English sentence embeddings models, including those trained with supervised signals. For multilingual representations learning, we discover that co-training CMLM with bitext retrieval and cross-lingual NLI finetuning achieves state-of-the-art performance. We also discover that multilingual representations have the same language bias and principal component removal  can eliminate the bias by separating language identity information from semantics.  
"," Fact-verification has been widely studied. There are many datasets available on various domains , among which the most influential one is FEVER shared task , which aims to develop systems to check the veracity of human-generated claims by extracting evidences from Wikipedia. Most existing systems  leverages a three-step pipeline approach by building modules for each of the step: document retrieval, rationale selection and fact verification. Many of them focus on the claim verification step , such as KGAT , one of the top models on FEVER leader board. On the other hand, there are some attempts on jointly optimizing rationale selection and stance prediction. TwoWingOS  leverages attentive CNN  to inter-wire two modules, while \citet{hidey2020deseption} used a single pointer network  for both sub-tasks. We propose another variation that directly links two modules by a dynamic attention mechanism.  Because   is a scientific version of FEVER , systems designed for FEVER can be applied to  in principle. However, as a fact-verification task in scientific domain,  task has inherited the common issue of lacking sufficient data, which can be mitigated with transfer learning by leveraging language models and introducing external dataset. The baseline model by \citet{Wadden2020FactOF} leverages Roberta-large  fine-tuned on FEVER dataset , while   leverages T5  and fine-tuned on MS MARCO dataset . In this work, in addition to fine-tuning Roberta-large on FEVER, we also explore domain adaptation  to mitigate the low resource issue.",246
" Self attention networks  have been widely studied on many natural language processing  tasks, such as machine translation , language modeling  and natural language inference . It is well accepted that SANs can leverage both the local and long-term dependencies through the attention mechanism, and are highly parallelizable thanks to their position-independent modeling method.  However, such position-independent models are incapable of explicitly capturing the boundaries between sequences of words, thus overlook the structure information that has been proven to be robust inductive biases for modeling texts . Unlike RNNs that model sequential structure information of words by using memory cells, or CNNs that focus on learning local structure dependency of words via convolution kernels, SANs learn flexible structural information in an indirect way almost from scratch. One way to integrate structural information into SAN models is via pre-training, such as BERT , which learns to represent sentences by using unsupervised learning tasks on the large-scale corpus. Recent studies  have shown the ability of pre-training models on capturing structure information of sentences.  Another method to deal with structural information is introducing structure priors into SANs by mask strategies. \citeauthor{shen2018disan} \shortcite{shen2018disan} proposed the directional self-attention mechanism, which employs two SANs with the forward and backward masks respectively to encode temporal order information. \citeauthor{guo2019gaussian} \shortcite{guo2019gaussian} introduced the Gaussian prior to the transformers for capturing local compositionality of words. Admittedly, structure priors can strengthen the model's capability of modeling sentences and meanwhile assist in capturing proper dependencies. With the help of these learned structure priors, SANs can model sentences accurately even in resource-constrained conditions.    Though these models get success on many NLP tasks, these studies commonly focus on integrating one single type of structure priors into SANs, thus fail at making full use of multi-head attentions. One straightforward advantage of using the multi-head attentions lies in the fact that different heads convey different views of texts . In other words, multi-head attentions enable the model to capture the information of texts at multiple aspects, which in return brings thorough views when modeling the texts.  Besides, it is well accepted that one type of structural prior can only reveal part of the structural information from one single perspective. A variety of types of structural priors are needed in order to gain complete structural information of texts. This can be achieved by introducing different structural priors into different parts of attention heads, where different structural priors can complement each other, guiding the SAN models to learn proper dependencies between words. Therefore, to gain a better representation of the texts, a desirable solution should make full use of the multi-head attention mechanism and utilize multiple types of structural priors.  To better alleviate the aforementioned problems, in this paper, we propose a lightweight self attention network, i.e., the Multiple Structural Priors Guided Self Attention Network . The novel idea behind our model lies in the usage of the multi-mask based multi-head attention , which helps our model to better capture different types of dependencies between texts. Thanks to the MM-MH Attention mechanism, our model can capture multiple structural priors, which in return brings benefits in modeling sentences.  Especially, the structural priors we employed come from two categories: the sequential order and the relative position of words. Since the standard SANs are incapable of distinguishing the order between words, we apply the direction mask  directly to each attention head. Motivated by the Bidirectional RNNs , we split the attention heads into two parts. For a given word, we apply the forward mask to the first half of attention heads, which allows it to attend on only the previous words when modeling the reference word. Accordingly, the backward mask is applied to the rest of the attention heads.  Since the direction masks take no consideration of the difference between long-distance words and nearby words, we employ the second category of structural prior as a complement, which could be measured by the distance between pair of words. We integrate two types of distance masks into different attention heads. The first one we utilized is the word distance mask, which describes the physical distance between each pair of words. Besides, for the purpose of capturing the latent hierarchical structure of sentences, we integrate another kind of distance information, i.e., dependency distance that is defined as the distance between each pair of words on a dependency syntax tree. The word distance mask helps our model to focus on the local words and the dependency distance mask enables our model to capture the hierarchical relationships between words. Consequently, they provide our model the ability of capturing the local and non-local dependency of words properly.  To illustrate the effectiveness of our model, we conduct experiments on two NLP tasks: natural language inference and sentiment classification. Experimental results show that MS-SAN outperforms other baselines and achieves a competitive performance comparing with the state-of-the-art models.   Our contributions are listed as follows:     Recently, self-attention mechanism has achieved great success.  \citeauthor{vaswani2017attention} \shortcite{vaswani2017attention} proposed the Transformer architecture and achieved great improvements on the machine translation task. Then, \citeauthor{yu2018qanet} \shortcite{yu2018qanet} proposed the QA-Net for the reading comprehension task. In addition, the language models based on transformers pre-trained on large corpus bring a huge improvement on many NLP tasks .   However, standard SANs is limited in modeling sentence structures, including the sequential and the hierarchical structure. For the former, previous studies follow the idea of BiLSTMs and CNNs, introducing similar information into SANs. \citeauthor{shen2018disan} \shortcite{shen2018disan} proposed the DiSAN, introducing the direction information into SANs. After that, \citeauthor{im2017distance} \shortcite{im2017distance} introduced the word distance on the basis of the DiSAN in order to capture more local dependencies. For the latter, traditional methods are mostly based on dependency syntax trees or constituency syntax trees , which is hard to parallelize with a huge training cost. However, SAN-based models can leverage these limitations with the assistance of the self-attention.  Both PSAN proposed by \citeauthor{wu2018phrase} \shortcite{wu2018phrase} and Tree-Transformer proposed by \citeauthor{wang2019tree} \shortcite{wang2019tree} utilized constituency trees to model sentences in phrase level. \citeauthor{wang2019self} \shortcite{wang2019self} improved the position encoding through a relative structural position extracted from dependency trees for modeling latent hierarchical structure. Our model merges both the sequential and hierarchical structure information through the multiple structural priors guided self attention.    In this work, we propose a novel paragraph-level multi-task learning model for  task. Experiments show that  The compact paragraph encoding method is beneficial over separately computing sentence embeddings.  With negative sampling, the joint training of rationale selection and stance prediction is beneficial over the pipeline solution.  
"," Recently, self-attention mechanism has achieved great success.  \citeauthor{vaswani2017attention} \shortcite{vaswani2017attention} proposed the Transformer architecture and achieved great improvements on the machine translation task. Then, \citeauthor{yu2018qanet} \shortcite{yu2018qanet} proposed the QA-Net for the reading comprehension task. In addition, the language models based on transformers pre-trained on large corpus bring a huge improvement on many NLP tasks .   However, standard SANs is limited in modeling sentence structures, including the sequential and the hierarchical structure. For the former, previous studies follow the idea of BiLSTMs and CNNs, introducing similar information into SANs. \citeauthor{shen2018disan} \shortcite{shen2018disan} proposed the DiSAN, introducing the direction information into SANs. After that, \citeauthor{im2017distance} \shortcite{im2017distance} introduced the word distance on the basis of the DiSAN in order to capture more local dependencies. For the latter, traditional methods are mostly based on dependency syntax trees or constituency syntax trees , which is hard to parallelize with a huge training cost. However, SAN-based models can leverage these limitations with the assistance of the self-attention.  Both PSAN proposed by \citeauthor{wu2018phrase} \shortcite{wu2018phrase} and Tree-Transformer proposed by \citeauthor{wang2019tree} \shortcite{wang2019tree} utilized constituency trees to model sentences in phrase level. \citeauthor{wang2019self} \shortcite{wang2019self} improved the position encoding through a relative structural position extracted from dependency trees for modeling latent hierarchical structure. Our model merges both the sequential and hierarchical structure information through the multiple structural priors guided self attention.",247
" Sequence-to-Sequence  learning~ has advanced the state of the art in various natural language processing  tasks, such as machine translation~, text summarization~, and grammatical error correction~. Seq2Seq models are generally implemented with an encoder-decoder framework, in which a multi-layer encoder summarizes a source sequence into a sequence of representation and another multi-layer decoder produces the target sequence conditioned on the encoded representation.   Recent studies reveal that fusing the intermediate encoder layers  is beneficial for Seq2Seq models, such as layer attention~, layer aggregation~, and layer-wise coordination~. Despite its effectiveness, not much is known about how fusing encoder layer representations work. The intuitive explanation is that fusing encoder layers exploits surface and syntactic information embedded in the lower encoder layers~.  However, other studies show that attending to lower encoder layers  does not improve model performance~, which is conflicted with existing conclusions. It is still unclear why and when fusing encoder layers should work in Seq2Seq models.  This paper tries to shed light upon behavior of Seq2Seq models augmented with EncoderFusion method. To this end, we propose a novel fine-grained layer attention to evaluate the contribution of individual encoder layers. We conduct experiments on several representative Seq2Seq NLP tasks, including machine translation, text summarization, and grammatical error correction. Through a series of analyses, we find that the uppermost decoder layer pays more attention to the encoder embedding layer. Masking the encoder embedding layer significantly drops model performance by generating hallucinatory  predictions. The encoded representation of the standard Seq2Seq models  may not have enough capacity to model both semantic and surface features . We call the problem described above the source representation bottleneck.  Based on this observation, we simplify the EncoderFusion approaches by only connecting the encoder embedding layer to softmax layer . The SurfaceFusion approach shortens the path distance between source and target embeddings, which can help to learn better bilingual embeddings with direct interactions. Experimental results on several Seq2Seq NLP tasks show that our method consistently outperforms both the vanilla Seq2Seq model and the layer attention model.  Extensive analyses reveal that our approach produces more aligned bilingual word embeddings by shortening the path distance between them, which confirm our claim.  Our main contributions are as follows:       \paragraph{EncoderFusion in Seq2Seq} Lower encoder layers that embed useful surface features are far away from the training signals, which poses difficulty for deep Seq2Seq models to exploit such useful features. Although residual connections~ have been incorporated to combine layers, these connections have been ``shallow'' themselves, and only fuse by simple, one-step operations~. In response to this problem, several approaches have been proposed to fuse the encoder layers with advanced methods, such as layer attention~, layer aggregation~, and layer-wise coordination~. Although these methods show promising results on different NLP tasks, not much is known about how the EncoderFusion works. In addition, some other studies show that exploiting low-layer encoder representations fail to improve model performance~.   In this paper, we consolidate the conflicting conclusions of existing studies by pointing out that the encoder embedding layer is the key, which can help Seq2Seq models to precisely predict target words. Based on this finding, we propose a novel SurfaceFusion to directly connecting the encoder embedding layer and the softmax layer, which consistently outperform current EncoderFusion approaches across different NLP tasks.  \paragraph{Variants of Feature Fusion} Feature fusion aims to merge two sets of features into one, which is frequently employed in CV tasks, such as semantic segmentation~ and object detection~.  \citet{DBLP:journals/corr/abs-1804-03821} shows that simply fusing surface and abstract features tends to be less effective due to the gap in semantic levels.   For NLP tasks, researchers investigated fusion models for language understanding~ and language generation~.  \citet{nguyen-chiang-2018-improving} propose to fuse features at representation-level, but we empirically find this kind of fusion method is not orthogonal to multi-layer models due to the large semantic gap. \citet{shallowfusion} combine the predictions produced by the Seq2Seq model and external LM predictions in a later fusion manner, which pose little impact to the original information flow. \citet{stahlberg-etal-2018-simple} improve upon it by removing the dependence on the manually defined hyper-parameter. In this work, we demonstrate the effectiveness of the two typical probability-level fusion methods on sequence-to-sequence learning tasks.  Unlike them that rely on an external model, our approach only requires a surface attention module that can be jointly trained with the vanilla Seq2Seq model.    In this work, we propose a novel hierarchical curriculum learning framework for training response selection models for multi-turn conversations. During training, the proposed framework simultaneously employs the corpus-level and instance-level curriculum to dynamically select suitable training data based on the state of learning process. Extensive experiments and analysis on two benchmark datasets show that our approach can significantly improve the performance of various strong matching models.   To test our approach, we conduct extensive experiments and analysis using three representative matching models. The results on two benchmark datasets demonstrate the effectiveness of the proposed approach.   Experimental results on two benchmark datasets using three representative matching models verify the effectiveness of the proposed approach.   
","  \paragraph{EncoderFusion in Seq2Seq} Lower encoder layers that embed useful surface features are far away from the training signals, which poses difficulty for deep Seq2Seq models to exploit such useful features. Although residual connections~ have been incorporated to combine layers, these connections have been ``shallow'' themselves, and only fuse by simple, one-step operations~. In response to this problem, several approaches have been proposed to fuse the encoder layers with advanced methods, such as layer attention~, layer aggregation~, and layer-wise coordination~. Although these methods show promising results on different NLP tasks, not much is known about how the EncoderFusion works. In addition, some other studies show that exploiting low-layer encoder representations fail to improve model performance~.   In this paper, we consolidate the conflicting conclusions of existing studies by pointing out that the encoder embedding layer is the key, which can help Seq2Seq models to precisely predict target words. Based on this finding, we propose a novel SurfaceFusion to directly connecting the encoder embedding layer and the softmax layer, which consistently outperform current EncoderFusion approaches across different NLP tasks.  \paragraph{Variants of Feature Fusion} Feature fusion aims to merge two sets of features into one, which is frequently employed in CV tasks, such as semantic segmentation~ and object detection~.  \citet{DBLP:journals/corr/abs-1804-03821} shows that simply fusing surface and abstract features tends to be less effective due to the gap in semantic levels.   For NLP tasks, researchers investigated fusion models for language understanding~ and language generation~.  \citet{nguyen-chiang-2018-improving} propose to fuse features at representation-level, but we empirically find this kind of fusion method is not orthogonal to multi-layer models due to the large semantic gap. \citet{shallowfusion} combine the predictions produced by the Seq2Seq model and external LM predictions in a later fusion manner, which pose little impact to the original information flow. \citet{stahlberg-etal-2018-simple} improve upon it by removing the dependence on the manually defined hyper-parameter. In this work, we demonstrate the effectiveness of the two typical probability-level fusion methods on sequence-to-sequence learning tasks.  Unlike them that rely on an external model, our approach only requires a surface attention module that can be jointly trained with the vanilla Seq2Seq model.",248
"   Indonesian colloquialism is everyday and everywhere, e.g. in social media posts and conversational transcripts. Yet, existing research on Indonesian NLP models including NMTs often disregards qualitative analysis when the models are given strictly colloquial inputs. This is mainly due to the fact that the data readily available for training and testing the models are in formal Indonesian. %This follow naturally due to the fact that the models are style-agnostic, that is,   Colloquial Indonesian has several different word choices from formal language due to the diversity of regional languages and dialects. We define the spoken colloquial as a clean colloquial. In addition, in written media,  colloquial Indonesian is often abbreviated, disemvoweled, or written with voice alteration, which we define as the noisy colloquial .               \end{table}   To better evaluate English-Indonesian MT systems against colloquial text, we first create 2 new test-sets of Indonesian-English colloquial pairs. The first test is a clean colloquial taken from a YouTube transcript. The second test-set is a noisy colloquial from Twitter annotated by our team of annotators. We found that NMT systems trained on formal dataset did not perform very well on these test-sets.  Next, we develop synthetic colloquial text data by performing word-level translation of several words in the formal text into a colloquial form based on a word-to-word dictionary. By combining the formal dataset and the synthesized colloquial dataset, we increase the NMT performance on the colloquial test-set by 2.5 BLEU points.      \citet{michel2018mtnt} developed a test-bed for noisy MT from social media. Related to that, \citet{vaibhav2019improving} showed that introducing synthetic noises improves NMT system evaluated on noisy test-set. This research draws inspiration from these works. However, the noise in colloquial Indonesian is different from the standard noise on social media, which is mostly typographical. In our case, the noise occurs in differences in the words used, because they are mixed with local languages or dialects, in addition of typographical noise.  Focusing on Indonesian-English MT itself, \citet{guntara-etal-2020-benchmarking} collected parallel English-Indonesian sentences in several domains, trained and benchmarked several NMT models, with some achieving state-of-the-art results. However, although the issue of colloquialism was briefly raised, there was no evaluation on model performance in translating either noisy or clean colloquial Indonesian to English. In this work, we make use of the training set they have provided.  On synthetic ``noise'' injection in Indonesian language, \citet{rizal-stymne-2020-evaluating} leveraged a dictionary-based approach to create a synthetic code-mixed data of Indonesian and English language and showed that the synthetized data is highly similar to an original code-mixed data. Code-mixing, especially with English, Arabic, and romanized Japanese or Korean, is a common type of colloqualism in Indonesian language.  Style transfer approaches have been attempted to standardize informal Indonesian with the motivation to serve as a preprocessing step for the downstream tasks , where the available models typically work well on formal Indonesian. High-quality pairs of informal-formal Indonesian data is again the main challenge in this work, and the authors pe  rformed iterative forward translation to generate more synthetic data, which was shown to improve the results up to some iterations.     The word segmentation is an essential and non-trivial task in Sindhi language. The white spaces between words are a good sign for predicting word boundaries, but the existence of space-omission and space-insertion bring ambiguity in the segmentation process. We proposed the SGNWS model, keeping in view the challenges related to SWS, respectively. The proposed model has the ability to learn and extract subword features automatically by eliminating the constraints such as hand-craft features for segmentation or any other type of prior domain-specific knowledge.    in this paper, we propose a deep BiLSTM-CRF based framework with subword representation learning. The novel character-level  For that task, we construct five benchmark datasets and empirically analyze proposed SGNWS and the chosen baselines approaches. The proposed model also surpases existing Sindhi word segmenters by achieving high F-Score of 98.13\ , 97.62\  on developed benchmark datasets Awami-Awaz, 96.26\  on Wiki-dumps, 97.37\  on twitter, 97.93\  on books corpus, and best F-Score of 98.51\  on the SGSEG dataset. The performance of Wiki-dumps is comparatively lesser due to the existence of noise in the text.    In this paper, we empirically demonstrate that our proposed model yields the best performance in SWS because of its high efficiency and robustness for sequential modeling tasks with great ability to capture the word information at the morphemic level for the prediction of word boundaries. The SGNWS model is an effective and elegant neural solution for SWS, which can also be applied to other sequence tagging problems.    
","  \citet{michel2018mtnt} developed a test-bed for noisy MT from social media. Related to that, \citet{vaibhav2019improving} showed that introducing synthetic noises improves NMT system evaluated on noisy test-set. This research draws inspiration from these works. However, the noise in colloquial Indonesian is different from the standard noise on social media, which is mostly typographical. In our case, the noise occurs in differences in the words used, because they are mixed with local languages or dialects, in addition of typographical noise.  Focusing on Indonesian-English MT itself, \citet{guntara-etal-2020-benchmarking} collected parallel English-Indonesian sentences in several domains, trained and benchmarked several NMT models, with some achieving state-of-the-art results. However, although the issue of colloquialism was briefly raised, there was no evaluation on model performance in translating either noisy or clean colloquial Indonesian to English. In this work, we make use of the training set they have provided.  On synthetic ``noise'' injection in Indonesian language, \citet{rizal-stymne-2020-evaluating} leveraged a dictionary-based approach to create a synthetic code-mixed data of Indonesian and English language and showed that the synthetized data is highly similar to an original code-mixed data. Code-mixing, especially with English, Arabic, and romanized Japanese or Korean, is a common type of colloqualism in Indonesian language.  Style transfer approaches have been attempted to standardize informal Indonesian with the motivation to serve as a preprocessing step for the downstream tasks , where the available models typically work well on formal Indonesian. High-quality pairs of informal-formal Indonesian data is again the main challenge in this work, and the authors pe  rformed iterative forward translation to generate more synthetic data, which was shown to improve the results up to some iterations.",249
"  Large-scale language models have greatly advanced NLP research in various sub-areas, such as question answering, text summarization, story generation and so on . However, these generation models still suffer from at least three major problems when applied to the dialogue system building, 1) generic and repeated responses ,   2) inconsistent statements with the dialogue context , and 3) uncontrollable task-oblivious replies  .  Many previous studies have attempted to address these problems . For instance, \citet{li2019inconsisent} penalized repetitive and inconsistent behaviors with unlikelihood loss in open-domain chats. \citet{song2020generate} detected and rewrote the contradicting responses to achieve a more consistent personality.  However, these methods optimize the language model by minimizing the loss in supervised learning, which may lead to exposure bias and uninterpretable behaviors, and consequently,  makes it harder for humans to regulate the model.   To alleviate these problems, previous work has explored RL-based methods in dialogue system building . %For instance,  integrated the goal of coherent into the reward design  and made the first step towards .designed for better generation.   However, such methods not only rely on hand-crafted user simulators that are inherently hard to build , but also require meaningful rewards that are difficult to design. To address these issues, we propose to teach the model to extract a policy directly from the data and learn from its own mistakes without the use of simulators. Leveraging decoding methods such as Nucleus Sampling , the language model finetuned on a persuasion task is able to generate lexically diverse response candidates given the same context. %One example is shown in Figure.  Some candidates are appropriate, while others are repetitive or inconsistent with the context. These good and bad examples are used as positive and negative feedback to the model through meaningful rewards in RL, and help refine the language model. During testing, to fully utilize the refined language model, we use it to generate multiple candidates again,  and filter out the repetition and inconsistency afterwards. Beyond being nonrepetitive and consistent, a good response also needs to accomplish the dialogue task, in our case, to persuade people. Therefore, we ask humans to demonstrate the persuasion process, and build a response imitator to imitate these human demonstrations and select the most persuasive response.  The above issues in language models are especially salient in complex strategic dialogue tasks such as persuasion and negotiation. These dialogues involve both a specific task goal and social contents to build rapport for better task completion, and therefore, have richer and more complicated language structures . Furthermore, due to their inherent similarity to task-oriented and open-domain dialogues, improvements made on these systems would also help in both dialogue settings. Therefore, we choose a strategic donation persuasion task  to perform our study, and conduct both automatic and human evaluations to evaluate our models.     This work  makes multiple contributions. First, we propose DialGAIL, an RL-based generative algorithm to refine MLE-based language models for dialogue  generation without the use of user simulators.  Second, we design an effective and practicable framework for strategic dialogue systems that achieves state-of-the-art performance on a complex persuasion task, with only small amount of human demonstration efforts.  %Such system achieves more diverse, consistent and fluent conversations with better persuasion outcomes on a complex persuasion task compared to the MLE-based baselines.   %a framework to automatically detect repetitive and inconsistent responses, and imitate human demonstration to select persuasive responses.  %Furthermore, experiments show that our model produces more diverse, consistent and fluent conversations with better persuasion outcomes on a complex persuasion task compared to the MLE-based baselines.  Previous dialogue research has mostly focused on pure task-oriented dialogues and pure social conversations; but looking forward, it becomes more and more important to pay attention to strategic dialogues that involves both task and social components. We sincerely hope this work could inspire more research and discussions on strategic dialogues in the community.   % how to refine the dialogue generation with limited amount of data? MLE fine-tuning woldn't work with the limited data % social content + a specific end-goal --> persuasionforgood. advance research in this area % how to easily get a usable lm without computational resources? % explore the possibility to apply GAIL in dialogue generation in a simple way  % the first to explore GAIL % raise more attention in persuasion in the community % with small amount of human demo % task-independent in repetition detection strengthen       language model Large-scale language models have achieved great success in multiple NLP tasks including reading comprehension, machine translation and so on .  However, these models still suffer from repetition and inconsistency when applied to dialogue tasks which requires long-term context memory and logical reasoning. There have been many previous studies to address these issues . \citet{zhang2019dialogpt} presented a response generation model named DialoGPT specifically trained on large conversation-like corpora, and obtained close-to-human performance in single-turn dialogue settings. \citet{li2019inconsisent} proposed to detect the inconsistency with natural language inference data, and penalize it with unlikelihood loss to achieve more consistent personality in open-domain dialogues. Our work tackles these problems with reinforcement learning to reduce exposure bias and encourage the model to generate multiple responses and learn from its own mistakes.    Our work is also closely related to response selection, which focus on obtaining good context representations to match the context and  retrieve the best response from a large collection of human-human conversations. However, such response selection models are highly dependent on the quality and availability of the underlying datasets. To address the data scarcity issue, \citet{henderson2019training} pretrained a response selection model with large conversational corpora, and finetuned it on new domains in task-oriented settings for a better context representation. Instead of retrieving candidates from human dialogues, we leverage language models' ability to generate coherent responses, and build a selector to imitate human selection process and choose among the generated candidates.     Non-collaborative dialogue tasks such as persuasion and anti-scam have emerged and attracted more attention recently, given its wide applications in industry and daily life . These tasks are close to human-human conversations and contain both a specific task goal and social components to build rapport for better task completion.   are at the crossroad where task-oriented and open-domain dialogues meet. That being said, they are usually more complex with longer context than pure task-oriented or open-domain dialogues. Towards non-collaborative dialogue system building, \citet{li2019missa} utilized large-scale language models to generate multiple coherent responses and applied human-defined rules to filter out inappropriate candidates. We take a similar approach to generate candidates but eliminate the manual work for rule design, and teach the model to select task-relevant candidates through human demonstration.            Despite the broad applications of the transformer model, it struggles to perform well for some NLP tasks when the training data is limited. In this work, we propose a theoretically justified optimization strategy to train deeper transformer model with improved generalization and faster convergence speed on small datasets, which is generally applicable to different NLP tasks and neural architectures. The proposed strategy is applied on Text-to-SQL semantic parsing, an important structural prediction task and achieve state of the art by successfully training significantly deeper relational transformer models. Further analyses show that increasing the depth of the transformer model trained with limited data can be helpful for the generalization on complicated structural prediction tasks, instead of harmful as previously assumed. Such observations indicate that the current understanding of the transformer architecture is still incomplete and shed light on the directions of future research.     
","  language model Large-scale language models have achieved great success in multiple NLP tasks including reading comprehension, machine translation and so on .  However, these models still suffer from repetition and inconsistency when applied to dialogue tasks which requires long-term context memory and logical reasoning. There have been many previous studies to address these issues . \citet{zhang2019dialogpt} presented a response generation model named DialoGPT specifically trained on large conversation-like corpora, and obtained close-to-human performance in single-turn dialogue settings. \citet{li2019inconsisent} proposed to detect the inconsistency with natural language inference data, and penalize it with unlikelihood loss to achieve more consistent personality in open-domain dialogues. Our work tackles these problems with reinforcement learning to reduce exposure bias and encourage the model to generate multiple responses and learn from its own mistakes.    Our work is also closely related to response selection, which focus on obtaining good context representations to match the context and  retrieve the best response from a large collection of human-human conversations. However, such response selection models are highly dependent on the quality and availability of the underlying datasets. To address the data scarcity issue, \citet{henderson2019training} pretrained a response selection model with large conversational corpora, and finetuned it on new domains in task-oriented settings for a better context representation. Instead of retrieving candidates from human dialogues, we leverage language models' ability to generate coherent responses, and build a selector to imitate human selection process and choose among the generated candidates.     Non-collaborative dialogue tasks such as persuasion and anti-scam have emerged and attracted more attention recently, given its wide applications in industry and daily life . These tasks are close to human-human conversations and contain both a specific task goal and social components to build rapport for better task completion.   are at the crossroad where task-oriented and open-domain dialogues meet. That being said, they are usually more complex with longer context than pure task-oriented or open-domain dialogues. Towards non-collaborative dialogue system building, \citet{li2019missa} utilized large-scale language models to generate multiple coherent responses and applied human-defined rules to filter out inappropriate candidates. We take a similar approach to generate candidates but eliminate the manual work for rule design, and teach the model to select task-relevant candidates through human demonstration.",250
"   Large-scale pre-training has draw much attention in both the community of Compute Vision  and Natural Language Processing  due to its strong capability of generalization and efficient usage of large-scale data. Firstly in CV, a series of models were designed and pre-trained on the large-scale dataset ImageNet, such as AlexNet , VGG  and ResNet , which effectively improved the capability of image recognition for numerous tasks. Recent years have witnessed the burst of pre-training in NLP, such as BERT , RoBERTa , XLNet  and BART , which greatly improve the capability of language understanding and generation. However, the above researches towards the single-modal learning and can only be used in single-modal  scenarios. %which greatly restricts their ability to process multi-modal  information. In order to adapt to multi-modal scenarios, a series of multi-modal pre-training methods were proposed and pre-trained on the corpus of image-text pairs, such as ViLBERT , VisualBERT  and UNITER , which greatly improve the ability to process multi-modal information. However, these models can only utilize the limited corpus of image-text pairs and cannot be effectively adapted to single-modal scenarios . %Moreover, the size of the corpus of image-text pairs is very limited, and large scale of single-modal data can't be effectively utilized.     A smarter AI system should be able to process different modalities of information effectively. There are large scale of data in different modalities on the Web, mainly textual and visual information. The textual knowledge and the visual knowledge usually can enhance and complement with each other. As the example shown in Figure , it's difficult to answer the question correctly only with the visual information in the image.  However, if we connect the visual information to the textual information which describes the background of a baseball game, it's very easy to determine the correct answer. Also, the visual information can make it easier to understand the scene described by the text. The research in neuroscience by \citet{van2018neuronal} reveals that the parts of the human brain responsible for vision can learn to process other kinds of information, including touch and sound. Inspired by the research, we propose to design a unified-modal architecture UNIMO which can process multi-scene and multi-modal data input, including textual, visual and vision-and-language data, as shown in Figure .  The greatest challenge to unify different modalities is to align and unify them into the same semantic space which are generalizable to different modalities of data. Existed cross-modal pre-training methods try to learn cross-modal representations based on only limited image-text pairs by simple image-text matching and masked language modeling . They can only learn specific representations for image-text pairs, which are not generalizable for single-modal scenarios. So their performance will drop dramatically when applied to language tasks . In this work, UNIMO learns visual representations and textual representations in similar ways, and unify them into the same semantic space via cross-modal contrastive learning  based on a large-scale corpus of image collections, text corpus and image-text pairs.  %Our unified-modal architecture can utilize large scale of image collections and text corpus, and align the visual and textual information into the same semantic space via cross-modal contrastive learning on image-text pairs. %Effectively utilizing large-scale of images and text corpus can improve the capability of vision and textual understanding respectively. UNIMO effectively utilizes the large-scale of text corpus and image collections to learn general textual and visual representations.  The CMCL aligns the visual representation and textual representation, and unifies them into the same semantic space based on image-text pairs. To facilitate different levels of semantic alignment between vision and language, we propose to utilize a series of text rewriting techniques to improve the diversity of cross-modal information. As shown in Figure , we utilize back-translation to generate several positive examples for an image-text pair. Also, to enhance the detail semantic alignment between text and image, we further parse the caption to scene graph  and randomly replace either the objects, attributes or relations in the caption to generate various negative samples. Sentence-level retrieval and replacement is also utilized to enhance the sentence-level alignment. In this way, our model can effectively unify different levels of visual and textual representations into the same semantic space.  The unified-modal architecture mainly has the following advantages compared with previous methods:        Existing researches on pre-training can be mainly classified into two categories: single-modal pre-training and multi-modal pre-training. The single-modal pre-training methods only focus on single-modal tasks, while the multi-modal pre-training methods only focus on multi-modal tasks.  \paragraph{Single-Modal Pre-training} The single-modal pre-training methods mainly consists of visual pre-training methods and language pre-training methods. Most visual pre-training methods are based on the multi-layer CNN architecture such as VGG  and ResNet , and trained on the ImageNet dataset. These pre-trained models only focus on visual tasks , however, they cannot be used in textual or multi-modal  tasks. The language pre-training methods are also more and more popular in NLP models, which are based on the multi-layer Transformer architecture, such as GPT , BERT , XLNET  and BART . All of them are trained on large-scale corpus by language modeling, which learn contextualized token representations by either predicting tokens based on their context for language understanding or predicting tokens auto-regressively for language generation. However, they can only be used on textual tasks. They cannot deal with multi-modal tasks with both image and text, such as visual question answering , image-text retrieval and image captioning.  \paragraph{Multi-Modal Pre-training} Recently, multi-modal pre-training methods have been more and more popular for solving the multi-modal tasks. All of them are trained on a corpus of image-text pairs, such as ViLBERT , VisualBERT , VL-BERT , Unicoder-VL  and UNITER . Based on the multi-layer Transformer network, they all employ the BERT-like objectives to learn multi-modal representations from a concatenated-sequence of vision features and language embeddings. Their architectures can be mainly classified into two categories: single-stream and two-stream. The two-stream methods, such as ViLBERT, utilize two single-modal Transformer to process visual features and language embeddings respectively, and then learn their interactions based on a cross-modal Transformer. The single-stream methods directly utilize a single Transformer network to model both the visual features and the language embeddings.  VisualBERT, VL-BERT, Unicoder-VL and UNITER all utilize the single-stream architecture, which validate that fusing cross-modal information early and freely by a single-stream network can achieve better performance. All existed multi-modal pre-training methods only focus on multi-modal tasks with both vision and language inputs. However, they cannot be effectively adapted to single-modal tasks. Their performance will drop dramatically when fine-tuned on language tasks . Moreover, they can only utilize the limited corpus of image-text pairs. By contrast, our unified-modal pre-training method UNIMO can employ large volumes of text corpus and image collections to enhance each other, and can be effectively adapted to both textual and multi-modal scenarios. UNIMO also achieves the best performance on multi-modal tasks including VQA, image caption and visual entailment.                           The problems with repetition and inconsistency still persist in dialogue response generation.  Large-scale language models still suffer from repetition and inconsistency problems when applied to dialogue response generations.   Current large-scale language models still suffer from repetition and inconsistency when applied to dialogue response generation.  Current dialogue systems suffer from repetition and inconsistency.    The repetition and inconsistency problems still persist in dialogue response generation with large-scale language models.  Large-scale language models still suffer from repetition and inconsistency when applied to dialogue generation. To address the exposure bias issue in MLE, we propose DialGAIL to  refine the MLE-based language model and extract a policy directly from the data without user simulators by learning from its own mistakes.   by penalizing its own mistakes.  With the same context, the model  generates multiple response candidates, some of which are repetitive and inconsistent. These negative examples send feedback to the model via a reward function to reduce repetition and inconsistency.  Furthermore, we provide human demonstration for the model to imitate human persuasion activity and select the most persuasive candidate. Experiments show that our model achieves state-of-the-art performance in a complex persuasion task, and produces more diverse, consistent, and persuasive conversations with small amount of human efforts. Looking into the future, strategic dialogues with both task and social contents will become more and more important, and it is our sincere hope that this work could inspire more research and discussion in strategic dialogue tasks in the community.  besides being nonrepetitive and inconsistent, a good response also contributes to task success. To achieve this, we provide human demonstration for the model to imitate human persuasion activities. Our experiments show that our model performs better than the baselines on both automatic metrics and human evaluations, and produces more diverse and persuasive conversations.                                                                                    \clearpage   
","  Existing researches on pre-training can be mainly classified into two categories: single-modal pre-training and multi-modal pre-training. The single-modal pre-training methods only focus on single-modal tasks, while the multi-modal pre-training methods only focus on multi-modal tasks.  \paragraph{Single-Modal Pre-training} The single-modal pre-training methods mainly consists of visual pre-training methods and language pre-training methods. Most visual pre-training methods are based on the multi-layer CNN architecture such as VGG  and ResNet , and trained on the ImageNet dataset. These pre-trained models only focus on visual tasks , however, they cannot be used in textual or multi-modal  tasks. The language pre-training methods are also more and more popular in NLP models, which are based on the multi-layer Transformer architecture, such as GPT , BERT , XLNET  and BART . All of them are trained on large-scale corpus by language modeling, which learn contextualized token representations by either predicting tokens based on their context for language understanding or predicting tokens auto-regressively for language generation. However, they can only be used on textual tasks. They cannot deal with multi-modal tasks with both image and text, such as visual question answering , image-text retrieval and image captioning.  \paragraph{Multi-Modal Pre-training} Recently, multi-modal pre-training methods have been more and more popular for solving the multi-modal tasks. All of them are trained on a corpus of image-text pairs, such as ViLBERT , VisualBERT , VL-BERT , Unicoder-VL  and UNITER . Based on the multi-layer Transformer network, they all employ the BERT-like objectives to learn multi-modal representations from a concatenated-sequence of vision features and language embeddings. Their architectures can be mainly classified into two categories: single-stream and two-stream. The two-stream methods, such as ViLBERT, utilize two single-modal Transformer to process visual features and language embeddings respectively, and then learn their interactions based on a cross-modal Transformer. The single-stream methods directly utilize a single Transformer network to model both the visual features and the language embeddings.  VisualBERT, VL-BERT, Unicoder-VL and UNITER all utilize the single-stream architecture, which validate that fusing cross-modal information early and freely by a single-stream network can achieve better performance. All existed multi-modal pre-training methods only focus on multi-modal tasks with both vision and language inputs. However, they cannot be effectively adapted to single-modal tasks. Their performance will drop dramatically when fine-tuned on language tasks . Moreover, they can only utilize the limited corpus of image-text pairs. By contrast, our unified-modal pre-training method UNIMO can employ large volumes of text corpus and image collections to enhance each other, and can be effectively adapted to both textual and multi-modal scenarios. UNIMO also achieves the best performance on multi-modal tasks including VQA, image caption and visual entailment.",251
" Although there are over 7,000 languages spoken worldwide~, only several dozen have enough data available to support supervised speech recognition, and many languages do not even employ a writing system~. In contrast, most people learn to use spoken language long before they learn to read and write, suggesting that linguistic annotation is not a prerequisite for speech processing systems. This line of reasoning motivates research that aims to discover meaningful linguistic abstractions  directly from the speech signal, with the intention that they could reduce the reliance of spoken language systems on text transcripts.  A rich body of work has recently emerged investigating representation learning for speech using visual grounding objectives~, as well as how word-like and subword-like linguistic units can be made to emerge within these models~. So far, these efforts have predominantly focused on inference, where the goal is to learn a mapping from speech waveforms to a semantic embedding space. Generation of speech conditioned on a point in a semantic space has been less explored, and is what we focus on in this work. We hypothesize that generative approaches offer interesting advantages over relying solely on inference. For example, prior works have demonstrated the capability of recognizing visually descriptive words, but have not been shown to learn non-visual words or grammar. Our experiments show that these aspects of spoken language are learned to some degree by a visually-grounded generative model of speech.  Specifically, we introduce a model capable of directly generating fluent spoken audio captions of images without the need for natural language text, either as an intermediate representation or a form of supervision during training . Tremendous progress has been made recently in natural language image caption generation~ and naturalistic text-to-speech synthesis ~.  Combining these models provides a means for generating spoken image descriptions, but existing approaches for training these models are reliant on text during training. Instead, we leverage sub-word speech units discovered using a self-supervised learning objective as a drop-in replacement for the text. We hypothesize that by using such techniques, an even wider variety of traditionally text-based NLP models could be applied to speech data without the need for transcription or automatic speech recognition  systems. Because all human languages utilize small, discrete phonetic inventories~, we posit that our framework should be applicable for any language in the world. In our experiments, we demonstrate that not just any set of discovered speech units can function in this role. We find the greatest success with units that are discrete, exhibit a low frame-rate, and highly robust to speaker and environmental variability. The main contributions of our paper are as follows:  1. The first methodology for fluent image-to-speech synthesis that does not rely on text. A critical aspect of our approach is factorizing the model into an Image-to-Unit  module and a Unit-to-Speech  module, where the speech units are discovered in a self-supervised fashion. This approach enables disentanglement of linguistic variability and acoustic/speaker variability.  2. Extensive analysis on the properties required for learned units to replace text. While the idea may seem simple and straightforward, obtaining proper units is not a trivial task. In fact, most of the units experimented in this paper fail to serve as drop-in replacements. Moreover, we demonstrate that what are deemed good units vary significantly for inference and generation.  3. Demonstrating insufficiency of beam search-based evaluation. We show that even when an I2U model fails to generate sensible caption through beam search decoding, it can still produce reasonable captions by sampling from the posterior, hinting that posterior mode-based evaluation can only inspect limited aspects of a model.  4. Proposing a semantic diversity-aware metric. We identify issues of an existing metric~ and propose M-SPICE for sampling-based evaluation to address the problems.  5. Over 600,000 spoken audio captions for the MSCOCO dataset. We collect 742 hours of speech from 2,352 people tasked with reading each caption out loud. This dataset will be made publicly available to support work at the intersection of speech, language, and vision.       Image-to-Text and Image-to-Speech Captioning. Significant progress towards generating realistic  captions that describe the content of visual images was made with the advent of deep neural networks~. Far less work has focused on generating spoken audio captions from natural images. Training an image-to-speech system using separate  and  datasets was explored in~. \citet{hasegawa2017image2speech} is the only prior work that has explored image-to-speech synthesis without using text~, but with limited results. In that work, BLEU scores were only computed in terms of unsupervised acoustic units, not an estimate of the actual words produced by the synthesizer, which can be problematic as discussed in Section. The resulting captions were not evaluated for fluency, naturalness, or intelligibility, and the BLEU scores in terms of the unsupervised units were very low  compared to ours .  \citet{wang2020show} is a concurrent work that proposes a text-free end-to-end image-to-speech model, which simplifies the task by using pairs of image and synthesized speech generated from a single-speaker TTS model to reduce the acoustic variation. In contrast, by leveraging robust learned units, the I2U module in our system can be trained on speech with abundant variation, while the U2S module serves as a vocoder and only requires a small amount of clean speech , which imposes less constraints on the data and still outperforms \citet{wang2020show} on captioning metrics.  Speech Synthesis without Text and Voice Conversion. Voice conversion is a classic problem in speech processing that involves resynthesizing a recording to sound as if it was spoken by a different person~. Voice conversion has recently seen progress using neural approaches~, but the most relevant work to our own is the ZeroSpeech 2019 challenge~, which addresses unsupervised learning of discrete speech units that can be used as the basis of a synthesizer. Unlike image-to-speech synthesis, these tasks only infer units from given audio recordings and do not require generation.  Speech Pre-Training for Downstream Tasks. Interest in un/self-supervised pre-training has recently surged in the speech community. Several papers have investigated masked prediction~, while others have implemented autoencoder models with various constraints applied to the latent representations, such as discreteness~ or disentanglement~. Predictive coding has recently seen a revival using deep neural networks~. Most work applied pre-trained speech models to only inference problems such as ASR or phone discrimination, with a notable exception being, which focuses on text-free machine translation.            }          In this work, we propose a unified-modal pre-training architecture UNIMO, which can leverage large-scale of non-paired text corpus and image collections for cross-modal learning. Our model can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Based on the unified-modal architecture, the textual knowledge and visual knowledge can enhance each other in the unified semantic space. Our UNIMO model outperforms previous methods on both the multi-modal and single-modal downstream tasks. In the future, we will utilize larger scale of image collections and text corpus for unified-modal learning, and extend UNIMO to other modalities of data such as video, audio and so on.    
","   Image-to-Text and Image-to-Speech Captioning. Significant progress towards generating realistic  captions that describe the content of visual images was made with the advent of deep neural networks~. Far less work has focused on generating spoken audio captions from natural images. Training an image-to-speech system using separate  and  datasets was explored in~. \citet{hasegawa2017image2speech} is the only prior work that has explored image-to-speech synthesis without using text~, but with limited results. In that work, BLEU scores were only computed in terms of unsupervised acoustic units, not an estimate of the actual words produced by the synthesizer, which can be problematic as discussed in Section. The resulting captions were not evaluated for fluency, naturalness, or intelligibility, and the BLEU scores in terms of the unsupervised units were very low  compared to ours .  \citet{wang2020show} is a concurrent work that proposes a text-free end-to-end image-to-speech model, which simplifies the task by using pairs of image and synthesized speech generated from a single-speaker TTS model to reduce the acoustic variation. In contrast, by leveraging robust learned units, the I2U module in our system can be trained on speech with abundant variation, while the U2S module serves as a vocoder and only requires a small amount of clean speech , which imposes less constraints on the data and still outperforms \citet{wang2020show} on captioning metrics.  Speech Synthesis without Text and Voice Conversion. Voice conversion is a classic problem in speech processing that involves resynthesizing a recording to sound as if it was spoken by a different person~. Voice conversion has recently seen progress using neural approaches~, but the most relevant work to our own is the ZeroSpeech 2019 challenge~, which addresses unsupervised learning of discrete speech units that can be used as the basis of a synthesizer. Unlike image-to-speech synthesis, these tasks only infer units from given audio recordings and do not require generation.  Speech Pre-Training for Downstream Tasks. Interest in un/self-supervised pre-training has recently surged in the speech community. Several papers have investigated masked prediction~, while others have implemented autoencoder models with various constraints applied to the latent representations, such as discreteness~ or disentanglement~. Predictive coding has recently seen a revival using deep neural networks~. Most work applied pre-trained speech models to only inference problems such as ASR or phone discrimination, with a notable exception being, which focuses on text-free machine translation.            }",252
"   Knowledge distillation is a technique to train smaller, more efficient student models by learning from larger teacher models, usually by mimicking the teacher's output. In the scope of neural machine translation , source-side monolingual data is run through the teacher model to produce an output that will be learnt by the student. The absence of parallel data requirements allows the student model to be trained with more data choices. This research focuses on exploring the use of monolingual datasets for knowledge distillation to find out what data should be used.  This research focuses on three aspects. The first is the language origin of the monolingual data. Student models can be trained with additional data in the form of source-side monolingual data. Besides that, the model can also be trained with back-translation data constructed from the target-side monolingual data. We show that using both source-side and target-side data are important because each of them improves performance , depending on the test-set's language origin.   Secondly, we explore the source of the monolingual data. Some research suggests or uses the same data between teacher and student. On the other hand, some research that makes use of knowledge distillation for NMT uses additional dataset, on top of the dataset learnt by the teacher. We explore whether using seen data is necessary, where we find that the student trained with a new unseen monolingual data performs equally with the one trained with the same dataset as the teacher.  The amount of data, including the synthetic ones affects model performance. Therefore, the last thing we explore is the monolingual data size. We find that adding to the monolingual data is generally better. However, varied training data based on language origin is much more important.         Smaller neural models usually have an advantage in terms of efficiency. However, they are usually of poorer quality than large models. Smaller models have shown to perform better if trained with knowledge distillation~ mechanism than trained from scratch, which we confirm in this research. In knowledge distillation, the small model will learn to follow the output distribution of a larger model.  \citet{kim2016sequence} proposed interpolated knowledge distillation for sequence-to-sequence models. In interpolated knowledge distillation, students learn directly from the output produced by their teachers. This interpolated knowledge distillation is easy to apply, as practically we simply produce forward-translation synthetic data with from a teacher model once. Despite its simplicity, interpolated knowledge distillation have shown to be useful for training small model without compromising quality. Similarly, some research explores model stealing: where a model learns from the output of a black-box model . This stealing model is conceptually similar to interpolated knowledge distillation. \citet{wallace2020imitation} demonstrated that stealing production NMT system is possible.      Human-written translations, or sometimes called translationese, show different characteristic compared to naturally written text. Word distribution in translationese is different to natural text, as the translators are influenced by the original language when producing the translation. In context of machine translation, the parallel data for test-set may originally come from the source-language, target-language, or neither. Hence, one part  of the test set is translationese.   Several studies have found that the performance of the NMT model is sensitive to translationese. A news translation task ranking changes depending on which part of the test set is used. The training data used also affects the performance of NMT. \citet{edunov-etal-2020-evaluation} have found that leveraging back-translation data improves the performance of test-sets derived from target-side data that are translated to source-side. Similarly, \citet{bogoychev2019domain}~found otherwise that the forward-translation synthesis data performed better in the test-set from the source-side. Given that our research in knowledge distillation makes use of monolinguals from multiple directions, we will conduct a performance evaluation based on the original source of the test set as well.    In this paper, we presented the first model capable of generating fluent spoken captions of images without relying on text, which almost matches the performance of early text-based image captioning models. Our comprehensive experiments demonstrated that learned units need to be robust, of low framerate, and encoding little or none duration information to be a drop-in replacement for text. We also identified the caveats of mode-based evaluation and proposed a new metric to address semantic diversity. As part of this work, a novel dataset of over 600k spoken captions for the MSCOCO dataset is introduced, which we will make publicly available to the research community.  Future work should investigate applying the proposed method to additional languages, devising improved speech unit representations, and jointly training the speech unit model with the I2S model. This would offer the opportunity to explore new analysis-by-synthesis training objectives.      
","   Smaller neural models usually have an advantage in terms of efficiency. However, they are usually of poorer quality than large models. Smaller models have shown to perform better if trained with knowledge distillation~ mechanism than trained from scratch, which we confirm in this research. In knowledge distillation, the small model will learn to follow the output distribution of a larger model.  \citet{kim2016sequence} proposed interpolated knowledge distillation for sequence-to-sequence models. In interpolated knowledge distillation, students learn directly from the output produced by their teachers. This interpolated knowledge distillation is easy to apply, as practically we simply produce forward-translation synthetic data with from a teacher model once. Despite its simplicity, interpolated knowledge distillation have shown to be useful for training small model without compromising quality. Similarly, some research explores model stealing: where a model learns from the output of a black-box model . This stealing model is conceptually similar to interpolated knowledge distillation. \citet{wallace2020imitation} demonstrated that stealing production NMT system is possible.      Human-written translations, or sometimes called translationese, show different characteristic compared to naturally written text. Word distribution in translationese is different to natural text, as the translators are influenced by the original language when producing the translation. In context of machine translation, the parallel data for test-set may originally come from the source-language, target-language, or neither. Hence, one part  of the test set is translationese.   Several studies have found that the performance of the NMT model is sensitive to translationese. A news translation task ranking changes depending on which part of the test set is used. The training data used also affects the performance of NMT. \citet{edunov-etal-2020-evaluation} have found that leveraging back-translation data improves the performance of test-sets derived from target-side data that are translated to source-side. Similarly, \citet{bogoychev2019domain}~found otherwise that the forward-translation synthesis data performed better in the test-set from the source-side. Given that our research in knowledge distillation makes use of monolinguals from multiple directions, we will conduct a performance evaluation based on the original source of the test set as well.",253
" NMT is the task of transforming a source sequence into a new form in a particular target language using deep neural networks. Such networks commonly have an encoder-decoder architecture , in which an encoder maps a given input sequence to an intermediate representation and a decoder then uses the same representation to generate candidate translations. Both encoder and decoder are neural networks that are trained jointly. Due to the sequential nature of the NMT task, early models usually relied on recurrent architectures , or benefited from the sliding feature of convolutional kernels to encode/decode variable-length sequences .   Recently, Transformers  have shown promising results for NMT and become the new standard in the field. They follow the same concept of encoding and decoding but in a relatively different fashion. A Transformer is fundamentally a feed-forward model with its unique neural components  that alter the traditional translation pipeline accordingly. Therefore, it is expected if such a model behaves differently than its recurrent or convolutional counterparts. Our goal in this research is to study this aspect in the presence of noise.     NMT engines trained on clean samples provide high-quality results when tested on similarly clean texts, but they break easily if noise appears in the input . They are not designed to handle noise by default and Transformers are no exception. Many previous works have focused on this issue and studied different architectures . In this work, we particularly focus on Transformers\footnote{We assume that the reader is already familiar with the Transformer architecture.} as they are relatively new and to some extent understudied.   A common approach to make NMT models immune to noise is fine-tuning , where a noisy version of input tokens is intentionally introduced during training and the decoder is forced to generate correct translations despite deformed inputs. FT is quite useful for almost all situations but it needs to be run with an optimal setting to be effective. In our experiments, we propose a slightly different learning-rate scheduler to improve FT. We also define a new extension that not only modifies input words but also adds complementary tokens to the target side. We refer to this extension as Target Augmented Fine-Tuning , which is the first contribution of this paper.   In our study, we realized that data augmentation techniques  might not be sufficient enough for some cases and we need a compatible training process and neural architecture to deal with noise. Therefore, we propose Controlled Denoising  whereby noise is added to source sequences during training and the encoder is supposed to fix noisy words before feeding the decoder. This approach is implemented via an auxiliary loss function and is similar to adversarial training. CD is our second contribution.   CD only takes care of noise on the encoder side, so we propose a Dual-Channel Decoding  strategy to study what happens if the decoder is also informed about the input noise. DCD supports multi-tasking through a -channel decoder that samples target tokens and corrects noisy input words simultaneously. This form of fusing translation knowledge with noise-related information has led to interesting results in our experiments. DCD is the third and last contribution of this work.   The remainder of the paper is organised as follows: First, we review previously reported solutions for the problem of noise in NMT in Section , then we present details of our methods and the intuition behind them in Section . To validate our methods, we report experimental results in Section . Finally, we conclude the paper and discuss possible future directions in Section .    Fine-tuning  is one of the most straightforward and reliable techniques to protect NMT systems from noise. \citet{berard2019naver}, \citet{dabre-sumita-2019-nicts}, and \citet{helcl2019cuni} studied its impact and showed how it needs to be utilized to boost Transformers.  Adversarial training is another common solution to build noise-robust models. \citet{doubly-adv} proposed a gradient-based method to construct adversarial examples for both source and target samples. Source-side inputs are supposed to attack the model while adversarial target inputs help defend the translation model. In their model, a candidate word is replaced with its semantically-close peer to introduce noise. This way, the neural engine visits different forms of the same sample, which extends its generalizability. In other words, the network is trained to deliver the same, consistent functionality even though it is fed with different forms of a sample. Although this strategy showed promising results, in our settings we replace input words with real noisy candidates instead of synonyms or semantically-related peers. We find this way of adding noise more realistic and closer to real-world scenarios.   \citet{karpukhin-etal-2019-training} experimented another idea by generating adversarial examples using synthetic noise. Their proposed architecture relies on Transformers but the encoder is equipped with a character-based convolutional model . This work is one of the few attempts that studied Transformers' behaviour in the presence of noise. However, their results are based on relatively small datasets. We know that NMT models' performance could change proportionally with a change in the size of training sets.  The convolutional component also makes it hard to figure out how transformers deal with noise.    The application of adversarial training is not limited to the aforementioned examples. \citet{towards-robust-nmt} defined additional loss functions which force the encoder and decoder to ignore perturbations and generate clean outputs. This idea is similar to our CD approach, but the underlying architecture is different. \citet{towards-robust-nmt} only reported results on recurrent NMT models.   Providing better representations is as important as designing tailored training strategies for noise-robust models. A group of researchers focused on how different segmentation schemes and encoding techniques can play a role. \citet{sennrich2015neural} and  \citet{michel-neubig-2018-mtnt} showed that subwords are better alternatives than surface forms  to handle perturbations and out-of-vocabulary words. \citet{belinkov2018synthetic} comprehensively studied this by using different character- and subword-based representations in different architectures. \citet{sakaguchi17robsut} also carried out a similar investigation where they proposed a new encoding that is invariant to the order of characters.    Besides these approaches, translating noisy inputs can be viewed as a two-pass process performed via two connected neural networks. The first one acts as a monolingual translator to correct noisy inputs and the second one is an ordinary translation engine that consumes denoised sequences to generate clean translations . This  idea can be implemented as an end-to-end, differentiable  solution or in a pipeline with multiple and segmented modules, but it should be noted that such a mechanism could be hard to deploy or slow to run in practice.    ~ Neural machine translation has become the dominant approach to machine translation in both research and practice. This article reviewed the widely used methods in NMT, including modeling, decoding, data augmentation, interpretation, as well as evaluation. We then summarize the resources and tools that are useful for NMT research.  Despite the great success achieved by NMT, there are still many problems to be explored. We list some important and challenging problems for NMT as follows:   
"," Fine-tuning  is one of the most straightforward and reliable techniques to protect NMT systems from noise. \citet{berard2019naver}, \citet{dabre-sumita-2019-nicts}, and \citet{helcl2019cuni} studied its impact and showed how it needs to be utilized to boost Transformers.  Adversarial training is another common solution to build noise-robust models. \citet{doubly-adv} proposed a gradient-based method to construct adversarial examples for both source and target samples. Source-side inputs are supposed to attack the model while adversarial target inputs help defend the translation model. In their model, a candidate word is replaced with its semantically-close peer to introduce noise. This way, the neural engine visits different forms of the same sample, which extends its generalizability. In other words, the network is trained to deliver the same, consistent functionality even though it is fed with different forms of a sample. Although this strategy showed promising results, in our settings we replace input words with real noisy candidates instead of synonyms or semantically-related peers. We find this way of adding noise more realistic and closer to real-world scenarios.   \citet{karpukhin-etal-2019-training} experimented another idea by generating adversarial examples using synthetic noise. Their proposed architecture relies on Transformers but the encoder is equipped with a character-based convolutional model . This work is one of the few attempts that studied Transformers' behaviour in the presence of noise. However, their results are based on relatively small datasets. We know that NMT models' performance could change proportionally with a change in the size of training sets.  The convolutional component also makes it hard to figure out how transformers deal with noise.    The application of adversarial training is not limited to the aforementioned examples. \citet{towards-robust-nmt} defined additional loss functions which force the encoder and decoder to ignore perturbations and generate clean outputs. This idea is similar to our CD approach, but the underlying architecture is different. \citet{towards-robust-nmt} only reported results on recurrent NMT models.   Providing better representations is as important as designing tailored training strategies for noise-robust models. A group of researchers focused on how different segmentation schemes and encoding techniques can play a role. \citet{sennrich2015neural} and  \citet{michel-neubig-2018-mtnt} showed that subwords are better alternatives than surface forms  to handle perturbations and out-of-vocabulary words. \citet{belinkov2018synthetic} comprehensively studied this by using different character- and subword-based representations in different architectures. \citet{sakaguchi17robsut} also carried out a similar investigation where they proposed a new encoding that is invariant to the order of characters.    Besides these approaches, translating noisy inputs can be viewed as a two-pass process performed via two connected neural networks. The first one acts as a monolingual translator to correct noisy inputs and the second one is an ordinary translation engine that consumes denoised sequences to generate clean translations . This  idea can be implemented as an end-to-end, differentiable  solution or in a pipeline with multiple and segmented modules, but it should be noted that such a mechanism could be hard to deploy or slow to run in practice.",254
"   Cross-lingual word embeddings  represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora  or bilingual dictionaries . However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning   or adversarial training .         Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods . In later work, \citet{ormazabal-etal-2019-analyzing} showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it.     In this paper, we propose an alternative approach that does not have this limitation, but can still work without any parallel resources. The core idea of our method is to fix the target language embeddings, and learn aligned embeddings for the source language from scratch. This prevents structural mismatches that result from independently training embeddings in different languages, as the learning of the source embeddings is tailored to each particular set of target embeddings. For that purpose, we use an extension of skip-gram that leverages translated context words as anchor points. So as to translate the context words, we start with a weak initial dictionary, which is iteratively improved through self-learning, and we further incorporate a restarting procedure to make our method more robust. Thanks to this, our approach can effectively work without any human-crafted bilingual resources, relying on simple heuristics  or an existing unsupervised mapping method to build the initial dictionary. Our experiments confirm the effectiveness of our approach, outperforming previous mapping methods on bilingual dictionary induction and obtaining competitive results on zero-shot cross-lingual transfer learning on XNLI.           \paragraph{Word embeddings.} Embedding methods learn static word representations based on co-occurrence statistics from a corpus. Most approaches use two different matrices to represent the words and the contexts, which are known as the input and output vectors, respectively . The output vectors play an auxiliary role, being discarded after training. Our method takes advantage of this fact, leveraging translated output vectors as anchor points to learn cross-lingual embeddings. To that end, we build on the Skip-Gram with Negative Sampling  algorithm , which trains a binary classifier to distinguish whether each output word co-occurs with the given input word in the training corpus or was instead sampled from a noise distribution.     \paragraph{Mapping CLWE methods.} Offline mapping methods separately train word embeddings for each language, and then learn a mapping to align them into a shared space. Most of these methods align the embeddings through a linear map---often enforcing orthogonality constraints---and, as such, they rely on the assumption that the geometric structure of the separately learned embeddings is similar. This assumption has been shown to fail under unfavorable conditions, severely hindering the performance of these methods . Existing attempts to mitigate this issue include learning non-linear maps in a latent space , employing maps that are only locally linear , or learning a separate map for each word . However, all these methods are supervised, and have the same fundamental limitation of aligning a set of separately trained embeddings .      \paragraph{Self-learning.} While early mapping methods relied on a bilingual dictionary to learn the alignment, this requirement was alleviated thanks to self-learning, which iteratively re-induces the dictionary during training. This enabled learning CLWEs in a semi-supervised fashion starting from a weak initial dictionary , or in a completely unsupervised manner when combined with adversarial training  or initialization heuristics . Our proposed method also incorporates a self-learning procedure, showing that this technique can also be effective with non-mapping methods.    \paragraph{Joint CLWE methods.} Before the popularization of offline mapping, most CLWE methods extended monolingual embedding algorithms by either incorporating an explicit cross-lingual term in their learning objective, or directly replacing words with their translation equivalents in the training corpus. For that purpose, these methods relied on some form of cross-lingual supervision, ranging from bilingual dictionaries  to parallel or document-aligned corpora . More recently, \citet{lample2018phrase} reported positive results learning regular word embeddings over concatenated monolingual corpora in different languages, relying on identical words as anchor points. \citet{wang2019crosslingual} further improved this approach by applying a conventional mapping method afterwards. As shown later in our experiments, our approach outperforms theirs by a large margin.       \paragraph{Freezing.} \citet{artetxe-etal-2020-cross} showed that it is possible to transfer an English transformer to a new language by freezing all the inner parameters of the network and learning a new set of embeddings for the new language through masked language modeling. This works because the frozen transformer parameters constrain the resulting representations to be aligned with English. Similarly, our proposed approach uses frozen output vectors in the target language as anchor points to learn aligned embeddings in the source language.          In this paper, we studied the problem of noise in the context of NMT and particularly focused on Transformers. We proposed three novel techniques to augment data and change the training procedure as well as the neural architecture. Experimental results show that our techniques can protect NMT engines from noise. Our models only affect the training phase and do not add any overhead in terms of space and/or time complexities at inference time. Findings of our research can be summarized as follows:    In this research, we ran an extensive number of experiments in order to find the best configuration of each model and optimize hyper-parameters, but there still exist some unexplored topics/areas. In our future work, we are planning to experiment with other language pairs with different morphological and grammatical structures. , e.g. it would be interesting to see how our models deal with a language such as Mandarin that mainly relies on characters.  We are also interested in studying other noise classes. We could only afford to work with one class and we selected natural noise as we find it more realistic among others, but this work can be extended to other noise classes. Finally, our models are not unique to Transformer and NMT. We aim to evaluate them in other language processing/understanding tasks.  
","     \paragraph{Word embeddings.} Embedding methods learn static word representations based on co-occurrence statistics from a corpus. Most approaches use two different matrices to represent the words and the contexts, which are known as the input and output vectors, respectively . The output vectors play an auxiliary role, being discarded after training. Our method takes advantage of this fact, leveraging translated output vectors as anchor points to learn cross-lingual embeddings. To that end, we build on the Skip-Gram with Negative Sampling  algorithm , which trains a binary classifier to distinguish whether each output word co-occurs with the given input word in the training corpus or was instead sampled from a noise distribution.     \paragraph{Mapping CLWE methods.} Offline mapping methods separately train word embeddings for each language, and then learn a mapping to align them into a shared space. Most of these methods align the embeddings through a linear map---often enforcing orthogonality constraints---and, as such, they rely on the assumption that the geometric structure of the separately learned embeddings is similar. This assumption has been shown to fail under unfavorable conditions, severely hindering the performance of these methods . Existing attempts to mitigate this issue include learning non-linear maps in a latent space , employing maps that are only locally linear , or learning a separate map for each word . However, all these methods are supervised, and have the same fundamental limitation of aligning a set of separately trained embeddings .      \paragraph{Self-learning.} While early mapping methods relied on a bilingual dictionary to learn the alignment, this requirement was alleviated thanks to self-learning, which iteratively re-induces the dictionary during training. This enabled learning CLWEs in a semi-supervised fashion starting from a weak initial dictionary , or in a completely unsupervised manner when combined with adversarial training  or initialization heuristics . Our proposed method also incorporates a self-learning procedure, showing that this technique can also be effective with non-mapping methods.    \paragraph{Joint CLWE methods.} Before the popularization of offline mapping, most CLWE methods extended monolingual embedding algorithms by either incorporating an explicit cross-lingual term in their learning objective, or directly replacing words with their translation equivalents in the training corpus. For that purpose, these methods relied on some form of cross-lingual supervision, ranging from bilingual dictionaries  to parallel or document-aligned corpora . More recently, \citet{lample2018phrase} reported positive results learning regular word embeddings over concatenated monolingual corpora in different languages, relying on identical words as anchor points. \citet{wang2019crosslingual} further improved this approach by applying a conventional mapping method afterwards. As shown later in our experiments, our approach outperforms theirs by a large margin.       \paragraph{Freezing.} \citet{artetxe-etal-2020-cross} showed that it is possible to transfer an English transformer to a new language by freezing all the inner parameters of the network and learning a new set of embeddings for the new language through masked language modeling. This works because the frozen transformer parameters constrain the resulting representations to be aligned with English. Similarly, our proposed approach uses frozen output vectors in the target language as anchor points to learn aligned embeddings in the source language.",255
"   One of the hallmarks of human intelligence is the ability to generalize seamlessly across heterogeneous sensory inputs and different cognitive tasks. We see objects, hear sounds, feel textures, smell odors, and taste flavors to learn underlying concepts present in our world. Much of AI's existing progress in multimodal learning, however, focuses primarily on a fixed set of predefined modalities and tasks that are consistent between training and testing. As a result, it is unclear how to transfer knowledge from models trained for one modality  to another  at test time. This scenario is particularly important for low-resource target modalities where unlabeled data is scarce and labeled data is even harder to obtain . In the unimodal case, this is regarded as meta-learning or few-shot learning. In contrast, we formally define the cross-modal generalization setting as a learning paradigm to train a model that can  quickly perform new tasks in a target modality  and  doing so while being trained on a different source modality. In this paper, we study the data and algorithmic challenges for cross-modal generalization to succeed. %Such a learning paradigm is particularly useful in leveraging high-resource source modalities to help low-resource target modalities, where unlabeled data is scarce and labeled data is even harder to obtain, such as audio from low-resource languages, real-world environments, and medical images.      As a motivating example, Figure illustrates a scenario where large-scale image classification benchmarks can help audio classification, which is a less studied problem with fewer large-scale benchmarks. In this ambitious problem statement, a key research question becomes: how can we obtain generalization across modalities despite using separate encoders for different source  and target  modalities? The technical challenge involves aligning shared knowledge learned from source image tasks with target audio tasks. Our problem statement differs from conventional meta-learning and domain adaptation where one can take advantage of the same source and target modality with shared encoders which helps generalization by having the same representation space. In our case, the discrepancies in modalities requires one to learn new output concepts expressed in new input modalities. As a result, cross-modal generalization requires new ideas to synchronize  multimodal sources and targets. What is the minimal extra supervision required to perform this alignment?  In this paper, we formalize the conditions required for successful generalization and show that another level of supervision is necessary under partial observability across modalities and tasks. Supervision comes in the form of cross-modal meta-alignment  to capture a space where representations of similar concepts in different modalities are close together while ensuring quick generalization to new tasks . We introduce a novel algorithm called \names\  that leverages readily available multimodal data from the internet  for meta-alignment and cross-modal generalization. Through theoretical analysis and empirical ablations, we study our proposed algorithm with both strongly and weakly paired multimodal data, showing that cross-modal generalization is possible even with limited extra supervision.  %How can one transfer knowledge learned from an image classification task to speech event classification? The problem of cross-modal generalization brings fundamental differences regarding how data is expressed across different modalities . In comparison to meta-learning and domain adaptation, the different input spaces now consist of extremely high-dimensional, complex, and heterogeneous source and target modalities. As a result, we are unable to use a shortcut by sharing encoders as commonly seen in same-modality, different domain settings which allow for the same representation space between source and target domains. This raises a fundamental research question: how can we obtain generalization across modalities despite using separate encoders for different source and target modalities? These discrepancies in modalities requires one to learn new output concepts expressed in new input modalities. %We show that existing domain adaptation, meta-learning, and transfer learning approaches are unable to bridge the gap between such heterogeneous paradigms where both input modalities and output tasks are different.  % emphasize cant share encoders, need explicit alignment % emphasize different label space, generalize meta-learning % formulate crossmodal ml and therefore we propose meta alignment % first para ok. like to learn but different modalities. % second para. compared to ml and da, 1 critical issue when trying to do crossmodal - have hetero data between source and target. cant use shortcut such as same encoder for images of different domains. need different encoders 1 for each. how do we solve this? need another level of supervision to help - where meta alignment comes in. what we propose - a technique to address the core technical challenge of crossmodal ml which is how to learn different encoders. meta alignment is a way to do that, a contrastive learning approach.  %To account for this technical challenge, we formalize the conditions required for successful generalization and show that another level of supervision is necessary under partial observability across modalities and tasks. This form of supervision comes in the form of cross-modal alignment to capture a space where representations of similar concepts in different modalities are close together while ensuring quick generalization to new low-resource tasks . Our analysis leads to a novel algorithm based on contrastive learning called \names\  that leverages either strongly or weakly paired multimodal data abundant on the internet. Finally, we carefully study the data and algorithmic requirements for our approach to succeed through theoretical analysis and empirical ablations.  %Very hard problem of crossmodal meta-learning. What is the minimal amount of supervision  required to solve this hard task of cross-modal meta-learning? In this paper we will explore this through theory and empirics  %We highlight two crucial distinctions:  the different input spaces consist of extremely high-dimensional, complex, and heterogeneous source and target modalities, and  there exist different task distributions between source and target modalities, such as the inherent differences between the label spaces when transferring from image to audio classification tasks. These discrepancies in both input and output spaces requires one to learn new output concepts expressed in new input modalities. We show that existing domain adaptation, meta-learning, and transfer learning approaches are unable to bridge the gap between such heterogeneous paradigms where both input modalities and output tasks are different.  % how do we handle limited resource modalities and task, we explore cross-modal approach % note: define modality, concept, task % note: a better way of saying cross-modal cross-task  %, which allows us to learn a classifier for transfer from source to target tasks. %This makes it particularly suitable for generalization across modalities and tasks due to the presence of unseen concepts and annotations in the target modality. %We show that this space:  groups similar concepts expressed across different modalities,  is well-clustered across concepts, and  generalizes well to new concepts, making it particularly suitable for generalization across modalities and tasks. %While our first attempt at meta-alignment uses strong pairings across source and target modalities , we further provide an extension to use only weak pairs between modalities. Weak pairs represent coarse groupings of semantic correspondence which better capture the many-to-many relations between real-world multimodal data  and allow us to use large banks of weakly paired multimodal data available on the internet and prepared for machine learning studies such as video data  and image captioning data . %Finally, we quantify the trade-offs between labeling more data in the target modality versus obtaining better source-target alignment.  %provide theoretical justification to quantify the benefits of our approach: {\color{red} ZIYIN TODO} \zing[ziyin: should mention and focus on the difficulty of definition and formalization] %instead of a classical generalization error in the target modality that scales wrt the sample complexity of the target modality, our approach is bounded by the sample complexity in the source modality. As a result, the error is therefore reduced with ample samples in the source modality and a well-aligned space.  We present experiments on three cross-modal tasks: generalizing from  text to image,  image to audio, and  text to speech. In all cases, the goal is to classify data from a new target modality given only a few  labeled samples. %We find that \names\ accurately performs few-shot alignment of concepts from different modalities, thereby allowing generalization from concepts in the source modality to new concepts in the target modality. We perform extensive experiments to compare with related approaches including target modality meta-learning that would be expected to perform well since they have seen thousands of labeled examples from the target modality during meta-training. Surprisingly, \names\ is competitive with these baselines and significantly outperforms other cross-modal approaches. In addition, we study settings where the target modality suffers from noisy or limited data, a scenario particularly prevalent in low-resource modalities. %While this setting makes it difficult to directly train in the target modality, our approach efficiently leverages cross-modal information to perform well.         Few-shot learning has enabled strong performance for settings with limited labeled data using techniques spanning data augmentation, metric learning, and learning better initializations. In the latter, meta-learning has recently emerged as a popular choice due to its simplicity in combination with gradient-based methods.  Transfer learning focuses on transferring knowledge from external data  to downstream tasks where labeled data is expensive. Domain adaptation similarly focuses on changing data distributions. However, existing works focus on data within the same modality  which simplifies the alignment problem.  Cross-modal alignment involves learning a joint space where the representations of the same concepts expressed in different modalities are close together. Alignment is particularly useful for cross-modal retrieval  and cross-modal  representation learning. Several objective functions for learning aligned spaces from varying quantities of paired and unpaired data have been proposed. However, cross-modal generalization is harder since:  one has to learn not just the associations between modalities but also associations to labels,  there is weak supervision both the target modality and in the label space ,  tasks in different modalities have different  label spaces, and  new tasks in the target modality have to be learned using only a few samples.  Cross-modal learning: Recent work has explored more general models that enable knowledge transfer across modalities. In particular, cross-modal data programming uses weak labels in a source modality to train a classifier in the target modality. Cross-modal transfer learning aims to classify the same task from different input modalities. Finally, few-shot learning within target modalities  has been shown to benefit from additional multimodal information  during training. However, these all require labeled data from the target modality during meta-training . In contrast, we study cross-modal generalization which do not assume any labeled data in the target except during few-shot classification.  In a parallel vein, co-learning studies how external information from another modality can help prediction in a source modality,   so both training and testing focuses on prediction in the source  and is unable to solve problems in an unseen target modality.  To the best of our knowledge, our approach is the first to tackle generalization from a source to target modality.     In this paper, we proposed and evaluated the objective-aware active learning strategy designed for screening classification and selecting efficiently item, predicate for annotating based on the overall classification objective. We demonstrated that objective-aware sampling outperforms uncertainty and random AL techniques under different conditions. We further aim to examine more screening datasets, extend this study to other classes of screening problems and hybrid crowd-machine algorithms.  
","     Few-shot learning has enabled strong performance for settings with limited labeled data using techniques spanning data augmentation, metric learning, and learning better initializations. In the latter, meta-learning has recently emerged as a popular choice due to its simplicity in combination with gradient-based methods.  Transfer learning focuses on transferring knowledge from external data  to downstream tasks where labeled data is expensive. Domain adaptation similarly focuses on changing data distributions. However, existing works focus on data within the same modality  which simplifies the alignment problem.  Cross-modal alignment involves learning a joint space where the representations of the same concepts expressed in different modalities are close together. Alignment is particularly useful for cross-modal retrieval  and cross-modal  representation learning. Several objective functions for learning aligned spaces from varying quantities of paired and unpaired data have been proposed. However, cross-modal generalization is harder since:  one has to learn not just the associations between modalities but also associations to labels,  there is weak supervision both the target modality and in the label space ,  tasks in different modalities have different  label spaces, and  new tasks in the target modality have to be learned using only a few samples.  Cross-modal learning: Recent work has explored more general models that enable knowledge transfer across modalities. In particular, cross-modal data programming uses weak labels in a source modality to train a classifier in the target modality. Cross-modal transfer learning aims to classify the same task from different input modalities. Finally, few-shot learning within target modalities  has been shown to benefit from additional multimodal information  during training. However, these all require labeled data from the target modality during meta-training . In contrast, we study cross-modal generalization which do not assume any labeled data in the target except during few-shot classification.  In a parallel vein, co-learning studies how external information from another modality can help prediction in a source modality,   so both training and testing focuses on prediction in the source  and is unable to solve problems in an unseen target modality.  To the best of our knowledge, our approach is the first to tackle generalization from a source to target modality.",256
" Cloud services have become increasingly popular and are expected to gain 331.212.6\%\ billion every year for the Fortune 1,000 . Amazon is estimated to have a 1004.11\%-91.58\%82.9\%76.3\% - 91.3\%$ in high impacted incidents. Model ablation analysis showed that each of the ML models we used provided a lift in the final ensemble for different incident types. To the best of our knowledge, we are the first one to present a deployed incident triage service for cloud-scale online services.  This paper makes three key contributions:   This paper is organized as follow: Section  presents the background of an incident management system; Section  provides details of   {\TransferAssistant}; Section  shows experimental results; Section  describes the deployment of {\TransferAssistant} in Azure; Section  discusses lessons learned and implications for implementing and deploying an incident triage service at cloud scale;  Section  presents related work;  and Section  concludes this paper.     Bug triage is very close to the problem we are solving where the system gives a ranked list of developers  for a bug report. Early work in bug triage using textual data from the report's description  and contextual features, e.g. product related data, from both the current report and previous reports . While this approach has suffered from the sparseness between bug reports and new developers, researchers have leveraged developer's expertise to increase the system's accuracy. Matter et al.  extract a developer's expertise from all codes that the developer contributed. On the other hand, Xie et al.  used topic modeling  to calculate the probability assigning a developer to the most dominant topic of a bug report.  In more specific context, researchers tried to reduce number of reassignments  with Markov-based models, called bug tossing graph, from textual data  with contextual  data . In addition to a single approach, Jonsson et al.  showed ensembling multiple models, e.g. Naive Bayes, SVM, KNN, and Decision Tree, will increase the accuracy.  Recently, with the emergence of deep neural networks, researchers have explored Convolutional Neural Network  with textual data  and sequence to sequence model  on textual content and tossing sequence. Moreover,  also reported their experience when deploying the approach in an industrial environment: using a human assistant instead of fully automated and the need to continuous refreshing  the model.   Besides all of the problems managing incidents  submitted by users, an online service's management system also proactively detects issues from its components using telemetry data  .   Going beyond incident detection, Lou et al.   tried to address the ``fixing'' stage by analyzing similar incidents and extracting actions to mitigate the current incidents. In additional to logs, an incident management system also handles CRIs .  Recently, DeepCT  proposed a Gated Recurrent Unit  model treating description entries  as a sequence of signal to improve the incident triage process.   {\TransferAssistant} took the best practices from previous work in bug triage and incident triage, i.e. combining textual and contextual data, ensemble multiple approaches including deep neural network, and frequently retraining. While addressing the same incident triage problem, there are major differences between  and our work: we do not only focus on deep neural network but ensemble it with other machine learning approaches and we shared lessons learned when deploying {\TransferAssistant} in a production environment. Lee et al.  also reported a deployed bug triage system. However, they aimed to assist human to triage bugs, while we work on online services and follow a road map from assisting human to semi-automated and, eventually, fully-automated triage incidents.      In this work, we proposed cross-modal generalization: a learning paradigm where abundant source modalities are used to help low-resource target modalities. We showed that meta-alignment using cross-modal data can allow quick generalization to new concepts across different modalities. Our experiments demonstrate strong performance on classifying data from an entirely new target modality under limited samples and noisy labels, which is particularly useful for generalization to low-resource images, speech, and languages.  \iffalse  
","  Bug triage is very close to the problem we are solving where the system gives a ranked list of developers  for a bug report. Early work in bug triage using textual data from the report's description  and contextual features, e.g. product related data, from both the current report and previous reports . While this approach has suffered from the sparseness between bug reports and new developers, researchers have leveraged developer's expertise to increase the system's accuracy. Matter et al.  extract a developer's expertise from all codes that the developer contributed. On the other hand, Xie et al.  used topic modeling  to calculate the probability assigning a developer to the most dominant topic of a bug report.  In more specific context, researchers tried to reduce number of reassignments  with Markov-based models, called bug tossing graph, from textual data  with contextual  data . In addition to a single approach, Jonsson et al.  showed ensembling multiple models, e.g. Naive Bayes, SVM, KNN, and Decision Tree, will increase the accuracy.  Recently, with the emergence of deep neural networks, researchers have explored Convolutional Neural Network  with textual data  and sequence to sequence model  on textual content and tossing sequence. Moreover,  also reported their experience when deploying the approach in an industrial environment: using a human assistant instead of fully automated and the need to continuous refreshing  the model.   Besides all of the problems managing incidents  submitted by users, an online service's management system also proactively detects issues from its components using telemetry data  .   Going beyond incident detection, Lou et al.   tried to address the ``fixing'' stage by analyzing similar incidents and extracting actions to mitigate the current incidents. In additional to logs, an incident management system also handles CRIs .  Recently, DeepCT  proposed a Gated Recurrent Unit  model treating description entries  as a sequence of signal to improve the incident triage process.   {\TransferAssistant} took the best practices from previous work in bug triage and incident triage, i.e. combining textual and contextual data, ensemble multiple approaches including deep neural network, and frequently retraining. While addressing the same incident triage problem, there are major differences between  and our work: we do not only focus on deep neural network but ensemble it with other machine learning approaches and we shared lessons learned when deploying {\TransferAssistant} in a production environment. Lee et al.  also reported a deployed bug triage system. However, they aimed to assist human to triage bugs, while we work on online services and follow a road map from assisting human to semi-automated and, eventually, fully-automated triage incidents.",257
" Dynamic models of text aim at characterizing temporal changes in patterns of document generation. Most successful dynamic language models are Bayesian in nature, and lag behind state-of-the-art deep language models in terms of expressibility. A natural space to study some of the temporal aspects of language is that of the large review datasets found in e-commerce sites.  The availability of millions of reviewed items, such as business or services, books or movies, whose reviews have been recorded in time scales of years, opens up the possibility to develop deep scalable models that can predict the change in taste and preference of users as time evolves. Originally, the interaction of users in these e-commerce sites were studied in the context of collaborative filtering, where the goal was to predict user ratings, based on user interaction metrics. Here we aim to look directly at the content of reviews as time evolves.  %More KDD probably, to much focus on the ratings and recommendations  %-------- %The shear size of e-commerce and review web sites naturally lend itself to the development of data mining tools which are able to provide users with a way to sort out relevant information. This is the task assigned to recommender systems.  Originally kick started by the Netflix competition, matrix factorization  methods through collaborative filtering, aim at predicting user ratings based on user interaction metrics. This rating based methods are lacking as they are unable to clarify the nature of the user preferences, in particular how those preferences change on time. In order to address this issue, methodologies that exploit costumers reviews are gaining attention.  %--------- Costumer reviews provide a rich and natural source of unstructured data which can be leverage to improve recommender system performance . Indeed, reviews are effectively a form of recommendation. % Recently, a variety of deep learning solutions for recommendation have profit from their ability to extract latent representations from review data, encoding rich information related to both users and items. % %Review  content naturally encodes  % This type of data  % Review content is of contextual nature, as the text arises from the interaction of user preferences and items at hand.  % Time represents yet another dimension of context, as user preference and item availability change with time % -- and indeed, % causal and temporal relations have been known to improve the performance of recommender systems  .  % Despite this fact, % recent natural language processing  methodologies for rating and reviews  lag behind at incorporating temporal structure in their language representations. In the present work we exploit recurrent neural network  models for point processes, and feed them neural representations of text, to characterize costumer reviews. Our goal is to capture the changes in user taste and item importance during time, and to exploit those changes to better predict when are new reviews arriving, and what do they actually say. We summarize our contributions as follows: {}  %     We present the related work in Section  and introduce our model in Section . The baseline models used for comparison in this paper are presented in Section . The experimental setup and results are presented in Section . Finally, in Section  we conclude and discuss future work.      The dynamics of language is of fundamental importance in social sciences as a proxy for cultural evolution . Complex system methods seek to understand the emergence of the encoding capabilities of language , and evolutionary approaches  -- following the Bayesian tradition from Phylogenetics,   study the competition between grammar and syntax in the context of historical linguistics . Closer to our line of work, research of online communities point to temporal linguistic changes as means to enforce community norms . Our methodologies aim at studying similar systems in the e-commerce review context, wherein linguistic change is relevant in time scales of months and years.  The work on language dynamics from the machine learning community has mainly focused on the dynamics of word embeddings and topics. On the one hand, different embeddings, as e.g. word2vec , are trained in slices of temporal data and alignments methods are performed a posteriori . The probabilistic framing of word embeddings has, in contrast, allowed for a stochastic-process point of view of embedding evolution .    On the other hand, within the dynamic topic modelling approach,     inheriting the Bayesian perspective from topic models   the parameters of models like Latent Dirichlet Allocation are defined to follow known stochastic processes in time .    Likewise, self-exciting point processes in time have allowed for clustering of document streams . Lastly, while writing this paper we found , in which a RNN language model is conditioned on a global dynamic latent variable.   More recently, and most closely related to our work,    RNN language models have been conditioned on a global dynamic latent variable.    In contrast to this work, our dynamic representations explicitly encode both timing and content of past reviews, and can capture non-Markovian dynamics.    From the point of view of the datasets used in the literature,   Finally, within the recommender system realm deep neural networks models of review data for rating predictions use embedding representations, as well as convolutional neural networks . They also provide characterization of review usefulness, use reviews for product description , and provide better representations for rating prediction . The need to interact with the costumer has also led to question answering solutions . Different from these works, we focus on the temporal aspects of review content.     advantages This study introduces an unsupervised machine learning approach to automatically discover topics from medical inquiries.  After the initial  effort for preprocessing  and hyper-parameters determination, the algorithm runs without requiring any human intervention, discovering key topics as medical inquiries are received.    Topics can be discovered even if only a small number of inquiries is present, and are generally specific, thus enabling targeted, informed decisions by medical experts.  Being completely unsupervised, the algorithm can discover topics that were neither known nor expected in advance, topics which often are the most valuable.   This is in stark contrast with ontology or supervised based approaches, where topics need to be defined a priori , and incoming text can be associated only to these predefined lists of topics, thus hindering the discovery of a priori unknown topics.   The machine learning approach introduced here does not use ontologies , and instead it incorporates domain knowledge via specialized biomedical word embeddings.   This allows to readily apply the topic discovery algorithm to different medicinal products, without the burden of having to develop specialized ontologies for each product or therapeutic area. Indeed, the algorithm is periodically analyzing medical inquiries for a total of sixteen Bayer\texttrademark\ medicinal products, encompassing cardiology, oncology, gynecology, hematology, and ophthalmology.    disadvantages Our approach has several limitations. First, it can happen that a small fraction of inquiries associated to a given topic are actually extraneous to it, especially for semantically broad topics.  This is because - due to the noise present in this real-world dataset - the soft clustering HDBSCAN algorithm must be applied with a low probability threshold for cluster assignment to avoid the majority of inquiries being considered as outliers .    Second, even though the topic names are generally quite informative, a medical expert needs to read the actual inquiries to fully grasp the topic meaning, especially if a decision will be made on the grounds of the discovered topics. This is however not burdensome because inspection is limited to the inquiries associated to a given topic .   Last, some discovered topics are judged by medical experts - based on their expert knowledge - so similar that they could have been merged in a single topic, but are considered distinct by the algorithm. In these cases, manual topic grouping might be required to determine the top topics by inquiry volumes. Still, these similar topics very often appear close to each other in the topic map.    value despite the limitations Despite these limitations, this study demonstrates that medical inquiries contain useful information, and that machine learning can extract this information in an automatic way, discovering topics that are judged by medical information specialists as meaningful and valuable. The hope is that this will stimulate mining of medical inquiries, and more generally the use of natural language processing and unsupervised learning in the medical industry.   Interesting future directions are the inclusion of a priori  expert knowledge  while at the same time maintaining the ability to discover new and previously unknown topics, and grouping topics in meta-topics though a clustering hierarchy.  \section{Methods}   Since our dataset comprises real-word medical inquiries, preprocessing is a crucial step to limit the amount of noise in the corpus.    acronyms The corpus contains numerous acronyms: a first step is thus acronym resolution, i.e. substitute a given acronym with its extended form. A dictionary for the most recurring acronyms  is compiled with the help of medical experts. Acronym resolution is performed via a curated dictionary for two reasons. First, the data is too scarce and noisy to train a reliable, custom-made word embedding to learn the acronym meanings from the corpus. Second, in pretrained word embeddings typically there is no suitable representation for the acronym, or the acronym in our corpus is used to indicate something different than in natural language . For example, in our corpus lode does not refer to a vein of a metal, but stands for lack of product effect.    Regular expressions are then used to remove non-informative strings .   Next. text is split into sentences, tokenized and lemmatized using the scispaCy library  . We disable the scispaCy parser; this gives a significant speed-up without affecting the topic discovery outcome.   Finally stopwords  are removed. In addition to standard English stopwords, there are non-standard stopwords which arise from the dataset being composed of medical inquiries e.g. ask, request, email, inquiry, patient, doctor, and product-dependent stopwords, typically the brand and chemical name of the medicinal product to which the inquiries refer to. It is also the case that in the medical inquiry corpus single words bear value, but when combined they are no longer relevant for medical topic discovery.  For example, the word years and old are generally of relevance, but if contiguous  they are no longer significant since this expression simply originates from a medical information specialists logging the age of the patient to which the inquiry refers to.  Another example is the word morning: when appearing alone it is of relevance, but when it is preceded by the word good it loses its relevance since the expression good morning does not bear any significance for medical topic discovery. We compile a short list of stop n-grams  and remove them from the corpus.   To represent medical inquiries, the scispaCy word embedding model en\_core\_sci\_lg-0.2.5 is used. No model re-training or fine-tuning is performed because of the small amount of data and the sparsity problem; since no labels are available, one would need to train a language model on noisy and short text instances which would likely lead the model to forget the semantics learned by the scispaCy model.   For each token, the  scispaCy embedding vector is retrieved; the sentence representation is then obtained simply by calculating the arithmetic average of the vectors representing each token over all tokens belonging to a given sentence.    Even though the overwhelming majority of out-of-vocabulary  words are not of interest for medical topic discovery, a very small  subset of important oov words would be missed if one were to simply use the word2vec model. We thus devise a strategy to overcome this, as described below.   For each product, the most recurring oov words are automatically detected; these words need to be included in the word2vec model so that they can be represented by a vector which accurately captures their meaning. Training a new embedding to include these new terms is not a good approach given the sparseness problem described above.  To overcome this, we combine a definition mapping and embedding strategy.   definition mapping of out-of-vocabulary words Specifically, first each of the relevant oov terms is manually mapped to a short definition; for example, the oov ReDOS is mapped to dose optimization study since ReDOS refers to a dose-optimisation phase 2 study on regorafenib .    definition embedding Then, using the text from these definitions, a meaningful vector representation for the oov words is obtained with the embedding strategy described above . This procedure has two main benefits. First, it does not require any training data nor any training effort. Second, it ensures by construction that the added word vectors are compatible with the word representation model in use.   Pharmaceutical product trade names are oov words of particular interest for medical topic discovery.   Indeed, being able to take into consideration drug trade names is of importance since there is a substantial amount of questions which mention for instance drug interactions.  However, they are are generally not included in the scispaCy model. Thus, a slightly different procedure is used to ensure that all trade names appearing in medical inquiries are added to the model, regardless of them belonging to the most recurring oov words or not.    Luckily, international non-proprietary names  of drugs are included. For instance, the oncology product trade name Stivarga\texttrademark\ is not present, while its corresponding INN  is.  Thus, to automatically detect drug trade names we utilize the scispaCy named entity recognizer  and the scispaCy UmlsEntityLinker as follows.   First, the NER is used to extract entities from the text; then, for each entity, the UmlsEntityLinker performs a linking with the Unified Medical Language System  by searching within a knowledge base of approximately 2.7 million concepts via string overlap as described in Ref. \onlinecite{neumann-2019}. To limit the number of false positive matches we increase the UmlsEntityLinker threshold to 0.85 from the default of 0.7. For each entities that has been successfully linked to UMLS, several information regarding the identified concepts are returned by the UmlsEntityLinker: concept unique identifier , concept preferred name, concept definition, concept aliases, and concept type unique identifier . In particular, the latter defines to which semantic group the linked concept belongs to ; an up-to-date list of semantic type mappings can be found at . A TUI value of T121 indicates that the concept found is a Pharmacologic Substance. Extracting the entities with TUI equal to T121 allows to automatically identify drug trade names.  Each drug trade name is then mapped to the concept preferred name; if that is not present, the concept definition is used; if that is also not present, drug trade name is replaced by to the phrase pharmaceutical medication drug.  Once this mapping is performed, the same embedding strategy used for the other oov words is followed in order to obtain semantically meaningful word vector representations.    The HDBSCAN algorithm starts by defining a mutual reachability distance based on a density estimation; the data is then represented as a weighted graph where vertices are data points and edges have weight equal to the mutual reachability distance between points.    The minimum spanning tree is built, and converted to a hierarchy of connected components via a union-find data structure: starting from an initial cluster containing all points, the data is subsequently split at each level of the hierarchy according to the distance, ultimately returning as many clusters as data points when the threshold distance approaches zero. This cluster hierarchy is commonly depicted as dendogram.   To obtain a meaningful set of clusters, this hierarchy needs to be condensed. The crucial point is to discern - at any given split - if two new meaningful clusters are formed by splitting their parent cluster, or instead the parent cluster is simply loosing points . In HDBSCAN, this decision is governed by the minimum cluster size hyper-parameter : a cluster split is accepted only if both newly formed clusters have at least min\_cluster\_size points. The final clusters are then chosen from this set of condensed clusters by means of a measure of stability as defined by Ref. \onlinecite{campello-2013}.       how we define the hyperparameters The main factor in defining min\_cluster\_size is the number of inquiries for a given product: we want to obtain  100 clusters so that results can be easily analyzed by medical experts.  It is important to point out that min\_cluster\_size does not strictly specify the number of clusters that will be formed, but rather provides to the algorithm an indication regarding the desired granularity, as outlined above. In our case, min\_cluster\_size ranges only between 5 and 10 depending on the number of inquiries. This small range of variation substantially facilitate the hyper-parameter search. Moreover, we noticed that - for approximately the same amount of inquiries and same min\_cluster\_size - the number of returned clusters increases with data variety, where data variety is qualitatively evaluated by manual inspection: for products with more diverse inquiries HDBSCAN tends to return a higher number of clusters, ceteris paribus.   ceteris paribus means all things being equal We utilize the leaf cluster selection method instead of the excess of mass algorithm because the former is known to return more homogeneous clusters .    we use the soft clustering Due to the noise in the dataset, using the standard  HDBSCAN clustering results in a large portion of the dataset  considered as outliers consistently across all products.  To overcome this, we use the soft HDBSCAN clustering, which returns - instead of a  cluster assignment - the probability that an inquiry belongs to a given cluster. We then define a probability threshold under which a point is considered to be an outlier; for all other points above this threshold, we associate them to the cluster with the highest probability through an argmax operation. This probability threshold ranges between  and   and it is chosen such that approximately 10\  of the inquiries are classified as outliers.   As mentioned in the main text, for computational reasons, we project via UMAP to a lower dimensional space before clustering is performed. Specifically, we project to 100 dimensions for products with less than 15,000 inquiries, and to 20 dimensions for products with more than 15,000 inquiries.   Moreover, inquiries longer than 800 characters are also considered as outliers: this is because the text representation  degrades for long sentences. These inquiries are gathered in the outlier cluster and made available to medical experts for manual inspection.   Given a topic, the vector representation for each word in the topic name is calculated; the topic name vector is then obtained by averaging the word vectors of the words present in the topic name. Topics are merged if their similarity - evaluated as cosine similarity between their topic name vectors - is larger than a threshold. Threshold values range between 0.8 and 0.95 depending on the medicinal product considered.    The most popular topic evaluation metrics for topic modelling on long text are UCI  and UMass . However, both UCI and UMass metrics are not good indicators for quality of topics in short text topic modelling due to the sparseness problem. In Ref. \onlinecite{quan-2015}, a purity measure is introduced to evaluate short text topic modelling; however, it requires pairs of short and long documents , and thus it is not applicable here because there is no long document associated to a given medical inquiry. Indeed, evaluation of short text topic modelling is an open research problem .    An additional challenge is the absence of labels. Performing annotations would require substantial manual effort by specialized medical professionals, and would be of limited use because one of the main goals is to discover previously unknown topics as new inquiries are received. The absence of labels precludes the use of the metrics based on purity and normalized mutual information proposed in Ref. \onlinecite{rosenberg-2007}, \onlinecite{huang-2013}, \onlinecite{yin-2014}.    distributional semantic Ref. \onlinecite{aletras-2013} bring forward the valuable idea of using distributional semantic to evaluate topic coherence, exploiting the semantic similarity learned by word2vec models. Topic coherence is assessed by calculating the similarity among the top n-words of a given topic: semantically similar top n-words lead to higher topic coherence. If this might be in general desirable, in the case of discovering medical topics it is actually detrimental: interesting  topics are often characterized by top n-words which are not semantically similar.  For example, a medical topic having as top 2-words rivaroxaban  and glutine is clearly relevant from a medical topic discovery standpoint. However, rivaroxaban and glutine are not semantically similar, and thus the metric proposed in Ref. \onlinecite{aletras-2013} would consider this as a low coherence  topic, in stark contrast with human expert judgment.   Analogous considerations apply to the indirect confirmation measures in Ref. \onlinecite{roeder-2015}: words emerging in novel topics would have rarely appeared before in a shared context. For this reason, we introduce a new measure of topic compactness which takes into account the semantics of the inquiries, and does not require any labeled data. Specifically, we compute the similarity of all inquiries belonging to a given topic with each other , sum the elements of the resulting similarity matrix, and divide by the total number of elements in this matrix. The topic semantic compactness  of topic  reads  where  is the cardinality of topic  ,   is the word vector representing inquiry  , and  is a function quantifying the semantic similarity between inquiry  and , taking values between 0 and 1 . Given the chosen normalization factor ,  and thus  can be directly used as  topic quality score.  The topic compactness maximum  is attained if and only if every sentence  contains exactly the same words. It is important to point out that  automatically takes semantics into account: different but semantically similar medical inquiries would still have high similarity score, and thus would lead  to a high topic semantic compactness, despite these inquiries using different words to express similar content.     add here example of glutine Contrary to Ref. \onlinecite{aletras-2013}, the topic semantic compactness  introduced in Eq. does not artificially penalize novel topics just because they associate semantically different words appearing in the same inquiry. To come back to the previous example, if numerous inquiries in a discovered topic contain the words rivaroxaban and glutine, the topic semantic compactness would be high , regardless from the fact that the top 2-words are not semantically similar since the similarity is evaluated at the inquiry level .  It is also beneficial to evaluate how representative the topic name is for the topic it represents. To this end, we calculate the name saliency  for medical topic  by calculating the similarity of the word vector representing the topic name with the word vectors representing the inquiries in the topic, sum these similarity values, and divide by the total number of inquiries in the topic. This reads  where  is the cardinality of topic  ,  is the word vector representing the name of topic , and  is the vector representing inquiry .  This returns a score  which quantifies how representative  the name is for the topic it represents. As in the case of the topic semantic compactness, the name saliency  takes natively semantics  into account via  in Eq. .   In both Eq.  and Eq. , the cosine similarity is used as similarity measure.  \section{Competing interests} Financial support for the research was provided by Bayer AG. The authors reports a  patent application on Topic Modelling of Short Medical Inquiries submitted on April 21st, 2020 .      {}  \section*{Author Contributions} A.Z. led and thereby ideated and implemented the topic discovery algorithm, and is the main author the manuscript. M.S., C.B., D.R. provided valuable suggestions on the topic discovery algorithm. C.B., O.T., and T.W. designed and implemented the software architecture and data engineering pipeline for the algorithm deployment. T.W., J.V., J.L., S.K., X.M., A.M., D.R., and M.S. provided the in-house resources for the study, supervised the overall project, and provided domain knowledge expertise. All authors revised and commented on the manuscript.  \section*{Data availability} The data used in the study are the proprietary of Bayer AG, and not publicly available.   A.Z. thanks Robin Williams and Nikki Hayward from Bayer\texttrademark\ Medical Information for providing expert insightful and in-depth feedback on the results of topic discovery.     include your own bib file like this:       
","   The dynamics of language is of fundamental importance in social sciences as a proxy for cultural evolution . Complex system methods seek to understand the emergence of the encoding capabilities of language , and evolutionary approaches  -- following the Bayesian tradition from Phylogenetics,   study the competition between grammar and syntax in the context of historical linguistics . Closer to our line of work, research of online communities point to temporal linguistic changes as means to enforce community norms . Our methodologies aim at studying similar systems in the e-commerce review context, wherein linguistic change is relevant in time scales of months and years.  The work on language dynamics from the machine learning community has mainly focused on the dynamics of word embeddings and topics. On the one hand, different embeddings, as e.g. word2vec , are trained in slices of temporal data and alignments methods are performed a posteriori . The probabilistic framing of word embeddings has, in contrast, allowed for a stochastic-process point of view of embedding evolution .    On the other hand, within the dynamic topic modelling approach,     inheriting the Bayesian perspective from topic models   the parameters of models like Latent Dirichlet Allocation are defined to follow known stochastic processes in time .    Likewise, self-exciting point processes in time have allowed for clustering of document streams . Lastly, while writing this paper we found , in which a RNN language model is conditioned on a global dynamic latent variable.   More recently, and most closely related to our work,    RNN language models have been conditioned on a global dynamic latent variable.    In contrast to this work, our dynamic representations explicitly encode both timing and content of past reviews, and can capture non-Markovian dynamics.    From the point of view of the datasets used in the literature,   Finally, within the recommender system realm deep neural networks models of review data for rating predictions use embedding representations, as well as convolutional neural networks . They also provide characterization of review usefulness, use reviews for product description , and provide better representations for rating prediction . The need to interact with the costumer has also led to question answering solutions . Different from these works, we focus on the temporal aspects of review content.",258
"  Most authentication methods commonly used today rely on users setting custom passwords to access their accounts and devices. Password-based authentications are popular due to their ease of use, ease of implementation and the established familiarity of users and developers with the method.   However studies show that users tend to set their individual passwords predictably, favoring short strings, names, birth dates and reusing passwords across sites.  Since chosen passwords exhibit certain patterns and structure, it begs the question whether it is possible to simulate these patterns and generate passwords that a human user realistically might have chosen.  Password guessing is an active field of study, until recently dominated by statistical analysis of password leaks and construction of corresponding generation algorithms . These methods rely on expert knowledge and analysis of various password leaks from multiple sources to generate rules and algorithms for efficient exploitation of learned patterns.  On the other hand, in recent years major advances in machine-driven text generation have been made, notably by novel deep-learning based architectures and efficient training strategies for large amounts of training text data. These methods are purely data driven, meaning they learn only from the structure of the input training text, without any external knowledge on the domain or structure of the data. % Deep learning models have recently shown remarkable performance concerning text classification and text generation.  Major advancements in the field have been fueled by the development in several central directions such as:    In this paper we will continue the exploration of data driven deep-learning text generation methods for the task of password-guessing. While some applications to password guessing already show promising results, most frameworks still can not reach or surpass state-of-the-art password generation algorithms. % On the other hand, considering password guessing problems, some popular frameworks  as well as a large body of state-of-art research suggest that advanced deep learning methodologies are still to be further explored.  Ideally, one would attempt to design more efficient password-guessing models aided by neural networks and cutting-edge practices.  Our findings and contributions can be summarized as follows:       Password generation has a long history outside of deep-learning architectures. There are tools available for purely rule-based approaches , which generate password candidates either by brute-force or dictionary attacks,  in which a dictionary of words or previously known passwords is augmented by a set of rules, either hand-written or machine generated .  Machine-learning based approaches to password guessing may come in their most simple form as regular -gram Markov Models  or more sophisticated approaches like probabilistic context free grammar  , which analyses likely structures in a password training set and applies various generation algorithms based on these observations.  Neural network based password generation has become an active field of study in the recent years. Ranging from relatively simple recurrent neural net  architectures  to recent seminal  works applying state-of-the-art text generation methods to password generation: Generative adversarial networks  , Wasserstein Autoencoders , and bidirectional RNNs trained with the aid of pre-trained Transformer models .  Our work extends this palette of deep learning architectures with the Variational Autoencoder  and Transformer-based language models . We additionally offer an extensive, unified and controlled comparison between the both various deep-learning based methods and more established methods mentioned above. This analysis yields a stable benchmark for the introduction of novel models.    The literature on password guessing is vast with methods ranging from probabilistic Markov and Rule-based ones \BG{Cite} to sophisticated hierarchical chains of deep neural networks \BG{Cite}.   The present paper builds upon and extends recent seminal works such as , , , in which various models based on deep neural networks have been put forward - PassGAN, WAE and Transformers. However, an extensive, unified and controlled comparison between the methods in terms of  password generation is missing. In our work this important analysis is provided and thus yields a stable benchmark for the introduction of novel models.    Our novel password-guessing approach based on variational autoencoders  follows the classical constructions in  combined with further dataset fragmentation and vocabulary augmentation.      In this work we introduced neural dynamic language models of text for review data. We are able to leverage dynamic representations of point process models in language modelling tasks, and augment the point processes with text representations.     . We provide two dynamical models, as well as their extension through two different language models: recurrent and temporal convolution networks.  We showed that our approach improves performance on both content and arrival times prediction, as well as opens the door for dynamic generative language models. Future work includes the implementation of attention mechanisms, as well as the inclusion of neural factorization machines aimed at predicting ratings values.     
","  Password generation has a long history outside of deep-learning architectures. There are tools available for purely rule-based approaches , which generate password candidates either by brute-force or dictionary attacks,  in which a dictionary of words or previously known passwords is augmented by a set of rules, either hand-written or machine generated .  Machine-learning based approaches to password guessing may come in their most simple form as regular -gram Markov Models  or more sophisticated approaches like probabilistic context free grammar  , which analyses likely structures in a password training set and applies various generation algorithms based on these observations.  Neural network based password generation has become an active field of study in the recent years. Ranging from relatively simple recurrent neural net  architectures  to recent seminal  works applying state-of-the-art text generation methods to password generation: Generative adversarial networks  , Wasserstein Autoencoders , and bidirectional RNNs trained with the aid of pre-trained Transformer models .  Our work extends this palette of deep learning architectures with the Variational Autoencoder  and Transformer-based language models . We additionally offer an extensive, unified and controlled comparison between the both various deep-learning based methods and more established methods mentioned above. This analysis yields a stable benchmark for the introduction of novel models.    The literature on password guessing is vast with methods ranging from probabilistic Markov and Rule-based ones \BG{Cite} to sophisticated hierarchical chains of deep neural networks \BG{Cite}.   The present paper builds upon and extends recent seminal works such as , , , in which various models based on deep neural networks have been put forward - PassGAN, WAE and Transformers. However, an extensive, unified and controlled comparison between the methods in terms of  password generation is missing. In our work this important analysis is provided and thus yields a stable benchmark for the introduction of novel models.    Our novel password-guessing approach based on variational autoencoders  follows the classical constructions in  combined with further dataset fragmentation and vocabulary augmentation.",259
" % 1 page  % Definition and importance of the causality knowledge.  % causality knowledge, as an important knowledge for artificial intelligence  systems, has been proven helpful in many downstream tasks, especially in the NLP domain. % % In this work, we follow ConceptNet and COPA to focus on the causal relations between daily events. % However, due to the lack of a high-quality and large-scale causality knowledge resource, the application of causality knowledge in downstream tasks is still limited.  Humans possess a basic knowledge about facts and understandings for commonsense of causality in our everyday life.  For example, if we leave five minutes late, we will be late for the bus; if the sun is out, it's not likely to rain; and if we are hungry, we need to eat. %Causality is an important commonsense reasoning that humans use all the time,  Such causality knowledge has been shown to be helpful for many NLP tasks. Thus, it is valuable to teach machines to understand causality.   Causal relations in the commonsense domain are typically contributory and contextual.  %   By contributory\footnote{The other two levels are absolute causality  and conditional causality , which commonly appear in the scientific domain rather than our daily life.}, we mean that the cause is neither necessary nor sufficient for the effect, but it strongly contributes to the effect.  By contextual, we mean that some causal relations only make sense in a certain context. The contextual property of causal relations is important for both the acquisition and application of causal knowledge. For example, if some people tell the AI assistant  ``they are hungry'' in a meeting, a basic assistant may suggest them to order food because it has the knowledge that `being hungry' causes `eat food'. A better assistant may suggest ordering food after the meeting because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the meeting context.  % \ye{I made small adaptation to this paragraph } % For example, if a person is in the middle of a meeting, he/she may tell the AI assistant  that he/she is hungry, a good AI assistant may suggest him/her to eat some food because it has the knowledge that `being hungry' cause `eat food', but an extraordinary AI assistant may suggest that ``I can help order some food for you to eat after the meeting'' because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the context of a meeting. Without understanding the contextual property of causal knowledge, achieving such a level of intelligence would be challenging.  To help machines better understand the causality commonsense, many efforts have been devoted into developing the causality knowledge bases.  For example, ConceptNet and ATOMIC leverage  human-annotation to acquire small-scale but high-quality causality knowledge. After that, people try to leverage linguistic patterns  to acquire causality knowledge from textual corpus. However, causality knowledge, especially those trivial knowledge for humans, are rarely formally expressed in documents, a pure text-based approach might struggle at covering all causality knowledge. Besides that, none of them take the aforementioned contextual property of causal knowledge into consideration, which may restrict their usage in downstream tasks.     % Causal relations in the commonsense domain are typically contributory and contextual.  % By contributory\footnote{The other two levels of causality are absolute causality  and conditional causality , which commonly appear in the scientific domain rather than our daily life.}, we mean that the cause is neither necessary nor sufficient for the effect, but it strongly contributes to the effect.  % By contextual, we mean that some causal relations only make sense in a certain context. % The contextual property of causal relations is important for both the acquisition and application of causality knowledge. % For example, if some people tell the AI assistant  ``they are hungry'' in a meeting, a basic assistant may suggest them to order food because it has the knowledge that `being hungry' causes `eat food'. A better assistant may suggest ordering food after the meeting because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the meeting context.  % Without understanding the contextual property of causality knowledge, achieving such a level of intelligence would be challenging.  %       %      %     } %      %      % \end{table}     % % limitation of existing acquisition methods % Conventional approaches  \ye{i think this should be more elaborated. maybe give an example?} However, two drawbacks of these approaches significantly limit their usage in downstream tasks: %     In this paper, we propose to ground causality knowledge into the real world and explore the possibility of acquiring causality knowledge from visual signals .  By doing so, we have three major advantages:  Videos can be easily acquired and can cover rich commonsense knowledge that may not be mentioned in the textual corpus;  Events contained in videos are naturally ordered by time. As discussed by, there exists a strong correlation between temporal and causal relations, and thus such time-consecutive images can become a dense causality knowledge resource;  Objects from the visual signals can act as the context for detected causality knowledge, which can remedy the aforementioned lack of contextual property issue of existing approaches.   To be more specific, we first define the task of mining causality knowledge from time-consecutive images and propose a high-quality dataset .  To study the contextual property of causal relations, for each pair of events, we provide two kinds of causality annotations: one is the causality given certain context and the other one is the causality without context.  Distribution analysis and case studies are conducted to analyze the contextual property of causality. An example from Vis-Causal is shown in Figure, where the causal relation between ``dog is running'' and ``blowing leaves'' only makes sense when the context is provided because the dog is running on the leaves, so its high speed and quickly-moved pow cause the leaves blow around. Without the context  ``leaves on the ground'', this causal relation is implausible. After that, we propose a Vision-Contextual Causal  model, which can effectively leverage both the pre-trained textual representation and visual context to acquire causality knowledge and can be used as a baseline method for future works. Experimental results demonstrate that even though the task is still challenging, by jointly leveraging the visual and contextual representation, the proposed model can better identify meaningful causal relations from time-consecutive images. To summarize, the contributions of this paper are three-fold:  We formally define the task of mining contextual causality from the visual signal;  We present a high-quality dataset Vis-Causal;  We propose a Vision-Contextual Causal  model to demonstrate the possibility of mining contextual causality from the vision signal. % Experimental results prove that considering context is crucial for understanding causality and representing the visual context with textual representation is helpful. % Further analysis shows that the proposed task is still challenging for current models, and we may need to consider injecting external knowledge to better understand the videos and acquire causality knowledge. % \ye{there's no real reference to the text part in the into, NLP people might think it's not suitable for ACL? maybe add that the models use some description and objects which are represented in a textual form}   %   %      In this section, we introduce related work about causality acquisition and visually-grounded NLP.   \Green{The reviewer also recommend us to cite related work in visual common sense. Here is the recommendation list. To the best of my knowledge, I think the most difference between these works and ours is that we leverage time-consecutive images.      }        As a crucial knowledge for many artificial intelligence  systems, causality has long been an important research topic in many communities with different focuses. For example, in the machine learning community, researchers are focusing on modeling causality from structured data . Different from them, researchers from the computer vision community are focusing on identifying key objects or events in images that can cause certain decision makings. Last but not least, previous works in the natural language processing  community are mostly working on acquiring causality knowledge via either crowd-sourcing or linguistic pattern mining and then applying the acquired knowledge for understanding human language. The ultimate goal of this paper is the same as previous NLP works that we are trying to acquire causality knowledge, which can be stored and used for downstream tasks. But the approach is different. To the best of our knowledge, this is the first work exploring the possibility of directly acquiring causality from the visual signal.   Moreover, in this work, we first model the contextual property of causal relations, which is common and vital in downstream applications. Another related work from the CV community is Visual-COPA, which asks models to identify if one image can cause another. The major difference is that our paper is trying to extract causality knowledge rather than leveraging external knowledge to predict image relations.       Prior works on causality knowledge acquisition typically rely on crowdsourcing or linguistic pattern mining over textual data.   Besides these approaches that focus only on the causality knowledge, based on the observation that there exist a strong correlation of temporal and causal relation between events, \citet{DBLP:conf/acl/RothWNF18} develops a joint training framework to learn temporal and causality knowledge simultaneously.    One shared drawback of these previous works is that they did not take the contextual property of causal relations, which is common and vital in real applications, into consideration.   To remedy this problem, we propose a novel setting that simulates how human beings see the world and leverage the visual signal as context for better causality knowledge representation and acquisition.     As the intersection of computer vision  and natural language processing , visually-grounded NLP research topics are popular in both communities. For example, image captioning aims at generating captions for images and scene graph generation tries to detect not just entities but also events or states from images. Besides these basic tasks, some other visually-grounded NLP tasks  and visual dialogue) are also created to test how well models can understand human language and visual signals jointly.   For example, visual question answering  tests models' ability to answer questions given certain images and visual dialogue aims at understanding dialogues and generating suitable dialog response with the help of the visual signal.   Different from these works, we are not focusing on a specific task. Instead, we are trying to mine knowledge , which can be applied in downstream NLP tasks, from the visual signal. Another line of related works is visual commonsense reasoning, which aim at either extracting commonsense knowledge from the images or evaluating models' commonsense reasoning abilities over images. Considering that the causality often happens between events that appear in the temporal order, which are unlikely to appear in the same image, we choose to work on time-consecutive image pairs rather than a single image.     The present work illustrates various deep learning password generation techniques. Conducting a thorough unified analysis we discuss password-matching capabilities, variability and quality of sampling and robustness in training. On one hand, we bridge and extend previous methods based on attention schemes, GANs and Wasserstein autoencoding; on the other hand, we provide a promising novel approach based on Variational Autoencoders that allows for efficient latent space modeling and further sampling mechanisms. Lastly, we hope our work will facilitate and provide benchmark lines for further deep learning and ML practitioners interested in the field of password guessing.  In terms of further investigation, the application of deep learning techniques to password generation poses further intriguing questions on the interplay between classical probabilistic methods and neural networks, where one would ultimately hope to construct more efficient and reliable domain-inspired password representation schemes - e.g. based on carefully crafted fragmentations.   
","    In this section, we introduce related work about causality acquisition and visually-grounded NLP.   \Green{The reviewer also recommend us to cite related work in visual common sense. Here is the recommendation list. To the best of my knowledge, I think the most difference between these works and ours is that we leverage time-consecutive images.      }        As a crucial knowledge for many artificial intelligence  systems, causality has long been an important research topic in many communities with different focuses. For example, in the machine learning community, researchers are focusing on modeling causality from structured data . Different from them, researchers from the computer vision community are focusing on identifying key objects or events in images that can cause certain decision makings. Last but not least, previous works in the natural language processing  community are mostly working on acquiring causality knowledge via either crowd-sourcing or linguistic pattern mining and then applying the acquired knowledge for understanding human language. The ultimate goal of this paper is the same as previous NLP works that we are trying to acquire causality knowledge, which can be stored and used for downstream tasks. But the approach is different. To the best of our knowledge, this is the first work exploring the possibility of directly acquiring causality from the visual signal.   Moreover, in this work, we first model the contextual property of causal relations, which is common and vital in downstream applications. Another related work from the CV community is Visual-COPA, which asks models to identify if one image can cause another. The major difference is that our paper is trying to extract causality knowledge rather than leveraging external knowledge to predict image relations.       Prior works on causality knowledge acquisition typically rely on crowdsourcing or linguistic pattern mining over textual data.   Besides these approaches that focus only on the causality knowledge, based on the observation that there exist a strong correlation of temporal and causal relation between events, \citet{DBLP:conf/acl/RothWNF18} develops a joint training framework to learn temporal and causality knowledge simultaneously.    One shared drawback of these previous works is that they did not take the contextual property of causal relations, which is common and vital in real applications, into consideration.   To remedy this problem, we propose a novel setting that simulates how human beings see the world and leverage the visual signal as context for better causality knowledge representation and acquisition.     As the intersection of computer vision  and natural language processing , visually-grounded NLP research topics are popular in both communities. For example, image captioning aims at generating captions for images and scene graph generation tries to detect not just entities but also events or states from images. Besides these basic tasks, some other visually-grounded NLP tasks  and visual dialogue) are also created to test how well models can understand human language and visual signals jointly.   For example, visual question answering  tests models' ability to answer questions given certain images and visual dialogue aims at understanding dialogues and generating suitable dialog response with the help of the visual signal.   Different from these works, we are not focusing on a specific task. Instead, we are trying to mine knowledge , which can be applied in downstream NLP tasks, from the visual signal. Another line of related works is visual commonsense reasoning, which aim at either extracting commonsense knowledge from the images or evaluating models' commonsense reasoning abilities over images. Considering that the causality often happens between events that appear in the temporal order, which are unlikely to appear in the same image, we choose to work on time-consecutive image pairs rather than a single image.",260
"   Intuitively, if you see a lot of examples of natural language questions about TV shows, it ought to also help understand similar syntax in questions about movies, or in questions that refer to both movies and TV shows together. Ideally, the training examples from the related domain should strictly improve performance, not hurt it. %[nkscales] FYI -- I reverted the below sentence to close to its original form to better match the tone of the first paragraph. If the sentence still doesn't sound right, let me know. If you can satisfy that property, then you have at least a chance at eventually achieving arbitrarily robust performance across a range of domains, given sufficient training data in aggregate. %You need to satisfy that property in order to at have a shot at achieving arbitrarily robust performance across a range of domains, given simply sufficient data across those domains in aggregate.  How and to what extent current machine learning  approaches can be made to robustly solve natural language understanding  at the scale of arbitrary natural language across domain -- with or without access to large quantities of training data -- remains, however, an open question.  On one hand, research into the scaling behavior of deep learning systems has found generalization loss to decrease reliably with training size and model size in a power law or related logarithmic relationship across a range of architectures and tasks, from image classification with convolutional neural networks~ to language modeling with Transformers~. Recent results in an i.i.d.\ setting show this pattern to persist across many orders of magnitude, with no established upper limit~.  At the same time, it has been shown that current ML systems continue to struggle to achieve robust performance in classes of tasks that require compositional generalization  % [nikola] IMO this part of the sentence does not contribute much. I suggest skipping it and only keeping the citation. %-- that is, tasks in which known building blocks must be composed at test time in ways unseen during training ~ -- an ability that has been argued to be crucial to robust language understanding~.  In this paper, we combine these two lines of research by investigating the effect of training size on error rates in the context of a compositional task. Specifically, we derive a suite of extended datasets based on the Compositional Freebase Questions  semantic parsing benchmark~. We then use the compositional structure of each example to construct controlled experiments that measure the error rates when increasing training size in settings requiring compositional generalization and in settings simulating scaling to a broader scope of natural language. We apply these experiments to analysis of Transformers~ in a setting of fixed computational cost -- that is, of fixed model size and fixed training steps -- and demonstrate key limits to their scalability in this setting.  Our contributions are the following:    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%     The relationship between training size and error rates or generalization loss has been studied in a variety of settings. Early research observed that accuracy varies roughly with the log of the training size, both in shallow ML approaches on a natural language disambiguation task~ and in convolutional neural networks on an object detection task~. Others, noting the connection with learning curves observed during training on a large train set, have tried fitting a power law learning curve to the relationship between training size and error rate~.  More recent research has observed consistent power law relationships between training size and cross-entropy error across a range of deep learning architectures, including LSTMs on language modeling and machine translation tasks~ and LSTMs or Transformers on language modeling tasks~, provided that training size and model size are increased in tandem. In particular \citet{kaplan2020scaling} observe that a highly consistent power law relationship between cross-entropy loss and either of training size or model size persists across many orders of magnitude when not bottlenecked by the other of the two. Our approach differs from this previous research by specifically investigating the effect of the compositional structure of a task on the training size to error rate relationship.  The degree to which additional training data from a related domain can help or hurt performance has been investigated also in the context of multi-domain learning~ and domain adaptation~.  Our experiments in Section resemble the scenario of multi-domain learning  in that we aim to train a single model that can apply to multiple domains, not just the single domain from which our test set is drawn. Multi-domain learning seeks, however, to improve performance on individual domains by explicitly distinguishing the domain of each example at train and test time, which is not our focus here. We expect that explicitly distinguishing domains would be less beneficial in the \starcfq{} tasks than in scenarios where the expected output for a given input can differ depending on the domain , but we would welcome investigation into the effectiveness of MDL techniques on \starcfq{} as future work.  Our experiments in Section can be considered a form of domain adaptation  in that a model targeting a single domain is trained using a combination of in-domain and out-of-domain data. Our approach corresponds most closely to the instance weighting approach, particularly the batch weighting variant, which \citet{wang2017instance} found to yield superior performance over other instance weighting techniques. Another popular DA technique which we have not evaluated in our experiments is that of pre-training on a broader data set followed by fine-tuning on in-domain data~, which may be augmented with selections from the out-of-domain data ranked by similarity to the in-domain distribution~.  In particular, much attention has been paid recently to the significant improvements in performance achievable on a variety of downstream natural language tasks through pre-training of large scale language models~, including in scenarios of domain adaptation and multi-domain learning~.  \citet{talmor2019multiqa} observe that the performance of BERT-large~ can be improved on several reading comprehension  benchmarks by additionally pre-training it on a selection of 75k supervised examples from each of 5 different large RC datasets, prior to finally fine-tuning it on the specific target dataset. In cases where the target dataset is large, however, they find that the additional pre-training from the other RC datasets improves performance only about half the time.  \citet{gururangan2020don} show that applying additional pre-training on unlabeled examples from the target domain improves performance on a number of classification tasks compared to use of RoBERTa~ alone.  Our approach differs from this previous research again by  investigating the effect of varying amounts of supervised rather than unsupervised training data, using knowledge of the compositional structure of the task to characterize the relationship between the target domain and related domains, and illustrating the effect of these factors on the slope and limit value of the performance curves.                                                               In this paper, we reviewed various end-to-end all neural automatic speech recognition systems and their optimization techniques for on-device applications. On-device speech recognition has huge advantages compared to the server-side ones in terms of user privacy, operation without internet, server-cost, and latency.  To operate speech recognition systems on embedded processors, we need to  consider several factors such as recognition accuracy, computational cost, latency,  and the model size. We compared pros and cons of different neural network components such as Long Short-Term Memory , Convolutional Neural Network ,   and attention mechanism. We explained and compared different end-to-end neural speech recognition architectures such as  a stack of LSTM layers with the Connectionist Temporal Classification  loss ,  Recurrent Neural Network-Transformer , attention-based models, and models based on Monotonic Chunk-wise Attention  . Further improvement is achieved by combining a streaming model with a low-latency non-streaming model,  by applying shallow-fusion with a Language Model , and by applying spell correction using a list of named entities . We also discussed several model compression techniques including quantization, singular value decomposition, pruning, and knowledge distillation.  These recent advances in all neural end-to-end speech recognition made it possible to commercialize all neural on-device end-to-end speech recognition systems  .                           
","   The relationship between training size and error rates or generalization loss has been studied in a variety of settings. Early research observed that accuracy varies roughly with the log of the training size, both in shallow ML approaches on a natural language disambiguation task~ and in convolutional neural networks on an object detection task~. Others, noting the connection with learning curves observed during training on a large train set, have tried fitting a power law learning curve to the relationship between training size and error rate~.  More recent research has observed consistent power law relationships between training size and cross-entropy error across a range of deep learning architectures, including LSTMs on language modeling and machine translation tasks~ and LSTMs or Transformers on language modeling tasks~, provided that training size and model size are increased in tandem. In particular \citet{kaplan2020scaling} observe that a highly consistent power law relationship between cross-entropy loss and either of training size or model size persists across many orders of magnitude when not bottlenecked by the other of the two. Our approach differs from this previous research by specifically investigating the effect of the compositional structure of a task on the training size to error rate relationship.  The degree to which additional training data from a related domain can help or hurt performance has been investigated also in the context of multi-domain learning~ and domain adaptation~.  Our experiments in Section resemble the scenario of multi-domain learning  in that we aim to train a single model that can apply to multiple domains, not just the single domain from which our test set is drawn. Multi-domain learning seeks, however, to improve performance on individual domains by explicitly distinguishing the domain of each example at train and test time, which is not our focus here. We expect that explicitly distinguishing domains would be less beneficial in the \starcfq{} tasks than in scenarios where the expected output for a given input can differ depending on the domain , but we would welcome investigation into the effectiveness of MDL techniques on \starcfq{} as future work.  Our experiments in Section can be considered a form of domain adaptation  in that a model targeting a single domain is trained using a combination of in-domain and out-of-domain data. Our approach corresponds most closely to the instance weighting approach, particularly the batch weighting variant, which \citet{wang2017instance} found to yield superior performance over other instance weighting techniques. Another popular DA technique which we have not evaluated in our experiments is that of pre-training on a broader data set followed by fine-tuning on in-domain data~, which may be augmented with selections from the out-of-domain data ranked by similarity to the in-domain distribution~.  In particular, much attention has been paid recently to the significant improvements in performance achievable on a variety of downstream natural language tasks through pre-training of large scale language models~, including in scenarios of domain adaptation and multi-domain learning~.  \citet{talmor2019multiqa} observe that the performance of BERT-large~ can be improved on several reading comprehension  benchmarks by additionally pre-training it on a selection of 75k supervised examples from each of 5 different large RC datasets, prior to finally fine-tuning it on the specific target dataset. In cases where the target dataset is large, however, they find that the additional pre-training from the other RC datasets improves performance only about half the time.  \citet{gururangan2020don} show that applying additional pre-training on unlabeled examples from the target domain improves performance on a number of classification tasks compared to use of RoBERTa~ alone.  Our approach differs from this previous research again by  investigating the effect of varying amounts of supervised rather than unsupervised training data, using knowledge of the compositional structure of the task to characterize the relationship between the target domain and related domains, and illustrating the effect of these factors on the slope and limit value of the performance curves.",261
"   % Automatic speech recognition  systems are typically trained on vast quantity of paired audio and text data to attain competitive performance. Obtaining these paired data requires substantial human annotation efforts and is often time-consuming, expensive and error-prone. With the emerging popularity of end-to-end ASR models, the need for large amounts of training data is more demanding than the conventional hybrid-based ASR systems. For this purpose, semi-supervised learning  is often investigated for speech recognition, where a model is trained using a finite amount of labeled data and a much larger amount of unlabeled data.   In the long history of semi-supervised learning  in speech recognition, self-training approach  and knowledge distillation , or known as teacher-student model training  are the two commonly used SSL methods. Recent success of representation learning enables a new approach towards leveraging unlabeled data. In natural language processing community,  BERT, ELMo, XLNet , GPT   and its follow-ups are classical examples of representation learning. The key philosophy of representation learning is based on using self-supervised learning, where we obtain `free' labels from unlabeled data and train them in a supervised manner via some proxy tasks. In the context of BERT, two proxy tasks are defined including masked language model task and two-sequence prediction task. These proxy tasks are designed to force the learning of a robust, meaningful representation.  After the representation has been learned, a downstream task model is then trained using labeled data with the learned representation. Optionally, the representation learning block and downstream task block can be fine-tuned together.   Learning efficient speech representation can be traced back to restricted Boltzmann machine , which allows pre-training on large amounts of unlabeled data before training the deep neural network speech models.  More recently, speech representation learning has drawn increasing attention in speech processing community and has shown promising results in semi-supervised speech recognition .  The design of proxy tasks in learning speech representation can be categorized into two types. The first type is based on contrastive loss and has been applied to speech representation such as wav2vec and its variants . The model is trained to learn representations containing information that most discriminates the future or masked frame from a set of negative samples via contrastive loss.  The second type is based on reconstructive loss. The proxy task for these representation learning methods is to reconstruct temporal slices of acoustic features based on contextual information. These reconstruction tasks can be defined as autoregressive reconstruction, or masked-based reconstruction. APC  and its follow-up  are examples to use autoregressive reconstruction loss.  In many state-of-the-art pretrained language model task, masked-based prediction is adopted in the proxy tasks such as BERT  and XLNet .  In speech, instead of prediction, we randomly mask temporal slices of acoustic features and attempt to reconstruct them .  Orthogonal to the contrastive-/reconstructive-loss based speech representation learning, vector-quantized speech representations have been proposed. One motivation to apply vector quantization  is that enforcing quantization can lead to better linguistic unit discovery  due to the discrete nature of phonetic units. In VQ-APC , the authors use VQ as a way to limit model capacity and control information needed in encoding representation. In VQ-wav2vec  and wav2vec 2.0 , the author use VQ to facilitate direct application of BERT and other NLP algorithms.  In this paper, we introduce DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We take inspirations from many recent advances in speech representation learning, and propose multiple improvements over vanilla DeCoAR. We summarize the contributions of this paper as follows:   % The rest of the paper is organized as follows. Section gives a brief overview of our previous DeCoAR method and related work in vector quantized speech representation learning. Section describes the proposed DeCoAR 2.0 approach. Experimental results on semi-supervised speech recognition are presented in Section followed by conclusion in Section.   % Learning robust speech representation has been exploited in recent years. Among these approaches, wav2vec 2.0  uses 10 minutes of labeled data with 53k hours of unlabeled data to achieve a word error rate  of 5.2/8.6 on LibriSpeech benchmark. The model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the contrastive loss formulation can result in several locally optimal codebooks, for exmaples, acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations.   %Furthermore, the codes at each time step the model select right after their feature encoder hardly contained meaningful phonetic information. So their contrastive approach might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.   % A simple workaround could be using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information in the codes, helping mitigatate the codebook learning problems in contrastive loss as discussed above. And compared to simple reconstruction where we utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. By utilizing the VQ layer, the model is able to keep the representation from those unwanted information flowing.     % Automatic speech recognition  systems are typically trained on vast quantity of paired audio and text data to attain competitive performance. Obtaining these paired data requires substantial human annotation efforts and is often time-consuming, expensive and error-prone. With the emerging popularity of end-to-end ASR models, the need for large amounts of training data is more demanding than the conventional hybrid-based ASR systems. For this purpose, semi-supervised learning  is often investigated for speech recognition, where a model is trained using a finite amount of labeled data and a much larger amount of unlabeled data.   % In the long history of SSL in speech recognition, self-training approach  is the most commonly used approach. In self-training methods, a `seed' ASR model is trained using paired audio/text data. The resulting model is then applied to transcribe the unlabeled audio data. The resulting hypotheses, combined with different data selection criteria, are treated as `pseudo-labels' and added to the original labeled dataset to retrain a new model. Simple in concept, self-training works well in practice with one major caveat - the pseudo-label injects systematic bias introduced by the seed model. To alleviate this, careful confidence calibration with system combinations are often used . Another family of SSL is based on knowledge distillation , or teacher-student model training , and is mostly applied to acoustic model training in hybrid-based ASR. In these setups, a teacher model  generates frame-wise soft label instead of hard label, and a student model is trained on the soft labels via KL divergence loss instead of a standard cross-entropy loss based on forced alignment. The knowledge distillation based SSL partially mitigates the systematic bias but is rarely being investigated towards sequence-level loss  or end-to-end ASR systems.    % Recent success of efficient representation learning, in particular in natural language processing , enables a new approach towards leveraging unlabeled data. Classical examples of representation learning for NLP include BERT, ELMo, XLNet , GPT and its follow-ups , to name but a few.  The key philosophy of representation learning is based on self-supervised learning, where we obtain `free' labels from unlabeled data and train them in a supervised manner via some proxy tasks. In the context of the well-known BERT, two proxy tasks are defined including masked language model task and two-sequence prediction task. These proxy tasks are defined in a way to force the learning of a robust, meaningful representation.  A downstream task is then trained on the labeled data with the learned representation. Optionally, the representation learning block and downstream task can be fine-tuned together.     % This paper presents DeCoAR 2.0, a follow-up on DeCoAR . We take inspiration from many recent advances in speech representation learning, and propose multiple improvements over vanilla DeCoAR. We summarize the contributions of this paper as follows: %   % The rest of the paper is organized as follows. Section gives an overview on related work in speech representation learning, and a brief recap of our previous DeCoAR method. Section describes the proposed vector quantized DeCoAR approach. Experimental results on semi-supervised speech recognition are presented in Section followed by conclusion in Section.   % In this work, we propose an improved speech representation learning paradigms towards semi-supervised speech recognition based on our previous work .   % Current state-of-the-art models for speech recognition require vast amounts of transcribed audio data to attain good performance. In particular, end-to-end ASR models are more demanding in the amount of training data required when compared to traditional hybrid models. While obtaining a large amount of labeled data requires substantial effort and resources, it is much less costly to obtain abundant unlabeled data.   % For this reason, semi-supervised learning  is often used when training ASR systems. Recently, self-supervised learning閳ユ攣 paradigm that treats the input itself or modifications of the input as learning targets閳 has obtained promising results. Those self-supervised speech representation can be fall into main categories: Contrastive Predictive Coding  incorporates contrastive objective to learn representations containing information that  most discriminates the future or masked frame from a set of negative samples. Another approach is Autoregressive Predictive Coding  , which tries to directly predict or reconstruct the frame based on context.  % More recently, vector-quantized representations of audio data has drawn increasing attention in speech processing . The motivation is that enforcing the quantization leads to a better representation for acoustic unit discovery due to the discrete nature of phonetic units. VQ-APC  also try to exactly quantified of information , to control the capacity of the models. And the use of vector quantization limited capacity are forced to retain information to achieve maximal prediction.   % Despite the success of the wav2vec 2.0 model , the model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the codes at each time step the model select right after feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation can result in several locally optimal codebooks. A few highly probable optima observed were acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations to enable a good contrastive loss. Hence, the codebook learning methodology using contrastive loss might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.    % A simple solution could be to enforce the codes to explicitly carry information about the input features in the process. Using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information, helping mitigatate the codebook learning problems in contrastive loss as discussed above. Thus, we propose an novel self-supervised model that learns vector quantized deep transformer acoustic representations based on frames reconstruction. Since simple reconstruction utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. And by utilizing the VQ layer to limit those unwanted information flow into final representation, Vector Quantized Deep Contextualized Acoustic Representations  are able to achieve much better representation that's better suited for semi-supervised ASR tasks. By using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using deep transformer and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification  loss for the ASR task. We only train the small ASR model instead of fine-tuning for computing-efficiency. Our approach showed that supervision with 10 hours of labeled data on DeCoAR 2.0 achieves performance on par with training on all 960 hours directly.          Learning efficient acoustic representation can be traced back to restricted Boltzmann machine , which allows pre-training on large amounts of unlabeled data before training the deep neural network acoustic models.  More recently, acoustic representation learning has drawn increasing attention   in speech processing and has shown promising results in semi-supervised speech recognition.  As mentioned in the introduction, the key to a robust representation learning is the design of the proxy supervised task.  Most proxy tasks in acoustic representation learning can fall into two categories.     The first one is based on contrastive loss and has been applied to speech representation such as wav2vec and its variants . To train acoustic representation with contrastive loss, an encoder network maps the original acoustic features  into a latent space. A context network maps multiple latent representations into a contextualized representation. The model is trained to learn representations containing information that most discriminates the future or masked frame from a set of negative samples via contrastive loss.      The second one is based on reconstructive loss. The proxy task for these representation learning methods is to reconstruct temporal slices of acoustic features. These reconstruction tasks can be defined as autoregressive reconstruction, or masked-based reconstruction. APC  and its follow-up  are examples to use autoregressive reconstruction loss.  In many state-of-the-at pretrained language model task, masked-based prediction is adopted in the proxy tasks such as BERT  and XLNet .  In speech, instead of prediction, we randomly mask slices of acoustic features and attempt to reconstruct them. .    For example, an autoregressive predictive coding model  was proposed in  for unsupervised speech representation learning and was applied to phone classification and speaker verification. WaveNet auto-encoders  proposed contrastive predictive coding  to learn speech representations and was applied on unsupervised acoustic unit discovery task. Wav2vec proposed a multi-layer convolutional neural network optimized via a noise contrastive binary classification and was applied to WSJ ASR tasks. As its follow-up work, Wav2vec-2.0 is introduced where the contrastive task is defined over a quantized latent representation. It was shown by training acoustic representation over 53,000-hour unlabeled audio data, they were able to attain competitive performance on LibriSpeech using only 10 minutes of labeled data. Similar idea has also been exploited to the APC model, where a vector-quantized APC model was proposed.     DeCoAR stands for deep contextualized acoustic representations, and was proposed in our previous work. As depicted in Figure, DeCoAR consists of two modules, an encoder module and a reconstruction module. For an input speech sequence , an encoder module consists of a stacked forward and backward LSTMs, and computes a hidden representation that encodes information from both previous and future frames . For each temporal slice , the reconstruction module takes the concatenated forward state at time  and backward state at  as inputs, and uses  position-dependent feed-forward networks to recontruct each frame. Formally, the DeCoAR objective is defined as follows:  where  is a position-dependent feed-forward network to reconstruct the -th frame in the slice.  The final loss  is calculated over all possible slices in the entire sequence in an autoregressive manner, defined as: .     \subsubsection{wav2vec 2.0} Wav2vec 2.0  is one of the successful examples in representation learning. It uses 10 minutes of labeled data with 53k hours of unlabeled data to achieve a word error rate  of 5.2\ /8.6\  on LibriSpeech benchmark. The model relies on a diverse codebook learned to correlate the underlying speech units to representations via contrastive loss. Discretizing the continuous representation enables applications of many state-of-the-art NLP algorithms. In wav2vec 2.0, after applying VQ operations, the model is trained using a masked LM style loss, similar to BERT.   One potential challenge in learning optimal codebooks with contrastive loss is posed by data with nuisance factors such as noise and other adverse conditions. In these cases, the codebook can be trivially optimized by assigning acoustic condition  to the codebook. A potential work-around is to use frame reconstruction as objective so that the network can leverage all available information of the input feature to guide the learning of a robust representation.  Another solution is to perform vector quantization operation at a higher layer , as opposed to a lower layer.    flow back to the the codes to preserve meaningful information, helping mitigate the codebook learning problems in contrastive loss as discussed above.    However, contrastive loss formulation can result in several locally optimal codebooks. For examples, acoustic condition-sensitive codebooks in which the model can easily be optimized by assigning acoustic condition  to the the codebooks, and temporal codebooks in which the model assigns specific codes to fixed temporal locations for a minimum contrastive loss. Thus, this codebook learning methodology using contrastive loss may not generalize well to all datasets, especially the real world data contained a lot of nuisance factor like noise, adverse environment.  A simple work-around is to use frame reconstruction as objective, so the network allows information of the input feature flow back to the the codes to preserve meaningful information, helping mitigate the codebook learning problems in contrastive loss as discussed above.  \subsubsection{VQ-APC} VQ-APC  introduced an novel approach that inserted a VQ layer before frame prediction. The motivation of using VQ is to quantify the information needed to encode speech representation and control the capacity of the models. The model uses autoregressive predictive coding  as objective, instead of a contrastive predictive coding . Their experiments showed APC/reconstruction objective performed better than CPC/constrastive objective under the same condition. They also demonstrated the learned VQ codes highly correlate to phoneme path, suggesting VQ can be used to capture linguistic units in an implicit way.    Since direct prediction make the model to utilize all the information available  to achieved maximal prediction. While most of those information are not important to ASR and also made the task complicated. And by utilizing the VQ layer to limit the capacity, the model is able to focus on only a few certain information. Their experiment also showed that APC objective performed better than CPC objective under the same condition.          Orthogonal to the contrastive-/reconstructive-loss based representation learning, vector-quantized representations of audio data has been proposed. The motivation is that enforcing the quantization leads to a better representation for acoustic unit discovery due to the discrete nature of phonetic units. VQ-APC  also try to exactly quantified of information , to control the capacity of the models \yuzong{Rewrite this}. And the use of vector quantization limited capacity are forced to retain information to achieve maximal prediction.       Despite the success of the wav2vec 2.0 model , the model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the codes at each time step the model select right after feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation can result in several locally optimal codebooks. A few highly probable optima observed were acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations to enable a good contrastive loss. Hence, the codebook learning methodology using contrastive loss might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.      A simple solution could be to enforce the codes to explicitly carry information about the input features in the process. Using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information, helping mitigatate the codebook learning problems in contrastive loss as discussed above. Thus, we propose an novel self-supervised model that learns vector quantized deep transformer acoustic representations based on frames reconstruction. Since simple reconstruction utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. And by utilizing the VQ layer to limit those unwanted information flow into final representation, Vector Quantized Deep Contextualized Acoustic Representations  are able to achieve much better representation that's better suited for semi-supervised ASR tasks. By using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using deep transformer and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification  loss for the ASR task. We only train the small ASR model instead of fine-tuning for computing-efficiency. Our approach showed that supervision with 10 hours of labeled data on DeCoAR 2.0 achieves performance on par with training on all 960 hours directly.     to reconstruct each frame in the original slice.     for each slice starting at time  and ending at . The reconstruction module takes the  and aims at reconstructing each frame     DeCoAR falls into the category of reconstructive loss based approach.  We train the DeCoAR model to predict a slice of acoustic feature vectors, given past and future acoustic vectors. As depicted on the left side of Figure, a stack of forward and backward LSTMs are applied to the entire unlabeled input sequence , where the  are 40-dimensional log filterbank features. The encoder network computes a hidden representation that encodes information from both previous and future frames  for each frame .  Given a sequence of acoustic feature inputs  , for each slice  starting at time step , our objective is defined as follows:      where  are the concatenated forward and backward states from the last LSTM layer, and      is a position-dependent feed-forward network with 512 hidden dimensions. The final loss  is summed over all possible slices in the entire sequence:       DeCoAR consists of two modules, a encoder network that encodes a temporal slice of filterbank features     To train DeCoAR, a deep bi-directional LSTM network encodes a temporal slice of filterbank features into a latent feature space       In our previous work , we proposed a different perspective towards semi-supervised speech recognition via representation learning. First, a large amount of unlabeled data is used to train robust acoustic representation by reconstructing temporal slices of filterbank features from past and future context frames in an autoregressive way. This acoustic representation is referred to as deep contextualized acoustic representations, or DeCoAR in short. The labeled data is then applied with the DeCoAR, which is used to train an ASR system. We observed highly comparable performance using limited amount of the labeled data using DeCoAR compared to using the entire 960-hour conventional filterbank features on LibriSpeech dataset.     In speech recognition, similar ideas have been investigated.  Those self-supervised speech representations fall into these categories: Contrastive Predictive Coding  incorporates contrastive objective to learn representations containing information that  most discriminates the future or masked frame from a set of negative samples. Another approach is Autoregressive Predictive Coding  , which tries to directly predict or reconstruct the frame based on context.    More recently, vector-quantized representations of audio data has drawn increasing attention in speech processing . The motivation is that enforcing the quantization leads to a better representation for acoustic unit discovery due to the discrete nature of phonetic units. VQ-APC  also try to exactly quantified of information , to control the capacity of the models. And the use of vector quantization limited capacity are forced to retain information to achieve maximal prediction.       Despite the success of the wav2vec 2.0 model , the model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the codes at each time step the model select right after feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation can result in several locally optimal codebooks. A few highly probable optima observed were acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations to enable a good contrastive loss. Hence, the codebook learning methodology using contrastive loss might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.      A simple solution could be to enforce the codes to explicitly carry information about the input features in the process. Using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information, helping mitigatate the codebook learning problems in contrastive loss as discussed above. Thus, we propose an novel self-supervised model that learns vector quantized deep transformer acoustic representations based on frames reconstruction. Since simple reconstruction utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. And by utilizing the VQ layer to limit those unwanted information flow into final representation, Vector Quantized Deep Contextualized Acoustic Representations  are able to achieve much better representation that's better suited for semi-supervised ASR tasks. By using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using deep transformer and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification  loss for the ASR task. We only train the small ASR model instead of fine-tuning for computing-efficiency. Our approach showed that supervision with 10 hours of labeled data on DeCoAR 2.0 achieves performance on par with training on all 960 hours directly.  In this work, we focused on the data-driven classification of chemical reactions with natural language processing methods and on the use of their embedded information to design reaction fingerprints. Our transformer-based models were able to learn the classification schemes using a broad set of chemical reactions as ground-truth, labeled by a  commercially available reaction classification tool. With the BERT classifier, we match the rule-based classification with an accuracy of 98.2\.\
","         Learning efficient acoustic representation can be traced back to restricted Boltzmann machine , which allows pre-training on large amounts of unlabeled data before training the deep neural network acoustic models.  More recently, acoustic representation learning has drawn increasing attention   in speech processing and has shown promising results in semi-supervised speech recognition.  As mentioned in the introduction, the key to a robust representation learning is the design of the proxy supervised task.  Most proxy tasks in acoustic representation learning can fall into two categories.     The first one is based on contrastive loss and has been applied to speech representation such as wav2vec and its variants . To train acoustic representation with contrastive loss, an encoder network maps the original acoustic features  into a latent space. A context network maps multiple latent representations into a contextualized representation. The model is trained to learn representations containing information that most discriminates the future or masked frame from a set of negative samples via contrastive loss.      The second one is based on reconstructive loss. The proxy task for these representation learning methods is to reconstruct temporal slices of acoustic features. These reconstruction tasks can be defined as autoregressive reconstruction, or masked-based reconstruction. APC  and its follow-up  are examples to use autoregressive reconstruction loss.  In many state-of-the-at pretrained language model task, masked-based prediction is adopted in the proxy tasks such as BERT  and XLNet .  In speech, instead of prediction, we randomly mask slices of acoustic features and attempt to reconstruct them. .    For example, an autoregressive predictive coding model  was proposed in  for unsupervised speech representation learning and was applied to phone classification and speaker verification. WaveNet auto-encoders  proposed contrastive predictive coding  to learn speech representations and was applied on unsupervised acoustic unit discovery task. Wav2vec proposed a multi-layer convolutional neural network optimized via a noise contrastive binary classification and was applied to WSJ ASR tasks. As its follow-up work, Wav2vec-2.0 is introduced where the contrastive task is defined over a quantized latent representation. It was shown by training acoustic representation over 53,000-hour unlabeled audio data, they were able to attain competitive performance on LibriSpeech using only 10 minutes of labeled data. Similar idea has also been exploited to the APC model, where a vector-quantized APC model was proposed.     DeCoAR stands for deep contextualized acoustic representations, and was proposed in our previous work. As depicted in Figure, DeCoAR consists of two modules, an encoder module and a reconstruction module. For an input speech sequence , an encoder module consists of a stacked forward and backward LSTMs, and computes a hidden representation that encodes information from both previous and future frames . For each temporal slice , the reconstruction module takes the concatenated forward state at time  and backward state at  as inputs, and uses  position-dependent feed-forward networks to recontruct each frame. Formally, the DeCoAR objective is defined as follows:  where  is a position-dependent feed-forward network to reconstruct the -th frame in the slice.  The final loss  is calculated over all possible slices in the entire sequence in an autoregressive manner, defined as: .     \subsubsection{wav2vec 2.0} Wav2vec 2.0  is one of the successful examples in representation learning. It uses 10 minutes of labeled data with 53k hours of unlabeled data to achieve a word error rate  of 5.2\ /8.6\  on LibriSpeech benchmark. The model relies on a diverse codebook learned to correlate the underlying speech units to representations via contrastive loss. Discretizing the continuous representation enables applications of many state-of-the-art NLP algorithms. In wav2vec 2.0, after applying VQ operations, the model is trained using a masked LM style loss, similar to BERT.   One potential challenge in learning optimal codebooks with contrastive loss is posed by data with nuisance factors such as noise and other adverse conditions. In these cases, the codebook can be trivially optimized by assigning acoustic condition  to the codebook. A potential work-around is to use frame reconstruction as objective so that the network can leverage all available information of the input feature to guide the learning of a robust representation.  Another solution is to perform vector quantization operation at a higher layer , as opposed to a lower layer.    flow back to the the codes to preserve meaningful information, helping mitigate the codebook learning problems in contrastive loss as discussed above.    However, contrastive loss formulation can result in several locally optimal codebooks. For examples, acoustic condition-sensitive codebooks in which the model can easily be optimized by assigning acoustic condition  to the the codebooks, and temporal codebooks in which the model assigns specific codes to fixed temporal locations for a minimum contrastive loss. Thus, this codebook learning methodology using contrastive loss may not generalize well to all datasets, especially the real world data contained a lot of nuisance factor like noise, adverse environment.  A simple work-around is to use frame reconstruction as objective, so the network allows information of the input feature flow back to the the codes to preserve meaningful information, helping mitigate the codebook learning problems in contrastive loss as discussed above.  \subsubsection{VQ-APC} VQ-APC  introduced an novel approach that inserted a VQ layer before frame prediction. The motivation of using VQ is to quantify the information needed to encode speech representation and control the capacity of the models. The model uses autoregressive predictive coding  as objective, instead of a contrastive predictive coding . Their experiments showed APC/reconstruction objective performed better than CPC/constrastive objective under the same condition. They also demonstrated the learned VQ codes highly correlate to phoneme path, suggesting VQ can be used to capture linguistic units in an implicit way.    Since direct prediction make the model to utilize all the information available  to achieved maximal prediction. While most of those information are not important to ASR and also made the task complicated. And by utilizing the VQ layer to limit the capacity, the model is able to focus on only a few certain information. Their experiment also showed that APC objective performed better than CPC objective under the same condition.          Orthogonal to the contrastive-/reconstructive-loss based representation learning, vector-quantized representations of audio data has been proposed. The motivation is that enforcing the quantization leads to a better representation for acoustic unit discovery due to the discrete nature of phonetic units. VQ-APC  also try to exactly quantified of information , to control the capacity of the models \yuzong{Rewrite this}. And the use of vector quantization limited capacity are forced to retain information to achieve maximal prediction.       Despite the success of the wav2vec 2.0 model , the model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the codes at each time step the model select right after feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation can result in several locally optimal codebooks. A few highly probable optima observed were acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations to enable a good contrastive loss. Hence, the codebook learning methodology using contrastive loss might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.      A simple solution could be to enforce the codes to explicitly carry information about the input features in the process. Using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information, helping mitigatate the codebook learning problems in contrastive loss as discussed above. Thus, we propose an novel self-supervised model that learns vector quantized deep transformer acoustic representations based on frames reconstruction. Since simple reconstruction utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. And by utilizing the VQ layer to limit those unwanted information flow into final representation, Vector Quantized Deep Contextualized Acoustic Representations  are able to achieve much better representation that's better suited for semi-supervised ASR tasks. By using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using deep transformer and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification  loss for the ASR task. We only train the small ASR model instead of fine-tuning for computing-efficiency. Our approach showed that supervision with 10 hours of labeled data on DeCoAR 2.0 achieves performance on par with training on all 960 hours directly.     to reconstruct each frame in the original slice.     for each slice starting at time  and ending at . The reconstruction module takes the  and aims at reconstructing each frame     DeCoAR falls into the category of reconstructive loss based approach.  We train the DeCoAR model to predict a slice of acoustic feature vectors, given past and future acoustic vectors. As depicted on the left side of Figure, a stack of forward and backward LSTMs are applied to the entire unlabeled input sequence , where the  are 40-dimensional log filterbank features. The encoder network computes a hidden representation that encodes information from both previous and future frames  for each frame .  Given a sequence of acoustic feature inputs  , for each slice  starting at time step , our objective is defined as follows:      where  are the concatenated forward and backward states from the last LSTM layer, and      is a position-dependent feed-forward network with 512 hidden dimensions. The final loss  is summed over all possible slices in the entire sequence:       DeCoAR consists of two modules, a encoder network that encodes a temporal slice of filterbank features     To train DeCoAR, a deep bi-directional LSTM network encodes a temporal slice of filterbank features into a latent feature space       In our previous work , we proposed a different perspective towards semi-supervised speech recognition via representation learning. First, a large amount of unlabeled data is used to train robust acoustic representation by reconstructing temporal slices of filterbank features from past and future context frames in an autoregressive way. This acoustic representation is referred to as deep contextualized acoustic representations, or DeCoAR in short. The labeled data is then applied with the DeCoAR, which is used to train an ASR system. We observed highly comparable performance using limited amount of the labeled data using DeCoAR compared to using the entire 960-hour conventional filterbank features on LibriSpeech dataset.     In speech recognition, similar ideas have been investigated.  Those self-supervised speech representations fall into these categories: Contrastive Predictive Coding  incorporates contrastive objective to learn representations containing information that  most discriminates the future or masked frame from a set of negative samples. Another approach is Autoregressive Predictive Coding  , which tries to directly predict or reconstruct the frame based on context.    More recently, vector-quantized representations of audio data has drawn increasing attention in speech processing . The motivation is that enforcing the quantization leads to a better representation for acoustic unit discovery due to the discrete nature of phonetic units. VQ-APC  also try to exactly quantified of information , to control the capacity of the models. And the use of vector quantization limited capacity are forced to retain information to achieve maximal prediction.       Despite the success of the wav2vec 2.0 model , the model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the codes at each time step the model select right after feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation can result in several locally optimal codebooks. A few highly probable optima observed were acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations to enable a good contrastive loss. Hence, the codebook learning methodology using contrastive loss might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.      A simple solution could be to enforce the codes to explicitly carry information about the input features in the process. Using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information, helping mitigatate the codebook learning problems in contrastive loss as discussed above. Thus, we propose an novel self-supervised model that learns vector quantized deep transformer acoustic representations based on frames reconstruction. Since simple reconstruction utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. And by utilizing the VQ layer to limit those unwanted information flow into final representation, Vector Quantized Deep Contextualized Acoustic Representations  are able to achieve much better representation that's better suited for semi-supervised ASR tasks. By using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using deep transformer and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification  loss for the ASR task. We only train the small ASR model instead of fine-tuning for computing-efficiency. Our approach showed that supervision with 10 hours of labeled data on DeCoAR 2.0 achieves performance on par with training on all 960 hours directly.",262
