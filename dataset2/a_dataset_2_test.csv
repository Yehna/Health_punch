document,summary,id
"  % 1. 娴犲绮涚拠顓㈢叾閸氬牊鍨氬Ο鈥崇烽崚鍡曡礋娑撱倓閲滈柈銊ュ瀻閿涘苯绱╅崙鐑樻拱閺傚洣瀵岀憰浣稿彠濞夈劎顑囨稉娑擃亝膩閸  Speech synthesis, also known as text-to-speech ,  has attracted a lot of attention and obtained satisfactory  results in recent years due to the advances in deep learning.  Several TTS systems based on deep networks were proposed,  such as Char2Wav , Tacotron2 ,  DeepVoice3 , Transformer TTS ,  FastSpeech  and ParaNet .  These systems usually first predict the acoustic feature sequence  from the input text sequence, and then generate waveform from  the acoustic feature sequence using vocoder such as  Griffin-Lim , WaveNet ,  WaveRNN , WaveGlow   and GAN-TTS .  % 2. 娴犲绮涢惄顔煎閻 閸忓厖绨琺el鐠嬮亶顣╁ù瀣秹缂佹粎娈戠拋鎹愵吀閸╃儤婀伴弬鐟扮础 LSTM, Conv, transformer  % According to the characteristics of network strucutre, current mainstream TTS systems can be divided into  % three types: RNN-based, CNN-based and Transformer-based.  % The RNN-based TTS systems, such as Char2Wav ,  % Tacotron 2  and Tacotron ,  % use the recurrent neural network  to design the main network structure,  % where the attention mechanism is applied to model the alignment  % between the acoustic feature sequence and the text sequence, % while the nature of RNN limits its parallelism.  % The CNN-based TTS systems, such as DeepVoice 3  % and ParaNet , adopt the convolution neural network  to model timing dependencies,  % which enable parallel processing at training.  % Especially in ParaNet , the iteratively refined attention mechanism is proposed to enable system  % to perform the inference process in parallel.  % The Transformer-based TTS systems, such as Transformer TTS , FastSpeech  and AlignTTS ,  % apply the architecture of Transformer to realize the process of speech synthesis.  % FastSpeech  uses the self-attention structure of Transformer to design a feed-forward network  % for predicting mel-spectrum in parallel, but needs guidance from an teacher autoregressive TTS model  % due to difficulty of learning alignment between text and mel-spetrum. % AlignTTS  proposes the alignment loss to make feed-forward TTS system capable of model the aligment  % without the guidance from other TTS systems.    % 2. 瀵洖鍤ぐ鎾冲鐠囶參鐓堕崥鍫熷灇鐎佃鏋冮張顒勬毐鎼达妇娈戦梽鎰煑  Although current speech synthesis systems have obtained  high-quality speech, it is difficult for them to achieve  satisfactory results in long text speech synthesis scenarios.  In the sequence-to-sequence TTS model, since the monotonicity  and locality properties of TTS alignment are not fully utilized,  the alignment procedure lacks robustness in inference,  which leads to skipping or repeating words, incomplete  synthesis, or an inability to synthesize long utterances  . To address the issue,  many monotonic attention mechanisms are presented  , where only  the alignment paths satisfying the monotonic condition  are taken into consideration at each decoder timestep.  In ,  the location-based GMM attention introduced in   is also studied in TTS systems to generalize to long utterances.  Especially, AlignTTS  proposes an alignment loss  to model the alignment between text and mel-spectrum, and uses  a length regulator to adjust the alignment, which solves the  instability problem of the alignment and is very efficient.  However, since the self-attention of Transformer   is used to model the dependencies  of input sequence elements in AlignTTS, the positional encodings  are required to introduce the positional information,  which limits the maximum length of input text.  In this paper, a novel self-attention mechanism is proposed to remove the need for the positional encodings and  lift the restriction of input text length.   % In Tacotron , the content-based attention mechanism introduced in   % is used to align the text and the melspectrum,  % but it does not exploit the monotonicity of TTS alignment. % Tacotron 2 uses the hybrid attention meachnism from   % which encourage the attention alignment to move forward consistently through the input sequence.   % which makes synthesis process instability. % long text sequence is not conducive to  % the calcualtion of the attention mechanism in TTS system,  % which affects the prediction of the acoustic feature and the stop token in inference.  % FastSpeech  and AlignTTS  use the length regulator instead of the attention mechanism,  % but the locational encoding of Transformer also limits its max length of input text.  % In order to lift this restriction, we design a novel self-attention mechanism  % to model the timing dependencies for TTS system.  % 3. 娑擃厽鏋冪拠顓㈢叾閸氬牊鍨氭稉顓炲彠娴滃酣鐓瑰瀣紦濡紕娈戦崚鍡樼 On the other hand, the prosody of speech directly affects  the overall listening perception of the voice, especially for  long utterances. In order to improve the naturalness of  synthetic speech, it is necessary for TTS systems to model  prosody information. In ,  a prosody embedding is introduced for emotional and  expressive speech synthesis, which enables fine-grained control  of the speaking style. In ,  an interpretable latent variable model for prosody based on  Tacotron 2 is presented to model phoneme-level and word-level  prosody information of speech.  proposes  a quantized latent feature for the prosody of speech, and trains  an autoregressive prior model to generate natural samples  without a reference speech. These prosody control methods  enable us to learn the prosody from speech and fine-grained  the synthesized speech, but they still cannot effectively  predict the correct prosody according to the input text.   One reason is that the prosody information of speech  generally depends on the meaning of text, while only the  phoneme information of text is used as the input in current  mainstream TTS systems, which limits the capabilities of  modeling the prosody of speech.  In ,  the textual knowledge from BERT  is introduced  into TTS systems to improve the prosody of speech,  but they ignore the variability of prosody.  For example, the same text may produce speech with  different prosody due to pronunciation uncertainty.    % 4. 閹崵绮ㄩ弬鍥ㄦ拱閻ㄥ嫬鍨遍弬鎵仯  In this works, we propose a novel self-attention mechanism,  named as local attention, to model the timing dependencies,  which abandons positional encoding and uses a relative  position matrix to model the influence of the positional  relationship of input sequence. At the same time,  we introduce the prosody learning mechanism for feed-forward  TTS systems, where a prosody embedding for each phoneme is  learned from the mel-spectrum in training. In addition,  a prosody predictor is designed to predict the prosody  embedding according to text and phoneme, where a pre-trained  language model is applied to introduce the meaning of text.  And the main contributions of our works as follows:              Based on the local attention, a feed-forward text-to-speech  system without the limitation of input text length is designed.  Meanwhile, the prosody learning mechanism is proposed to model  the prosody of speech, where the prosody information is learned  from speech by a prosody learner in training process.  In order to predict more satisfactory prosody in inference,  the pre-trained language model is used to introduce the semantic  feature. Experiments on English synthesis and Mandarin synthesis  show that a significant improvement in the prosody of speech has  been obtained in our proposed TTS systems.  
","     Recent neural speech synthesis systems have gradually    focused on the control of prosody to improve the quality    of synthesized speech, but they rarely consider the    variability of prosody and the correlation between prosody    and semantics together. In this paper, a prosody learning    mechanism is proposed to model the prosody of speech based    on TTS system, where the prosody information of speech is    extracted from the mel-spectrum by a prosody learner and    combined with the phoneme sequence to reconstruct the    mel-spectrum. Meanwhile, the sematic features of text from    the pre-trained language model is introduced to improve the    prosody prediction results. In addition, a novel self-attention    structure, named as local attention, is proposed to lift    this restriction of input text length, where the relative    position information of the sequence is modeled by the    relative position matrices so that the position encodings    is no longer needed. Experiments on English and Mandarin show    that speech with more satisfactory prosody has obtained    in our model. Especially in Mandarin synthesis,    our proposed model outperforms baseline model with a MOS gap    of 0.08, and the overall naturalness of the synthesized    speech has been significantly improved.",0
"  Conventional SLU pipeline mainly consists of two components : an Automatic Speech Recognition module generates transcriptions or N-hypotheses, and a Natural Language Understanding  module classifies transcriptions into intents, in which speech recognition error propagation will be amplified during sub-sequence NLU process. Although with the rapid development of end-to-end speech recognition systems, the performance of SLU has been significant improved , it still can not satisfy the application requirements, due to the complexity of scenarios.  %The improved performance of SLU mainly benefits from the increasing maturity of ASR. The application of deep neural networks in acoustic models and language models together with the rapid development of end-to-end technique make ASR systems extend to other research domains .    Usually not all errors from speech recognition harm the SLU module, and those errors have no impact on the eventual performance . The SLU component only keeps its attention on keywords while discarding most of the other irrelevant words . Thus the joint optimization approach can strengthen the focus of the model on improving the transcription accuracy that relates to target events . Recently, many efforts have been dedicated on end-to-end SLU in which the domain and the intent are predicted directly from input audio . Previous researches have shown that a large amount of data is the determining factor for the excellent performance of a model . However, due to the lack of audio and the ambiguity of intents, it is difficult to obtain sufficient in-domain labeled data. Transfer learning methodology has become a common strategy to address insufficient of data problem . %which is a vital technique that can generalizes models trained for one setting or task to other settings or tasks. Different transfer learning strategies have been applied in SLU model and all of them result in competitive complementary results . In this paper, this strategy is also applied to amplify the feature extraction capability of the encoder component, it pre-train the encoder with a large amount of speech recognition labeled data, and then transfer the encoder to the SLU model.   Recently,  proposed and compared various of encoder-decoder approaches to optimize each module of SLU in end-to-end manners and have proved that intermediate text representation is crucial for SLU and jointly training the full model is advantageous. Attention-based models have been widely used in speech recognition and provide impressive performance . Inspired by this, we propose a Transformer based multi-task strategy to adopt textual information in the SLU model. Since text information only acts on the decoder component in speech recognition task, it can be treated as an adaptive regularizer to adjust the encoder parameters such that contributing to improve intent prediction performance.  It should be noticed that the lack of textual corpus is also a major challenge when training language models. To address this problem, various of methods have been carried out to expand corpus in the past decade . In addition, textual level transfer learning strategy by merging a pre-trained representation to the decoder is also explored. The pre-trained representation is obtained with the BERT model, which is designed to pre-train the deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .   Encoder and decoder are mutual independent but are connected by the attention block, through which can get a collaborated optimization in training.  To maximize the performance, both encoder and decoder are optimized with transfer leaning strategies. In this paper, we first propose a self-attention based end-to-end SLU structure, and applied cross-lingual transfer learning method to solve insufficient acoustic data problem. Then we propose a Transformer based multi-task strategy that conducts intent classification and speech recognition in parallel. Finally, a textual-level transfer learning structure is designed to aggregate the pre-trained BERT model into the decoder component to improves the feature extraction capability of the decoder, indirectly.      In this paper, we propose an attention-based end-to-end SLU model and evaluated different augmentation strategies based on this model.   The results on test set grabbed from the Fluent Speech Commands corpus, veri閾夸躬d the effectiveness of proposed approaches. We show that cross-lingual encoder pre-training, multi-task strategy, and BERT-fusion have abilities of improving the intent classification performance. These enhancement strategies can also extend to other areas such that improve their performance.   In addition, the successful application of end-to-end SLU model will improve the performance of more natural language processing task like Voice assistant and facilitate people閳ユ獨 life.   Due to the limited availability of speech data labeled with Speaker-Intent,  Due to the limitation of data, the model is prone to over-fitting and sensitive to model parameters. More investigation on how to efficiently solve data sparsity in model training will be conducted in future.   The enhancement strategies describedin this article can be extended to other areas and provide contributions  performance of future structures.  
","   End-to-end Spoken Language Understanding  models are made increasingly large and complex to achieve the state-of-the-art accuracy. However, the increased complexity of a model can also introduce high risk of over-fitting, which is a major challenge in SLU tasks due to the limitation of available data. In this paper, we propose an attention-based SLU model together with three encoder enhancement strategies to overcome data sparsity challenge. The first strategy focuses on the transfer-learning approach to improve feature extraction capability of the encoder. It is implemented by pre-training the encoder component with a quantity of Automatic Speech Recognition annotated data relying on the standard Transformer architecture and then fine-tuning the SLU model with a small amount of target labelled data. The second strategy adopts multi-task learning strategy, the SLU model integrates the speech recognition model by sharing the same underlying encoder, such that improving robustness and generalization ability. The third strategy, learning from Component Fusion  idea, involves a Bidirectional Encoder Representation from Transformer  model and aims to boost the capability of the decoder with an auxiliary network. It hence reduces the risk of over-fitting and augments the ability of the underlying encoder, indirectly. Experiments on the FluentAI dataset show that cross-language transfer learning and multi-task strategies have been improved by up to $4.52\%$ and $3.89\%$ respectively, compared to the baseline.",1
" Most speech synthesis models take two-stage procedures to generate waveform audio from the text. First stage generates spectrogram conditioned on linguistic features such as text or phoneme. In second stage, generally refer to as vocoder stage, audio samples are generated through model capable of estimating audio samples from the acoustic features. Traditional approaches estimated audio samples either directly from the spectral density model or hand-crafted acoustic model, but these approaches tended to produce low-quality audio.  After the emergence of the WaveNet, models that generate audio samples on previously generated samples had shown exceptional works in the field.. Nevertheless, dilated causal convolution networks used in the model require sequential generation process during the inference, which infers that real-time speech synthesis is hard to achieve because parallel inference can't be utilized. For this reason, generating high-quality waveform audio in real-time has become a challenging task.  To overcome the structural limitation of the auto-regressive model, most of the recent works are focused on non-autoregressive models such as knowledge distillation, generative adversarial network, and flow-based generative model. We focus on the flow-based generative model since it can model highly flexible approximate posterior distribution in variational inference. The transformation from a single data-point to a Gaussian noise is one-to-one, which makes the parallel generation possible. However, we have to acknowledge that audio samples are discrete data. In other words, naive modeling of a continuous probability density on discrete data can produce arbitrary high likelihood on discrete location. This can lead to degraded generation performance in flow-based neural vocoder. Therefore, dequantization is required before the transformation.  In this paper, we present various audio dequantization schemes that can be implemented in the flow-based neural vocoder. In image generation, adding continuous noise to data-points to dequantize the data is commonly used. However, to the best of our knowledge, the effectiveness of data dequantization in audio domain is still an unknown area, so further investigation is needed. Unlike pixels of the image, audio samples are bounded to signed integer. To overcome this domain issue, we either normalize range of noise values or range of audio samples with different normalization method. In addition, we adapt flow block from flow-based neural vocoder to generate more flexible noises known as variational dequantization.      In this paper, we proposed various audio dequantization schemes that can be implemented in flow-based neural vocoder. For the uniform dequantization, we compressed the range of audio domain to match with conventional uniform dequantization method by using mu-law companding compression. In addition, we implemented iw dequantization to resolve the noise issue that occurs from the lossy compression. For the Gaussian dequantization, we applied hyperbolic tangent normalization on data-oriented Gaussian noise to properly fit the data within the audio range. Lastly, we modified flow block in flow-based neural vocoder to construct variational dequantization model to apply more flexible noise. From the experiments, we demonstrate that implementing audio dequantization can supplement the flow-based neural vocoder to produce better audio quality with fewer artifacts.  \clearpage   
","     In recent works, a flow-based neural vocoder has shown significant improvement in real-time speech generation task. The sequence of invertible flow operations allows the model to convert samples from simple distribution to audio samples. However, training a continuous density model on discrete audio data can degrade model performance due to the topological difference between latent and actual distribution. To resolve this problem, we propose audio dequantization methods in flow-based neural vocoder for high fidelity audio generation. Data dequantization is a well-known method in image generation but has not yet been studied in the audio domain. For this reason, we implement various audio dequantization methods in flow-based neural vocoder and investigate the effect on the generated audio. We conduct various objective performance assessments and subjective evaluation to show that audio dequantization can improve audio generation quality. From our experiments, using audio dequantization produces waveform audio with better harmonic structure and fewer digital artifacts.",2
"  Unsupervised machine learning has led to a marriage of symbolic learning and vectorized representation learning . In the computer music community, the MusicVAE  enables the interpolation in the learned latent space to render some smooth music transition. The EC-VAE  manages to disentangle certain interpretable factors in music and also provides a manipulable generation pathway based on these factors. Pati et al.  further utilizes the recurrent networks to learned music representations for longer-term coherence.   % With advances in machine learning, the idea of combining symbolic music generation with representation learning has become popular  % . As one of the most successful models,  variational autoencoders  learn a compact  latent representation of music, which has lots of applications in music generation. For example, MusicVAE  introduces latent space interpolation to make smooth music transitions; ECVAE  disentangles the latent space into interpretable factors and manipulates them to generate new pieces; and Lerch et al.  use the representation of a music segment as a token to generate longer-term music using recurrent networks.   Unfortunately, most of the success has been limited to monophonic music.  The generalization of the learning frameworks to polyphonic music is not trivial, due to its much higher dimensionality and more complicated musical syntax. % richer underlying factorization.  The commonly-adopted MIDI-like event sequence modeling or the piano-roll formats fed to either recurrent or convolutional networks have fell short in learning good representation, which usually leads to unsatisfied generation results . In this paper, we hope to pioneer the development of this challenging task. To begin with, we conjecture a proper set of inductive bias for the desired framework: -a sparse encoding of music as the model input; -a neural architecture that incorporates the hierarchical structure of polyphonic music .  % However, most of the aforementioned progress is achieved on VAEs for monophonic music. As we will demonstrate, the success cannot be easily generalized to polyphonic music using commonly-used MIDI-like event sequence or piano-roll formats with standard recurrent or convolutional neural encoders/decoders.  The main reason is that, compared with monophony, polyphonic data is higher dimensional with a more complex and structured distribution. To tackle this challenge, we need a proper inductive bias, specifically: %  %       % 閺傛澘鍟撻惃鍕唽閽鏂ょ窗瀵啰鏁 % Guided by such design principles, we propose PianoTree VAE, a VAE structure that learns the latent representation of polyphonic music in a hierarchical manner. For data representation, we adopt a tree-structured hierarchical music syntax. In a top-down order: a  contains a series of  events, a  consists of multiple  events, and each  has several attributes. In this paper, we focus on a simple yet common form of polyphonic music---piano score with only pitch and duration attributes. Note that this tree structure can be generalized to multiple instruments and expressive performance by adding extra attributes such as voice, expressive timing, dynamics, etc.  % We expect this syntax provides a sufficient inductive bias to learn a semantically-meaningful latent representation, [while still compatible with the current VAE architectures.]  Guided by the aforementioned design principles, we propose PianoTree VAE, a hierarchical representation learning model under the VAE framework. We adopt a tree structured musical syntax that reflects the hierarchy of musical concepts, which is shown in \figref{fig:example}. In a top-down order: we define a   as a series of  events , a  as multiple  events sharing the same onset , and each  has several attributes such as pitch and duration. In this paper, we focus on a simple yet common form of polyphonic music---piano score, in which each note has only pitch and duration attributes. For future work, this syntax can be generalized to multiple instruments and expressive performance by adding extra attributes such as voice, expressive timing, dynamics, etc.  %     The whole neural architecture of PianoTree VAE can be seen as a tree. Each node represents the embedding of either a , , or , where a higher level representation has larger receptive fields. The edges are bidirectional where a recurrent module is applied to either encode the children into the parent or decode the parent to generate its children.  % As for the model structure, both the encoder and the decoder of PianoTree VAE are hierarchical recurrent networks, and this hierarchy has a one-to-one correspondence with the proposed tree structured polyphonic syntax. At each level, a recurrent network either encodes the children into their parent or decodes the parent to generate its children. % We believe that this architecture provides a reasonable inductive bias because the encoding/decoding procedures are analogous to how musicians memorize/interpret a score. For example, we usually ``roll a chord'', and similarly, the model uses recurrent networks to expand a  into 's.  Through  extensive  evaluations, we show that PianoTree VAE yields semantically more meaningful latent representations and further downstream generation quality gains, on top of the current state-of-the-art solutions. % 娴犮儰绗呯粭顑跨閸欍儲妲搁弬鏉垮晸閻ㄥ嫸绱濈粭顑跨癌閸欍儲妲搁崢鐔告降閻ㄥ嫨鍌涘灉鐟欏绶卞▽鈥虫殣閻㈩煉绱濋崚鐘辩啊閵 %The mechanism of recurrent modules is analogous to the procedure such as  ``roll a chord'' in pitch ascending order.      % Reviewer閿涙瓬etter to specify the representation being compared. % Finally, we compare our PianoTree VAE with baseline VAEs using data representation of either piano-roll or MIDI-like event sequence with corresponding model structure and show that the learned latent representation yields more accurate reconstruction, smoother and more musical latent space interpolation, and better downstream music generation when combined with standard sequence generative models.        In conclusion, we proposed PianoTree VAE, a novel representation-learning model tailored for polyphonic music. The key design of the model is to incorporate both the music data structure and model architecture with the sparsity and hierarchical priors. Experiments show that with such inductive biases, PianoTree VAE achieves better reconstruction, interpolation, downstream generation, and strong model interpretability.   Finally, both the data structure and learning algorithm shares a hierarchy to prioritize the importance of metric structure and pitch order.  In the future, we plan to extend PianoTree VAE for more general musical structures, such as motif development and multi-part polyphony.   First, the data structure reveals one possible syntactic construct of polyphonic music. In addition, PianoTree VAE leads to a structured representation learning algorithm using the VAE framework.        Swap Table Figure here         End of Figure                          For bibtex users:  
"," The dominant approach for music representation learning involves the deep unsupervised model family  . However, most, if not all, viable attempts on this problem have largely been limited to monophonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the PianoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning.  % It consists of multiple layers of encoding and decoding networks, which learn the representation of a tree-structure polyphonic segment in a hierarchical manner.  The experiments prove the validity of the PianoTree VAE via -semantically meaningful latent code for polyphonic segments; -more satisfiable reconstruction aside of decent geometry learned in the latent space; -this model闁炽儲鐛 benefits to the variety of the downstream music generation.\!\!\footnote{Code and demos can be accessed via \url{https://github.com/ZZWaang/PianoTree-VAE}}   % Music representation learning by variational autoencoders  has proven to be a promising direction towards better music generation. However, most successful studies  focus on monophonic music and it is still difficult to generalize the learning methods to polyphonic pieces. This is mainly because polyphonic data has higher dimensionality and contains more complex structures, which is difficult to be captured by the existing VAE architectures for sequential data.  In this paper, we contribute PianoTree VAE, a VAE that considers polyphonic music a tree-structure data and learns the representation in a hierarchical manner. The model effectively learns a semantically meaningful latent code of a polyphonic music segment. Experiments show that the latent code yields better reconstruction and latent space interpolation. The learned latent embedding also leads to better downstream music generation when combined with standard sequence generative models.",3
"  Over the past few years, developments in sequence-to-sequence  neural text-to-speech  research have led to synthetic speech that sounds almost indistinguishable from human speech . However, large amounts of high-quality recordings are typically required from a professional voice talent to train models of such quality, which can make them prohibitively expensive to produce. To counter this issue, investigations into how S2S models can facilitate multi-speaker data has become a popular topic of research. %\EJ{I don't like starting a sentence with a citation if the citation is a number in brackets} \MK{Agreed} A study by, for example, showed that multi-speaker models can perform as well or even better than single-speaker models when large amounts of target speaker data are not available, and that single-speaker models only perform better when substantial amounts of data are used. Their research also showed that the amount of data necessary for an additional speaker can be as little as 1250 or 2500 sentences without significantly reducing naturalness. With regards to parametric synthesis, investigated the effect of several multi-speaker modeling strategies for class imbalanced data. Their research found that for limited amounts of speech, multi-speaker modeling and oversampling could improve speech naturalness compared to single speaker models, while undersampling was found to generally have a harmful effect. They also showed that ensemble methods can further improve naturalness, but this strategy comes with a considerable computational cost that is usually not feasible for S2S modeling.  Although the above research shows that multi-speaker modeling can be an effective strategy to reduce data requirements, it is not a suitable solution for many languages for which large quantities of high-quality multi-speaker data are not available. Multilingual multi-speaker synthesis aims to address this issue by training a multilingual model on the data of multiple languages. Among the first to propose a neural approach to multilingual modeling was. Instead of modeling languages separately, they modeled language variation through cluster adaptive training, where a mean tower as well as language basis towers were trained. They found that multilingual modeling did not harm naturalness for high-resource languages, while low-resource languages benefited from multilingual modeling. Another study by scaled up the number of unseen low-resource languages to twelve, and similarly found that multilingual models tend to outperform single speaker models.  More recently, multilingual modeling was also adopted in S2S architectures, however mostly for the purposes of code-mixing and cross-lingual synthesis. Language information was typically represented either with a language embedding or with a separate encoder for each language, while applied both approaches to code-mixing and accent conversion. With regards to multilingual modeling, showed that multilingual models can attain a naturalness and speaker similarity that is comparable to that of a single speaker model for high-resource target languages, while research from obtained promising results with a crosslingual transfer learning approach.  While research into S2S multilingual modeling is clearly vibrant, there appears to exist little systematic research into how S2S multilingual models could be used to increase speech naturalness for low-resource languages. To fill this void, this paper investigated to what extent results that are found in S2S monolingual multi-speaker modeling are transferable to multilingual multi-speaker modeling, and if it is possible to attain higher naturalness on low-resource languages with multilingual models than with single speaker models. Because multilingual modeling can benefit from the inclusion of large amounts of non-target language data, we also experimented with several data addition strategies and evaluated to what extent these strategies are effective to improve naturalness for low-resource languages. As this research is primarily addressing the viability of different approaches with regards to low-resource languages, our focus is not so much on maximizing naturalness but rather on gaining a better understanding of how different strategies work and would potentially scale up using larger amounts of data.  The rest of this paper is organized as follows. In Section, we  describe the architecture used to conduct our experiments. In Section, we describe the experimental design and give details about training and evaluation. In Section, we provide the experimental results. Finally, in Section, we discuss conclusions and directions for future research.      This paper aimed to investigate the effectiveness of multilingual modeling to improve speech naturalness of low-resource language neural speech synthesis.  speakers. Our results showed  have shown that the addition of auxiliary non-target language data can positively impact the naturalness of low-resource language speech and can be a viable alternative to auxiliary target language data when such data is not readily available. We furthermore found that when more target language data was available, the inclusion of the auxiliary non-target language data did not negatively affect naturalness. Although  in this research we did not compare multilingual models with single speaker models for even larger amounts of target language data in this research, we expect that results from multilingual modeling will largely mimic the effects observed in studies of monolingual multi-speaker modeling. Finally, we explored several strategies for including additional non-target language data. We showed that not all data addition strategies are equally effective, and reported that language diversity and minimizing class imbalances appear to be the most important variables to consider when adding data.  Based on our conclusions, we identify several directions for future research. First of all, the current research didn't consider the issue of language proximity on the effect of multilingual modeling. Although languages are modeled separately in the encoders, language proximity may positively affect naturalness. Additionally, this research evaluated low-resource language speech naturalness at a general level, while it may be more interesting to focus on the naturalness of language-specific characteristics such as language-specific phonemes or stress patterns. We furthermore note that the amount of auxiliary data used was relatively limited in our experiments. Further analysis could be done to find out whether our findings hold when scaled up with more data. Finally, we found that the MULT-2k+16x2k model was most effective to improve naturalness of target language speech, but this result does not clarify whether this effect can be attributed to the large variation in languages and speakers, or to the minimization of class imbalances. It would be interesting to disentangle these variables by comparing this model to a monolingual multi-speaker model with similar amounts of data per speaker.   
"," Recent advances in neural TTS have led to models that can produce high-quality synthetic speech. However, these models typically require large amounts of training data, which can make it costly to produce a new voice with the desired quality. Although multi-speaker modeling can reduce the data requirements necessary for a new voice, this approach is usually not viable for many low-resource languages for which abundant multi-speaker data is not available. In this paper, we therefore investigated to what extent multilingual multi-speaker modeling can be an alternative to monolingual multi-speaker modeling, and explored how data from foreign languages may best be combined with low-resource language data. We found that multilingual modeling can increase the naturalness of low-resource language speech, showed that multilingual models can produce speech with a naturalness comparable to monolingual multi-speaker models, and saw that the target language naturalness was affected by the strategy used to add foreign language data.",4
" % \dcrm{In a standard Question Answering system, a user enters a natural language question,e.g., Who founded Tesla?}. Knowledge Graph based Question Answering  systems use a background Knowledge Graph to answer queries posed by a user. Let us take the following question as an example :  Who founded Tesla?. The standard sequence of steps for a traditional Entity Linking system is as follows: The system tries to identify Tesla as a span of interest. This task is called Mention Detection  or Span Detection. Then an attempt is made to link it to the appropriate entity in the Knowledge Base.  In this work we focus on Knowledge Bases in the form of graphs, hence the entity linker in this case tries to link Tesla to the appropriate node in the graph.  For a human, it is evident that the question is looking for a person's name who created an organisation named Tesla, since the text contains the relation .  Hence, it is important that the entity linker understands the same nuance and ignores other entity nodes in the Knowledge Graph which also contain Tesla in their labels, e.g.,  when considering the example of the Wikidata knowledge graph.  The task of ignoring the wrong candidate nodes, and identifying the right candidate node instead, is called Entity Disambiguation . The cumulative process involving Mention Detection and Entity Disambiguation is called Entity Linking .     Typically, the MD and ED stages are implemented by different machine learning models which require separate training. Especially for the MD part, sentences with marked entity spans are a requirement. In practice, such data is not easily available. Moreover, errors introduced by the MD phase cascade on to the ED phase. Hence, a movement towards end-to-end Entity Linkers began  . Such systems do not require labelled entity spans during training. In spite of the benefits of end-to-end models some challenges remain: Due to the lack of a span detector at the initial phase, each word of the sentence needs to be considered as an entity candidate for the disambiguation which leads to the generation of a much larger number of entity candidates. To re-rank these candidates a large amount of time is consumed, not just in processing the features of the candidates, but also in compiling their features. %Some systems fetch neighbouring entities and relations on the fly  for each candidate entity, a step that can take more than a minute for certain entities on large KGs.   In this work, we remain cognizant of these challenges and design a system that completely avoids querying the Knowledge Graph during runtime. PNEL  instead relies on pre-computed and pre-indexed TransE embeddings and pre-indexed entity label and description text as the only set of features for a given candidate entity. We demonstrate that this produces competitive performance while maintaining lower response times when compared to VCG .  While there is a wide variety of KG embeddings to choose from, we confine our experiments to pre-computed TransE over Wikidata supplied by PyTorch-BigGraph. Our choice was based on the popularity and ease of availability of these embeddings.  Traditionally, the Knowledge Graphs of choice for Question Answering research have been DBpedia, Freebase  and YAGO. However, in recent times Wikidata has received significant attention owing to the fact that it covers a large number of entities . DBpedia, YAGO and Wikidata source their information from Wikipedia, however DBpedia and YAGO filter out a large percentage of the original entities, while Wikidata does not. While Wikidata has a larger number of entities it also adds to noise which is a challenge to any EL system. Wikidata also allows direct edits leading to up-to-date information, while DBpedia depends on edits performed on Wikipedia. Freebase has been discontinued and a portion of it is merged into Wikidata. Moreover DBpedia now extracts data directly from Wikidata, apart from Wikipedia \footnote{https://databus.dbpedia.org/dbpedia/wikidata}  . %Wikidata allows wiki based edits and is hence up-to-date.  %Both DBpedia and Freebase have decided to merge with Wikidata in some form.  Hence, we decide to base this work on the Wikidata knowledge graph and the datasets we evaluate on are all based on Wikidata.\\   In this work our contributions are as follows:                The paper is organised into the following sections:  Related Work, outlining some of the major contributions in entity linking used in question answering;  PNEL, where we discuss the pointer networks and the architecture of PNEL Dataset used in the paper  Evaluation, with various evaluation criteria, results and ablation test   Error Analysis  Discussion and future direction.     In this work we have proposed PNEL, an end-to-end Entity Linking system based on the Pointer Network model. We make no modifications to the original Pointer Network model, but identify its utility for the problem statement of EL, and successfully model the problem so the Pointer Network is able to find the right set of entities.   We evaluate our approach on three datasets of varying complexity and report state of the art results on two of them. On the third dataset, WebQSP, we perform best in precision but lag behind in recall. We select such features that require no real time KG queries during inference. This demonstrates that the Pointer Network model, and the choice of features presented in this work, result in a practical and deployable EL solution for the largest Knowledge Graph publicly available - Wikidata.   \\  \dc{Our main design goal for the system is speed and deploybility; hence, we refrain from querying the underlying Knowledge Graph during inference. Instead, we solely rely on pre-trained TransE KG Embeddings, which potentially encodes the structural information of a Knowledge Graph. Additionally, we also incorporate entity label and description information into PNEL which further benefits the model. As evident from the results, PNEL exhibits state-of-the-art performance on both LC-QuAD 2.0, and SimpleQuestions datasets and has the best precision and comparable F1-scores for WebQSP as well. Hence, it can be concluded that our proposed feature sets can encode the KG information implicitly while achieving comparable or better performance than systems which explicitly relies on KG structural information such as 1, and 2-hop relation information. }   The design goal for the models being ""no KG query during inference"", we rely on pre-computed TransE embeddings to incorporate KG structural information. Additionally, we use entity labels and descriptions which are pre-indexed in a text database. It must be noted that we do not consider neighbourhood relation information explicitly. Since our system achieves state-of-the-art numbers on some datasets, and performs competitively overall, it can be said that our limited choice of pre-indexed features performs at par with a large variety of systems that also consider 1-hop and 2-hop relation information explicitly. \\  For future work: PNEL being based on the LSTM cell inevitably processes tokens sequentially increasing the response times. This limitation could be overcome by using some variant of the Transformer model instead, which is not only a powerful model but also able to process tokens in parallel.  As a future work we would also like to explore different entity embedding techniques and investigate which characteristics make an embedding suitable for the entity linking task.   
"," Question Answering systems are generally modelled as a pipeline consisting of a sequence of steps. In such a pipeline, Entity Linking  is often the first step. Several EL models first perform span detection and then entity disambiguation. In such models errors from the span detection phase cascade to later steps and result in a drop of overall accuracy. Moreover, lack of gold entity spans in training data is a limiting factor for span detector training. Hence the movement towards end-to-end EL models began where no separate span detection step is involved. In this work we present a novel approach to end-to-end EL by applying the popular Pointer Network model, which achieves competitive performance. We demonstrate this in our evaluation over three datasets on the Wikidata Knowledge Graph.",5
"   Slot filling is one of the major but challenging tasks in spoken language understanding because it aims to automatically extract semantic concepts by assigning a set of task-related slots to each word in a sentence.  was the first reported work that applied recurrent neural network  to the slot filling task and encouraged the follow-up deep learning work for the task. The next works focused on deep learning:  tried to replace the vanilla RNNs with more advanced RNN cells based on long short-term memory   or bi-directional LSTM ,  focused on recursive neural networks, and  utilizes an attention-based RNN.   In this study, we firstly generalize the variational inference -based dropout regularization in the LSTM-RNNs to more advanced RNN architectures such as gated recurrent unit   and bi-directional LSTM/GRU. Then, the RNN models with the VI-based dropout regularization are employed in the slot filling task on the ATIS database. Compared with , this work presents a slight modification of the LSTM-RNNs that can lead to better baseline result, and more RNN architectures with and without VI-based dropout regularization are tested in our experiments. As opposed to , our methods are much easier to implement than the attention-based RNN, but similar results can be obtained in practice.   Since it has been shown that RNNs overfit very quickly , various regularization methods, such as early stopping or small and under-specified models , have to be used during the RNN training stage. Although dropout is normally taken as a simple and effective regularization to overcome the problem of overfitting in deep neural networks , it has been concluded that the naive dropout regularization to recurrent weights in RNNs cannot reliably solve the RNN overfitting problem because noise added in the recurrent connections leads to model instabilities .   However, a recent work  has shown that dropout regularization is a variational approximation technique in Bayesian learning. In addition, the variational inference provides a new variant of dropout regularization, where the same dropout masks are separately shared along time for embedding, decoding, and recurrent weights, so that they can be successfully applied to recurrent layers in RNNs.   The remainder of the paper is organized as follows: Section  presents the VI-based dropout regularization in RNNs. Section  develops the GRU and bi-directional LSTM/GRU-based RNNs with the VI-based dropout regularization. Section  shows the experimental results on the ATIS database and the paper is concluded in Section .     This work has proposed variational inference-based dropout regularization for RNNs with LSTM, GRU, and bi-directional LSTM/GRU cells. Contrary to the naive dropout regularization for embedding and decoding layers, the VI-based dropout regularization is applied to all RNN layers including recurrent layers by sharing the same dropout masks in the RNN layers. The experiments on the slot filling task on ATIS database showed that the variational RNN models obtain better results than the naive dropout regularization-based RNN models. In particular, the variational bi-directional LSTM/GRU obtains the best results in terms of F-measure.   \vfill\pagebreak       References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   -------------------------------------------------------------------------    
"," This paper proposes to generalize the variational recurrent neural network  with variational inference -based dropout regularization employed for the long short-term memory  cells to more advanced RNN architectures like gated recurrent unit  and bi-directional LSTM/GRU. The new variational RNNs are employed for slot filling, which is an intriguing but challenging task in spoken language understanding. The experiments on the ATIS dataset suggest that the variational RNNs with the VI-based dropout regularization can significantly improve the naive dropout regularization RNNs-based baseline systems in terms of F-measure. Particularly, the variational RNN with bi-directional LSTM/GRU obtains the best F-measure score.",6
" The percolation of social media throughout the world has facilitated unprecedented ease of access to the flow of information. The rise of the internet and its availability have also enabled every user to to not only consume, but also contribute to the information flow. However, the benefits of such ecosystems come at the cost of mistrust in the veracity of information. In recent years, the social media scene has witnessed the proliferation of false information campaigns, in which ordinary users are intentionally or otherwise both consuming false news and also spreading it among their communities.   This phenomenon is commonly referred to as , broadly defined as broadcasting of information that is intentionally and verifiably false . The rise of fake news and its societal impact has been studied in the context of numerous recent events, such as the Brexit referendum and the 2016 US presidential elections . Fake news has thus proven to be a major threat to democracy, journalism, and freedom of expression . The exposure of users to fake news has been shown to have numerous deleterious effects, instances of which include inducing attitudes of inefficacy, alienation, trusting in false propaganda, cynicism toward certain political candidates and communities, that can at times give rise to the violent events. For example, coordinated fake news and propaganda campaigns on Facebook are considered to have been key in inciting the Myanmar genocide in 2016-2017 . Also, the recent proliferation of false information about 5G communication networks being the cause of the novel Coronavirus outbreak has resulted in attacks against the employees and infrastructure of cellular careers in the UK . Fake news can also affect financial markets, as observed in the case of fake news claiming that Barack Obama was injured in an explosion resulting in a loss of \$130 billion in stock value . Hence, there is a growing need for effective tools and techniques to detect and control the spread of false information campaigns on social media.   Fake news classification is the process of determining whether the news contains false news and misinformation or not. Traditionally, this classification is performed by subject-matter experts and journalists via comparing the claims of an article with established facts and cross-checking with trusted and alternative sources. However, the high volume and velocity of information flow on such platforms render such manual approaches infeasible. Therefore, recent efforts of the stakeholders and the research community have been focused on automated techniques for classification and detection of fake news. A promising solution in this domain is to leverage the recent advances in machine learning and Natural Language Processing  to automated the processing and classification of the high-dimensional and complex text of news articles and posts . %We purpose a model where news article is classified by dividing the overall tasks into three parts: Style-Based Classification, Knowledge-Based Classification, and Propagation and Credibility-Based Classification. This paper is a focus on Style-based classification.  % %Machine learning  proven to useful in detecting fake news. The n-gram, part of speech tagging and probabilistic context free grammar were widely used in linguistic analysis before neural networks. Mihalcea and Strapparava  used n-gram approach for lie detection by  training Naive Bayes and Support Vector Machine  classifiers. They used crowd sourcing for creating their own datasets on three different topics, opinion on abortions, opinion on death penalty and feelings about best friend. They applied minimal pre-processing on the datasets with tokenization and stemming but without performing feature selection and stop words removal. They received the average accuracy of 70.8\% in NB and 70.1\% in SVM, %Ott et al.  trained a SVM classifiers using relative POS tag frequencies of texts as features. They found a probable relationship between deceptive spam and imaginative writing based on POS distributional similarities. %Feng et al.  investigated the syntactic stylometry for deception detection. They found that the features driven from Context Free Grammar parse trees improved the deception detection over Ott et al.    While the literature on the applications of machine learning to fake news classification has grown rapidly, the body of work on the classification of short-text claims remains relatively thin. This issue is of paramount importance, as many of the posts on social media such as Twitter contain only a short claim extracted from the longer text of news articles. The short form of such claims poses a challenge to the classification task, as it provides very limited information  and thus constrains the applicability of machine learning models trained on full-length articles and texts. Over the past few years, a number of datasets and models have been proposed for the classification of short-text claims, notable instances of which are the studies based on the LIAR dataset of short statements . However, the performance of machine learning models trained on this dataset remain at impractical levels, with the best accuracy values reported to be \~41.5\% . %  reported study  The problem with non neural network approach is that the news articles are longer in length and when using non neural network approach the semantic and syntactic features of the sentences cannot be extracted and exploited properly to full extent with non neural network approaches. The solution to this is neural network methods.  %Rashkin et al.  trained an LSTM model that takes sequence of words as the input and predicts the Politifact rating, and found it to be more accurate than NBC and Maximum Entropy models. They also concatenated LSTM output with Linguistic Inquiry and Word Count  features before undergoing the activation layers. The NBC and Maximum Entropy models are improved with LIWC but LSTM did not perform well. The reason might be that the LSTM can learn the in formations in LIWC by themselves. Wang  used deep learning based CNN model with LIAR dataset and found better results than the non-neural network methods.   %Qian et al.  proposed two models, the first one is Two-Level Convolutional Neural Network  a variant of CNN and second one is User Response Generator . The TCNN captured the semantic information from articles' text representing it at the sentence and word level. And URG learns a generative responses to news article text from historical user responses that assist in classification.  In this paper, we introduce Sentimental LIAR, which extends the LIAR dataset by including new features based on the sentiment and emotion analysis of claims. Our extended dataset also proposes a modified encoding of textual attributes to mitigate unintended bias in modeling. Furthermore, we propose a novel deep learning architecture based on the BERT-Base language model for the classification of claims as genuine or fake. Our results demonstrate that the proposed architecture trained on Sentimental LIAR can achieve an accuracy of 70\%, which is an improvement of ~30\% over previously reported results for the LIAR benchmark. The Sentimental LIAR dataset and the proof-of-concept code are made available on GitHub.  %In this paper, we present the series of experiments we performed using BERT-Base and the extended LIAR datasets and compare the results. The base BERT-Base model is modified by adding linear neural net on top and the other modification is done by adding CNN model on top. The modified models are tested with different version of LIAR datasets. We modified the LIAR dataset by extending it with sentiment score and sentiment of the statement. The other extension is done by adding the five emotions of the statement  The remainder of this paper is organized as follows: Section  presents the technical background and an overview of relevant datasets and literature on false claim classification. Section  describes the extended features of Sentimental LIAR, and details the proposed deep learning architectures for false claim detection. The experimental evaluation of our proposed techniques is reported in Section . Finally,  concludes the paper with a discussion on the results and remarks on future directions of work.       This paper introduced Sentimental LIAR as an extension of the LIAR dataset, and proposed novel model architectures based on BERT-Base for fake claim detection in short text. The proposed architectures extend BERT-Base by adding  a feedforward neural network, or  a CNN. The LIAR dataset is extended by adding emotions anger, sad, fear, anger and disgust by using IBM NLP API and added sentiment score using Google NLP API. We also included speaker credit as an input attribute to our models.   The experiments performed with BERT-Base + feedforward NN, the accuracy ranged from 68.8\  to 69\  within the five experiments. These experiments were performed by changing the input structure in the first three experiments and by changing the hidden layers in the latter two experiments. A slight improvement of 1\  was observed in the accuracy and no improvements in the F1 Score. This suggests that the model may need to be revised to handle the complexity of the input data. Hence, a CNN-based architecture was investigated in our further experiments.   The experiments were performed with BERT-Base + CNN, the accuracy ranged from 68.82\  to 70\   within six experiments, and also major improvements were observed in the F1 Score . The best performing model is found to be one where the text attribute is fed directly into BERT-Base, and the output of BERT-Base is concatenated with the emotions, speaker's credit and sentiments before being passed to the CNN. Undeutsch hypothesis  and the four-factor theory  supported the intuition that the emotional and sentimental attributes can help to distinguish the fake claims, which can be verified by the observation of the model performing better when EMO and SEN were added. Adding the SEN and EMO with BERT-Base output supplemented the features which boosted the CNN model performance.   For both models, it can be observed that adding the metadata  increased the accuracy of model. Also, both the model accuracy and the F1 Score improved with the CNN-based architecture.   The training loss VS validation Loss graphs for BERT-Base + feedforward NN is given in Fig., and for BERT-Base + CNN in Fig.. These plots suggest that the models were overfitted only after 2 epochs, which is mostly due to the small size of the dataset. Also, it must be noted that the dataset is imbalanced, with 65\  of data labeled as false and only 35\  labeled as true. These observations demonstrate the need for the curation of larger and more representative datasets of short-text claims.  Furthermore, our results further verify that fake claims can be detected in short-text according to exaggerated expressions and strong emotions demonstrated in the text. The proposed architecture also sets a new state-of-the-art benchmark for fake claim classification on the LIAR dataset with an accuracy of 70\ .     
"," The rampant integration of social media in our every day lives and culture has given rise to fast and easier access to the flow of information than ever in human history. However, the inherently unsupervised nature of social media platforms has also made it easier to spread false information and fake news. Furthermore, the high volume and velocity of information flow in such platforms make manual supervision and control of information propagation infeasible. This paper aims to address this issue by proposing a novel deep learning approach for automated detection of false short-text claims on social media. We first introduce Sentimental LIAR, which extends the LIAR dataset of short claims by adding features based on sentiment and emotion analysis of claims. Furthermore, we propose a novel deep learning architecture based on the BERT-Base language model for classification of claims as genuine or fake. Our results demonstrate that the proposed architecture trained on Sentimental LIAR can achieve an accuracy of 70\%, which is an improvement of ~30\% over previously reported results for the LIAR benchmark. %improve the previously reported accuracy of the task by     Focusing on the prevalent short-text format of claims on social media such as Twitter, our work   to an unprecedented challenge in  . Fake news is not only threatening to undermine democracy but equally has been proven to cause violence, disruption, and chaos in the world. Hence, in this research paper, we are going to use the machine learning approach to classify the fake news from the true ones. The rise of Natural Language Processing makes it possible to analyze the news articles. We are proposing a model composed of three perspectives. The first perspective is the Style Based Classification where we classify the article based on its intention is misleading or not, by analyzing the text pattern from the attribute-based and structure-based language features. The second perspective is Knowledge-based classification which is going to classify the news articles based on its authenticity by knowledge extraction and fact-checking. The third perspective is the Propagation and Credibility based classification by analyzing the propagation model of fake news and the credibility of the engaging users. This research paper currently focused on first perspective i.e. Style based classification by deception detection using deep neural networks where we performed experiments using LIAR Dataset by changing it into binary labels and BERT-Base.",7
"  The ever-growing amount of user-generated data on social media platforms be it Facebook, Twitter, blogs or any other electronic medium introduces new challenges in terms of automatic content moderation, especially regarding hate speech  and  offensive language detection. Not only is hate speech more likely to happen on the Internet,  where anonymity is easily obtained and speakers are psychologically distant from their audience, but its online nature also gives it a far-reaching and determinative impact. User content mostly consists of microposts, where the context of a post can be missing or inferred only from current events. Manual verification of each posting by a human moderator is infeasible due to the high amount of postings created every day. Consequently, automated detection of such attacking postings is the only feasible way to counter this kind of hostility. However, this task is challenging because natural language is fraught with ambiguities, and language in social media is extremely noisy. The classification system that would be prepared for the task, needed to be generalized for various test corpora as well. In this paper I have described the system consisting of a sequential pipeline with text feature extraction and classification as its main components. Firstly, a bag-of-words model is used for encoding the sentences into corresponding integer sequence. Thereafter, vectors are generated from these sequences and fed to a series of BiLSTM layers for training. Then a softmax layer is used for ternary classification into the corresponding offensive language categories.  The rest of the paper has been organized as follows. Section  describes the data, on which, the task was performed. The methodology followed is described in Section . This is followed by the results and concluding remarks in Section  and  respectively. % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  %.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. % }      In this paper, I have presented a model which performs satisfactorily in the given task. The model is based on a many2one sequence learning based architecture. There is scope for improvement by including more manually extracted features  to increase the performance. I could use only 25\  of the whole dataset due to lack of computational resources. Data required for a Deep Learning model is quite high. Using the whole database would surely give excellent results.  Removing the data constraints might lead to better accuracies and f1 metrics. Use of regularizers can also lead to proper generalization of model, henceforth increasing the metrics.      
"," SemEval-2020 Task 12 was OffenseEval: Multilingual Offensive Language Identification in Social Media . The task was subdivided into multiple languages and datasets were provided for each one. The task was further divided into three sub-tasks: offensive language identification, automatic categorization of offense types, and offense target identification. I have participated in the task-C, that is, offense target identification. For preparing the proposed system, I have made use of Deep Learning networks like LSTMs and frameworks like Keras which combine the bag of words model with automatically generated sequence based features and manually extracted features from the given dataset. My system on training on 25\% of the whole dataset achieves macro averaged f1 score of  47.763\%.",8
" The discourse structure of a document describes discourse relationships between its elements as a graph or a tree. Discourse parsing is largely dominated by greedy parsers~. Global parsing is rarer because the dependency between node's label and its internal split point can make prediction computationally prohibitive. % resulting in a large grammar constant. % This expense comes from the dependency relation % between the labels assigned to a node and the % split point that separates its children, which results in % a large constant for global inference in terms of time % complexity, making the inference process extremely slow.  In this work, we propose a CKY-based global parser with tractable inference using a new independence assumption that loosens the coupling between the identification of the best split point label prediction. % For a particular node, we first decide the split % point without considering the labels; and then based on the % split point, we make the decisions for the labels of % current node.   % However, when we apply recursion, the total score of this % node is the sum of scores of split point and label % assignments instead of recursing with the only split % score. % By making independence decisions for split point and label % assignment, we remove the large constant in terms of time % complexity;  and by recursing with the sum of all scores, % dependency relations are maintained. Doing so gives us the advantage that we can search for the best tree in a larger space. % One side effect of this % is that we do not need complex models to represent EDUs. Greedy discourse parsers have to use complex models to ensure each step is correct because the search space is limited. For example,   manually crafted features and feature transformations to encode elementary discourse units ;  and  used multi-task learning for a better EDU representation. Instead, in this work, we use a simple recurrent span representation to build a parser that outperforms previous  global parsers.%  and is comparable to the state-of-art % greedy parsers.  Our contributions are: []   % we have an independence assumption that works    global parser outperforms previous global methods for the task.   % we are better than all greedy parsers that use the same   % representation      %%% Local Variables: %%% mode: latex %%% TeX-master: ""main"" %%% End: % % File emnlp2019.tex % %% Based on the style files for ACL 2019, which were %% Based on the style files for EMNLP 2018, which were %% Based on the style files for ACL 2018, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp-ijcnlp-2019} \usepackage{times} \usepackage{latexsym} \usepackage{mlsymbols} \usepackage{mystyle} \usepackage{symbol} \usepackage{comment}  \usepackage{url}   \\\And%     Omri Koshorek \\     Tel-Aviv University \\     omri.koshorek@cs.tau.ac.il \\\AND%\\     Vivek Srikumar \\     University of Utah \\     svivek@cs.utah.edu \\\And%     Jonathan Berant \\     Tel-Aviv University\\     joberant@cs.tau.ac.il   }  \date{}                  \newpage \appendix        In this work, we propose a new independence assumption for global inference of discourse parsing, which makes globally optimal inference feasible for RST trees.  By using a global inference, we develop a simple neural discourse parser. Our experiments  show that the simple parser can achieve comparable performance to state-of-art parsers using only learned span representations. 
","     Discourse parsing is largely dominated by     greedy parsers with manually-designed     features, while global parsing is rare due to its     computational expense.  In this paper, we propose a     simple chart-based neural discourse parser that does not     require any manually-crafted features and is based on     learned span representations only. To overcome the     computational challenge, we propose an independence     assumption between the label assigned to a node in the     tree and the splitting point that separates its children,     which results in tractable decoding. We empirically     demonstrate that our model achieves the best performance     among global parsers, and comparable performance to     state-of-art greedy parsers, using only learned     span representations.",9
"  Language models that exhibit one- or few-shot learning are of growing interest in machine learning applications because they can adapt their knowledge to new information . One-shot language learning in the physical world is also of interest to developmental psychologists; , the ability to bind a new word to an unfamiliar object after a single exposure, is a much studied facet of child language learning . Our goal is to enable an embodied learning system to perform fast-mapping, and we take a step towards this goal by developing an embodied agent situated in a 3D game environment that can learn the names of entirely unfamiliar objects in a single exposure, and immediately apply this knowledge to carry out instructions based on those objects. The agent observes the world via active perception of raw pixels, and learns to respond to linguistic stimuli by executing sequences of motor actions. It is trained by a combination of conventional RL and predictive  learning.   We find that an agent architecture consisting of standard neural network components is sufficient to follow language instructions whose meaning is preserved across episodes. However, learning to fast-map novel names to novel objects in a single episode relies on semi-supervised prediction mechanisms and a novel form of  external memory, inspired by the dual-coding theory of knowledge representation . With these components, an  agent can exhibit both slow word learning and fast-mapping. Moreover, the agent exhibits an emergent propensity to integrate both fast-mapped and slowly acquired word meanings in a single episode, successfully executing instructions such as ``put the dax in the box"" that depend on both slow-learned  and fast-mapped  word meanings.   %An embodied learning system that executed fast-mapping with the same flexibility as the best large-scale text-based language models could lead to similarly improved human-agent interaction between users and game-based agents, virtual-reality avatars or robotic assistants.     Via controlled generalization experiments, we find that the agent is reasonably robust to a degree of variation in the number of objects involved in a given fast-mapping task at test time. The agent also exhibits above-chance success when presented with the name for a particular object in the ShapeNet taxonomy  and then instructed  to interact with a different exemplar from the same object class, and this propensity can be further enhanced by specific meta-training. We find that both the number of unique objects observed by the agent during training and the temporal aspect of its perceptual experience of those objects contribute critically to its ability to generalize, particularly its ability to execute fast-mapping with entirely novel objects. Finally, we show that a dual-coding memory schema can provide a more effective basis to derive a signal for intrinsic motivation than a more conventional  memory.   %Equipped with this intrinsic curiosity, an agent can resolve long episodes requiring fast-binding when there are no intermediate environment rewards to stimulate the requisite information discovery.         Biological learners, including humans, are surrounded by qualitatively distinct physical media such as sound and light, and have evolved specialized mechanisms for processing input from these modalities. For certain tasks relating to language learning, useful information can reside not just in the aggregate content of these input streams, but in the identification of specific knowledge with a particular modality. An episodic memory system that retains knowledge in explicit, modality-specific locations can endow learners with important advantages.   Our experiments have highlighted various benefits of having an explicitly multi-modal episodic memory system. First, mechanisms that allow the agent to query its memory in a modality-specific way  can better allow them to rapidly infer and exploit connections between perceptual experience and words, and therefore to realize , a notable aspect of human learning. Second, external  memories can achieve better performance for the same number of memory `slots' than Transformer-based memories. This greater `memory-efficiency' may be increasingly important as agents are applied to real-world tasks with very long episodic horizons. Third, in cases where it is useful to estimate the degree of novelty or ``surprise"" in the current state of the environment , a more informative signal may be obtained by separately estimating novelty based on each modality and aggregating the result. Finally, an episodic memory system may ultimately be essential for fast knowledge~. The potential for memory buffers and offline learning processes such as  to support knowledge consolidation is not a new idea . For language learning agents, the need to both rapidly acquire  multi-modal knowledge may further motivate explicit external memories. Retaining in memory visual experiences together with aligned  language  may facilitate something akin to offline `supervised' language learning. We leave this possibility for future investigations, which we will facilitate by releasing publicly the environments and tasks described in this paper.    We also highlight several other implications of this work regarding the proximate challenge of developing linguistic agents that can adapt flexibly to human input. First, we found that semi-supervised learning to support richer observation representations was critical for success regardless of the memory architecture that was used. This observation coincides with other recent demonstrations of the efficacy of such techniques for representation learning~. Second, we have shown that, much like GPT-3 and other large text-based models, goal-directed and embodied neural network agents that acquire visual and motor skills in interaction with their environment can also apply meta-learning to integrate `fast' and `slow' lexical knowledge. Many substantial challenges remain for this research programme, however. These include widening the agent's scope to unconstrained user language , and the capacity to respond via language or motor behaviour and to combine both types of response in optimal ways.         
"," Recent work has shown that large text-based neural language models acquire a surprising propensity for one-shot learning. Here, we show that an agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional RL algorithms. After a single introduction to a novel object via visual perception and language , the agent can manipulate the object as instructed , combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for , a fundamental pillar of human cognitive development and a potentially transformative capacity for artificial agents.",10
" Transfer learning is a rapidly growing field of machine learning that aims to improve the learning of a data-deficient task by knowledge transfer from related data-sufficient tasks.  % Various factors may affect the availability of sufficient training data to calibrate a well-performed model.  Witness the success of deep learning, deep transfer learning has been widely studied and demonstrated remarkable performance over various applications, such as medical image classification, electronic health data analysis, and credit modeling.   A fundamental block of deep transfer learning is deep neural network, which is vulnerable to different attacks aiming to detect sensitive information contained in the training dataset. Moreover, in most of the real-world applications where deep transfer learning is used, the source and target datasets always reside in two different organizations. As a result, deep transfer learning also faces potential privacy threats, i.e, the client in the target organization can leverage the vulnerability of deep learning models to detect sensitive information contained in the source organization. Specifically, applying deep transfer learning comes with the interaction between the source and target domains. Thus, the data transmission between these domains may unintentionally disclose private information.   %A typical example is to apply transfer learning on medical image classification. Considering two hospitals, one holds massive labeled images  and another only has a small dataset . To improve the model quality on the target task, a typical deep transfer learning method is to first obtain  %Despite the success of deep transfer learning, since  %the source and target datasets always reside in two different organizations, some privacy issues %are posed in many real-world scenarios. Recently, these privacy issues have drawn increasing attentions form both the industrial/academic communities with the publication of various data privacy regulations, such as  Europe閳ユ獨 General Data Protection Regulation .  Existing studies on analyzing privacy leakages focus on either general machine learning models or in a federated learning setting where model is collaboratively trained by multiple clients by sharing and aggregating the gradients via a server. However, there no such study on  transfer learning paradigms.  To this end, we are the first to provide a general categorization for deep transfer learning models based on the potential information leakages. This is not trivial since there are numerous methods for deep transfer learning. Given the goal of privacy leakage analysis, we care more about the interaction manner between source and target domains.  %In this paper, we aim to analyze the potential information leakage in different deep transfer learning algorithms. Thus, we divide previous works into three categories, as illustrated in Figure 1:  model-based paradigm where the whole model structure and parameters are shared  mapping-based where the hidden features are shared  parameter-based where the parameter gradients are shared.  Based on that, the previous works can fall into the above categories or a hybrid of them.  For example, fine-tuning based approaches obviously belong to the first category. The prior work is based on the mapping-based paradigm, since it uses the correlation alignment loss which further depends on the shared hidden features. Similarly, previous works that minimize the domain representation difference by variants of distribution divergence metrics such as maximum mean discrepancy also fall into the second category. Fully-shared and shared-private transfer learning models  can be regarded as parameter-based, as they both jointly train a shared network via gradient updates in a multi-task fashion, just to name a few.  % the MMD-based metric also fall into the second category. % variants of MaximumMean Discrepancy, Kullback-Leibler Divergence, Wasser-stein distance, and etc   [t!]     ^S\mathcal{D}^Txywh$ for feature representations.}              Based on the general categorization, we can build customized attacks against each paradigm and demonstrate information leakages in deep transfer learning. At a high level, we consider inferring two types of sensitive information, i.e., membership and property information. This sensitive information can be revealed by the transmission data between the two domains as the above discussed. Specifically, in the model-based paradigm, we build the membership attack which takes the model  as input and determines whether a specific sample is used for training the model. In the mapping-based setting, we can build the property attack to infer properties contained in the training dataset. For example, the attacker resides on the target domain aims to infer properties of the source domain based on the shared hidden features. In the parameter-based setting, we can similarly perform the property inference attack, i.e., the attacker can infer properties of the source domain data based on the shared gradients. More details of these attacks can be found in Section.  Empirically, to demonstrate the effectiveness of attacks, we conduct a set of experiments under the three types of transfer learning settings. Our key observation is that all these types of models do unintentionally leak information of the training data under membership/property attacks.  Model-based paradigm is possible to leak membership information. Parameter-based paradigm without revealing individual gradient  leaks much less property information, compared to the mapping-based paradigm where hidden features  are shared. % As illustrated in the experiments,   % . In summary, our main contributions are as follows:           %which comes with different privacy leakage profiles.     %   % The rest of this paper is organized as follows: Section  introduces the basic setting of deep transfer learning and background of inference attacks against deep learning models. Section  provides the general categorization of deep transfer learning and detailed privacy analysis. Section  shows the information leakage in deep transfer learning empirically. % Section briefly summarizes some % related works and Section draws the conclusion.  %% %% This is file `sample-sigconf.tex', %% generated with the docstrip utility. %% %% The original source files were: %% %% samples.dtx   %%  %% IMPORTANT NOTICE: %%  %% For the copyright see the source file. %%  %% Any modified versions of this file must be renamed %% with new filenames distinct from sample-sigconf.tex. %%  %% For distribution of the original source see the terms %% for copying and modification in the file samples.dtx. %%  %% This generated file may be distributed as long as the %% original source files, as listed above, are part of the %% same distribution.  %% %% The first command in your LaTeX source must be the \documentclass command. \documentclass[sigconf]{acmart} %%%% As of March 2017, [siggraph] is no longer used. Please use sigconf  for SIGGRAPH conferences.  %%%% Proceedings format for SIGPLAN conferences  % \documentclass[sigplan, anonymous, review]{acmart}  %%%% Proceedings format for SIGCHI conferences % \documentclass[sigchi, review]{acmart}  %%%% To use the SIGCHI extended abstract template, please visit % https://www.overleaf.com/read/zzzfqvkmrfzn  \usepackage{xcolor} \usepackage{soul} \usepackage{url} \usepackage{caption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{amsthm} \usepackage{booktabs} %\usepackage{algorithm} %\usepackage{algorithmic} \usepackage{multirow} \usepackage{listings} \usepackage{array}  \renewcommand{\lstlistingname}{Code} \lstset{frame=tb,   language=C,   aboveskip=3mm,   belowskip=3mm,   showstringspaces=false,   columns=flexible,   basicstyle={{*)}, %  frame=single, } \usepackage[linesnumbered, ruled, boxed]{algorithm2e}  [1]{} \urlstyle{same}  } [1]{{[]} }   %% %% \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{%   \providecommand\BibTeX{{%           %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%  %% %% The majority of ACM publications use numbered citations and %% references.  The command  switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%  %% %% end of the preamble, start of the body of the document source.   %% %% The ""title"" command has an optional parameter, %% allowing the author to define a ""short title"" to be used in page headers.  % \title{A Comprehensive Privacy Analysis of Deep Transfer Learning} \title{A Comprehensive Analysis of Information Leakage in Deep Transfer Learning}  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research.  \author{Cen Chen, Bingzhe Wu, Minghui Qiu, Li Wang, Jun Zhou}  % \authornote{Both authors contributed equally to this research.}  @antgroup.com}   \affiliation{%              }  % \author{Lars Th{\o}rv{\""a}ld} % \affiliation{% %   rv{\""a}ld Group} %   rv{\""a}ld Circle} %    %    % } %   % \author{Valerie B\'eranger} % \affiliation{% %    %    %    % }  %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose.   \renewcommand{  %% %% The abstract is a short summary of the work to be presented in the %% article.  Transfer learning is widely used for transferring knowledge from a source domain to the target domain where the labeled data is scarce. Recently, deep transfer learning has achieved remarkable progress in various applications. However, the source and target datasets usually belong to two different organizations in many real-world scenarios, potential privacy issues in deep transfer learning are posed. In this study, to thoroughly analyze the potential privacy leakage in deep transfer learning, we first divide previous methods into three categories. Based on that, we demonstrate specific threats that lead to unintentional privacy leakage in each category.  Additionally, we also provide some solutions to prevent these threats. To the best of our knowledge, our study is the first to provide a thorough analysis of the information leakage issues in deep transfer learning methods and provide potential solutions to the issue. Extensive experiments on two public datasets and an industry dataset are conducted to show the privacy leakage under different deep transfer learning settings and defense solution effectiveness.  %  Extensive experiments are conducted to show the privacy leakage of typical deep transfer learning on a number of public datasets.   %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %%  <ccs2012> <concept> <concept_id>10002978.10003022</concept_id> <concept_desc>Security and privacy~Software and application security</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10010147.10010257.10010258.10010262.10010277</concept_id> <concept_desc>Computing methodologies~Transfer learning</concept_desc> <concept_significance>500</concept_significance> </concept> </ccs2012>   %  %     %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.    %% This command processes the author and affiliation and title %% information and builds the first part of the formatted document.     %       In this study, we provide a general categorization of different deep transfer learning paradigms depending on how the domains interact with each other. Based on that, we then analyze their respective privacy leakage profiles, design different attack models for each paradigm and provide potential solutions to prevent these threats. Extensive experiments have been conducted to examine the potential privacy leakage and effectiveness of defense solutions.   under different deep transfer learning settings.   \clearpage       The next two lines define the bibliography style to be used, and    the bibliography file.     \endinput       End of file `sample-sigconf.tex'. 
"," Transfer learning is widely used for transferring knowledge from a source domain to the target domain where the labeled data is scarce. Recently, deep transfer learning has achieved remarkable progress in various applications. However, the source and target datasets usually belong to two different organizations in many real-world scenarios, potential privacy issues in deep transfer learning are posed. In this study, to thoroughly analyze the potential privacy leakage in deep transfer learning, we first divide previous methods into three categories. Based on that, we demonstrate specific threats that lead to unintentional privacy leakage in each category.  Additionally, we also provide some solutions to prevent these threats. To the best of our knowledge, our study is the first to provide a thorough analysis of the information leakage issues in deep transfer learning methods and provide potential solutions to the issue. Extensive experiments on two public datasets and an industry dataset are conducted to show the privacy leakage under different deep transfer learning settings and defense solution effectiveness.  %  Extensive experiments are conducted to show the privacy leakage of typical deep transfer learning on a number of public datasets.",11
"   Emphasis selection is an emerging research problem  in the natural language processing domain, which involves automatic identification of words or phrases from a short text that would serve as good candidates for visual emphasis. This research is most relevant to visual media such as flyers, posters, ads, and motivational messages where certain words or phrases can be visually emphasized with the use of different color, font, or other typographic features. This type of emphasis can help with expressing an intent, providing more clarity, or drawing attention towards specific information in the text. Automatic emphasis selection is therefore useful in graphic design and presentation applications to assist users with appropriate choice of text layout.   Prior works in speech processing  have modeled word-level emphasis using acoustic and prosodic features. Understanding emphasis in speech is critical to many downstream applications such as text-to-speech synthesis , speech-to-speech translation , and computer assisted pronunciation training . In computational linguistics, emphasis selection is very closely related to the problem of keyphrase extraction . Keyphrases typically refer nouns and noun-phrases that capture the most salient topics in long documents such as scientific articles , news articles , web pages , etc. In contrast, emphasis selection deals with very short texts , and also emphasis could be applied to words belonging to various parts of speech.  The goal of SemEval 2020 - Task 10 is to design methods for automatic emphasis selection in short texts. To this end, the organizers  provided a dataset consisting of over 3,000  sentences annotated for token-level emphasis by multiple annotators. The authors employed the standard I-O tagging schema, which is widely used in annotation of token-level tags. We approached emphasis selection as a sequence labeling task solved using a Bidirectional Long Short-term Memory  model, where the individual tokens are represented using various contextual embedding models. We also employ label distribution learning   approach, which elegantly accounts for disagreements between the annotators.     In this paper, we present our submission to the SemEval 2020 - Task 10 on emphasis selection in written text. Our best performing model achieved an overall matching score of 0.783, placing us 15th out of 31 participating teams. We approached emphasis selection as sequence prediction problem solved using BiLSTMs. Our experimental work demonstrates the effect of model architectures, trainability of layers, and embeddings on the performance. We analyze the results in terms of parts of speech tags and sentence lengths. Our analysis provides some interesting insight into some of the shortcomings of the models and also the challenges with emphasis selection.   
"," This paper presents our submission to the SemEval 2020 - Task 10 on emphasis selection in written text. We approach this emphasis selection problem as a sequence labeling task where we represent the underlying text with various contextual embedding models. We also employ label distribution learning to account for annotator disagreements. We experiment with the choice of model architectures, trainability of layers, and different contextual embeddings. Our best performing architecture is an ensemble of different models, which achieved an overall matching score of 0.783, placing us 15th out of 31 participating teams. Lastly, we analyze the results in terms of parts of speech tags, sentence lengths, and word ordering.",12
"     A series of countries from our world are multilingual, which implies that there are multiple languages spoken by their population. People tend to mix them at the phrase or sentence level in order to express ideas with ease, thus creating a phenomenon called code-mixing or code-switching. As it is expected, this embedding of a language into another one makes its appearance in the virtual space, as well. For example, Twitter users combine Hindi or Spanish phrases with English words, thus creating a bilingual phrase that can lead to understanding problems for non-natives.  However, the virtual space adds more layers of difficulty in identifying the sentiment of the author. Usually, social media users tend to adopt phonetic typing, which implies that the words will not take their original form, they will be adapted such that it will be faster to express the main idea.  As a particular case, for the Hindi-English users, a new problem arises: Hindi and English use different alphabets, which determines the user to romanize the Hindi words such that both languages will use the same alphabet throughout the text. At the same time, social media users tend to express their sentiments by repeating certain vocals in words.  Furthermore, they use emojis, which will add an extra layer of complexity for analyzing the text.  This entire process creates new opportunities for research, given the importance of sentiment analysis in this area. The SemEval-2020 Task 9:  Sentiment Analysis for Code-Mixed Social Media Text challenges the research community to solve the previously mentioned problem by introducing two subtasks, focusing on three of the world's most spoken languages: Hindi and Spanish, alongside English. We proposed a series of neural models that intend to solve this issue, contributing under the usernames eduardgzaharia and clementincercel, respectively. Firstly, we experimented with Recurrent Neural Network  solutions alongside word embeddings. After that, we performed the leap towards Transformer-based models that usually perform better and offer more insight for the combined language models. Furthermore, adding an auxiliary task for training a multi-task learning  architecture can lead to even better results, as the models become able to learn new features from the input texts.  The paper is structured as follows. In section 2, we perform an analysis of existing solutions for several code-mixed tasks and sentiment analysis. In section 3, we detail the proposed approaches for code-mixed sentiment analysis. Section 4 details the performed experiments, including data and preprocessing, experimental setup, and a discussion of the results. Finally, we draw conclusions in section 5.    This paper presents our solutions for the code-mixed sentiment analysis shared task, organized by SemEval 2020. We experimented with state-of-the-art natural language processing models, alongside training dataset extensions and a MTL technique. Sentiment analysis proved to become challenging if the subject data is written in a code-mixed format. Because of the difficulty imposed by detecting ideas and sentiments from texts based on two different languages, merged into a single one, it becomes a requirement to develop ways to analyze the sources.  However, the performance of our models is vastly influenced by a series of prerequisites, one of them being represented by the word embeddings. Proper word embeddings can boost performances by a large margin, considering the fact that it already offers the model an insight into that language. The problem becomes more complicated if we deal with two languages instead of one. This situation requires a modality of mapping two sets of embeddings into a single vectorial space for the solutions that are not based on Transformers.  Furthermore, various network architectures can lead to different results. Networks that intend to capture the sequentiality of the text prove to be extremely efficient, obtaining better results. At the same time, networks with layers that increase in specificity can perform certain analysis tasks for distinct levels of the text. Additionally, XLM-RoBERTa performs better when exposed to synthetic data alongside a MTL training technique. This aspect can be attributed to its ability to better identify features inside the texts, including synthetic ones, considering that it was trained on the largest amount of data out of the models used for experiments.   For future work, we intend to experiment with the large versions of the previously mentioned language models. Theoretically, a greater number of parameters should help us improve our performance. Moreover, we also intend to include a different auxiliary task for the MTL approach, by avoiding the hard assignment of the language class and replacing it with a soft assignment.    include your own bib file like this:       
"," Sentiment analysis is a process widely used in opinion mining campaigns conducted today. This phenomenon presents applications in a variety of fields, especially in collecting information related to the attitude or satisfaction of users concerning a particular subject. However, the task of managing such a process becomes noticeably more difficult when it is applied in cultures that tend to combine two languages in order to express ideas and thoughts. By interleaving words from two languages, the user can express with ease, but at the cost of making the text far less intelligible for those who are not familiar with this technique, but also for standard opinion mining algorithms. In this paper, we describe the systems developed by our team for SemEval-2020 Task 9 that aims to cover two well-known code-mixed languages: Hindi-English and Spanish-English.  We intend to solve this issue by introducing a solution that takes advantage of several neural network approaches, as well as pre-trained word embeddings. Our approach  achieves promising performance on the Hindi-English task, with an average F1-score of 0.6850, registered on the competition leaderboard, ranking our team \ out of 62 participants. For the Spanish-English task, we obtained an  average F1-score of 0.7064 ranking our team \ out of 29 participants by using another multilingual Transformer-based model, XLM-RoBERTa.",13
" The recent outbreak of SARS-CoV-2 has led to a global pandemic with the total number of infections exceeding 6 million with more than 370000 mortality already. The disease has been code named COVID-19 and had far reaching repercussions the world over. This article aims to uncover the life science universe of the Corona virus and related ailments by employing some of the state-of-the-art natural language processing technologies applied to biomedical domain. We took the corpus of about 40000 titles and abstracts released as a part of CORD-19 Open Research Challenge and applied our entity recognition and relationship discovery models to construct a knowledge graph related to COVID-19. In the process, we uncovered about 40000 entities and 80000 relationships.  This article presents our salient findings and is organized as follows. Section  briefly describes our masked entities model and masked relationship model. Section  presents a network analysis of the knowledge network discovered by mining CORD-19 dataset. The coverage of CORD-19 dataset may be not exhaustive and up-to-date. We took snapshot around April 15, 2020. Nevertheless, the primary aim of this work is to demonstrate the application of artificial intelligence on condensing unstructured information in the biomedical domain to a sufficiently low entropy state so that some important leads can be established.    We undertook a comprehensive concept identification and network analysis for COVID-19. We demonstrated the use of a novel concept recognition and relationship discovery engine that crafts some of the latest advances in natural language processing into a state-of-the-art solution for biomedical entity recognition and relationship discovery problem. Several new drugs were uncovered through the studies and many different treatment modalities were brought to the surface. We envision these solutions to have a wide ranging impact through the length and breadth of drug discovery process spanning all therapeutic areas.     
"," We extract entities and relationships related to COVID-19 from a corpus of articles related to Corona virus by employing a novel entities and relationship model. The entity recognition  and relationship discovery models are trained with a multi-task learning objective  on a large annotated corpus. We employ a concept masking paradigm to prevent the evolution of neural networks functioning as an associative memory and induce right inductive bias guiding the network to make inference using only the context. We uncover several import subnetworks, highlight important terms and concepts and elucidate several treatment modalities employed in related ailments in the past.",14
"  The COVID-19 pandemic urged various science disciplines to do their best so as to contribute to understanding and relieving its impact. Thus, scholars and practitioners working on information sciences have been dedicating significant effort to help. Collecting and analyzing data published on social media platforms have become the focus in this respect. We joined the community that aims at organizing data collected from social media , as informative and uninformative. The WNUT-2020 Task 2 considers tweets about recovered, suspected, confirmed and death cases as well as location or travel history of the cases as informative. All other tweets are considered to be uninformative. The organizers did not share an annotation manual nor was a baseline system made available, presumably to prevent use of any other manually annotated data and to encourage broad participation respectively.\footnote{\url{http://noisy-text.github.io/2020/covid19tweet-task.html}, accessed on September 4, 2020.}  The effort was managed in terms of a shared task, in which the organizers share a dataset that consists of annotated tweets and conduct the evaluation of the submissions. The task requires the participating teams to develop short-text classification systems that facilitate the training and development data to generalize to the test set they release. Although the gold labels of the training and development data were available to the participants, neither the gold labels of the test data nor the annotation guidelines for any part of the data were shared with the participants. Moreover, the test instances were unknown to the participating teams. They were hidden in a larger dataset. Each team was allowed to submit only two outputs of the systems they developed for classifying tweets on the Codalab page of the task.\footnote{\url{https://competitions.codalab.org/competitions/25845}, accessed on September 4, 2020.} The highest score in terms of F1 positive class of each team was used to rank them in the leaderboard.  Integrating automatically created machine learning based  models with manually formulated rules to tackle a text classification task promises the best of both worlds. We pursued this goal by integrating the output of two deep learning models and a rule-based system under the team name COVCOR20. Although the integration slightly improves the total performance on the training and development sets in a cross-validation setting, the overall performance on the test data turned out to be slightly worse than our best ML system. Our best submission was ranked 22nd among 55 teams. The integration of our systems would be ranked 27th if its score were used as the final score for our team.  The deep learning models and the rule-based system are introduced in Sections and respectively. Next, the Section describes how we integrate the output of these systems. Then Section provide the results and their discussion. Finally, we conclude this report and share our future plans continuing in this line of research in Section.              NO: hyphens are only used if in state of the art is used as a premodifying adjective   We have presented our effort in the scope of a shared task that aims at pushing the state of the art for classifying short-texts , as informative or uninformative.   We could extend the training set with cluster mining using Relevancer, use the rule-based system to extend the training set, or use the rule-based system to generate fine-grained data that can be used in a multi-task setting.     The authors from Ko鑾 University are funded by the European Research Council  Starting Grant 714868 awarded to Dr. Erdem Y鏋歳鐪塳 for his project Emerging Welfare.          
"," In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of  the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the integration was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating machine learning and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.",15
"      The phenomenon of combining two or more languages in the same message is known as code-switching or code-mixing . Code-switching is an indicator of bilingual competence  , and it is also motivated by social and cultural factors such as social status, race, age, etc. . %   Instead of consider it as an indicator of lack of competence , there are cultural and social factors which motivate its study . Although this phenomenon has been studied extensively in linguistics , it is still challenging for machines to process mixed natural languages. Code-switching is notoriously present on social media posts and chats such as Twitter, Facebook or WhatsApp;  consequently making it more difficult to process the sentiment expressed in such contents. %Multilingual people, who are non-native English speakers, tend to code-mix using English-based phonetic typing and the insertion of anglicisms in their main language.  %In addition to mixing languages at the sentence level, it is fairly common to find the code-mixing behavior at the word level.  %This linguistic phenomenon cannot be tackled with conventional NLP systems, which are based on monolingual resources to handle the combination of multiple languages.   %Statistics show that half of the messages on Twitter are in a language other than English. This evidence suggests that other languages, including multilingualism and code-mixing, need to be considered by the NLP community. %    In this work, we present a Convolutional Neural Network  system to predict the sentiment of a given code-mixed tweet. The sentiment labels are either positive, negative, or neutral, and the languages involved are English and Spanish. Our best model utilizes only Spanish word embeddings from tweets  and does not require manual feature engineering.  %  Before classification, English texts were normalized to anonymize some entities, label stylistic patterns, and transform words to tackle some typical issues of the texts on Twitter.  %  We highlight the contributions of this work as follows:  %  %        %This paper is structured into six different sections. Section 2 contains the dataset description. As for section 3, contains the literature review that presents the existing related work on code-mixing. Section 4 depicts our methodology. Section 5 is devoted to the presentation and discussion of our experimental results. Finally, our recommendations for future research opportunities along with the conclusion are reported in section 6.   % ======================== Article section    Code-switching is an interesting problem holding an important presence in social media, which combined with informal writing style increases the challenges for social media processing such as sentiment analysis.  We experimented with the Sentimix Spanglish dataset using CNN model and only Spanish embeddings. We achieve a precsion, recall, and F1 score of 0.80, 0.64, and 0.71 respectively.    reporting good enough results for the competition .  Our analyses suggest that a deep learning model can be easily biased by the presence of cue words such as vulgar expressions for sentiment analysis. We found that this occurs mostly when the cue word is in English. This observation requires a deeper analysis.    .  We also highlight the need to address complex language usage such as informality and sarcasm.       as well as dealing with messages involving extralinguistic information which usually needs world knowledge understanding for being processed.  Furthermore, we also pointed out that subjectivity in the annotation of sentiment labels is a problem that deserves to be addressed.  We plan to test contextual multilingual embeddings  and leverage the language tags and other non-linguistic constructs such as hashtags and emojis.            ======================== Article section   Do not include this section when submitting your paper for review.   
"," %   Code-switching is a phenomenon in which two or more languages are used in the same message. Nowadays, it is quite common to find messages with languages mixed in social media. This phenomenon presents a challenge for sentiment analysis. % forcing the models to use a mix of language resources. In this paper, we use a standard convolutional neural network model to predict the sentiment of tweets in a blend of Spanish and English languages. Our simple approach achieved a F1-score of $0.71$ on test set on the competition. We analyze our best model capabilities and perform error analysis to expose important difficulties for classifying sentiment in a code-switching setting.",16
"  .     %      % % final paper: en-us version      %        % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. }   Emphasis selection for written text in visual media is proposed by  and . The purpose of this shared task is to design automatic methods for emphasis selection, i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring. For example,  mentions that such a technique can be applied to some graphic design applications such as Adobe Spark to perform automatic text layout using templates that include images and text with different fonts and colors. The major challenge is that given only thousands of annotated short text data without any context about the text or visual background images, we are asked to learn the author- or domain-specific emphatic about the short text. Besides, these short text data are annotated by crowd-sourcing workers. And we find that different annotators have different standards, which increases the difficulty of this task.   To identify the most important words, we model the task as a sequential labeling problem. Our base models leverage different unsupervised language model such as ERNIE 2.0 , XLM-ROBERTA , ROBERTA  and ALBERT . These large unsupervised models are pre-trained on a large amount of unannotated data and carry valuable lexical, syntactic, and semantic information in training corpora. Our approach is as follows: firstly, the word-level output representations for the sentence are computed by pre-trained models and then fed into a designed downstream neural network for word selections; secondly, we finetune the downstream networks together with the pre-trained models on the annotated training data; thirdly, we investigate several different objective functions to learn our model; and finally, we apply feature engineering and several data augmentation strategies for further improvement.   The rest of the paper is organized as follows. In Section , we will briefly overview some related works to our system. Section  shows the details of our approach. Our experiments will be shown in Section , and Section  concludes.     In this paper, we present our system that ranks first in SemEval-2020 Task 10. Our solution contains several strategies and we provide detailed experiments to analyze which of them are effective. Our experiments show that models empowered by pre-trained language models are most effective, especially for ERNIE 2.0. Besides, lexical features, pairwise loss, and data augmentation can also bring improvement for some of our models.     include your own bib file like this: 
","   This paper describes the system designed by ERNIE Team which achieved the first place in SemEval-2020 Task 10: Emphasis Selection For Written Text in Visual Media. Given a sentence, we are asked to find out the most important words as the suggestion for automated design. We leverage the unsupervised pre-training model and finetune these models on our task. After our investigation, we found that the following models achieved an excellent performance in this task: ERNIE 2.0, XLM-ROBERTA, ROBERTA and ALBERT. We combine a pointwise regression loss and a pairwise ranking loss which is more close to the final $Match_{m}$ metric to finetune our models. And we also find that additional feature engineering and data augmentation can help improve the performance. Our best model achieves the highest score of 0.823 and ranks first for all kinds of metrics.",17
"  Coreference resolution aims at identifying all the expressions that refer to the same entity in a text.  It helps to derive the correct interpretation of a text by binding antecedents  with their pronouns together and recognizing the syntactic relationship among them. The coreference resolution is considered as a critical preprocessing step for various high-level natural language processing  tasks including document summarization, question answering, and information extraction .   Existing coreference resolution approaches can be divided into two major categories: mention-pair models  and entity-mention models . One of the main shortcomings of the mention-pair model is making each coreference decision without entity-level information. Moreover, the lack of information about the preceding clusters may result in contradictory links. The entity-mention model tries to make use of the non-local information by encouraging the sharing of features across all mentions that point to the same real-world entity. However, the coreferent mentions usually spread far apart in a text, which makes it extremely difficult to define effective global features.    Previous studies either count on the long-term memory  or their variants to implicitly capture the global features   or seek to incorporate the features of the clusters already formed to determine whether a mention is coreferent with a preceding cluster . The former might miss out some important features for specific pairwise predictions without the help of the explicit entity-level features, while the latter may suffer from error propagation as false clusters are used to create entity-level features when making future predictions.  Taking the text of ``On November 3, 1992, Clinton was elected the 42nd president of the United States, and the following year Hillary Clinton became the first lady. In 2013, he won the Presidential Medal of Freedom."" as an example, we assume that three mentions ``Clinton"", ``Hillary Clinton"", and ``he"" have been well identified. The traditional mention-pair model is very likely to group these three mentions into a cluster as shown in Figure  since ``Clinton"" and ``Hillary Clinton"" share the same surname, and ``he'' agrees with ``Clinton"" both in gender and number.  To make use of information about the clusters already formed, recent studies try to better represent the current mention by incorporating the features derived from the preceding cluster it will most probably join .  However, those methods only allow such information to be shared in a forward fashion, i.e., from antecedent expressions to postcedent ones, and are prone to reaching the results as shown in Figure  and .  The reason is that once ``Hillary Clinton'' is merged with ``Clinton'' to form a cluster, the pronoun ``he'' either joins the formed cluster or begins a new one by itself.  Even though these errors might be recovered by using a proper decoding algorithm at test time, such as the maximum spanning tree algorithm, similar errors cannot be completely eliminated.  If such information can be shared iteratively in both forward and backward ways, the disagreement in gender between ``Hillary Clinton'' and ``he'' will be detected when the representation of ``Clinton'' is updated by its two possible co-references, which helps to find the correct result as Figure .  Recently, graph neural network  has gained increasing popularity due to its ability in modeling the dependencies between nodes in a graph . For the coreference resolution, mentions are linked to each other via the edges modeling how likely two linked mentions refer to the same entity. The features between nodes  can be shared in each direction with message passing or neighborhood aggregation in an iterative way. We found the entity-centric features can be well captured by GNN, achieving close to state-of-the-art performance.  To avoid contradictory links in mention clustering results, we propose to use a variant of the maximum spanning tree algorithm, second-order decoding algorithm instead of the traditional greedy search algorithm  and the beam search algorithm .  We factorize the score of a tree into the sum of its arc-pair scores.  A pair of arcs link three different mentions, and the connected mentions can be viewed as a small cluster.  Our global inference algorithm up to second-order features helps to define powerful entity-level features between clusters of mentions by aggregating the scores of those small clusters.   Traditional coreference resolution methods usually include three successive steps: mention detection, candidate pair generation, and mention clustering .  However, recent studies  show that joint solutions usually lead to improved performance over pipelined systems by avoiding error propagation. We follow the line of these research and formulate coreference resolution in a joint manner.  Our contributions are summarized as follows:  graph neural networks are introduced to perform coreference resolution, which aims to better leverage the entity-centric information by encouraging the sharing of features across all mentions that refer to the same entity;   a global inference algorithm up to second-order features is presented to optimally cluster mentions into consistent groups;   we show our GNN-based method combing with the second-order decoding algorithm achieved close to state-of-the-art performance on the CoNLL-2012 coreference resolution benchmark.     We proposed a coreference resolution system based on graph neural networks and enhanced with the second-order decoding algorithm. Modeling the mentions and their relationships by the multiple-layer graph neural networks makes it possible to aggregate the features of the mentions pointing to the same entity in an iterative way, while the global inference algorithm up to second-order features helps to produce optimal and consistent clustering results. Experiments on the English CoNLL-2012 shared task dataset demonstrated that our model achieved close to state-of-the-art performance in the coreference resolution task.      
"," One of the major challenges in coreference resolution is how to make use of entity-level features defined over clusters of mentions rather than mention pairs. However, coreferent mentions usually spread far apart in an entire text, which makes it extremely difficult to incorporate entity-level features. We propose a graph neural network-based coreference resolution method that can capture the entity-centric information by encouraging the sharing of features across all mentions that probably refer to the same real-world entity. Mentions are linked to each other via the edges modeling how likely two linked mentions point to the same entity.  Modeling by such graphs, the features between mentions can be shared by message passing operations in an entity-centric manner. A global inference algorithm up to second-order features is also presented to optimally cluster mentions into consistent groups. Experimental results show our graph neural network-based method combing with the second-order decoding algorithm  achieved close to state-of-the-art performance on the English CoNLL-2012 Shared Task dataset.",18
" Encoding linguistic units such as words, phrases or sentences into low-dimensional vectors has been the core and preliminary task for deep learning of natural language. The current language representation learning is usually done in different individual levels, typically, word or sentence. The former includes pioneering works such as word2vec, GloVe and fastText , and the latter includes the very recent so-called contextualized representations such as ELMo, GPT, BERT, XLNet and ELECTRA . Nevertheless, few works were done to uniformly learning and representing linguistic units in different hierarchies in the same vector space. Actually, nearly all existing work still focus on individual granular language unit for representation learning .  However, universal representation among different levels of linguistic units may offer a great convenience when it is needed to handle free text in language hierarchy in a unified way. As well known that, embedding representation for a certain linguistic unit  enables linguistics-meaningful arithmetic calculation among different vectors, also known as word analogy. For example,  results in . Thus universal representation may generalize such good analogy features or meaningful arithmetic operation onto free text with all language levels involved together. For example, Eat an onion : Vegetable :: Eat a pear : Fruit.   In this paper, we explore the regularities of representations including words, phrases and sentences in the same vector space. To this end, we introduce universal analogy tasks derived from Google's word analogy dataset. In addition, we train a Transformer-based model and compare it with currently popular representation methods. Experimental results demonstrate that well-trained Transformer-based models are able to map sequences of variable lengths into a shared vector space where similar sequences are close to each other. Meanwhile, addition and subtraction of embeddings reflect semantic and syntactic connections between sequences. In addition, we explore the applicability of this characteristic in retrieval-based chatbots by evaluation on an insurance FAQ task, where the universal representation models significantly outperform TF-IDF and BM25.     This work concentrates on the less concentrated language representation, seeking to learn a uniform vector form across different linguistic unit hierarchies. Far apart from learning either word only or sentence only representation, we find that training Transformer models on a large-scale corpus effectively learns a universal representation from words, phrases to sentences. We especially provide universal analogy datasets  \footnote{Our annotated datasets will be publicly released after the anonymous reviewing period.}  and an insurance FAQ dataset to evaluate models from different perspectives. The well-trained universal representation model holds the promise for demonstrating accurate vector arithmetic with regard to words, phrases and sentences and in applications such as FAQ retrieval tasks.   
"," Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific level of linguistic unit, which cause great inconvenience when being confronted with handling multiple layers of linguistic objects in a unified way. Thus this work introduces and explores the universal representation learning, i.e.,  embeddings of different levels of linguistic unit in a uniform vector space through a task-independent evaluation. We present our approach of constructing analogy datasets in terms of words, phrases and sentences and experiment with multiple representation models to examine geometric properties of the learned vector space. Then we empirically verify that well pre-trained Transformer models incorporated with appropriate training settings may effectively yield universal representation. Especially, our implementation of fine-tuning ALBERT on NLI and PPDB datasets achieves the highest accuracy on analogy tasks in different language levels. Further experiments on the insurance FAQ task show effectiveness of universal representation models in real-world applications.",19
"   The ability to learn tasks continuously during a lifetime and with limited supervision is a hallmark of human intelligence. This is enabled by efficient transfer of knowledge from past experience. On the contrary, when current deep learning methods are subjected to learning new tasks in a sequential manner, they suffer from catastrophic forgetting , where previous information is lost due to the shift in data distribution.  Non-stationarity is inevitable in the real world where data is continuously evolving. Thus, we need to design more robust machine learning mechanisms to deal with catastrophic interference.   Lifelong learning, also known as continual learning , aims at developing models that can continuously learn from a stream of tasks in sequence without forgetting existing knowledge but rather building on the information acquired by previously learned tasks in order to learn new tasks . One conceptualization of this is to accelerate learning by positive transfer between tasks while minimizing interference with respect to network updates . Many approaches to continual learning employ manually-designed techniques such as regularization  or gradient alignment  to mitigate catastrophic forgetting, which have been shown effective in computer vision and reinforcement learning tasks.   A recent trend in continual learning, as well as machine learning in general, is to directly learn generalizable solutions via meta-learning . Meta-learning  aims to learn new tasks quickly using a limited number of examples by training on many related tasks. In continual learning, meta-learning has been applied with the objective of learning new tasks continually with a relatively small number of examples per task   or in a traditional continual learning setup by interleaving with several past examples from a memory component, i.e. experience replay  . While a high rate of experience replay  usually mitigates catastrophic forgetting, it comes closer to a multi-task learning than a lifelong learning setup and is computationally expensive when learning on a data stream in real-life applications.  In natural language processing , continual learning still remains relatively unexplored . Despite the success of large pre-trained language models such as BERT , they still require considerable amounts of in-domain examples for training on new tasks and are prone to catastrophic forgetting . Existing continual learning approaches to language processing tasks include purely replay-based methods , a meta-learning based method  as well as a generative replay-based method . However, these approaches suffer from several important limitations: they require task identifiers, a high rate of replay and multiple epochs of training, which deviates from a realistic lifelong learning scenario; or tend to have an expensive inference step .   In this paper, we propose a novel approach to lifelong learning on language processing tasks using meta-learning and experience replay that is sparse in time and size. We consider the realistic setting where only one pass over the training set is possible and no task identifiers are available. We extend two algorithms, namely online meta-learning   and a neuromodulatory meta-learning algorithm   to the domain of NLP and augment them with an episodic memory module for experience replay. While their original objective is to continually learn a new sequence of tasks during testing time, we enhance them for the conventional continual learning setup where evaluation is on previously seen tasks, thus directly addressing the problem of catastrophic forgetting. Furthermore, by realizing experience replay as a query set, we directly optimize to prevent forgetting. We show that combining a strong language model such as BERT along with meta-learning and sparse replay produces state-of-the-art performance on lifelong text classification and relation extraction benchmarks when compared against current methods under the same realistic setting. To the best of our knowledge, ours is the first meta-learning approach to lifelong learning of language tasks that incorporates sparse replay. Through further experiments, we demonstrate that our approach is considerably more efficient than previous work in terms of computational complexity as well as memory usage. To facilitate further research in the field, we make our code publicly available.    We showed that pre-trained transformer-based language models, meta-learning and sparse experience replay produce a synergy that improves lifelong learning on language tasks. This is an important step in moving away from manually-designed solutions into simpler, more generalizable methods to ultimately achieve human-like learning. Meta-learning could further be exploited for the combined setting of few-shot and lifelong learning. It might also be promising in  learning distinct NLP tasks in a curriculum learning fashion.         
"," Lifelong learning requires models that can continuously learn from sequential streams of data without suffering catastrophic forgetting due to shifts in data distributions. Deep learning models have thrived in the non-sequential learning paradigm; however, when used to learn a sequence of tasks, they fail to retain past knowledge and learn incrementally. We propose a novel approach to lifelong learning of language tasks based on meta-learning with sparse experience replay that directly optimizes to prevent forgetting. We show that under the realistic setting of performing a single pass on a stream of tasks and without any task identifiers, our method obtains state-of-the-art results on lifelong text classification and relation extraction. We analyze the effectiveness of our approach and further demonstrate its low computational and space complexity.",20
" Humans exhibit resilience to orthographic variation in written text .  As a result, spelling mistakes and typos are often left unnoticed.  This flexibility of ours, however, is shown to be detrimental for neural machine translation  systems, which typically are trained on curated corpora and tend to break when faced with noisy data . Achieving NMT robustness to human blunder, however, is important when translating texts of less formal origins, such as chat conversations, social media posts and web pages with comment sections.   In this work, we propose, to augment NMT system's training data with data where source sentences are corrupted with adversarial examples of different types. There have been various studies on the impact of different types and sources of noise on NMT . In this work, we focus on the noise caused by orthographic variation of words, such as unintentional misspellings and deliberate spelling alternations as well as noise due to misplaced and omitted punctuation. Thus, the closest to this study is the work on black-box adversarial training of NMT systems , where models are trained on adversarial examples that are generated without accessing the model's parameters.  Unlike the previous work, which focuses only on adversarial examples that model unintentional changes of spelling, we also model deliberate orthographic alternation, such as omission and substitution of diacritical signs. As we show in our experiments, such orthographic variation has a more substantial negative impact on MT outputs than the other types of noise and thus is more important to be accounted for.  Further, to overcome the lack of curated evaluation datasets as required by the previous work , we propose an automatic evaluation method that measures the noise invariance of MT outputs without relying on a reference translation. By measuring noise invariance of MT outputs the method also allows us to assess whether MT system translation consistency improves when facing small variations in the source text.  [h] '' Were possible, noise is marked in bold, otherwise it is indicated with `\_'.}  \toprule \# & Type                    & \multicolumn{1}{c}{Examples} \\  \midrule 1  & introduce extra letters & Balzta j濂磖a, za鍕焌 zeme.      \\ 2  & delete letters          & \_alta j濂磖a, za鍕焌 zeme.        \\ 3  & permute letters         & Batla j濂磖a, za鍕焌 zeme.       \\ 4  & confuse letters         & Balta j濂磖a, xa鍕焌 zeme.       \\ 5  & add diacritic           & Balta j濂磖a, za鍕焌 z鑶縨e.       \\ 6  & sample substitute       & Balta j濂磖a, za鍕焌 zemi.       \\ \midrule 7 & remove punctuation      & Balta j濂磖a\_ za鍕焌 zeme\_         \\ 8 & add comma               & Balta, j濂磖a, za鍕焌 zeme.     \\ \midrule 9  & latinize                & Balta jura, zala zeme.       \\ 10 & phonetic latinize       & Balta juura, zalja zeme.     \\          We have proposed a simple generative noise model for the generation of adversarial examples for training data augmentation of NMT systems.  Our results demonstrate that NMT systems that are trained using adversarial examples are more resilient to noisy input data. We show that while for the baseline NMT systems, noisy inputs cause a substantial drop in the translation quality , for the systems that are trained using adversarial examples translation quality changes comparatively little . In terms of translation robustness, systems trained on adversarial examples on average yield 50\  consistency improvement when compared to baselines trained on clean data. Methods proposed here will be useful for achieving NMT robustness to orthographic and interpunctual variation in input data. This will be especially beneficial in use cases where NMT systems are used to translate texts of informal origins, such as chat conversations, social media posts and web pages with comment sections.  
"," Neural machine translation systems typically are trained on curated corpora and break when faced with non-standard orthography or punctuation. Resilience to spelling mistakes and typos, however, is crucial as machine translation systems are used to translate texts of informal origins, such as chat conversations, social media posts and web pages. We propose a simple generative noise model to generate adversarial examples of ten different types. We use these to augment machine translation systems' training data and show that, when tested on noisy data, systems trained using adversarial examples perform almost as well as when translating clean data, while baseline systems' performance drops by 2-3 BLEU points. To measure the robustness and noise invariance of machine translation systems' outputs, we use the average translation edit rate between the translation of the original sentence and its noised variants. Using this measure, we show that systems trained on adversarial examples on average yield 50\% consistency improvements when compared to baselines trained on clean data.",21
"  Semantic role labeling , namely semantic parsing, is a shallow semantic parsing task that aims to recognize the predicate-argument structure of each predicate in a sentence, such as who did what to whom, where and when, etc. Specifically, SRL seeks to identify arguments and label their semantic roles given a predicate. SRL is an important method for obtaining semantic information that is beneficial to a wide range of natural language processing  tasks, including machine translation, question answering, and discourse relation sense classification and relation extraction.  SRL can be split into four subtasks: predicate detection, predicate disambiguation, argument identification, and argument classification.  For argument annotation, there are two formulizations .  One is based on constituents , while the other is based on dependencies. The other, proposed by the CoNLL-2008 shared task, is also called semantic dependency parsing and annotates the heads of arguments rather than phrasal arguments. Figure  shows example annotations.    In prior SRL work, considerable attention has been paid to feature engineering, which struggles to capture sufficient discriminative information compared to neural network models, which are capable of extracting features automatically. In particular, syntactic information, including syntactic tree features, has been known to be extremely beneficial to SRL since the large scale of empirical verification of~. Despite their success, their work suffered from erroneous syntactic input, leading to an unsatisfactory performance.  To alleviate the above issues,  proposed a simple but effective neural model for SRL without syntactic input. Their work suggested that neural SRL does not have to rely on syntactic features, contradicting the belief that syntax is a necessary prerequisite for SRL, which was believed as early as~. This dramatic contradiction motivated us to make a thorough exploration on syntactic contribution to SRL.  Both span and dependency are effective formal representations for semantics, though it has been unknown which form, span or dependency, would be better for the convenience and effectiveness of semantic machine learning and later applications for a long time. This topic has been roughly discussed in , who both concluded that the  dependency SRL system at then clearly outperformed the span-based  system through gold syntactic structure transformation; however, due to the different requirements of downstream task applications, span and dependency both remain focuses of research. Additionally, the two forms of SRL may benefit from each other joint rather than separated development.  We, therefore, revisit the syntax roles under a more solid empirical basis and explore the syntax roles for the two styles with syntax information in equal quality, respectively.  Recent works on syntax contributions have been limited to individual models and the ways in which syntax has been utilized. The conclusions drawn for syntax roles therefore have some limitations. In order to reduce these limitations, we explored three typical and strong baseline models and two categories of syntactic utilization methods. In addition, pre-trained language models, such as ELMo  and BERT , that build contextualized representations, continue to provide gains on NLP benchmarks, and  showed that structure of syntax information emerges in the deep models' word representation spaces. Whether neural SRL models can further benefit from explicit syntax information in addition to this implicit syntax information, however, is another issue we consider.  %This paper will focus on semantic dependency parsing and formulate SRL as one or two sequence tagging tasks with predicate-specific encoding. With the help of the proposed -order argument pruning algorithm over syntactic tree, our model obtains state-of-the-art scores on the CoNLL benchmarks for both English and Chinese.  Besides, most of SRL literature is dedicated to impressive performance gains on English, while other multiple languages receive relatively little attention. Although human languages have some basic commonalities in syntactic structure and even different levels of grammar, their differences are also very obvious. The study of syntactic roles needs to be examined in the context of multiple languages for verifying its effectiveness and applicability.  In order to quantitatively evaluate the contribution of syntax to SRL, we adopt the ratios between labeled F score for semantic dependencies  and the labeled attachment score  for syntactic dependencies, F score for syntactic constituents. This ration was first introduced by CoNLL-2008  Shared Task as an evaluation metric. Considering that various syntactic parsers contribute different syntactic inputs with varying levels of quality, the ratio provides a fairer comparison between syntactically-driven SRL systems, which our empirical study surveys.     This paper explores the role of syntax for the semantic role labeling task. We presented a systematic survey based on our recent works on SRL and a recently popular pre-trained language modeling. Through experiments on both the dependency and span formalisms, and the sequence-based, tree-based and graph-based modeling approaches, we conclude that although the effects of syntax on SRL seem like a never-ending topic of research, with the help of current unsupervised pre-trained language models, the syntax improvement provided to SRL model performance seems to be gradually reaching its upper limit. Beyond presenting approaches that lead to improved SRL performances, we performed a detailed and fair experimental comparison between span and dependency SRL formalisms to show which is more fit for machine learning. In addition, we have studied a variety of methods of syntax integration and have shown that there is unacclimation for the hard pruning in the deep learning model which is very popular in the pre-NN era.      
"," Semantic role labeling  is dedicated to recognizing the semantic predicate-argument structure of a sentence.  Previous studies in terms of traditional models have shown syntactic information can make remarkable contributions to SRL performance; however, the necessity of syntactic information was challenged by a few recent neural SRL studies that demonstrate impressive performance without syntactic backbones and suggest that syntax information becomes much less important for neural semantic role labeling, especially when paired with recent deep neural network and large-scale pre-trained language models. Despite this notion, the neural SRL field still lacks a systematic and full investigation on the relevance of syntactic information in SRL, for both dependency and both monolingual and multilingual settings.  This paper intends to quantify the importance of syntactic information for neural SRL in the deep learning framework. We introduce three typical SRL frameworks , sequence-based, tree-based, and graph-based, which are accompanied by two categories of exploiting syntactic information: syntax pruning-based and syntax feature-based. Experiments are conducted on the CoNLL-2005, 2009, and 2012 benchmarks for all languages available, and results show that neural SRL models can still benefit from syntactic information under certain conditions. Furthermore, we show the quantitative significance of syntax to neural SRL models together with a thorough empirical survey using existing models.",22
"   Building a dialogue system that can converse with people naturally and meaningfully is one of the most challenging problems towards high-level artificial intelligence, and has been drawing increasing interests from both academia and industry area. Most existing dialogue systems are either generation-based or retrieval-based. Given the dialogue context, generation-based approaches synthesize a response word by word with a conditional language model, while retrieval-based methods select a proper response from a candidate pool. In this paper, we focus on retrieval-based approaches that are superior in providing informative responses and have been widely applied in several famous commercial products such as XiaoIce from Microsoft and AliMe Assist from Alibaba.  We consider the response selection task in multi-turn dialogues, where the retrieval model ought to select a most proper response by measuring the matching degree between a multi-turn dialogue context and a number of response candidates. Earlier studies concatenate the context to a single utterance and calculate the matching score with the utterance-level representations. Later, most response selection models  perform context-response matching within the representation-matching-aggregation paradigm, where each turn of utterance is represented individually and sequential information is aggregated among a sequence of utterance-response matching features. To further improve the performance of response selection, some recent approaches consider multiple granularities  of representations for matching or propose more complicated interaction mechanisms between the context and the response.    Recently, a wide range of studies have shown that pre-trained language models , such as BERT, XLNET and RoBERTa, on the large corpus can learn universal language representations, which are helpful for various downstream natural language processing tasks and can get rid of training a new model from scratch. To adapt pre-trained models for multi-turn response selection,  and  make the first attempt to  utilize BERT to learn a matching model, where context and the candidate response are first concatenated and then fed into the PLMs for calculating the final matching score.  These pre-trained language models can well capture the interaction information among inter-utterance and intra-utterance through multiple transformer layers. Although PLM-based response selection models demonstrate superior performance due to its strong representation ability, it is still challenging to effectively learn task-related knowledge during the training process, especially when the size of training corpora is limited. Naturally, these studies typically  learn the response selection model with only the context-response matching task %learn the matching model with the single response prediction task,  and overlook many potential training signals  contained in dialogue data. %come from rich characteristics of dialogue text. Such training signals might  be  beneficial  for  context  understanding  and  produce better  features  for  response  prediction.  Besides, the response retrieved by existing dialogue systems supervised by the conventional way still faces some critical challenges, including  incoherence  and  inconsistency.     On account of the above issues, in this paper, instead of configuring complex context-response matching models, we propose learning the context-response matching model with auxiliary self-supervised tasks designed for dialogue data based on pre-trained language models . Specifically, we introduce four self-supervised tasks  including  , ,  and , and  jointly  train  the  PLM-based  response  selection  model with  these  auxiliary  tasks  in  a  multi-task  manner.  On the one hand, these auxiliary tasks help improve the capability of the response selection model to understand the dialogue context and measure the semantic relevance, consistency or coherent between the context and the response candidates. On the other hand, they can guide the matching model to effectively learn task-related knowledge with a fixed amount of train corpora and produce better features for response prediction.   We conduct experiments on two benchmark data sets for multi-turn response selection: the Ubuntu Dialog Corpus and the E-commerce Dialogue Corpus. Evaluation results show that our proposed approach is significantly better than all state-of-the-art models on both datasets. Compared with the previous state-of-the-art methods, our model achieves 2.9\% absolute improvement in terms of  for the Ubuntu dataset and 4.8\% absolute improvement for the E-commerce dataset. Furthermore, we applied our proposed self-supervised learning schema to some non-PLM-based response selection models, e.g., dual LSTM and ESIM. Experimental results indicate that our learning schema can also bring consistent and significant improvement to the performance of the existing matching models. Surprisingly, with self-supervised learning, a simple ESIM even performs better than BERT on the ubuntu dataset, demonstrating that our approach is beneficial for various matching architectures. %   In summary, our contributions are three-fold:      In this paper, we propose learning a context-response matching model with four auxiliary self-supervised tasks designed for the dialogue data. Jointly trained with these auxiliary tasks, the matching model can effectively learn task-related knowledge contained in dialogue data, achieve a better local optimum and produce better features for response selection. Experiment results on two benchmarks indicate that the proposed auxiliary self-supervised tasks bring significant improvement for multi-turn response selection in retrieval-based dialogues, and our PLM-based model achieves new state-of-the-art results on both datasets.     In the unusual situation where you want a paper to appear in the   references without citing it in the main text, use   
"," Building an intelligent dialogue system with the ability to select a proper response according to a multi-turn context is a great challenging task. Existing studies focus on building a context-response matching model with various neural architectures or PLMs and typically learning with a single response prediction task. These approaches overlook many potential training signals contained in dialogue data, which might be beneficial for context understanding and produce better features for response prediction.  Besides, the response retrieved from existing dialogue systems supervised by the conventional way still faces some critical challenges, including incoherence and inconsistency. To address these issues, in this paper, we propose learning a context-response matching model with auxiliary self-supervised tasks designed for the dialogue data based on pre-trained language models. Specifically, we introduce four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination, and jointly train the PLM-based response selection model with these auxiliary tasks in a multi-task manner.   By this means, the auxiliary tasks can guide the learning of the matching model to achieve a better local optimum and select a more proper response. Experiment results on two benchmarks indicate that the proposed auxiliary self-supervised tasks bring significant improvement for multi-turn response selection in retrieval-based dialogues, and our model achieves new state-of-the-art results on both datasets.",23
" 	 	Named Entity Recognition  is the process of identification of named entities  in natural language text. The present paper concentrates on three low resource languages : Bhojpuri, Maithili and Magahi , which belong to the Indo-Aryan language family. This work may be seen as the first attempt to develop an NER tool for Bhojpuri, Maithili and Magahi. There is no previous work on NER for these languages as far as we know. The main aim of the present paper is to start with insights from the NER systems that are developed for Indian Languages with more resources and based on that we try to develop an NER System for BMM. 	 	The NER module can be an important component  in  Natural  Language  Processing and Information Extraction systems.  It is an essential task for computational purposes like Machine Translation , developing search engines, automatic indexing, document classification  and  text  summarization, questiona answering etc., because it is not possible to build end-to-end Deep Learning systems for these languages due to the lack of data. It  will also  be helpful  in  many  cross-linguistic  applications  as  it is relevant for other Indian Languages, particularly LRLs. The present study mainly focuses on Named Entities  for BMM with machine translation as the goal. 	 	  	The concept of Named Entity was introduced in the Sixth Message of Understanding Conference . It was often seen as part of an Information Extraction system, which refers to the automatic extraction of structured information such as entities, relationships between entities and attributes describing entities from unstructured sources. The role of NER system is to locate and classify words in a text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities etc. The NEs could be identified in two conventional ways, before the recent success of machine learning and then Deep Learning based techniques: 	 	 		 	 	It is a challenging task to implement NER for Indian languages due to the absence of capitalization in their writing systems. On the other hand, these systems are phonetically organized and designed, which makes it easily possible to use phonetic features for NER for Indian languages. Preparing a gazetteer閳ユ獨 list for all nouns is impossible because there can be a vast number of unknown named entities in the world in terms of a corpus versus a language. Here, one important point to be noted is that not much work has been reported for NER for Low Resource languages due to insufficient lexical resources and also due to morphological richness. There have been efforts on major Indian languages, i.e., Hindi, Tamil, Telugu, Urdu, Punjabi, but no efforts on Low Resource Indian languages such as BMM. 	 	 	 	Bhojpuri is often considered a major `sub-language' of Hindi. It is not only a language which is spoken in various states of India but in other countries as well, viz. Nepal, Mauritius, Fiji, Surinam etc. The writing system of Bhojpuri was earlier Kaithi script but now Devanagari script is used more to write Bhojpuri. According to 2011 census~, there are 5,05,79,447 Bhojpuri speakers. 	 	Maithili belongs to the Indo-Aryan language family, while Bhojpuri and Magahi are considered `sub-languages'  of Hindi and are mainly spoken in Eastern Uttar Pradesh, Bihar and Jharkhand states of India. Maithili is included in the 22 `scheduled' languages of the Republic of India . Maithili was added in the Constitution of India in 2003 by the 92nd Constitutional Amendment Act. Maithili, a sister language of Hindi, is spoken in India, particularly in Bihar, Jharkhand, Uttar Pradesh etc. as well as in Nepal. It is the only language in the Bihari sub-family that is included in the eighth schedule of the Indian constitution. There are 1,35,83,464 Maithili speakers . It is also one of the 122 recognised languages of Nepal. In 2007, Maithili was included in the interim Constitution of Nepal and in March 2018, it received the second official language status in the Jharkhand state of India. It too was earlier considered a sub-language or a dialect. 	 	Magahi or Magadhi, also considered a major sub-language of Hindi, is chiefly spoken in some districts of Bihar, Jharkhand, and also in the Maldah district of West Bengal. Magahi was also written in the Kaithi script in earlier days, but at present it is usually written in the Devanagari script. There are 1,27,06,825 Magahi speakers . 	 	Earlier work on machine translation  has reported that proper handling of named tokens can improve the translation quality and performance. These named tokens would have been translated during source to target translation without an NER module, but with an NER module they can instead be simply transliterated. The current BMM machine translation systems for which we plan to use our NER module, is based on a transfer-based approach to machine translation. Even though the MT systems are based on a transfer approach, the NER module  can be based on machine learning or Deep Learning, not a rule-based approach. Due to this, we have annotated some corpus and developed an NER system for these three languages and have reported the lower and a higher baseline results. The former is based on CRF and the latter on a combination of Long Short Term Memory , Convolutional Neural Networ  and Conditional Randon Fields , called LSTM-CNNs-CRF. 	 	 	As there is no prior work on the NER problem for Bhojpuri, Maithili and Magahi, the contributions in this paper are as follows: 	 		 	 	 	 	  	Bhojpuri, Maithili and Magahi are Purvanchal languages which are often considered  dialects of Hindi, even though they are widely spoken in parts of India. Bhojpuri is spoken even outside India. Partly due to their dialectal nature, they show more linguistic variations such as nominal case inflection, emphatic expressions. Like other computational resources, there is a lack of any NER system for these languages. We describe a first attempt at this. This attempt includes the creation of a dataset as well as reporting the results for two baseline systems, one that uses CRF and the other that uses an LSTM-CNNs-CRF model. These NER systems are planned to be used in machine translation system for Bhojpuri, Maithili and Magahi to Hindi. The NER dataset, prepared by native speaker linguists, consists of 228373, 157468 and 56190 tokens, out of which 12351, 19809 and 7152 are NE閳ユ獨. The tagset used is a union of ENAMEX, TIMEX and NUMEX tagsets, having a total of 22 labels. The results obtained  are 70.56\  for 61.41\  for Bhojpuri with CRF and LSTM-CNNs-CRF, respectively. The results for Maithili are 73.19\  and 71.38\  and for Magahi, they are 84.18\  and 86.39\  for the two models. Even though the total data size is more for Bhojpuri, the scores are lower as the number of NEs in the dataset of this languages is relatively much less than for the other languages. In other words, the results are consistent with the number of NEs in the datasets, rather than with the total size of the dataset in number of tokens. 	 	 	
"," 			In Natural Language Processing  pipelines, Named Entity Recognition  is one of the preliminary problems, which marks proper nouns and other named entities such as Location, Person, Organization, Disease etc. Such entities, without a NER module, adversely affect the performance of a machine translation system. NER helps in overcoming this problem by recognising and handling such entities separately, although it can be useful in Information Extraction systems also. Bhojpuri, Maithili and Magahi are low resource languages, usually known as Purvanchal languages. This paper focuses on the development of a NER benchmark dataset for the Machine Translation systems developed to translate from these languages to Hindi by annotating parts of their available corpora. Bhojpuri, Maithili and Magahi corpora of sizes 228373, 157468 and 56190 tokens, respectively, were annotated using 22 entity labels. The annotation considers coarse-grained annotation labels followed by the tagset used in one of the Hindi NER datasets. We also report a Deep Learning based baseline that uses an LSTM-CNNs-CRF model. The lower baseline F$_1$-scores from the NER tool obtained by using Conditional Random Fields models are 96.73 for Bhojpuri, 93.33 for Maithili and 95.04 for Magahi. The Deep Learning-based technique  achieved 96.25 for Bhojpuri, 93.33 for Maithili and 95.44 for Magahi.",24
" Since   propose the sequence-to-sequence  model for machine translation, the development of NLP applications has been almost inseparable from this framework. In the field of abstractive summarization, the seq2seq model is first applied by  to summarize sentences. With the recent bloom of the attention mechanism and pre-trained models, more summarization models are built as extensions of seq2seq . Albeit the rapid theoretical evolution, applications of neural abstractive summarization that are adequately mature to be industrialized have yet to exist. Inspire by Google's Neural Machine Translation  , this study makes an exploratory attempt to improve the established abstractive summarization models with a more reliable solution on the coverage problems.   In this study, a multi-document summarization is improved by the paragraph-level attention-aware inference\footnote{In this paper, the paragraph-level attention distribution is a normalized vector comprised of the total attention weights at all time steps for each source paragraph.}. In comparison to single-document summarization, multi-document summarization has a higher requirement for summary coverage as it always includes massive information from different sources. Paragraph-level attention-aware inference is theoretically applicable to all seq2seq models which adopt the attention mechanism to capture the cross-paragraph relationships.\footnote{For the single-document summarization, a sentence-level attention-aware inference is preferred.} To prove the universality of attention-aware inference empirically, in addition to using Liu's Hierarchical Transformer  , we also design a Parallel HT  as the seq2seq summarization model. Both HTs adopt the multi-head attention to represent the relationships between paragraphs, but Liu's HT integrates these representations into tokens before modelling cross-token dependencies, whilst PHT represents cross-paragraph and cross-token relationships parallelly.  Beam search is the backbone of sequence inference. However, the vanilla beam search tends to generate typical and dull sentences to avoid making mistakes  which results in neglecting salient information,   suggest a structure regularization to disallow excessive attention to the same source. The vital disadvantage of this approach is its potential of cutting the attention on actually important sources due to the lack of knowledge on the attention distribution. To fill in the gap between neural translation and summarization, we argue that the inference of the latter could be as tightly regulated as the translation inference with regards to a specific optimal attention distribution. Unlike the one-to-one alignment in NMT, which leads to the sum of attention weights of each word equaling to 1 , the optimal attention distribution of summarization is a hypothetical concept depending on the source documents. This study taps into the determination of the optimal attention distribution of input paragraphs for multi-document summarization by learning the paragraph-level attention distribution generated based on trained HT parameters and gold summary. With the predicted optimal attention distribution, the attention-aware inference refines the score function of the beam search in order to produce summaries that come along with attention closest to the optimality.    Overall, the authors believe the attention-aware inference is provided with the following three advantages.        To the best of our knowledge, this paper is the first to introduce attention-aware from NMT into NAS and we have to admit that the process is not straightforward.      In this section, we introduce  the motivation of attention-aware inference for NAS and how it is linked to the coverage penalty in NMT,  why we choose trained paragraph attention rather than extractive probabilities as the optimal label.    The idea of the attention-aware inference is inspired by Google's NMT , where candidates in the beam search are re-ranked according to a refined score function with the length normalization and coverage penalty. The penalty function is based on the assumption of one-to-one alignment in the translation so that , where  indicates the attention weight of the  generated word on the  source word. To penalize the situation that source words are not fully covered, i.e. the sum of attention weights of a source word is less or more than one, the coverage penalty is defined as:   This assumption is not tenable for summarization as uniform coverage is no longer required. Pointer-generator  re-defines the coverage loss for summarization as:  where  is the  attention weight in the word-level attention distribution ,  is the  element in the coverage vector . The definition of  is similar to that in the Eq.. In this way, repeated attention is penalized according to the overlap between each attention distribution and the coverage til time step .    further corporate this concept to their structural-coverage regularization, forcing the generation to focus on different source sentences to raise the diversity of the summary. In detail, the structural-coverage is defined as:  which is rather similar to the coverage function of Pointer-generator  except that  consider the sentence-level attention weight  .    Most attention-based decoders including ours realize summaries word by word, thus their attention mechanisms are to learn dependencies between  summary word and  source sentence/word, so it is quite clear that 's decoder is different from ours in that it generates summaries sentence by sentence and organize sentences word by word. This means it is hard for our model to use this regularization directly.  Overall, both the Pointer-generator  and structural-coverage regularization  build their models based on the principle of searching for words/sentences that have previously attracted less attention or avoiding highly regarded ones, thus to increase coverage. This searching process is less rigorous compared with the NMT's coverage penalty function, because the latter knows how much attention each word deserves, which makes the generated content more comprehensive covering source information. Lack of the optimal attention distribution for summarization forces the two summarization models  to remove the attention-aware mechanism. In contrast, we predict the optimal attention distribution for each source based on its content layout, so that NMT's coverage penalty can be used in the summarization tasks. To the best of our knowledge, this paper is the first to take the initiative to extend attention-aware mechanism from NMT to summarization.    An alternative way to obtain the optimal attention distribution is to use the paragraph ranking generated by an extractive model that predicts the probability each source sentence/paragraph appears in the final summary. However, the prediction of extractive probabilities is a separate unit from the summarization model which results in problematic inconsistency with the paragraph attention during the decoding process.     To support this argument with empirical evidence, Figure  randomly selects 10,000 training samples to compare the paragraph rankings  with the corresponding normalized paragraph attention by the trained HT decoder. In general, the decoder assigns higher attention to paragraphs with higher rankings. However, the outliers suggest that there are several cases of different judgements by the two approaches, which lead to potential conflicts during the inference given the inconsistent measures between the optimal and real attention. Therefore, we make our prediction model to learn paragraph attention from the trained decoder directly. These attentions are considered optimal as the training targets are gold summaries. The prediction model maps the connection between source documents and optimal attention distributions, to allow the inference algorithm to maximally approach the optimal attention distribution if the gold summary is unknown.   In addition to the inconsistency problem, it is easier to achieve the attention weight, because the attention mechanism is almost existing all NAS models but many NAS tasks  do not require extractive probabilities. Besides, the attention-prediction model directly extracts input vectors and labels from the summarization model, while the extractive method has to spend extra time on representing inputs and making labels.                      We use the ranked version of the Wikisum Dataset produced by  , where each sample has forty ranked paragraphs with maximum length of 100 tokens as source inputs, and a target summary with an average length of 140 tokens. The dataset is split with 1,579,360 samples for training, 38,144 for validation and 38,205 for testing. Meanwhile, Subword tokenization  is adopted to tokenize our vocabulary into 32,000 subwords to better handle unseen words.   To verify that the attention-aware inference can not only improve the matching PHT model, but also other hierarchical model, the inference is also applied to Liu's HT  by taking it's Inter-paragraph Attention as the paragraph-level attention distribution. In the following description, we will use  As the original study  summarizes single document using a hierarchical decoding algorithm to first decode sentence by sentence then realize the sentence word by word, we need to modify the regularization to adjust to the word-by-word inference. Therefore, we re-define   form the attention of the  generated sentence on the  source sentence to the  generated word on the  source paragraph. To obtain an independent observation on the effect of the structural coverage, we skip the structural-compression regularization and modifications on the loss function discussed by  .  { is the HT summarization model with the proposed attention-aware inference.    The summarization model PHT is trained on a single 2080ti with 0.3 dropout rate and an Adam optimizer of 16,000 warm-up steps. We stack 3-layer encoder-decoder of the PHT with 256 hidden units, 1024 FFN units and 4 headers, top 3000 tokens  are used to train the PHT.  The hyper-parameters of Liu's HT  are similar to the PHT, other settings including layers and token length are consistent with Liu's final model. Liu's HT truncates 1600 tokens  for training and longer inputs may harm the learning effect because it is based on the Flat Transformer which is hard to learn the long-term dependency over 2000 tokens . Both models are trained for approximately 600,000 steps. Checkpoints are saved every 20,000 steps and the best result on the validation set is used to generate the final summary. All parameters are randomly initialized including token embeddings.   For the attention prediction model, we construct a two-layer Transformer encoder with dropout rate 0.5. The complete set of the training data is used to train the attention prediction for approximately 100,000 steps. Given the nature of the prediction is regression, Mean Square Error  is used as the loss function.  In the decoding process, we take 3000 tokens as inputs to generate summaries for both HT models. Since the fixed positional encodings are used, so the attention-prediction model can accept inputs of dynamic length. We set the beam size to 5 and terminate the beam search til the length exceeds 200. In addition, we disallow the repetition of tri-grams, at the same time block two tokens  before the current step to prevent degeneration situations such as Joe is a writer and writer.       ROUGE-1 \& -2 and ROUGE-L  scores  are used to evaluate the informativeness and fluency of summaries generated by the summarization models.   We first probe the optimal value of the attention-aware coefficient           Table  shows the average ROUGE  scores of all models investigated. The attention-aware inference promotes the quality of summaries by both HTs, raising ROUGE-1 by , ROUGE-2 by , ROUGE-L by  for the Parallel HT and ROUGE-1 by , ROUGE-2 by , ROUGE-L by  for Liu's HT.  It turns out that the attention-aware inference can be applied to other hierarchical model as long as it has an attention mechanism. In section 4.2, we have analysed why not choose extractive probabilities as the optimal paragraph-level attention distribution, the experiment also indicates that when extractive probabilities serve as the optimal attention distribution, there is a marked decline in the improvement of the attention-aware inference, only increasing ROUGE-L by  and . Besides, the structural-coverage mechanism hinders the performance of HTs with reduced ROUGE scores. We print the beam-search scores of HT+strCov and find consecutive zeros as the generated sequences get longer, resulted from the  word attention on the  paragraph  remains lower than the its cumulative attention . Therefore, it is concluded that the structural coverage regularization is not particularly suitable for word-by-word summarization with lengthy inputs, such as multiple documents.            \\[0.5cm]                             Attention-aware provides an easy way to compress source paragraphs before inference. We rank paragraphs according to their predicted attention and select the first  paragraphs with the highest attention weights. Table  presents the results given different values of . According to the ROUGE-Recall and ROUGE- scores, this compression mechanism improves original summaries by limiting the number of paragraphs to [20:25].    We select the Parallel HT as the summarization model to conduct the human evaluation. We make several electronic questionnaires and ask seven assessors to score 40 randomly-selected examples  regarding two criteria,  Informativeness ,  Conciseness . Five levels ranging from very poor  to very good  are included to evaluate each of the criteria. Besides, a side-by-side preference test is provided. Each assessor is asked to choose their preference among 40 pairs of summaries generated by vanilla beam search and attention-aware. The selected method in each pair receives one point. If the assessor finds a particular pair difficult to choose, neither method scores.               According to Table , attention-aware inference enhances HT significantly in both Informativeness and Concise. Due to the strict regularization to prevent repetitions for all inference strategies, repetitions could be hardly observed in summaries. Nonetheless, attention-aware raises the concise score of HT by 0.18, indicating the optimal attention distribution contributes to redundancy reduction. The preference ratio indicates attention-aware is approximately twice as likely to be better than vanilla beam search. Two examples of summaries are provided in Table  to show that attention-aware reduces redundancy  and enhances informativeness .   In this paper, we empirically show the paragraph-level attention-aware enhances the quality of multi-document summarization. Meanwhile, the authors believe that the sentence-level attention-aware can achieve similar results on single-document summarization as they both serve for the hierarchical model and the number of attention units are typically below 100. In contrast, word-level attention-aware requires the prediction of hundreds or even thousands of attention units, bringing a large challenge in obtaining the optimal attention distribution. Nevertheless, the usage of word-level attention-aware inference is no longer confined to the hierarchical architecture and can be adopted to all attention-based seq2seq models including the pre-trained model BART . As a result, the follow-up work of the authors targets at investigating the capacity of word-level attention-aware.   This study taps into a novel inference strategy for abstractive summarization which is inspired by the coverage penalty in Google's NMT . Before studies  adjust the NMT's coverage penalty to NAS by simply neglecting attention-aware given the difficulty in obtaining the optimal attention distribution of NAS. As a solution, we design the attention-aware mechanism by predicting the optimal attention distribution according to the source content, which is proved to be able to improve the quality of generated summaries. On the other hand, the attention-prediction model, that directly extracts labels and inputs from the trained summarization model, not only approaches to the optimal attention distribution, but also assures the computational efficiency. Last but not the least, this inference strategy has the potential to accommodate all seq2seq summarization models as long as they adopt the attention mechanism which nowadays is almost a necessity in the seq2seq architecture.          
"," Inspired by Google's Neural Machine Translation   that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization  to regulate generated summaries to attend to source contents with the optimal coverage. Unlike NMT, NAS is not based on one-to-one transformation. Instead, its attention distribution for the input should be irregular and depend on the content layout of the source documents. To address this matter, we construct an attention-prediction model to learn the dependency between the optimal attention distribution and the source. By refining the vanilla beam search with the attention-aware mechanism, significant improvements on the quality of summaries could be observed. Last but not the least, the attention-aware inference has strong universality that can be easily adopted to different hierarchical summarization models to promote the models' performance. \footnote{See supplements for  the code and best checkpoints.}",25
"  Pretraining ever-larger language models  on massive plain text corpora has led to significant improvements on a wide range of NLP tasks ]{radford2018improving,devlin2018bert,liu2019roberta,raffel2019exploring}. A standard approach is to replace the pretrained model's output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions , allowing pretrained LMs to solve them without any or with only very few labeled examples .     Very recently,  introduced \gpt{}, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as language modeling problems, \gpt{} achieves near state-of-the-art results for some tasks in the SuperGLUE benchmark  given just 32 labeled examples. This is achieved through : \gpt{} is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed. While being straightforward to use, this method has two major drawbacks:   	. 	           as the context window of most LMs is limited to a few hundred tokens.   An alternative to priming is   , which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While \pet{} additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, \pet{} only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way.   In this work, we modify \pet{} to also work for tasks that require predicting more than one token. We then show that in combination with ALBERT , \pet{} and its iterative variant  both outperform \gpt{} on SuperGLUE with 32 training examples, while requiring only 0.1\% of its parameters . Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to \pet{}'s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM.     We have shown that it is possible to achieve few-shot performance similar to GPT-3 on SuperGLUE with LMs that have three orders of magnitude fewer parameters. This is achieved using \pet{}, a method that reformulates tasks as cloze questions and trains an ensemble of models for different reformulations. We have proposed a simple yet effective modification to \pet{} that enables us to use it for tasks that require predicting multiple tokens. In extensive experiments, we have identified several factors responsible for the strong performance of \pet{} combined with pretrained ALBERT: the possibility to concurrently use multiple patterns for transforming examples into cloze questions, the ability to compensate for patterns that are difficult to understand, the usage of labeled data to perform parameter updates    as opposed to using them for priming , and the underlying LM itself. To enable comparisons with our work, we make our dataset of few-shot training examples publicly available.    For future work, it would be interesting to see whether further improvements are possible by using \pet{} in a multi-task setting.       
"," When scaled to hundreds of billions of parameters, pretrained language models such as \gpt{}  achieve remarkable few-shot performance on challenging natural language understanding benchmarks. In this work, we show that performance similar to \gpt{} can be obtained with language models whose parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain some form of task description, combined with gradient-based optimization; additionally exploiting unlabeled data gives further improvements. Based on our findings, we identify several key factors required for successful natural language understanding with small language models.\footnote{Our implementation is publicly available at \url{https://github.com/timoschick/pet}.}",26
"  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Most neural machine translation systems are autoregressive, hence decoding latency grows linearly with respect to the length of the target sentence. For faster generation, several work proposed non-autoregressive models with sub-linear decoding latency given sufficient parallel computation~.   As it is challenging to precisely model the dependencies among the tokens without autoregression, many existing non-autoregressive models first generate an initial translation which is then iteratively refined to yield better output~.  While various training objectives are used to admit refinement , the generation process of these models is similar in that the refinement process happens in the  space of sentences.  Meanwhile, another line of work proposed to use  latent variables for non-autoregressive translation, such that the distribution of the target sentences can be factorized over time given the latent variables~.  Unlike the models discussed above, finding the most likely target sentence under these models requires searching over continuous latent variables. To this end,  proposed an EM-like inference procedure that optimizes over a hybrid space consisting of both continuous and discrete variables. By introducing a deterministic delta posterior, it maximizes a proxy lowerbound by alternating between matching the delta posterior to the original approximate posterior , and finding a target sentence that maximizes the proxy lowerbound .  In this work, we propose an iterative inference procedure for latent variable non-autoregressive models that purely operates in the continuous space.} Given a latent variable model, we train an inference network to estimate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. At inference time, we find the target sentence that approximately maximizes the log probability by  initializing the latent variable e.g. as the mean of the prior, and  following the gradients estimated by the inference network.  We compare the proposed approach with the EM-like inference~ on three machine translation datasets: {\wmtende}, {\wmtroen} and { at the expense of  BLEU score.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%                                                                                        We propose an efficient inference procedure for non-autoregressive machine translation that refines translations purely in the continuous space. Given a latent variable model for machine translation, we train an inference network to approximate the gradient of the marginal log probability with respect to the target sentence, using only the latent variable. This allows us to use gradient based optimization to find a target sentence at inference time that approximately maximizes the marginal log probability. As we avoid discrete search over a large vocabulary, our inference procedure is more efficient than previous inference procedures that refine in the token space.  We compare our approach with a recently proposed delta inference procedure that optimizes jointly in discrete and continuous space on three machine translation datasets: {\wmtende}, {\wmtroen} and {\iwsltdeen}. With the same underlying latent variable model, the proposed inference procedure using a learned score function has following advantages:  it is twice as fast as delta inference, and  it is able to find target sentences resulting in higher marginal probabilities and BLEU scores.  While we showed that iterative inference with a learned score function is effective for spherical Gaussian priors, more work is required to investigate if such an approach will also be successful for more sophisticated priors, such as Gaussian mixtures or normalizing flows. This will be particularly interesting, as recent study showed latent variable models with a flexible prior give high test log-likelihoods, but suffer from poor generation quality as inference is challenging~.  
"," We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation~, we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality , we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure~ that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on {\wmtende}, {\wmtroen} and {, for instance, our approach is able to decode $6.2$ times faster than the autoregressive model with minimal degradation to translation quality .",27
"  Deep learning methods have revolutionized the NLP field in the past ten years. Although LSTM networks  have been around for over two decades, the NLP community only learned how to train and use them effectively in the past ten years.  introduced a new sequence-to-sequence method, boosting the field of neural machine translation significantly . The same year  presented the attention mechanism aimed at focusing on specific words within the prefix, in order to make the most accurate prediction of the next word while mapping one sequence to another. During the same period new text representation methods were adapted, complementing the following representation methods: bag-of-words , tf-idf, and one-hot vectors with dense representations, such as the very prominent word2vec  and Glove  embeddings, which served as the go-to methods in many works .   introduced a pre-trained transformer  based on the attention mechanism without any recurrent connections. BERT provided another advancement in the field of pre-trained text representations, showing enhanced performance on various NLP tasks .   Many research directions were shaped by  pre-trained word embeddings and representations with several software toolkits available for training deep neural networks. While the Keras  toolkit was widely used for text classification  with padding, the DyNet  and PyTorch  toolkits excelled at tasks in which a dynamic computation graph of the recurrent networks was exploited to achieve better predictive performance with sentences of varying length .  An important advancement in the dense representation area occurred with the introduction of TensorFlow  Hub in 2018. According to Google.     We share the results of our experimentation with two different NLP tasks. In both cases we experimented with small proprietary datasets from domains that suffer from a serious lack of labeled data. In these experiments we used the very promising and prominent BERT method and off-the-shelf TensorFlow Hub modules with the aim of outperforming several baselines on the tasks of proper word choice and political perspective identification. We used both pre-trained off-the-shelf and fine-tuned proprietary models. We failed to outdo the earlier folklore baselines as well as an advanced LSTM-based baselines, with a straightforward and systematic way of applying BERT.  Over 30 years ago,  argued that the software development process is hard at its very essence. They could not envision an advanced programming language capable of solving the complexity of performing high-quality software development projects on time. Analogously, a more user-friendly framework for pre-trained models can't guarantee excellent predictive performance. Training high performing models is essentially difficult. It requires deep understanding of the task data processing expertise. Fine-tuning a pre-trained model might be a good starting point, but the developer will still be required to delve deeply into a model's details in order to excel at predictive performance.     Please add the following required packages to your document preamble:   \usepackage{multirow}      
"," One of the challenges in  the NLP field is training  large  classification  models, a task that is both difficult and tedious. It is even harder when GPU hardware is unavailable. The increased availability of pre-trained and off-the-shelf word embeddings, models, and modules aim at easing the process of training large models and achieving a competitive performance.   We explore the use of off-the-shelf BERT models and share the results of our experiments and compare their results to those of LSTM networks and more simple baselines. We show that the complexity and computational cost of BERT is not a guarantee for enhanced predictive performance in the classification tasks at hand.",28
" Autoregressive models are ubiquitous in natural language processing.  Due to the sequential nature of text generation, they are often the tool of choice for tackling sequence-to-sequence problems such as translation , summarization , and dialogue .  Furthermore, they form the backbone of several successful generative pre-training architectures .  Two recent trends have made autoregressive models cumbersome to deploy in real-world, natural language generation  applications.  First, state-of-the-art models have grown larger and larger, amounting to hundreds of millions and even billions of parameters .  The increase in size and depth dramatically slows down inference speed.  Second, the architecture of choice for autoregressive models seems to have shifted from the recurrent neural network   to the Transformer .  Though the Transformer's self-attention mechanism improves performance, it also increases the computational complexity of the step-by-step generation algorithms that are used at test time.  Thus, both of these trends have contributed to significantly increasing inference time costs, especially on CPUs and low-resource devices, hindering their use in production systems.  % The increasing memory and inference time costs of these enormous models make them cumbersome to deploy in real-world settings.  Inference on a CPU can already be quite slow, much less a smartphone device.  Thus, there exists a need to scale down these large autoregressive models for practical purposes.     is one popular method for model compression.  It transfers the information learned by a large, pretrained  to a smaller, untrained  .  In comparison to other methods such as weight pruning and quantization, KD allows the compressed model's architecture to significantly differ from that of the original teacher.  This feature enables models trained with KD to achieve high performance while meeting particular inference requirements .   , proposed by , is the dominant technique for autoregressive KD in the current NLG literature, especially for machine translation .  This method trains a student model using a modified dataset generated by the teacher model and the standard negative log-likelihood objective.  While SeqKD is simple and efficient, we argue that it does not take advantage of the teacher's full potential.    %This method is a two-step procedure that 1) generates full sequences using the teacher model to produce a modified dataset and 2) trains the student model on the modified dataset with standard negative log-likelihood  training.  While seqKD is conceptually simple and efficient to implement, we argue that reducing the teacher's impact to a static dataset does not take advantage of its full potential.  % Autoregressive models are often trained in a way that is different from how they are used at inference time.  During training, the true sequence is available, so the model learns to predict one-step-ahead given the ground-truth context.  However, at inference time, the model must generate the entire sequence from scratch by repeatedly using its own outputs as context for subsequent steps.  This training-inference inconsistency leads to the  problem, which may be manifested as a decrease in sequence quality as the number of generation steps increases.  The seqKD algorithm is simply NLL training with a modified dataset, so it also experiences this issue.   Training the student model with a  dataset leads to the exposure bias problem. During training, the student model learns to predict the next token given previous tokens provided by the data. However, at inference time, the student generates the entire sequence from scratch by repeatedly using its own outputs as context for subsequent steps.  This training-inference inconsistency causes a decrease in generation quality.  Alternatively, we propose that the student can leverage the teacher in a  fashion during the learning process.  % Our main contributions are the following:  We recast distillation for autoregressive models as an imitation learning problem, drawing parallels between SeqKD and behavioral cloning.   From this perspective, we design a new compression algorithm aimed at addressing exposure bias for autoregressive models called  .   We conduct several experiments in translation and summarization, demonstrating that ImitKD is especially suitable for compressing deep Transformers that achieve high performance into shallow RNNs that generate much faster at inference time.  %The key insight of ImitKD is to treat the teacher model as an oracle that corrects the student閳ユ獨 generations at every step.  Thus, the student explicitly learns how to generate during training.  Our method consistently outperforms other popular distillation algorithms, such as SeqKD.  It yields student models that beat models trained without a teacher by 1.4 to 4.8 points on the Bleu and Rouge metrics.     We devise a new compression algorithm for autoregressive models called  .  It is inspired by an imitation learning  perspective on the autoregressive distillation problem.  Our algorithm trains a student model within an IL framework by treating the teacher as an oracle, and allows the student to explore its own generation during training.  The teacher corrects the student's generation at every time step, thereby guiding the student in learning how to generate. %  %   Experimental results in translation and summarization show that ImitKD is especially suitable for compressing deep Transformer models that achieve high performance into shallow RNNs that generate up to 14 times faster at inference time.  Our method consistently outperforms other distillation algorithms , and yields student models that beat models trained without a teacher by 1.4 to 4.8 points on generation metrics such as BLEU and ROUGE. %  In this work, we developed a new knowledge distillation technique inspired by imitation learning for compressing large and cumbersome autoregressive models into smaller and faster counterparts.  We demonstrated the empirical success of our method over popular baselines on several natural language generation tasks.  We are excited about several possible avenues for future work.  One branch of ideas involves incorporating more advanced IL algorithms beyond DAgger, such as LOLS , to further improve the distillation process.  Another possibility is to design imitation-based fine-tuning analogs to the SeqInter method.  Finally, although our experiments in this paper focused on sequence-to-sequence settings, we are interested in exploring the use of ImitKD for compressing large language models aimed at transfer learning.  
"," The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.  However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings.  We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation.  The algorithm is designed to address the exposure bias problem.     On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation.  Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.\footnote{Our code can be found at \url{https://github.com/asappresearch/imitkd}.}",29
"   Extracting event temporal relations from raw text data has attracted surging attention in the NLP research community in recent years as it is a fundamental task for commonsense reasoning and natural language understanding. It facilitates various downstream applications, such as forecasting social events and tracking patients' medical history. Figure shows an example of this task where an event extractor first needs to identify events  in the input and then a relation classifier predicts all pairwise relations among them, resulting in a temporal ordering as illustrated in the figure. For example,  is \temprel{before} ;  \temprel{includes} ; the temporal ordering between  and  cannot be decided from the context, so the relation should be \temprel{vague}.        In conclusion, we propose a general framework that augments deep neural networks with distributional constraints constructed using probabilistic domain knowledge. We apply it in the setting of end-to-end temporal relation extraction task with event-type and relation constraints and show that the MAP inference with distributional constraints can significantly improve the final results.  We plan to apply the proposed framework on various event reasoning tasks and construct novel distributional constraints that could leverage domain knowledge beyond corpus statistics, such as the larger unlabeled data and rich information contained in knowledge bases.  
","  Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori  inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.",30
" Encoder-decoder architecture, which uses an encoder to create a representation of source sequence and a decoder to predict target sequence, have been established as state of the art approaches in neural machine translation   . Recurrent neural network based  model , convolutional neural network  model and self-attention network based  model  are representative encoder-decoder models, and most of NMT models are variants or combination of these three. NMT models based on encoder-decoder architecture are similar in  some aspects, such as stack of layers having the same structure.  Stack of layers increases the complexity of model to approximate nonlinear function. Viewing all layers as one function, every single layer captures different information from input. Looking into every single NMT model such as RNN-based model or SAN-based model, models always try to make representation of one word containing information of whole sentence in every layer. However, empirically, one layer alone cannot result in satisfactory result.   It is common to regard sentence in NMT model as a directed complete simple graph, which views words as nodes and relationships between words as edges. However, this perspective only focuses on relationship between words, while ignoring other information, such as relationship between phrases or relationship between different fragments of sentences. As a result, structure of simple graph cannot fully reflect all information.   To overcome the shortcomings of simple graph, we view sentence as a multigraph  in SAN-based model. In multigraph , multiple edges exist between two nodes. Edge connects not only nodes but also subgraphs of  which reflects relationship between different fragments of sentences more than relationship of word-pair. Encoding is also regarded as a process of generating a multigraph to approximate  infinitely. Compared with simple graph, multigraph can explain th essence of encoding more comprehensively, and explain relationship between words in a more general way.  One layer in NMT model can capture the incremental information automatically compared with its previous layer. Fusion of the previous and incremental information makes representation more rich and thus benefits translation. From the perspective of multigraph, incremental information can be described as a set of higher-order subgraphs generated by this layer. Even though the current NMT models can capture information of subgraphs of different orders, fusing them into a representation with a fixed weight makes the model difficulty to pay more attention on really salient part.   To solve this problem, we propose a graph-based SAN empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current NMT models. First of all, we generally define a full representation as the fusing result of all concerned subgraph representations. Then let the representation of one layer split into two parts, previous representation and incremental representation. The previous representation reflects full representation from previous layer, and the incremental representation reflects new information generated in this layer. Based on this, the encoding process is modified to adapt to such representation division. We split the original self-attention into three independent parts to generate incremental representation. Our method accommodates subgraphs of different orders into different parts of incremental representation, and reduces the information redundancy. To fuse the full representation, We consider three fusing strategies in terms of different weighting schemes so that let the model focus on important parts of representation.        In experiments on WMT14 English-to-German  and IWSLT14 German-to-English , results of experiments prove our model can improve performance of translation with a few parameters increasing. Our model achieves a performance outperforming the Transformer with an improvement of 1.1 BLEU points in En-De and 1.0 BLEU points in De-En.          Instead of treating MT as seq2seq learning in the current NMT, this work presents the first graph-to-sequence NMT model, Graph-Transformer. Considering that graph other than sequence is a generalized structure formalism, modeling graph information inside model may facilitate NMT model to learn important subgraph information from source. As the multigraph defined over the sentence cannot be immediately by one part of the model such as just one layer, we assign every layer of the model to learn subgraphs with different orders, respectively. As our model implementation, we revise the SAN so that it may acquire such explicit subgraph information through our introduced incremental representation. Results of experiments show that our method can effectively boost the Transformer.        
"," Neural machine translation  usually works in a seq2seq learning way by viewing either source or target sentence as a linear sequence of words, which can be regarded as a special case of graph, taking words in the sequence as nodes and relationships between words as edges. In the light of the current NMT models more or less capture graph information among the sequence in a latent way, we present a graph-to-sequence model facilitating explicit graph information capturing. In detail, we propose a graph-based SAN-based NMT model called Graph-Transformer by capturing information of subgraphs of different orders in every layers. Subgraphs are put into different groups according to their orders, and every group of subgraphs respectively reflect different levels of dependency between words. For fusing subgraph representations, we empirically explore three methods which weight different groups of subgraphs of different orders. Results of experiments on WMT14 English-German and IWSLT14 German-English show that our method can effectively boost the Transformer with an improvement of 1.1 BLEU points on WMT14 English-German dataset and 1.0 BLEU points on IWSLT14 German-English dataset.",31
" %The web has became important source of accessing knowledge and information in our daily lives. It's necessary to develop some intelligent applications that can help users access and understand the web information easily.  Spoken dialogue system  is an application that can help users complete their goals efficiently.  % Users can achieve their goals, such as booking a restaurant, by communicating with a SDS in natural language over multiple dialogue turns.  An SDS usually has a logic engine, called dialogue manager, which involves two main sub-tasks for determining how the system will respond to the users: dialogue state tracking and dialogue policy learning. The task we discuss in this paper is dialogue state tracking, which allows the system maintaining an internal representation of the state of the dialogue as the dialogue progress.   Dialogue state tracking involving a single domain has been extensively studied and achieved much progress. As a more challenging task, Multi-domain dialogue state tracking  has been introduced in and attracts much attention in the research community.  %The { domain, and the system answers a restaurant name { and u3 in Figure). Thus a correctly modeling of these dependencies can improve slot-value extraction and cross-turn inference. In this work, we build an Interactive Encoder which completely accords with the dependencies expressed in Figure to jointly model the in-turn dependencies and cross-turn dependencies.   The interactive nature of dialogues also implies that the value for a slot tends to be specified frequently either by a system or by a user. For example, the values for slots involving names, such as { are likely to be provided by the system. And the values for the slots like {  are usually provided by the user. This observation inspires our designing of the distributed copy mechanism, which allows the state generator choosing to copy words from either the historical system utterances or the historical user utterances.  The other aspect is the slot overlapping problem in MDST. Unlike single-domain DST, slot overlapping is common in MDST, and these overlapping slots share similar values. For example, both the { domain have a slot { in Figure) or values for specific slot . The user usually act as an option provider     %each  pair can obtain better features for this  pair. 2) unlike single-domain DST problem, slot overlapping exists in multi-domain DST problems and these overlapping domains share the similar values. A global feature extractor may fail to extract correct features from the dialogue history. For example, both { domain has a slot { domain and the { and { and { value at turn 1), summary from the utterance  pair at turn 4).   This paper studies the problem of state generation for multi-domain dialogues. Existing generation-based models fail to model the dialogue dependencies and ignore the slot-overlapping problem in MDST. To overcome the limitation of existing models, we present novel Parallel Interactive Networks  for more accurate and robust dialogue state generation. The design of the PIN model is inspired by the interactive nature of the dialogues and the overlapping slots in the ontology. The Interactive Encoder characterizes the cross-turn dependencies and the in-turn dependencies. The slot-overlapping problem is solved by introducing the slot-level context. Furthermore, a distributed copy mechanism is introduced to perform a selective copy from either the historical system utterances or the historical user utterances. Empirical studies on two benchmark datasets demonstrate the effectiveness of the PIN model.       File emnlp2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \usepackage{amsmath} \usepackage{amssymb} \usepackage{float} \usepackage{booktabs} \usepackage{enumerate} \usepackage{tikz}  \usetikzlibrary{calc}  \DeclareMathOperator{   \renewcommand{\UrlFont}{\ttfamily      Enter the acl Paper ID here     You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.    \title{Parallel Interactive Networks for Multi-Domain Dialogue State Generation} \author{   Junfan Chen \\   BDBC and SKLSDE\\   Beihang University, China \\   chenjf@act.buaa.edu.cn \\   \And   Richong Zhang\thanks{Corresponding author}\\   BDBC and SKLSDE\\   Beihang University, China \\   zhangrc@act.buaa.edu.cn \\   \AND   Yongyi Mao \\   School of EECS\\   University of Ottawa, Canada \\   ymao@uottawa.ca \\   \And    Jie Xu \\   School of Computing\\   University of Leeds, United Kingdom \\   j.xu@leeds.ac.uk \\ }    \date{}  \maketitle        
"," The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multi-domain dialogue state tracking  models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks  to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.  % Multi-domain dialogue state tracking  involves large-size ontology and cross-turn inference, making it a challenge in research community. Recent MDST models fail to considering the interactive dependencies and the slot overlapping in multi-domain dialogues. In this work, we propose a robust generation-based model, Parallel Interactive Networks  , to tackle with the MDST challenge. More precisely, PIN incorporates an Interactive Encoder to jointly model the cross-turn dependencies and in-turn dependencies. The slot-level context is introduced in PIN to extract more expressive features for different slots. And a distributed copy mechanism is utilized in PIN to selectively copy words from historical system utterances and history user utterances. The PIN is demonstrated to outperform existing models on bench-marking multi-domain state tracking datasets.	   %The model however ignore the dependencies between words from system-side and user-side, and the context of decoder in these models fails to incorporating local features from specific domain and slot. In this paper, we propose a parallel-interactive recurrent neural network to modeling the human-system-interaction nature of the dialogues and introduce local context modeling to enhance the state generation performance. And a special distributed-copy operation is designed in the decoder that can copy a word from either the system-side utterances or the user-side utterances, which improves the robustness of the model. The proposed model is demonstrated to outperform existing models on bench-marking multi-domain state tracking data sets.	 	 %Multi-domain dialogue state tracking involves complex dialogue context and domain transferring, making it a challenge in research community. The traditional classification-based dialogue state tracking models need predefined ontology and are unable to dealing with unknown slot-values. The recent generation-based models tackle with this issue by incorporating the copy mechanism and generating the value sequence using the sequence-to-sequence framework. However, these generation-based models are limited in their simple encoder that insufficiently considers the human-system interaction property of the dialogues, and the context of decoder in these models fails to incorporating local features from specific domain and slot. In this paper, we propose a parallel-interactive recurrent neural network to modeling the human-system-interaction nature of the dialogues and introduce local context modeling to enhance the state generation performance. And a special adversarial-copy operation is designed in the decoder that can copy a word from either the system-side utterances or the user-side utterances, which improves the robustness of the model. The proposed model is demonstrated to outperform existing models on bench-marking multi-domain state tracking data sets.",32
"  %%General subject  is the process of generating coherent natural language text from non-linguistic data. Despite community agreement on the text and speech output of these systems, there is far less consensus on what the input should be. A large number of inputs have hence been employed for  systems, including images , numeric data, and  data. Practical applications can be found in domains such as weather forecasts , feedback for car drivers , diet management . %%%specific problem subject  Presently, the generation of natural language from  %, more precisely from   data has gained substantial attention. The RDF-to-text task has hence been proposed to investigate the quality of automatically generated texts from  .  %Moreover,  has demonstrated a promising ability to support the creation of  benchmarks.  With the emergence of neural methods, end-to-end data-to-text models have been introduced to learn input-output mappings directly. These approaches rely much less  %\todo{less is a comparative, ergo less than what?}  on explicit intermediate representations compared to rule-based approaches.   Although Neural  models have been achieving very good results  %\todo{cite paper where this is shown} , English is the only language that has been widely targeted.   % \todo[inline]{why it is important to be able to generate different language text with the same model} % \todo[inline]{What is the motivation behind investigating generation for different language families?} In this work, we alleviate this language limitation by proposing a multilingual approach, named NABU. The motivation behind multilingual models lies in several directions, mainly in  transfer learning; when low-resource language pairs are trained together with high-resource languages, the translation quality improves;  zero-shot translation, where multilingual models are able to translate between language pairs from similar families that were never seen during training;  Easy deploy, a multilingual model achieving same performance on many languages in comparison to several separate language-specific models are much more desirable for companies in terms of deployment.  Our approach, NABU, is based on the fact that knowledge graphs are language-agnostic and hence can be used on the encoder side to generate multilingual text. NABU consists of an encoder-decoder architecture which incorporates structural information of RDF triples using an encoding mechanism inspired by . In contrast to recent related work, NABU relies on the use of a reification  %\todo{sure?}  strategy for modeling the graph structure of RDF input. The decoder part  %\todo{do you mean decoder?}  is based on the vanilla Transformer model along with an unsupervised tokenization model.  %which implements  and unigram language model for handling  multilinguality. %\todo{Is the statement below really necessary in here?Would make sense to add that in the details of the approach.}  %Note that NABU follows the same strategy of recent literature on multilingual  models in which a special token is used in the encoder to determine to what target language to translate.  %evaluation We evaluate NABU on the standard benchmarking WebNLG datasets in three settings: monolingual, bilingual and multilingual. For the monolingual setting, we compare NABU with state-of-the-art English approaches and also perform experiments on Russian and German. The goal of the bilingual setting is to analyze the performance of NABU for language families. To achieve this goal, we train and evaluate bilingual models using NABU on English-German and on English-Russian. In the multilingual setting, we compare NABU with a multilingual Transformer model on English, German and Russian. %%%results Our results show that NABU outperforms state-of-the-art approaches on English and achieves 66.21 BLEU. NABU also achieves consistent results across all languages on multilingual settings with 56.04 BLEU. In addition, NABU presents promising results on the bilingual models with 61.99 BLEU.  %\todo{numbers?}  Our findings suggest that NABU is able to generate multilingual text with similar quality to that generated by humans. %conclusion The main contributions of this paper can be summarized as follows:       -Transformer architecture for generating multilingual text from RDF KGs.       The version of NABU used in this paper and also all experimental data are publicly available.~\footnote{https://github.com/dice-group/NABU}.      \todo[inline]{BLEURT: Learning Robust Metrics for Text Generation] https://arxiv.org/abs/2004.04696)}   \todo[inline]{Say that we plan to investigate the generation from different graphs [Kaffee et al. : Mind the  Gap: Generation of Multilingual Wikipedia Summaries from Wikidata for ArticlePlaceholders]} We presented a multilingual RDF verbalizer which relies on graph attention  along with a reification strategy.   We carried out an extensive evaluation set for certifying the quality of our approach.  Our experiments suggest that our approach, named NABU, outperforms state-of-the-art approaches in English. Additionally, NABU presented consistent results across the languages used in our evaluation. NABU is language-agnostic, which means it can be ported easily to languages other than those considered in this paper.  Moreover, we reported the challenges for training bilingual models with languages of distinct families.  To the best of our knowledge, we are the first approach to exploit and achieve the multilinguality successfully in the RDF-to-text task. As future work, we aim to exploit other graph-based neural architecture and other reification approaches for improving NABU's performance. Additionally, we plan to investigate how to deal with the similarity of relations by combining language models and new evaluation metrics. Moreover, we plan to investigate our methodology in the context of low-resource scenarios as well as on different .  
"," The RDF-to-text task has recently gained substantial attention due to continuous growth of Linked Data. In contrast to traditional pipeline models, recent studies have focused on neural models, which are now able to convert a set of RDF triples into text in an end-to-end style with promising results. However, English is the only language widely targeted. We address this research gap by presenting NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English. NABU is based on an encoder-decoder architecture, uses an encoder inspired by Graph Attention Networks and a Transformer as decoder. Our approach relies on the fact that knowledge graphs are language-agnostic and they hence can be used to generate multilingual text. We evaluate NABU in monolingual and multilingual settings on standard benchmarking WebNLG datasets. Our results show that NABU outperforms state-of-the-art approaches on English with 66.21 BLEU, and achieves consistent results across all languages on the multilingual scenario with 56.04 BLEU. %Moreover, we trained bilingual models for analyzing the capability of NABU to model jointly distinct language families such as English-Russian.  %\todo{Which conclusion did you reach from this training?}",33
"   We are digitally surrounded by computational Language Models  that guide us while writing to reduce the user effort, suggest different options for words/sentences to enhance our style, or fix our grammatical/correctness errors accurately . Many of the keys we press while writing on a keyboard act as part of the inputs to compose new datasets for those models that shape how we communicate with others. Nevertheless, does it happen in the same way when we write code?  Succinctly, yes. According to some recent surveys found in the literature  , the Natural Language Processing  subfield related to programming language includes examples of LMs used in several tasks and contexts. For example, the authors of  used different techniques such as graph-based statistical LMs, probabilistic LMs, or Deep Learning  LMs to suggest code to programmers similarly to auto-completer features in IDEs. LMs were used to generate automated source code based on sample code inputs or pseudo-code and evaluating how this generated code performs . Another exciting application of NLP into source code languages is the automatic translation between different languages. The work reported in  explores different supervised and unsupervised approaches to migrate code between different programming languages to improve interoperability or port codebases written in obsolete or deprecated languages . Another example found is the use of Bayesian networks, attention mechanisms, and pointer networks  to fill a given code portion with missings.  There is a more general understanding of the natural languages閳 different characteristics in the NLP broad field. Since there exist many research fields related to human languages, there is a richer background on existing language characteristics. For example, there is much knowledge on aspects like the minimal representation units of a word in a specific language, the most used words of a language, or if a word is a neologism or not. Programming languages share some syntax similarities with spoken languages. However, it does not have the same restrictions in the sense of common words or neologisms , or other syntax restrictions and features such as punctuation, format, or style. Every programming language has indeed reserved words and symbols to denote different actions, resources, or syntax. However, there is an essential part of the source code that is only limited by the programmer閳ユ獨 imagination, the conventions existing, or the guides for good practices. As  claims,    [...] traditional language models limit the vocabulary to a fixed set of common words. For code, this strong assumption has been shown to have a significant negative effect on predictive performance [...]  In that paper, Karampatsis and Sutton  present how segmenting words into subword units can improve source code modeling. Similarly, other researchers  dug in representing source code vocabulary with a similar emphasis on modeling words using sub-word units and envisioning their importance when using neural networks . Nevertheless, how that word segmentation affect the accuracy or the appropriateness of the code generated or auto-completed in some modern LM using deep learning approaches? That kind of question raises the main goal for this paper: discover what kinds of associations between different modern neural network architectures and tokenization models produce the best results when creating LMs to generate and auto-complete source code.  To pursue that goal, this research aims to conduct experiments combining different deep neural network  architectures with different tokenization and pre-trained models over an existing Python dataset. Using that experimentation, we want to investigate the combinations that improve code generation and auto-completion tasks  while checking the outcomes from those tasks using metrics like accuracy and human assessment.  The rest of the paper is as follows: Section 2 presents the different approaches followed during the research, the DNNs used, the software methods and data employed. Section 3 describes results achieved during the research according to different metrics and tests, while section 4 discusses these findings and the implications of the results as appropriate. Finally, Section 5 presents some conclusions.     Considering the results obtained, one could convincingly assert that the tokenization model used profoundly affects the results when generating automated source code. Although that may be accurate, we must discuss it carefully.     First, our overall results are consistent with the existing literature . Sub-word tokenization works better in the case of modeling source code, as  stated. Every result obtained is consistent in that sense. Even more, as  envision, char tokenization probably should the best option to try by default when dealing with LMs and source code. Furthermore, according to the results achieved, models such as GPT-2 -using a tokenization model based on BPE over raw bytes- can outperform LSTM/QRNN models like those we tested to grasp better the internals of a programming language. As showcased during the results, even if GPT-2 was not the best model in terms of accuracy, it gave better code outputs than the other ones selected for the comparison.  As future work, it would be great to check if the better textual output in the case of GPT-2 is because of a) it is a much bigger and better pre-trained model , b) it is related to dataset's size or quality, c) if it is related to both causes or, d) if it is related to other issues.  Continuing with the comments about the accuracy, one may note that the textual outputs generated by AWD-LSTM char, AWD-QRNN char, and GPT-2 could be polished to be more accurate. The final training loss is higher than the validation loss for the three selected DNN architectures, which can be a sign of underfitting. We find the same issue  for BERT and RoBERTa-based models. Whether the purpose of this paper is not to produce state-of-the-art results per se, we continued the training for over five more epochs to verify it. The improvement obtained from extending the training for the best approaches was residual in general, so we decided to report the results for 1+30 epochs in AWD-LSTMs and AWD-QRNNs, and 1+10 epochs for Transformer.    Regarding the pre-training and transfer learning, every pre-trained model  got better accuracy than its non-pre-trained counterparts except in word tokenization models. It seems to be strongly related to the statements we introduced at the beginning of this paper, citing  about the source code does not have the same restrictions in the sense of common words or neologisms. In this sense, the conclusion comes into our mind rapidly: if we consider source code words in 閳ユ辅ord閳 units, they probably will not fit in the fixed set of words used in a human language like English. So, the LM閳ユ獨 knowledge acquired during the pre-training is not entirely valid when we get out of that fixed set of words that compose a language. Most words in the programming language are neologisms for the LMs pre-trained in English, and thus, it needs to incorporate them and their relationships into the learned knowledge. For the sub-word units, the LM can be less sensible to the neologisms. Potentially, it could be more robust the more divided a word is since the set of bytes or chars is more straightforward than the chunks present in richer constructions or information units.  Going deeper into this research, concerning the pre-training effect over LMs modeling source code, it could be worth researching the relationship between the pre-training in different human-spoken languages and the LM ability to work with existing source code specific programming languages.    About the tests made generating source code or filling in the blanks using the trained LMs, we think that, in general, the textual results obtained are not so good, yet they are informative of how LMs are working and how they can be improved. One of the things that can explain these results is the dataset used. In this case, we used a public dataset that other researchers can use to make results and experiments comparable and replicable. In the literature, we do not find a standard dataset for these tasks against which we can compare easily. Other papers  use custom datasets, but we find a lack in the literature of well-recognized code datasets to use. Comparing with other recent papers in the NLP field used as the basis for this research , the dataset may be relatively small to train a big LM to accomplish appropriately challenging tasks like generating source code or auto-completing it. Future work may be testing these or new approaches in bigger datasets to train big LMs focused on modeling the Python language and checking whether the results are better. Recent examples of LMs -such as GPT-3 - claim to produce accurate textual outputs even in contexts in which they were not trained. Part of the explanation given for that ability is the use of gargantuan datasets combined with Transformer and other attention-based architectures. So, those approaches can also be relevant to other contexts like ours.  Another line for future research can be using datasets focused on specific libraries or Python aspects and verify if these approaches specialize positively for those contexts the DNN models used in this paper.  Related to evaluating the code generated or filled, we observed in the literature different approaches . In the context of LMs modeling source code, many papers and software libraries devoted to translating between programming languages typically evaluate text generation using methods and metrics like BLEU , or variants like SacreBLEU . Other papers like  rely on the accuracy to assess an LM閳ユ獨 performance based on deep learning. Some models can even solve different tasks that are part of existing benchmarks  or are evaluated, checking their perplexity . The current tendency in large models is to evaluate them using human intervention to evaluate the output閳ユ獨 quality . We assessed the models using accuracy during our experiments and evaluated the models閳 textual outputs based on our prior human knowledge. It would be interesting for the future to plan new evaluation processes involving larger cohorts of source code experts to evaluate the models such as  do. One of the potential new assessments can be usability tests conducted with programmers. They can compare the code they would write against the code proposed by any of the DNNs presented here and the result from other common code auto-completion tools included in integrated development environments. As we outlined in the results section, relying only on metrics like accuracy should not be enough. As in our case, accuracy and the other metrics can be a good indicator of the model閳ユ獨 performance, yet we need to verify LMs behavior and quality using complementary methods like specialized metrics or human evaluation. For tasks like auto-completion or source code generation, there are no existing specialized metrics , so one of the future research lines is improving the evaluation of LMs for source code. Based on some existing ideas in broad NLP, there are many opportunities to explore in that sense. From new test suites for language models used in source code contexts  to behavioral testing  or human-centric evaluation of the models  with particular emphasis on reproducible and unbiased assessments, or combinations of automatic testing and human-centric assessments.    This paper compares how different approaches to tokenization models, deep neural network architectures, pre-trained models, and transfer learning affect the results from language models used to generate source code or auto-complete software pieces. We studied different DNN architectures like AWD-LSTM, AWD-QRNN, and Transformer to seek which kind of them work better with different tokenization models . Also, we compared the pre-training effect on the results given by LMs after training them and fine-tuning them via transfer learning to work with other languages . As a result of this work, we find that in small LMs , the tokenization using char-sized chunks works better than using any other tokenization models. In larger models like the Transformer GPT-2, the accuracy was slightly worse than the other architectures. However, GPT-2 raised better results on the source code generation tests . For source code auto-completion, we tested some transformer models like BERT and RoBERTA. While their accuracy was above any other models, they did not perform very well when performing the tasks proposed in our tests. In general, we find that pre-trained models work better, even if they were not trained initially for a programming language like Python . Finally, related to evaluating tasks like automating source code generation and source code auto-completion, we raise concerns about the literature gaps and propose some research lines to work on in the future.    We thank the IBM Quantum team and the IBM Research ETX team for the insightful discussions about this research and the support received during the development of this research.     
"," In recent years, the use of deep learning in language models gained much attention. Some research projects claim that they can generate text that can be interpreted as human-writing, enabling new possibilities in many application areas. Among the different areas related to language processing, one of the most notable in applying this type of modeling is programming languages. For years, the Machine Learning community has been researching this software engineering area, pursuing goals like applying different approaches to auto-complete, generate, fix, or evaluate code programmed by humans. Considering the increasing popularity of the Deep-Learning-enabled language models approach, we detected a lack of empirical papers that compare different deep learning architectures to create and use language models based on programming code. This paper compares different neural network architectures like AWD-LSTMs, AWD-QRNNs, and Transformer while using transfer learning and different tokenizations to see how they behave in building language models using a Python dataset for code generation and filling mask tasks. Considering the results, we discuss each approach闁炽儲鐛 different strengths and weaknesses and what gaps we find to evaluate the language models or apply them in a real programming context.",34
"    A dominant approach to text generation  is to use autoregressive models learned by maximum likelihood estimation  on supervised data. However, this approach introduces two well-known discrepancies between training and evaluation objectives that lead to undesired generations.  % First, the training loss is negative log-likelihood, whereas the evaluation is based on human judgment of the output quality.  Under model misspecification, MLE tends to over-generalize, assigning large probability mass to both high-quality and low-quality sequences . Therefore, in practice, we must carefully select the  decoding algorithms to produce high-quality outputs.  Second, during training, the autoregressive model conditions on the gold history/prefix; however, at inference time it conditions on model-generated history. This is known as the exposure bias problem . In the worst case, one incorrect prediction can produce a low-probability prefix under the gold data distribution, and errors compound in each of the following steps . In practice, prior work has observed problems such as repetition and hallucination partly due to exposure bias .   We aim to bridge the gap between training and evaluation in this paper. To match training and evaluation objectives, ideally we should maximize output quality given model-generated histories. This corresponds to the reinforcement learning  objective: maximizing the expected reward  over trajectories  induced by the policy .  However, optimizing this objective is notoriously difficult. Prior RL approaches mainly focus on fine-tuning a learned model to optimize sequence-level metrics such as BLEU~, but empirically it remains unclear if RL is beneficial to text generation . % Note that many challenges in RL arise from exploring an exponentially large space of sequences,  with sparse rewards only on those close to the reference. We thus propose to learn from only the reference sequences without interaction . Specifically, we use off-policy policy gradient with importance weighting , where training examples with higher probability under the model are weighted higher.  Further, our reward functions approximate human judgment of the output quality  by estimating how likely a human would have generated a sequence.  We call our algorithm \algoname .    Results on news summarization,  question generation,  and machine translation show that \algoname leads to better model performance than MLE and RL fine-tuning by both task metrics and human-rated quality.  Further, our analysis shows that \algoname learns high-precision models that are less sensitive to decoding algorithms. In addition, it alleviates exposure bias: the output quality does not degrade much as generation length increases.      We provide an efficient algorithm that addresses the two train/test discrepancies in MLE training for text generation: likelihood as learning objective vs. quality as evaluation metric; gold history in training vs. model-generated history in inference. We have demonstrated that off-policy RL is a promising framework for text generation, with matched train/test objectives and optimization advantages like MLE. We believe more advanced off-policy learning techniques  can be easily integrated into text generation and further improve performance.   
","     Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation.     This paradigm leads to       diverse but low-quality samples due to mismatched learning objective and evaluation metric      and  exposure bias due to mismatched history distributions .      To alleviate these problems, we frame text generation as an  reinforcement learning  problem with expert demonstrations ,     where the goal is to maximize quality given model-generated histories.      We propose \algoname :     an easy-to-optimize algorithm that learns from the demonstrations by importance weighting.      Intuitively, \algoname upweights confident tokens and downweights unconfident ones in the reference during training,      avoiding optimization issues faced by prior RL approaches that rely on online data collection.     According to both automatic and human evaluation,     models trained by \algoname outperform those trained by MLE and policy gradient      on summarization, question generation, and machine translation.      Further, our models are less sensitive to decoding algorithms     and alleviate exposure bias.",35
"  \let\thefootnote\relax\footnote{  Corresponding author.}  Recent years have witnessed significant improvements in vision and language communities, which have consequently led to substantial attention in vision-language multi-modality tasks such as visual grounding , image captioning , and visual question answering . Furthermore, as video becomes ubiquitous, as a daily source of information and communication, video-language tasks such as video captioning , video moment retrieval , and video question answering   are emerging as important topics. Among these topics, video QA is especially challenging, as it requires fine-grained understanding of both video and language.        In this paper, we propose novel training schemes  specialize in multiple-choice video QA. We first pre-train our model with a transformed problem format  for a better weight initialization.  then train  model with the original QA problem format while being guided by contrastive representation learning. Our model achieves state-of-the-art performance on three highly challenging video QA datasets. We expect that our proposed method can be applied for various multiple-choice video QA tasks, bringing further performance improvement.  
"," Video Question Answering  requires fine-grained understanding of both video and language modalities to answer the given questions. In this paper, we propose novel training schemes for multiple-choice video question answering with a self-supervised pre-training stage and a supervised contrastive learning in the main stage as an auxiliary learning. In the self-supervised pre-training stage, we transform the original problem format of predicting the correct answer into the one that predicts the relevant question to provide a model with broader contextual inputs without any further dataset or annotation. For contrastive learning in the main stage, we add a masking noise to the input corresponding to the ground-truth answer, and consider the original input of the ground-truth answer as a positive sample, while treating the rest as negative samples. By mapping the positive sample closer to the masked input, we show that the model performance is improved. We further employ locally aligned attention to focus more effectively on the video frames that are particularly relevant to the given corresponding subtitle sentences. We evaluate our proposed model on highly competitive benchmark datasets related to multiple-choice video QA: TVQA, TVQA+, and DramaQA. Experimental results show that our model achieves state-of-the-art performance on all datasets. We also validate our approaches through further analyses.",36
" With increasingly larger amounts of unstructured text becoming digitally available in many different fields, the need for robust geographically-aware retrieval of information from large textual collections is now more urgent than ever.  Textual data is often deeply geographical and it has been shown that geographic queries make for a large part of all search queries . Toponym resolution is a class of entity linking that focuses specifically on geographical entities. Given a toponym  that has been recognized in text,\footnote{We do not consider toponym detection as part of the toponym resolution task in this paper. There is a large body of research in the natural language processing community that deals with the specific problem of named entity recognition, of which toponym detection is a part.} its aim is to resolve it to its spatial footprint . This step requires an external source of knowledge which usually comes in the shape of a gazetteer, that is, a dictionary of geographical entities with their associated alternative place names and geospatial information. On the other hand, candidate selection is the task of identifying the potential entities that can be referred to by a named entity recognized in text. As the intermediary step between named entity recognition and the downstream task of entity disambiguation, candidate selection is an integral part of entity linking. And yet, it has often been an overlooked component of the entity linking pipeline, even though it has been shown to have a significant impact on the final performance , especially in noisy or non-standard text.  Toponyms are particularly prone to name variations and changes, which can arise from multiple causes, such as regional spelling differences, diachronic spelling variation, and change of the geopolitical status . In toponyms, variation is common not only at a token-level , but also at a character-level , and at both token- and character-level . In addition to these, noisy text often presents other types of character-level variations, such as spelling errors, typographical errors, and OCR errors . The number of potential variations can be very high, and yet candidate selection should ensure that the correct location is provided among the pool of retrieved entities.  In this paper, we present a new and flexible deep learning approach to geographical candidate selection through toponym matching, which is specifically tailored to dealing with these challenges characteristic of noisy scenarios. Our method consists of two main components:  toponym matching, formulated as a binary classification of toponym query-candidate pairs, and  candidate selection, formulated as a ranking task where the aim is to rank the good candidates first while minimizing the presence of noisy candidates. The main contributions of this paper are:       ).       Our method has been designed to be as language-independent as possible. It only relies upon a character tokenizer when processing the string inputs and a reference gazetteer. We have tested its downstream application on datasets from different languages, time periods and origins, from seventeenth century Latin America to nineteenth century Britain and the United States. All codes, datasets, gazetteers and evaluation settings are openly available to support research reproducibility and to foster the use of DeezyMatch in other downstream tasks.\footnote{DeezyMatch codes can be found here: \url{https://github.com/Living-with-machines/DeezyMatch/}. For a more detailed description of the DeezyMatch architecture and functionalities, see . All experiments can be found here: \url{https://github.com/Living-with-machines/LwM_SIGSPATIAL2020_ToponymMatching}. We provide all resources to allow full reproducibility of the results.}  %% SECTION    In this paper, we discussed the importance of precisely identifying candidates in order to resolve toponyms to their real-world referents. In particular, we highlighted its necessity when working with noisy and non-standard texts . To foster further research on this intermediary step, we have introduced DeezyMatch, a flexible deep learning method for candidate selection through toponym matching. It is based on the state-of-the-art neural network architectures and has been tested in different evaluation settings, considering various challenging scenarios  and in comparison with a series of well established baselines. DeezyMatch, the evaluation framework presented in this paper, and all other resources employed are useful contributions to other researchers working at the intersection of geospatial information retrieval and digital humanities.       The acknowledgments section is defined using the ""acks"" environment    . This ensures the proper    identification of the section in the article metadata, and the    consistent spelling of the heading.           The next two lines define the bibliography style to be used, and    the bibliography file. 
"," %   Recognizing toponyms and resolving them to their real-world referents is required for providing advanced semantic access to textual data. This process is often hindered by the high degree of variation in toponyms. Candidate selection is the task of identifying the potential entities that can be referred to by a toponym previously recognized. While it has traditionally received little attention in the research community, it has been shown that candidate selection has a significant impact on downstream tasks , especially in noisy or non-standard text. In this paper, we introduce a flexible deep learning method for candidate selection through toponym matching, using state-of-the-art neural network architectures. We perform an intrinsic toponym matching evaluation based on several new realistic datasets, which cover various challenging scenarios . We report its performance on candidate selection in the context of the downstream task of toponym resolution, both on existing datasets and on a new manually-annotated resource of nineteenth-century English OCR'd text. %",37
" 	% \{-0.3em} 	% General introduction 	Belief tracking  is an important component in task-oriented dialog systems. The system tracks user goals through multiple dialog turns, i.e. infers structured belief states expressed in terms of slots and values , to query an external database . 	Different belief tracking models have been proposed in recent years, either trained independently  or within end-to-end  trainable dialog systems . 	 	% problem 	Existing belief trackers mainly depend on supervised learning with human annotations of belief states for every user utterance. However, collecting these turn-level annotations is labor-intensive and time-consuming, and often requires domain knowledge to identify slots correctly. Building E2E trainable dialog systems, called E2E dialog systems for short, even further magnifies the demand for increased amounts of labeled data .  	 	 	 	% idea 	Notably, there are often easily-available unlabeled dialog data such as between customers and trained human agents accumulated in real-world customer services. 	In this paper, we are interested in reducing the reliance on belief state annotations in building E2E task-oriented dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. 	Intuitively, the dialog data, even unlabeled, can be used to enhance the performance of belief tracking and thus benefit the whole dialog system, because there are cues from user inputs and system responses which reveal the belief states, as shown in Figure . 	 	%The underlying idea is very simple: as the system makes responses based on its belief of user goals, we should be able to use the system response to infer the corresponding belief state.  	%The correlation between belief states and system responses have also been reported in previous works , which shows that learning belief tracking and response generation together is beneficial to both tasks. % mutual information  	 	% proposed model 	Technically, we propose a latent variable model for task-oriented dialogs, called the LAtent BElief State  dialog model. 	The model generally consists of multiple  turns of user inputs  and system responses  which are observations, and belief states  which are latent variables. 	Basically, \modelname{} is a conditional generative model of belief states and system responses given user inputs, i.e. . 	Once built, the model can be used to infer belief states and generate responses. 	More importantly, such latent variable modeling enables us to develop semi-supervised learning on a mix of labeled and unlabeled data under the principled variational learning framework . 	In this manner, we hope that the LABES model can exploit the cues for belief tracking from user inputs and system responses. 	Furthermore, we develop \modelname{}-S2S, which is a specific model instantiation of \modelname{}, employing copy-augmented Seq2Seq  based conditional distributions in implementing .  	 	%To leverage this correlation, we propose the LAtent BElief State dialog model , a conditional generative model that models belief states and system responses jointly given the user inputs.  	%In particular, we represent the structured belief state as discrete latent variables, e.g. a sequence of words defined on the vocabulary space.  	 	%With the recent advances of neural variational inference  , effective methods are proposed to address structured latent representation learning , discrete latent variable modeling  and sequential inference . Inspired by these works, we propose a VI-based scheme to learn the latent belief states sequentially over multiple dialog turns, which is employed under unsupervised scenarios. Thus our model can conduct semi-supervised learning from both labeled and unlabeled dialog data. 	 	We show the advantage of our model compared to other E2E task-oriented dialog models, and demonstrate the effectiveness of our semi-supervised learning scheme on three benchmark task-oriented datasets: CamRest676 , In-Car  and MultiWOZ  across various scales and domains.  	In supervised experiments, \modelname{}-S2S obtains state-of-the-art results on CamRest676 and In-Car, and outperforms all the existing models which do not leverage large pretrained language models on MultiWOZ.  	In utilizing unlabeled dialog data, semi-supervised \modelname{}-S2S significantly outperforms both supervised-only and prior semi-supervised baselines.  	Remarkably, we can reduce the annotation requirements to 50\% without performance loss on MultiWOZ, which is equivalent to saving around 30,000 annotations.  	 	 	  	In this paper we are interested in reducing belief state annotation cost for building E2E task-oriented dialog systems. We propose a conditional generative model of dialogs - \modelname{}, where belief states are modeled as latent variables, and unlabeled dialog data can be effectively leveraged to improve belief tracking through semi-supervised variational learning.  	Furthermore, we develop LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. 	We show the strong benchmark performance of \modelname{}-S2S and the effectiveness of our semi-supervised learning method on three benchmark datasets. In our experiments on MultiWOZ, we can save around 50\ , i.e. around 30,000 belief state annotations without performance loss.  	 	There are some interesting directions for future work. 	First, the \modelname{} model is general and can be enhanced by, e.g. incorporating large-scale pre-trained language models, allowing other options for the belief state decoder and the response decoder such as Transformer based. 	 Second, \modelname{} provides a principled framework to build E2E dialog systems with less reliance on annotations, which enable semi-supervised learning and, as we explain below, reinforcement learning as well.  	 as we do not fix the form of backbone model to implement the conditional probabilities in , we may simply substitute the copy-augmented Seq2Seq implementation by more sophisticated models such as GPT-2 and train via the same semi-supervised learning principles to achieve better performance.  	Second, we can analogously introduce dialog acts  as latent variables to define the joint distribution , which can be trained with semi-supervised learning and reinforcement learning as well. 	 	
"," 		%濞存粣绠戠槐閬嶆晬鐏炲墽澹岄柟璇″枟閸ㄦ粓鎯冮崚鐞籺ro闁挎稑濂旈幈銊╁绩鐟欏嫭鍠呴悷 		Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. 		In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. 		We propose a probabilistic dialog model, called the LAtent BElief State  model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. 		Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. 		Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES\footnote{Code available at https://github.com/thu-spmi/LABES}. 		In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. 		Remarkably, we can reduce the annotation demands to 50\% without performance loss on MultiWOZ.",38
"  Deep learning has achieved significant successes, but these successes heavily rely on massive annotated data. Few-Shot Learning  is one of the keys to breaking such shackle, and commits to learning new tasks with only a few examples  . FSL has made impressive progress in many areas, such as computer vision  . But the progress of FSL in natural language processing  is much slower.  One of the primary constraints is the lack of a unified benchmark for few-shot NLP, thus new methods cannot be easily compared and iteratively improved.  %Similar to computer vision,  Existing few-shot NLP researches mainly focus on simple N-classification problems, such as  text classification  and  entity relation classification .  However, on one hand, these works often report results on their own constructed few-shot data, which is pretty inefficient in results comparison and thus hinders cumulative progress. On the other hand, these simple N-classification problems cannot reflect the complexity of real-world NLP tasks.  NLP tasks often face the challenges of structure prediction problems, such as sequence labeling  and parsing . More importantly, different NLP tasks are often deeply related to each other, i.e. multi-task problems .  One typical scenario of complex NLP is the Dialogue Language Understanding problem, which includes two sub-tasks: Intent Detection  and Slot Tagging .  As a multi-task problem, these two sub-tasks are proved to strongly promote and depend on each other .  One of the main obstacles in constructing the NLP FSL benchmark comes from the special evaluation paradigm of FSL. Few-shot models are usually first pre-trained on data-rich domains  and then tested on unseen few-shot domains. %where pre-training and test tasks need to be related . Thus, FSL evaluations always need a lot of different domains to conquer the result-randomness from domain selection and limited learning shots.  But it is often hard to gather enough domains for NLP tasks.  To solve this, existing works  construct fake domains from a single dataset. They split all labels into training labels and testing labels.  Then, they construct fake pre-training and testing domains with training and testing labels respectively, so that testing labels are unseen during pre-training. Such simulation can yield plenty of related domains, but lacks reality and only works when the label set is large.  Actually, splitting labels is impractical for many real-world NLP problems.  For example of the Name Entity Recognition, the label sets are often too small to split .  In this paper, we present FewJoint, a novel FSL benchmark for joint multi-task learning, to promote FSL research of the NLP area.  To reflect the real word NLP complexities beyond simple N-classification, we adopt a sophisticated and important NLP problem for the benchmark: Task-oriented Dialogue Language Understanding.  Task-oriented Dialogue is a rising research area that develops dialogue systems to help users to achieve goals, such as booking tickets. Language Understanding is a fundamental module of Task-oriented Dialogue that extracts semantic frames from user utterances . It contains two sub-tasks: Intent Detection and Slot Tagging.  With the Slot Tagging task, our benchmark covers one of the most common structure prediction problems: sequence labeling. Besides, thanks to the natural dependency between Intent Detection and Slot Tagging, our benchmark can embody the multi-task challenge of NLP problems. %Fig  shows an example for few-shot joint language understanding.  To conquer randomness and make an adequate evaluation,  we include 59 different dialogue domains from real industrial API, which is a considerable domain amount compared to all existing few-shot and dialogue data. We also provide a Few-shot Learning platform to ease the experiment set up and comparison.   In summary, our contribution is three-fold:  We present a novel Few-shot learning benchmark with 59 real-world domains, which allows evaluating few-shot models without constructing fake domains.  We propose to reflect real-world NLP complexities by covering the structure prediction problems and multi-task learning problems.  We propose a Few-shot Learning platform to ease comparison and implement of few-shot methods.   [t] 	}; 	 	 	%	   %   In this paper, we present a novel few-shot learning benchmark for NLP tasks, which is the first few-shot NLP benchmark for joint multi-task learning.  Compared to existing few-shot learning data, our benchmark reflects real-world NLP complexities better by covering the structure prediction problem and multi-task learning problem.  Also, our benchmark consists of 59 real dialogue domains. This allows to evaluate few-shot model without constructing fake domain like existing works.   
"," Few-shot learning  is one of the key future steps in machine learning and has raised a lot of attention. However, in contrast to the rapid development in other domains, such as Computer Vision, the progress of FSL in Nature Language Processing  is much slower.  One of the key reasons for this is the lacking of public benchmarks.  NLP FSL researches always report new results on their own constructed few-shot datasets, which is pretty inefficient in results comparison and thus impedes cumulative progress. In this paper, we present FewJoint, a novel Few-Shot Learning benchmark for NLP.  Different from most NLP FSL research that only focus on simple N-classification problems, our benchmark introduces few-shot joint dialogue language understanding, which additionally covers the structure prediction and multi-task reliance problems.  This allows our benchmark to reflect the real-word NLP complexity beyond simple N-classification.  Our benchmark is used in the few-shot learning contest of SMP2020-ECDT task-1.\footnote{The Eighth China National Conference on Social Media Processing. Link: \url{https://smp2020.aconf.cn/smp.html}}  We also provide a compatible FSL platform to ease experiment set-up.\footnote{The dataset and platform is available at \url{https://github.com/AtmaHou/MetaDialog}}",39
"  Event coreference resolution aims to identify which event mentions in a document refer to the same event . For example, the two event mentions in Figure , departing and leave, refer to the same EndPosition event of Nokia's CEO.  Traditional event coreference resolution methods usually rely on a series of upstream components , such as entity recognition and event detection. Such a pipeline framework, unfortunately, often suffers from the error propagation problem. For instance, the best event detection system in KBP 2017 only achieved 56 F1 , and it will undoubtedly limit the performance of the follow-up event coreference task . Furthermore, most previous approaches use hand-crafted features , which heavily depend on other NLP components  and thus are hard to generalize to new languages/domains/datasets.    In this paper, we propose an End-to-End Event Coreference method --  neural network, which can predict event chains from a raw text in an end-to-end manner. For example, taking the raw text in Figure  as input,  will directly output two event coreference chains, \{departing, leave, goodbye\} and \{rejoin\}. By jointly modeling event detection and event coreference,  neural network does not require any prior components, and the representations/pieces of evidence between different tasks and different decisions can be shared and reinforced. Besides,  are learned in an end-to-end manner, which can inherently resolve the error propagation problem.  End-to-end event coreference, however, is challenging due to the mention diversity and the long-distance coreference. First, event mentions are highly diversified , which may be a variety of syntactic objects, including nouns, verbs, and even adjectives. For example, an EndPosition event can be triggered by departing, leave, goodbye and former. By contrast, mentions in entity coreference are mostly noun phrases . Second, coreferential event mentions commonly appear over long-distance sentences, therefore event coreference is intricately governed by long-distance, semantic-dependent decisions . For example, in Figure  the closest antecedent\footnote{In this paper, antecedents are coreferential mentions that appear earlier in the document.} of the mention goodbye -- leave, is far from it. To resolve the coreference between these two distant, diverse event mentions, a system can only rely on their semantic meanings, i.e., they both describe the same EndPosition event but from different perspectives. By contrast, most of entity mentions' closest antecedents are in the same or immediately preceding sentence , which can be resolved more easily using local and syntactic clues.  To resolve the mention diversity problem and the long-distance coreference problem, this paper further proposes a type-guided mechanism into our  neural network. This mechanism bridges distant, diverse event mentions by exploiting event type information in three folds: 1) type-informed antecedent network which enables  to capture more semantic information of event mentions by predicting coreferential scores and type scores simultaneously; 2) type-refined mention representation which enhances mention representation with type information, therefore even lexically dissimilar mentions can be bridged together, such as the two diverse EndPosition mentions goodbye and departing; 3) type-guided decoding algorithm which can exploit global type consistency for more accurate event chains.  The main contributions of this paper are:  1. We propose an end-to-end neural network for event coreference resolution 閳-  neural network.  can jointly model event detection and event coreference, and learn to automatically extract features from raw text. To the best of our knowledge, this is the first end-to-end neural event coreference model that can achieve state-of-the-art performance.  2. We design a type-guided mechanism for event coreference, which can effectively resolve the mention diversity problem and the long-distance coreference problem in event coreference resolution.  3. We conduct experiments on two standard datasets: KBP 2016 and KBP 2017, which show that  achieves new state-of-the-art performance. And additional ablation experiments verify the effectiveness of the proposed type-guided mechanism.      This paper proposes a state-of-the-art, end-to-end neural network for event coreference resolution --  neural network, which jointly models event detection and event coreference, and learns to extract features from the raw text directly. A type-guided mechanism is further proposed for resolving the mention diversity problem and the long-distance coreference problem, which: 1) informs coreference prediction with type scoring, 2) refines mention representation using type information, and 3) guides decoding under type consistency. Experiments show that our method achieves state-of-the-art performances on KBP 2016 and KBP 2017. For future work, we will focus on the bottleneck of event coreference, e.g., event detection and argument modeling.      
","   Traditional event coreference systems usually rely on pipeline framework and hand-crafted features, which often face error propagation problem and have poor generalization ability.   In this paper, we propose an End-to-End Event Coreference approach -- $\text{E}^{3}\text{C}$ neural network, which can jointly model event detection and event coreference resolution tasks, and learn to extract features from raw text automatically.   Furthermore, because event mentions are highly diversified and event coreference is intricately governed by long-distance, semantic-dependent decisions, a type-guided event coreference mechanism is further proposed in our $\text{E}^{3}\text{C}$ neural network.   Experiments show that our method achieves new state-of-the-art performance on two standard datasets.",40
" Sequence labeling assigns each token with a label in a sequence. Tasks such as Named Entity Recognition  , Part-Of-Speech  tagging  and chunking  can all be formulated as sequence labeling tasks. BiLSTM-CRF  is one of the most successful neural sequence labeling architectures. It feeds pretrained  word representations into a single layer bi-directional LSTM  encoder to extract contextual features and then feeds these features into a CRF  decoder layer to produce final predictions. The CRF layer is a linear-chain structure that models the relation between neighboring labels. In the traditional CRF approach, exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are applied for training and prediction respectively.  %The Viterbi algorithm is applied to exactly find the best label sequence in inference and the forward-backward algorithm is applied to compute posterior marginal distributions exactly for each position in training.  In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently.  In practice, we sometimes require very fast sequence labelers for training  and prediction . The BiLSTM encoder and the CRF layer both contain sequential computation and require  time over  input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure , distill larger encoders into smaller ones  or in other settings . The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks. %More recently,  proposed BiLSTM-LAN to replace the CRF layer, which has a lower time complexity, but the network introduces 3 additional LSTM layers that require sequential computations. The CRF layer is still necessary for better accuracy in many tasks, which limits the speed. %  showed such an algorithm can be unfolded as an RNN on grid-structure, we expand the work on the sequence structure and unfold the MFVI algorithm as an RNN as will  In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference  to approximately decode the linear-chain CRF. MFVI iteratively passes messages among neighboring labels to update their distributions locally. Unlike the exact probabilistic inference algorithms, MFVI can be parallelized over different positions in the sequence, achieving time complexity that is constant in  with full parallelization. %Similar to , we show that such an algorithm can be unfolded as an RNN,  Previous work  showed that such an algorithm can be unfolded as an RNN for grid CRF structure. We expand on the work for the linear-chain CRF structure and unfold the algorithm as an RNN which can be connected with the encoder to form an end-to-end neural network that is amenable to parallelization for both training and prediction. We call the unfolded RNN an approximate inference network . In addition to linear-chain CRFs, we also apply AIN to factorized second-order CRF models, which consider relations between more neighboring labels. Our empirical results show that AIN significantly improves the speed and achieves competitive accuracy against the traditional CRF approach on 4 tasks with 15 datasets.      In this paper, we propose approximate inference networks  that use Mean-Field Variational Inference  instead of exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms for training and prediction on the conditional random field for sequence labeling. The MFVI algorithm can be unfolded as a recurrent neural network and connected with the encoder to form an end-to-end neural network. AINs can be parallelized over different positions in the sequence. Empirical results show that AINs are significantly faster than traditional CRF and do very well in tasks that require more local information. Our approaches achieve competitive accuracy on 4 tasks with 15 datasets over three encoder types. Our code is publicly available at .   In future work, we plan to further improve the accuracy of our approaches through knowledge distillation that transfers the knowledge of stronger teacher models such as state-of-the-art sequence labelers with contextual embeddings to our AIN models. 
"," %with pretrained word embeddings and contextual feature extractors such as RNN or CNN  The linear-chain Conditional Random Field  model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.",41
" Neural Machine Translation  established on the encoder-decoder framework, where the encoder takes a source sentence as input and encodes it into a fixed-length embedding vector, and the decoder generates the translation sentence according to the encoder embedding, has achieved advanced translation performance in recent years . So far, despite the big advance in model architecture, most models keep taking a standard assumption to translate every sentence independently, ignoring the implicit or explicit sentence correlation from document-level contextual clues during translation.  % 1 涓轰粈涔圖ocument鍦∟MT涓噸瑕侊紝璇存槑浣犵殑璇鹃鏈夋剰涔夈 However, document-level information has shown helpful in improving the translation performance from multiple aspects: consistency, disambiguation, and coherence . If translating every sentence is completely independent of document-level context, it will be difficult to keep every sentence translations across the entire document consistent with each other. Moreover, even sentence independent translation may still benefit from document-level clues through effectively disambiguating words by referring to multiple sentence contexts. At last, document-level clues as a kind of global information across the entire text may effectively help generate more coherent translation results compared to the way only adopting local information inside a sentence alone.  % 2 宸叉湁鏂规硶濡備綍鍒╃敤Document锛屾湁浣曚紭缂虹偣銆傝繖閲屽彲浠ョ畝瑕佷粙缁嶅凡鏈夋柟娉曪紝涓嶇敤鍏紡銆% %% 銆愬彲浠ラ厡鎯呬慨鏀瑰垹鍑忋 There have been few recent attempts to introduce the document-level information into the existing standard NMT models. Various existing methods  focus on modeling the context from the surrounding text in addition to the source sentence.  For the more high-level context,  propose a multi-head hierarchical attention machine translation model to capture the word-level and sentence-level information. The cache-based model raised by  uses the dynamic cache and topic cache to capture the inter-sentence connection.  integrate their proposed Hierarchical Modeling of Global Document Context model  into the original Transformer model to improve the document-level translation.  % 3 閽堝宸叉湁鏂规硶鐨勭己鐐癸紝鏈枃鎻愬嚭涓绉峏XX鏂规硶锛岀壒鐐瑰拰浼樼偣鏄粈涔 % 鍙叧娉ㄥ叏灞鎴栬呭眬閮ㄧ殑淇℃伅锛屾病鏈夌患鍚堣冭檻 However, most of the existing document-level NMT methods focus on introducing the information of disambiguating global document or the surrounding sentences but fail to comprehend the relationship among the current sentence, the global document information, and the local document information, let alone the refined global document-level clues.  In this way, our proposed model can focus on the most relevant part of the concerned translation from which exactly encodes the related document-level context.  The empirical results indicate that our proposed method significantly improves the BLEU score compared with a strong Transformer baseline and performs better than other related models for document-level machine translation on multiple tasks.       and Future Work [TODO]}  In this paper, we explore more comprehensive document-level neural machine translation. Assuming that document-level clues are indeed helpful for better translation, it is kept an open problem for finding a good way to effectively introduce such helpful clues into sentence-independent NMT. Taking document embedding as our default representation for document-level clues, we distinguish two types of document embeddings, the global and the local, which targetedly capture both the general information in the whole document scope and the specific detailed information in the surrounding text. For the concerned document-level NMT, we for the first time survey multiple ways for generating document embeddings and conduct extensive experiments. Taking a strong Transformer baseline, our experimental results show that our global and local document embeddings may effectively enhance the baseline systems, showing that more sufficient and richer document clues indeed greatly help standard sentence-independent NMT.    In our future work, we will apply the context attention to the decoder and investigate the effect with the different memory sizes.   TBD    \clearpage     
"," Standard neural machine translation  is on the assumption of document-level context independent. Most existing document-level NMT methods are satisfied with a smattering sense of brief document-level information, while this work focuses on exploiting detailed document-level context in terms of multiple forms of document embeddings, which is capable of sufficiently modeling deeper and richer document-level context. The proposed document-aware NMT is implemented to enhance the Transformer baseline by introducing both global and local document-level clues on the source end. Experiments show that the proposed method significantly improves the translation performance over strong baselines and other related studies.",42
"  Historically, metrics for evaluating the quality of machine translation  have relied on assessing the similarity between an MT-generated hypothesis and a human-generated reference translation in the target language.  Traditional metrics have focused on basic, lexical-level features such as counting the number of matching n-grams between the MT hypothesis and the reference translation. Metrics such as {  remain popular as a means of evaluating MT systems due to their light-weight and fast computation.   Modern neural approaches to MT result in much higher quality of translation that often deviates from monotonic lexical transfer between languages.  %A single reference translation might not always be sufficient to accommodate the expressiveness of such translations.  For this reason, it has become increasingly evident that we can no longer rely on metrics such as { and fail to adequately differentiate the highest performing MT systems.  %The findings of the Metrics Shared Task highlight that segment-level evaluation and strong neural MT systems are major challenges, with none of the submitted metrics achieving satisfactory levels of correlation with human judgements .  In this paper, we present {rosslingual  Optimized Metric for Evaluation of Translation.}, a PyTorch-based framework for training highly multilingual and adaptable MT evaluation models that can function as metrics. Our framework takes advantage of recent breakthroughs in cross-lingual language modeling  to generate prediction estimates of human judgments such as Direct Assessments  , Human-mediated Translation Edit Rate   and metrics compliant with the Multidimensional Quality Metric framework .   Inspired by recent work on Quality Estimation  that demonstrated that it is possible to achieve high levels of correlation with human judgements even without a reference translation  , we propose a novel approach for incorporating the source-language input into our MT evaluation models. Traditionally only QE models have made use of the source input, whereas MT evaluation metrics rely instead on the reference translation. As in , we show that using a multilingual embedding space allows us to leverage information from all three inputs and demonstrate the value added by the source as input to our MT evaluation models.  To illustrate the effectiveness and flexibility of the { framework and the trained MT evaluation models described in this paper to the research community upon publication.     In this paper we present { will look at the impact of more compact solutions such as DistilBERT .  Additionally, whilst we outline the potential importance of the source text above, we note that our { We are grateful to Andr\'e Martins, Austin Matthews, Fabio Kepler, Daan Van Stigt, Miguel Vera, and the reviewers, for their valuable feedback and discussions. This work was supported in part by the P2020 Program through projects MAIA and Unbabel4EU, supervised by ANI under contract numbers 045909 and 042671, respectively.       [!ht]  framework to train the presented models.}  
"," We present {, Human-mediated Translation Edit Rate and Multidimensional Quality Metrics. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems. %Furthermore, they show promising results towards solving the current challenges of accurate segment-level evaluation and robustness to top performing systems.",43
"   Aspect detection, which is a vital component of aspect-based sentiment analysis , aims at identifying predefined aspect categories  discussed in segments  of online reviews. Table shows an example review about a television from several different aspects, such as Image, Sound, and Ease of Use. With a large number of reviews, automatic aspect detection allows people to efficiently retrieve review segments of aspects they are interested in. It also benefits many downstream tasks, such as review summarization  and recommendation justification .  [!t]     {!}{     {m{19em}c}     \toprule     Sentence & Aspect \\  can leverage annotated labels of aspect categories but suffer from domain adaptation problems . Another research direction consists of unsupervised approaches and has gained a lot of attention in recent years. Early unsupervised systems are dominated by Latent Dirichlet Allocation  based topic models . However, several recent studies have revealed that LDA-based approaches do not perform well for aspect detection and the extracted aspects are of poor quality  . Compared to LDA-based approaches,  deep learning models, such as aspect-based autoencoder  , have shown excellent performance in extracting coherent aspects and identifying aspect categories for review segments. However, these models require some human effort to manually map model discovered aspects to aspects of interest, which may lead to inaccuracies in mapping especially when model discovered aspects are noisy. Another research direction is based on weakly supervised approaches that leverage a small number of aspect representative words  for the fine-grained aspect detection . Although these models outperform unsupervised approaches, they do make use of human annotated data to extract high-quality aspect seed words, which may limit their application. In addition, they are not able to automatically discover new aspects from review corpus.  We focus on the problem of unsupervised aspect detection  since massive amount of reviews are generated every day and many of them are for newer products. It is difficult for humans to efficiently capture new aspects and manually annotate segments for them at scale. Motivated by ABAE, we learn interpretable aspects by mapping aspect embeddings into word embedding space, so that aspects can be interpreted by the nearest words. To learn better representations for both aspects and review segments, we formulate UAD as a self-supervised  representation learning problem and solve it using a contrastive learning algorithm, which is inspired by the  success of self-supervised contrastive learning in visual representations . In addition to the learning algorithm, we also resolve two problems that deteriorate the performance of ABAE, including its self-attention mechanism for segment representations and aspect mapping strategy . Finally, we discover that the quality of aspect detection can be further improved by knowledge distillation . The contributions of this paper are summarized as follows: [leftmargin=*,topsep=0pt,itemsep=1pt,partopsep=1pt, parsep=1pt]           % [!t] %     {!}{ %     {m{20em}c} %     \toprule %     Sentence & Aspect \\\relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21}  % DO NOT CHANGE THIS \usepackage{times}  % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier}  % DO NOT CHANGE THIS \usepackage[hyphens]{url}  % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm}  % DO NOT CHANGE THIS \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing  % DO NOT CHANGE THIS {8.5in}  % DO NOT CHANGE THIS {11in}  % DO NOT CHANGE THIS  %new added start \usepackage{booktabs} \usepackage{footnote}  \usepackage{amsmath,amssymb,mathrsfs} \usepackage[ruled,linesnumbered]{algorithm2e}  \usepackage{epstopdf} \usepackage{multirow} \usepackage[skip=0pt]{subcaption} \usepackage{soul}  \usepackage{tabularx} \renewcommand\tabularxcolumn[1]{m{#1}}  \usepackage{enumitem}  \renewcommand\vec[1]{\overrightarrow{#1}}   \usepackage{microtype} \usepackage[switch]{lineno}  %new added end    % %Leave this % /Title  % Put your actual complete title  within the parentheses in mixed case % Leave the space between \Title and the beginning parenthesis alone % /Author  % Put your actual complete list of authors  within the parentheses in mixed case. % Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, % remove them.  % DISALLOWED PACKAGES % \usepackage{authblk} -- This package is specifically forbidden % \usepackage{balance} -- This package is specifically forbidden % \usepackage{color  % \usepackage{CJK} -- This package is specifically forbidden % \usepackage{float} -- This package is specifically forbidden % \usepackage{flushend} -- This package is specifically forbidden % \usepackage{fontenc} -- This package is specifically forbidden % \usepackage{fullpage} -- This package is specifically forbidden % \usepackage{geometry} -- This package is specifically forbidden % \usepackage{grffile} -- This package is specifically forbidden % \usepackage{hyperref} -- This package is specifically forbidden % \usepackage{navigator} -- This package is specifically forbidden %  %  -- This package is specifically forbidden % \usepackage{setspace} -- This package is specifically forbidden % \usepackage{stfloats} -- This package is specifically forbidden % \usepackage{tabu} -- This package is specifically forbidden % \usepackage{titlesec} -- This package is specifically forbidden % \usepackage{tocbibind} -- This package is specifically forbidden % \usepackage{ulem} -- This package is specifically forbidden % \usepackage{wrapfig} -- This package is specifically forbidden % DISALLOWED COMMANDS %  \author{     %Authors     % All authors must be in the same font size and format.     Tian Shi\textsuperscript{\rm 1}, Liuqing Li\textsuperscript{\rm 2}, Ping Wang\textsuperscript{\rm 1}, Chandan K. Reddy\textsuperscript{\rm 1}\\ } \affiliations{     %Afiliations     \textsuperscript{\rm 1}Department of Computer Science, Virginia Tech\\     \textsuperscript{\rm 2}Verizon Media\\     tshi@vt.edu, liuqing.li@verizonmedia.com, ping@vt.edu,      reddy@cs.vt.edu     % See more examples next }  \author {     % Author     Author Name \\ }  \affiliations{     Affiliation \\     Affiliation Line 2 \\     name@example.com } \fi   \author {     % Authors          First Author Name,\textsuperscript{\rm 1}         Second Author Name, \textsuperscript{\rm 2}         Third Author Name \textsuperscript{\rm 1} \\ } \affiliations {     % Affiliations     \textsuperscript{\rm 1} Affiliation 1 \\     \textsuperscript{\rm 2} Affiliation 2 \\     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com } \fi      Unsupervised aspect detection  aims at automatically extracting interpretable aspects and identifying aspect-specific segments  from online reviews. However, recent deep learning based topic models, specifically aspect-based autoencoder, suffer from several problems such as extracting noisy aspects and poorly mapping aspects discovered by models to the aspects of interest. To tackle these challenges, in this paper, we first propose a self-supervised contrastive learning framework and an attention-based model equipped with a novel smooth self-attention  module for the UAD task in order to learn better representations for aspects and review segments. Secondly, we introduce a high-resolution selective mapping  method to efficiently assign aspects discovered by the model to the aspects of interest. We also propose using a knowledge distillation technique to further improve the aspect detection performance. Our methods outperform several recent unsupervised and weakly supervised approaches on publicly available benchmark user review datasets. Aspect interpretation results show that extracted aspects are meaningful, have a good coverage, and can be easily mapped to aspects of interest. Ablation studies and attention weight visualization also demonstrate effectiveness of SSA and the knowledge distillation method.             In this paper, we propose a self-supervised contrastive learning framework for aspect detection. Our model is equipped with two attention modules, which allows us to represent every segment with word embeddings and aspect embeddings, so that we can map aspect embeddings to the word embedding space through a contrastive learning mechanism. In the attention module over word embeddings, we introduce a SSA mechanism. Thus, our model can learn robust representations, since SSA encourages the model to capture phrases and multiple keywords in the segments. In addition, we propose a HRSMap method for aspect mapping, which dramatically increases the accuracy of segment aspect predictions for both ABAE and our model. Finally, we further improve the performance of aspect detection through knowledge distillation. BERT-based student models can benefit from pretrained encoders and overcome the disadvantages of data preprocessing for the teacher model. During training, we introduce entropy filters in the loss function to ensure student models focus on high confidence training samples. Our models have shown better performance compared to several recent unsupervised and weakly-supervised models on several publicly available review datasets across different domains. Aspect interpretation results show that extracted aspects are meaningful, have a good coverage, and can be easily mapped to gold-standard aspects. Ablation studies and visualization of attention weights further demonstrate the effectiveness of SSA and entropy filters. 
"," Unsupervised aspect detection  aims at automatically extracting interpretable aspects and identifying aspect-specific segments  from online reviews. However, recent deep learning based topic models, specifically aspect-based autoencoder, suffer from several problems such as extracting noisy aspects and poorly mapping aspects discovered by models to the aspects of interest. To tackle these challenges, in this paper, we first propose a self-supervised contrastive learning framework and an attention-based model equipped with a novel smooth self-attention  module for the UAD task in order to learn better representations for aspects and review segments. Secondly, we introduce a high-resolution selective mapping  method to efficiently assign aspects discovered by the model to the aspects of interest. We also propose using a knowledge distillation technique to further improve the aspect detection performance. Our methods outperform several recent unsupervised and weakly supervised approaches on publicly available benchmark user review datasets. Aspect interpretation results show that extracted aspects are meaningful, have a good coverage, and can be easily mapped to aspects of interest. Ablation studies and attention weight visualization also demonstrate effectiveness of SSA and the knowledge distillation method.",44
"     There are several recent studies that aim to predict the aspect ratings using deep neural network based models with multi-task learning framework . In this setting, rating predictions for different aspects, which are typically highly correlated and can share the same review encoder, are treated as different tasks. However, these models rely on hand-crafted aspect keywords to aid in rating/sentiment predictions . Thus, their results, especially case studies of reviews, are biased towards pre-defined aspect keywords. In addition, these models only focus on improving the prediction accuracy, however, knowledge discovery  from review corpus still relies on unsupervised  and rule-based methods , which limits applications of current MARP models . In the past few years, model uncertainty of deep neural network classifiers has received increasing attention , because it can identify low-confidence regions of input space and give more reliable predictions. Uncertainty models have also been applied to deep neural networks for text classification . However, few existing uncertainty methods have been used to improve the overall prediction accuracy of multi-task learning models when crowd-sourcing annotation is involved in the MARP task. In this paper, we attempt to tackle the above mentioned issues. The primary contributions of this paper are as follows:  [leftmargin=*,topsep=1pt,itemsep=1pt, partopsep=1pt, parsep=1pt]    The rest of this paper is organized as follows: In Section , we introduce related work of MARP task and uncertainty estimation methods. In Section , we present details of our proposed FEDAR model, AKR method and LEAD uncertainty estimation approach. In Section , we introduce different MARP datasets, baseline methods and implementation details, as well as analyze experimental results. Our discussion concludes in Section.%% %% This is file `sample-sigconf.tex', %% generated with the docstrip utility. %% %% The original source files were: %% %% samples.dtx   %%  %% IMPORTANT NOTICE: %%  %% For the copyright see the source file. %%  %% Any modified versions of this file must be renamed %% with new filenames distinct from sample-sigconf.tex. %%  %% For distribution of the original source see the terms %% for copying and modification in the file samples.dtx. %%  %% This generated file may be distributed as long as the %% original source files, as listed above, are part of the %% same distribution.  %% %% The first command in your LaTeX source must be the \documentclass command. \documentclass[sigconf]{acmart}  \usepackage{amsmath,amssymb,multicol,mathrsfs} \usepackage[ruled,linesnumbered]{algorithm2e} \usepackage{graphicx} \usepackage{balance}  \usepackage{epstopdf} \usepackage{multirow} \usepackage{color,soul}  \usepackage{tabularx} \renewcommand\tabularxcolumn[1]{m{#1}}  \usepackage[normalem]{ulem} \usepackage{enumitem}   \usepackage{flushend} \usepackage{tikz} \usepackage{pgf} \usepackage[eulergreek]{sansmath}  \usepackage{graphicx} \usepackage{subcaption}  \renewcommand\vec[1]{\overrightarrow{#1}}   \usepackage{url}  [1]{>{}  %%%% As of March 2017, [siggraph] is no longer used. Please use sigconf  for SIGGRAPH conferences.  %%%% As of May 2020, [sigchi] and [sigchi-a] are no longer used. Please use sigconf  for SIGCHI conferences.  %%%% Proceedings format for SIGPLAN conferences  % \documentclass[sigplan, anonymous, review]{acmart}  %%%% Proceedings format for conferences using one-column small layout % \documentclass[acmsmall,review]{acmart}  %% %% \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{%   \providecommand\BibTeX{{%             %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%  %% %% The majority of ACM publications use numbered citations and %% references.  The command  switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%  %% %% end of the preamble, start of the body of the document source.   %% %% The ""title"" command has an optional parameter, %% allowing the author to define a ""short title"" to be used in page headers. \title{Deliberate Self-Attention Network with Uncertainty Estimation for Multi-Aspect Review Rating Prediction}  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research. \author{Tian Shi} \affiliation{Virginia Tech}   \author{Ping Wang} \affiliation{Virginia Tech}   \author{Chandan K. Reddy} \affiliation{Virginia Tech}   %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose. \renewcommand{  %% %% The abstract is a short summary of the work to be presented in the %% article.   In recent years, several online platforms have seen a rapid increase in the number of review systems that request users to provide aspect-level feedback. Multi-Aspect Rating Prediction , where the goal is to predict the ratings from a review at an individual aspect level, has become a challenging and an imminent problem. To tackle this challenge, we propose a deliberate self-attention deep neural network model, named as FEDAR, for the MARP problem, which can achieve competitive performance while also being able to interpret the predictions made. As opposed to the previous studies, which make use of hand-crafted keywords to determine aspects in sentiment predictions, our model does not suffer from human bias issues since aspect keywords are automatically detected through a self-attention mechanism. FEDAR is equipped with a highway word embedding layer to transfer knowledge from pre-trained word embeddings, an RNN encoder layer with output features enriched by pooling and factorization techniques, and a deliberate self-attention layer. In addition, we also propose an Attention-driven Keywords Ranking  method, which can automatically extract aspect-level sentiment-related keywords from the review corpus based on the attention weights. Since crowdsourcing annotation can be an alternate way to recover missing ratings of reviews, we propose a LEcture-AuDience  strategy to estimate model uncertainty in the context of multi-task learning, so that valuable human resources can focus on the most uncertain predictions. Our extensive set of experiments on different DMSC datasets demonstrate the superiority of the proposed FEDAR and LEAD models. Visualization of aspect-level sentiment keywords demonstrate the interpretability of our model and effectiveness of our AKR method.   %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %%  <ccs2012> <concept> <concept_id>10002951.10003317.10003347.10003353</concept_id> <concept_desc>Information systems~Sentiment analysis</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10002951.10003317.10003347.10003356</concept_id> <concept_desc>Information systems~Clustering and classification</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10002951.10003317.10003347.10003352</concept_id> <concept_desc>Information systems~Information extraction</concept_desc> <concept_significance>300</concept_significance> </concept> </ccs2012>       %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.   %% A ""teaser"" image appears between the author and affiliation %% information and the body of the document, and typically spans the %% page. %  %    %    %   \Description{Enjoying the baseball game from the third-base %   seats. Ichiro Suzuki preparing to bat.} %    %   %% %% This command processes the author and affiliation and title %% information and builds the first part of the formatted document.         %% %% The acknowledgments section is defined using the ""acks"" environment %% . This ensures the proper %% identification of the section in the article metadata, and the %% consistent spelling of the heading. %  % To Robert, for the bagels and explaining CMYK and color spaces. %   \newpage     \endinput %% %% End of file `sample-sigconf.tex'.      In this paper, we proposed a multi-task deep learning model, namely FEDAR, for the problem of multi-aspect review rating prediction. Different from previous studies, our model does not require hand-crafted aspect-specific keywords to guide the attention and boost model performance for the task of rating prediction. Instead, our model relies on  a highway word embedding layer to transfer knowledge from pre-trained word vectors on a large corpus,  a sequential encoder layer whose output features are enriched by pooling and feature factorization techniques, and  a deliberate self-attention layer which maintains the interpretability of our model. Experiments on various MARP datasets have demonstrated the superior performance of our model. In addition, we also developed an Attention-driven Keywords Ranking  method, which can automatically extract aspect and sentiment keywords from the review corpus based on attention weights. Aspect-level sentiment word-cloud visualization results have demonstrated the interpretability of our model and effectiveness of our AKR method. Finally, we also proposed a LEcture-AuDience  method to measure the uncertainty of deep neural networks, including our FEDAR model, in the context of multi-task learning. Our experimental results on multiple real-world datasets  demonstrate the effectiveness of the proposed work. 
","  In recent years, several online platforms have seen a rapid increase in the number of review systems that request users to provide aspect-level feedback. Multi-Aspect Rating Prediction , where the goal is to predict the ratings from a review at an individual aspect level, has become a challenging and an imminent problem. To tackle this challenge, we propose a deliberate self-attention deep neural network model, named as FEDAR, for the MARP problem, which can achieve competitive performance while also being able to interpret the predictions made. As opposed to the previous studies, which make use of hand-crafted keywords to determine aspects in sentiment predictions, our model does not suffer from human bias issues since aspect keywords are automatically detected through a self-attention mechanism. FEDAR is equipped with a highway word embedding layer to transfer knowledge from pre-trained word embeddings, an RNN encoder layer with output features enriched by pooling and factorization techniques, and a deliberate self-attention layer. In addition, we also propose an Attention-driven Keywords Ranking  method, which can automatically extract aspect-level sentiment-related keywords from the review corpus based on the attention weights. Since crowdsourcing annotation can be an alternate way to recover missing ratings of reviews, we propose a LEcture-AuDience  strategy to estimate model uncertainty in the context of multi-task learning, so that valuable human resources can focus on the most uncertain predictions. Our extensive set of experiments on different DMSC datasets demonstrate the superiority of the proposed FEDAR and LEAD models. Visualization of aspect-level sentiment keywords demonstrate the interpretability of our model and effectiveness of our AKR method.",45
" Response generation for dialogue systems has stimulated great interests for researchers recently . The core idea of dialogue generation is to formulate the task as a sequence translation problem and translate the query to a response. One common neural model is the sequence-to-sequence  encoder-decoder framework. Many approaches have been proposed to improve the basic S2S model for better human-computer conversation performance.   Despite their popularity, these approaches assume that each training sample, namely, query-response pair, contributes equally to the model and ignore the consideration of different response quality contrastively. Table depicts some example responses for a particular query in a dialogue dataset. Both of the first and the second response are relevant to the query but the first one is obviously better when considering informativeness and interestingness. The third response is acceptable for the conversation but quite universal, meaning that, it can also be used to answer other queries. Thus, its quality is not as good as that of the first two responses. The fourth response is poor since it directly copies part of the query. Although the fourth response is not acceptable, it is still better when compared with the fifth response, which is completely irrelevant to the query.  [!t]     {!}{     {L{2.5cm}|L{5cm}|L{4cm}}     \Xhline{3\arrayrulewidth}        Query & Response & Quality \\ {gbsn}1. 娑撳娲﹂搹鐣屽姧鐢附娼靛鍫濐樋娑撳秳绌, 閸欘垰銇婇梼鍏呯瘍婵傝姤妫ら幆鍛櫓, 鐏忚鲸鏇熸 & \multirow{2}{*}{Relevant and interesting} \\         & Although rains are inconvenient, sunny day is no better than rainy day since I am afraid of the sun. &  \\         & {UTF8}{gbsn}2. 閺堢拋銊ュ笁娑撳娲︽径! & \multirow{2}{*}{Relevant but simple}. \\         & I hate rainy days! & \\         {UTF8}{gbsn}閸欏牆绱戞慨瀣╃瑓闂嗐劋绨 & {UTF8}{gbsn}3. 娴ｇ姷骞囬崷銊╂綒瀹娑滅箷閺勵垰绠嶅鐑囩吹 & \multirow{2}{*}{Acceptable but universal} \\         It started to rain again & Are you in Qingdao or Guangzhou now? &  \\         & {UTF8}{gbsn}4. 閸, 閸欏牅绗呴梿銊ょ啊 & \multirow{2}{*}{Quiet boring} \\         & Yes, it rains again. & \\         & {UTF8}{gbsn}5. 鏉╂瑦澧嶆径褍顒熼崪宀鍎撴竟顐︽珶閸ヮ厼绶㈡潻  & \multirow{2}{*}{Irrelevant} \\         & The university is near martyrs cemetery. & \\ %\piji{DO NOT show this kind of informaitn}     \Xhline{3\arrayrulewidth}     }                  Some initial attempts have been conducted to consider the quality of the training data. Following the idea of instance weighting,  pre-train a calibration network to calculate the response quality score for each training sample  and update the model with the weighted combination of the sample loss. Similarly,  estimate the instance score based on the corpus-level n-gram co-occurrence and the length of the response. Both of them are simple to implement but they still have some limitations:  The calibration network is only trained on relevant responses and irrelevant responses from other queries and therefore cannot capture the fine-grained response quality, as exemplified in Table;  The instance weighting strategy treats all tokens in the response as equal importance to the query by assigning them with the same quality score, which may erroneously encourage the generation of some uninformative words in the relevant responses .  To tackle the issues mentioned above, we introduce the Contrastive Learning paradigm to model the multi-level fine-grained quality of the responses with respect to the query. Specifically, we develop a Rank-aware Calibration  network aiming for modeling the fine-grained quality and characterizing the response properties  that will affect the conversation experience with a multi-scale response quality score. The rank-aware calibrator adopts the strategies of pointwise regression and pairwise ranking for gauging the quality of the query-response pair. Besides, to address the second limitation aforementioned, we design a more exquisite strategy to consider the different importance of tokens instead of simply scaling the training sample loss with the response-level quality score. Concretely, we propose to conditionally sample a response via Monte-Carlo Rollout for each gold standard response token and deem the quality scores of the sampled responses as the importance of the tokens in the sample loss estimation.  It is also observed that some meaningful words such as ``university'' and ``martyrs cemetery'' in the fifth response in Table are very likely to receive low quality scores due to the irrelevance to the query. Thus, we propose Knowledge Inference  component to explicitly encourage the generation of the informative tokens in the gold standard responses. This component firstly associates the query and the decoder hidden representation with the memories of the informative tokens and then incorporates the summarized memories into each decoding step.  In summary, our contributions are as follows:  ank-aware Calibration  network to construct the multi-level contrastive objectives. We further design a strategy to calibrate the model training with token-level quality information.\\ nowledge Inference  component. \\ \indent  We build a dataset with fine-grained response annotations and conduct extensive evaluations. The experimental results validate the effectiveness of the proposed framework.   Code and the labelled dataset will be public to facilitate the research.     We propose a multi-level contrastive learning paradigm to exploit the fine-grained response quality to calibrate the training of the response generation models. We design a Rank-aware Calibration  network to construct the contrastive optimization objectives. We further build a Knowledge Inference  component to capture the keyword knowledge from the reference during training and exploit such information to encourage the generation of informative words. We evaluate the proposed model on a carefully annotated short-text conversation dataset and the results suggest that our model can generate more relevant and diverse responses compared to the baseline models.   
"," Most of the existing works for dialogue generation are data-driven models trained directly on corpora crawled from websites. They mainly focus on improving the model architecture to produce better responses but pay little attention to considering the quality of the training data contrastively. In this paper, we propose a multi-level contrastive learning paradigm to model the fine-grained quality of the responses with respect to the query. A Rank-aware Calibration  network is designed to construct the multi-level contrastive optimization objectives. Since these objectives are calculated based on the sentence level, which may erroneously encourage/suppress the generation of uninformative/informative words. To tackle this incidental issue, on one hand, we design an exquisite token-level strategy for estimating the instance loss more accurately. On the other hand, we build a Knowledge Inference  component to capture the keyword knowledge from the reference during training and exploit such information to encourage the generation of informative words. We evaluate the proposed model on a carefully annotated dialogue dataset and the results suggest that our model can generate more relevant and diverse responses compared to the baseline models.",46
"  Knowledge Distillation  is a popular model acceleration and compression approach . It assumes that a lightweight network  can learn to generalize in the same way as a large network . To this end, a simple method is to train the student network with predicted probabilities of the teacher network as its targets.  In KD, the student network is a ``copycat'' of the teacher network because the knowledge is learned from the teacher prediction. Rather, a more straightforward way is to transfer the knowledge in parameters between two networks, as the parameters are the sources of the predictions. Such an idea has been recently found to be effective in the pre-training  fine-tuning paradigm . For example, the parameters learned on large-scale unlabeled data can be used as a good start to train a complex network on the target task. However, parameter reuse is not applicable to model acceleration and compression because the teacher and student networks might be of different width and depth\footnote{In a multi-layer neural network, the number of neurons in a hidden layer is referred to as network width, and the number of stacked layers is referred to as network depth.}.  In this paper, we propose   to transfer the parameters of the teacher network to the student network. We design a parameter generator to model the transformation from teacher network parameters to student network parameters, even if they have different sized weight matrices. After that, a fine-tuning process is performed to improve the quality of the transferred parameters. See \fig{fig:compare} for a comparison of KD and WD.  We test the WD method in a well-tuned Transformer-based machine translation system. The experiments are run on three machine translation tasks, including WMT16 English-Roman , NIST12 Chinese-English , and WMT14 English-German . With a similar speedup, the student network trained by WD is 0.51};           };            {background}             ;           ;           ;           ;           ;            {background}             ;             };            {background}             ]  {};                       %% predictions           ;           ;           ;           ;           ;            {background}             ;             ;           ;           ;           ;           ;            {background}             ;              = [rectangle,minimum width=0.3cm,inner sep=0pt]         \tikzstyle{teacher prob} = [prob,fill=ugreen!45]         \tikzstyle{student prob} = [prob,fill=orange!45]         \tikzstyle{ground truth prob} = [prob,fill=lyyblue!45]          \tikzstyle{pgnode} = [circle,fill=lyyblue,minimum size=0.2cm,inner sep=0pt]          % Teacher         };         };         $};          {background}           ]  {};                   %% predictions         ;         ;         ;         ;         ;          {background}           ;           ;         ;         ;         ;         ;          {background}           ;           ;         ]pgmid) {};         ]pgmid) {};         ]pgmid) {};          \draw[-latex,lyyblue]  to ;         \draw[-latex,lyyblue]  to ;         \draw[-latex,lyyblue]  to ;         \draw[-latex,lyyblue]  to ;          {background}           ]  {};                   % Connections         \draw[-latex',red]  .. controls + and + .. ;         \draw[-latex',red]  to ;          \draw[-latex',red]  .. controls + and + .. ;         \draw[-latex',red]  .. controls + and + .. ;         \draw[-latex',red]  .. controls + and + .. ;          \draw[-latex',red]  .. controls + and + .. ;         \draw[-latex',red]  .. controls + and + .. ;          \draw[-latex,densely dashed]  to ;         \draw[-latex,densely dashed]  to ;         \draw[-latex,densely dashed]  to ;          \draw[-latex,densely dashed]  to ;         \draw[-latex,densely dashed]  to ;         \draw[-latex,densely dashed]  to ;                 }               In this work, we propose weight distillation to transfer knowledge in parameters of the teacher network to the student network. It generates the student network from the teacher network via a parameter generator. Our experiments show that weight distillation consistently outperforms knowledge distillation by producing a faster and better student network on three machine translation tasks.   
","   Knowledge distillation has been proven to be effective in model acceleration and compression. It allows a small network to learn to generalize in the same way as a large network. Recent successes in pre-training suggest the effectiveness of transferring model parameters. Inspired by this, we investigate methods of model acceleration and compression in another line of research. We propose  to transfer the knowledge in the large network parameters through a parameter generator. Our experiments on WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight distillation can train a small network that is 1.88$\sim$2.94$\times$ faster than the large network but with competitive performance. With the same sized small network, weight distillation can outperform knowledge distillation by 0.51$\sim$1.82 BLEU points.",47
"     Aggressive language detection  which aims to automatically detect abusive, offensive language and hate speech in social media texts, as one of the important applications of Natural Language Processing , has recently received increasing research attention. Yet there are still limited efforts paid for ALD task. Current works mostly treat ALD as a regular text classification by neural networks, e.g., Long-short Term Memory  , Convolutional Neural Networks   or Transformer , with sophisticated features, e.g., pre-trained embeddings .     Nevertheless, social media texts often differ substantially from the written texts, that is, social media texts can be much noisy and contain typos , e.g., abbreviations, letter repetition, etc. Such characteristic of unnormalized texts can greatly hinder the detection of aggressive contents. Taking the examples sentence  in Fig. , the raw unnormalized expressions that carry crucial signals for indicating offensive languages, can be difficult for a detector to give correct prediction when merely seeing the surface forms. However, if these unnormalized contents are transformed into the normalized standard texts, the inferences of the detector can be much easier.          Based on the above observation, in this paper, we propose to improve the ALD task by simultaneously handling the text normalization . A multi-task learning  framework is adopted for the joint training of these two tasks. As depicted in Fig. , first, the shared encoder is expected to learn the underlying common features over two tasks, while the private encoders for ALD and TN learn the task-relevant features, respectively, based on which the decoders can make their own task predictions. To further enhance the capabilities of the shared and private feature representations, respectively, we suggest the adversarial training architecture . Technically, a task discriminator is used for distinguishing the separate learning of ALD and TN tasks.     We conduct experiments on four widely used ALD datasets, including TRAC , HSOL , KTC  and OLI , based the annotated text normalization data, Lexnorm15 .  Results show that the aggressive language detection can benefit much from the joint learning with text normalization. Our model outperforms baseline methods by a large margin, with 64.0\% and 53.6\% F1 score in TRAC-FB and TRAC-TW test sets, respectively, and average 90.5\% F1 score for other three datasets. In-depth analysis is performed for further understanding of how the TN influences the ALD task, as well as the mechanism of our proposed adversarial multi-task learning framework.              In this work, we proposed to improve the aggressive language detection  by jointly performing text normalization , via a adversarial multi-task learning framework. The private encoders for ALD and TN focused on the task-specific feature retrieving, respectively, and the shared encoder learned the underlying common features over two tasks. During adversarial training, the task discriminator distinguished the separate learning of ALD or TN. Experimental results on four ALD datasets showed that our model outperformed all baselines by large margins under differing settings, demonstrating the necessity of joint learning the TN with ALD.   
"," Aggressive language detection , detecting the abusive and offensive language in texts, is one of the crucial applications in NLP community. Most existing works treat ALD as regular classification with neural models, while ignoring the inherent conflicts of social media text that they are quite unnormalized and irregular. In this work, we target improving the ALD by jointly performing text normalization , via an adversarial multi-task learning framework. The private encoders for ALD and TN focus on the task-specific features retrieving, respectively, and the shared encoder learns the underlying common features over two tasks. During adversarial training, a task discriminator distinguishes the separate learning of ALD or TN. Experimental results on four ALD datasets show that our model outperforms all baselines under differing settings by large margins, demonstrating the necessity of joint learning the TN with ALD. Further analysis is conducted for a better understanding of our method.",48
"  Deep neural networks  have been proved vulnerable to adversarial attacks, which maliciously craft adversarial examples to fool the victim model . For instance, highly poisonous phrases with minor modification can easily deceive Google's toxic comment detection system . With the broad use of DNN-based natural language processing  systems, such as spam filtering  and malware detection , there is growing concern about their security. As a result, research into textual adversarial attacking becomes increasingly important.  %by perturbing the original input In recent years plenty of adversarial attack models have been proposed .  Nevertheless, few of them work satisfactorily in real-world attack situations. Existing adversarial attack models can be roughly classified into four categories according to the accessibility to the victim model: gradient-based, score-based, decision-based and blind models. First, gradient-based models, also known as white-box models, require full knowledge of the victim model to perform gradient computation . % attack models work in the white-box setting only , where full knowledge of the victim model is required for gradient computation. Unfortunately, we hardly know the architecture of the victim model in real-world attack situations, let alone compute the gradients.  Second, blind models do not need to know anything about the victim model, but their attack performance is usually not good enough, precisely because of complete ignorance about the victim model.  Specifically, existing blind models either implement character-level random perturbations  or conduct sentence-level distracting  and paraphrasing . However, character-level attacks are easy to repulse , and sentence-level attacks cannot guarantee attack validity, i.e, keeping the ground-truth label of the adversarial example the same as original input. More importantly, the attack success rates of most blind models are unsatisfactory. % and adversarial example quality, including grammaticality and language naturality. %  % inclined to craft invalid adversarial examples, which have different ground-truth labels from original input, or   Finally, score- and decision-based attack models seem to be more suitable for real-world adversarial attack situations. They only need to know the output of the victim models -- the former requires prediction scores and the latter just needs the final prediction decision. % which is normally practicable in real-world adversarial attacking situations % Attack models between the above two kinds of models seem to more suitable for the real-world situation of adversarial attacking, where we are usually able to invoke the victim model and obtain its output.  Existing score- and decision-based attack models have achieved great attack performance , but they have a significant problem. To craft an adversarial example, these models have to iteratively make perturbations and query the victim model too many times, e.g., a very recent score-based model needs to query the victim model more than  times on average to generate an adversarial example . % They utilize the victim model output as guidance and iteratively conduct perturbations until finding an adversarial example . % PWWS濞屸剝婀乮teratively % Although achieving good attacking performance, these models usually need to invoke the victim model too many times, e.g., the attack model in  needs to invoke the victim model more than  times on average when attack one instance. It is neither efficient nor practical to invoke the victim model so many times in real-world situations of adversarial attacking.  We argue that the low efficiency of existing score- and decision-based attack models results from that they have no learning ability and simply follow certain fixed optimization rules to attack, e.g., greedy algorithm , genetic algorithm  and particle swarm optimization .  % these model -> these score- and decision-based models? % For each instance, they start to attack from scratch. % And no lessons are learned from previous attacks. %For example,  ?  To solve this problem, we propose to build an attack model possessing learning ability, which can learn lessons from attack history and store them in its parameters so as to improve attack efficiency. % learn the weak sides of data and the victim model% data? % from history so as to launch deadly attacks efficiently. Considering no labeled data are available in adversarial attacking, we design our model following the reinforcement learning paradigm. There are two main operations in our model, including identifying key words in the original sentences that crucially influence the decision of the victim model, and selecting appropriate substitutes to replace them. Our model is aimed at learning an optimal policy under which a series of substitution operations are iteratively conducted to generate adversarial examples.  % The prober is aimed at locating where is the most vulnerable in a sentence, i.e., which word in a sentence is easiest to attack. % The attacker is supposed to find the most fatal attack, i.e., which word should replace the most vulnerable word in the original input. %Our attack model is highly adaptable and can be combined with different word substitution methods.   In experiments, we evaluate our attack model on the benchmark datasets of three typical NLP tasks including sentiment analysis, text classification and natural language inference. The victim models are respective  state-of-the-art models of the datasets, namely ALBERT , XLNet  and RoBERTa , and two open APIs. Since our model can work in both score- and decision-based attack settings, we carry out experiments in the two settings. Experimental results show that our attack model consistently outperforms the baseline methods on all the datasets in terms of both attack success rate and attack efficiency. % within whatever the limit of the number of victim model query times.  We also find our model can bring more robustness improvement to the victim model by adversarial training. % conduct quantitative analyses to exhibit the learning ability of our model.  % 閸滃矁鐦濋弴鎸庡床閺傝纭堕惃鍕波閸氬牊褝绱 % score閸滃畳ecision based閻ㄥ嫭甯归崙鐚寸吹 % 娣囶喗鏁奸悳鍥ㄦЦ閸氾箒顩︽担婊璐熸稉娑擃亪鍣哥憰浣瑰瘹閺嶅浄绱垫稉宥勭稊娑撴椽鍣哥憰浣瑰瘹閺嶅洤鎯傞敍灞惧壈娑斿绗夐弰顖滃閸掝偅妲戠涵&閹存垳婊戦惃鍕侀崹瀣躬鐠囥儲瀵氶弽鍥︾瑐濞屸剝婀侀弰搴㈡▔娴兼ê濞嶉妴    In this paper, we propose a reinforcement learning-based textual adversarial attack model aimed at real-world adversarial attack situations. It can work in both score- and decision-based attack settings and possesses learning ability so as to launch attacks more efficiently.  We also find that our model can bring more robustness improvement to the victim model by adversarial training as compared with existing baselines.  In the future, we will work towards further enhancing attack efficiency and improving attack performance in the situation of extremely limited victim model queries. In addition, we will explore how to make model more robust by adversarial training or other methods.    
"," Adversarial attacking aims to fool deep neural networks with adversarial examples. In the field of natural language processing, various textual adversarial attack models have been proposed, varying in the accessibility to the victim model. Among them, the attack models that only require the output of the victim model are more fit for real-world situations of adversarial attacking. However, to achieve high attack performance, these models usually need to query the victim model too many times, which is neither efficient nor viable in practice. To tackle this problem, we propose a reinforcement learning based attack model, which can learn from attack history and launch attacks more efficiently. In experiments, we evaluate our model by attacking several state-of-the-art models on the benchmark datasets of multiple tasks including sentiment analysis, text classification and natural language inference. Experimental results demonstrate that our model consistently achieves both better attack performance and higher efficiency than recently proposed baseline methods. We also find our attack model can bring more robustness improvement to the victim model by adversarial training. All the code and data of this paper will be made public.",49
"  In natural languages, lexical items can often be used in multiple word classes without overt changes in word form. For instance, the word buru in Mundari can be used as a noun to denote `mountain', or as a verb to denote `to heap up' . Known as word class flexibility, this phenomenon is considered one of the most challenging topics in linguistic typology . We present a computational methodology to quantify the regularity in word class flexibility across languages.   There is an extensive literature on how languages vary in word class flexibility, either directly  or through related notions such as word class conversion  . However, existing studies tend to rely on analyses of small sets of lexical items that may not be representative of word class flexibility in the broad lexicon. Critically lacking are systematic analyses of word class flexibility across many languages, and existing typological studies have only focused on qualitative comparisons of word class systems.   We take to our knowledge the first step towards computational quantification of word class flexibility in \NumLanguages languages, taken from the Universal Dependencies project . We focus on lexical items that can be used both as nouns and as verbs, i.e., noun-verb flexibility. This choice is motivated by the fact that the distinction between nouns and verbs is the most stable in word class systems across languages: if a language makes any distinction between word classes at all, it will likely be a distinction between nouns and verbs . However, our understanding of cross-linguistic regularity in noun-verb flexibility is impoverished.  We operationalize word class flexibility as a property of lemmas. We define a lemma as flexible if some of its occurrences are tagged as nouns and others as verbs. Flexible lemmas are sorted into noun dominant lemmas, which occur more frequently as nouns, and verb dominant lemmas that occur more frequently as verbs. Our methodology builds on contextualized word embedding models  to quantify semantic shift between grammatical classes of a lemma, within a single language. This methodology can also help quantify metrics of flexibility in the lexicon across  languages.  We use our methodology to address one of the most fundamental questions in the study of word class flexibility: should this phenomenon be analyzed as a directional word-formation process similar to derivation, or as a form of underspecification? Derived words are commonly argued to have a lower frequency of use and a narrower range in meaning compared to their base . If word class flexibility is a directional process, we should expect that flexible lemmas are subject to more semantic variation in their dominant word class than in their less frequent class. We also test the claim that noun-to-verb flexibility  involves  more  semantic shift  than  verb-to-noun flexibility.  While previous work has explored these questions, it remains challenging to quantify semantic shift and semantic variation, particularly across different languages.  We present a novel probing task that reveals the ability of deep contextualized models to capture semantic information across word classes. Our utilization of deep contextual models predicts human judgment on the spectrum of noun-verb flexible usages including homonymy , polysemy , and word class flexibility. We find that BERT outperforms ELMo and non-contextual word embeddings, and that the upper layers of BERT capture the most semantic information, which resonates with existing probing studies .      We use contextual language models to examine shared tendencies in word class flexibility across languages. We find that the majority class often exhibits more semantic variation than the minority class, supporting the view that word class flexibility is a directional process. We also find that in English, noun-to-verb flexibility is associated with more semantic shift than verb-to-noun flexibility, but this is not the case for most languages.  Our probing task reveals that the upper layers of BERT contextual embeddings best reflect human judgment of semantic similarity. We obtain similar results in different datasets and language models in English that support the robustness of our method. This work demonstrates the utility of deep contextualized models in linguistic typology, especially for characterizing cross-linguistic semantic phenomena that are otherwise difficult to quantify.  
"," Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes , and we apply this method to \NumLanguages languages\footnote{Code and data to reproduce the experimental findings are available at: \url{https://github.com/SPOClab-ca/word-class-flexibility}.}. We find that contextualized embeddings not only capture human judgment of  class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology.",50
"  Coreference resolution is the task of identifying mentions in a document that co-refer to the same entity. It is an important task facilitating many applications such as question answering and text summarization.   proposed the first neural end-to-end architecture for coreference resolution. Most recent state-of-the-art systems use it as a backbone while utilizing better scoring functions, pruning procedures, or pre-trained token representations. Despite this usage, to our knowledge, no in-depth analysis has been done to better understand the inner workings of such an influential system. This understanding is important: for example, 's analysis of the then-best classical coreference systems inspired many important follow-up works . However, it is unknown if observations on such classical feature-based and often highly pipelined systems extend to the current end-to-end models.  In this paper, we empirically analyze the best instantiation of this model family, SpanBERT + c2f-coref, by investigating the interaction between its two components: the mention detector and mention linker. Specifically, we study how the errors in each independently or jointly affect the final clustering.  Using the CoNLL-2012 and PreCo datasets, we highlight the low-precision, high-recall nature of the detector. While traditionally only recall is emphasized for the detector as a design decision , we show huge degradation from noisy mentions and that, perhaps surprisingly, increasing the number of candidates considered by the baseline linker only deteriorates the performance. While some classical coreference pipelines focused on detector precision, it is rarely emphasized for modern end-to-end systems. We hence stress the importance of a precision-recall balance for the detector and demonstrate how pruning hyperparameters, in addition to reducing computational complexity, help control this trade-off. However, we show the difficulty of obtaining a high-precision detector by demonstrating the importance of anaphoricity decisions and the inability of the detector to make such decisions. Finally, we highlight the high potential of the linker and that the remaining errors besides anaphoricity decisions mainly involve pronoun resolution. We hope these findings shed light on the internal mechanism of the mainstream coreference system and lay out an empirical foundation for future research.     We analyzed the complex interaction between the mention detector and linker in the mainstream coarse-to-fine coreference system. Using oracle experiments, we showed that, while detector recall is important, higher non-singleton mention precision would lead to dramatically better linker performance, though achieving this is difficult. We also demonstrated that the oracle linker performance is near perfect and that the vast majority of remaining linker errors besides anaphoricity decisions are about pronoun resolution. We hope these discoveries will help future research in coreference systems.     .}   
","  Coreference resolution is an important task for discourse-level natural language understanding. However, despite significant recent progress, the quality of current state-of-the-art systems still considerably trails behind human-level performance. Using the CoNLL-2012 and PreCo datasets, we dissect the best instantiation of the mainstream end-to-end coreference resolution model that underlies most current best-performing coreference systems, and empirically analyze the behavior of its two components: the mention detector and mention linker. While the detector traditionally focuses heavily on recall as a design decision, we demonstrate the importance of precision, calling for their balance. However, we point out the difficulty in building a precise detector due to its inability to make important anaphoricity decisions. We also highlight the enormous room for improving the linker and that the rest of its errors mainly involve pronoun resolution. We hope our findings will help future research in building coreference resolution systems.",51
"    Neural machine translation   enables end-to-end training of translation models and is known to give state-of-the-art results for a large variety of language pairs. NMT for high-resource language pairs is straightforward: choose an NMT architecture and implementation, and train a model on all existing data. In contrast, for low-resource language pairs, this does not work well due to the inability of neural networks to generalize from small amounts of data. One reason for this is the strong over-fitting potential of neural models .  There are several solutions that address this issue of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual  transfer learning , pseudo-parallel data generation  , or multi-task learning . On the other hand, model regularization techniques place constraints on the learning of model parameters in order to aid the model to learn robust representations that positively impact model performance. Among existing model regularization methods, dropout  is most commonly used and is known to be effective regardless of the size of data. We thus focus on designing a technique that can complement dropout especially in an extremely low-resource situation.  The most common way to train NMT models is to minimize a softmax cross-entropy loss, i.e., cross-entropy between the softmax distribution and the smoothed label distribution typically represented with a one-hot vector. In other words, the NMT model is trained to produce a softmax distribution that is similar to the label. In high-resource settings, this may never happen due to the diversity of label sequences.  However, in low-resource settings, due to lack of the diversity, there is a high chance of this occurring and over-fitting is said to take place. We consider that a simple manipulation of the softmax distribution may help prevent it.  This paper presents our investigation into   during training NMT models in order to address the over-fitting issue. Softmax tempering is realized by simply dividing the pre-softmax logits with a positive real number greater than 1.0.  This leads to a smoother softmax probability distribution, which is then used to compute the cross-entropy loss. Softmax tempering has been devised and used regularly in knowledge distillation , albeit for different purposes. We regard softmax tempering as a means of deliberately making the softmax distribution noisy during training with the expectation that this will have a positive impact on the final translation quality.    We primarily evaluate the utility of softmax tempering on extremely low-resource settings involving English and 11 languages in the Asian Languages Treebank  . Our experiments reveal that softmax tempering with a reasonably high temperature improves the translation quality. Furthermore, it makes the greedy search performance of the models trained with softmax tempering comparable to or better than the performance of the beam search using the models that are trained without softmax tempering, enabling faster decoding.  We then expand the scope of our study to high-resource settings, taking the WMT 2019 English-to-German translation task, as well as multilingual settings using the ALT data. We also show that softmax tempering improves the performance of NMT models using recurrently stacked layers that heavily share parameters. Furthermore, we clarify the relationship between softmax tempering and dropout, i.e., the most widely used and effective regularization mechanism. Finally, we analyze the impact of softmax tempering  on the softmax distributions and on the gradient flows during training.    In this paper, we explored the utility of softmax tempering for training NMT models. Our experiments in low-resource and high-resource settings revealed that not only does softmax tempering lead to an improvement in the decoding quality but also bridges the gap between greedy and beam search performance. Consequently, we can use greedy search while achieving better translation quality than non-tempered models leading to 1.5 to 3.5 times faster decoding. We also explored the compatibility of softmax tempering with multilingualism and extreme parameter sharing, and explicitly investigated the complementarity of softmax tempering and dropout, where we show that softmax tempering can be an alternative to dropout in high-resource settings, while it is complementary to dropout in low-resource settings. Our analysis of the softmax entropies and gradients during training confirms that tempering gives precise softmaxes while enabling the model to learn with strong gradient signals even during late training stages.  In the future, we will explore the effectiveness of softmax tempering in other natural language processing tasks.  
"," Neural machine translation  models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against smoothed gold labels. In low-resource scenarios, NMT models tend to over-fit because the softmax distribution quickly approaches the gold label distribution. To address this issue, we propose to divide the logits by a temperature coefficient, prior to applying softmax, during training. In our experiments on 11 language pairs in the Asian Language Treebank dataset and the WMT 2019 English-to-German translation task, we observed significant improvements in translation quality by up to 3.9 BLEU points. Furthermore, softmax tempering makes the greedy search to be as good as beam search decoding in terms of translation quality, enabling 1.5 to 3.5 times speed-up. We also study the impact of softmax tempering on multilingual NMT and recurrently stacked NMT, both of which aim to reduce the NMT model size by parameter sharing thereby verifying the utility of temperature in developing compact NMT models. Finally, an analysis of softmax entropies and gradients reveal the impact of our method on the internal behavior of NMT models.",52
" Neural text generation is one of the extensively studied tasks of natural language processing , as it forms the basis for dialogue systems, machine translation, and text summarization. However, often monotonous or dull, texts generated from existing methods do not fully reflect the rich diversity and expression in human language. In particular, models tend to overproduce words frequently appearing in the data, while hardly utilizing informative words  . % along with fast-evolving model architectures and pre-training techniques. \dkp{} %  Even pre-training techniques on large corpora fail to resolve the issue.  %\dkpc{Current logic: three causes, and we choose to solve the last  Better logic: one cause - drawback, second cause - drawback, the third cause - directly addressing the model, thus optimal!}  Possible causes for text degeneration have been illuminated, such as a defect specific to model architectures or the discrepancy between training data and a true distribution. Recently, the emphasis has been placed on investigating the flaws in the maximum likelihood objective. Concretely, the likelihood training pays little attention to the top ranks in terms of the target token probabilities, or maximizing likelihood itself does not adequately reflect human language processing. Therefore, with the maximum likelihood-based training, models learn to produce tokens frequently appearing in the data more often.   We argue, however, that the primary reason behind the sub-optimal performance of the likelihood objective is essentially the imbalanced token distribution inherent in natural language. Natural language is extremely skewed in distribution, where the top hundred most frequently-used  words occupy nearly half of the total corpus following the Zipf's law. Training a classifier with the inherently imbalanced data on the maximum likelihood estimation  leads to biased classification boundaries in favor of majority classes. In other words, models play a difficult role in learning with the imbalanced label  distribution. %  %   Although the above might be contributing factors, we found that word distribution itself provides clues to the text degeneration.   %We set our work in line with the last category. However, unlike the previous approaches, we claim that data distribution can provide clues as to why text degenerates.  %Prior studies report a number of main causes that for text degeneration. First, it is attributed to the Attention mechanisms, where \dkp{bulabula}. Another reason lies in the training of the machine over a fixed corpora of which the distribution does not agree with the real-world language distribution. Lastly, maximum-likelihood objective, by which the machine is trained with, has been questioned. Following the maximum-likelihood, little attention is made to the top ranks of the next \dkpc{next of what?} token probabilities. \dkpc{this should be explained crystal clear, since this phenomenon is directly related to our model}. and that it differs from human behavior.  [t]     % [t] %  %  %   We hypothesize that text generation can be enriched by balancing out the training data distribution. To this end, we introduce F-Softmax , Section), which factorizes the probability distribution of the target token into a product of two conditional probabilities of  frequency class, and  token from the target frequency class. It ensures training over balanced data, since the frequency classes are designed to have the distribution close to uniformity, and token distributions within a class are confined to  subsets of vocabularies grouped with similar frequencies. To this end, all unique tokens are assigned to a frequency class prior to the training, by our novel mean efficiency maximization , Section). MefMax evaluates and maximizes the class-labeling performance with the normalized entropy , having the probability distributions to be learned as uniform as possible.  % Athe probability distributions to be learned by the model are as uniform as possible, without introducing any hyperparameter. %            % We propose a factorized softmax that achieves this by introducing the concept of classes and decomposing the output probabilities using the classes. It computes the probability distributions of tokens in a factorized manner; probability distribution of the class and conditional probability distribution of the next token given the class. The probability of the next token is computed within a subset of the vocabulary, rather than full vocabulary. Well structured subsets of vocabulary allows model to benefit from the balanced output distributions. We assign each token to a unique class utilizing our proposed mean efficiency maximization algorithm so that each data distribution to be trained has a distribution that is as uniform as possible.  We conduct extensive performance evaluations on seven relevant metrics that quantify the diversity and quality of generated texts. In terms of the diversity of generated texts, our approach significantly outperforms not only the MLE baseline but also other diversity-promoting alternatives . We also achieve state-of-the-art results on most of the quality performances.     In this paper, we proposed F-Softmax, a simple but effective method for better learning the rich diversity in text. F-Softmax encourages models to diversify text generation by readjusting class formation and motivating models to learn a more balanced token distribution. Quantitative and qualitative analyses validate the diversity-promoting performances of our approach. Since it can be quickly adopted to replace the traditional likelihood objective, we believe in broader applicability of F-Softmax. Thus, future work involves extending the method to other related tasks, such as machine translation and text summarization, and investigating the potential gains from transfer learning.  
"," %Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is largely attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose F$^2$-Softmax to enable a balanced training over the tokens with skewed frequency distribution. \dkp{By decomposing the softmax function, F$^2$-Softmax confines probability distribution to subsets of vocabularies which are more uniformly distributed. The subsets are further optimized by our novel mean efficiency maximization , without introducing any hyperparameter.} Significant performance gains across generation quality metrics suggest that our methods achieve human-like diversity in text generation.  Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, F$^2$-Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. F$^2$-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of  frequency class, and  token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies.  Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.  % Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose F$^2$-Softmax for a balanced training even with the skewed frequency distribution. F$^2$-Softmax decomposes a probability distribution of the target token into a product of two conditional probabilities of  frequency class  token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. The subsets are further optimized by our novel mean efficiency maximization . It maximizes the . Significant performance gains across generation quality metrics suggest that our method achieves human-like diversity in text generation, without compromising the quality of generated texts. Significant performance gains on seven relevant metrics suggest the supremacy of our approach improves both diversity and quality in text generation.",53
"  Natural language understanding  is a key component of conversational dialogue systems, converting user's utterances into the corresponding semantic representations for certain narrow domain . As a core task in NLU, slot tagging is usually formulated as a sequence labeling problem.  Recently, motivated by commercial applications like Amazon Alexa, Apple Siri, Google Assistant, and Microsoft Cortana, great interest has been attached to rapid domain transfer and adaptation with only a few samples. Few-shot learning approaches become appealing in this scenario , where a general model is learned from existing domains and transferred to new domains rapidly with merely few examples .  The similarity-based few-shot learning methods have been widely analyzed on classification problems, which classify an item according to its similarity with the representation of each class. These methods learn a domain-general encoder to extract feature vectors for items in existing domains, and utilize the same encoder to obtain the representation of each new class from very few labeled samples . This  scenario has been successfully adopted in the slot tagging task by considering both the word-label similarity and temporal dependency of target labels. Nonetheless, it is still a challenge to devise appropriate word-label similarity metrics for generalization capability.  In this work, a vector projection network is proposed for the few-shot slot tagging task in NLU. To eliminate the impact of unrelated label vectors but with large norm, we exploit projections of contextual word embeddings on each normalized label vector as the word-label similarity. Moreover, the half norm of each label vector is utilized as a threshold, which can help reduce false positive errors.   %It first normalizes the vector representation of each label as a unit vector, and then exploits projections of contextual word embeddings on these unit label vectors as the word-label similarities.  One-shot and five-shot experiments on slot tagging and named entity recognition  tasks show that our method can outperform various few-shot learning baselines, enhance existing advanced methods like TapNet and prototypical network, and achieve state-of-the-art performances.  Our contributions are summarized as follows:           In this paper, we propose a vector projection network for the few-shot slot tagging task, which can be interpreted as a normalized linear model with an adaptive bias. Experimental results demonstrate that our method can significantly outperform the strongest few-shot learning baseline on SNIPS and NER datasets in both 1-shot and 5-shot settings. Furthermore, our proposed vector projection based similarity metric can remarkably surpass others variants.   For future work, we would like to add a learnable scale factor for bias in Eqn. .       
","  Few-shot slot tagging becomes appealing for rapid domain transfer and adaptation, motivated by the tremendous development of conversational dialogue systems. In this paper, we propose a vector projection network for few-shot slot tagging, which exploits projections of contextual word embeddings on each target label vector as word-label similarities. Essentially, this approach is equivalent to a normalized linear model with an adaptive bias. The contrastive experiment demonstrates that our proposed vector projection based similarity metric can significantly surpass other variants. Specifically, in the five-shot setting on benchmarks SNIPS and NER, our method outperforms the strongest few-shot learning baseline by $6.30$ and $13.79$ points on F$_1$ score, respectively. Our code will be released at \url{https://github.com/sz128/few_shot_slot_tagging_and_NER}.",54
"  % Neural machine translation  has made great progress in recent years.  Recently, more and more novel network structures of neural machine translation have been proposed , among which Transformer  achieves the best results. One important difference between Transformer and other translation models is its multi-head attention mechanism.   % The original intention of introducing multi-head attention is to capture different aspects of context information and it indeed improves the performance of NMT models. Some interesting phenomena of the attention heads are discovered recently.  find that only a small subset of heads appears to be important for the translation task and vast majority of heads can be removed without seriously affecting performance.  also find that several heads can be removed from trained transformer models without statistically significant degradation in test performance. It turns out that not all heads are equally important.  We speculate that this can be attributed to the imbalanced training of multi-head attention, as some heads are not trained adequately and contribute little to the model. However, this can be turned into the bottleneck for the whole model. For an analogy, if a soccer player gets used to using the right foot and spares more training opportunities for it, it will be stronger and stronger. As a result, the right foot is further relied on, while the left foot receives less training and gradually turns into the limitation.  In this paper, we firstly empirically confirm the inequality in multi-head attention. Then a new training method with two variants  % \SJ{reconsider the choice of words: stragety, ways, methods... You need two. }  is proposed to avoid the bottleneck and improve the translation performance. Further analyses are also made to verify the assumption.  %    In this paper, we empirically validate the inequality of attention heads in Transformer and come up with an assumption of imbalanced training. Correspondingly, we propose a specific method in two ways to resolve the issue. Experiments show the improvements on multiple language pairs. And detailed analysis shows the alleviation of the problem and the effectiveness of our techniques.   
"," Recent studies show that the attention heads in Transformer are not equal~. We relate this phenomenon to the imbalance training of multi-head attention and the model dependence on specific heads. To tackle this problem, we propose a simple masking method: HeadMask, in two specific ways. Experiments show that translation improvements are achieved on multiple language pairs. Subsequent empirical analyses also support our assumption and confirm the effectiveness of the method.",55
"   Two widely-known formalisms are commonly used to represent the syntactic structure of sentences in human languages: constituent and dependency representations.  Constituent trees, which are commonly used in tasks where span information is crucial, describe the syntax of a sentence in terms of constituents  and their hierarchical order. We can find two kinds of constituent trees: continuous and discontinuous  and , respectively). The latter extend the former by allowing  %the representation of  crossing branches and constituents with gaps in the middle. These are necessary for describing some wh-movement, long-distance extractions, dislocations, cross-serial dependencies and other linguistic phenomena common in free word order languages such as German .  On the other hand, a dependency tree straightforwardly connects each word of a sentence as a dependent of another, which is considered its head word. This structure composed of binary syntactic dependencies is known for representing information closer to semantic relations and can be classified as projective or non-projective  and , respectively). %, being the  Non-projective dependency trees  %are a more complex structure that allows to allow crossing dependencies, and can model the same linguistic phenomena described by discontinuous constituent trees.  Since the information described in a  %regular  constituent tree cannot be fully represented in a  %regular  dependency tree and vice versa ,  %typically there are parsers that are typically parsers are exclusively trained to produce either dependency or constituent structures and, in some cases,  %they are  restricted to the less complex continuous/projective representations. %, supporting just one out of the four syntactic structures described before.    generates continuous and projective structures with a single  model, and the sequence labeling parser of  combines continuous constituents with non-projective dependency structures. In both cases, which are discussed in more detail in Section,  representations are shown to benefit each other in terms of accuracy.}     we propose a novel multitask transition-based parser that can efficiently generate unrestricted constituent and dependency structures  from a single trained model. We design an encoder-decoder neural architecture that is jointly trained across the syntactic information represented in the two formalisms by following a multitask learning strategy . Inspired by  , we model constituent trees as augmented dependency structures  and use two separate task-specific decoders to produce both regular and augmented dependency trees. Each decoder relies on Pointer Networks  and a biaffine classifier  to incrementally generate labelled dependencies from left to right, as proposed by . Finally, the decoding runtime $) and the required memory space of our multi-representational approach remains the same as the single-task dependency parser by , since a single model is trained and the multitask learning strategy has no impact on decoding time, allowing both decoders to be run in parallel.    We test our multi-representational neural model\footnote{Source code available at \url{https://github.com/danifg/MultiPointer}.} on the continuous English and Chinese Penn Treebanks  and on the discontinuous NEGRA  and TIGER  datasets. In all benchmarks, our approach outperforms single-task parsers , which proves that learning across regular dependency trees and constituent information  leads to gains in accuracy in both tasks, obtaining competitive results in all cases and surpassing the current state of the art %by a wide margin  in several datasets.    [t]  \& \& {\tiny ADVP} \& {\tiny ADJP} \& \\  \deproot{2}{} \depedge[edge unit distance=4ex]{2}{1}{ROOT+S\#2} \depedge{2}{3}{VP\#1} \depedge[edge unit distance=3ex]{2}{4}{VP\#1} \depedge[edge unit distance=3ex]{2}{5}{ROOT+S\#2}  %  [theme = simple]  \deproot[edge unit distance=2ex]{4}{} \depedge[edge unit distance=5ex]{4}{1}{nsubj} \depedge[edge unit distance=4ex]{4}{2}{cop} \depedge{4}{3}{advmod} \depedge[edge unit distance=4ex]{4}{5}{punct} \\ % {\tiny a) Continuous constituent tree.}   {\tiny b) Projective augmented dependency tree.}  {\tiny c) Projective dependency tree.}\\  %%%%%%%%%%%%%%%%%%%%%%%%%   %   \deproot[edge unit distance=4ex]{2}{} \depedge[edge unit distance=2ex]{4}{1}{NP\#2} \depedge{4}{3}{NP\#1} \depedge[edge unit distance=4.5ex]{2}{4}{S\#1} \depedge[edge unit distance=4ex]{2}{5}{VROOT\#2}  %  [theme = simple]  \deproot[edge unit distance=2.5ex]{2}{} \depedge[edge unit distance=2ex]{4}{1}{APP} \depedge{4}{3}{DET} \depedge[edge unit distance=4.5ex]{2}{4}{SUBJ} \depedge[edge unit distance=4ex]{2}{5}{punct} \\ % {\tiny d) Discontinuous constituent tree.}   {\tiny e) Non-projective augmented dependency tree.}  {\tiny f) Non-projective dependency tree.}        We propose a novel encoder-decoder neural architecture based on Pointer Networks that, after being jointly trained on regular and constituent-based dependency trees, can syntactically parse a sentence to   two of the most extended formalisms.  both constituent and dependency trees. Apart from just requiring to train a single model, our approach can produce not only the simplest continuous/projective trees, but also discontinuous/non-projective structures in just  runtime. We test our parser on the main dependency and constituent benchmarks, obtaining competitive results in all cases and reporting state-of-the-art accuracies in several datasets.  As future work, we plan to perform auxiliary-task learning and train a separate model for each task, testing different weights for the loss computation. This will lose the advantage of training a single model to undertake both tasks, but will certainly lead to further improvements in accuracy.    
"," We propose a transition-based approach that, by training a single model, can efficiently parse any input sentence with both constituent and dependency trees, supporting both continuous/projective and discontinuous/non-projective syntactic structures. To that end, we develop a Pointer Network architecture with two separate task-specific decoders and a common encoder, and follow a multitask learning strategy to jointly train them. The resulting quadratic system, not only becomes the first parser that can jointly produce both unrestricted constituent  and dependency   trees from a single model, but also proves that both syntactic formalisms can benefit from each other during training, achieving state-of-the-art accuracies in several widely-used benchmarks such as the continuous English and Chinese Penn Treebanks, as well as the discontinuous German NEGRA and TIGER datasets.",56
"   %GOAL: introducing rule based% %Traditional solutions for task-oriented dialogue systems decompose the task of building a complete task-oriented dialogue system into several sequential steps, including ,  and ~ . In this paper we focus on the dialogue policy that is a key component in dialogue management; it decides what actions the system should take at each time step according to the context and user feedback.  The aim of dialogue policies in  is to select appropriate actions at each time step according to the current context of the conversation and user feedback~. In early work, dialogue policies were manually designed as a set of rules that map the dialogue context to a corresponding system action~. %That is only feasible when domain is not complex. %, an approach that suffers from limited task scalability and from an inability of easily being updated as user behavior changes.  %When the task domain is not complex and the possible conversation scenarios can be predefined explicitly, the dialogue policy can be represented as a set of rules that map the dialogue context to a corresponding system action .  The ability of rule-based solutions is limited by the domain complexity and task scalability. Moreover, the design and maintenance of these rules require a lot of effort and domain knowledge.   %GOAL: introducing supervised learning and its disadvantages% Due to recent advantages in deep learning and the availability of labeled conversational datasets,  can be employed for dialogue policy training to overcome the disadvantages of rule-based systems. %Dialogue context-action pairs are fed to the model to infer the underlying relation between dialogue context and corresponding dialogue actions with supervised learning methods.  \todo{It looks obvious what is the task from he first sentence the paragraph} The downside of the supervised learning approach is that the dialogues observed in the datasets are unlikely to represent all possible conversation scenarios; in some extreme cases, the required conversational dataset cannot be collected or acquiring it might cost-prohibitive.   %GOAL: introducing RL and its disadvantages% The success of  in other areas holds promises for dialogue ~. Using  techniques, we can train dialogue policies and optimize automatically, from scratch and utilizing interactions with users~.  % Handcrafting complex rules is not essential anymore and the expense and pressure of maintaining the policy over time can be alleviated.  In -based solutions, the dialogue system takes actions that are controlled by the dialogue policy, and user feedback , which is provided when the dialogue is finished, is utilized to adjust the initial policy~.  %These methods assume that the system has access to a  at the end of each dialogue. In practice, reward signals are not always available and may be inconsistent~.  As it is not practical to ask for explicit user feedback for each dialogue during policy training, different strategies have been proposed to design a rule-based user simulator along with a reward function that can approximate the real  which exists only in each user's mind.  Designing an appropriate user simulator and accurate reward function requires strong domain knowledge.  This process has the same disadvantages as rule-based dialog systems~.  The difference is that rule-based approaches to system design meet this problem at the dialogue agent side while rule-based user simulators need to solve it at the environment side.    %To train a dialogue agent with reinforcement learning, we have to handcraft a rule-based user simulator and it will suffer the same problem with rule-based dialogue agent when the task in becoming complex.  %The only difference is that one approach meets this problem at the dialogue agent side  while another one has to solve it at the environment side .  %%GOAL: Describing the bottleneck% If the task is simple and easy to solve, why not just build a rule-based system rather than a user-simulator that is then used with  techniques to train the dialogue system, where more uncontrollable factors are involved?  And if the task domain is complex and hard to solve, is it easier to design and maintain a complicated rule-based user simulator than to build a rule-based dialogue agent? % Training a model-based user simulator~ with real human dialogue dataset is an alternative solution but it is still data-hungry.  %Besides, there is no guarantee that human-designed and model-based simulator can cover all possible dialogue scenarios.  % With respect to the comparison between reinforcement learning and supervised learning, s Supervised learning methods do not suffer from these issues but require labeled conversational data; in some exceptional cases, if the data cannot be collected for privacy reasons,  is the solution. However, collecting labeled data is feasible for many applications~. % Therefore in this work seek to answer the following research question:  focusing purely on advancing -based methods?}   To address this question, we introduce three dialogue  methods which do not require a user simulator. The proposed methods can achieve comparable or even higher performance compared to   methods.  The first method utilizes an action decoder to predict dialogue combinations.  %The sequential decision setup can make use of dependency information between different atomic dialogue actions in the same response.  The second method regards the dialogue  task as a multi-label classification problem.  Unlike previous work, we assign a dense layer to each action label in the action space. % This change provides the dialogue agent with more stable and higher performance.  Based on the second method, we propose an adversarial learning method for dialogue  without utilizing .  To backpropagate the loss from the reward model to the policy model, we utilize the Gumbel-Softmax to connect the policy model and the reward model in our third method.  % We compare our methods with  and adversarial  based dialogue training solutions to show how we can achieve comparable performance without a utilizing costly user simulator.  To summarize, our contributions are: [nosep,leftmargin=*]  performance in dialogue  with fewer efforts and costs compare to existing -based solutions.      In this work, we proposed two supervised learning approaches and one adversarial learning method to train the dialogue policy for  without building user simulators. The proposed methods can achieve state-of-the-art performance suggested by existing approaches based on  and adversarial learning. However, we have demonstrated that our methods require fewer training efforts, namely the domain knowledge needed to design a user simulator and the intractable parameter tuning for  or adversarial learning. Our findings have questioned if the full potential of supervised learning for dialogue  has been exerted and if  methods have been used in the appropriate  scenarios.   \clearpage  
"," %\todo[maybe a more interesting title? like ``rethinking supervised learning and reinforcement learning in dialogue policy learning""] Dialogue policy learning for  has enjoyed great progress recently mostly through employing  methods. However, these approaches have become very sophisticated. It is time to re-evaluate it. Are we really making progress developing dialogue agents only based on ? We demonstrate how ~traditional supervised learning together with ~a simulator-free adversarial learning method can be used to achieve performance comparable to  -based methods.  First, we introduce a simple dialogue action decoder to predict the appropriate actions. Then, the traditional multi-label classification solution for dialogue policy learning is extended by adding dense layers to improve the dialogue agent performance. Finally, we employ the Gumbel-Softmax estimator to alternatively train the dialogue agent and the dialogue reward model without using .  Based on our extensive experimentation, we can conclude the proposed methods can achieve more stable and higher performance with fewer efforts, such as the domain knowledge required to design a user simulator and the intractable parameter tuning in reinforcement learning. Our main goal is not to beat  with supervised learning, but to demonstrate the value of rethinking the role of  and supervised learning in optimizing .",57
" The de-facto supervised neural network training paradigm requires a large dataset with annotations.  It is time-consuming, difficult and sometimes even infeasible to collect a large number of data-points  due to task nature. A typical example task is medical diagnosis.  In addition, annotating datasets also is costly, especially in domains where experts are difficult to recruit. In a traditional annotation process, the  human-machine communication bandwidth is narrow. Each label provides  bits per sample for a -class classification problem.  However, humans don't solely rely on such low bandwidth communication to learn. They instead learn through natural language communication, which grounds on abstract concepts and knowledge. Psychologists and philosophers have long posited  natural language explanations  as central, organizing elements to human learning and reasoning. Following this intuition, we explore methods to incorporate natural language explanations in learning paradigms to improve learning algorithm's data efficiency.        earning  w\underline{i}th  \underline{C}ontrastive  % Natural Language \underline{E}xplanations }}}{#1}} [1]{{{{ALICE}}}{#1}}    [1]{[{WX: #1}]} % [1]{{}}     % % File emnlp2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily  \usepackage{booktabs} \usepackage{multirow} \usepackage{graphicx}  \usepackage{subcaption} \usepackage{comment}  \usepackage{amsmath} \usepackage{amsfonts} \usepackage{amssymb}  \usepackage{algorithm} \usepackage{algorithmic}   \renewcommand{\algorithmicrequire}{Input:} \renewcommand{\algorithmicensure}{Output:}    %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  [1]{{ {\TeX}  \title{ ALICE: Active Learning with Contrastive Natural Language Explanations  }  \author{Weixin Liang \\   Stanford University \\    \\\And   James Zou \\   Stanford University \\    \\\And   Zhou Yu \\   University of California, Davis \\    \\   }        \date{}        % abstract    \footnotetext[1]{Co-supervised project.}     \clearpage       We propose an expert-in-the-loop training framework  \modelabbrevname{} to utilize  contrastive natural language explanations  to improve a learning algorithm's data efficiency.  We extend the concept of active learning to  class-based active learning for choosing  the most informative query pair.  We incorporate the extracted knowledge from expert natural language explanation by changing our algorithm's neural network structure. Our experiments on two visual recognition tasks  show that  incorporating natural language explanations  is far more data-efficient  than adding extra training data.  In the future,  we plan to examine  the hierarchical classification architecture's potential for reducing computational runtime.       Our image encoder  could be any off-the-shelf visual backbone model and we use Inception v3.     We implement our semantic parser on top of the Python-based SippyCup  following previous work~.  Our framework could support applications in other languages by changing a semantic parser for corresponding languages.    For the experiments on class-based active learning,    we do not need to collect    all possible  contrastive natural language explanations.    We could collect the explanations in an on-demand manner because our class-based active learning is empirically insensitive    to the change of random seeds and hyper-parameter .  We provide more details in Appendix.    More implementation details are included     in the Appendix.    We experiment on two tasks, bird species and social relationship classification and describe the details below.     
","  % abstract   \input abstract   % Long papers may consist of up to 8 pages of content, plus unlimited pages for references; final versions of long papers will be given one additional page of content  so that reviewers闁 comments can be taken into account.",58
" 	 	 	Causal explanation detection  aims to detect whether there is a causal explanation in a given message . Linguistically, there are coherence relations in messages which explain how the meaning of different textual units can combine to jointly build a discourse meaning for the larger unit. The explanation is an important relation of coherence which refers to the textual unit  in a message that expresses explanatory coherent semantics . As shown in Figure , M1 can be divided into three discourses, and D2 is the explanation that expresses the reason why it is advantageous for the equipment to operate at these temperatures. CED is important for tasks that require an understanding of textual expression . For example, for question answering, the answers of questions are most likely to be in a group of sentences that contains causal explanations . Furthermore, the summarization of event descriptions can be improved by selecting causally motivated sentences . Therefore, CED is a problem worthy of further study. 	 	 	 	 	The existing methods mostly regard this task as a classification problem . At present, there are mainly two kinds of methods, feature-based methods and neural-based methods, for similar semantic understanding tasks in discourse granularity, such as opinion sentiment classification and discourse parsing . The feature-based methods can extract the feature of the relation between discourses. However, these methods do not deal well with the implicit instances which lack explicit features. For CED, as shown in Figure , D2 lacks explicit features such as , , or the features of tenses, which are not friendly for feature-based methods. The methods based on neural network are mainly Tree-LSTM model  and hierarchical Bi-LSTM model . The Tree-LSTM models learn the relations between words to capture the semantics of discourses more accurately but lack further understanding of the semantics between discourses. The hierarchical Bi-LSTM models can employ sequence structure to implicitly learn the relations between words and discourses. However, previous work shows that compared with Tree-LSTM, Bi-LSTM lacks a direct understanding of the dependency relations between words. Therefore, the method of implicit learning of inter-word relations is not prominent in the tasks related to understanding the semantic relations of messages . Therefore, how to directly learn the relations between words effectively and consider discourse-level correlation to further filter the key information is a valuable point worth studying. 	 	Further analysis, why do the relations between words imply the semantics of the message and its discourses? From the view of computational semantics, the meaning of a text is not only the meaning of words but also the relation, order, and aggregation of the words. In other simple words is that the meaning of a text is partially based on its syntactic structure . In detail, in CED, the core and subsidiary words of discourses contain their basic semantics. For example, as D1 shown in Figure , according to the word order in syntactic structure, we can capture the  of  is . We can understand the basic semantic of D1 which expresses some kind of  is  via root words  and its affiliated words. Additionally, why the correlation and key information at the discourse level are so important to capture the causal explanatory semantics of the message? Through observation, the different discourse has a different status for the explanatory semantics of a message. For example, in M1, combined with D1, D2 expresses the explanatory semantics of , while D3 expresses the semantic of transition. In detail, D1 and D2 are the keys to the explanatory semantics of M1, and if not treated D1, D2, and D3 differently, the transitional semantic of D3 can affect the understanding of the explanatory semantic of M1. Therefore, how to make better use of the information of keywords in the syntactic structure and pay more attention to the discourses that are key to explanatory semantics is a problem to be solved. 	 	To this end, we propose a Pyramid Salient-Aware Networks  which utilizes keywords on the syntactic structure of each discourse and focuses on the key discourses that are critical to explanatory semantics to detect causal explanation of messages. First, what are the keywords in a syntactic structure? From the perspective of syntactic dependency, the root word is the central element that dominates other words, while it is not be dominated by any of the other words, all of which are subordinate to the root word . From that, the root and subsidiary words in the dependency structure are the keywords at the syntax level of each discourse. Specifically, we sample 100 positive sentences from training data to illuminate whether the keywords obtained through the syntactic dependency contain the causal explanatory semantics. And we find that the causal explanatory semantics of more than 80\% sentences be captured by keywords in dependency structure\footnote{Five Ph.D. students majoring in NLP judge whether sentences could be identified as which containing causal explanatory semantics by the root word and its surrounding words in syntactic dependency, and the agreement consistency is 0.8}. Therefore, we extract the root word and its surrounding words on the syntactic dependency of each discourse as its keywords.  	 	Next, we need to consider how to make better use of the information of keywords contained in the syntactic structure. To pay more attention to keywords, the common way is using attention mechanisms to increase the attention weight of them. However, this implicitly learned attention is not very interpretable. Inspired by previous researches , we propose a bottom graph-based word-level salient network which merges the syntactic dependency to capture the salient semantics of discourses contained in their keywords. Finally, how to consider the correlation at the discourse level and pay more attention to the discourses that are key to the explanatory semantics? Inspired by previous work , we propose a top attention-based discourse-level salient network to focus on the key discourses in terms of explanatory semantics. 	 	In summary, the contributions of this paper are as follows:  	 		yramid Salient-Aware Network  to detect causal explanations of messages which can effectively learn the pivotal relations between keywords at word level and further filter the key information at discourse level in terms of explanatory semantics. 		 		 	 	  	In this paper, we devise a pyramid salient-aware network  to detect causal explanations in messages. PSAN can effectively learn the key relation between words at the word level and further filter out the key information at the discourse level in terms of explanatory semantics. Specifically, we propose a bottom word-level salient-aware module to capture the salient semantics of discourses contained in their keywords based on a the syntactic-centric graph. We also propose a top discourse-level salient-aware module to modify the dominance of different discourses in terms of global explanatory semantic constraint via an attention mechanism. Experimental results on the open-accessed commonly used datasets show that our model achieves the best performance. 	 	
"," 		Causal explanation analysis  can assist us to understand the reasons behind daily events, which has been found very helpful for understanding the coherence of messages. In this paper, we focus on , an important subtask of causal explanation analysis, which determines whether a causal explanation exists in one message. We design a Pyramid Salient-Aware Network  to detect causal explanations on messages. PSAN can assist in causal explanation detection via capturing the salient semantics of discourses contained in their keywords with a bottom graph-based word-level salient network. Furthermore, PSAN can modify the dominance of discourses via a top attention-based discourse-level salient network to enhance explanatory semantics of messages. The experiments on the commonly used dataset of CEA shows that the PSAN outperforms the state-of-the-art method by 1.8\% F1 value on the  task.",59
"     Event coreference resolution  is a task about determining which event mentions in a document refer to the same real-world event. Event coreference resolution is an important part of NLP systems such as summarization, text-level event extraction, question answering and so on. Besides, compared to considerable research of entity coreference resolution, there is less attention on event coreference resolution. Therefore, event coreference resolution is still a challenging task and the performance should be improved.           Event mentions that refer to the same event can occur both within a document  and across multiple documents . We focus on WD event coreference in this paper because WD event coreference is the basic work of CD event coreference.     The main task of WD event coreference is judging whether a pair of events are coreferential or not.     Figure shows two coreferential event pairs from two documents. The first event pair in D1 is about  event and the second event pair in D2 is about  event.      In order to judge the coreference of a event pair, most approaches for solving event coreference resolution relied on various linguistic properties especially , which contains {spatio-temporal} information of events. For instances, in Figure, the words with red front are events. And the words with blue, green and orange front are participant, time, location of the events respectively.            Although event arguments contain useful information for event coreference resolution, there are two problems for using event arguments information in event coreference resolution. Firstly, it's difficult to extract event arguments accurately due to the diversity of the expression of event arguments. The performance of event argument extraction is only 55.7 in ACE corpus. For instance, in D1, the arguments about  event in the two sentences are the same but are expressed differently. In details, in D1, the participant, time, location of  event are ,  and  in S1, but ,  and  in S2 respectively. Secondly, not every event mention contains all arguments of one event that may make model confused about the coreference of two events in a event pair. For instance, in D2, the  that the location of  event is in S1 but not in S2. Besides, in D2, devoid of event arguments,  event and  event are coreferential in context.      As aforementioned, the arguments of events are difficult to extract. It is also difficult to use arguments to solve all the problems of event coreference resolution even if they are extracted. Thus, the context about event mentions is more important and effective for event coreference resolution. In order to use context information efficiently, we propose a multi-loss neural network model  which doesn't need any argument information to accomplish within-document event coreference resolution task. We propose two sub-models which use context information to detect the coreference of two events in a event pair and train them jointly. One is a classifier which  predicts whether the two events in one pair are coreferential, and another is a scorer which calculates similarity scores between them to assist infer coreference.  	     The final stage about event coreference resolution is event clustering. After all event pairs are predicted and scored, we filter event pairs according to the results of classifier and scorer. Then, we use a dynamic connectivity algorithm to construct a graph for event clustering. Each node in graph is a event mention and each edge between two nodes represent whether the two event are coreferential or not. Finally, all events connected in one graph are considered to be in one event cluster. 	     We evaluate our model on ECB+ corpus and use , ,  and  as measures. The experimental results show that our model achieve a significant improvement compared to the state-of-the-art methods which use event argument features.             We present a multi-layer feed-forward neural network for event mention extraction and a multi-loss neural network model for within-document event coreference resolution respectively. We do not use any information about event argument in our system. Additionally, we test our system in ECB+ corpus and achieve a significant improvement than the state-of-the-art methods.      Due to the incomplete annotation and the propagation of errors about event mentions and arguments extraction in pipeline systems, we will try to design a joint model to accomplish the event extraction, argument extraction, event coreference resolution tasks jointly in the future.                                                                               Acknowledgements. 閿熸枻鎷疯阿                                                                 \Acknowledgements{This work is supported by the Natural Science Foundation of China . This work was also supported by Alibaba Group through Alibaba Innovative Research  Program and Huawei Tech. Ltm through Huawei Innovation Research Program.).}                                                                               Supplements. 閿熸枻鎷烽敓鏂ゆ嫹閿熸枻鎷烽敓, 閿熻鎲嬫嫹閫                                                                  \Supplements{Appendix A.}                                                                          Reference section. 閿熻娇鍖℃嫹閿熸枻鎷烽敓鏂ゆ嫹         citation in the content using ""some words"".         ~ is needed to make the reference number is on the same line with the word before it.                                                                                                                                                       Appendix sections. 閿熸枻鎷峰綍閿熼摪鏂ゆ嫹, 閿熻鎲嬫嫹閫                                                                        
","Event coreference resolution is an important task in Natural Language Processing  and nearly all the existing approaches to this task  rely on event argument information. However, these methods tend to suffer from error propagation from the stage of event argument extraction. Besides, not every event mention contains all arguments of an event and argument information may confuse the model that events have arguments to detect event coreference in real text. Furthermore, the context information of an event is useful to infer coreference between events. Thus, in order to reduce the errors propagated from event argument extraction and use context information effectively, we propose a multi-loss neural network model which does not need any argument information to do the within-document event coreference resolution task and achieve a significant performance than the state-of-the-art methods.",60
" A task-oriented spoken dialogue system usually consists of three modules: input,output and control, shown in Fig.. The input module which consists of automatic speech recognition  and spoken language understanding  extracts semantic-level user dialogue actions from user speech signal. The control module  has two missions. One is to maintain dialogue state, an encoding of the machine's understanding about the conversation. Once the information from the input module is received, the dialogue state is updated by dialogue state tracking . The other is to choose a semantic-level machine dialogue action to response the user, which is called dialogue decision policy.    The output consists of natural language generation  and text-to-speech  synthesis, which convert dialogue action to audio. Dialogue management is an important part of a dialogue system. Nevertheless, there are inevitable ASR and SLU errors which make it hard to track true dialogue state and make decision. In recent statistical dialogue system, the distribution of dialogue state, i.e. belief state, is tracked. A well-founded theory for belief tracking and decision making is offered by partially observable Markov Decision Process  framework.  Previous DST algorithms can be divided into three families: hand-crafted rules, generative models, and discriminative models. Recently, since the Dialog State Tracking Challenges  have provided labelled dialog state tracking data and a common evaluation framework and test-bed, a variety of machine learning methods for DST have been proposed. These methods rely strictly on set of labelled off-line data. Since the labelled data are off-line, the learning process of these supervised learning methods is independent on the dialogue policy module. The key issues of these supervised learning methods are poor generalization and over-tuning. Due to the lack of labels, these approaches can not be easily used for on-line update of DST.   This work marks first step towards employing the deep reinforcement learning  method into dialogue state tracking  module. The performance of the DST module is optimized during the conversation between the user and the dialogue system. We call the DRL-based DST module as the tracking agent. In order to bound the search space of the tracking agent, we propose a companion teaching framework . Furthermore, under this framework, we can train tracking agent and dialogue policy agent jointly with respective deep reinforcement learning  algorithms in order to make these two agents adaptive to each other. % And there are two main types of DST systems in the current dialogue system. One is the semantic-based dialogue state tracking system, and the other is the text-based dialogue state tracking system that implicitly or explicitly removes spoken language understanding  module. In this paper, we explain the proposed tracking agent framework based on the semantic-based dialogue state tracking  system.  The paper has two main contributions:     The rest of the paper is organized as follows. Section  gives an overview of related work. In Section , the framework of on-line DST are presented. The implementation detail is represented in Section . In Section , the joint training process is introduced. Section  presents experiments conducted to evaluate the proposed framework, followed by the conclusion in Section .     This paper provides a DRL-based companion teaching framework to optimize the DST module of the dialogue system. Under this framework, the tracker can be learned during the conversations between the user and the SDS rather than produced by the off-line methods. We can also choose to jointly train dialogue policy agent and the tracking agent under this framework. The experiments showed that the proposed companion teaching framework for the on-line DST system achieved promising performances in DSTC2 and DSTC3.     
"," Dialogue state tracking  is a crucial module in dialogue management. It is usually cast as a supervised training problem, which is not convenient for on-line optimization. In this paper, a novel companion teaching based deep reinforcement learning  framework for on-line DST optimization is proposed. To the best of our knowledge, this is the first effort to optimize the DST module within DRL framework for on-line task-oriented spoken dialogue systems. In addition, dialogue policy can be further jointly updated. Experiments show that on-line DST optimization can effectively improve the dialogue manager performance while keeping the flexibility of using predefined policy. Joint training of both DST and policy can further improve the performance.",61
"   {T}{he} task-oriented spoken dialogue system  aims to assist a human user in accomplishing a specific task . The dialogue management is a core part of SDS. There are two main missions in dialogue management: dialogue belief state tracking  and dialogue decision-making . In this work, we only focus on devising a policy that chooses which dialogue action to respond to the user.   The sequential system decision-making process can be abstracted into a partially observable Markov decision process . Under this framework, reinforcement learning approaches can be used for automated policy optimization.   In the past few years, there are many deep reinforcement learning  algorithms閿 which use neural networks  as function approximators, investigated for dialogue policy .  Most of these approaches focus on dialogue policy optimization in a single dialogue task. However, in real-life scenarios, a dialogue agent can be asked by many users for different dialogue tasks, e.g., Apple Siri can support many dialogue tasks .  In the multi-task setup, the traditional DRL-based approaches have to train an individual policy for each dialogue task.  It means that each dialogue policy has its independent model parameters, whose scale will increase proportionally with the number of the tasks. One solution is to train a { and {  In this paper, we propose  the Structured Actor-Critic  Reinforcement Learning for Universal Dialogue Management  . With the scalability of the GNN , a single set of parameters can be used for different dialogue tasks. That makes it possible to train a generic policy among multiple dialogue tasks. To tackle the efficiency problem, we deploy an advanced off-policy actor-critic algorithm, which combines decoupled acting and learning with a novel off-policy correction method called V-trace. Combining the improved optimization algorithm with the structured dialogue policy, we can make the generic policy learning process more stable and efficient than the original GNN-based dialogue policy .  We evaluate the performance of STRAC on PyDial benchmark, which includes six environments and three dialogue domains. Results show that our unified dialogue agent STRAC gets the best performance on most of the 18 tasks of the benchmark.     This paper proposed a scalable distributed dialogue policy STRAC to train a generic dialogue policy on all available data collected from different dialogue tasks. STRAC increased scalability, stability and efficiency of the NN-based policy through combining structured dialogue policy and effective off-policy actor-critic algorithm. Compared with the traditional approaches, STRAC-M can be trained parallel on multiple tasks and gets the better performance, especially in the data-limited situation. Compared with another GNN-based policy optimization approach FM-GNN, the training process of STRAC is more stable and efficient. The final gains are more considerable in more complex environments. Compared with recent proposed generic policy DQNDIP-M, STRAC-M not only can be trained using the data from all the available dialogue tasks but also can model the relations among all the sub-agents. In future work, we will test STRAC with real users instead of the agenda-based user simulator.                 if have a single appendix:  [Proof of the Zonklar Equations]   or      for no appendix heading   do not use 
"," Traditional dialogue policy needs to be trained independently for each dialogue task. In this work, we aim to solve a collection of independent dialogue tasks using a unified dialogue agent. The unified policy is parallelly trained using the conversation data from all of the distributed dialogue tasks. However, there are two key challenges: the design of a unified dialogue model to adapt to different dialogue tasks;  finding a robust reinforcement learning method to keep the efficiency and the stability of the training process. Here we propose a novel structured actor-critic approach to implement structured deep reinforcement learning , which not only can learn parallelly from data of different dialogue tasks\footnote{In the experimental setup of this work, each dialogue task has only one dialogue domain.} but also achieves stable and sample-efficient learning. We demonstrate the effectiveness of the proposed approach on 18 tasks of PyDial benchmark. The results show that our method is able to achieve state-of-the-art performance.",62
"  %When building a dialogue system, complex tasks that require more information exchange are often more challenging. One example is to handle the restaurant reservation consultation in multiple areas during a single conversation. Specifically, this type of task that needs to complete some subtasks  in order to finish the conversation is called the composite task.  Composite tasks are different from multi-domain dialogue tasks. The latter is often mentioned in papers that focusing on transfer learning. In most case, multi-domain dialogue tasks involve only one domain in a single dialogue, and the performance of this one domain model is tested on different domains in order to highlight its transferability. On the contrary, composite dialogue tasks may involve multiple domains in a single dialogue, and the agent must complete all subtasks  in order to get positive feedback.  Consider the process of completing a composite task . An agent first chooses a subtask , then make a sequence of decisions to gather related information  until all information required by users are provided and these subtasks are completed, and then choose the next subtask  to complete. The state-action space will increase with the number of subtasks. Thus, dialogue policy learning for the composite task needs more exploration, and it needs to take more dialogue turn between agent and user to complete a composite task. The sparse reward problem is further magnified.   Solving composite tasks using the same method as the one solving single domain tasks may hit obstacles. The complexity of the composite task makes it hard for an agent to learn an acceptable strategy. While hierarchical deep reinforcement learning  shows its promising power, by introducing the framework of options over Markov Decision Process , the original task can be decomposed into two parts: deciding which subtask to solve and how to solve one subtask, thus simplifying the problem.  However, in previous works, multi-layer perceptrons  are often used in DQN to estimate the Q-value. MLPs use the concatenation of the flatten dialogue state as its inputs. In this way, it cannot capture the structural information of the semantic slots in that state easily, which results in low sampling efficiency. In our work, we propose ComNet, which makes use of the Graph Neural Network  to better leverage the graph structure in the observations  and being coherent with the HDRL method.  Our main contributions are three-fold: 1. We propose a new framework ComNet combining HDRL and GNN to solve the composite tasks while achieving sample efficiency. 2. We test ComNet based on PyDial  benchmark and show that our result over-performed the vanilla HDRL systems and is more robust to noise in the environment. 3. We test the transferability of our framework and prove that under our framework, an efficient and accurate transfer is possible.    In this paper, we propose ComNet, which is a structured hierarchical dialogue policy represented by two graph neural networks . By replacing MLPs in the traditional HDRL methods, ComNet makes better use of the structural information of dialogue state by separately feeding observations  and the top-level decision into slot-dependent, slot-independent and subtask nodes and exchange message between these nodes. We evaluate our framework on modified PyDial benchmark and show high efficiency, robustness and transferability in all settings.    
"," Dialogue policy training for composite tasks, such as restaurant reservation in multiple places, is a practically important and challenging problem. Recently, hierarchical deep reinforcement learning  methods have achieved good performance in composite tasks. However, in vanilla HDRL, both top-level and low-level policies are all represented by multi-layer perceptrons  which take the concatenation of all observations from the environment as the input for predicting actions. Thus, traditional HDRL approach often suffers from low sampling efficiency and poor transferability. In this paper, we address these problems by utilizing the flexibility of graph neural networks . A novel ComNet is proposed to model the structure of a hierarchical agent. The performance of ComNet is tested on composited tasks of the PyDial benchmark. Experiments show that ComNet outperforms vanilla HDRL systems with performance close to the upper bound. It not only achieves sample efficiency but also is more robust to noise while maintaining the transferability to other composite tasks.",63
"  Relation extraction  aims to identify the semantic relations between named entities in text. While previous work  focuses on extracting relations within a sentence, a.k.a.~-level RE, recent studies  have escalated it to the  level, since a large amount of relations between entities usually span across multiple sentences in the real world. According to an analysis on Wikipedia corpus , at least 40.7\% of relations can only be extracted on the document level.  Compared with sentence-level RE, document-level RE requires more complex reasoning, such as logical reasoning, coreference reasoning and common-sense reasoning. A document often contains many entities, and some entities have multiple mentions under the same phrase of alias. To identify the relations between entities appearing in different sentences, document-level RE models must be capable of modeling the complex interactions between multiple entities and synthesizing the context information of multiple mentions.   Figure shows an example of document-level RE. Assume that one wants to extract the relation between ``Surfers Riverwalk"" in S11 and ``Queensland"" in S1. One has to find that ``Surfers Riverwalk"" contains ``Pacific Fair"" , and  ``Pacific Fair""  is located in ``Queensland"" . This chain of interactions helps infer the inter-sentential relation ``located in"" between ``Surfers Riverwalk"" and ``Queensland"".    %====================     In this paper, we proposed GLRE, a global-to-local neural network for document-level RE. Entity global representations model the semantic information of an entire document with R-GCN, and entity local representations aggregate the contextual information of mentions selectively using multi-head attention. Moreover, context relation representations encode the topic information of other relations using self-attention. Our experiments demonstrated the superiority of GLRE over many comparative models, especially the big leads in extracting relations between entities of long distance and with multiple mentions. In future work, we plan to integrate knowledge graphs and explore other document graph modeling ways  to improve the performance.\\   This work is supported partially by the National Key R\&D Program of China , the National Natural Science Foundation of China , and the Water Resource Science \& Technology Project of Jiangsu Province .     The next two lines define the bibliography style to be used, and the bibliography file.         
"," Relation extraction  aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.",64
"  Dialogue state tracker is a core part of the task-oriented dialogue system, which records the dialogue state. The dialogue state consists of a set of  triples, where the specific value represents the user goal, e.g., . The dialogue system responds to the user just based on the dialogue state. Thus, in order to make the dialogue process natural and fluent, it is essential to extract the dialogue state from the dialogue context accurately. However, the paucity of annotated data is the main challenge in this field. In this work, we solve a key problem that how to learn from the unlabeled data in DST task. We design a dual learning framework for DST task, where the dialogue state tracker is the primal agent and the dual agent is the utterance generator. Within the dual learning framework, these two primal-dual agents help to update each other through external reward signals and reconstruction errors by using unlabeled data. It only needs a few of labeled dialogue data to warm up these two primal-dual agents.  However, there are two main challenges when combining dual learning framework with previous dialogue state tracking  methods:  How to represent dialogue state under dual learning framework? Dual learning method is first proposed in the neural machine translation  task. The outputs of the primal-dual agents in NMT task are both sequential natural languages. However, in DST task, the output of the dialogue state tracker consists of isolated domain-slot-value triples. The traditional DST task is formulated as a classification problem with the given ontology, where all the possible values of the corresponding slot are listed. Under this problem definition, the previous classification methods just choose the right value for each slot. The recent innovated tracker TRADE directly generates the values slot by slot using copy mechanism from dialogue context. However, these tracker methods get slot values independently. During the dual learning loop, it is hard to get reward signal from these independent slot values. The reward signal from dual utterance generator is also hard to allocate to these isolated value generation processes. Since the relations of the predicted values are not modeled and they are assumed to be independent with each other, it would face serious reward sparse problem. In this work, we reformulate the dialogue state tracking task as a sequential generation task. The whole dialogue state is represented by a sequence with structured information. For example, the state  can be represented as ``\textless\textgreater  \textless\textgreater  \textless\textgreater   \textless\textgreater  \textless\textgreater  \textless\textgreater  \textless\textgreater''.  [tb]             Is it reasonable that generating the whole dialogue context from dialogue state? The intuitive dual task of the state tracker is dialogue context generation. However, in MultiWOZ 2.1 dataset, the dialogue context has more than 10 turns on average and the average length of each sentence is over 10 tokens. It is very difficult in generating accurately a dialogue context with a dialogue state. Because the dialogue context is too long, it is hard to guarantee that the generated dialogue context contains the same semantics with the given state. In this work, we simplify the dual task into a user utterance generation task which ignores the specific values of the given state. The input of the dual task is composed of two parts , and its output is the delexicalized user utterance. The delexicalized script is copied from the released code \footnote{https://github.com/ConvLab/ConvLab}. The system utterance and user utterance can be lexicalized respectively according to the given turn state. We get a new pseudo-labeled dialogue turn. In order to produce multi-turn pseudo-labeled data, we sample a labeled dialogue data and combine it with the pseudo-labeled dialogue turn, where the dialogue turn directly adds to the end of the sampled dialogue context and the turn state covers into the label of the sampled state. Finally, we get a new dialogue context and pseudo label of the state, as the intuitive dual-task does.  The main contributions of this paper are summarized as follows:          In this work, we first reformulate the dialogue state tracking task as a sequence generation task. Then we adopt a coarse-to-fine decoding method to directly generate the structured state sequence. The proposed coarse-to-fine tracker achieves the best performance among BERT-free methods. The main contribution of this work lies on building a dual learning framework for multi-domain DST task. The experimental results indicate that our proposed dual learning method can efficiently improve the pretrained tracker with unlabeled data. In future work, we will further improve the state tracking model and dual utterance generation model using pretrained models, e.g. BERT.      These are the instructions for authors for IJCAI-20.  \documentclass{article} \pdfpagewidth=8.5in \pdfpageheight=11in   The file ijcai20.sty is NOT the same than previous years' \usepackage{ijcai20}    Use the postscript times font! \usepackage{times} \usepackage{soul} \usepackage{url}   \usepackage[hidelinks]{hyperref} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{amsthm} \usepackage{booktabs} \usepackage{algorithm} \usepackage{algorithmic} \urlstyle{same} \usepackage[bookmarks=false]{hyperref}    the following package is optional:  \usepackage{latexsym}     See https://www.overleaf.com/learn/latex/theorems_and_proofs   for a nice explanation of how to define new theorems, but keep   in mind that the amsthm package is already included in this   template and that you must *not* alter the styling. {Example} {Theorem}    Following comment is from ijcai97-submit.tex:   The preparation of these files was supported by Schlumberger Palo Alto   Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.   Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.   Patel-Schneider, of AT\&T Bell Laboratories collaborated on their   preparation.    These instructions can be modified and used in other conferences as long   as credit to the authors and supporting agencies is retained, this notice   is not changed, and further modification or reuse is not restricted.   Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as   contacts for providing assistance without their prior permission.    To use for other conferences, change references to files and the   conference appropriate and use other authors, contacts, publishers, and   organizations.   Also change the deadline and address for returning papers and the length and   page charge instructions.   Put where the files are available in the appropriate places.  \title{IJCAI--PRICAI--20 Formatting Instructions}    Single author syntax \author{     Christian Bessiere     \affiliations     CNRS, University of Montpellier, France     @example.com, third@other.example.com, fourth@example.com } \fi    
"," In task-oriented multi-turn dialogue systems, dialogue state refers to a compact representation of the user goal in the context of dialogue history. Dialogue state tracking  is to estimate the dialogue state at each turn. Due to the dependency on complicated dialogue history contexts, DST data annotation is more expensive than single-sentence language understanding, which makes the task more challenging. In this work, we formulate DST as a sequence generation problem and propose a novel dual-learning framework to make full use of unlabeled data. In the dual-learning framework, there are two agents: the primal tracker agent  and the dual utterance generator agent . Compared with traditional supervised learning framework, dual learning can iteratively update both agents through the reconstruction error and reward signal respectively without labeled data. Reward sparsity problem is hard to solve in previous DST methods. In this work, the reformulation of DST as a sequence generation model effectively alleviates this problem. We call this primal tracker agent dual-DST. Experimental results on MultiWOZ2.1 dataset show that the proposed dual-DST works very well, especially when labelled data is limited. It achieves comparable performance to the system where labeled data is fully used.",65
"  % P1 intro {T}{ask-oriented} dialog system aims for users to achieve goals such as finding attractions or booking restaurants. Developing such a system typically requires the following dialog components to construct a pipeline as illustrated in \fig : natural language understanding  to extract user's intents and slot-values , dialog state tracking  to update belief states , querying database, dialog policy  to decide the system's next action , and natural language generation  to generate system responses . Although recent advances in neural approaches in the natural language domain have greatly improved the performance of individual dialog components, errors in each component are accumulated in the pipelined system, resulting in degradation of overall performance. Therefore, designing an effective architecture and optimizing the entire dialog system in an end-to-end fashion are still challenging.  % P2 % e2e Recently, several end-to-end  neural dialog systems have proposed . % Modularized % seq2seq Sequence-to-sequence approaches directly generate system responses given user utterance inputs, but they have limitations that querying the external database is unavailable , and system actions are not interpretable . % RL in e2e Moreover, a few previous researchers have investigated dialog policy optimization by reinforcement learning in end-to-end neural task-oriented dialog systems .  % GPT-2 Meanwhile, recent approaches that transfer general linguistic knowledge from large pre-trained language model, GPT-2 , to goal-oriented dialog have shown remarkable improvements . They employed the GPT-2 backbone as it is, and fine-tuned the model to auto-regressively generate dialog states, system actions, and responses in a sequence. Although leveraging the rich knowledge allows the models to generate more natural and appropriate responses, reinforcement learning on transformer-based architectures has been reported as unstable , and learning dialog policy on those models has not been explored yet.  % P4 our approaches    In this paper, we present an end-to-end trainable neural dialog system with reinforcement learning for multi-domain task-completion tasks, SUMBT+LaRL, which consists of two components:  an extended version of SUMBT for a word-level dialog state tracker and  LaRL for a word-level policy model.  In addition to SUMBT that updates belief states employing the slot-utterance matching mechanism, SUMBT+ predicts domains and user-intents from the user utterance.  Then given the predictions by SUMBT+, the LaRL models a categorical latent system action spaces and generates system responses. In our training framework, we emphasize the importance of separately pre-train SUMBT+ and LaRL and fine-tune the entire model in an end-to-end fashion. Then, the trained dialog policy is further optimized by off-line reinforcement learning using REINFORCE algorithm to succeed dialog tasks. During reinforcement training, the policy gradients at latent actions decouple the discourse-level decision making and language generation by the decoder, enabling stable and effective reinforcement learning. We further propose new success  criteria in which the system has to respond to more requestable slots and calculate the match performance using the belief state estimated by SUMBT+.  We demonstrated the efficacy of the proposed system on MultiWOZ2.0, implementing on ConvLab platform for user simulator-based evaluations. Our extensive experimental results on both corpus and simulator-based evaluation shows the effectiveness of the proposed pretraining and end-to-end fine-tuning framework as well as reinforcement learning on the latent action space. From the results and qualitative analysis of simulated dialog examples, we also present the discrepancy problem of corpus and automatic evaluations, the limitations of off-line reinforcement learning for dialog systems, and the needs of advanced reward design and success criteria. Our model achieved the new state-of-the-art success rate in the end-to-end corpus-based evaluation on the MultiWOZ2.0, as well as outperformed the challenge winner of the 8th dialog system technology challenge  challenge in the simulator-based evaluation.  % P5 contribution summary In summary, the main contributions of this paper are three-fold:         % section intro   \sect 2 briefly reviews end-to-end multi-domain task-completion dialog systems and the DSTC8 Challenge. \sect 3 explains the detailed architecture of the proposed SUMBT+LaRL and training procedures. Related work is described in \sect 4 and experimental results are presented in \sect 5.  %\newpage %\clearpage   In this paper, we propose an end-to-end trainable task-oriented dialog system with reinforcement learning that consists of SUMBT+ and LaRL. The SUMBT+ estimates user-acts as well as dialog belief states, and the LaRL models latent system action spaces and generates responses given the estimated contexts. We experimentally demonstrated that the training framework in which the SUMBT+ and LaRL are pretrained, then the entire system is fine-tuned significantly increases dialog success rates. Besides, the end-to-end fine-tuning showed that the supervisions from the next system response also encourages to improve dialog state tracking, achieving the performance comparable to the state-of-the-art joint accuracy. We further trained the model with REINFORCE using the reward levels and success criteria we designed. Using the estimated belief states to assess the reward level 1 attained the best success rates in the end-to-end corpus evaluation, whereas the oracle belief states and reward level 2 achieved the highest in the automatic evaluations. The experiment results and the qualitative analysis of simulated dialog examples present the discrepancy problem of corpus and automatic evaluations, the limitations of off-line reinforcement learning for dialog policy, and the needs of advanced reward design and success criteria. Our model achieved the new state-of-the-art success rate of 85.4\  in end-to-end corpus-based evaluation on MultiWOZ corpus, as well as outperformed the challenge winner of the DSTC8 Track 1 challenge by showing 81.4\   in the simulator-based evaluation.    if have a single appendix:  [Proof of the Zonklar Equations]   or      for no appendix heading   do not use 
"," The recent advent of neural approaches for developing each dialog component in task-oriented dialog systems has remarkably improved, yet optimizing the overall system performance remains a challenge. In this paper, we propose an end-to-end trainable neural dialog system with reinforcement learning, named SUMBT+LaRL. The SUMBT+ estimates user-acts as well as dialog belief states, and the LaRL models latent system action spaces and generates responses given the estimated contexts. We experimentally demonstrate that the training framework in which the SUMBT+ and LaRL are separately pretrained and then the entire system is fine-tuned significantly increases dialog success rates. We propose new success criteria for reinforcement learning to the end-to-end dialog system as well as provide experimental analysis on a different result aspect depending on the success criteria and evaluation methods. Consequently, our model achieved the new state-of-the-art success rate of 85.4\% on corpus-based evaluation, and a comparable success rate of 81.40\% on simulator-based evaluation provided by the DSTC8 challenge.  %The recent advent of neural approaches for developing each dialog component in task-oriented dialog systems has remarkably improved, yet optimizing the overall system performance remains a challenge. In this paper, we propose an end-to-end trainable neural dialog system with reinforcement learning, named SUMBT+LaRL. The SUMBT+ estimates user-acts as well as dialog belief states, and the LaRL models latent system action spaces and generates responses given the estimated contexts. We experimentally demonstrate that the training framework in which the SUMBT+ and LaRL are separately pretrained and then the entire system is fine-tuned significantly increases dialog success rates. We propose new success criteria for reinforcement learning to the end-to-end dialog system as well as provide experimental analysis on a different result aspect depending on the success criteria and evaluation methods. Consequently, our model achieved the new state-of-the-art success rate of 85.4% on corpus-based evaluation, and a comparable success rate of 81.40% on simulator-based evaluation provided by the DSTC8 challenge.",66
"   The massive rise in user-generated web content, alongside with the freedom of speech in social media and anonymity of the users has brought about an increase in online offensive content and anti-social behavior. The consequences of such behavior on genuine users of the social media have become a serious concern for researchers in Natural Language Processing and related fields in recent years.  The shared task number 6 at SemEval 2019, OffensEval , proposes to model the task of offensive language identification hierarchically, which means identifying the offensive content, whether it is targeted, and if so, the target of the offense. In OffensEval, offensive language is defined as ``any form of non-acceptable language  or a targeted offense, which can be veiled or direct'' which includes ``insults, threats, and posts containing profane language or swear words'' .  We have participated in the first two subtasks  of OffensEval with the proposed approach of a deep model consisting of a Recurrent Neural Network  for word-level and Convolutional Neural Network  for character-level processing. Character-level processing is beneficial, as offensive comments are likely to follow unorthodox writing styles, contain obfuscated words, or have irregular word separation which leads to tokenization issues . We also experimented with two other methods, a Support Vector Machine  with TFIDF and count features and another SVM with BERT  -encoded sentences as input, both with lower performances comparing with the deep model.  After overviewing the related work in section , we discuss the methodology and the data in details in section , and the results in section . In section , we analyze the results and conclude the paper in section .       In this paper, we introduced Ghmerti team's approach to the problems of `offensive language identification' and `automatic categorization of offense type' in shared task 6 of SemEval 2019, OffensEval. In subtask A, the neural network-based model outperformed the other methods, including an SVM with word TFIDF and character count features and another SVM with BERT-encoded tweets as input. Furthermore, analysis of the results indicates that sarcastic language, inability to discern the emotions such as anger, and ethnic and racial slurs constitute a considerable portion of the errors. Such deficiencies demand larger training corpora and variety of other features, such as information on sarcasm, emotion, personality, etc.    
"," This paper presents the models submitted by Ghmerti team for subtasks A and B of the OffensEval shared task at SemEval 2019. OffensEval addresses the problem of identifying and categorizing offensive language in social media in three subtasks; whether or not a content is offensive , whether it is targeted  towards an individual, a group, or other entities . The proposed approach includes character-level Convolutional Neural Network, word-level Recurrent Neural Network, and some preprocessing. The performance achieved by the proposed model for subtask A is 77.93\% macro-averaged F\textsubscript{1}-score.",67
"  A wide range of Natural Language Processing  tasks, such as Machine Translation , speech recognition, information retrieval, data mining, and creating text resources for low-resource languages benefit from the upstream task of language identification. The Cuneiform Language Identification  task in VarDial 2019  tries to address the problem of identifying languages and dialects of the texts written in cuneiform symbols.   Identifying languages and dialects of the cuneiform texts is a difficult task, since such languages lack resources and also there is the problem of tokenization. Although there are some work addressing the problem of tokenization in some of these languages or dialects, there is not any universal method or tool available for tokenization of cuneiform texts, as such a task depends on the rules of that language, simply because cuneiform writing system is a syllabic as well as a logographic one.  As a result, all the endeavors in this paper are based on character-level features. This work investigates different machine learning methods which are proven to be effective in text classification and compares them by their obtained F\textsubscript{1}-score, accuracy, and training time.  In this paper, we first review the literature of language identification and the work on languages written using cuneiform writing system in , introduce the models used to tackle the problem of identifying such languages and dialects in , describe the training data in , and discuss the results in .  % You can begin with a brief description of the task and an overview of your approach.   % We would like to ensure that future readers of your paper can find the relevant task description, data and results. So, we ask that you cite the shared task report paper  in your introduction.    In this paper, we investigated different machine learning methods, such as SVM and neural networks, and compared their performance in the task of language and dialect identification of cuneiform texts. The best performance was achieved by a combination of SVM and naive Bayes, using only character-level features. It was shown that characters are enough to obtain at least 72.10\  F\textsubscript{1}-score. However, the best model was not able to achieve a good result classifying some of the dialects which indicates a need for other kinds of features, such as word-level ones, and/or embedded or transferred knowledge of these languages and dialects to be used in training the deep models.    Here you conclude your paper. The readers are interested not only in your system performance but also in what could be learned with your submission.\\    You can also include ideas for future work.   
"," Identification of the languages written using cuneiform symbols is a difficult task due to the lack of resources and the problem of tokenization. The Cuneiform Language Identification task in VarDial 2019 addresses the problem of identifying seven languages and dialects written in cuneiform; Sumerian and six dialects of Akkadian language: Old Babylonian, Middle Babylonian Peripheral, Standard Babylonian, Neo-Babylonian, Late Babylonian, and Neo-Assyrian. This paper describes the approaches taken by  \tt{SharifCL} -score of 72.10\%.",68
"    In recent years, there has been a growing interest in hierarchical multi-label classification  which can be applied in a wide range of applications such as International Patent Classification , product annotation  and advertising recommendation . In the common flat classification problem, each input sample is only associated with a single label from a set of disjoint labels. However, in HMC problem, the labels are organized in the form of a tree or a Directed Acyclic Graph  and each input sample is usually associated with multiple labels, which made it more challenging.  The most straight-forward approach in dealing with HMC problem is to convert it to a flat multi-label classification problem by simply ignoring the relevance between labels . The main disadvantage in doing so is the loss of the useful hierarchical information. Alternatively, the local approach  is designed to perform multi-label classification, where the classifications are carried out at each level of the label hierarchy . The overall classification results are then generated based on these local predictions. While hierarchical information can be better utilized in local approaches,  misclassifications are easily propagated to the next levels . Global approaches are proposed to learn a single global model for all labels to reduce the model size and consider the entire label hierarchy at once . These global classifiers are typically built on flat classifiers with modifications made to integrate the hierarchical information of labels  into the model. Recently, more algorithms which combine the local and global approaches are proposed .  All algorithms introduced above only focus on the design of hierarchical classifier while ignoring the hierarchical features which may be extracted and they are important in HMC as well.  and  consider hierarchical feature extraction in their work. However, the extraction process is designed and fulfilled by applying the typical attention mechanism over the whole text. Since in HMC problem the text may be associated with multiple labels at each hierarchy level, the features extracted from typical attention may be diluted.  We believe it is reasonable to hypothesize that a label-based attention, where information extraction is performed based on different labels at different hierarchical levels, would allow the model to be more interpretable and have an overall better performance in accuracy. Given the above motivations, we propose LA-HCN --- a HMTC model with a label-based attention to facilitate label-based hierarchical feature extraction, where we introduce the concept and mechanism of component which is an intermediate representation that helps bridge the latent association between the words and the labels for label-based attention.     Main contributions of this work:          We proposed LA-HCN, a novel algorithm for HMTC, where label-based attention are learned for the text at different hierarchical levels. Furthermore, both local and global text embeddings are generated for local and global classification respectively. At different levels, meaningful attention can be learned based on different labels. Comprehensive experiments demonstrate the effectiveness of LA-HCN, which outperforms other neural network-based state-of-the-art HMTC algorithms across four benchmark datasets of different properties. Moreover, the visualization of the learned label-based attentions reveals its interpretability. However, the practical meaning of learned components is not well explored so far and may be considered in our future work, where the explicit label structure may also be taken into consideration in the design of the components.   
"," Hierarchical multi-label text classification  has been gaining popularity in recent years thanks to its applicability to a plethora of real-world applications. The existing HMTC algorithms largely focus on the design of classifiers, such as the local, global, or a combination of them. However, very few studies have focused on hierarchical feature extraction and explore the association between the hierarchical labels and the text. In this paper, we propose a Label-based Attention for Hierarchical Mutlti-label Text Classification Neural Network  , where the novel label-based attention module is designed to hierarchically extract important information from the text based on the labels from different hierarchy levels. Besides, hierarchical information is shared across levels while preserving the hierarchical label-based information. Separate local and global document embeddings are obtained and used to facilitate the respective local and global classifications. In our experiments, LA-HCN outperforms other state-of-the-art neural network-based HMTC algorithms on four public HMTC datasets. The ablation study also demonstrates the effectiveness of the proposed label-based attention module as well as the novel local and global embeddings and classifications. By visualizing the learned attention , we find that LA-HCN is able to extract meaningful information corresponding to the different labels which provides explainability that may be helpful for the human analyst.",69
"  Multi-task learning is the problem of minimizing the average error across  tasks, as measured on held-out samples, and motivated by the observation that sometimes learning a single model with partially shared parameters performs better than  single-task models. In the learning-to-learn setting, we worry about our error on a task . Both of these settings apply to randomly initialized base learners, as well as architectures pre-trained on yet another task. In learning-to-learn, the new task  is assumed to come from an ambiguity set defined by the  tasks.  Unsurprisingly, most approaches to multi-task learning minimize the average loss across the training samples available for these tasks. This does not always lead to the best solution, however, since the relations between loss and error may differ across tasks. Several off- and online methods for normalizing these relations have been proposed , but even with this, minimizing average loss across tasks has two disadvantages:  Performance on outlier tasks may be very poor ; and  in the learning-to-learn setting, minimizing average loss is only optimal if the task selection is unbiased .   Minimizing the worst-case loss across tasks instead of the average loss, in theory solves these two problems, which is why this approach is popular in algorithmic fairness  and domain adaptation under covariate shift assumptions . In some multi-task settings, it is possible to directly modify the loss that is minimized in multi-task learning , but this is for example not possible in the common approach to multi-task learning where each batch is sampled from one of  tasks at random . We present a more general approach to multi-task learning with worst-case-aware loss minimization, instead relying on automated curriculum learning .    We present an automated curriculum learning approach to robust multi-task transfer learning. Our approach is general and parameterizes a family of worst-case-aware objectives, with minimax and loss-proportional minimization at the two extremes. In a series of experiments on the GLUE multi-task benchmark , we show that several of these objectives lead to better performance on the benchmark itself, but more importantly, also lead to much better  generalization to other out-of-domain data sets. %Finally, we show that the shared models learned using worst-case-aware curriculum learning also perform better in learning-to-learn settings.        In this work, we introduce a worst-case-aware approach to automated curriculum learning for zero-shot and few-shot applications of multi-task learning architectures. Our models achieves competitive or slightly better average performance on the GLUE benchmark and, more importantly, improves  the performance on outlier tasks. Furthermore, we showed that our worst-case-aware strategies have better  generalization to out-of-domain tasks in zero-shot and few-shot   learning-to-learn settings. Our approach also generally leads to better performance on small tasks. We analyze the learning dynamics of automated curriculum learning in this context and show how it learns a very different sampling strategy from commonly used baseline heuristics.     
"," Multi-task transfer learning based on pre-trained language encoders achieves state-of-the-art performance across a range of tasks. Standard approaches implicitly assume the tasks, for which we have training data, are equally representative of the tasks we are interested in, an assumption which is often hard to justify. This paper presents a more agnostic approach to multi-task transfer learning, %, relying on $\alpha$-ball  which uses automated curriculum learning to minimize a new family of worst-case-aware losses across tasks. Not only do these losses lead to better performance on outlier tasks; they also lead to better performance in zero-shot and few-shot transfer settings.",70
" There is a wide of range of existing natural language processing  toolkits such as CoreNLP , UDPipe , FLAIR , spaCy{State-of-the-art} \\{Performance} & [c]{@{}c@{}}{All Chinese} \\{Fundamental Tasks} & [c]{@{}c@{}}{Multi-task} \\{Learning} \\ \hline       %      CoreNLP        & Java                 &              &                  &                   &            \\        %      spaCy          & Python               &              &                  &                   &            \\       % CoreNLP        & Java                 &              &                  &                   &            \\        LTP       & C++                  &              &                               & $            In this paper, we introduce , a PyTorch-based Chinese natural language processing toolkit for NLP, which built in the SOTA pre-trained model. As shown in Figure, given Chinese corpus,  produces comprehensive analysis results, including lexical analysis, syntactic parsing, and semantic parsing. In addition,  is user-friendly where the fundamental tasks API and visualization tool are provided. %in-depth result analysis. As shown in Table, compared to the existing widely-used NLP toolkits,   has the following advantages:             The existing NLP toolkit for the Chinese language all adopts independent models for each task, which ignores the shared knowledge across tasks.         %     Considering that pipelined approaches usually suffer from error propagation due to their independent models in Chinese NLP tasks,          To alleviate this issue, we propose to use the multi-task framework  to take advantage of the shared knowledge across all tasks.         Meanwhile, multi-task learning with shared encoder for all five tasks can greatly reduce the occupied memory and improve the speed, which makes   more efficient, reducing the need for hardware.          In addition, to enable the multi-task learning enhancing each subtask performance, we follow  to adopt the distillation method single-task models teach a multi-task         model, helping the multi-task model surpass its all single-task teachers.         %                 %   We adopt the multi-task learning with shared encoder for all five tasks, which can greatly reduce the occupied space, making   more efficient.         %    In addition, to enable the multi-task learning enhancing each subtask performance, we follow  to adopt the distillation method single-task models teach a multi-task         %  model, helping the multi-task model surpass its all single-task teachers.         %             supports all Chinese NLP tasks including lexical analysis , syntactic parsing, and semantic parsing .         To the best of our knowledge, this is the first neural Chinese toolkits that supports all Chinese fundamental NLP tasks.      works with users閳 custom modules. Users can easily add a new pre-trained model with a configuration file. We have made all task training configuration file open-sourced. Hence, you can change the pretrained model to any BERT-like model supported by Transformers  easily by changing the config.       First, we provide all fundamental tasks API, which is convenient for users to use the toolkit without the need for any knowledge.   Second, we provide a visualization tool, so that users can view the processing results directly.    We evaluate          on a total of five Chinese NLP tasks, and 閾夸苟d that it achieves state-of-the-art or competitive performance at each task.     is fully open-sourced and can support all Chinese fundamental NLP tasks. We hope  can facilitate Chinese NLP research and applications.    We presented N-LTP, a Python natural language processing toolkit for Chinese. To the best of our knowledge, this is the first neural Chinese toolkit that supports all Chinese fundamental NLP tasks. N-LTP is evaluated on five fundamental Chinese NLP tasks and obtains the state-of-the-art performance. We hope N-LTP can facilitate Chinese NLP research and applications. In the future, we will keep extending N-LTP by adding new state-of-the-art models going forward.    
","   We introduce N-LTP, an open-source Python Chinese natural language processing toolkit supporting five basic tasks: Chinese word segmentation, part-of-speech tagging, named entity recognition, dependency parsing, and semantic dependency parsing. N-LTP adopts the multi-task framework with the pre-trained model to capture the shared knowledge across all Chinese relevant tasks. In addition, we propose to use knowledge distillation where single-task models teach a multi-task model, helping the multi-task model surpass its single-task teachers.   Finally, we provide fundamental tasks API and a visualization tool to make users easier to use and view the processing results directly.   To the best of our knowledge, this is the first toolkit to support all Chinese NLP fundamental tasks.   Source code, documentation, and pre-trained models are available at \url{https://github.com/HIT-SCIR/ltp}.",71
" Building robust task-oriented dialogue systems is challenging due to complex system design and limited availability of human-annotated data. A dialogue agent is expected to learn dialogue reasoning, decision making, and language generation, which require a large amount of training data.  However, collecting and annotating data for training a dialogue system is time-intensive and not transferable among domains. One possible workaround is to leverage the pre-trained language model to reduce human supervision.   Recent progress in pre-training language models has been shown to be promising in alleviating the data scarcity problem.  Such models are typically pre-trained on large-scale plain text with self-supervised objectives, e.g., language modeling and language denoising. Fine tuning pre-trained language models improves a wide range of natural language processing applications, notably machine translation, and personalized dialogue response generation. However, adapting pre-trained language models to task-oriented dialogue systems is not trivial.   Current state-of-the-art  approaches in task-oriented dialogue rely on several tasks-specific modules, such as State Operation Predictor for dialogue state tracking, and CopyNet for end-to-end dialogue task completion.   Such modules are usually absent in the pre-training stage. Therefore, tasks-specific architecture modifications are required in order to adapt pre-trained language models to different dialogue tasks.    In this work, we aim to simplify the process of transferring the prior knowledge of pre-trained language models for improving task-oriented dialogue systems.  We propose Minimalist Transfer Learning , a simple yet effective transfer learning framework that allows to plug-and-play pre-trained sequence-to-sequence  models and jointly learn dialogue state tracking  and dialogue response generation. Unlike previous approaches, which use a copy mechanism to ``carryover'' the previous dialogue states and generate new dialogue states, we introduce Levenshtein belief spans  which models the difference between old states and new states. In practice, MinTL first decodes the  for updating the previous dialogue state; then, the updated state is used to search the external knowledge base; and finally, a response decoder decodes response by conditioning on the dialogue context and knowledge base match result.   MinTL is easy to set up by using different pre-trained seq2seq backbones.  We conduct extensive experiments on both DST and end-to-end dialogue response generation tasks with two pre-trained seq2seq models, such as  T5 and BART. The experimental result on a large-scale task-oriented dialogue benchmark MultiWOZ suggests that our proposed method significantly improves SOTA performance in both the full data and simulated low resource setting. Our contributions are summarized as follows:         framework that efficiently leverages pre-trained language models for task-oriented dialogue without any ad hoc module.     -based systems achieve competitive results compared to the SOTA.     [!t]                 In this paper, we proposed MinTL, a simple and general transfer learning framework that effectively leverages pre-trained language models to jointly learn DST and dialogue response generation. The  is proposed for reducing the DST complexity and improving inference efficiency. In addition, two pre-trained Seq2Seq language models: T5 and BART are incorporated in our framework. Experimental results on MultiWOZ shows that, by using MinTL, our systems not only achieve new SOTA result on both dialogue state tracking and end-to-end response generation but also improves the inference efficiency. In future work, we plan to explore task-oriented dialogues domain-adaptive pre-training methods to enhance our language model backbones, and extend the framework for mixed chit-chat and task-oriented dialogue agents.   
"," In this paper, we propose Minimalist Transfer Learning  to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation.  Unlike previous approaches, which use a copy mechanism to ``carryover'' the old dialogue states to the new one, we introduce Levenshtein belief spans , that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5~ and BART~, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20\% training data, and 3) $Lev$ greatly improves the inference efficiency\footnote{Code available in \url{https://github.com/zlinao/MinTL}}.",72
"   Recent advancements in the area of generative modeling have helped increase the fluency of generative models. However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data .  One reason could be that the generation task is typically construed as an end-to-end system. This is in contrast to traditional approaches, which incorporate a sequence of steps in the NLG system, including content determination, sentence planning, and surface realization . A review of literature from psycholinguistics and cognitive science also provides strong empirical evidence that the human language production process is not a monolith .       Prior approaches have indeed incorporated content planning into the NLG system, for example data-to-text generation problems  as well as classic works that include planning, based on speech acts  .  Our work closely follows these prior approaches, with one crucial difference: our planners are not based on dialogue acts or speech acts.   Consider the example in Fig.. An input utterance by Person B, a statement , followed by a question , can be effectively responded to using plans, learned and generated, prior to the realization phase. The realization output can then include the mention of , consistent with the generated plan .   Dialogue acts  , by their nature, encompass a wide variety of realized output, and hence cannot sufficiently constrain the language model during the generation process. Research has addressed this issue by adapting existing taxonomies  towards their own goals . We instead use an adapted and extended form of lexical-conceptual structures  to help constrain the realization output more effectively .  Our work makes the following contributions: \\ % [nolistsep,noitemsep] %%[nosep,wide]             We address the task of natural language generation in open-ended dialogue systems. We test our hypothesis that decoupling the generation process into planning and realization can achieve better performance than an end-to-end approach.  In the planning phase, we explore three methods to generate response plans, including a Symbolic Planner and two learned planners, the Context Attention and Pseudo Self Attention models. Through linguist expert evaluation, we are able to determine the efficacy of the response plans towards realization. In the realization phase, we use the Pseudo Self Attention model to make use of the learned response plans to generate responses.  	extbf{Our key finding through two separate human crowdsourced studies is that decoupling realization, and planning phases outperforms an end-to-end No Planner system across three metrics .}  In this work, we have taken an initial step towards the goal of replicating human language generation processes. Thorough and rigorous evaluations are required to fully support our claims, e.g., by including additional metrics and more diverse corpora. In this work, we limit the types to  GIVE, GAIN, LOSE, and PERFORM. However, we do not restrict the ask action and target at all. Also, since our symbolic planner can be used to obtain silver standard training data, straightforward changes like adding additional lexicons would enable us to generalize to other corpora as well as include additional ask types in our pipeline. Another natural extension would be to explore training the planning and realization phases together in a hierarchical process . This would, in principle, further validate the efficacy of our approach.   
"," Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation  are typically construed as end-to-end architectures that do not adequately model human generation processes.    To investigate, we decouple generation into two separate phases: planning and realization. In the planning phase, we train two planners to generate plans for response utterances. The realization phase uses response plans to produce an appropriate response. Through rigorous evaluations, both automated and human, we demonstrate that decoupling the process into planning and realization performs better than an end-to-end approach.",73
"   A metaphor is a figurative form of expression that compares a word or a phrase to an object or an action to which it is not literally applicable but helps explain an idea or suggest a likeness or analogy between them. Metaphors have been used extensively in all types of literature and writings, especially in poetry and songs to communicate complex feelings, emotions, and visuals present in the text to readers effectively. Metaphors are ubiquitous in natural language and help in structuring our understanding of the world even without our conscious realization of its presence. Given the prevalence and significance of metaphorical language, effective detection of metaphors plays an essential role in many natural language processing applications, for example, language understanding, information extraction, sentiment analysis, etc.  However, automated detection of metaphorical phrases is a difficult problem primarily due to three reasons. First, there is a subjective component involved: the metaphoricity of expression may vary across humans. Second, metaphors can be domain and context dependent. And third, there is a lack of annotated data, which is required to train supervised machine learning algorithms to facilitate automated detection accurately.   Most of the previous approaches for detection of metaphorical phrases, have either relied on manual and lexical detection which requires heavily handcrafted features built from linguistic resources, that are costly to obtain and greatly limits their applicability or have used supervised machine learning based algorithms with limited forms of linguistic context, for example using only the subject verb objects triplets . Although these techniques automate the detection of metaphorical phrases, however, the prediction accuracies are not as good as the prediction accuracies of these techniques in other text classification tasks.  Inspired by recent works in the field of NLP and transfer learning, in this paper, we present an end-to-end method composed of deep contextualized word embeddings, bidirectional LSTMs and multi-head attention mechanism to address some of the limitations aforementioned. Our method is notable in the sense that unlike many existing approaches, it requires only the raw text sequences as input and does not depend on any complex or fine-tuned feature pipelines.      In this work, we presented an end-to-end method composed of deep contextualized word embeddings, bidirectional LSTMs, and multi-head attention mechanism to address the task of automatic metaphor detection and classification. Our method requires only the raw text sequences as input and does not depend on any complex or fine-tuned feature pipelines. Our method established new state-of-the-art on both the datasets for metaphor detection.     The Appendices part is started with the command ;    appendix sections are then done as normal sections         
"," %% Text of abstract Metaphors are ubiquitous in natural language, and their detection plays an essential role in many natural language processing tasks, such as language understanding, sentiment analysis, etc. Most existing approaches for metaphor detection rely on complex, hand-crafted and fine-tuned feature pipelines, which greatly limit their applicability. In this work, we present an end-to-end method composed of deep contextualized word embeddings, bidirectional LSTMs and multi-head attention mechanism to address the task of automatic metaphor detection. Our method, unlike many other existing approaches, requires only the raw text sequences as input features to detect the metaphoricity of a phrase. We compare the performance of our method against the existing baselines on two benchmark datasets, TroFi, and MOH-X respectively. Experimental evaluations confirm the effectiveness of our approach.",74
"  For text editing, the sequence-to-sequence  framework has been applied to text simplification , punctuation restoration , grammatical error correction , machine translation post-editing , and etc. We observe that current inference methods can be roughly grouped into two categories: End-to-end   and Tagging . For models from both categories, the encoders extract and encode information from the source text sequence. Yet, the goal of the decoders is different for End2end and Tagging. Upon receiving the encoder's hidden states that comprise the source text information, the decoder of End2end directly decodes the hidden states and generates the completely edited target text sequence. But, the decoder of Tagging produces a sequence of editing operations, such as deletion and insertion, that is later applied to the source text to yield the edited text via a realization step . The mechanisms of End2end and Tagging are illustrated in Figure .        We propose a recurrent inference method, Recurrence, that edits a given text sequence iteratively such that in each iteration the programmer determines a single step of editing action and the interpreter executes the action. Our method outperforms the other two inference methods, End2end and Tagging, in three arithmetic equation editing tasks we introduced. For future work, we plan to apply Recurrence to open-domain natural language data and investigate on how to relax its need for intermediate editing steps as extra supervision signals. We also wish to experiment with applying pointer attention  to replace the position component in actions.   
","  In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration , Arithmetic Equation Simplification , Arithmetic Equation Correction . Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods.",75
" There is a broad consensus among grammar formalisms that the composition of form and meaning in natural language is a resource-sensitive process, with the words making up a phrase contributing exactly once to the resulting whole. The sentence ``the Mad Hatter offered'' is ill-formed because of a  of grammatical material, ``offer'' being a ditransitive verb; ``the Cheshire Cat grinned Alice a cup of tea'' on the other hand is ill-formed because of an  of material, which the intransitive verb ``grin'' cannot accommodate.  Given the resource-sensitive nature of language, it comes as no surprise that Linear Logic ,  in particular its intuitionistic version ILL, plays a central role in current logic-based grammar formalisms. Abstract Categorial Grammars and Lambda Grammars use ILL ``as-is'' to characterize an abstract level of grammatical structure from which surface form and semantic interpretation are obtained by means of compositional translations. Modern typelogical grammars in the tradition of the Lambek Calculus, e.g.~Multimodal TLG, Displacement Calculus, Hybrid TLG, refine the type language to account for syntactic aspects  of word order and constituency; ILL here is the target logic for semantic interpretation, reached by a homomorphism relating types and derivations of the syntactic calculus to their semantic counterparts.  A common feature of the aforementioned formalisms is their adoption of the parsing-as-deduction method: determining whether a phrase is syntactically well-formed is seen as the outcome of a process of logical deduction. This logical deduction automatically gives rise to a program for meaning composition, thanks to the remarkable correspondence between logical proof and computation known as the Curry-Howard isomorphism, a natural manifestation of the syntax-semantics interface. The Curry-Howard -terms associated with derivations are neutral with respect to the particular semantic theory one wants to adopt, accommodating both the truth-conditional view of formal semantics and the vector-based distributional view, among others.  Despite their formal appeal, grammars based on variants of linear logic have fallen out of favour within the NLP community, owing to a scarcity of large-scale datasets, but also due to difficulties in aligning them with the established high-performance neural toolkit. Seeking to bridge the gap between formal theory and applied practice, we focus on the  of linear logic, a lean graphical calculus that does away with the bureaucratic symbol-manipulation overhead characteristic of conventional prooftheoretic presentations . Integrating proof nets with recent advances in neural processing, we propose a novel approach to linear logic proof search that eliminates issues commonly associated with higher-order types and hypothetical reasoning, while greatly reducing the computational costs of structure manipulation, backtracking and iterative processing that burden standard parsing techniques .  Our proposed methodology relies on two key components. The first is an encoder/decoder-based supertagger that converts raw text sentences into linear logic judgements by dynamically constructing contextual type assignments, one primitive symbol at a time. The second is a bi-modal encoder that contextualizes the generated judgement in conjunction with the input sentence. The contextualized representations are fed into a Sinkhorn layer, tasked with finding the valid permutation that brings primitive symbol occurrences into alignment. The architecture induced is trained on labeled data, and assumes the role of a formally grounded yet highly accurate parser, which transforms raw text sentences into linear logic proofs and computational terms of the simply typed linear -calculus, further decorated with dependency annotations that allow reconstruction of the underlying dependency graph .     We have introduced neural proof nets, a data-driven perspective on the proof nets of ILL, and successfully employed them on the demanding task of transcribing raw text to proofs and computational terms of the linear -calculus. The terms construed constitute type-safe abstract program skeletons that are free to interpret within arbitrary domains, fulfilling the role of a practical intermediary between text and meaning. Used as-is, they can find direct application in logic-driven models of natural language inference.  Our architecture marks a departure from other parsing approaches, owing to the novel use of the Sinkhorn operator, which renders it both fully parallel and backtrack-free, but also logically grounded. It is general enough to apply to a variety of grammar formalisms inheriting from linear logic; if augmented with Gumbel sampling, it can further a provide a probabilistic means to account for derivational ambiguity. Viewed as a means of exposing deep tecto-grammatic structure, it paves the way for graph-theoretic approaches at syntax-aware sentential meaning representations.  
"," Linear logic and the linear $\lambda$-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on {\AE}thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear $\lambda$-calculus with an accuracy of as high as $70\%$.",76
"  Autoregressive language models are functions that estimate a probability distribution over the next word in a sequence from past words, . This requires capturing statistical dependencies between words over short timescales, where syntactic information likely dominates , as well as long timescales, where semantic and narrative information likely dominate . Because this probability distribution grows exponentially with sequence length, some approaches simplify the problem by ignoring long-range dependencies. Classical -gram models, for example, assume word  is independent of all but the last  words, with typical  . Hidden Markov models  assume that the influence of previous words decays exponentially with distance from the current word .  In contrast, neural network language models such as recurrent  and transformer networks  include longer-range interactions, but simplify the problem by working in lower-dimensional representational spaces. Attention-based networks combine position and content-based information in a small number of attention heads to flexibly capture different types of dependencies within a sequence . Gated recurrent neural networks  compress information about past words into a fixed-length state vector . The influence each word has on this state vector tends to decay exponentially over time. However, each element of the state vector can have a different exponential time constant, or ``timescale'' , enabling gated RNNs like the long short-term memory  network to flexibly learn many different types of temporal relationships . Stacked LSTM networks reduce to a single layer , showing that network depth has an insignificant influence on how the LSTM captures temporal relationships. %Yet in both types of networks this flexibility comes at a cost, since the models must learn the shape of these dependencies from the data. %\ahcomment{rewrote this again substantially. think it's quite a bit better now?} Yet in all these networks the shape of the temporal dependencies must be learned directly from the data. This seems particularly problematic for very long-range dependencies, which are only sparsely informative .  This raises two related questions: what should the temporal dependencies in a language model look like? And how can that information be incorporated into a neural network language model?  To answer the first question, we look to empirical and theoretical work that has explored the dependency statistics of natural language.  quantified temporal dependencies in English and French language corpora by measuring the mutual information between tokens as a function of the distance between them. They observed that mutual information decays as a power law, i.e.\  for constant . This behavior is common to hierarchically structured natural languages  as well as sequences generated from probabilistic context-free grammars  . %While the precise shape of this dependency function may vary between languages and corpora, we shall take as a given that its mathematical form follows a power law.  Now to the second question: if temporal dependencies in natural language follow a power law, how can this information be incorporated into neural network language models? To our knowledge, little work has explored how to control the temporal dependencies learned in attention-based models. However, many approaches have been proposed for controlling gated RNNs, including updating different groups of units at different intervals , gating units across layers , and explicitly controlling the input and forget gates that determine how information is stored and removed from memory . Yet none of these proposals incorporate a specific shape of temporal dependencies based on the known statistics of natural language.  %\vvcomment{Rewrote that last sentence -- claiming that it's unclear how to relate them to this theoretical stuff seems a bit odd since we directly control the input/forget gates to relate them to the theory :D} \ahcomment{LOVE IT}%Yet it is unclear how to relate these largely practical modifications to theoretical properties of how these models capture temporal dependencies.  %\ahcomment{SHOULD WE SAY SOMETHING ABOUT TRANSFORMERS HERE? I'D LIKE TO, BUT I'M NOT EXACTLY SURE WHAT!} %\vvcomment{I'm tempted to claim at the top of the paragraph  that nobody's really thought through how to measure, let alone control, how transformers are encoding temporal dependencies between words. So they are not a good candidate for incorporating this information because we need a way to measure how a specific, controllable mechanism or group of mechanismin the model encode temporal dependencies between words. Then we can say that some thought has been put into this for RNNs...etc.}  %\ahcomment{good suggestion!}  In this work, we build on the framework of  to develop a theory for how the memory mechanism in LSTM language models can capture temporal dependencies that follow a power law. This relies on defining the  of an individual LSTM unit based on how the unit retains and forgets information. We show that this theory predicts the distribution of unit timescales for LSTM %We show that this theory predicts specific characteristics--the distribution of timescales across LSTM units--of  models trained on both natural English  and formal languages . Further, we show that forcing models to follow this theoretical distribution improves language modeling performance. These results highlight the importance of combining theoretical modeling with an understanding of how language models capture temporal dependencies over multiple scales. %dependencies over multiple timescales.   %Effective language models should capture the statistical properties of natural language, including information that varies at multiple timescales. For example, syntactic effects evolve at the timescale of words, whereas semantics, emotions, and narratives can evolve at much longer timescales of tens to hundreds or thousands of words. The importance of long timescale information is evident in results showing that neural networks have outperformed classical n-gram models on many language modeling benchmarks . This difference is attributed to these networks' ability to capture long timescale dependencies that that are impossible for n-gram models. Yet it is difficult to interpret how neural language models represent information at different timescales, and unclear how these timescale representations should be controlled to yield better or more interpretable models.  %One popular architecture for neural language models is recurrent neural networks, in particular Long Short-Term Memory  . Efforts to interpret the representations learned by LSTMs using probing tasks have shown that LSTM language models are capable of learning both short timescale information about word order ~, and long timescale semantic information~. Other methods have attempted to interpret the timescale of LSTM representations using predictive models of brain responses to natural language~. Yet the question of how and where information about different timescales is maintained in LSTM representations still does not have a satisfying answer.  %One alternative to interpreting representations in existing models is to construct language models in which different layers or groups of units are explicitly constrained to operate at different timescales. Several approaches have been proposed for building such explicitly multi-timescale models, including updating different groups of units at different intervals , gating units across layers , and including explicit control of the input and forget gates that determine how information is stored and removed from memory . These approaches ease interpretation by controlling the timescales represented by different units. Yet this raises a new concern: unlike standard LSTMs, explicitly multi-timescale models are unable to flexibly learn the statistics of natural language. This can decrease the performance of these models  and diminish their utility. Thus, when constructing explicitly multi-timescale language models it is important to consider which timescales are present in natural language.  % quantified the distribution of timescales in natural language by measuring the mutual information between tokens as a function of the distance between them. They observed that mutual information decays as a power law, which is common to many hierarchical structures . It would be desirable for a language model to retain temporal information that mimics these statistics. However, it is not clear how to attain power law using LSTMs, which are fundamentally designed to decay information exponentially across time .  %In this work, we present a method to control the timescales of information represented by each unit of an LSTM language model, resulting in interpretable multi-timescale representations. Building on the theoretical grounding of , we quantify the timescale represented in each unit using forget gate activations. We use this framework to analyze an existing LSTM language model  and show how different layers of the model retain information across time. Next, we use this framework to construct explicitly multi-timescale language models where the timescale of each LSTM unit is controlled by setting the forget and input gate biases. To determine the distribution of timescales within this model we used a prior that mimics the power law statistical properties of natural language  through a combination of exponential timescales. Finally, we show that this prior creates interpretable representations in which long and short timescale information is selectively routed into different parts of the network.     In this paper we developed a theory for how LSTM language models can capture power law temporal dependencies. We showed that this theory predicts the distribution of timescales in LSTM language models trained on both natural and formal languages. We also found that explicit multi-timescale models that are forced to follow this theoretical distribution give better performance, particularly over very long timescales. Finally, we show evidence that information dependent on different timescales is routed through specific units, demonstrating that the unit timescales are highly interpretable. This enhanced interpretability makes it possible to use LSTM activations to predict brain data, as in , and estimate processing timescales for different brain regions . These results highlight the importance of theoretical modeling and understanding of how language models capture dependencies over multiple timescales.   We would like to thank Shailee Jain for valuable feedback on the manuscript and useful discussions, and the anonymous reviewers for their insights and suggestions. Funding support for this work came from the Burroughs Wellcome Fund Career Award at the Scientific Interface , Intel Research Award, and Alfred P. Sloan Foundation Research Fellowship.      anthology    \clearpage       anthology       
","  Language models must capture statistical dependencies between words at timescales ranging from very short to very long. %, but how much information is needed for each timescale? Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. %However, it is unclear how power law decay of information should manifest in neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory  language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency  words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % EMNLP version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %Although neural language models are effective at capturing statistics of natural language, their representations are challenging to interpret. In particular, it is unclear how these models retain information over multiple timescales.  %In this work, we construct explicitly multi-timescale language models by manipulating the input and forget gate biases in a long short-term memory  network. %%In this work, we quantify and control the timescale of each unit in a LSTM language model via the the input and forget gate biases.  %The distribution of timescales is selected to approximate power law statistics of natural language through a combination of exponentially decaying memory cells. %%We then design a prior based on statistical properties of natural language and construct a multi-timescale LSTM language model.  %We then empirically analyze the timescale of information routed through each part of the model using word ablation experiments and forget gate visualizations. %%Next, we propose word ablation experiments and forget gate visualizations to interpret the timescale of information routing through the different parts of a model.  %These experiments show that the multi-timescale model successfully learns representations at the desired timescales, and that the distribution includes longer timescales than a standard LSTM.  %%Moreover, it outperforms the standard model on the language modeling task on the Penn Treebank and WikiText-2 datasets, especially on rare words. \ahcomment{change last sentence to point about interpretability} %Further, information about high-, mid-, and low-frequency words is routed preferentially through units with the appropriate timescales. %Thus we show how to construct language models with interpretable representations of different information timescales.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %Shivangi's version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Language models should ideally capture the statistical properties of natural language varying over multiple timescales. However, representations within language models  are challenging to interpret. Hence, it is unclear how different layers of an LSTM LM retain information over different timescales.  % % In this paper, we propose a mechanism to interpret and control the timescale of information routing through an LSTM unit. We observed that a standard LSTM LM favors representations of short timescale information . We then introduce a prior based on the statistical properties of language to control the distribution of the timescales across LSTM units to achieve an effective multi-timescale language model. In addition to this, we present a word ablation experiment and forget gate visualization to interpret the timescale of information routing through the different parts of the model. % % The proposed model learns representations of both short as well as long-timescale information. It also achieves better prediction performance than a standard LSTM LM on Penn Treebank and WikiText-2 datasets, especially on rare words.",77
"          The advancement in the field of Computer Vision ~ and Natural Language Processing ~ over the last decade, has introduced several interesting machine learning techniques. %problems more convenient.          The problems such as object detection~, segmentation~, and  image classification~ in CV, and machine translation~, question answering~, biomedical and clinical text mining~ , speech recognition~ in NLP, are being solved much more efficiently than ever before. This has facilitated the researchers to indulge into solving interdisciplinary problems that demand knowledge of both the fields.                   Visual Question Answering ~ has emerged as one such problem. In VQA, the task is poised as questions being asked with respect to an image, where the machine needs to learn and generate answers of such questions based on the learned features of the input image. In contrast to the typical CV tasks which largely focus on %have singular and          solving problems such as %restricted problems ~ and Inception-Resnet-v2~, respectively. We fuse the representations together and pass it to the specific answer prediction model at the leaf node. For the task of question classification in the root node, we propose a question segregation technique. We use Support Vector Machine ~ as the classifier with hand-engineered and word frequency-based features for QS. We use the machine learning technique for QS, as the rule-based strategy suffers from the problem of defining too many rules that may not extend to other datasets~. The following examples from RAdiology Data ~ show the difficulty of the rule-based approach in the medical domain.                                         Evidence of hemorrhage in the kidneys? \\ { `Yes/No'              Is the spleen present?  \\ { `Others'                                    Careful analysis of the question reveals that the first example expects a descriptive type answer that is to list out the facts that indicate kidney hemorrhage , while the second example expects to confirm the presence/absence of spleen . The presence of such anomalies in the question acts as a hindrance in the formation of robust rules for the classification of questions into their correct type.                  We perform all our experiments in the RAD and  ImageCLEF2018 VQA-Med 2018  datasets, as they perfectly capture the problem statement that we intend to solve. Detailed discussion on the dataset can be found in Section. Experimental evaluation demonstrates promising results, showing the effectiveness of our proposed approach. %'s efficiency.          Additionally, error analysis of the system's outputs %error analysis          shows the future direction in this research area by addressing different kinds of errors.          The organization of this paper is as follows. We first discuss the related work in VQA. Then we present the details of the methodologies that we implemented to solve our specific problem. In particular, we explain our proposed HQS-VQA models in detail. Basically, we discussed the technique used for the question segregation module and the VQA components used to generate the query-specific answers. Details of experiments along with the evaluation results and necessary analysis are reported. %We then perform the experiments and show the results with qualitative and quantitative analysis.          Finally, we conclude and provide the future directions of our work.                 The motivation behind our work are stemmed from the following facts: %of the medical visual question answering are listed as follows:            "", ``{x-ray}"", etc.) and reports for the patients are easily accessible with the increase in the use of medical portals. But the patient still needs to visit a medical expert to fully understand those reports and get answers to their queries. This process is both time-consuming and cost-sensitive. %involves investment of time and money, even to get simple queries answered.          On the other hand, clinicians also find an efficient VQA system very useful to understand the results of a complex medical image. They may use such a system as a second opinion just to boost their self-confidence in the understanding of some specific aspects of such medical images. Although it is possible to search queries on search engines, the search results may be inaccurate, spurious, vague, and, or enormous. In the case of medical reports such inaccurate, spurious, or vague results could lead to serious after-effects. In this context, the VQA in the medical domain is getting attention as an important research problem trying to provide the answer to end-user queries related to medical images.   ' or `No' as the answer. Whereas, the queries from clinicians and medical experts are expected to be more problem-specific, which requires elaborate answers. Again, a skilled trainee is expected to ask more specific and sophisticated questions, while queries from beginners are likely to be simple and straightforward. For example, a naive trainee may inquire about the presence of any abnormality in the image, whereas a senior trainee may identify the abnormality of `intraventricular hemorrhage' from the image, and want to understand more about the grade and effect of the hemorrhage. They can then draw inferences from the acquired data for effective treatment. [!ht]             {-0.9in}{-1.2in}               m{0.45\linewidth}  m{0.2\linewidth}}                 { & {{*}{} & Is this a cyst in the left lung? &  No \\                  & Has the left lung collapsed? & Yes \\                  & Where is the nodule? & Below the 7th rib in the right lung \\                  & What are the densities in both mid-lung fields? & pleural plaques                 \\ \hline                                                                                                                               In this paper, we propose a hierarchical multi-modal approach to tackle the VQA problem in the medical domain. In particular, we use a question segregation module at the top level of our hierarchy to divide the input questions into two different types , followed by individual and independent models at the leaf level, each dedicated to the type of question segregated at the previous level. Our proposed approach can be applied to any related problem where such segregation is possible but it does require non-trivial changes in the architecture.  We use SVM for QS but based on the requirements more rigorous QS techniques can be implemented.          To evaluate the usefulness of our proposed model, we conduct experiments on two different datasets, RAD and CLEF18. We also perform experiments on the combined data of the above two datasets to show the generalisability of our approach. Models, when trained with the proposed hierarchy with QS, scored better, outperforming all the stated baseline models. It suggests that questions with different types learn better in isolation having their individual learning paths. Experimental results indicate the effectiveness of our work, depicting its value for the VQA in the medical domain. We also find out that even simple versions of our model are competitive.          Further analysis of the obtained results reveals that the evaluation metric needs improvement to evaluate VQA in the medical domain. For future work, we plan to investigate a better evaluation strategy for evaluating the task apart from devising a detailed scheme for QS. We also plan to introduce better individual models to handle each of the leaf node tasks. 
","        {        Visual Question Answering in Medical domain  plays an important role in providing medical assistance to the end-users. These users are expected to raise either a straightforward question with a Yes/No answer or a challenging question that requires a detailed and descriptive answer. The existing techniques in VQA-Med fail to distinguish between the different question types sometimes complicates the simpler problems, or over-simplifies the complicated ones. It is certainly true that for different question types, several distinct systems can lead to confusion and discomfort for the end-users. To address this issue, we propose a hierarchical deep multi-modal network that analyzes and classifies end-user questions/queries and then incorporates a query-specific approach for answer prediction. We refer our proposed approach as Hierarchical Question Segregation based Visual Question Answering, in short HQS-VQA.      %   We first use the Support Vector Machine  with the hand-engineered features to classify the questions into yes/no and descriptive types.    % Then, based on the question types, we employ different strategies to provide the answer. The Yes/No type questions are treated as a binary classification problem. We generate the answer from a fixed vocabulary for the descriptive type question.     Our contributions are three-fold, viz. firstly, we propose a question segregation  technique for VQA-Med; secondly, we integrate the QS model to the hierarchical deep multi-modal neural network to generate proper answers to the queries related to medical images; and thirdly, we study the impact of QS in Medical-VQA by comparing the performance of the proposed model with QS and a model without QS. We evaluate the performance of our proposed model on two benchmark datasets, viz. RAD and CLEF18. Experimental results show that our proposed HQS-VQA technique outperforms the baseline models with significant margins. We also conduct a detailed quantitative and qualitative analysis of the obtained results and discover potential causes of errors and their solutions.         }",78
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.      In this work, we focus on the problem of mitigating gender bias in neural dialogue models. We propose an adversarial training framework Debiased-Chat to reduce the bias of a dialogue model during the training process. With the help of a disentanglement model, we design an adversarial learning framework that trains dialogue models to cleverly include unbiased gender features and exclude biased gender features in responses. Experiments on two human conversation datasets demonstrate that our model successfully mitigates gender bias in dialogue models and outperforms baselines by producing more engaging, diverse, and gender-specific responses. In the future, we will investigate debiasing retrieval-based dialogue models and more complicated pipeline-based dialogue systems.    File emnlp2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily       Enter the acl Paper ID here      You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.    \title{Instructions for EMNLP 2020 Proceedings}  \author{First Author \\   Affiliation / Address line 1 \\   Affiliation / Address line 2 \\   Affiliation / Address line 3 \\   email@domain \\\And   Second Author \\   Affiliation / Address line 1 \\   Affiliation / Address line 2 \\   Affiliation / Address line 3 \\   email@domain \\}  \date{}    
"," This document contains the instructions for preparing a manuscript for the proceedings of EMNLP 2020. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.",79
"  The Transformer model has achieved the state-of-the-art performance on various natural language preprocessing  tasks, originally in neural machine translation , and recently in massive multilingual machine translation , crosslingual pretraining , and many other tasks. There has been a growing interest in increasing the model capacity of Transformers, which demonstrates improved performance on various sequence modeling and generation tasks .   %However, there are still two challengee  Training Transformers with increased or variable depth is still an open problem. Depending on the position of layer norm sub-layer, backpropagating gradients through multiple layers may suffer from gradient vanishing . In addition, performance does not always improve by simply stacking up layers . When used for multilingual or multi-task pretraining, such as multilingual machine translation, crosslingual language modeling, etc., the simplicity of using one shared Transformer network for all languages  is appealing. However, how to share model capacity among languages  so as to facilitate positive transfer while mitigating negative transfer has not been well explored.   %how to determine which part of the model to share     In this work, we present a novel approach to train deep Transformers, in which the layers to be used  and the effective depth are not static, but learnt based on the underlying task. Concretely, we model the decision to use each layer as a latent variable, whose distribution is jointly learnt with the rest of the Transformer parameters. %inference network  which acts as layer weighting during training, and  %\asa{I would say: 'At training time we approximate the discrete choice with a Gumbel-Softmax  distribution. The `soft weights' sampled from this distribution also act as gradient normalization for each layer, and this allows us to train very deep Transformers  without using regular layer normalization layers. At inference time, the learnt discrete choice can be used to directly derive a compact model by pruning layers with low probability, but we have the choice of leaving the learned layer selection probabilities as soft weights.} At training time we approximate the discrete choice with a Gumbel-Softmax  distribution. The `soft weights' sampled from this distribution also act as gradient normalization for each layer, and this allows us to train very deep Transformers  without using regular layer normalization layers. At inference time, the learnt discrete choice can be used to directly derive a compact model by pruning layers with low probability, but we have the choice of leaving the learned layer selection probabilities as soft weights. %At inference time, the learnt discrete choice can be used to directly derive a compact model by pruning layers with low probability. Such dynamic ``soft weighting"" also acts as gradient normalization for each layer, and the proposed approach can be used to train very deep Transformer  without using regular layer normalization layers.  By evaluating on WMT'16 English-German machine translation  and masked language modeling  tasks , we show that we can successfully train deeper Transformer  and outperform existing approaches in terms of quality and training stability. %is learnt jointly with the rest of Transfor weighs each layer during training  contribution of  to learn the contribution from each layer by  %automatically learn which Transformer layer to use for each language in multilingual machine translation task. Our approach allows training deep Transformer without vanishing or exploding gradient. As a result, it enables using increased model capacity during training, and at inference time pruning to a more compact model using the layer selection policies. %Our approach learns layer selection optimized for improving translation quality for each language in a single Transformer network. At inference time, the learnt sub-network  %Our contributions are as follows:   We show this approach can be extended to learn task-specific sub-networks by learning different layer selection probabilities for each language pair in multilingual machine translation. %learning multiple ``views"" of layer selection in a shared Transformer.  This result contributes to the growing interest of learning efficient architectures for multi-task and transfer learning in natural language understanding and generation . %\asa{selfish plug to cite BERT and PALs here?}  %Particularly, deep models are trained on the WMT'16 English-German machine translation task, and the crosslingual masked language modeling task , with 64-layer and 96-layer encoders/decoders, respectively.      The main contributions of this paper are as follows. We present a probabilistic framework to learn which layers to select in the Transformer architecture. Based on this framework, we propose a novel method to train one shared Transformer network for multilingual machine translation with different layer selection probabilities for each language pair. The proposed method alleviates the vanishing gradient issue and enables stable training of deep Transformers. We conduct experiments on several tasks to evaluate the proposed approach: WMT'16 English-German machine translation, masked language modeling, and multilingual many-to-one as well as one-to-many machine translation with diverse languages.                 % First, we present a principled approach to utilize layers in the Transformer architecture. Our method learns both  weighting and  selection of Transformer layers. \xian{Is this sentence clear?} %Our method learns latent layers which can be used by both  weighting and  selection in one training without additional distillation steps \asa{This isn't that clear to me?}.  % Second, the proposed method improves vanishing gradient issue and thus enables stable training of deep Transformers. We evaluate deep models with latent layers for the WMT'16 English-German machine translation task  task, and a 96-layer model for crosslingual masked language modeling .  % Third, we show a variant of this approach to learn language-aware layer selections and automatically determine which layers to share in a multilingual Transformer.  % We evaluated the effectiveness of this approach in  machine translation and  masked language modeling with multiple languages. Our work contributes to the growing interest of learning efficient architectures for multi-task and transfer learning in natural language understanding and generation,                                                   %Our results contributes to the, opens the potentials of    %The rest of the paper is organized as follows. We present relevant background in Sections  and present our approach in Section . We present experiments and evaluation results in Section  and an analysis of the proposed approach in Section .      We proposed a novel method to enable training deep Transformers, which learns the effective network depth, by modelling the choice to use each layer as a latent variable.  and automatically learn the effective depth.  Experiments on machine translation and masked language modeling demonstrate that this approach is effective in leveraging increased model capacity and achieves improved quality. We also presented a variant of this method in a multilingual setting where each language can learn its own sub-network with controllable parameter sharing. This approach can be extended to use a shared Transformer for multi-task learning in NLP tasks, and offers insight into which layers are important for which tasks.    \bigskip     
"," %We propose two approaches to improve Transformer for multilingual tasks such as multilingual machine translation, XLM-R, etc. We propose a new method to adaptively learn which layers to share in a multilingual Transformer. Our approach increased the depth of the decoder with stable training. Besides achieving superior quality, the learnt layer selection leads to a compact architecture with reduced inference cost.   %We present a principled approach to utilize layers in Transformers based on task distribution. Our method uses variational inference to learn optimal weighting of each layer. This approach enables training very deep Transformers without vanishing gradients. At inference time, the learnt layer selection posterior can be used for pruning to derive a compact model with reduced depth. We demonstrate the quality improvement from this method in multilingual sequence modeling, specifically multilingual machine translation and crosslingual masked language modeling , compared to strong baselines such as existing layer drop and wide models with similar number of parameters. Further analysis reveals such data-driven layer selection learns both specialization layers as well as common layers shared across languages.       The Transformer model has achieved state-of-the-art performance in many sequence modeling tasks. However, how to leverage model capacity with large or variable depths is still an open challenge. We present a probabilistic framework to automatically learn which layer to use by learning the posterior distributions of layer selection. As an extension of this framework, we propose a novel method to train one shared Transformer network for multilingual machine translation with different layer selection posteriors for each language pair. The proposed method alleviates the vanishing gradient issue and enables stable training of deep Transformers . We evaluate on WMT English-German machine translation and masked language modeling tasks, where our method outperforms existing approaches for training deeper Transformers. Experiments on multilingual machine translation demonstrate that this approach can effectively leverage increased model capacity and bring universal improvement for both many-to-one and one-to-many translation with diverse language pairs.   % In particular, we show that this approach can learn adaptive sub-networks by learning multiple ``views"" of layer selections in one shared Transformer. Our approach enables training deep Transformers  without vanishing gradients.  %training with latent layers.   %For multilingual machine translation we can learn a sub-network for each language pair by using different 'layer selection' probabilities per-language.      %\asa{bring universal improvement -> improves performance?}  %\xian{Here I want to express we can improve both average performance as well as performance for individual languages pairs; commenting discussion for now since I want to have a sharable version while we continue iterating}  % to leverage increased model capacity, which also can be pruned to its ``effective depth"" to derive a compact model.  .     %We present a principled approach to improve the training of Transformers by learning to both ``soft weight"" and ``hard select"" each layer via variational inference.     %This approach allows us to train very deep Transformers without layer normalization. At inference time, the learnt layer selection posterior can be used for pruning to derive a compact model with reduced depth. We evaluate the proposed approach on machine translation and masked languaage modeling tasks, where we can train deeper Transformer up to  100 layers with improved quality. On WMT En-De, and multilingual translation.   %We demonstrate the quality improvement from this method in several sequence modeling tasks such as machine translation , specifically multilingual machine translation and crosslingual masked language modeling , improving on strong baselines such as layer drop as well as wide models with a similar number of parameters. Analyzing the data-driven layer selection, we find bottom layers are shared across all languages, while top layers specialize to a particular language.",80
" In recent years, Transformers  have defined state-of-the-art performance on a variety of NLP tasks, including machine translation  and language modeling. While large Transformer models can learn uniquely rich representations, they are also highly overparameterized . Several studies have therefore attempted to prune Transformers during or after training while retaining as much performance as possible . Some methods have been fairly successful, achieving compression ratios up to 10 depending on the downstream task.  Looking beyond task performance, however, it remains unclear how widely-used pruning methods affect a model's learned representations. For example, a pruned Transformer may translate text at the same BLEU, but does pruning affect the model in ways unaccounted for by this metric?  Motivated by this question, we apply recent analysis techniques to study the representations of increasingly sparse Transformers trained on MT. We perform magnitude pruning in an iterative, lottery-ticket fashion to identify Transformers at competitive sparsities with no drop in task performance . We examine the internal structures of our models as sparsity increases, specifically addressing the following questions:     Using iterative magnitude pruning , we train an En-De Transformer that retains 99.4\% of BLEU at 66.4\% sparsity. During IMP, we obtain eight Transformer models at varying levels of sparsity, along with the original unpruned model. We probe these models' representations for learned linguistic knowledge on eighteen auxiliary syntactic and semantic tasks . We then perform an unsupervised comparison of the representations and attention distributions between dense and sparse models, adopting metrics posed in . Our key conclusions are as follows:           A consistent theme in our analysis is the behavioral shift of early layers , which occurs gradually as sparsity increases. Our probing results find that lower layers of sparse models more directly encode POS and syntax information compared to dense models, even though performance of the final encoder representations is similar . Moreover, our similarity analyses conclude that early layer encoder hidden representations  and attention distributions  trend closer towards their respective final representations in sparse models. Information-theoretically, sparse layers have less maximum capacity for encoding, so each individual layer must shoulder more load for the final representations to remain predictively salient. Conversely, an overparameterized dense model can compensate for weak lower layer representations with its upper layers. Indeed, upper FC layers are pruned more than lower FC layers , reflecting the shift in modeling power away from higher layers.  We also observe a gradual loss of information stored in model representations as weights are pruned, especially in later layers. Individual neurons diverge from their dense counterparts , causing a drop in overall representational complexity in the encoder and decoder. Correspondingly, sparse models perform worse at higher-order semantic tasks that are less relevant to BLEU . The reduced overall complexity of sparse representations may partially explain why final layers are observed to be closer to early layers .  Finally, we find that sparse models' attention distributions remain largely similar to their values in the dense model. This ability to reduce weights in attention modules while maintaining nearly identical representations affirms other lines of work . Of the three attention types, encoder-decoder is pruned least , varies most across sparsities, and exhibits most within-model, inter-layer heterogeneity . These results corroborate existing evidence of its unique importance . Meanwhile, decoder self-attention is extremely homogenous across layers and sparsities, perhaps because encoder-decoder attention is more relevant to creating rich representations.   Our work focuses on pruned Transformers for which BLEU remains similar to the original model. However, BLEU is an imperfect measure of translation quality , and it is possible that our pruned models actually perform worse on the task at lower sparsities than suggested by BLEU. Still, we think our work is relevant given that sparse models are typically only held to the standard of matching unpruned task performance.  Next, we emphasize that our work focuses solely on magnitude pruning, which may not be representative of how other pruning methods impact Transformers. We chose this style of pruning primarily because it allows for higher overall sparsity without drop in performance . Further, while it might be expected  that pruning entire neurons or attention heads would substantially change e.g.~the distributions of the model's outputs, we found less existing work specifically measuring the effects of magnitude pruning. This dearth of analysis seemed particularly egregious given recent growth in work on unstructured sparsity .  Finally, a note on probing classifiers: as has been widely discussed by the community , probes measure correlation between model outputs and auxilliary information. Differences in probe performance do not necessarily imply anything about what information actually uses during its forward pass. Especially since we find some evidence suggesting that sparse models may be encoding information across layers, it is possible that their differing structure may explain worse probe performance, as opposed to fundamentally weaker linguistic feature extraction. We hope future work supplements our results by analyzing a model's encoded knowledge in other ways.    We evaluate how unstructured pruning affects the behavior of Transformers while task performance is maintained. We use probing classifiers to demonstrate that pruning degrades semantic knowledge before affecting BLEU, and that early layers of sparse models better encode low-level linguistic information. Unsupervised similarity analysis reveals that pruning induces representational changes in the encoder and decoder, particularly in higher layers, and that early sparse representations are more similar to their final representations. Meanwhile, attention distributions remain remarkably similar, even at high sparsities.    We thank Yonatan Belinkov and Jonathan Frankle for their advice during the initial stages of the project. We thank Nelson F. Liu for providing access to preprocessed probing datasets.     } {0}  \renewcommand\thetable{\thesection\arabic{table}} {0}      For initial experiments, we trained a linear probe mapping our 1024-dimension token representations to the number of output classes . For subsequent MLP probing, we use an MLP with one 1024-neuron hidden layer with ReLU activation. All weights are trained using Adam with learning rate  for at most 50 epochs with 3 epochs of early stopping patience.   For more complete task descriptions, please refer to . Of the eighteen tasks, five are pairwise, i.e. they involve predicting a property about a pair of tokens. These tasks are syntactic arc prediction, syntactic arc classification, semantic arc prediction, semantic arc classification, and coreference resolution . For these prediction tasks involving pairs of tokens, we input the two token embeddings  and  in addition to their elementwise product  .   Because our model uses Moses tokenization and byte-pair encoding, the source tokens in our preprocessed probing datasets are further split into subtokens by our model. We aggregate subtoken representations by averaging representations. Finally, we noticed that tasks with smaller train/test sets displayed some run-to-run variability, so for these tasks , we report averaged metrics across five replicate runs with different random seeds .  } &   \multicolumn{1}{c}{} &   \multicolumn{1}{c}{} &   \multicolumn{1}{c}{} &   \multicolumn{1}{c}{} \\ \midrule LTH0 & 0.000 & 0.000 & 27.77 & 27.77 \\ LTH1 & 0.168 & 0.200 & 28.04 & 27.59 \\ LTH2 & 0.302 & 0.360 & 28.00 & 27.81 \\  LTH3 & 0.410 & 0.488 & 27.70 & 27.46 \\ LTH4 & 0.496 & 0.590 & 27.93 & 27.24 \\ LTH5 & 0.565 & 0.672 & 27.80 & 26.90 \\ LTH6 & 0.620 & 0.738 & 27.76 & 26.51 \\ LTH7 & 0.664 & 0.790 & 27.61 & 26.14 \\  LTH8 & 0.669 & 0.832 & 27.19 & 25.82 \\ LTH9 & 0.727 & 0.865 & 27.16 & 25.33 \\ \bottomrule                                 
"," Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model's learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. Attention mechanisms remain remarkably consistent as sparsity increases.",81
"  In rule-based machine translation , a linguist formalises linguistic knowledge into lexicons and grammar rules, which is used by the system to analyse sentences in the source language and translate them. While this approach does not require any parallel corpora for training and grants control over the translations created by the system, the process of encoding linguistic knowledge requires a great amount of expert time. Notable examples of RBMT systems are the original, rule-based Systran , Lucy LT  and the Apertium platform .  Instead, corpus-based machine translation  systems learn to translate from examples, usually in the form of sentence-level aligned corpora. On the one hand, this approach is generally computationally more expensive and offers limited control over the generated translations. Furthermore, it is not feasible for language pairs that have limited to no available parallel resources. On the other hand, if parallel resources are available, it boasts a much higher coverage of the targeted language pair. Examples of corpus-based MT paradigms are phrase-based statistical machine translation   and neural machine translation  .  In this work, we focused on leveraging RBMT knowledge for improving the performance of NMT systems in an under-resourced scenario. Namely, we used the information provided by Lucy LT, an RBMT system where the linguistic knowledge is formalised by human linguists as computational grammars, monolingual and bilingual lexicons. Grammars are collections of transformations to annotated trees. Monolingual lexicons are collections of lexical entries, where each lexical entry is a set of feature-value pairs containing morphological, syntactic and semantic information. Bilingual lexicon entries include source-target lexical correspondences and, optionally, contextual conditions and actions. The Lucy LT system divides the translation process into three sequential phases: analysis, transfer, and generation. During the analysis phase, the source sentence is morphologically analysed using a lexicon that identifies each surface form and all its plausible morphological readings. Next, the Lucy LT chart parser together with an analysis grammar consisting of augmented syntactic rules extracts the underlying syntax tree structure and annotates it. The transfer and generation grammars are then applied in succession on that tree, which undergoes multiple annotations and transformations that add information about the equivalences in the target language and adapt the source language structures to the appropriate ones in the target language. Finally, the terminal nodes of the generation tree are assembled into the translated sentence. We focused on the analysis phase, with a special interest for two of the features used: the morphological category  and the inflexion class  or classes of the lexical entries.   %%% NE/TERM Additionally, we focused on two language phenomena that are easily addressable when using RBMT but present a challenge when using corpus-based MT: named entities and terminological expressions.  A named entity  is a word or a sequence of words that unequivocally refer to a real-world object, such as proper nouns, toponyms, numbers or dates. In the context of MT, NEs present different challenges. For example, if an English sentence starts with the word Smith, we do not know a priori if we are dealing with the name of a profession, that will have to be translated, or a proper noun that may have to be left untranslated, or maybe transliterated to a different script. A second issue may arise when using subword units: while word-level models may accidentally preserve an out-of-vocabulary NE, the subword level model will generate a  translation for it. NEs are one of the main out-of-vocabulary word classes, which often cause translation problems that seriously affect the meaning of the sentence .  Similarly, a terminological expression can consist of a single word or a sequence of words that may have a different meaning depending on the context or domain they appear. Hence, the translation for the term might be different depending on the context or domain. Moreover, different contexts and domains may impose additional restrictions on the language used, such as different modes or the use of active or passive voice, and the presence of particular terminology may suggest that a translation is not acceptable even if the meaning of the source sentence is preserved. Accurate terminology translation is crucial to produce adequate translations .  In this work we extend and further analyse the injection of morphological information technique that we proposed in a previous word  and we propose an approach to NEs and terminology that does not rely on any particular technology and can be applied to any MT approach using any kind of resource to detect and translate the NEs and terminological expressions.  To test our proposed approach, we focused on English-Spanish , English-Basque, English-Irish and English-Simplified Chinese language pairs in an under-resourced scenario, using corpora with around one million parallel entries per language pair and domain. Additional test sets that contain several examples of terms, NEs and rich morphology have also been selected and used to further explore the performance of the proposed approaches. Results suggest that, while obtaining results that are not statistically significantly different than the baseline in several scenarios, the proposed approaches show appropriate behaviours such as keeping the passive voice characteristic of some domains.   %Results suggested that adding morphological information to the source language is as effective as using subword units in this particular setting.      In this work, we explored the use of rule-based machine translation  knowledge to improve the performance of neural machine translation  models in an under-resourced scenario, showing that the models had limited ability to learn from the external information.   adding morphological information to the source language is as effective as using subword units in this particular setting.   We also found that RBMT translations were often adequate but both BLEU and TER poorly reflected this, often scoring worse than incorrect NMT-generated translations.  We also tested different approaches to inject named entities  and terminological expressions contained in the RBMT model to NMT. The approaches treat the NMT model as a black box, that is, in such a way that there is no need to know or modify the inner workings of the system, thus being applicable to any model, implementation and architecture. Only the approaches injecting terminology in word-based models improved the baseline, albeit not statistically significantly. In some scenarios, the use of some approaches led to translations that, while not having a significantly different automatic evaluation score, appear to be closer to the style of the targeted text; namely, in the case of terminology translation, some strategies managed to retain the passive voice of the corpus.    One of the paths of our future work will further focus on a more sophisticated extraction of RBMT knowledge. Namely, we plan to use the transfer rules to improve the performance of the NMT model. One of the paths of our future work will further focus on the extraction of RBMT knowledge and the inclusion of transfer rules to improve the performance of the NMT model. The model that was trained following the structure with the parse tree was not able to properly deal with the information, and generally performed worse than the rest; integrating this information differently might produce better results.   Use a second encoder with the RBMT output as input.  A second path is using approaches that modify the architecture of the neural network. For example, using multiple encoders to take both the source sentence and the output of the RBMT system. This approach has been used to improve the performance of NMT. As previously mentioned, corpus-based MT gives limited control over the output to the user, especially when dealing with homographs and terminology; instead, RBMT gives total control. Combining the source sentence with the RBMT output that contains the user-selected translations might lead to improvements in domain-specific or low resource scenarios.    A second improvement path would be using multiple encoders. This approach has been used to improve the performance NMT~, but, in our scenario, one of the inputs would be the output of the RBMT system. As previously mentioned, corpus-based machine translation gives limited control over the output to the user, specially when dealing with homographs and terminology; instead, RBMT gives total control. Combining the source sentence with the RBMT output that contains the user-selected translations might lead to improvements in domain-specific or low resource scenarios.   Use of other sources of information . Finally, we also plan to leverage information contained in other freely available RBMT systems, such as Apertium, that contains features similar to the ones used in this work.   While Apertium is a shallow-transfer system,>Apertium is now deep transfer  meaning that there is less syntactic information, features similar to the ones used in this work are available in Apertium.                     
"," Rule-based machine translation is a machine translation paradigm where linguistic knowledge is encoded by an expert in the form of rules that translate text from source to target language. While this approach grants extensive control over the output of the system, the cost of formalising the needed linguistic knowledge is much higher than training a corpus-based system, where a machine learning approach is used to automatically learn to translate from examples. In this paper, we describe different approaches to leverage the information contained in rule-based machine translation systems to improve a corpus-based one, namely, a neural machine translation model, with a focus on a low-resource scenario. Three different kinds of information were used: morphological information, named entities and terminology. In addition to evaluating the general performance of the system, we systematically analysed the performance of the proposed approaches when dealing with the targeted phenomena. Our results suggest that the proposed models have limited ability to learn from external information, and most approaches do not significantly alter the results of the automatic evaluation, but our preliminary qualitative evaluation shows that in certain cases the hypothesis generated by our system exhibit favourable behaviour such as keeping the use of passive voice. %Our results suggest that adding morphological information to the source language is as effective as using subword units in this particular setting.",82
" Task-oriented dialogue systems are designed to help users achieve predefined goals, such as booking restaurants or movie recommendations via natural language interactions. These systems are deeply connected with external Knowledge Bases  since the system responses are guided by the output from the KB and the dialogue history.   The current state-of-the-arts are end-to-end pipelined systems that rely on Dialogue State Tracking  and Speech Act  annotations. Aside from the annotation cost, which is knowingly high, these pipelined systems must predict a valid DST for querying the KB, execute the query, generate a response template, and finally fulfill it with the retrieved information. The resulting systems are usually overly complicated, and they require multiple steps, including a direct interaction with the KB.   On the other end of the spectrum, there are end-to-end trainable models that use both the KB and the dialogue history as input, and they directly generate system responses. Most of the implementations use either the Gold KB as input or an intermediate API call to retrieve part of the KB . These systems require at least the DST annotation for generating the API calls or to select the gold KB. Moreover, even with the most advanced transformer architecture, end-to-end models struggle when the input becomes too large. For example, in MWOZ, there are 22K entities just for one of the domains. Interested readers can refer to Appendix C for an overview of different task-oriented methodologies.  On the other hand,  discovered a simple yet effective way to query factual knowledge from BERT. Later on,  fine-tuned a pre-trained language model, T5, on just question-answers pairs, without letting the model access any external context or knowledge. These results suggest that the actual knowledge is stored in the model parameters. However, in task-oriented dialogue systems, KB entities do not appear in news articles or Wikipedia, e.g., hotel addresses or postcodes, and thus the aforementioned methods cannot be straightforwardly applied, especially when the KB dynamically changes .  In this paper, we propose a method to store the KB directly into the model parameters using a novel Knowledge Embedded  approach. The resulting model does not use any DST or template responses, nor a KB as input at the inference time, and it can be used in dynamically changing KBs via fine-tuning. The KE approach consists of a newly defined user goal query that generates equivalents KE dialogues from the KB  using minimal annotation effort. Figure shows a high level overview of our approach. To verify the effectiveness of our proposed methodology, we extensively experiment, using both automatic and human metrics, in five task-oriented datasets with small, medium, and large KBs. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all five datasets.  % Additionally, we show that end-to-end models can perform as well as pipelined modularized systems that uses both DST and S-ACT.     In this paper, we propose to learn the KB directly into the model parameters using a novel Knowledge Embedded approach, that is fundamentally different from giving the KB as input or using the DST for querying the KB. We demonstrate that our approach is scalable to different KB sizes and it can be used with dynamically changing KBs via fine-tuning. Automatic and human evaluations confirm that models with embedded KBs achieve competitive performance in all evaluated datasets. Finally we show, for the first time, that end-to-end models can perform as well as pipelined modularized systems in the MWoZ single domain dataset.   
"," %Task-Oriented Dialogue Systems are either modularized with separate dialog %state tracking  and management steps, or end-to-end trainable. In either case, %, and they can be very large. Task-oriented dialogue systems are either modularized with separate dialogue state tracking  and management steps or end-to-end trainable. In either case, the knowledge base  plays an essential role in fulfilling user requests. Modularized systems rely on DST to interact with the KB, which is expensive in terms of annotation and inference time. End-to-end systems use the KB directly as input, but they cannot scale when the KB is larger than a few hundred entries. In this paper, we propose a method to embed the KB, of any size, directly into the model parameters. The resulting model does not require any DST or template responses, nor the KB as input, and it can dynamically update its KB via fine-tuning. We evaluate our solution in five task-oriented dialogue datasets with small, medium, and large KB size. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all evaluated datasets\footnote{Code available in \url{https://github.com/HLTCHKUST/ke-dialogue}}. % The resulting model do not access any external resource during the user interaction, and do not require any KB as input. % to learn to embed structured knowledge of any size directly with model parameters. % % We propose to fine-tune large pre-trained models for task-oriented dialog system with our approach to learning task-specific structured knowledge.   %This has the advantage of  as part of the input nor as an external source during the user interaction.",83
" Open domain question answering~ involves finding answers to questions from an open corpus. The task has led to a growing interest in scalable end-to-end retrieval systems for question answering. Recent neural retrieval models have shown rapid improvements, surpassing traditional information retrieval~ methods such as BM25.   When QA is formulated as a reading comprehension task, cross-attention models like BERT have achieved better-than-human performance on benchmarks such as the Stanford Question Answering Dataset . Cross-attention models are especially well suited for problems involving comparisons between paired textual inputs, as they provide early fusion of fine-grained information within the pair. This encourages careful comparison and integration of details across and within the two texts.   However, early fusion across questions and answers is a poor fit for retrieval, since it prevents pre-computation of the answer representations. Rather, neural retrieval models independently compute embeddings for questions and answers typically using dual encoders for fast scalable search. Using dual encoders results in late fusion within a shared embedding space.  For machine reading, early fusion using cross-attention introduces an inductive bias to compare fine grained text spans within questions and answers. This inductive bias is missing from the single dot-product based scoring operation of dual encoder retrieval models. Without an equivalent inductive bias, late fusion is expected to require additional training data to learn the necessary representations for fine grained comparisons.  To support learning improved representations for retrieval, we explore a supervised data augmentation approach leveraging a complex classification model with cross-attention between question-answer pairs.  Given gold question passage pairs, we first train a cross-attention classification model as the supervisor. Then any collection of questions can be used to mine potential question passage pairs under the supervision of the cross-attention model. The retrieval model training benefits from additional training pairs annotated with the graded predictions from the cross-attention model augmenting, the existing gold data.   Experiments are reported on MultiReQA-SQuAD and MultiReQA-NQ, with retrieval models establishing significant improvements on Precision at   and Mean Reciprocal Rank  metrics.     In this paper, we propose a novel approach for making use of an early fusion classification model to improve late fusion retrieval models. The early fusion model is used to supervised data mining that augments the training data for the later model. The proposed approach mines 53\ ~ and 12\ ~ more examples for MultiRQA-NQ and MultiRQA-SQuAD, respectively. The resulting retrieval models improve +8.6\  and +1.0\  on P@1 on NQ and SQuAD, respectively. The current pipeline assumes there exists annotated in-domain question answer pairs to train the cross-attention model. With a strong general purpose cross-attention model, our supervised data mining method could be modified to train in-domain retrieval models without gold question answer pairs.  We leave this direction to the future work.  
"," Neural models that independently project questions and answers into a shared embedding space allow for efficient continuous space retrieval from large corpora. Independently computing embeddings for questions and answers results in late fusion of information related to matching questions to their answers. While critical for efficient retrieval, late fusion underperforms models that make use of early fusion . We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate classification model with cross-attention between questions and answers. The accurate cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at $N$  and Mean Reciprocal Rank .",84
" Topic models, such as Latent Dirichlet Allocation  , aim to discover underlying topics and semantic structures from text collections. Due to its interpretability and effectiveness, LDA has been extended to many Natural Language Processing  tasks . Most of these models employ mean-field variational inference or collapsed Gibbs sampling  for model inference as a result of their intractable posteriors. However, such inference algorithms are model specific and require dedicated derivations.  To address such limitation, neural topic models with black-box inference have been explored, with more flexible training schemes. Inspired by variational autoencoder  ,  proposed Neural Variational Document Model which interprets the latent code in VAE as topics. Following this way,  adopted the logistic normal prior rather than Gaussian to mimic the simplex properties of topic distribution. Logistic normal is a Laplace approximation to the Dirichlet distribution . However, logistic normal can not exhibit multiple peaks at the vertices of the simplex as the Dirichlet distribution. Therefore, it is less capable of capturing the multi-modality which is crucial for topic modeling .  To overcome such limitation,  proposed Adversarial-neural Topic Model , a topic model based on Generative Adversarial Networks   and sampling topics directly from the Dirichlet distribution to impose a Dirichlet prior. ATM employs a generator transforming randomly sampled topic distributions to word distributions, and an adversarially trained discriminator estimating the probability that a word distribution came from the training data rather than the generator. Although ATM was shown to be effective in discovering coherent topics, it can not be used to induce the topic distribution given a document due to the absence of a topic inference module. Such limitation hinders its application to downstream tasks, such as text classification. Moreover, ATM fails to deal with document labels which can help extract more coherent topics. For example, a document labeled as  more likely belongs to topics such as  or  rather than  or .  To address such limitations of ATM, we propose a novel neural topic modeling approach, named Topic Modeling with Cycle-consistent Adversarial Training . In ToMCAT, topic modeling is cast into the transformation between topic distributions and word distributions. Specifically, the transformation from topic distributions to word distributions is used to interpret topics, and the reverse transformation is used to infer underlying topics for a given document. Under such formulation, ToMCAT employs a generator to transform topic distributions randomly sampled from the Dirichlet prior into the corresponding word distributions, and an encoder to reversely transform documents represented as word distributions into their topic distributions. To encourage the generator/encoder to produce more realistic target samples, discriminators for word/topic distributions are introduced to enable adversarial training. Additional cycle-consistency constraints are utilized to align the learning of the encoder and the generator to prevent them from contradicting each other. Furthermore, for documents with labels, we propose sToMCAT that introduces an extra classifier to regularize the topic modeling process.  The main contributions of the paper are:        We have presented ToMCAT, a neural topic model with adversarial and cycle-consistent objectives, and its supervised extension, sToMCAT. ToMCAT employs a generator to capture semantic patterns in topics and an encoder to encode documents into their corresponding topics. sToMCAT further incorporates document labels into topic modeling. The effectiveness of ToMCAT and sToMCAT is verified by experiments on topic modeling and text classification. In the future, we plan to extend our model to cope with external word or document semantics. It would also be interesting to explore alternative architectures other than CycleGAN under our formulation of topic modeling.  
","   Advances on deep generative models have attracted significant research interest in neural topic modeling.   The recently proposed Adversarial-neural Topic Model models topics with   an adversarially trained generator network   and employs Dirichlet prior to capture the semantic patterns in latent topics.   It is effective in discovering coherent topics but unable to infer topic distributions for given documents   or utilize available document labels.   To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training    and its supervised version sToMCAT.   ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics.   Adversarial training and cycle-consistent constraints are used to   encourage the generator and the encoder to produce realistic samples that coordinate with each other.   sToMCAT extends ToMCAT by incorporating document labels   into the topic modeling process to help discover more coherent topics.   The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and   text classification.   The experimental results show that our models can produce both coherent and informative topics,   outperforming a number of competitive baselines.",85
"  Probabilistic topic models  are tools for discovering main themes from large corpora. The popular Latent Dirichlet Allocation   and its variants  are effective in extracting coherent topics in an interpretable manner, but usually at the cost of designing sophisticated and model-specific learning algorithm. Recently, neural topic modeling that utilizes neural-network-based black-box inference has been the main research direction in this field. Notably, NVDM  employs variational autoencoder   to model topic inference and document generation. Specifically, NVDM consists of an encoder inferring topics from documents and a decoder generating documents from topics, where the latent topics are constrained by a Gaussian prior.  argued that Dirichlet distribution is a more appropriate prior for topic modeling than Gaussian in NVDM and proposed ProdLDA that approximates the Dirichlet prior with logistic normal. There are also attempts that directly enforced a Dirichlet prior on the document topics. W-LDA  models topics in the Wasserstein autoencoders  framework and achieves distribution matching by minimizing their Maximum Mean Discrepancy  , while adversarial topic model  directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network  .  Recently, due to the effectiveness of Graph Neural Networks   in embedding graph structures, there is a surge of interests of applying GNN to natural language processing tasks . For example, GraphBTM  is a neural topic model that incorporates the graph representation of a document to capture biterm co-occurrences in the document. To construct the graph, a sliding window over the document is employed and all word pairs in the window are connected.  A limitation of GraphBTM is that only word relationships are considered while ignoring document relationships. Since a topic is possessed by a subset of documents in the corpus, we believe that the topical neighborhood of a document, i.e., documents with similar topics, would help determine the topics of a document. To this end, we propose Graph Topic Model , a neural topic model that a corpus is represented as a document relationship graph where documents and words in the corpus are nodes and they are connected based on document-word co-occurrences. In GTM, the topical representation of a document node is aggregated from its multi-hop neighborhood, including both document and word nodes, using Graph Convolutional Network  . As GCN is able to capture high-order neighborhood relationships, GTM is essentially capable of modeling both word-word and doc-doc relationships. In specific, the relationships between relevant documents are established by their shared words, which is desirable for topic modeling as documents belonging to one topic typically have similar word distributions.  The main contributions of the paper are:        We have introduced Graph Topic Model, a neural topic model that incorporates corpus-level neighboring context using graph convolutions to enrich document representations and facilitate the topic inference. Both quantitative and qualitative results are presented in the experiments to demonstrate the effectiveness of the proposed approach. In the future, we would like to extend GTM to corpora with explicit doc-doc interactions, e.g., scientific documents with citations or social media posts with user relationships. Replacing GCN in GTM with more advanced graph neural networks is another promising research direction.   
","   Graph Neural Networks    that capture the relationships between graph nodes via message passing   have been a hot research direction   in the natural language processing community.   In this paper, we propose Graph Topic Model , a GNN based neural topic model   that represents a corpus as a document relationship graph.   Documents and words in the corpus become nodes in the graph and   are connected based on document-word co-occurrences.   By introducing the graph structure,   the relationships between documents are established through their shared words   and thus the topical representation of a document is enriched by   aggregating information from its neighboring nodes using graph convolution.   Extensive experiments on three datasets were conducted   and the results demonstrate the effectiveness of the proposed approach.",86
" % {jiaqi: outlines}  % \ys{Need to put more Covid information here. Logic is that we need to do so for covid 19 rather than we have the information and then we can do so. }   In this work, we report the system architecture and results of the team TEST\_POSITIVE in the competition of W-NUT 2020 sharred Task-3: extracting COVID-19 event from Twitter.   Since February 2020, the pandemic COVID-19 has been spreading all over the world, posing a significant threat to mankind in every aspect. The information sharing about a pandemic has been critical in stopping virus spreading. With the recent advance of social networks and machine learning, we are able to automatically detect potential events of COVID cases, and identify key information to prepare ahead.  %   % Users share a wide range of information on social networks. Large platforms, such as Twitter and Facebook, provide sufficient user-generated content for natural language processing applications. For example, massive tweet data posted by users have nourished a variety of applications, e.g. sentiment analysis ~, disaster monitoring ~, event extraction ~ and etc.  We are interested in COVID-19 related event extraction from tweets. With the prevalence of coronavirus, Twitter has been a valuable source of news and information. Twitter users share COVID-19 related topics about personal narratives and news on social media . The information could be helpful for doctors, epidemiologists, and policymakers in controlling the pandemic. However, manual extracting useful information from tremendous amount of tweets is impossible. Hence, we aim to develop a system to automatically extract structured knowledge from Twitter.  % \ys{According to Chieh-Yang, using global model solved the issue of limited annotation, while using the various types of tasks to use all event data to do the training.} Extracting COVID-19 related events from Twitter is non-trivial due to the following challenges: \\  How to deal with limited annotations in heterogeneous events and subtasks?. The creation of the annotated data relies completely on human labors, and thus only a limited amount of data can be obtained in each event categories. There are a variety types of events and subtasks. % Due to the sparsity of the positive samples,  %  the annotation cannot scale up properly and thus only a limited amount of data can be obtained. % The training dataset relies on manual annotation. Hence, we can only obtain a limited number of training data.  Many existing works solve these low resource problem by different approaches, inlcuding crowdsourcing , unsupervised training , or multi-task learning . Here we adopt multi-task training paradigm to benefit from the inter-event and intra-event  information sharing. In this way, \ours learns a shared embedding network globally from all events data. In this way, we implicitly augment the dataset by global training and fine-tuning the language model.    % because our events and subtasks share similarities % to make use of the fundamental relations across different subtasks and events in learning a global embedding network. %  Heterogeneous types of events and subtasks.  Existing work  did not encode the information of different subtask types into the model, while it could be useful in suggesting the candidate slot entity type. In order to make type-aware predictions, we propose a NER-based post-processing procedure in the end of \ours pipeline. We use NER to automatically tag the candidate slots and remove the candidate whose entity type does not match the corresponding subtask type. For example, as shown in Figure, in subtask ``Who'', ``my wife's grandmother'' is a valid candidate slot, while ``old persons home'', tagged as location entity, would be replaced with ``Not Specified'' during the post-processing.   % UK閳 will not be a valid slot for the subtask 閳ユ辅ho閳,as 閳ユ辅ho閳 would require a human-related descrip-tion but 閳ユ矾K閳 is tagged as location-related entityby NER. %   %   % and trains   % tackles each event separately and trains multiple models for different events.   % .}  % To tackle the aforementioned challenges, we propose \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model.  % Built upon a joint event multi-task learning framework, \ours benefits from the training data across all the event types.  % In this way, we implicitly augment the dataset by global training and fine-tuning the embedding parameters.  % Furthermore, we design a type-aware post-processing step to automatically remove the predictions whose entities do not match the corresponding subtask types by leveraging the named entity recognition  .  % For example, ``UK'' will not be a valid slot for the subtask ``who'', as ``who'' would require a human-related description but ``UK'' is tagged as location-related entity by NER. %  % For example, if a predicted slot for subtask ``who'' is tagged with a location related entity, we invalidate the prediction by ``Not Specified''.  %  In summary, \ours is enabled by the following technical contributions:\\ % [itemsep=0mm]    With the unified global training framework, we train and fine-tune the language model across all events and make predictions based on multi-task learning to learn from limited data. \\ %   In this way, \ours benefit from annotated training data of different events and subtasks.\\   We leverage NER tagging on the model predictions and filter out wrong predictions based on subtask types. In this way, \ours benefits from subtask type prior knowledge and further boosts the performance. %     %  %     :Twitter provides sufficient content for NLP tasks. Event extraction is useful in multi application scenarios. COVID-19 situation. %     :  To automatically extract structured knowledge on events. Accurate analysis of COVID-19 tweets can help know the true situation and control the spread.      %     :  Limited annotated data; ;  Type-aware prediction %     : it can be introduced based on the challenges %     :  noisy text data processing;  Joint event multi-task learning;  A well-packaged systematic framework to deal with similar tasks. %    % covid-19 wide spreading  % To automatically extract structured knowledge on events % related to COVID-19 from Twitter is useful for epidemiologists, journalist or policymakers.    % Challenges:  Noisy text in Twitter;  Limited training data;   % In this work, we propose a joint event multi-task learning model for noisy text slot filling tasks with limited training data.  % Our Contributions: %  %        % s   In this work, we build \ours upon a joint event multi-task learning framework. We use NER-based post-processing to generate type-aware predictions. The results show \ours significantly boosts the performance of extracting COVID-19 events from noisy tweets over BERT and CT-BERT baselines. In the future, we would like to extend \ours to open domain event extraction tasks, which is more challenging and requires a more general pipeline.          \clearpage          
","  The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important questions . To tackle these challenges, we propose the \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language model.  Moreover, we implement a type-aware post-processing procedure using named entity recognition  to further filter the predictions. \ours outperforms the BERT baseline by $17.2\%$ in micro F1.\footnote{\url{https://github.com/Chacha-Chen/JOELIN}}    % Extracting structured knowledge from Twitter is non-trivial because:  Limited annotated data: structured knowledge needs to be annotated manually;   Various types of tasks: there are different types of slot filling tasks for different events and subtasks.   % To tackle these challenges, we propose \underline{Jo}int \underline{E}vent Mu\underline{l}ti-task Learn\underline{in}g  model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language embedding parameters.  Moreover, we implement a type-aware post-processing procedure using NER-based techniques to further filter the predictions.\footnote{\url{https://github.com/Chacha-Chen/JOELIN}} %  % \jq{Thanks! Kenneth}",87
"  In the era of digitization, most businesses are turning towards leveraging artificial intelligence  techniques to exploit the information contained in business documents. Traditional information extraction  approaches utilize Natural Language Processing  methods to process the information from documents expressed in the form of natural language text . However, documents contain rich multi-modal information that includes both text and the document layout. The document layout organises the textual information into different formats such as sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues are also indicated through figures/charts/logos etc. and the overall document page appearance. In general, information in a document spans over multiple pages which gives rise to a variety of complex document layouts that can be observed in scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing and understanding these documents is a challenging endeavor and requires a multi-disciplinary perspective combining NLP, computer vision , and knowledge-representation to learn a generic document representation suitable for different downstream applications .  Recent approaches towards document analysis have explored frameworks that utilize information from document text, document layout and document image in different capacities  for specific document tasks.  have proposed joint training of document text and structure for the task of IE from form-like documents, while  combine text and image information for the task of semantic segmentation of documents. Their proposed frameworks optimize the network performance with respect to downstream task which are not suitable for other tasks. To address this limitation,  proposed a pre-training technique based on the BERT transformer architecture , to combine text and layout information from scanned documents. They showcase applicability of their pre-trained network on different downstream tasks further utilizing the image information during fine-tuning for each task. Although  presents a pre-trained framework to learn document representation, there are two limitations to their approach -  the framework only allows for single page documents and  proposed pre-training tasks cannot utilize image information for learning document representation. In the real-world scenario, multi-page documents are common with different pages potentially containing different information across text, layout, and image dimensions. Also, the page image captures the overall layout beyond the appearance of text tokens in the document. Thus, for serving different documents tasks, a unified pre-training framework that learns a generic document representation from all three modalities and works on multi-page documents is necessary.  [t] .     In this paper, we propose such a generic document representation learning framework that takes as input the document text, layout, and image information applicable to different document tasks. Specifically, we encode the multi-modal document information as -  text and position embeddings similar to BERT   text token 2D position embeddings to capture the layout,  text token image embeddings to capture their appearance, and  document page image and position embeddings to learn the document representation capable of handling multi-page documents. In order to handle large token sequences courtesy of multi-page documents, we utilize the Longformer model proposed by  as the backbone of our framework which introduces an attention mechanism that scales linearly with the sequence length. Following the work of , we utilize the Masked Visual Language Modelling  task and a document classification task that enforces the joint pre-training of all the input embeddings. To further ensure the network learns from the image embeddings, we introduce two additional self-supervised pre-training tasks in our framework -  document topic modeling  and  document shuffle prediction . Similar to the work of , we mine the latent topics from the document text and train our framework to predict the topic distribution using only the document page image embeddings for the task of DTM. On the other hand, DSP involves shuffling the page image order while keeping the other embeddings intact for randomly sampled documents during training to identify if the document is tampered with. While DSP task enforces the joint pre-training of the image embeddings with the text and layout embeddings, DTM task helps to learn richer page image embeddings. As explored by different approaches in prior art , we employ a multi-task learning framework to simultaneously train multiple objectives of the different pre-training tasks to learn shared representations across the text, layout, and image modalities of the documents. We train our network on the publicly available ArXiv dataset  which contains millions of research articles spanning a variety of STEM domains such as mathematics, physics, computer science, etc.  Fig.  signifies the applicability of our pre-trained embeddings for different document tasks. We evaluate the performance of our framework on the following tasks and datasets -  Form Understanding and IE from scanned forms    Document Classification    Table Token Classification  and  Document Retrieval . We conduct an exhaustive set of experiments to analyze the performance of our pre-trained embeddings against state-of-the-art  baselines and ablations of our framework. We're able to beat the SOTA baselines trained on comparable dataset size and network parameters for most of these tasks. In summary, the main contributions of this work are:  % We're able to beat the SOTA performance for certain tasks and achieve comparable performance in other cases utilizing only the pre-trained embeddings for fine-tuning on each task. In summary, the main contributions of this work are      We present a multi-modal pre-training framework that utilizes multi-task learning to learn a generic document representation. Our framework encodes the visual, layout and textual information and supports real-world multi-page documents. Our network is pre-trained on the publically available Arxiv dataset utilizing self-supervised tasks that promote learning multi-modal shared representations. We fine-tune our pre-trained network to showcase state-of-the-art performance on different document tasks such as document classification, information extraction and document retrieval. In future, we will investigate pre-training on large datasets such as PublayNet  to analyze the performance gain for different tasks and further explore new architecture designs that will enable document image tasks such as object detection/segmentation using our framework.     We present a multi-modal neural network architecture that utilizes multi-task learning to learn a generic document representation. Our proposed architecture can encode multiple pages while encoding the visual, layout and textual components ubiquitous in real-world PDF documents. We further finetune our architecture across various downstream tasks, and compare our results with existing baselines. Our model significantly outperforms existing baselines in FUNSD, while attains comparable scores in RVL-CDIP, even when pretrained on a much smaller dataset compared to LayoutLM. We also demonstrate that our model is capable table token detection and document retrieval tasks. Novel to our approach, our architecture can utilize visual, layout \& textual components during pretraining and hence can generalize better even when pretrained on a smaller dataset. We also introduce two novel pretraining tasks that helps to learn richer visual representations and enforces joint representation learning for both visual and language modalities. Hence, our model pretrained on all the four pretraining tasks acheives the highest performance across all downstream tasks. We also conduct an ablation to demonstrate the efficacy of the two proposed tasks.     In future research, we will investigate pretraining our architecture on a larger subset of the Arxiv dataset and use the larger PublayNet dataset .   Add more future work   The framework proposed in this paper for learning a generic document representation enables a system to read, understand and interpret digital documents. Such a framework is applicable in a variety of enterprise settings. Typical enterprise applications depend on experts to put in hours of work in collecting, filtering, reading, searching and analysing business documents to mine useful insights for business. Common examples include government officers validating user submitted documents for passport application, loan officers analysing user business documents to ascertain income status of the owner, corporate lawyers analysing contracts to identify loopholes etc. For all of these different scenarios, the upside to using our proposed framework is huge since it dramatically reduces the manual effort for all these different experts in conducting their routine tasks. For e.g., our framework fine-tuned on a dataset of passport applications is capable of analysing and extracting all the submitted fields by the applicant in their application. A system based on our framework deployed with the concerned government agency would assist its officials to quickly go through all the fields and approve/reject the application. Additionally, the officials do not need to acquire any specialised skills or undergo training to understand how the system works. On the other hand, it is difficult to come up with a scenario where our proposed framework can be ill-used without malicious intent. Users can potentially utilize our framework to mine personal information of applicants/employees from enterprise documents. For e.g., a corporate human resources  officer could keep a database of all applicants by mining their personal information from submitted resumes by using our framework that is fine-tuned on a dataset of resumes. Hence, in our opinion, the proposed framework enables decision making for different users by providing document insights which can be used to have both a positive or negative impact.   In the era of digitization, most businesses are turning towards leveraging artificial intelligence  techniques to exploit the information contained in business documents. Traditional information extraction  approaches utilize Natural Language Processing  methods to process the information from documents expressed in the form of natural language text . However, documents contain rich multi-modal information that includes both text and the document layout. The document layout organises the textual information into different formats such as sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues are also indicated through figures/charts/logos etc. and the overall document page appearance. In general, information in a document spans over multiple pages which gives rise to a variety of complex document layouts that can be observed in scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing and understanding these documents is a challenging endeavor and requires a multi-disciplinary perspective combining NLP, computer vision , and knowledge-representation to learn a generic document representation suitable for different downstream applications .  Recent approaches towards document analysis have explored frameworks that utilize information from document text, document layout and document image in different capacities  for specific document tasks.  have proposed joint training of document text and structure for the task of IE from form-like documents, while  combine text and image information for the task of semantic segmentation of documents. Their proposed frameworks optimize the network performance with respect to downstream task which are not suitable for other tasks. To address this limitation,  proposed a pre-training technique based on the BERT transformer architecture , to combine text and layout information from scanned documents. They showcase applicability of their pre-trained network on different downstream tasks further utilizing the image information during fine-tuning for each task. Although  presents a pre-trained framework to learn document representation, there are two limitations to their approach -  the framework only allows for single page documents and  proposed pre-training tasks cannot utilize image information for learning document representation. In the real-world scenario, multi-page documents are common with different pages potentially containing different information across text, layout, and image dimensions. Also, the page image captures the overall layout beyond the appearance of text tokens in the document. Thus, for serving different documents tasks, a unified pre-training framework that learns a generic document representation from all three modalities and works on multi-page documents is necessary.     In this paper, we propose such a generic document representation learning framework that takes as input the document text, layout, and image information applicable to different document tasks. Specifically, we encode the multi-modal document information as -  text and position embeddings similar to BERT   text token 2D position embeddings to capture the layout,  text token image embeddings to capture their appearance, and  document page image and position embeddings to learn the document representation capable of handling multi-page documents. In order to handle large token sequences courtesy of multi-page documents, we utilize the Longformer model proposed by  as the backbone of our framework which introduces an attention mechanism that scales linearly with the sequence length. Following the work of , we utilize the Masked Visual Language Modelling  task and a document classification task that enforces the joint pre-training of all the input embeddings. To further ensure the network learns from the image embeddings, we introduce two additional self-supervised pre-training tasks in our framework -  document topic modeling  and  document shuffle prediction . Similar to the work of , we mine the latent topics from the document text and train our framework to predict the topic distribution using only the document page image embeddings for the task of DTM. On the other hand, DSP involves shuffling the page image order while keeping the other embeddings intact for randomly sampled documents during training to identify if the document is tampered with. While DSP task enforces the joint pre-training of the image embeddings with the text and layout embeddings, DTM task helps to learn richer page image embeddings. As explored by different approaches in prior art , we employ a multi-task learning framework to simultaneously train multiple objectives of the different pre-training tasks to learn shared representations across the text, layout, and image modalities of the documents. We train our network on the publicly available ArXiv dataset  which contains millions of research articles spanning a variety of STEM domains such as mathematics, physics, computer science, etc.  Fig.  signifies the applicability of our pre-trained embeddings for different document tasks. We evaluate the performance of our framework on the following tasks and datasets -  Form Understanding and IE from scanned forms    Document Classification    Table Token Classification  and  Document Retrieval . We conduct an exhaustive set of experiments to analyze the performance of our pre-trained embeddings against state-of-the-art  baselines and ablations of our framework. We're able to beat the SOTA baselines trained on comparable dataset size and network parameters for most of these tasks. In summary, the main contributions of this work are:    We're able to beat the SOTA performance for certain tasks and achieve comparable performance in other cases utilizing only the pre-trained embeddings for fine-tuning on each task. In summary, the main contributions of this work are     In the era of digitization, most businesses are turning towards leveraging artificial intelligence  techniques to exploit the information contained in business documents. Traditional information extraction  approaches utilize Natural Language Processing  methods to process the information from documents expressed in the form of natural language text . However, documents contain rich multi-modal information that includes both text and the document structure. The document structure organises the textual information into different formats such as sections, paragraphs, tables, multi-column etc. utilising different font-types/colors/positions/sizes/styles. Further, important visual cues are also indicated through figures/charts/logos, etc. In general, information in the documents spans over multiple pages and the different document structures give rise to a variety of complex document layouts which can be observed in scientific articles, invoices, receipts, emails, contracts, presentations, blogs, etc. Analyzing and understanding these documents is a challenging endeavor and requires a multi-disciplinary perspective combining NLP, computer vision , and knowledge-representation to learn a generic document representation suitable for different downstream applications .  Although traditional approaches to document processing involve analysing the textual information  for document classification, summarisation etc., recent approaches have explored frameworks that utilize information from text, document structure and document image in different capacities  for specific downstream tasks.  has proposed joint training of document text and structure for the task of IE from form-like documents, while  combine text and image information for the task of semantic segmentation of documents. These approaches propose a framework with the objective of optimizing the network performance w.r.t. the downstream task and do not learn a generic document representation applicable to different downstream tasks. To address these limitations,  proposed a pre-training technique based on the BERT transformer architecture , to combine text and structure information from scanned documents. They incorporate modifications to the BERT pre-training tasks to make it suitable for training on documents and further showcase applicability on different downstream tasks utilizing the image information during fine-tuning for each task. Although  presents a pre-trained framework to learn document representation, there are two limitations to their approach -  the framework only allows for single page documents and  proposed pre-training tasks cannot utilize image information for learning document representation. In the real-world scenario, multi-page documents are common with different pages potentially containing different information across text, structure, and image dimensions. Also, the page image contains important visual cues about different document elements such as tables/figures/charts, etc. and the overall layout beyond the appearance of text tokens in the document. Thus, for serving different documents tasks, a unified pre-training framework that learns a generic document representation from all three modalities and works on multi-page documents is necessary.     In this paper, we propose such a generic document representation learning framework that takes as input the document text, structure, and image information applicable to different document tasks. Specifically, we encode the multi-modal document information as -  text and position embeddings similar to BERT   text token 2D position embeddings to capture the structure,  text token image embeddings to capture their appearance, and  document page image and position embeddings to learn the document representation capable of handling multi-page documents. In order to handle large token sequences courtesy of multi-page documents, we utilize the Longformer model proposed by  as the backbone of our framework which introduces an attention mechanism that scales linearly with the sequence length. Following the work of , we utilize the Masked Visual Language Modelling  task that enforces joint pre-training of the text, structure, and page embeddings and a document classification task that enforces the joint pre-training of all the input embeddings. To further ensure the network learns from the overall page image embeddings, we introduce two additional self-supervised pre-training tasks in our framework -  topic modeling  and  document shuffle prediction . Similar to the work of , we mine the latent topics from the document text and train our framework to predict the topic distribution using only the document page image embeddings for the task of DTM. On the other hand, DSP involves shuffling the page image order while keeping the other embeddings intact for randomly sampled documents during training to identify if the document is tampered with. Both of these tasks enforce the joint pre-training of the page image embeddings with the text and structure embeddings. As explored by different approaches in prior art , we employ a multi-task learning framework to simultaneously train multiple objectives of the different pre-training tasks to learn shared representations across the text, structure, and image modalities of the documents. We train our network on the publicly available ArXiv dataset  which contains millions of research articles spanning a variety of STEM domains such as mathematics, physics, computer science, etc.  Fig.  signifies the applicability of our pre-trained embeddings for different document tasks. We evaluate the performance of our framework on the following tasks and datasets -  Form Understanding and IE from scanned forms    Document Classification    Table Token Classification  and  Document Retrieval . We conduct an exhaustive set of experiments to analyze the performance of our pre-trained embeddings against state-of-the-art  baselines and ablations of our framework. We're able to beat the SOTA baselines trained on comparable dataset size and network parameters for most of these tasks. In summary, the main contributions of this work are:    We're able to beat the SOTA performance for certain tasks and achieve comparable performance in other cases utilizing only the pre-trained embeddings for fine-tuning on each task. In summary, the main contributions of this work are   \def\year{2021}\relax  File: formatting-instructions-latex-2021.tex  release 2021.1 \documentclass[letterpaper]{article}   DO NOT CHANGE THIS  \usepackage[switch]{lineno}  \usepackage{aaai21}    DO NOT CHANGE THIS \usepackage{times}    DO NOT CHANGE THIS \usepackage{helvet}   DO NOT CHANGE THIS \usepackage{courier}    DO NOT CHANGE THIS \usepackage[hyphens]{url}    DO NOT CHANGE THIS \usepackage{graphicx}   DO NOT CHANGE THIS \usepackage{fixltx2e} \urlstyle{rm}   DO NOT CHANGE THIS \def\UrlFont{\rm}    DO NOT CHANGE THIS \usepackage{natbib}    DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption}   DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing    DO NOT CHANGE THIS {8.5in}    DO NOT CHANGE THIS {11in}    DO NOT CHANGE THIS    Leave this   /Title    Put your actual complete title  within the parentheses in mixed case   Leave the space between \Title and the beginning parenthesis alone   /Author    Put your actual complete list of authors  within the parentheses in mixed case.   Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,   remove them. \usepackage{amsfonts}    DISALLOWED PACKAGES   \usepackage{authblk} -- This package is specifically forbidden   \usepackage{balance} -- This package is specifically forbidden   \usepackage{color    \usepackage{CJK} -- This package is specifically forbidden   \usepackage{float} -- This package is specifically forbidden   \usepackage{flushend} -- This package is specifically forbidden   \usepackage{fontenc} -- This package is specifically forbidden   \usepackage{fullpage} -- This package is specifically forbidden   \usepackage{geometry} -- This package is specifically forbidden   \usepackage{grffile} -- This package is specifically forbidden   \usepackage{hyperref} -- This package is specifically forbidden   \usepackage{navigator} -- This package is specifically forbidden       -- This package is specifically forbidden   \usepackage{setspace} -- This package is specifically forbidden   \usepackage{stfloats} -- This package is specifically forbidden   \usepackage{tabu} -- This package is specifically forbidden   \usepackage{titlesec} -- This package is specifically forbidden   \usepackage{tocbibind} -- This package is specifically forbidden   \usepackage{ulem} -- This package is specifically forbidden   \usepackage{wrapfig} -- This package is specifically forbidden   DISALLOWED COMMANDS     {2}  May be changed to 1 or 2 if section numbers are desired.    The file aaai21.sty is the style file for AAAI Press   proceedings, working notes, and technical reports.      Title    Your title must be in mixed case, not sentence case.   That means all verbs ,   nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while   articles, conjunctions, and prepositions are lower case unless they   directly follow a colon or long dash    \title{Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning}   \author {         Author       Anonymous authors \\   }    \author {       Authors         Subhojeet Pramanik\thanks{Equal Contribution}\textsuperscript{\rm 1},         Shashank Mujumdar\textsuperscript{*\rm 2},         Hima Patel\textsuperscript{\rm 2}\\ } \affiliations{         \textsuperscript{1}IBM Cloud, India \\         \textsuperscript{2}IBM Research, India\\         \{subhojeet,shamujum,himapatel\}@in.ibm.com         }   \fi            
"," In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, layout, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and state-of-the-art baselines. We discuss the current limitations and next steps for our work and make the code available to promote future research in this direction.   % In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, structure, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, document table structure detection, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and SOTA baselines.  % To the best of our knowledge, this is the first approach in which multiple pages \& token-level visual information is encoded along with text and layout during pre-training.  % Our model outperforms existing SOTA baselines pre-trained on comparable dataset sizes across various downstream tasks. We discuss the current limitations and next steps for our work and make the code available to promote future research in this direction.",88
"  Discourse coherence has been the subject of much research in Computational Linguistics thanks to its widespread applications . Most current methods can be described as either stemming from explicit representations based on the Centering Theory , or deep learning approaches that learn without the use of hand-crafted linguistic features.  Our work explores a third research avenue based on the Rhetorical Structure Theory  . We hypothesize that texts of low/high coherence tend to adhere to different discourse structures. Thus, we pose that using even silver-standard RST features should help in separating coherent texts from incoherent ones. This stems from the definition of the coherence itself - as the writer of a document needs to follow specific rules for building a clear narrative or argument structure in which the role of each constituent of the document should be appropriate with respect to its local and global context, and even existing discourse parsers should be able to predict a plausible structure that is consistent across all coherent documents. However, if a parser has difficulty interpreting a given document, it will be more likely to produce unrealistic trees with improbable patterns of discourse relations between constituents. This idea was first explored by  , who followed an approach similar to   by estimating entity transition likelihoods, but instead using discourse relations  that entities participate in as opposed to their grammatical roles. Their method achieved significant improvements in performance even when using silver-standard discourse trees, showing potential in the use of parsed RST features for classifying textual coherence.       Our work, however, is the first to develop and test a neural approach to leveraging RST discourse representations in coherence evaluation. Furthermore,  only tested their proposal on the sentence permutation task, which involves ranking a sentence-permuted text against the original. As noted by , this is not an accurate proxy for realistic coherence evaluation. We evaluate our method on their more realistic Grammarly Corpus Of Discourse Coherence , where the model needs to classify a naturally produced text into one of three levels of coherence. Our contributions involve:  RST-Recursive, an RST-based neural tree-recursive method for coherence evaluation that achieves 2\% below the state of the art performance on the GCDC while having 62\% fewer parameters.  When ensembled with the current state of the art, namely Parseq , we achieve a notable improvement over the plain ParSeq model.  We demonstrate the usefulness of silver-standard RST features in coherence classification, and establish our results as a lower-bound for performance improvements to be gained using RST features.    In this paper, we explore the usefulness of silver-standard parsed RST features in neural coherence classification. We propose two new methods, RST-Recursive and Ensemble. The former achieves reasonably good performance, only 2\  short of state of the art, while more robust with 62\  fewer parameters. The latter demonstrates the added advantage of RST features in improving classification accuracy of the existing state of the art methods by setting new state of the art performance with a modest but promising margin. This signifies that the document's rhetorical structure is an important aspect of its perceived clarity. Naturally, this improvement in performance is bounded by the quality of parsed RST features and could increase as better discourse parsers are developed.   In the future, exploring other RST-based architectures for coherence classification, as well as better RST ensemble schemes and improving RST parsing can be avenues of potentially fruitful research. Additional research on multipronged approaches that draw from Centering Theory, RST and deep learning all together can also be of value.                               
"," This paper evaluates the utility of Rhetorical Structure Theory  trees and relations in discourse coherence evaluation. We show that incorporating silver-standard RST features can increase accuracy when classifying coherence. We demonstrate this through our tree-recursive neural model, namely RST-Recursive, which takes advantage of the text's RST features produced by a state of the art RST parser. We evaluate our approach on the Grammarly Corpus for Discourse Coherence  and show that when ensembled with the current state of the art, we can achieve the new state of the art accuracy on this benchmark. Furthermore, when deployed alone, RST-Recursive achieves competitive accuracy while having 62\% fewer parameters.  %This paper explores the impact of silver-standard Rhetorical Structure Theory  trees and relations on discourse coherence evaluation. We show that incorporating discourse features benefits the previous state of the art model and also propose three models based on Recursive Neural Networks. We evaluate our models on the Grammarly Corpus for Discourse Coherence , showing promising results with one model achieving new state of the art performance on discourse classification, and another nearing previous state of the art accuracy. In addition, we provide valuable insights with respect to the application and behaviour of RST relations and trees in discourse analysis, and motivate future work in this area.",89
"    Medical code assignment categorizes clinical documents with sets of codes to facilitate hospital management and improve health record searching~.  These clinical texts comprise physiological signals, laboratory tests, and physician notes, where the International Classification of Diseases  coding system is widely used for annotation. Most hospitals rely on manual coding by human coders to assign standard diagnosis codes to the discharge summaries for billing purposes. However, this work is and error-prone~.  Incorrect coding can cause billing mistakes and mislead other general practitioners when patients are readmitted. Intelligent automated coding systems could act as a recommendation system to help coders to allocate correct medical codes to clinical notes.   Automatic medical code assignment has been intensively researched during the past decades~. Recent advances in natural language processing  with deep learning techniques have inspired many methods for automatic medical code assignment~.   incorporated structured knowledge into medical text representations by preserving translational property of concept embeddings. However, several challenges remain in medical text understanding. Diagnosis notes contain complex diagnosis information, which includes a large number of professional medical vocabulary and noisy information such as non-standard synonyms and misspellings.  Free text clinical notes are lengthy documents, usually from hundreds to thousands of tokens.  Thus, medical text understanding requires effective feature representation learning and complex cognitive process to enable multiple diagnosis code assignment.   Previous neural methods for medical text encoding generally fall into two categories.  Medical text modeling is commonly regarded as a synonym of recurrent neural networks  that capture the sequential dependency. Such works include AttentiveLSTM~, Bi-GRU~ and HA-GRU~.  The other category uses convolutional neural networks  such as  CAML~ and MultiResCNN~.  These methods only capture locality but have achieved the optimal predictive performance on medical code assignment.    Inspired by the generic temporal convolutional network  architecture~, we consider medical text modeling with causal constraints, where the encoding of the current token only depends on previous tokens, using the dilated convolutional network. We combine it with the label attention network for fine-grained information aggregation.    The MultiResNet is currently the state-of-the-art model. It applies multi-channel CNN with different filters to learn features and further concatenates these features to produce a final prediction.  In contrast, our model extends the TCN to sequence modeling that uses a single filter and the dilation operation to control the receptive field. In addition, instead of weight tying used in the TCN, we customize it with label attention pooling to extract relevant rich features.    We contribute to the literature in three ways.  We consider medical text modeling from the perspective of imposing the sequential causal constraint in medical code assignment using dilated convolutions, which effectively captures long sequential dependencies and learns contextual representations in the long clinical notes.   We propose a dilated convolutional attention network , coupling residual dilated convolution, and label attention network for more effective and efficient medical text modeling.    Experiments in real-world medical data show improvement over the state of the art. Compared with multi-channel CNN and RNN models, our model also offers a smaller computational cost.           Recent years extensively studies the automatic medical code assignment. Neural clinical text encoding models use CNNs to extract local features and RNNs to preserve sequential dependency. This paper combines both by using dilated convolution. The dilated convolutional attention network  consists of dilated convolution layers, residual connections, and the label attention layer. The DCAN model obeys the causal constraint of sequence encoding and learns rich representations to capture label-aware importance. Through experiments on the MIMIC-III dataset, our model shows better predictive performance than the state-of-the-art methods.   
"," Medical code assignment, which predicts medical codes from clinical texts, is a fundamental task of intelligent medical information systems. The emergence of deep models in natural language processing has boosted the development of automatic assignment methods.  However, recent advanced neural architectures with flat convolutions or multi-channel feature concatenation ignore the sequential causal constraint within a text sequence and may not learn meaningful clinical text representations, especially for lengthy clinical notes with long-term sequential dependency. This paper proposes a Dilated Convolutional Attention Network , integrating dilated convolutions, residual connections, and label attention, for medical code assignment. It adopts dilated convolutions to capture complex medical patterns with a receptive field which increases exponentially with dilation size. Experiments on a real-world clinical dataset empirically show that our model improves the state of the art.",90
"  The Transformer translation model , which has outperformed previous RNN/CNN based sequence-to-sequence models, is based on multi-head attention networks. The multi-head attention mechanism, which computes several scaled dot-product attention in parallel, can be more efficiently parallelized at sequence level than RNNs , while addressing the drawback of CNNs  which can only model contexts inside a fixed window.  Even though the advantages in parallelization of the multi-head attention mechanism, recent studies  suggest that the computation the scaled dot-product attention is not sufficiently efficient, especially when handling very long sequences, due to the quadratic increasing size of the attention matrix.  In this paper, we study to accelerate the inference of the scaled dot-product attention in another perspective. Specifically, we propose to learn a hard retrieval attention which only attends to one position in the sequence rather than all tokens to simplify the computation of the scaled dot-product attention. Since the hard attention mechanism only attends to one token, the matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can be achieved by a simple and efficient retrieval operation.  Our contributions are as follows:   	     We propose to accelerate the inference of the scaled dot-product attention by learning a hard retrieval attention which only attends to one token in the sentence rather than all tokens. With the one-on-one hard attention matrix, the matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can be replaced by a simple and efficient retrieval operation.  Our hard retrieval attention mechanism can accelerate both long and short sequences and is  times fast as the scaled dot-product attention. In our experiments on a wide range of machine translation tasks, we demonstrate that using the hard retrieval attention for cross attention networks can lead to competitive performance.  
"," The Transformer translation model that based on the multi-head attention mechanism can be parallelized easily and lead to competitive performance in machine translation. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation subspaces at different positions. Though its advantages in parallelization, many previous works suggest the computation of the attention mechanism is not sufficiently efficient, especially when processing long sequences, and propose approaches to improve its efficiency with long sentences. In this paper, we accelerate the inference of the scaled dot-product attention in another perspective. Specifically, instead of squeezing the sequence to attend, we simplify the computation of the scaled dot-product attention by learning a hard retrieval attention which only attends to one token in the sentence rather than all tokens. Since the hard attention mechanism only attends to one position, the matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can be replaced by a simple and efficient retrieval operation. As a result, our hard retrieval attention mechanism can empirically accelerate the scaled dot-product attention for both long and short sequences by $66.5\%$, while performing competitively in a wide range of machine translation tasks when using for cross attention networks.",91
" Neural Machine Translation  has opened up new opportunities in transfer learning from high-resource to low-resource language pairs . While transfer learning has shown great promise, the transfer between languages with different scripts brings additional challenges. For a successful transfer of the embedding layer, both the parent and the child model should use the same or a partially overlapping vocabulary . It is common to merge the two vocabularies by aligning identical subwords and randomly assigning the remaining subwords from the child vocabulary to positions in the parent vocabulary .   This works well for transfer between languages that use the same script, but if the child language is written in an unseen script, most vocabulary positions are replaced by random subwords. This significantly reduces the transfer from the embedding layer.  argue that romanization can improve transfer to languages with unseen scripts. However, romanization can also introduce information loss that might hurt translation quality. In our work, we study the usefulness of romanization for transfer from many-to-many multilingual MT models to low-resource languages with different scripts. Our contributions are the following:            We analyzed the value of romanization for transferring multilingual models to low-resource languages with different scripts. While we cannot recommend romanization as the default strategy for multilingual models and transfer learning across scripts because of the information loss inherent to it, we find that it benefits transfer between related languages that use different scripts. The uconv romanization tool outperforms uroman because it preserves more information encoded in the original script and consequently causes less information loss. Furthermore, we demonstrated that romanization can also be successful on the target side if followed by an additional, learned deromanization step. We hope that our results provide valuable insights for future work in transfer learning and practical applications for low-resource languages with unseen scripts.  
"," Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary.   This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.",92
" Machine learning  models used in practice today are predominantly supervised models and rely on large datasets labeled for training. However, the cost of collecting and maintaining labeled training data remains a bottleneck for training high-capacity supervised models. Data programming aims to address the difficulty of collecting labeled data by using a programmatic approach to weak supervision by heuristics, where domain experts are expected to provide data programs  incorporating their domain knowledge. Prior work on data programming focuses on modeling and aggregating labeling functions written manually or generated automatically to denoise labeling functions.  % However, little is known about user experience  % in writing labeling functions and how to improve it.   Writing data programs can be, however, challenging and time consuming.  Most domain experts or lay users have no or little programming literacy, and even for those who are proficient programmers, it is often difficult to convert domain knowledge to a set of rules by writing programs.       %  By extending data programming with programming by example, we bridge the gap between scalable training data generation and domain experts. To address these challenges, we introduce data programming by demonstration ,  a new framework that aims to make creating labeling functions  easier by learning them from users' interactive visual demonstrations. DPBD moves the burden of writing labeling functions to an intelligent synthesizer while enabling users to steer the synthesis process at multiple semantic levels, from providing rationales relevant for their labeling choices to interactively filtering the proposed functions. DPBD draws from two lines of prior research; programming by demonstration  or example , e.g.,, which aims to make programming easier by synthesizing them based on user interactions or input and output examples, and  interactive learning from user-provided features or rationales .    We operationalize our framework with }.   %  along with the materials and anonymized results of the user study %  \documentclass[sigconf]{acmart} \usepackage[moderate]{savetrees} \usepackage{booktabs} % For formal tables \usepackage{listings} \usepackage{latexsym} \usepackage[sets]{cryptocode} \usepackage{amsmath} \usepackage{amssymb} \usepackage{graphicx} \usepackage{setspace} \usepackage{fullpage} \usepackage{xspace} \usepackage{xcolor} \usepackage{caption} \usepackage{subfigure} \usepackage{courier} \usepackage{enumitem} \usepackage[font=normal,skip=2pt]{caption} \usepackage{times} \usepackage{microtype} \usepackage{balance} % to better equalize the last page \usepackage{xcolor} \usepackage[hang,flushmargin]{footmisc} {8pt plus 2pt minus 2.0pt}  \renewcommand % where to search for the images % Copyright  {}  %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%  %% %% The majority of ACM publications use numbered citations and %% references.  The command  switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%  %% %% end of the preamble, start of the body of the document source. %Conference  % %   % %  %% These commands are optional %% % % % \definecolor{tomato}{rgb}{1,0.2,0} \definecolor{turqoise}{rgb}{0.03, 0.91, 0.87} \definecolor{grey}{rgb}{0.4,0.4,0.4} atay:}\textcolor{tomato}{#1}]}\fi} \DeclareRobustCommand{\textcolor{turqoise}{#1}]}\fi} \DeclareRobustCommand{} \DeclareRobustCommand{\xspace} \DeclareRobustCommand{\ruler}{\mbox{{\mbox{{\mbox{ \DeclareRobustCommand{\thenum}{ten\xspace} } } }} \DeclareRobustCommand{}         {0pt}       {1.5em}       {1em}       {0.5em} } }        \renewcommand{     \title[]{Data Programming by Demonstration:\\A Framework for Interactively Learning Labeling Functions}  \author{Sara Evensen} \affiliation{%    } %  \author{Chang Ge} \authornote{Work done during internship at Megagon Labs.} \affiliation{%    } %  \author{Dongjin Choi} \authornotemark[1] \affiliation{    } %  \author{a\u{g}atay Demiralp} \affiliation{    } %   % \renewcommand{  % \renewcommand{    %     \pagestyle{plain}                         Accessibility is a key to wider adoption of any technology and machine learning is no exception. Here  we introduced  data programming by demonstration , a general human-in-the-loop framework that aims to ease writing labeling functions, improving the accessibility and efficiency of data programming.  We then presented  We evaluate   We recruited 10 participants with Python programming experience through our professional network. All participants had significant programming experience . Their experience with Python programming ranged from  to  years with an average of  years .      We carried out the study using  a within-subjects experiment design, where all participants performed tasks using both conditions .  The sole independent variable controlled was the method of creating labeling functions. We counterbalanced the order in which the tools were used, as well as which classification task we performed with which tool.    We asked participants to write  labeling functions for two prevalent labeling tasks: spam detection and sentiment classification.  They performed these two tasks on  YouTube Comments and Amazon Reviews, respectively. Participants received 15 mins of instruction on how to use each tool, using a topic classification task  over a newsgroup dataset as an example. We asked participants to write as many functions as they considered necessary for the goal of the task.  There were given 30 mins to complete each task and we recorded the labeling functions they created and these functions' individual and aggregate performances.  After completing both tasks, participants also filled out an exit survey, providing their qualitative feedback.  For the manual programming condition, we provided a Jupyter notebook interface based on the Snorkel tutorial. The notebook had a section for writing functions, a section with diverse analysis tools, and a section to train a logistic regression model on the labels generated.       We evaluate   We recruited participants with Python programming experience through our professional network . Note that   We carried out the study using  a within-subjects experiment design, where all participants performed tasks using both conditions .  The sole independent variable controlled was the method of creating labeling functions. We counterbalanced the order in which the tools were used, as well as which classification task we performed with which tool.      We asked participants to write  labeling functions for two prevalent labeling tasks: spam detection and sentiment classification.  They performed these two tasks on  YouTube Comments and Amazon Reviews, respectively. Participants received 15 mins of instruction on how to use each tool, using a topic classification task  over a newsgroup dataset as an example. We asked participants to write as many functions as they considered necessary for the goal of the task.  There were given 30 mins to complete each task and we recorded the labeling functions they created and these functions' individual and aggregate performances.  After completing both tasks, participants also filled out an exit survey, providing their qualitative feedback.    For the manual programming condition, we iteratively developed a Jupyter notebook interface based on the Snorkel tutorial. We provided a section for writing functions, a section with diverse analysis tools, and a section to train a logistic regression model on the labels they had generated.     .         We evaluate our framework against a baseline of manual programming of labeling functions . Their experience with Python programming ranged from  to  years with an average of  years .  Only two participants had used data programming in the past, but all had experience training supervised models and collecting training data.     We asked participants to write  labeling functions for two prevalent labeling tasks, spam detection and sentiment classification.  They performed these two tasks on  YouTube Comments and Amazon Reviews, respectively. We asked participants to write as many functions as they considered necessary for the goal of the task.  They were given 30 mins to complete each task. Participants were also tutored for 15 mins on writing labeling functions using a topic  classification  task on a newsgroup dataset.       Before the experiment began, users were asked to complete a questionnaire that elicited information on their educational background and programming and model development experiences. This way we could ensure that our treatment groups were reasonably balanced across several dimensions.     Each user was scheduled to complete two sessions, never on the same day  and not more than 3 days apart. These sessions were conducted over zoom and began with a 15 minute tutorial to learn how to use the tool by practicing on the newsgroup dataset.   Next, the user was given 30 minutes to complete their assigned task. They were allowed to ask questions and access the internet as desired.   Before each tutorial and task, the user was given as much time as they wanted to read a task description consisting of a short paragraph describing the task and 5 examples from each class.     We cannot expect a 30 minute experiment to be a realistic representation of what generating training data labels is like, but it is likely a good approximation of what the first 30 minutes of generating training data is like. Given constrained resources, we consider this the best method of evaluation.    Throughout each task, we recorded the labeling functions created by participants and these functions' individual and aggregate performances on each task. At the end of the session, participants completed an exit survey to provide their qualitative feedback. After the second such session, the user was asked to complete a final survey comparing the two tools.    For instance,  if spam detection was assigned to be the first task to be completed for a user, and Snorkel the first tool, then in the first session the participant would complete a Snorkel tutorial, then the spam detection task using Snorkel and a survey. On a later day, they would complete a Ruler tutorial, followed by the sentiment analysis task using Ruler, then a survey about Ruler, and a survey comparing the two tools.          Given a dataset  of data records and a set of labels ,  we aim to develop a framework that enables human labelers to interactively assign a label from  for each data record efficiently sampled from   for the labeler to choose from.  Finally, we want the framework to optimally aggregate all the chosen rules  in order to create a labeled training set from  \{tokens \mid conditions\}\Rightarrow  label TlhsrhsrhsTrhsoprhsrhsoprhsop\{ is in the conjunctive form, the order of labeler's interactions does not matter.     If the labeler thinks this data record has a positive sentiment, she can express her decision rationale using GLM. First, she may select two tokens that are related to the sentiment: book and great. Assume there are two concepts the labeler previously created:  itembook, electronics; and  padjwonderful. The labeler realizes the token great can be generalized by the padj concept, which means that the labeling rule will still be valid if this token is replaced by any tokens in the concept, so she adds this token to the concept.  Finally, the labeler creates a positional relationship from book to token great to indicate that they appear in the same sentence, before completing the labeling process. These operations compile into the labeling rule . }  This rule is sent to the synthesizer for expansion and program synthesis.   Given the compiled labeling rule from the labeling interface, the synthesizer extends one single labeling rule from labeler's interaction to a set of more general labeling rules; and translates those labeling rules into computer programs. It is straightforward to translate the rules into executable computer programs , so in this section, we focus on how to synthesize the extended labeling rules.  Given the labeling rule compiled from a labeler's interaction, the synthesizer generates more labeling rules while optimizing two competing goals: maximizing generalization, so that more  data can be accurately labeled; and maximizing the coverage of the labeler's interaction, simply because labeler's interaction is the most valuable signal for labeling based on the domain knowledge. Of course, the larger the set of annotations in an interaction, the larger the set of labeling functions that can be synthesized. To keep rule selection as easy as possible for the user, in this case we prioritize rules that cover more of the interaction, assuming that there is little redundancy.  in the labeler's interaction.  We achieve generalization of the given rules using the following heuristics:  substituting tokens with concepts;  replacing general coexistence relationships with position-specific ones; and  applying the available transformations over the tokens .    Since the labeling rule in GLM has conjunctive conditions, Algorithm  generalizes each predicate in the conditions.    Inside, Line to Line substitute token with concept.   Line can be implemented explicitly by matching token to concept set, as well as sophisticated data-dependent processing via transformation .   For example, in our system for text labeling , in addition to matching values with labeler defined concepts, we also apply named-entity recognition  where the named-entities are implicit concepts that a token can be a member of.    Line to Line replace the positional with co-occurrence relationship by removing the condition that specifies the positional context.   The conditions for extended labeling rules is a conjunctive combination of single predicates, one from each extended condition set .   In addition, for special cases of binary labeling, the algorithm also considers the rule which flips over the label by adding negation to the conditions .     Once the extended rules are generated, the rules are ranked by their generalization score---a measurement of how applicable a certain rule is. We define a data-independent generalization score for a labeling rule  as: x^* x^* = \argmax_x -  p_\theta\log{p_\theta}p_\thetaxL_iD=L=LD^\prime  = D   The data programming by demonstration  framework   has two input sources: the human , and the  that is to be labeled. The labeler is the subject matter expert who has sufficient domain understanding to extract useful signals from data. Given a dataset, our framework enables the labeler to label each record with a categorical label, while providing their labeling rationales by interactively marking relevant parts of the record and specifying semantics and relationships among them.   The output is a labeling model, which is trained to automatically produce labels for the large set of unlabeled data.      Inherited from the traditional data programming~, our framework also assumes that a set of labeled data is available for tuning model hyperparameters.   The DPBD framework has four main components. The labeler interacts with data via the . The labeling interface records the labeler's interaction and compiles the interaction into a labeling rule.   The  synthesizes labeling rules and translates those chosen by the labeler into program functions. Third, the selected functions are passed to the , which builds a labeling model by optimally aggregating the generated functions. Until a certain stopping criterion is met  or the labeler decides to exit, the  selects the next data record to present the labeler.       In the rest of this section, we describe the details of each component.       The labeling interface is the workplace where the labeler encodes domain knowledge into labeling rules. It provides a way to express noisy explanations for labeling decisions using a visual interaction language,  which allows the user to express domain knowledge without having to formalize their ideas into computer programs or natural language explanations. This allows for more focus on patterns in the data while abstracting away any implementation concerns.       Inspired by the entity-relationship model in database modeling, the generalized labeling model  models the data records with  and .  The GLM views the data record as a series of tokens,   where a token is a continuous subset of a record with no semantics attached.    For example, in text data, a token can be any span  of the data record; in an image data record, it would be a 2D region, rectangular or free form; and in an audio data record, it would be a 1D window of the data record .      A  is a group of tokens that the labeler believes share common semantics. For instance, over text data, the labeler might define a concept of positive adjectives consisting of a set of tokens, each of which can imply a positive review.    When labeling audio data, the labeler might create a concept to aggregate all clips that express excitement, or of a specific speaker.  This abstraction allows the user to teach the GLM which generalizations are relevant to the task.    A  represents a binary correlation between token-token, token-concept, or concept-concept. Some examples are membership , co-existence , and positional .              Given the GLM specification described above, our framework also defines the operations that can be applied on GLM elements. Table lists the GLM elements and the corresponding operations. The implementation of both the labeling interface and the operations described in Table would vary across data types and token definitions. To add expressivity, the GLM may also perform transformations over the set of tokens, as we describe in the next section.        Once the labeler finishes annotating an example using the provided operations, and selects a label, the tokens are extracted from the annotation and used    as the initial set of conditions from which to build rules.   The synthesizer combines these conditions into labeling rules by selecting subsets of the conditions to be combined with different conjunctive formulas, according to the relationships the user has annotated.   The synthesizer extends the initial set of labeling rules and presents the extended labeling rules for the labeler to select from, choosing desired ones based on  domain knowledge.    A labeling rule serves as an intermediate language, interpretable by both the labeler and the synthesizer. In our framework, we adapt the notation of domain relational calculus to represent these rules, which can be expressed as:    .   The variable tokens is a sequence of tokens with existential quantification, and    conditions is a conjunctive formula over boolean predicates that is tested over tokens on a data record.     The predicates are first-order expressions, and each can be expressed as a tuple .  is an optional transformation function on a token identifier, a process of mapping the raw token to more generalized forms. Some example transformations are word lemmatization for text labeling, speech-to-text detection in audio labeling, or object recognition in image labeling.      is a token, while  is can be either token, literal or a set.   If  denotes a token, the transformation function  may also apply to .     is an operator whose type depends of the type of .  If  is a token or literal,   detects a positional or an equality relationship. Otherwise, if  is a set,  is one of the set operators  =\{\}=\{\}r: \{t_1, t_2 \mid t_1=book \land t_2  \land idx < idx  \}\Rightarrow positiver G = \prod_{c) is generated using heuristics  and .  Labeling rule~ and~ are synthesized by using heuristics  and , respectively.   Note that labeling rule~ is more general than~ and~ because all data records that can be labeled by~ and~ will be labeled the same way using labeling rule~.    Labeling rules~ are due to flipping over the binary label with heuristics .    }     Once the extended labeling rules are generated, the labeler can help confirm the validity in order to achieve faster convergence.   The top-k candidates ranked by the generalization score are displayed in the labeling interface for the labeler to accept or reject.       The modeler component trains a model that can be used to automatically annotate unlabeled datasets.   Naively aggregating the labeling functions  can be either inaccurate , or does not scale with large set of unlabeled data.    This is simply because labeling functions are noisy: they may overlap, conflict and even depends with each other, and can only provide limited signals in weak supervision.   Instead, the modeler encapsulates the ideas from traditional data programming to first build a generative model to denoise labeling functions, and then train a discriminative model to leverage other features beyond what are expressed by the labeling functions.       To improve the model quality at faster rates, our framework uses an active sampler to choose the next data record for labeling.    The active sampler can plug in any custom active learning policy.  By default, it selects the data record  with the highest    entropy :   where  is the probability that example  belongs to class , as predicted by the trained label model.  Machine learning  models used in practice today are predominantly supervised models and rely on large datasets labeled for training. However, the cost of collecting and maintaining labeled training data remains a bottleneck for training high-capacity supervised models. Data programming aims to address the difficulty of collecting labeled data by using a programmatic approach to weak supervision by heuristics, where domain experts are expected to provide data programs  incorporating their domain knowledge. Prior work on data programming focuses on modeling and aggregating labeling functions written manually or generated automatically to denoise labeling functions.    However, little is known about user experience    in writing labeling functions and how to improve it.   Writing data programs can be, however, challenging and time consuming.  Most domain experts or lay users have no or little programming literacy, and even for those who are proficient programmers, it is often difficult to convert domain knowledge to a set of rules by writing programs.          By extending data programming with programming by example, we bridge the gap between scalable training data generation and domain experts. To address these challenges, we introduce data programming by demonstration ,  a new framework that aims to make creating labeling functions  easier by learning them from users' interactive visual demonstrations. DPBD moves the burden of writing labeling functions to an intelligent synthesizer while enabling users to steer the synthesis process at multiple semantic levels, from providing rationales relevant for their labeling choices to interactively filtering the proposed functions. DPBD draws from two lines of prior research; programming by demonstration  or example , e.g.,, which aims to make programming easier by synthesizing them based on user interactions or input and output examples, and  interactive learning from user-provided features or rationales .    We operationalize our framework with .      along with the materials and anonymized results of the user study    \documentclass[sigconf]{acmart} \usepackage[moderate]{savetrees} \usepackage{booktabs}   For formal tables \usepackage{listings} \usepackage{latexsym} \usepackage[sets]{cryptocode} \usepackage{amsmath} \usepackage{amssymb} \usepackage{graphicx} \usepackage{setspace} \usepackage{fullpage} \usepackage{xspace} \usepackage{xcolor} \usepackage{caption} \usepackage{subfigure} \usepackage{courier} \usepackage{enumitem} \usepackage[font=normal,skip=2pt]{caption} \usepackage{times} \usepackage{microtype} \usepackage{balance}   to better equalize the last page \usepackage{xcolor} \usepackage[hang,flushmargin]{footmisc} {8pt plus 2pt minus 2.0pt}  \renewcommand   where to search for the images   Copyright  {}        Submission ID.    Use this when submitting an article to a sponsored event. You'll    receive a unique submission ID from the organizers    of the event, and this ID should be used as the parameter to this command.           The majority of ACM publications use numbered citations and    references.  The command  switches to the    ""author year"" style.       If you are preparing content for an event    sponsored by ACM SIGGRAPH, you must use the ""author year"" style of    citations and references.    Uncommenting    the next command will enable that style.           end of the preamble, start of the body of the document source.  Conference                These commands are optional          \definecolor{tomato}{rgb}{1,0.2,0} \definecolor{turqoise}{rgb}{0.03, 0.91, 0.87} \definecolor{grey}{rgb}{0.4,0.4,0.4} atay:}{#1}]}\fi} \DeclareRobustCommand{{#1}]}\fi} \DeclareRobustCommand{} \DeclareRobustCommand{\xspace} \DeclareRobustCommand{\ruler}{\mbox{{\mbox{{\mbox{ \DeclareRobustCommand{\thenum}{ten\xspace} } } }} \DeclareRobustCommand{}    \author{Sara Evensen} \affiliation{     }    \author{Chang Ge} \authornote{Work done during internship at Megagon Labs.} \affiliation{     }    \author{Dongjin Choi} \authornotemark[1] \affiliation{    }    \author{a\u{g}atay Demiralp} \affiliation{    }       \renewcommand{    \renewcommand{        \maketitle  \pagestyle{plain}                    
"," % problem & importance   Data programming is a programmatic weak supervision approach to efficiently curate large-scale labeled training data. Writing data programs  requires, however, both programming literacy and domain expertise. Many subject matter experts have neither programming proficiency nor time to effectively write data programs. Furthermore, regardless of one's expertise in coding or machine learning, transferring domain expertise into labeling functions by enumerating rules and thresholds is not only time consuming but also inherently difficult.  % proposed solution  Here we propose a new framework, data programming by demonstration , to generate labeling rules using interactive demonstrations of users. DPBD aims to relieve the burden of writing labeling functions from users, enabling them to focus on higher-level semantics such as identifying relevant signals for labeling tasks.  We operationalize our framework with \system, an interactive system that synthesizes labeling rules for document classification by using span-level annotations of users on document examples.  % evidence that it works  We compare \system with conventional data programming  through a user study conducted with 10 data scientists creating labeling functions for sentiment and spam classification tasks.  We find that \system is easier to use and learn  and offers higher overall satisfaction, while providing discriminative model performances comparable to ones achieved by conventional data programming.",93
"      Deep neural networks are typically trained on a large amount of a single task data through a time-consuming optimization phase. This assumes that the distribution over data points is fixed. However, such neural models do not scale to complex, realistic environments and are prone to distributional shifts and adversarial data points. Online learning on the other hand does not make any distributional assumption and naturally involves an adversarial scenario. However, due to the larger number of training parameters and non-convex optimization landscape, the deep neural networks are hard to train in online settings.     % where the data points are made available over time in an streaming fashion.          {r}{0.6\textwidth}                         $ is exposed to two types of update: the loss gradient and the fast-weights updates. The fast-weights are generated by a meta-learner network based on the gradients w.r.t the slow-weights. The fast-weights maintain information at a time scale that is longer than the network activations, but shorter than the slow-weights, enabling a fast adaptation.         }                     \vskip -0.45in               Meta-learning  has emerged as a promising technique for fast training of deep neural networks by acquiring and transferring knowledge across different tasks through a learned learning algorithm. This work proposes a meta-learning approach to learn sequential adaptation algorithms for deep neural networks. We introduce a sparse variant of Meta Networks to perform an online and continual fast adaptation of deep neural networks over a data stream with non-stationary distribution.           In Sparse Meta Networks , fast-weights are generated sparsely at each step by a meta-learner and accumulated across multiple steps. When the sparse fast-weights are accumulated in this way, across different tasks, they all together act as an mixture of multiple experts in a single Sparse-MetaNet model. Such sparsely generated recurrent fast-weights are not only computationally efficient; and thus can be applied with large scale deep neural networks, but also crucial to maintain a far past memory over the streaming data.           To demonstrate the effectiveness of our approach, we introduce a new vision based benchmark called Online Cifar. In the Online Cifar setup, Sparse-MetaNet shows a better flexibility and a less catastrophic interference, and achieves the best classification accuracy compared with gradient based baselines. We also evaluate Sparse-MetaNet on Wisconsin Card Sorting Test , a simple online reinforcement learning problem adapted from the human cognitive test and large scale language modelling benchmarks. When used along with Transformer-XL for adaptive language modelling, our Sparse-MetaNet achieves 1.00 bpc on enwik8 and 22.67 perplexity on WikiText-103 datasets, improving upon the original Transformer-XL result of 1.06 bpc and 24.0 perplexity, respectively.           {r}{0.5\textwidth}                                              \vskip -0.45in             The only universal learning algorithm that we are aware of is how humans learn. Human learning is robust and flexible -- it relies on causality, has an ability of fast and sequential adaptation and balances memory encoding and active forgetting, across a large number of familiar and unfamiliar scenarios. Meta-learning offers a promising computational paradigm to learn such a universal learning algorithm in a data-driven way.  In this work, we proposed a meta-learning approach to learn a sequential adaptation algorithm for arbitrary deep neural network architectures. Our approach performs sequential adaptation with a bounded compute and memory across changing environment and tasks. The proposed Online Cifar setup can serve as a useful benchmark for studying flexible models and algorithms that go beyond the fixed distribution regime.   In the current state of the Sparse-MetaNet method, a sparsity mask is sampled from a fixed distribution. A future work should explore learning-based approaches for a conditional mask distribution, so that a Sparse-MetaNet model can selectively encode a fast-weight memory from past gradients. The current work has a limited focus on the catastrophic interference issue in neural networks. A future work can extend the Sparse-MetaNet approach for mitigating this issue.  
","     Training a deep neural network requires a large amount of single-task data and involves a long time-consuming optimization phase. This is not scalable to complex, realistic environments with new unexpected changes.      Humans can perform fast incremental learning on the fly and memory systems in the brain play a critical role.     We introduce Sparse Meta Networks -- a meta-learning approach to learn online sequential adaptation algorithms for deep neural networks, by using deep neural networks.      We augment a deep neural network with a layer-specific fast-weight memory. The fast-weights are generated sparsely at each time step and accumulated incrementally through time providing a useful inductive bias for online continual adaptation. We demonstrate strong performance on a variety of sequential adaptation scenarios, from a simple online reinforcement learning to a large scale adaptive language modelling.",94
"  The advent of open-source software and question and answering websites contributed for improving the way developers produce code. Nowadays, code search permeates the development activities. Developers can spend 15\% of their time searching online for how a piece of code works, how to fix a bug, and how to use an API . According to , at Google, developers search for code 12 times a day, clicking on 2 to 3 results in average per search session.    Most developers use general-purpose search engines  to look for code , which uses page rank and other indexes tactics that are not optimized for searching code. Then, general-purpose search engines do not adequately find code snippets unless they have accompanying descriptions. According to , developers spend more time, visit more pages, and change queries more often when they are doing code-related searches. In particular, newcomers to a project can greatly benefit from semantic search since they face a variety of entrance barriers .   GitHub, a popular source code hosting platform, has attempted to build a semantic code search. They extracted millions of lines of code from its repositories and matched each code snippet to a docstring. The final results were not satisfactory as the tool could find a relevant code snippet only if the user provided a query that matched the docstring description . According to , users' intents were better matched to questions collected from question-answering sites related to programming, e.g., Stack Overflow. Those sites allow users to ask a question and approve the best answer for it. Other users vote for the most helpful answer and mark the wrong or not helpful ones. Those collective actions curate and organize information.  Initial code search studies were based on deductive-logic rules and manually extracted features . The recent success of artificial neural networks has shifted recent works to a machine learning-based approach.  coined a name, neural code search, i.e., code search based on neural networks.  Recent works applied neural networks to summarize and retrieve code snippets.  proposed a neural network with attention mechanism and  presented a recurrent neural network. Our novel approach is based on Convolutional Neural Networks . For the best of our knowledge, CNNs have not yet been used to search for code, but have achieved good results in selecting answers . CNNs prioritize local interactions  and its translation invariant, which are important traits for our task.   In our study, we answer the following research questions:             Our model, CoNCRA, achieved an MRR score 5\  higher on average than Unif, a state-of-the-art technique. We could rank the most relevant code snippet among the first 3  positions in 78\  of cases. Our technique achieved a TOP-1 accuracy of 60\ , while the other techniques achieved 50\ .   The results seem promising, and we plan further investigation to check if our model is invariant to other datasets. We will also investigate transfer learning, e.g., checking if a model trained on a Stack Overflow dataset  can find relevant code snippets in a GitHub corpus . Our approach is based on an NLP technique proposed by , and future work may use transformers and autoencoders, as those techniques showed good results in many NLP tasks.  
"," Software developers routinely search for code using general-purpose search engines. However, these search engines cannot find code semantically unless it has an accompanying description. We propose a technique for semantic code search: A Convolutional Neural Network approach to code retrieval . Our technique aims to find the code snippet that most closely matches the developer's intent, expressed in natural language. We evaluated our approach's efficacy on a dataset composed of questions and code snippets collected from Stack Overflow. Our preliminary results showed that our technique, which prioritizes local interactions , improved the state-of-the-art  by 5\% on average, retrieving the most relevant code snippets in the top 3  positions by almost 80\% of the time. Therefore, our technique is promising and can improve the efficacy of semantic code retrieval.",95
" In recent years, deep learning methods have become the standard for solving information retrieval tasks. These methods can effectively map words and phrases to vector representations. These representations can facilitate better matching between phrases that have similar meanings. Phrases closer in meaning will be represented closer to each other in a vector space. In information retrieval, many ways to develop relevance scores have been used, such as counting word overlap between query and document. Recently, more complex machine learning models use human-verified datasets to train models to assign similarity scores used for rankings. Applying deep learning to Natural Language Processing problems has given rise to new approaches that can better represent a sentence閳ユ獨 meaning using neural networks. For instance,  models with an attention mechanism allow for word relationships to be constructed between different sentences and thus for words to be better placed in context, rather than just by examining the words closest to them. A breakthrough development in Natural Language Processing, the  architecture, extracts word and consequently sentence representations by masking words throughout a sentence and predicting the omitted words, using self-attention to encode the entire sentence at once. Within the BERT framework, the model can also be trained to predict the next sentence out of a few choices, given an input sentence. \\  Even with these advances, deep learning methods still struggle with some inherent difficulties in IR tasks. These challenges result from discrepancies in query and document vocabulary, limited size of data used for training, and weaknesses in a given human-generated query. In an effort to mitigate these effects, our team閳ユ獨 approach was inspired by an existing method, , which for a given input document uses a transformer model architecture to predict plausible queries leading to that document. Although it was shown that the expanded documents indeed allowed improved retrieval performance by a downstream ranking model, this approach requires that all documents in the collection of interest are first ``pre-indexed'' by feeding them as input to the transformer model, which is not practical. Instead, we propose a  method that takes a given query as input and generates several queries similar in meaning. The hope is to create a more powerful query by augmenting the generated queries and the given query into a single representation, which is used to match a desired passage. To complete our architecture, we then feed the expanded queries to a pre-trained BERT model which can predict similarity scores between queries and documents and produce a final ranking. The goal of our approach is to reduce surface form 閳ユ笜oise閳 within a certain query by generating other queries that ask for the same information, but in different ways. By having different representations of the 閳ユ笩ame閳 query, we hope to create more holistic queries and as a result obtain an end-to-end method which can generalize better and potentially reduce the problems which modern IR faces.      This report describes Brown University's entry to the TREC 2019 Deep Learning Track, in which we produced the final ranking of a set of 1000 candidate passages for given queries. Our method aims at enriching the meaning and surface form of a query by expanding it with similar queries, in the hopes that during the subsequent ranking process, the expanded query would provide extra semantic information or vocabulary overlap that would facilitate the retrieval of more relevant documents. \\  We found this retrieval method to be promising in terms of retrieval results, albeit with significant margins for future improvement. A natural focus point of future work is improving the semantic similarity between generated queries and the original query. In this work, we simply use the top 3 output beams in terms of estimated log-likelihood. However, different metrics could be used to re-order and prioritize a larger number of generated outputs. In addition, further investigation can be carried out in terms of various ways of synthesizing the query information or condensing the documents' representation.  
"," This paper describes Brown University's submission to the TREC 2019 Deep Learning track. We followed a 2-phase method for producing a ranking of passages for a given input query: In the the first phase, the user's query is expanded by appending 3 queries generated by a transformer model which was trained to rephrase an input query into semantically similar queries. The expanded query can exhibit greater similarity in surface form and vocabulary overlap with the passages of interest and can therefore serve as enriched input to any downstream information retrieval method. In the second phase, we use a BERT-based model pre-trained for language modeling but fine-tuned for query - document relevance prediction to compute relevance scores for a set of 1000 candidate passages per query and subsequently obtain a ranking of passages by sorting them based on the predicted relevance scores.  According to the results published in the official Overview of the TREC Deep Learning Track 2019, our team ranked 3rd in the passage retrieval task , and 2nd when considering only re-ranking submissions.",96
" % Background Collecting a sufficient amount of electronic health records is a challenging task with various factors . Due to this problem, researchers in the medical field are often provided with only a small amount of data given. Owing to the fact that deep learning techniques perform better on large amounts of data, a number of studies using machine learning techniques have been conducted to solve specific medical problems, regarding a limited number of data . Dementia is also one of many medical symptoms facing this situation.  % Alzheimer's Dementia Dementia, a syndrome in which there is deterioration in cognitive function beyond what might be expected from normal ageing, is mostly affected by Alzheimer閳ユ獨 Disease . % Although studies with Dementia also faces the problem with lacking dataset,  There were previous researches with various approaches to recognize Alzheimer's Dementia , which has shown excellent performance. % However, the dataset used in these works were more adequete with quantity than the one used in this paper. However, datasets used in these works were sufficient with quantity than the one used in this paper.  % The ADReSS challenge The ADReSS challenge  at INTERSPEECH 2020 hosts two tasks: Alzheimer閳ユ獨 Dementia  classification and Mini Mental Status Examination  regression, while providing a refined dataset. The dataset is equally balanced of AD and non-AD participants with the metadata of age and gender. % Each data is a conversation between a participant and an investigator composed of acoustic and textual information. % Each data is a conversation between a participant and an investigator where a participant spontaneously describes the picture given by an investigator. % Each data is a conversation where a participant spontaneously describes the picture given by an investigator with acoustic and textual modality. Each data is a conversation in which participants, in both audio and text modalities, spontaneously describes the picture given by the investigator. % proposing work Participants of the challenge are suggested to solve hosted tasks using only the given data, where the numbers of train and test data are 108 and 48, respectively.  For recognizing AD with small amounts of data, we determined it would be beneficial to use both acoustic and textual features. % why? % we thought it would be best to use as many information as possible for recognizing AD 闉氭帾鐓遍瀬婵庢簜鎼 姘氭棄闈栨棶? Furthermore, we leverage models pre-trained on large scale datasets as feature extractor to get better representation. To this end, this paper focus on exploiting various multi-modal features, and design suitable network architecture. % 闇嬨倢妫 闆﹥妲 闆尗姣勯湆 鑷ф粚娈 鑷у嫴鐏ラ爟姗佺煀 闈广倠鐛忛爟姗佽荡 We compare 3 and 4 different acoustic and textual features, respectively, and use the hand-crafted  feature and part-of-speech  tagging as additional inputs. The usage of POS and HC is influenced by previous research, which has approved that using these features gained from transcript can improve the performance . The proposed network is a modified version of Convolutional Recurrent Neural Network ; capable of computing conversations with variable lengths, and implemented with methods to fit with a small amount of data. Also, the model is able to compute using the acoustic feature only, without any metadata, which can be efficient considering the real-world situation. Our experimental results show using features of the pre-trained network leads to performance gain than that of raw, and regression results imply the potential of network classifying classes of cognitive impairment based on MMSE score.         For the classification task, the best test accuracy using only acoustic input is 72.92\ , while using both modality results in 81.25\ . For the regression task, we achieved an RMSE score of 3.7749 . Additionally, our 5-fold cross-validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment, categorized by the MMSE score, with an accuracy of 78.70\ .  This paper demonstrates extracted features from pre-trained networks are satisfactory for handling small amounts of data, to recognize Alzheimer's Dementia. The proposed model can compute variable lengths of dialogue and also introduce productive methods to fit the network with a little amount of data. Furthermore, our model does not require any metadata and also can perform well without transcript, which may be practical in real-world situations. Our test result outperforms baseline's with both tasks, and our regression results imply the potential of network classifying classes of cognitive impairment based on the MMSE score.    validation 闆碱煃纭犻瀽鎰冲妧 unimodal鎼 鑷 modality闉 闆介爟 鐡寸摚鑷ｆ post 鐡撮灊鏇ф殔 鐡寸摚鑷, audio闉愭劤鍔闆 姣电儎绋婄摽 text闉愭劤鍔闆 闊规顒呯, 鏀 姘氭﹤瀵戦灇 鐡村眾姣庤嚙 闉涘牗妫冮浖. 闉氭帾銈 鐡跨娊鐗戦爠鍫ㄦ綁 闇 bimodal network鑷 闇 modality姣 merge 闋冩﹤鈹愰澒 闉庡牗顣伴灇 鏀垫數鍕虫綁 姘氬姙鐏涢爟姗冾潊 闉庡﹫纰 闇呮﹤鐗 闋冩瑬濮烽爟姗冩３ 闈瑰姜濮 闉濋爟姗佺殰 闉涘牗妫冮浖銈婄 鏂兼棈娴爟 闈 闉涘牕濮呴浛, 闉氭帾銈 姘氣晥顫呴爟姗傚 姘囶煄宓 鐢戭剣鈥滄 future work闉欒導顢 姝嗗嫶鏋嗛爟 闈 闉涘牗娼 鐡村喒婢婇浖. 闇涚紕纰 闉濅緟鏌庢挨 闈 闉涘牗娼 闇    VGGish & Transformer-XL   VGGish wrong - 18   Transformer-XL wrong - 5    bimodial wrong - 9     future work to improve multi-modal network   Some modifications in the model architecture can be done to merge different modalities with beneficial effects on each other as future work. There were validation samples with no overlapping error results of the unimodal network of each modality, where the bimodal network using the same modality features above was able to reach the accurate answer for these typical samples. Yet, some samples that the unimodal networks could deduce correctly were wrong by the bimodal network. Accordingly, mechanisms effectively fusioning divergent features can be applied in expectation of performance gain  .    For future work, some modifications in the model architecture can be done to merge different modalities with mechanisms effectively fusioning divergent features can be applied in expectation of performance gain  .  For future work, with the expectation of performance gain, mechanisms effectively fusioning different modality features   can be applied in the model architecture.        
"," % The ADReSS Challenge at INTERSPEECH 2020 regards to discern patients suspicious of Alzheimer闁炽儲鐛 Dementia by providing acoustic and textual data. Since the given training dataset only comprised of 108 conversations, leveraging pre-trained models is effective than fitting from scratch. Therefore, this paper aims to recognize Alzheimer闁炽儲鐛 Dementia by exploiting various multi-modal features from pre-trained networks. With the given dataset of conversational form, we modify a Convolutional Recurrent Neural Network based structure to compute input modalities. Our model performs classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. For the classification task, the best test accuracy using only acoustic input is 72.92\%, while using both modality results in 81.25\%. For the regression task, we achieved an RMSE score of 3.7749 . Additionally, our 5-fold cross-validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment, categorized by the MMSE score, with an accuracy of 78.70\%.   %We use 5-fold cross-validation for measuring model performance. For the classification task, the best F1 score using only acoustic input is 86.28\%, while using both modality results in 94.54\%. For the regression task, the best RMSE score is 3.3493.   Collecting and accessing a large amount of medical data is very time-consuming and laborious, not only because it is difficult to find specific patients but also because it is required to resolve the confidentiality of a patient's medical records. On the other hand, there are deep learning models, trained on easily collectible, large scale datasets such as Youtube or Wikipedia, offering useful representations. It could therefore be very advantageous to utilize the features from these pre-trained networks for handling a small amount of data at hand. In this work, we exploit various multi-modal features extracted from pre-trained networks to recognize Alzheimer's Dementia using a neural network, with a small dataset provided by the ADReSS Challenge at INTERSPEECH 2020. The challenge regards to discern patients suspicious of Alzheimer闁炽儲鐛 Dementia by providing acoustic and textual data. % With the given dataset, we assess features extracted from the pre-trained networks using a neural network. With the multi-modal features, we modify a Convolutional Recurrent Neural Network based structure to perform classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. % Our model performs classification and regression tasks simultaneously and is capable of computing conversations with variable lengths. % For the classification task, the best test accuracy using only acoustic input is 72.92\%, while using both modality results in 81.25\%. For the regression task, we achieved an RMSE score of 3.7749 . Additionally, our 5-fold cross-validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment, categorized by the MMSE score, with an accuracy of 78.70\%. Our test results surpass baseline's accuracy by 18.75\%, and our validation result for the regression task shows the possibility of classifying 4 classes of cognitive impairment with an accuracy of 78.70\%.",97
"   Transformer  is one of the state-of-the-art approaches for Neural Machine Translation , and hence, being widely accepted. For example, in WMT19 machine translation tasks, it is reported that 80\% of submitted systems have adopted the Transformer architecture . Note that high translation quality of Transformer models entails a large number of parameters. Moreover, the Transformer model is inherently much slower than conventional machine translation approaches  mainly due to the auto-regressive inference scheme  incrementally generating each token. As a result, deploying the Transformer model to mobile devices with limited resources involves numerous practical implementation issues.  To address such implementation challenges with little degradation in translation quality, we study a low-bit quantization strategy for Transformer to accomplish high-performance on-device NMT. We note that most previous studies to compress Transformer models utilize uniform quantization . While uniform quantization may be effective for memory footprint savings, it would face various issues to improve inference time and to maintain reasonable BLEU score. For example, even integer arithmetic units for inference operations present limited speed up  and resulting BLEU score of quantized Transformer can be substantially degraded with low-bit quantization such as INT4 .  While determining the number of quantization bits for Transformer, it is crucial to consider that each component of Transformer may exhibit varied sensitivity of quantization error toward degradation in translation quality . Accordingly, a mixed precision quantization can be suggested as an effort to assign different numbers of quantization bits depending on how each component after quantization is sensitive to the loss function. In addition, as we illustrate later, even assigning different quantization bits for each row of an embedding block can further reduce the overall number of quantization bits of the entire Transformer model. Our proposed quantization strategy, thus, provides a finer-grained mixed precision approach compared to previous methods, such as  that consider layer-wise or matrix-wise mixed precision.  % One important aspect is that each block in Transformer contributes to the inference computation and the translation accuracy differently. Transformer consists of three major blocks: embedding, encoder, and decoder. The embedding block has a huge number of parameters due to its dependence on the vocabulary size, easily in scale of tens of thousands. On the contrary, the matrices in encoder and decoder are relatively small since they are independent of the vocabulary size.  As a result, embedding block causes a major memory and latency consumption. Since the decoding steps are not parallelizable at inference time, it also contributes largely to the inference computation.  % In consideration of these, we propose a mixed precision quantization strategy for Transformer quantization with efficient inference computation and reasonable accuracy loss. Accommodating distinguished implementation properties  of each component in Transformer, we propose the following methodologies to decide precision of a block: 1) in the case of embedding block, statistical importance of each word is taken into account and 2) for encoder and decoder blocks, sensitivity of each quantized sub-layer is considered. The main contributions of this paper are as follows:  [noitemsep]     %       In this work, we analyze each block and sub-layer of the Transformer and propose an extremely low-bit quantization strategy for Transformer architecture. Our 2.6-bit quantized Transformer model achieves 11.8 model compression ratio with reasonable -0.5 BLEU. We also achieve the compression ratio of 8.3 in memory footprints and 3.5 speed up on a mobile device .       \clearpage   
","  The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits . For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8$\times$ smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3$\times$ reduction in run-time memory footprints and 3.5$\times$ speed up  such that our proposed compression strategy enables efficient implementation for on-device NMT.",98
" Code completion has become an essential feature of Integrated Development Environments . It speeds up the process of software development by suggesting the next probable token based on existing code. The main goal of most existing code completion systems is to suggest accurate variables, arguments, or APIs to developers. Recently, along with the development of deep learning technologies and easy-to-acquire open-source codebases, researchers have started to tackle code completion by learning from large-scale code corpora.  In this paper, we define a new code completion task: full-line code completion. Given a partially completed code snippet, full-line code completion requires predicting the next line of code, different from traditional code completion which only predicts the next code element. Figure 1 shows a motivating example for our task. To complete the last line in Figure 1, traditional code completion needs to predict at least six times separately, and each time the developer needs to choose the correct token. But if we generate the entire line simultaneously, even if the prediction is only partially correct, the developer can correct the code line with fewer operations.   Currently, the most popular technique in the research area of code completion is language models, especially neural language models. Neural language model is a powerful tool for predicting the next token given a token sequence, which naturally fits the scenario of code completion. Recent researches have shown that large-scale neural language models like GPT-2  are capable of generating long text, which brings the potential of code sequence generation.   One of the key challenges in full-line code generation is to guarantee the syntactical correctness of the generated code.  To tackle this challenge, we draw lessons from past researches on semantic parsing. We adopted a widely used framework for syntax-based code generation, which converts the generation of a code snippet into generating its abstract syntax tree  with a sequence of construction actions.  We conduct experiments on two public Python datasets that contain Python files crawled from Github repositories. One dataset is in Python2, and the other one is in Python3. We evaluate the performance of the state-of-the-art approach for traditional code completion, along with a group of neural language models. Our results show that on both datasets, Transformer language models outperform RNN-based models, which is consistent with past researches in language modeling. We also find that syntax-based approaches do not outperform token-based approaches, indicating that directly applying techniques in syntax-based code generation into full-line code completion can be ineffective.  [h]     The main contributions of this paper are summarized as follows:  1) We propose a novel code completion task: full-line code completion and build datasets for this task.  2) We evaluate state-of-the-art models used in traditional code completion and a group of neural language models on our datasets.  3) We analyze the performance of plain token sequence-based language models versus syntax-based language models, and discussed the effectiveness of incorporating syntax information in full-line code completion and some possible improvements in the future.      In this paper, we define a new task, full-line code completion, and studied the performance of neural language models on this task. Apart from token-based and BPE-based approaches, which have already been evaluated on token-level code completion tasks, we additional conduct experiments with ASDL syntax-based models. Our experiments show that Transformer language model on token sequences currently performs best on our datasets.  In the future, we plan to further improve the effectiveness of language models on full-line code completion by training on more data and using models with larger parameter size. Meanwhile, we aim to utilize more powerful software analyzing tools to further narrow down the output space of our model, e.g., adding restrictions on variable names and API usage. Furthermore, we would like to improve our neural model to incorporate syntax structures like parent-child links in ASTs and incorporate BPE or copy mechanism to tackle the out-of-vocabulary problem.  \bigskip  
"," A code completion system suggests future code elements to developers given a partially-complete code snippet. Code completion is one of the most useful features in Integrated Development Environments . Currently, most code completion techniques predict a single token at a time. In this paper, we take a further step and discuss the probability of directly completing a whole line of code instead of a single token. We believe suggesting longer code sequences can further improve the efficiency of developers. Recently neural language models have been adopted as a preferred approach for code completion, and we believe these models can still be applied to full-line code completion with a few improvements. We conduct our experiments on two real-world python corpora and evaluate existing neural models based on source code tokens or syntactical actions. The results show that neural language models can achieve acceptable results on our tasks, with significant room for improvements.",99
" As neural networks are being adopted to solve real-world problems, while some parts of the network may be easy to develop, other unknown aspects such as hyperparameters, have no clear method of derivation. Ongoing research focuses on developing new network architectures and training methods. When developing neural networks, the question at hand is how to set the hyperparameter values to maximize results and set the training configuration. For network architecture design, important hyperparameters include the type of network, the number of layers, the number of units per layer, and unit type. For training configurations, important hyperparameters include learning algorithm, learning rate, and dropout ratio. All these hyperparameters interact with each other and affect the performance of neural networks. This interaction between hyperparameters can be referred to as epistasis. Thus they need to be tuned simultaneously to get optimum results.\\  The motivation behind this research is to replace tedious manual tuning of hyperparameters with an automatic method performed by computers. Current methods of optimization are limited to trivial methods like Grid search. Grid search is a simple method for hyperparameter optimization. However, as the number of hyperparameters increases, Grid search becomes time consuming and computationally taxing. This is because the number of lattice points increases in an exponential way with an increase in the number of hyperparameters . For example, if there are ten hyperparameters to be tuned and we only try five values for each parameter, and this alone requires more than 9 Million evaluations: 5^{10} = 9765625. For this reason, the grid search is not feasible for certain applications. To solve this, we look to a GA for a higher-performing and less computationally taxing solution. The use of a GA for neural network hyperparameter optimization has been explored previously in . \\   We present an empirical study of GAs for neural network models in machine translation of natural language specifically Japanese to English. We describe the experiment setup in Section 2, our GA method in Section 3,  and results in Section 4. The preliminary findings suggest that a simple GA encoding has the potential to find optimum network architectures compared to a random search baseline.    This work introduces an advanced GA for hyperparameter optimization and applies it to machine translation optimization. We demonstrate that optimization of hyperparameters via a GA can outperform a random selection of hyperparameters. Specifically, outperform is defined by the ability of the algorithm to arrive at the goal with less individuals added. Finally, we propose future research directions which are expected to provide additional gains in the efficacy of GAs.  
","  With neural networks having demonstrated their versatility and benefits, the need for their optimal performance is as prevalent as ever. A defining characteristic, hyperparameters, can greatly affect its performance. Thus engineers go through a process, tuning, to identify and implement optimal hyperparameters. That being said, excess amounts of manual effort are required for tuning network architectures, training configurations, and preprocessing settings such as Byte Pair Encoding . In this study, we propose an automatic tuning method modeled after Darwin's Survival of the Fittest Theory via a Genetic Algorithm . Research results show that the proposed method, a GA, outperforms a random selection of hyperparameters.",100
"  In the past few decades, knowledge graph construction and applications have been rapidly developed and achieved significant outcomes.  For better relevancy in web search, Google has been leveraging knowledge graph that represents real-world entities and their relationships to one another since 2012. %, there are also a large amount of publicly available knowledge graphs, such as freebase, Dbpedia, YAGO that have been constructed and used to many real-world intelligent applications. To identify those entities from text, named entity recognition  techniques have been extensively studied and applied in many areas  including e-commerce search . Such NER systems usually work with a well defined ontology to classify tokens in a sequence of words . A comprehensive and domain-specific PT ontology is beneficial to product search and discovery in an e-commerce platform . At The Home Depot , PT ontology has been used tremendously by the online search to improve query understanding and product retrieval. For example, Figure  shows a snippet of our PT ontology that consists of known PT classes. The PTs in the ontology serve as the entity reference for the NER task  as well as the classes for SKU-PT mapping  on the catalog side that facilitates the retrieval of relevant products.  %. Kutiyanawala et al. also proposed an product ontology framework created specially for e-commerce search and retrieval .  %comprehensive and domain-specific Ontology is required in order to better understand customers閳 intent and account for the expanding catalog. The Ontology enrichment has been proved effective to boost search relevancy. For example, given the customer query ""shower curtain hook"", the system would also return some ""shower curtain"" products since it failed to infer the proper product type due to the lack of knowledge. By introducing a new product type ""shower curtain hook"", the system is able to remove the noise and provide more relevant results. %  %  %  % &\overbrace{whirlpool}^{brand} \quad \overbrace{7.4\:cu\:ft}^{dimension} \quad \overbrace{gas\:dryer}^{product}% \quad\overbrace{gas}^{attribute} %   \\ %   &\overbrace{milwaukee}^{brand} \quad \overbrace{cheap}^{other} \quad \overbrace{drill}^{product}% %  %   %     \[ %   z = \overbrace[1pt][5pt]{ge}^{brand}\ \overbrace{7.3\:cu\:ft}^{dim} \quad\overbrace{dryer}^{product}\quad\overbrace{gas}^{attribute} %   \] %In the domain of e-commerce, a strong and well-structured knowledge graph also plays pivotal roles for both business to business  communications and customer search and navigation experience.   %A structured and standardized product ontology which define product description, catalog formats and business documents support electric data exchange between vendors and buyers.  %The Home Depot  is a world leading home improvement retailer for customers and business. Orange Graph  is the repository and access point for THD domain-specific knowledge, which includes rich product information, project information and their relationships. By adopting well-structured knowledge graph, a high-level of search quality, project-based buying features, marketing and customer services can be offered at THD e-commerce and enterprise systems.  % [htbp!] % {!}{ % {c||c} %  \\ %  vs. Induction Ranges  \\ %  vs. Wood Glue  \\ % } %  %  %  Discovering valid PTs is a key task to build or expand a PT ontology with a fundamental challenge regarding the definition of a PT. % given it's a concept instead of fact. A PT can be defined from the demand side as atomic keywords/phrase that describes what customers look for  or from the supply side as a semantic tag/label that uniquely identifies a product. Within THD, we also have practical guidelines to distinguish between valid and invalid PTs like  %Product type  is an essential component of a PT ontology.  %it is widely used in e-commerce domain to group the similar products together. For instance, consider Appliances category, our goal is to discover distinct types of refrigerators which in this case it could be: ""Side By Side"", ""French Door"", etc.  %Although there are different definitions for a valid PT, In this paper, we define a valid PT as a leaf-level description of an entity.   no common attributes like color, brand, material, style etc in PTs  and  it requires significant differences in the form, functionality or usage location to make a new PT comparing to existing ones . %Another determiner for whether adding a token to a product type makes it a new product type  is if the addition of the new token changes the form, function or usage location. In our example, cordless doesn't change it for drill, while utility does for sink.   Obviously, neither the definition is definite nor the guidelines are exhaustive enough and there are always complicated cases and exceptions in which human judgement based on knowledge in merchandising, customer preference or just common sense is required. %without involving human knowledge which is usually expensive in term of time and monetary cost. %automatically determine if a candidate .  %Although aforementioned definition would generally help to distinguish between valid and invalid PTs, there are several challenges in this task  %as depicted in Table.  %First and foremost, it is crucial to determine a right level of granularity for discovered PTs. Very generic PTs are generally ambiguous as they could be attributable to a broad set of products with different use cases. For example, PT chairs can be ambiguous as it can comprise outdoor chairs, office chairs, dining chairs and each of these chairs types has a different usage location. %Specifically, domain experts have great advantage in  For example, a generic PT range can be broken down into more granular ones by fuel type like gas range, electric range or by other attribute like induction range, convention range. The word ""wood"" is material in wood rolling pin while is about usage in wood glue.   % Moreover, it is often subjective to determine in what level of granularity PT discovery should be stopped and based on what criteria a generic PT should be broken down into more granular PTs. For instance, given a generic PT ranges we can break it down by fuel type  or features . In this example, we can consider one of them as the PT and the other one as an attribute; alternatively they can be combined and construct a more granular PT.  % Another challenge is to automatically identify if a token in a PT is an attribute or not. As an example, consider wood rolling pin and wood glue; token wood in the latter change the use case of the glue, while in the former is a material.     However, leveraging human knowledge in large scale problems is usually timely and expensive.  To reduce such cost, this paper proposes  %The main contribution of this paper is as follows: proposing  an active learning framework that minimizes human effort in PT discovery by 1) identifying high quality candidates using phrase mining and user behavior. 2) limiting number of PT candidates for human validation.  %%%%%%%    In this work, we propose an active learning framework for product type discovery that leverage domain expertise in an efficient way.  The effectiveness of the framework is demonstrated by the quality and coverage of the resulting product types in the experiments as well as the positive business impact.  Experiment results also show that training data denoising is significantly beneficial to method performance. There are two kinds of future work including: 1) Feature engineering of PT classifier by exploiting more textual and/or image data 2) Design a denoise procedure and add it as an additional component into the framework.       
"," Entity-based semantic search has been widely adopted in modern search engines to improve search accuracy by understanding users' intent. %behind the search terms.  %In e-commerce domain, product type  is a central concept in intent understanding as well as catalog organization. %indicating customers' intent in their search queries.  %be identified from customers' queries for understanding  In e-commerce, an accurate and complete product type  ontology is essential for recognizing product entities in queries and retrieving relevant products from catalog.  However, finding product types  to construct such an ontology is usually expensive due to the considerable amount of human efforts it may involve.  In this work, we propose an active learning framework that efficiently utilizes domain experts' knowledge for PT discovery.  We also show the quality and coverage of the resulting PTs in the experiment results.",101
" Distributional word representations trained on large-scale corpora are widely used in modern natural language processing  systems, which aims to describe the meaning of words and sentences with vectorized representations . Recent studies  addressed the state-of-the-art word embedding performance on various NLP tasks, where start to focus on how to evaluate the performance between different word embeddings accurately. However,  and  have demonstrated that even for the same word embedding, most of the existing evaluation methods do not provide the constantly correlative results between intrinsic evaluation and extrinsic evaluation. Therefore, evaluating the performance of word embeddings with a unified metric is challenging in NLP tasks.   proposed a new evaluation framework called CogniVal, which applied traditional neural networks for regression and considered both intrinsic and extrinsic measurements based on collected human natural language processing-related cognitive data sources across three modalities: electroencephalography , functional magnetic resonance imaging , and eye-tracking. CogniVal is potentially identified as a pioneer of multi-modal cognitive word embedding evaluation framework, which conducts vectorized word embeddings evaluation by predicting how much they reflect the semantic representations against cognitive data sources that recorded when human processing natural language.   However, CogniVal framework ignored to measure some characteristics of human physiological signals. Specifically, all three modalities  of cognitive data used in their experiment featuring with non-stationary and non-linear motions . Inspired by , we assume that neural networks and fuzzy systems as computational intelligence methods are suitable tools for modelling expert knowledge and dealing with uncertain non-linear processes or non-stationary time series in a dynamic system, because approximate reasoning characteristics of fuzzy systems could present a practical model to handle uncertainty and disturbances in real data for complex hybrid non-linear or non-stationary problems . For this reason, we proposed a fuzzy-based neural network  framework for evaluating word embeddings with cognitive datasets, name CogniFNN, which expects to enhance the quality of evaluating the performance of word embeddings with cognitive data sources , and achieve a higher ratio of significant results with random word embeddings as well.    The main contributions of our study are shown as follows:         In this paper, we proposed a CogniFNN framework using fuzzy-based neural networks to explore the non-linear and non-stationary characteristics of physiological signals for improving the evaluation performance of word embeddings against cognitive datasets which recorded when subjects were understanding natural language . Our findings showed that CogniFNN achieved smaller prediction errors and higher significant ratios on both context-independent  and context-sensitive  word embeddings against 15 cognitive data sources across EEG, fMRI and eye-tracking. Our contributions could be a useful evaluation strategy which is beneficial to the exhaustive investigation on word embedding evaluations with corresponding cognitive features.         
"," Word embeddings can reflect the semantic representations, and the embedding qualities can be comprehensively evaluated with human natural reading-related cognitive data sources. In this paper, we proposed the CogniFNN framework, which is the first attempt at using fuzzy neural networks to extract non-linear and non-stationary characteristics for evaluations of English word embeddings against the corresponding cognitive datasets. In our experiment, we used 15 human cognitive datasets across three modalities: EEG, fMRI, and eye-tracking, and selected the mean square error and multiple hypotheses testing as metrics to evaluate our proposed CogniFNN framework. Compared to the recent pioneer framework, our proposed CogniFNN showed smaller prediction errors of both context-independent  and context-sensitive  word embeddings, and achieved higher significant ratios with randomly generated word embeddings. Our findings suggested that the CogniFNN framework could provide a more accurate and comprehensive evaluation of cognitive word embeddings. It will potentially be beneficial to the further word embeddings evaluation on extrinsic natural language processing tasks.",102
" As a key step in constructing a knowledge graph, relation extraction is a task to extract the relation between the entities expressed in a sentence.  Previous work has largely focused on intra-sentence binary relation extraction, where the goal is to extract the relation between an entity pair in the sentence.   However, some relations require more than two entities and may span multiple sentences, which is defined as n-ary cross-sentence relation extraction. As the example shown in Table, the relation ``educate'' includes four entities, the person's ""name``, ""academic degree``, ""academic major`` and ""school``. In addition, this relation spans in four sentences in the example. Some prior works have applied a supervised learning approach to tackle this task, but they require large-scale labeled training data. [ht] , which has the ``educate'' relation. ``edu'' denotes that the sentence represents the ``educate'' relation and ``--'' denotes it does not.} {\linewidth}{ c X c c} \toprule Pos. & Sentence & DS & R\\ \midrule 3   &   Alan Turing worked on hyper computation in Princeton University.& edu & --  \\ 4   &  He obtained his PhD in 1938.  & edu &edu   \\ 18  & Alan Turing studied logic and computer science in Princeton.  & -- & edu   \\ 20  &  His PhD advisor is Alonzo Church & -- & edu  \\    To obtain large-scale annotated data, some work assumes that if the consecutive sentences  contain the entities that have a relation in a knowledge base, these sentences as a whole describe that relation.  This assumption is referred to as distant supervision in the n-ary cross-sentence relation extraction task.  Even though methods based on distant supervision can quickly annotate sentences, they still have two main limitations: 1) they suffer from a noisy labeling problem;  2) the strong distant supervision assumption does not consider the non-consecutive sentences, which reduces the generalizability of the trained model. As the example shown in Table, the sentences at the 18th and 20th positions describe the fact but are not labeled using distant supervision because they are not consecutive. The first sentence is incorrectly labeled and is a noisy labeled data, which describes Alan Turing's work instead of his education.  To address the first limitation, we propose to train a , which is a two-level agent reinforcement learning model. This provides a well-trained model that can select the high-quality labeled sentence groups and alleviate the impact of noisy data. There are previous works on applying reinforcement learning  to remove binary intra-sentence noisy data and achieve state-of-the-art  performance. When applying RL for n-ary cross-sentence relation extraction, a key challenge is that the RL model should not only learn sentence features, but also know the context and relation between each sentence. In this paper, the process of selecting sentences is not only influenced by the feature of the sentence itself, but also by the indicators we defined , which measure the semantic relationship between sentences. Moreover, whether a sentence is selected in a state or not is going to affect the decision of the next state. This state transition property provides the ability to choose the best combination of sentences in each sentence group.  To address the second limitation,  we relax the strong distant supervision assumption that lies at the heart of prior work by replacing it with a weaker distant supervision assumption. The assumption is that the sentence that has at least one main entity or two supplementary entities is annotated with the relation of these entities. We follow the Wikidata Knowledge Base scheme, where the main entity is the ``value'' of each fact and the supplementary entity is the ``qualifer'' of each fact. This assumption introduces some non-consecutive sentences and we propose a novel universal relation extractor to encode both consecutive and non-consecutive sentence groups. This relation extractor has a self-attention and soft attention mechanism layer, which compares the similarity between the word-level features and the relation query vectors. The relation extractor also encodes each sentence via a Piece-wise Convolution Neural Network  layer. The PCNN output is used to learn how the information transforms through sentences via a non-linear transformation layer.    We proposed  a sentence distribution estimator to alleviate the impact of noisy distant supervision labeled data for n-ary cross-sentence relation extraction;   a weaker distant supervision assumption, which considers non-consecutive sentences; and  a universal relation extractor, which is a hybrid model of attention mechanism and non-linear transformation layer that encodes both non-consecutive and consecutive sentence groups. The experiments showed that the proposed model reduces the impact of noisy data and achieves significantly better performance for n-ary cross sentence relation extraction compared to SotA models.  
"," The models of n-ary cross sentence relation extraction based on distant supervision assume that consecutive sentences mentioning $n$ entities describe the relation of these $n$ entities. However, on one hand, this assumption introduces noisy labeled data and harms the models' performance. On the other hand, some non-consecutive sentences also describe one relation and these sentences cannot be labeled under this assumption. In this paper, we relax this strong assumption by a weaker distant supervision assumption to address the second issue and propose a novel sentence distribution estimator model to address the first problem. This estimator selects correctly labeled sentences to alleviate the effect of noisy data is a two-level agent reinforcement learning model. In addition, a novel universal relation extractor with a hybrid approach of attention mechanism and PCNN is proposed such that it can be deployed in any tasks, including consecutive and non-consecutive sentences. Experiments demonstrate that the proposed model can reduce the impact of noisy data and achieve better performance on general n-ary cross sentence relation extraction task compared to baseline models.",103
" Healthcare information systems store huge volumes of electronic health records  that contain detailed visit information about patients over a period of time. The data is structured in three levels from top to bottom: the patient journey, the individual visit and the medical code. Fig. provides a typical example of this structure. An anonymous patient visits his/her doctor, a pathology lab and is admitted to the hospital on different days. The procedures and diagnoses performed at each of these visits are recorded as industry-standard medical codes. Each medical code, i.e. International Classification of Diseases  and Current Procedure Terminology , at the lowest level, records an independent observation while the set of codes at a higher level can depict the medical conditions of a patient at a given time point. At the top level, all occurrences of medical events at different time-stamps are chained together as a patient journey, which offers more informative details. Predicting sequential medical outcomes based on a patient's journey, such as hospital re-admissions and diagnoses, is a core research task that significantly benefits for healthcare management by hospitals and governments. For example, re-admission statistics could be used to measure the quality of care; Diagnoses can be used to understand more fully a patient's problems and relevant medical research. However, researchers have encountered many challenges in their attempts to represent patient journeys and predict medical outcomes from EHR data with the characteristics of temporality, high-dimensionality and irregularity.   Recurrent neural networks  have been widely used to analyze sequential data, unsurprisingly including medical events modelling for clinical prediction. For example, Choi et al. proposed a multi-level representation learning, which integrates visits and medical concepts based on visit sequences and the co-occurrence of medical concepts. They indirectly exploited an RNN to embed the visit sequences into a patient representation for downstream prediction tasks. Some other research works directly employed RNNs to model time-ordered patient visits for predicting diagnoses.  However, when the length of the patient visit sequence grows, such RNN-based models are restricted by the less expressive power of RNNs, such as vanishing gradient and forgetfulness.  However, such RNN-based models are constrained by forgetfulness, i.e., their predictive power drops significantly when the sequence of patient visits grows too long.  To memorize historical records, LSTM and GRU have been developed to utilize memory and gate mechanism for mitigating these issues.  To go further, Song et al. proposed to utilise attention mechanism in a deep framework to model sequential medical events.  It is worth noting that sequences of medical events are often found to be lengthy, especially when a patient suffers from chronic disease. Hence, due to the restricted ability of RNNs for long-term dependency modeling , the traditional RNNs, even with memory cells and gates, usually underperform in the cases of a long sequence of medical events. In light of this, a neural model that can overcome the performance bottleneck of RNN-based models is particularly desirable for medical predictions based on longitudinal EHR data.   %%%%%%%%  WHAT THE RELATION BETWEEN SHEN2018DISAN AND THIS ONE?? in a Directional self-attention networks can alleviate long sequence problems to improve the accuracy of predictions, as these models can be trained on all available input information - past and future.. CAN WE COME TO THE CONCLUSION: ONE OF CONTRIBUTION IS WE HAVE FULLY CONSIDERED ALL MEDICAL EVENTS COMPARING TO OTHER WORKS THAT CAN ONLY PARTIALLY CONSIDER.  % Recently, attention mechanism has been integrated into RNNs to model sequential EHRs data, which achieves good prediction accuracy. Although the attention-based RNNs relatively improves the prediction performance, the limitations of RNNs weaken the advantage of attention mechanism. In natural language processing , a sole attention mechanism has been used to construct a sequence to sequence model that achieves a state-of-the-art quality score on the neural machine translation  task. The attention mechanism has more flexibility in sequence length than RNN, and is more task/data-driven when modeling dependencies. Unlike sequential models, its computation can be easily and significantly accelerated by existing distributed/parallel computing schemes. However, to the best of our knowledge, a neural net entirely based on attention has not been designed for patient journey in EHRs data.  Most recently, attention mechanisms have sprung to the fore as effective integrations with RNNs for modeling sequential EHR data. So far, these approaches have shown satisfactory prediction accuracy, but some argue that the power of attention in an RNN is limited by weaknesses in the RNN itself . In particular, Vaswani et al. used a sole attention mechanism, i.e., multi-head attention and self-attention, to construct a sequence-to-sequence model for neural machine translation tasks and achieved a state-of-the-art quality score. And according to  Shen et al., self-attention mechanism allows for more flexibility in sequence lengths than RNNs and is more task/data-driven when modeling contextual dependencies. Unlike recurrent models, attention procedure is easy to compute and the computation can also be significantly accelerated with distributed/parallel computing schemes.  For example, Song et al. proposed to employ 1D CNN  to model local context and use attention mechanism  to capture long-term dependency for sequential medical events.  However, when applied to EHR data instead of regular sequential data , the current attention models cannot appropriately deal with some aspects of EHR data, such as arbitrary time-stamps and hierarchical data format.  Hence, to the best of our knowledge, a neural network-based entirely on attention has never been designed for analytics with EHR data.   To bridge the gap in this literature and address some of the open issues listed above, we propose a novel attention mechanism called Masked Encoder  for temporal context fusion. It uses self-attention to capture contextual information and temporal dependencies between a patient's visits.  Then, we propose an end-to-end neural network, called Bidirectional temporal encoder Network , to predict medical outcomes by leveraging a learned representation of the patient journey,  where the representation is generated solely by the proposed attention mechanism, MasEnc. BiteNet constructs a multi-level self-attention network to represent visits and patient journeys simultaneously, using attention pooling and stacked MasEnc layers. It is worth noting that, compared to the existed RNN-based methods, BiteNet can yield better prediction performance for long sequences of medical records.   Experiments conducted on two supervised prediction and two unsupervised clustering tasks with real-world EHR datasets demonstrate that the proposed BiteNet model is superior to prior state-of-the-art baseline methods.   To summarize, our main contributions are:  	  % The remainders of this paper are organized as follows. Section reviews related studies. In Section, we briefly discuss some preliminary, and details about our model are presented in Section. In Section, we demonstrate the experimental results conducted on real-world datasets. Lastly, we conclude our study in Section.%and outline our future work  %   In this paper, we proposed a novel prediction model called BiteNet. The model framework comprises a MasEnc module that captures the contextual information and the temporal relationships between the visits in a patient's healthcare journey and attention pooling that construct the hierarchical structure of three-levelled EHR data. The output is a representation of a patient journey that, once learned by the model, can be used to predict medical outcomes with an end-to-end sole self-attention network. We evaluated BiteNet's performance of the model against several baseline methods with supervised and unsupervised tasks, and conducted an ablation study to examine the contributions of each component. The results show that BiteNet produces more accurate predictions than baseline methods.  
"," Electronic health records  are longitudinal records of a patient's interactions with healthcare systems. A patient's EHR data is organized as a three-level hierarchy from top to bottom: patient journey - all the experiences of diagnoses and treatments over a period of time; individual visit - a set of medical codes in a particular visit; and medical code - a specific record in the form of medical codes. As EHRs begin to amass in millions, the potential benefits, which these data might hold for medical research and medical outcome prediction, are staggering - including, for example, predicting future admissions to hospitals, diagnosing illnesses or determining the efficacy of medical treatments. Each of these analytics tasks requires a domain knowledge extraction method to transform the hierarchical patient journey into a vector representation for further prediction procedure. The representations should embed a sequence of visits and a set of medical codes with a specific timestamp, which are crucial to any downstream prediction tasks. Hence, expressively powerful representations are appealing to boost learning performance. To this end, we propose a novel self-attention mechanism that captures the contextual dependency and temporal relationships within a patient's healthcare journey. An end-to-end bidirectional temporal encoder network  then learns representations of the patient's journeys, based solely on the proposed attention mechanism. We have evaluated the effectiveness of our methods on two supervised prediction and two unsupervised clustering tasks with a real-world EHR dataset. The empirical results demonstrate the proposed BiteNet model produces higher-quality representations than state-of-the-art baseline methods.",104
" The International Classification of Diseases  establishes a standardized fine-grained classification system for a broad range of diseases, disorders, injuries, symptoms, and other related health conditions . It is primarily intended for use by healthcare workers, policymakers, insurers and national health program managers. The United States incurs administrative costs in billions of dollars annually arising from a complex billing infrastructure . Specifically, the ICD code assignment is typically a manual process, consuming on average between 25 to 43 minutes per patient depending on the ICD version . It is also prone to errors resulting from inexperienced coders, variation between coders, incorrect grouping of codes or mistakes in the patient discharge summaries. These errors are very costly with one report estimating that preventable errors in ICD coding have cost Medicare system 31.6 billion in FY2018 .\\\\ Recent work  has tried to automate the task of ICD code assignment using deep learning.  Typically framed as a multilabel classification problem, researchers have trained Convolutional Neural Networks , Recurrent Neural Networks , and Transformer models to predict ICD-9 codes from patient discharge summaries.  These models have outperformed rule-based approaches and those utilizing conventional algorithms such as Logistic Regression, Support Vector Machines, Random Forests etc., achieving competitive micro F1-scores in the range 42\% - 68\%. Amongst these models, those based on CNNs have achieved the best performance.   Neural network models have revolutionized the field of NLP and SOTA models for various NLP tasks involve deep neural network models such as BERT, Bidirectional RNN or CNN-based methods. Recent works  have shown a particular vulnerability of such deep models to adversarial examples that are often produced by adding small and imperceptible perturbations to the input data. The state of the art models of NLP are no exceptions to such perturbations.  provides a review of different adversarial attacks and defense strategies in the NLP literature. Based on granularity of the perturbation, adversarial attack strategies in NLP can be classified into three types - character-level attacks, word-level attacks and sentence-level attacks. In a character-level attack strategy, the model induces noise at the character level. Character-level noise can be induced due to naturally occurring reasons such as typos and misspellings or due to intentional modification by a malicious third-party.  are some of the existing character-level attack strategies in NLP. To accurately model the naturally occurring typos,  restrict the typos distribution based on the character constraints found in a standard English keyboard. We follow this strategy in our work. Furthermore, we assume a white-box setting where the adversary has access to gradients of the loss function wrt to the model inputs. To our knowledge, this is the first work to investigate the effects of adversarial samples in clinical NLP domain.      This work is a first step at exploring the robustness of NLP models used for automatic ICD-9 code classification. Clinical documents are different from regular documents as they are typically generated in a fast-paced environment with higher than average typos and non-standard acronyms. As a result, clinical NLP models are more susceptible to adversarial samples compared to a regular NLP model trained on a standard English dataset. A key extension of the work would be to consider a dictionary learnt from clinical documents and biomedical literature as a defense against these character-level perturbations. Although this might mitigate the decrease in performance, it wouldn't completely solve it. A more rigorous way to deal with this would be to account for this in the tokenization strategy. It is easy to push a word out of vocabulary when using tokenization strategies like word2vec and GloVe. Other strategies that model words unseen in training dataset such as word-piece and byte-pair encoding will also break when typos are introduced because these models learn sub words from a standard dictionary. Therefore, any defense must account for these typos in the fundamental tokenization strategy. An interesting direction would be to learn a word similarity metric and map an unknown word to a closer word in the vocabulary given the input word and the context in which it appears. Building a robust tokenization strategy would be the first step towards a robust NLP model against character-level adversarial attacks.     \medskip  \small    
","   Manual annotation of ICD-9 codes is a time consuming and error-prone process. Deep learning based systems tackling the problem of automated ICD-9 coding have achieved competitive performance. Given the increased proliferation of electronic medical records, such automated systems are expected to eventually replace human coders. In this work, we investigate how a simple typo-based adversarial attack strategy can impact the performance of state-of-the-art models for the task of predicting the top 50 most frequent ICD-9 codes from discharge summaries. Preliminary results indicate that a malicious adversary, using gradient information, can craft specific perturbations, that appear as regular human typos, for less than $3\%$ of words in the discharge summary to significantly affect the performance of the baseline model.",105
"   Systematic Generalization has been characterized as the capacity to understand and produce a potentially infinite number of novel combinations from known components . For example, in Figure, a model could be exposed to a set of facts , but not to all the possible facts that can be inferred by combination of the known components . More recent work has examined systematic generalization in terms of the ability of ``a model to manipulate concepts in new combinations after being trained on all concepts, but only on a limited set of their combinations'' . This view of systematic generalization shifts emphasis from reasoning to learning. %If a model is able to perfectly accomplish a task by leveraging existing facts to infer new ones, we deem the model is generalizing systematically. Here we examine systematic generalization through measuring the ability of a model to reason about new inference step combinations despite being trained on a limited subset of them. %, and conditioning upon a small subset of active relationships at inference time.   Recent developments in natural language processing  have shown that Transformer  Language Models  are able to capture linguistic knowledge , and yield state-of-the-art performance for many NLP tasks , including but not limited to answering reading comprehension questions  and generating factual knowledge  with little to no task supervision. These models are optimized on large corpora to predict the next words or a set of masked words in a sentence. While yielding impressive results, it is not clear if TLMs rely on many superficial patterns in the data or if they actually learn re-usable skills, enabling them to generalize to new tasks by leveraging the compositionality of those skills . Training on massive data can give certain advantages with respect to understanding the meanings of words, but we conjecture that such data gives models less experience with reasoning over inference chains.  [14]{R}{0.25\textwidth}                   In our work, we study the less understood issues related to how well TLMs are able to perform long chains of reasoning. In particular, we use TLMs for the task of theorem proving, where facts and proofs are specified in natural language. Using theorem proving, we test if TLMs can generate interpretable proofs with logically consistent language modeling as their main objective. % In this setting, language models have various attractive properties: they require no logical rule engineering while still being interpretable, do not need human annotations, and are easy to extend to more data. % Language models have many advantages over theorem provers: they require no rule engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. In particular, we study their behavior as logical reasoners on text by analyzing the generated proofs and the final answer. This setup allows us to evaluate the reasoning and generalization capabilities of TLMs. Recent work such as  suggest that language models can be treated as knowledge bases. This directly motivates us to investigate if language models can also learn certain reasoning strategies. Studying these abilities can give us insights to facilitate the use of such models as dynamic knowledge bases that could infer new knowledge even if it is not seen during pre-training.  For natural language theorem proving, we use the question answering CLUTRR benchmark suite  to perform controlled studies. This dataset is of interest because:  the compositional nature of tasks involved make it well suited for evaluating systematic generalization, and  each question--answer pair is accompanied by a proof that can be used to explain how to arrive at the answer. %Our goal is not to obtain state-of-the-art results on this dataset, rather, We use this dataset as a medium to understand the reasoning capacity of TLMs.  Our experiments reveal the following: [itemsep=0pt,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]    to proofs requiring more proof steps than seen during training time.   %In contrast, they suffer less severely to intrapolate to proofs requiring less proof steps than what it encountered during training.   % However, they do interpolate to unseen proofs requiring less proof steps than the most complex proof seen in training.     To the best of our knowledge, we are the first to use a language modeling objective to do interpretable theorem proving with a Transformer. We hope that this work can shed some light on the reasoning capacity of TLMs and inspire future research to design models with greater reasoning capacity.        ========================================================   we are interested in understanding the current limitations of Transformers  In this work, we carefully crafted a series of experiments to understand the systematic generalization capacity of Transformer language models in a symbolic reasoning question answering dataset.  While being powerful language modelers, we believe that if Transformers are to be part of our future personal assistants, they should be able to capture logical statements expressed in natural language and to extrapolate them to unseen proofs. TLMs are state of the art models for a wide variety of natural language processing tasks. Given their widespread use, it is important to understand the limits of their ability to reason on knowledge expressed in natural language and to extrapolate learned inference procedures to unseen problem instances. Our explorations reveal multiple insights. Firstly, TLMs suffer from length-generalization issues in generating proofs. Secondly, TLMs get better at reasoning when trained with longer, exhaustive proofs.  TLMs also generalize better by leveraging backward-chaining proofs than the forward-chaining proofs.  of these properties of Transformers provides a first important evaluation in this setting.   and has led us to insights allowing us to dramatically increase their ability to systematically generalize through a simple named entity transformation. In addition, the fact that backward-chaining proof models perform better than forward-chaining ones makes us believe that backward-chaining strategies are easier to use albeit being harder to generate. Moreover, we find that no-proof models perform better than those trained to produce proofs. We conjecture that benefiting from naturally stated logical proof statements requires more complex internal representations.   At the same time, we believe that in some cases, people would prefer an interpretable system at the cost of slightly lower accuracy.   We will explore both of these directions in future research projects. Recent work on developing position-agnostic attention mechanisms for Transformers  can be useful as a future direction to develop generalizable models. Furthermore, our results motivates the use of neuro-symbolic methods such as Neural Theorem Provers  as an alternative avenue to achieving systems that systematically generalize on logical and compositional reasoning tasks. Combining these approaches with large pre-trained language models is left as future research. We hope that this work will inspire research on the systematic generalization capacity of language models and motivate further study and the creation of neural models with greater reasoning capacity.   rather than with greater number of parameters and training data.   shed some light on their symbolic reasoning capacity of Transformers and inspire future research directions   
"," We are interested in understanding how well Transformer language models  can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies.",106
" Singing voice synthesis  aims to synthesize high-quality and expressive singing voices based on musical score information, and attracts a lot of attention in both industry and academia ~. Singing voice synthesis shares similar pipeline with text to speech synthesis, and has achieved rapid progress~ with the techniques developed in text to speech synthesis~.   Most previous works on SVS~ adopt the same sampling rate  as used in text to speech, where the frequency bands or sampling data points are not enough to convey expression and emotion as in high-fidelity singing voices. However, simply increasing the sampling rate will cause several challenges in singing modeling. First, the audio with higher sampling rate contains wider and higher frequency bands\footnote{According to Nyquist-Shannon sampling theorem~, a sampling rate  can cover the frequency band up to . Therefore, the frequency band for the audio with 48kHz sampling rate spans from 0$, which also increases the difficulty of vocoder modeling in time domain. As a consequence, even if some previous works~ adopt higher sampling rate , they either leverage coarse-grained MFCC~ as acoustic features in slow autoregressive neural vocoder~, or use non-neural vocoder such as Griffin-Lim~ and WORLD~ to generate waveform, which do not fully exploit the potential of high sampling rate and thus cannot yield good voice quality.  In this paper, we develop HiFiSinger, an SVS system towards high-fidelity singing voices. HiFiSinger adopts FastSpeech~ as the acoustic model and Parallel WaveGAN~ as the vocoder since they are popular in speech synthesis~ to ensure fast training and inference speed and also high quality. %. Instead of using Griffin-Lim, WORLD or autoregressive neural model such as WaveRNN and WaveNet as the vocoder, HiFiSinger leverages  To address the challenges of high sampling rate in singing modeling , we design multi-scale adversarial training on both acoustic model and vocoder, and introduce several additional systematic designs and findings that are crucial to improve singing modeling: [leftmargin=*]   We conduct experiments on our internal singing voice synthesis datasets that contain 11 hours high-fidelity singing recordings with 48kHz sampling rate. Experiment results demonstrate the advantages of our developed HiFiSinger over previous singing voice synthesis system. Further ablation studies verify the effectiveness of each design in HiFiSinger to generate high-fidelity voices.     In this paper, we have developed HiFiSinger, an SVS system to synthesize high-fidelity singing voice. To address the challenges caused by high sampling rate, we designed a SF-GAN on acoustic model to better model the wider frequency band, a ML-GAN on vocoder to better model longer waveform sequences, and introduced several systematic designs and findings that are important to improve singing modeling. Experiment results show that HFiSinger synthesizes singing voices with much higher quality than previous systems. For future work, we will continue to close the quality gap between the synthesized voices and recordings, and also apply our fidelity solution in HiFiSinger to text to speech synthesis.         
"," High-fidelity singing voices usually require higher sampling rate  with large range of frequency to convey expression and emotion. However, higher sampling rate causes the wider frequency band and longer waveform sequences and throws challenges for singing modeling in both frequency and time domains in singing voice synthesis . Conventional SVS systems that adopt moderate sampling rate  cannot well address the above challenges. In this paper, we develop HiFiSinger, an SVS system towards high-fidelity singing voice using 48kHz sampling rate. HiFiSinger consists of a FastSpeech based neural acoustic model and a Parallel WaveGAN based neural vocoder to ensure fast training and inference and also high voice quality. To tackle the difficulty of singing modeling caused by high sampling rate , we introduce multi-scale adversarial training in both the acoustic model and vocoder to improve singing modeling. Specifically, 1) To handle the larger range of frequencies caused by higher sampling rate , we propose a novel sub-frequency GAN  on mel-spectrogram generation, which splits the full 80-dimensional mel-frequency into multiple sub-bands  and models each sub-band with a separate discriminator. 2) To model longer waveform sequences caused by higher sampling rate, we propose a multi-length GAN  for waveform generation to model different lengths of waveform sequences with separate discriminators. 3) We also introduce several additional designs and findings in HiFiSinger that are crucial for high-fidelity voices, such as adding F0  and V/UV  as acoustic features, choosing an appropriate window/hop size for mel-spectrogram, and increasing the receptive field in vocoder for long vowel modeling in singing voices. Experiment results show that HiFiSinger synthesizes high-fidelity singing voices with much higher quality: 0.32/0.44 MOS gain over 48kHz/24kHz baseline and 0.83 MOS gain over previous SVS systems. Audio samples are available at \url{https://speechresearch.github.io/hifisinger/}.",107
" %%%%%% % TH % % First we mention the recent progress of TTS systems due to seq2seq and end-to-end training.  Text-to-speech  systems have made great strides with the introduction of sequence-to-sequence  neural models, combined with end-to-end trainable architectures . Neural models typically take character as input and learn a direct mapping to spectrogram or waveform output, without the need for feature engineering.  % Then we explain that the common paradigm is sentence-based synthesis and start defining the vocabulatory for past/future, left/right/full context.   However, most of these neural TTS systems are designed to work at the sentence level, i.e. the synthetic speech signal is generated after the user has typed a complete sentence. When processing a given word, the system can thus rely on its full linguistic context  to build its internal representation. % Now we explain why these paradim is problematic in several context   Despite its ability to generate high-quality speech, this synthesis paradigm is not ideal for several applications. For example, when used as a substitute voice by people with severe communication disorders or integrated in a dialog system , the system's need to wait until the end  of a sentence introduces a latency which might be disruptive to conversational flow and system interactivity.  % Now we introduce iTTS  Incremental TTS  aims to address these issues by synthesizing speech on-the-fly, that is by outputting audio chunks as soon as a new word  become available. This task is particularly challenging since producing speech without relying on the full linguistic context can result in both segmental  and supra-segmental  errors .   % Now we present the state of the art in iTTS  % first for HMM-based synthesis Early iTTS systems were developed in the context of HMM-based speech synthesis . In this paradigm, models are trained on a set of explicit linguistic features . The authors of  developed coping mechanisms to handle missing features when making predictions for iTTS: unknown future context information is replaced with the most common values for these features at inference time in , whereas uncertainty on those features is explicitly integrated at training time by . In , an adaptive decoding policy based on the online estimation of the stability of the linguistic features is proposed: the synthesis of a given word is delayed if its part-of-speech  is likely to change when additional  words are added.    Several strategies have been proposed to reduce the latency of a sequence-to-sequence model with input text for neural machine translation  or incremental speech translation. However, only a few studies have attempted to adapt these models for iTTS . The authors of  proposed an approach that consists in  marking three subunits within the training sentences using start, middle and end tags,  training a Tacotron 2 TTS model with these tags so it learns intrasentential boundary characteristics, and   synthesizing sentences by inputting chunks of length  words  with the appropriate middle or end tag. An alternate policy  reported in  \laurent{ consists in having access to a future context of  input tokens while generating speech output. They also rely on the soft attention to learn the relationship between the predicted spectrogram and the currently available source text.} %triggering the synthesis when the attention weights of the Tacotron decoder have moved past the increment the model is processing.  These two approaches give promising results but introduce a fixed size  latency.   %%%1) splitting the training corpus into incremental units of fixed size   2) training a standard Tacotron 2 while considering each incremental unit as a training sequence, 3) triggering the synthesis each time a new incremental unit is available.   The goal of the present paper is to pave the way toward an adaptive decoding policy for a neural iTTS. Similarly to the HMM-based iTTS system described in , the envisioned neural iTTS is expected to modulate the lookahead  by the uncertainty on some features due to the lack of future context. However, the gain in naturalness provided by end-to-end models  is also accompanied by reduced interpretability. Because of the black box nature of the models, studying the importance of missing features is a challenging task. To address this, %we finely investigate how a neural TTS such as the Tacotron2 exploits the future context to build its internal representations. We  we analyse the evolution of the encoder representations of a neural TTS  when words are incrementally added . We also investigate which text features are the most influential on this evolution towards the final encoder representation. Finally, we evaluate the effect of the lookahead at the perceptual level using a MUSHRA listening test.    %The rest of this paper is organized as follows: we describe our methodology and experimental material in section .  We follow with results and discussion in section . We finally present our conclusions in section .  %%%%% %Most current text-to-speech  systems rely on full sentence input to produce natural sounding speech. This type of system however is not ideal for applications where the user wants to communicate in real time, like in simultaneous speech interpretation or assistive technologies for the speech impaired. \laurent{In those situations, the system has to produce an  output before it has access to the entire input. To deal with this low latency constraint, several strategies were proposed for encoder-decoder models with input text but only a few works have investigated incremental speech translation or incremental text-to-speech synthesis .  This latter task is particularly challenging since}  %with automatic interpreters or assistive technologies for the speech impaired; the latency that waiting until the end of a sentence implies is very disruptive to conversational flow. Conversely, producing speech before the full context is known can result in segmental errors and unnatural prosody.  %\laurent{In this work we study the behavior of a neural sequence-to-sequence TTS system when used in an incremental mode, i.e. when generating speech output for token , the system has access to  tokens from the text sequence.}  %To find a balance between latency and naturalness, we measure the relative importance of different degrees of future context by studying the encoder representations of the Tacotron 2  system.  We believe that by measuring the changes in representation for different values of lookahead parameter  it might be possible define an adaptive decoding policy for iTTS. We also estimate how much future context  is needed to decode a cohesive audio output.  %The rest of this paper is organized as follows: we present in section  related work pertaining to iTTS. We describe our methodology and experimental material in section .  We follow with results and discussion in section . We finally present our conclusions in section .   %     In this study, we investigated the behaviour of a non-incremental sequence-to-sequence TTS system when used in an online manner. This study presents several experiments which probe the impact of future context in a neural TTS system, based on a sequence-to-sequence model, both in terms of encoder representation and perceptual effect. Reported experimental results allow us to draw the contours of an adaptive decoding policy for an incremental neural TTS, which modulates the lookahead  by potential change in internal representations.   The results obtained in our study are first clues that can be exploited to design an adaptive lookahead policy for iTTS:   1)~Shorter words are more dependent on future context than longer ones, and 2) when the lookahead gives access to a short word, it will be less informative than a longer one. It may therefore be more useful to define lookahead in terms of future syllables rather than words.  Shorter words are more dependent on future context than longer ones. Therefore, in a practical iTTS, if the lookahead buffer is fed a short word, it may be preferable to delay its synthesis because  internal representation associated with it is likely to change when additional tokens become available. Also, it may be more useful to define the lookahead parameter in terms of future syllables rather than words.  In addition,  perceptual evaluation shows that the dynamics between  encoder and  decoder are such that even if the encoder representation of an individual token changes slightly, the length of the encoder representation sequence will influence the way in which the decoder treats that token.   By examining the attention weights the decoder uses when making predictions, we see that focus is placed on the current character's immediate surroundings , but focus is also placed on the end of the encoded sequence.  We can conjecture that the decoder is regulating the duration of each segment with respect to sequence length. This will be addressed in future work by examining  attention weights the decoder uses when making predictions.  This is likely to change if the model is retrained in incremental mode. Now that the importance of future context has been assessed, we also plan to work on context extension through prediction of future tokens using contextualized language models .     
"," In incremental text to speech synthesis , the synthesizer produces an audio output before it has access to the entire input sentence. In this paper, we study the behavior of a neural sequence-to-sequence TTS system when used in an incremental mode, i.e. when generating speech output for token $n$, the system has access to $n+k$ tokens from the text sequence. We first analyze the impact of this incremental policy on the evolution of the encoder representations of token $n$ for different values of $k$ . The results show that, on average, tokens travel $88\%$ of the way to their full context representation with a one-word lookahead and $94\%$ after 2 words. We then investigate which text features are the most influential on the evolution towards the final representation using a random forest analysis. The results show that the most salient factors are related to token length. We finally evaluate the effects of lookahead $k$ at the decoder level, using a MUSHRA listening test. This test shows results that contrast with the above high figures: speech synthesis quality obtained with 2 word-lookahead is significantly lower than the one obtained with the full sentence.",108
" Text summarization aims to produce condensed summaries covering salient and non-redundant information in the source documents. Recent studies on single-document summarization  benefit from the advances in neural sequence learning  as well as pre-trained language models  and make great progress.  However, in multi-document summarization  tasks, neural models are still facing challenges and often underperform classical statistical methods built upon handcrafted features.    We observe two major challenges when adapting advanced neural SDS methods to MDS:   Large search space.  MDS aims at producing summaries from multiple source documents, which exceeds the capacity of neural SDS models  and sets learning obstacles for adequate representations, especially considering that MDS labeled data is more limited. For example, there are 287K training samples  on the CNN/Daily Mail SDS dataset and only 30 on the DUC 2003 MDS dataset .  High redundancy. In MDS, the same statement or even sentence can spread across different documents. Although SDS models adopt attention mechanisms as implicit measures to reduce redundancy, they fail to handle the much higher redundancy of MDS effectively .       There have been attempts to solve the aforementioned challenges in MDS. Regarding the large search space, prior studies  perform sentence filtering using a sentence ranker and only take top-ranked  sentences. However, such a hard cutoff of the search space makes these approaches insufficient in the exploration of the  labeled data and limited by the ranker since most sentences are discarded,\footnote{ is set to 7 in~ and 15 in~. One document set in DUC 2004, for example, averages 265.4 sentences.} albeit the discarded sentences are important and could have been favored. As a result, although these studies perform better than directly applying their base SDS models  to MDS,  they do not outperform state-of-the-art MDS methods.  Regarding the high redundancy,  various redundancy measures have been proposed, including heuristic post-processing such as counting new bi-grams  and cosine similarity, or dynamic scoring that compares each source sentence with the current summary like Maximal Marginal Relevance .   Nevertheless, these methods still use lexical features without semantic representation learning. One extension of these studies uses capsule networks to improve redundancy measures. However, its capsule networks are pre-trained on SDS and fixed as feature inputs of classical methods  without end-to-end representation learning.  In this paper, we present a deep RL framework, MMR-guided Reinforcement Learning  for MDS, which unifies advances in SDS and one classical MDS approach, MMR through end-to-end learning. \ours addresses the MDS challenges as follows:  \ours overcomes the large search space through soft attention. Compared to hard cutoff, our soft attention favors top-ranked candidates of the sentence ranker . However, it does not discard low-ranked ones, as the ranker is imperfect, and those sentences ranked low may also contribute to a high-quality summary. Soft attention restrains the search space while allowing more exploration of the limited labeled data, leading to better representation learning. Specifically, \ours infuses the entire prediction of MMR into  its neural module by attending  to important sentences and downplaying the rest instead of completely discarding them.  \ours resolves the high redundancy of MDS in a unified way: the explicit redundancy measure in MMR is incorporated into the neural representation of the current state, and the two modules are coordinated by RL reward optimization, which encourages non-redundant summaries.  We conduct extensive experiments and ablation studies to examine the effectiveness of \ours. Experimental results show that \ours achieves state-of-the-art performance on the DUC 2004 and TAC 2011 datasets . A comparison between various combination mechanisms demonstrates the benefits of soft attention in the large search space of MDS . In addition, ablation and manual studies confirm that \ours is superior to applying either RL or MMR to MDS alone, and MMR guidance is effective for redundancy avoidance .    We present an RL-based MDS framework that combines the advances of classical MDS and neural SDS methods via end-to-end learning.   We show that our proposed soft attention is better than the hard cutoff of previous methods for learning adequate neural representations. Also, infusing the neural representation of the current summary with explicit MMR measures significantly reduces summary redundancy.  We demonstrate that \ours achieves new state-of-the-art results on benchmark MDS datasets.      We present a reinforcement learning framework for MDS that unifies neural SDS advances and Maximal Marginal Relevance  through end-to-end learning. The proposed framework leverages the benefits of both neural sequence learning and statistical measures, bridging the gap between SDS and MDS. We conduct extensive experiments on benchmark MDS datasets and demonstrate the superior performance of the proposed framework, especially in handling the large search space and high redundancy of MDS. In the future, we will investigate the feasibility of incorporating classical MDS guidance to abstractive models with large-scale pre-training  and more challenging settings where each document set may contain hundreds or even thousands of documents.  
"," While neural sequence learning methods have made significant progress in single-document summarization , they produce unsatisfactory results on multi-document summarization . We observe two major challenges when adapting SDS advances to MDS:  MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations;  MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present \ours, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS.  \ours casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that \ours achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.\footnote{Code can be found at \url{https://github.com/morningmoni/RL-MMR}.}",109
" Natural language generators  for task-oriented dialogue take meaning representations  as inputs, i.e. a set of dialogue acts with attributes and their values, and output natural language utterances realizing the MR.  Current NLGs are trained end-to-end with a corpus of MR/utterance pairs where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. However, when building an NLG for a new domain ontology, it should be possible to re-use data built on existing domain ontologies.  If this were possible, it would speed up development of new dialogue systems significantly.  [t!bh]  {p{.15in}|p{.49in}|p{2.8in}|p{2.2in}} \toprule recommend[yes]}, inform & \underline {{I suggest you go to [{ and \underline{atmosphere} \underline{are all excellent}}, even if it is {expensive}. Its in [{, area[area],    {near[point-of-interest]})  &  [{]}} in [{]}}. It has a {\underline{high customer rating}}.   \\ , {eatType[restaurant-type]},  {food = excellent}, location[area], {near[point-of-interest]}, {customer-rating[high]}, {d\'ecor = excellent, service=excellent, price=expensive}) &  {[{.  It is a {[{ in [{]} with a {high customer rating}, but it is {expensive}. \\      blue} and NYC is in {red}. Some attributes are shared    between both sources: here the unique dialogue acts and attributes    for each source are underlined in E1 and E2.  E3 illustrates an MR    from the target test set that we dub COM. All the MRs in COM combine dialogue acts    and attributes from E2E and NYC. There is no training data     corresponding to E3.     %: the goal of the task is to re-use    %the existing training data from E2E and NYC %and train an NLG that    %can generalize to unseen combinations such %as shown in E3.      The MRs    illustrate how some attribute values, e.g. {  Here we experiment with one version of this task by building a new domain ontology based on { ontology not seen in the training data, e.g. for MRs that specify values for {, {.  Figure illustrates this task. Example E1 is from a training set referred to as NYC, from previous work on controllable sentence planning in NLG , while E2 is from the E2E NLG shared task . As we describe in detail in Section, E1 and E2 are based on two distinct ontologies.  Example E3  %in Figure  illustrates the task addressed in this paper: we create a test set of novel MRs for the combined ontology, and train a model to generate high quality outputs where individual sentences realize attributes from both ontologies.  To our knowledge, this is a completely novel task.  While it is common practice in NLG to construct test sets of MRs that realize attribute combinations not seen in training, initial experiments showed that this task is surprisingly adversarial.  However, methods for supporting this type of generalization and extension to new cases would be of great benefit to  task-oriented dialogue systems, where it is  common to start with a restricted set of attributes and then enlarge the domain ontology over time. New attributes are constantly being added to databases of restaurants, hotels and other entities to support better recommendations and better search.  Our experiments test whether existing data that only covers a subset of attributes can be used to produce an NLG for the enlarged ontology.   We describe below how we create a test set --- that we call {\sc com} --- of combined MRs to test different methods for creating such an NLG.  A baseline sequence-to-sequence NLG model has a slot error rate  of .45 and only produces semantically perfect outputs 3.5\% of the time. To improve performance, we experiment with three different ways of conditioning the model by incorporating { produce many {    We start in Section by defining the task in more detail, describe our models and metrics in Section, and results in Section.  We discuss related work throughout the paper where it is most relevant and in the conclusion in Section.         This paper presents the first experiments on training an NLG for an extended domain ontology by re-using existing within-domain training data.  We show that we can combine two training datasets for the restaurant domain, that have different ontologies,  relying on distinct sets of dialogue acts and attributes, and generate output that combines attributes from both sources, by applying a combination of neural supervision and a novel self-training method.  While it is common practice to construct test sets with unseen attribute combinations, we know of no prior work based on constructing a new combined ontology. Our experiments show that the task is surprisingly adversarial, consistent with recent work suggesting that neural models often fail to generalize .  Work on  domain transfer shares similar goals to the experiments presented here  , but these methods do not produce NLG outputs that integrate attributes from two different sources into the same sentence. Our final results show that the ability of our self-training method to automatically construct new training instances  results in high quality natural, coherent and grammatical outputs with high semantic accuracy.    In future, we hope to generalize our novel self-training method to build an NLG that can combine two distinct domains, e.g.  hotels or movies combined with restaurants in multi-domain dialogue . Ideally systems that cover multiple domains should be able to produce utterances that seamlessly integrate both domains, if data exists for each domain independently.  However, there may be additional  challenges in such combinations. Our results require the initial neural models to generate { In    We also plan to investigate whether stylistic attributes from  one source can be injected into utterances from another  source.      
"," Natural language generators  for task-oriented dialogue typically take a meaning representation  as input, and are trained end-to-end with a corpus of MR/utterance pairs, where the MRs cover a specific set of dialogue acts and domain attributes. Creation of such datasets is labor intensive and time consuming. Therefore, dialogue systems for new domain ontologies would benefit from using data for pre-existing ontologies.   Here we explore, for the first time, whether  it  is possible to train  an NLG for a new { ontology. We create a new, larger { method that identifies  model outputs, automatically  constructs a corrected MR input to form a new  training pair, and then repeatedly adds these new instances back into the training data. %that combine attributes from both sources %and then automatically construct an MR that matches the string that %was actually generated .   %We repeatedly construct and add these %new instances back into training, resulting in a self-trained %model that produces semantically perfect outputs 83\% of the time. %We repeatedly construct and add these %new instances back into training, resulting  We then test the resulting model on a new test set. The result is a self-trained model whose performance is an absolute 75.4\% improvement over the baseline model.  %can produce semantically perfect outputs 83\% of the time. %improves the proportion of semantically perfect outputs for the new combined ontology  from 5.5\% to 83\%.  We also report a human qualitative evaluation of the final  model showing that it achieves high naturalness, semantic coherence and grammaticality.",110
" In recent years, neural LMs  have shown profound abilities to generate texts that could be almost indistinguishable from human writings . Neural LMs could be used to generate concise summaries , coherent stories , and complete documents given prompts . It is natural to question their source and extent of rhetorical knowledge: What makes neural LMs articulate, and how?  While some recent works query the linguistic knowledge , this open question remain unanswered. We hypothesize that contextualized neural LMs encode rhetorical knowledge in their intermediate representations, and would like to quantify the extent they encode rhetorical knowledge.  To verify our hypothesis, we hand-craft a set of 24 rhetorical features including those used to examine rhetorical capacities of students , and evaluate how well neural LMs encode these rhetorical features in the representations while encoding texts.  Recent work has started to evaluate encoded features from hidden representations. Among them, probing  has been a popular choice. Previous work probed morphological , agreement , and syntactic features . Probing involves optimizing a simple projection model from representations to features. The loss of this optimization measures the difficulty to decode features from the representations.   In this work, we use a probe containing self attention mechanism. We first project the variable-length embeddings to a fixed-length latent representation per document. Then, we apply a simple diagnostic classifier to detect rhetorical features from this latent representation. This design of probe reduces the total number of parameters, and enable us to better understand each model's ability to encode rhetorical knowledge. We find that: [noitemsep]       These observations allow us to investigate the mechanisms of neural LMs to better understand the degree to which they encode linguistic knowledge. We demonstrate how discourse-level features can be queried and analyzed from neural LMs. All of our code and parsed tree data will be available at github.     In this paper, we propose a method to quantitatively analyze the amount of rhetorical information encoded in neural language models. We compute features based on Rhetorical Structure Theory  and probe the RST features from contextualized representations of neural LMs. Among six popular neural LMs, we find that contextualization helps to generally improve the rhetorical capacities of LMs, while individual models may vary in quality. In general, LMs attending to contexts from both directions  encode rhetorical knowledge in a more stable manner than those using uni-directional contexts  or permuted contexts .  Our method presents an avenue towards quantitatively describing rhetorical capacities of neural language models based on unlabeled, target-domain corpus. This method may be used for selecting suitable LMs in tasks including rhetorical acts classifications, discourse modeling, and response generation.  
"," Recently, neural language models  have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, to date, there has been no analysis of the inter-sentential, rhetorical knowledge.  In this paper, we propose a method that quantitatively evaluates the rhetorical  capacities of neural LMs. We examine the capacities of neural LMs understanding the rhetoric of discourse by evaluating their abilities to encode a set of linguistic features derived from Rhetorical Structure Theory . Our experiments show that BERT-based LMs outperform other Transformer LMs, revealing the richer discourse knowledge in their intermediate layer representations. In addition, GPT-2 and XLNet apparently encode less rhetorical knowledge, and we suggest an explanation drawing from linguistic philosophy. Our method presents an avenue towards quantifying the rhetorical capacities of neural LMs.",111
"  Our WeChat AI team participates in the WMT 2020 shared news translation  task on ChineseEnglish. In this year閳ユ獨 translation task, we mainly focus on exploiting several effective model architectures, better data augmentation, training and model ensemble strategies.  For model architectures, we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with larger filter-size and the average attention based transformer. For the RNMT, we use the deep transition based DTMT model. We finally ensemble four kinds of models in our system.  For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method to leverage the target side monolingual data and the knowledge distillation method to leverage source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source side monolingual data and golden parallel data.  Furthermore, data augmentation methods, including noisy fake data and sampling, are used for training more robust NMT models. %We also apply these techniques on the corresponding side of golden parallel data.  For training strategies, we mainly focus on the parallel scheduled sampling, the target denoising and minimum risk training algorithm for in-domain finetuning.  We also exploit a self-bleu  based model ensemble approach to enhance our system. As a result, our constrained ChineseEnglish system achieves the highest case-sensitive BLEU score among all submitted systems.  In the remainder of this paper, we start with an overview of model architectures in Section.  Section describes the details of our systems and training strategies.  Then Section shows the experimental settings and results.  Finally, we conclude our work in Section.      In this paper, we introduce the system WeChat submitted for the WMT 2020 shared task on ChineseEnglish news translation. Our system is based on the Transformer with different variants and the DTMT architecture. Data selection, several effective synthetic data generation approaches , advanced finetuning approaches  and self-bleu based model ensemble are employed and proven effective in our experiments. Our constrained ChineseEnglish system achieved 36.9 case-sensitive BLEU score which is the highest among all submissions.   
"," We participate in the WMT 2020 shared news translation task on Chinese$\to$English. Our system is based on the Transformer~ with effective variants and the DTMT~ architecture. In our experiments, we employ data selection, several synthetic data generation approaches ,  advanced finetuning approaches and self-bleu based model ensemble. Our constrained Chinese$\to$English system achieves 36.9 case-sensitive BLEU score, which is the highest among all submissions.",112
"  Social media has become an essential element of our society by which people communicate and exchange information on a daily basis. The strong influence of social media on internet users has been of great benefit to many individuals, businesses, and organizations. Many companies and organizations nowadays use social media to reach customers, promote products, and ensure customer satisfaction. Despite the benefits associated with the widespread use of social media, they remain vulnerable to ill-intentioned activities, as the openness, anonymity, and informal structure of these platforms have contributed to the spread of harmful and violent content. \par  Although social media service providers have policies to control these ill-intentioned behaviors, these rules are rarely followed by users. Social media providers also allow their users to report any inappropriate content, but unreported content may not be discovered due to the huge volume of data on these platforms. Some countries have restricted the use of social media, and others have taken legal action regarding violent or harmful content that might target particular individuals or communities. However, these violations might end up unpunished due to the anonymous nature of these platforms, allowing ill-intentioned users to fearlessly share harmful content by using nicknames or fake identities. One of the most-shared harmful content on social media is hate content, which might take different forms such as text, photos, and/or video. Hate speech is any expression that encourages, promotes, or justifies violence, hatred, or discrimination against a person or group of individuals based on characteristics such as color, gender, race, sexual orientation, nationality, religion, or other attributes. Online hate speech is rapidly increasing over the entire world, as nearly \% of the world閳ユ獨 population  communicates on social media. Studies have shown that nearly \% of Americans have experienced online hate and harassment. This result is \% higher than the results of a comparable questionnaire conducted in  . For younger people, the results show that \% of teenagers frequently encounter hate speech on social media.  \par   One of the most dangerous and influential forms of online hate speech is led and spread by supporters of extreme ideologies who target other racial groups or minorities. White supremacists are one of the ideological groups who believe that people of the white race are superior and should be dominant over people of other races; this is also referred to as white nationalism in more radical ideologies. White supremacists claim that they are undermined by dark skin people, Jews, and multicultural Muslims, and they want to restore white people閳ユ獨 power, violently if necessary. They have also claimed responsibility for many violent incidents that happened in the s, including bank robberies, bombings, and murders. The white supremacist ideology has been adopted by both right-wing and left-wing extremists who combine white supremacy with political movements. \par   White supremacist hate speech has become a significant threat to the community, either by influencing young people with hateful ideas or by creating movements to implement their goals in the real world. A study has also suggested links between hate speech and hate crimes against others . Several recent brutal attacks have also been committed by supporters of radical white supremacists who were very active members on social media. The mass shootings in New Zealand, Texas, and Norway were committed by white supremacists who had shared their opinions and ideologies on social media. The attacker of two mosques in Christchurch, New Zealand, was a 28 year old man who identified himself as a white nationalist hero, and posted a manifesto that discussed his intent to kill people as a way to reinforce the sovereignty of white extremists. From a psychological point of view, any violent attack must be preceded by warning behaviors, which includes any behavior that shows before a violent attack that is associated with it, and can in certain situations predict it. Warning behaviors can be either real-world markers  or linguistic markers or signs  which can happen in real life and/or online.  \par   Automatic detection of white supremacist content on social media can be used to predict hate crimes and violent events. Perpetrators can be caught before attacks happen by examining online posts that give strong indications of an intent to make an attack. Predicting violent attacks based on monitoring online behavior would be helpful in crime prevention, and detecting hateful speech on social media will also help to reduce hatred and incivility among social media users, especially younger generations. \par  Studies have investigated the detection of different kinds of hate speech such as detecting cyberbullying , offensive language  , or targeted hate speech in general by distinguishing between types of hate speech and neutral expressions. Others have dealt with the problem by detecting a specific types of hate speech, such as anti-religion, jihadist, sexist, and racist. However, less attention has been given to detecting white supremacism in particular, with limited studies.   \par  White supremacist extremists tend to use rhetoric   in their language. They also use specific vocabulary, abbreviations, and coded words to express their beliefs and intent to promote hatred or encourage violence to avoid being detected by traditional detection methods. They mostly use hate speech against other races and religions, or claim that other races are undermining them. Figure shows an example of a white supremacist tweet.  \par    In this paper, we aim to detect white supremacist tweets based on textual features by using deep learning techniques. We collected about  tweets from white supremacist accounts and hashtags to extract word embeddings, and then we labeled about  subsets of the data corpus to build a white supremacist dataset. We applied two approaches: the first uses domain-specific word embedding learned from the corpus and then classifies  tweets using a Bidirectional LSTM-based deep model. This approach is evaluated on multiple dataset and achieved different results depending on the datasets that ranged from a \% to a \% F1-score. The second approach uses a pre-trained language model that is fine-tune on the white supremacist dataset using Neural Network dense layer. The BERT language model F1-scores ranged from \% to \%. Thus, the research contribution can be summarized as follow:    \par  The rest of the paper proceeds with the Background Section , which provides information on the methodology used, related studies in the Literature Review section , a detailed description of methods in the Methodology section , details of the used datasets in the Dataset section , specifications of the methodologies and the results of each approach in the Experiments and Results section , observations and analysis of the performance of each approach in the Discussion section , and finally, the Conclusion and Future Work section .        The first approach of domain-specific experiments in  , the results show that domain-specific embedding with Bidirectional LSTM model outperforms the results of  who used randomly initialized word embedding with LSTM. Their accuracy was \ , while our accuracy is \ . Although our model exceeds their accuracy, but we expected much higher accuracy than only 2 points, which means that random initialization does not perform very badly. It is important to mention that white supremacist corpus for the pretrained word embedding was about 1 million tweets, increasing the corpus size would provide better performance, but we were limited by Twitter閳ユ獨 policies. This experiment shows that the Bidirectional LSTM based deep model gave good performance for the white supremacy detection, which contradicts, who said that LSTM did not give a good performance because the length of tweets was limited to 180 characters; however, now it is 280 characters. \par From the feature perspective comparison, Table shows how WSW2V performs in comparison with other domain-agnostic models using the same classifier and datasets; the WSW2V outperforms other models on both the Stormfront and Balanced datasets, but GloVe Twitter outperforms WSW2V, and this is because the big size difference of the data trained on, i.e,  for  GloVe Twitter and  for WSW2V. From the classifier perspective comparison, the Bidirectional LSTM-based deep model outperforms LR on two datasets , but LR outperforms the Bidirectional LSTM-based deep model on the Twitter dataset.  \par The second experiment involved using the BERT model on the dataset to assess its performance on the white supremacist hate speech classification task. As shown in Table, BERT outperforms all the distributional-based embeddings  with the Bidirectional LSTM-based deep model in Table. This means that the BERT model gives a closer meaningful vector of the words due to its training strategy  and the large corpus trained on. The BERT language model combines the advantages of domain-agnostic and domain-specific embeddings in its training strategy, it is petrained on a large corpus and add extra layer for training your specific task.  \par Finally, narcissists often use first-person singular pronouns and profane and aggressive language in their social media communications ,  while individuals with an argumentative personality often comment on other people閳ユ獨 posts or frequently post on similar topics to prove their point. White supremacists usually associate themselves with radical groups by either identifying themselves as a member in their profiles or by encouraging or promoting their ideological perspectives. This study focuses on tweets or textual features to detect white supremacy, and not account for profile features. Thus, we only focus on tweet features that help to identify white supremacists閳 characteristics. Further account analysis will be included in future work. 	      From the experiments, we have shown that a combination of word embedding, and deep learning perform well for the problem of white supremacist hate speech. Some of the datasets are imbalanced to simulate real-world data, and others are balanced to assess the model閳ユ獨 performance under an ideal situation. The BERT model has also proved that it provides the state of art for this problem. For future work, the corpus size will be maximized in order to generate more meaningful embeddings, and experiments will be done on multiclass problems instead of binary class problems and by combining Google Word2Vec and domain-specific Word2Vec.   I would like to thank all the researchers who have made their resources available to the research community.    
","  White supremacists embrace a radical ideology that considers white people superior to people of other races. The critical influence of these groups is no longer limited to social media; they also have a significant effect on society in many ways by promoting racial hatred and violence. White supremacist hate speech is one of the most recently observed harmful content on social media. Traditional channels of reporting hate speech have proved inadequate due to the tremendous explosion of information, and therefore, it is necessary to find an automatic way to detect such speech in a timely manner. This research investigates the viability of automatically detecting white supremacist hate speech on Twitter by using deep learning and natural language processing techniques. Through our experiments, we used two approaches, the first approach is by using domain-specific embeddings which are extracted from white supremacist corpus in order to catch the meaning of this white supremacist slang with bidirectional Long Short-Term Memory  deep learning model, this approach reached a 0.74890 F1-score. The second approach is by using the one of the most recent language model which is BERT, BERT model provides the state of the art of most NLP tasks. It reached to a 0.79605 F1-score. Both approaches are tested on a balanced dataset given that our experiments were based on textual data only. The dataset was combined from dataset created from Twitter and a Stormfront dataset compiled from that white supremacist forum.",113
"   Graph Neural Networks  have in recent years been shown to provide a scalable and highly performant means of incorporating linguistic information and other structural biases into NLP models. They have been applied to various kinds of representations  and shown effective on a range of tasks, including relation extraction~, question answering~, syntactic and semantic parsing tasks~, summarization ~, machine translation~ and abusive language detection in social networks~.     While GNNs often yield strong performance, % such models are % complex, and it can be difficult to understand the `reasoning' behind their predictions. For NLP practitioners, it is highly desirable to know which linguistic information a given model encodes and how that encoding happens~. The difficulty in interpreting GNNs represents a barrier to such analysis. %  Furthermore,  this opaqueness decreases user trust% , impedes the discovery of harmful biases, and complicates error analysis% ~,   an issue for GNNs where seemingly small implementation differences can make or break models~.  In this work, we focus on {[topsep=0pt,itemsep=0pt]      across layers, as paths are one of the most natural ways of presenting GNN reasoning patterns to users;      to be applicable to modern GNN-based NLP models;     ~ as possible, providing insights into how the model truly arrives at the prediction.      A simple way to perform interpretation is to use  erasure search~, an approach wherein attribution happens by searching for a maximal subset of features which can be entirely removed without affecting model predictions. % The removal guarantees that all information about the discarded features is ignored by the model. This  contrasts with approaches which use heuristics to define feature importance, for example attention-based methods~ or back-propagation techniques~. They do not guarantee that the model ignores low-scoring features, attracting criticism in recent years . % The trust in erasure search is reflected in the literature through other methods % motivated as approximations of erasure~, or through new attribution techniques % evaluated using erasure search as ground truth~.  Applied to GNNs, erasure search would involve a search for the largest subgraph which can be completely discarded. Besides faithfulness considerations and conceptual simplicity, discrete attributions would also simplify the comparison of relevance between paths; this is in contrast to continuous attribution to edges, where it is not straightforward to extract and visualize important paths. Furthermore, in contrast to techniques based on artificial gradients~, erasure search would provide implementation invariance~. This is important in NLP, as models commonly use highly parametrized decoders on top of GNNs, e.g.~.   While arguably satisfying criteria  and  in our desiderata, erasure search unfortunately fails on tractability. In practical scenarios, it is infeasible, and even approximations, which remove one feature at a time~ and underestimate their contribution due to saturation~,  remain prohibitively expensive.   Our GraphMask aims at meeting the above desiderata by achieving the same benefits as erasure search in a scalable manner. That is, our method makes easily interpretable hard choices on whether to retain or discard edges such that discarded edges have no relevance to model predictions, while remaining tractable and model-agnostic~. GraphMask  can be understood as a differentiable form of subset erasure, where, instead of finding an optimal subset to erase for every given example, we learn an erasure function which predicts for every edge  at every layer  whether that connection should be retained. Given an example graph , our method returns for each layer  a subgraph  such that we can faithfully claim that no edges outside  influence the predictions of the model. To enable gradient-based optimization for our erasure function, we rely on sparse stochastic gates~.  In erasure search, optimization happens individually for each example. This can result in a form of overfitting where even non-superfluous edges are aggressively pruned, because a similar prediction could be made using an alternative smaller subgraph; we refer to this problem as hindsight bias. % Because our model relies on a parametrized erasure function rather than an individual per-edge choice, we can address this issue by amortizing parameter learning over a training dataset through a process similar to the readout bottleneck introduced in~. As we demonstrate in Section, this strategy avoids hindsight bias.   Our contributions are as follows: [nosep]      to analyse GNN models for two NLP tasks: semantic role labeling~ and multi-hop question answering~.      We introduced , a post-hoc interpretation method applicable to any GNN model. By learning end-to-end differentiable hard gates for every message and amortizing over the training data,  is faithful to the studied model, scalable to modern GNN models, and capable of identifying both how edges and paths influence predictions. We applied our method to analyze the predictions of two NLP models from the literature -- a semantic role labeling model, and a question answering model.  uncovers which edge types these models rely on, and how they employ paths when making predictions. While these findings may be interesting per se, they also provide an illustration of types of analysis enabled by .  Here we have focused on applications to NLP, where there is a strong demand for interpretability techniques applicable to graph-based models injecting linguistic and structural priors -- we leave the application of our method to other domains for future work.   Authors want to thank Benedek Rozemberczki, Elena Voita, Wilker Aziz, and Dieuwke Hupkes for helpful discussions. This project is supported by the Dutch Organization for Scientific Research  VIDI 639.022.518, SAP Innovation Center Network and ERC Starting Grant BroadSem . \else \clearpage \fi     \clearpage   
"," Graph neural networks  have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs  contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. % Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected  $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.",114
"    [t]     ''         LUKE outputs contextualized representation for each word and entity in the text.         The model is trained to predict randomly masked words  and entities .         Downstream tasks are solved using its output representations with linear classifiers.}        Many natural language tasks involve entities, e.g., relation classification, entity typing, named entity recognition , and question answering . Key to solving such entity-related tasks is a model to learn the effective representations of entities. Conventional entity representations assign each entity a fixed embedding vector that stores information regarding the entity in a knowledge base  . Although these models capture the rich information in the KB, they require entity linking to represent entities in a text, and cannot represent entities that do not exist in the KB.  By contrast, contextualized word representations  based on the transformer , such as BERT , and RoBERTa , provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling. Many recent studies have solved entity-related tasks using the contextualized representations of entities computed based on CWRs . However, the architecture of CWRs is not well suited to representing entities for the following two reasons:  Because CWRs do not output the span-level representations of entities, they typically need to learn how to compute such representations based on a downstream dataset that is typically small.  Many entity-related tasks, e.g., relation classification and QA, involve reasoning about the relationships between entities. Although the transformer can capture the complex relationships between words by relating them to each other multiple times using the self-attention mechanism , it is difficult to perform such reasoning between entities because many entities are split into multiple tokens in the model. Furthermore, the word-based pretraining task of CWRs is not suitable for learning the representations of entities because predicting a masked word given other words in the entity, e.g., predicting ``Rings'' given ``The Lord of the [MASK]'', is clearly easier than predicting the entire entity.  In this paper, we propose new pretrained contextualized representations of words and entities by developing LUKE . LUKE is based on a transformer  trained using a large amount of entity-annotated corpus obtained from Wikipedia. An important difference between LUKE and existing CWRs is that it treats not only words, but also entities as independent tokens, and computes intermediate and output representations for all tokens using the transformer . Since entities are treated as tokens, LUKE can directly model the relationships between entities.  LUKE is trained using a new pretraining task, a straightforward extension of BERT's masked language model  . The task involves randomly masking entities by replacing them with  entities, and trains the model by predicting the originals of these masked entities. We use RoBERTa as base pre-trained model, and conduct pretraining of the model by simultaneously optimizing the objectives of the MLM and our proposed task. When applied to downstream tasks, the resulting model can compute representations of arbitrary entities in the text using  entities as inputs. Furthermore, if entity annotation is available in the task, the model can compute entity representations based on the rich entity-centric information encoded in the corresponding entity embeddings.  Another key contribution of this paper is that it extends the transformer using our entity-aware self-attention mechanism. Unlike existing CWRs, our model needs to deal with two types of tokens, i.e., words and entities. Therefore, we assume that it is beneficial to enable the mechanism to easily determine the types of tokens. To this end, we enhance the self-attention mechanism by adopting different query mechanisms based on the attending token and the token attended to.  We validate the effectiveness of our proposed model by conducting extensive experiments on five standard entity-related tasks: entity typing,  relation classification, NER,  cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset , relation classification on the TACRED dataset , NER on the CoNLL-2003 dataset , cloze-style QA on the ReCoRD dataset , and extractive QA on the SQuAD 1.1 dataset . We publicize our source code and pretrained representations at \url{https://github.com/studio-ousia/luke}.  The main contributions of this paper are summarized as follows: [leftmargin=10pt,topsep=1pt,itemsep=0pt]          In this paper, we propose LUKE, new pretrained contextualized representations of words and entities based on the transformer. LUKE outputs the contextualized representations of words and entities using an improved transformer architecture with using a novel entity-aware self-attention mechanism. The experimental results prove its effectiveness on various entity-related tasks. Future work involves applying LUKE to domain-specific tasks, such as those in biomedical and legal domains.       
","     Entity representations are useful in natural language tasks involving entities.     In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer .     The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.     Our model is trained using a new pretraining task based on the masked language model of BERT .     The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia.     We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens  when computing attention scores.     The proposed model achieves impressive empirical performance on a wide range of entity-related tasks.     In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity , TACRED , CoNLL-2003 , ReCoRD , and SQuAD 1.1 .     Our source code and pretrained representations are available at \url{https://github.com/studio-ousia/luke}.",115
" Despite the success of self-supervised \pting over \light{large-scale `task-external' data} to learn NLP tasks in a `text-to-text' framework  there are three major, interdependent, open challenges. First, a reliance on large to Web-scale, `end-task external \pting data'  on extensive \pting hardware , create a need for more data efficient models . Second, ``current fine-tuned probes introduce uncontrolled external biases into the evaluation results of text encoders, whereas zero-shot probing avoids these biases'' . Third, concerns about unintended contra-minority biases and resource costs . This created calls for evaluation of   and ``closer to real-world'' long-tail settings, where learnable training signals  that \light{imbalanced, few or zero-shot learning become default settings} . Although improved zero-shot and rare phenomenon  prediction from very limited data is paramount in algorithmic bias and disease detection considerations, current \light{self-supervised} encoder \pting methods require increasingly larger \pting data in NLP  or vision . \light{Unfortunately, since the long-tail grows with data size, simply adding more data and compute only aggravates the bias against long-tail  information and resource cost or access concerns.} %  %  Contributions: To address the above text encoder \pting limitations and evaluation challenges,   and evaluate it under an appropriately small \pting data task  that follows a noisy long-tail class distribution seen in . CLESS proposes self-supervised \pting over pseudo label embeddings. This makes it inherently capable of  zero-shot prediction, unlike methods that rely on supervised \pting for zero-shot prediction  or self-supervision approaches that are incapable of zero-shot prediction  -- details in . By predicting labels as word embeddings and words as pseudo label embeddings, we can . % as text input  and text output  prediction. This trains an NLP task as a `text-to-text' objective like in , but learns to match `text-embedding to label-embeddings', where positive and negative pseudo or real labels are sampled for contrastive training of a single binary match classifier . This classifier is reusable for any unseen label tasks, where new labels are expressed in words and embedded via e.g.\ . % TODO insert mention that we evaluate in a time-split fashion to avoid overly optimistic evaluation as pointed out in https://arxiv.org/pdf/2102.01951.pdf Unlike , CLESS does not require external Web-scale \pting data and can effectively pretrain a text-encoder on 3 orders of magnitude smaller data than for example the size of the English Wikipedia -- details in .  Findings: As a result of contrastive, self-supervised \pting, CLESS boosts minority, long-tail class prediction performance and learning speed considerably -- see , . During few-shot learning, contrastive self-supervised \pting produces large performance improvements and doubled convergence, while also removing learning instability compared to training from scratch, especially in extreme few-shot settings . We also find that  increases zero to few-shot , end-task and long-tail performance over baselines that: are either optimized via generalization methods  or , i.e.\ without self-supervised pseudo label \pting -- details .  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    We showed that label embedding prediction, modified for self-supervised pretraining on a challenging long-tail, low-resource dataset substantially improves low-resource few and zero-shot performance. We find that increased self-supervision, in place of increased data size or resorting to large-scale pretraining, strongly boosts few and zero-shot performance, even in challenging low-resource, long-tail settings. With CLESS we propose a stepping stone towards helping to democratize NLP resource restrictions by minimizing data and thus also compute resource requirements. In future, we envision that CLESS could be applied when little in-domain training data is available, e.g. in medicine , or where new labels emerge at test time as is the case in hashtag prediction .  We also want to investigate advanced label embedding compatible \pting losses, as well as applications to other fields and XAI techniques to improve low-data efficient text-encoder \pting. Code and data splits are available at . 
","  % ALTERNATIVE % core problem % For natural language processing `text-to-text' tasks, the prevailing approaches heavily rely on \pting large self-supervised models on massive external data sources, which led to exceptional \pting data requirements and a diminished ability to effectively pretrain on small data. However, fundamental \pting method capabilities like few to zero-shot learning or preserving minority concept  prediction performance along with accordingly designed evaluation scenarios remain open challenges.  % % core problem For natural language processing `text-to-text' tasks, the prevailing approaches heavily rely on \pting large self-supervised models on massive external data sources, which incurs exceptional \pting data requirements and a diminished ability to pretrain over small datasets. However, fundamental \pting method capabilities like few to zero-shot learning or preserving minority concept  prediction performance along with accordingly designed evaluation scenarios remain open challenges.  % solution We thus propose Contrastive Label-Embedding Self-Supervision  \pting, which enables , while still strongly improving fully supervised, long-tail, few-shot and self-supervised zero-shot learning abilities. % eval Accordingly, we analyse improvements in learning dynamics over baselines on a challenging long-tailed, low-resource, multi-label text classification scenario with noisy, highly sparse labels and many minority concepts.   % result  We find that long-tailed zero and few-shot learning markedly benefit from increasing `dataset-internal' self-supervised \pting signals, to help reduce the reliance on large external sources.",116
"  Modern methods of natural language processing  are based on complex neural network architectures, where language units are represented in a metric space . Such a phenomenon allows us to express linguistic features  mathematically.   The method of obtaining such representation and their interpretations were described in multiple overview works. Almeida and Xex\'eo surveyed different types of static word embeddings , and Liu et al.  focused on contextual representations found in the most recent neural models. Belinkov and Glass  surveyed the strategies of interpreting latent representation. Best to our knowledge, we are the first to focus on the syntactic and morphological abilities of the word representations. We also cover the latest approaches, which go beyond the interpretation of latent vectors and analyze the attentions present in state-of-the-art Transformer models. %analyzed matrix representation of the neural networks. %.    %\tltodo{Maybe use ToC as instroduction to section and remove them from here} %The survey is organized in the following way: %In Section, we introduce several types of NLP models that are going to be analyzed. Section shortly describes the metrics used to evaluate syntactic information captured by the models. The observations and results for static and contextual word embeddings are presented in Section. The observations on attention matrices for different Transformer architectures are described in Section. We summarize our findings in Section. %for attention matrices in Transformer models. %We conclude the survey by mentioning supervised approaches to enhance syntactic signal.   %      Main observations:   1. The unsupervised neural networks capture syntax.   2. Contextual embeddings are more suited for probing for syntactic features than static word embedding.   3. Static word embeddings perform better on task that does not require contextual information, such as syntactic analogies retrival   4. An easy pre-training task, such as auto-encoding, requires syntactic information to lesser extent, therefore it is worse captured by the word embeddings.   5. Embeddings obtained from language models and machine translation system give similar results when probed for part of speech when trained on the corpora of the same size. However, the performance rises with the amount of data and typically language models can be trained with larger corpora, and therfore yield better results in transfer learning to syntactic probing.   6. Some attention matrices in Transformer architecture are aligned with dependency relations.   7. Usually the middle layer of Language Models are more syntactic.   8. In Machine Translation the top layers of the encoder are more syntactic. This may be because of the fact that the model's output is not predicted directly from their output and latent representation is more syntactic.     Word Embeddings and Neural Networks trained on large corpora capture syntactic information.    This phenomena  In this overview, we survey that syntactic structures are latently learned by the neural models for natural language processing tasks.  naturally underlay the natural language and is reflected by unsupervised models. We have compared multiple approaches of others and described the features that affect the ability to capture the syntax. The following aspects tend to improve the performance on syntactic tasks such as POS tagging:  Our meta-analysis of latent states showed that the most syntactic representation could be found in the middle layers of the model. They tend to capture more complex relations than initial layers, and the representations are less dependent on the pretraining objectives than in the top layers.    In this work We have shown to what extent systems trained for a non-syntactic task can learn grammatical structures. The question we leave for further research is whether providing explicit syntactic information to the model can improve its performance on other NLP tasks. ?   
","  Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. %The syntax is captured by the natural language processing models even when not provided as a supervision signal. This %This phenomenon  indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. % This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. %This overview paper covers approaches to evaluating of syntactic information in the representation of words in neural networks. We compare the spectrum of model architectures and the training data. We mainly summarize research on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models.   %Particularly we consider corpora in one language, mainly English used for training Language Models, and multilingual data for Machine Translation Systems and Multilingual Language Models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.  % We hope that our comparison will help in finding pretrained model for transfer   % The survey covers the research on producing representation of language and evaluation of captured syntactic information. I focus on the works that do not use syntactic supervision during training of the representation, and are obtained on large mono or multilingual corpora.  % The aim of this work is to examine to what extent syntactic features can be extracted from plain text and how it can be compared to expert annotations.",117
" Texts represent the main source of knowledge for our society. However, they can be written in various manners, thus creating a barrier between the readers and the ideas they intend to convey. Therefore, document comprehension is the main challenge users have to overcome, by understanding the meaning behind troublesome words and becoming familiar with them. Complex Word Identification  is a task that intends to identify hard-to-understand tokens, highlighting them for further clarification and assisting users to grasping the contents of the document.  Motivation. Each culture includes exclusive ideas, available only for the ones who can pass the obstacle of language. However, properly understanding language can prove to be a difficult task. By identifying complex words, users can make consistent steps towards adapting to the culture and accessing the knowledge it has to offer. As an example, entries like ""mayoritariamente""  or ""gobernatura""  in the Spanish environment can create understanding problems for non-native Spanish speakers, thus requiring users to familiarize themselves with these particular terms.  Challenges. The identification task becomes increasingly more difficult, as proper complex word identification is not guaranteed. For example, if we use human identification techniques, language learners may consider a new word to be complex, while others might not share the same opinion by relying on their prior knowledge in that language. Therefore, universal annotation techniques are required, such that a ground truth can be established and the same set of words is considered complex in any context.  Proposed Approach. We consider state-of-the-art solutions, namely multilingual Transformer-based approaches, to address the CWI challenge. First, we apply a zero-shot learning approach. This was performed by training Recurrent Neural Networks  and Transformer-based models on a source language corpus, followed by validating and testing on a corpus from a target language, different from the source language.  A second experiment consists of a one-shot learning approach that considers training on each of the three languages , but only keeping one entry from the target language, and validating and testing on English, German, Spanish, and French, respectively.   In addition, we performed few-shot learning experiments by validating and testing on a language, and training on the others, but with the addition of a small number of training entries from the target language. The model learns sample structures from the language and, in general, performs better when applied on multiple entries. Furthermore, this training process can help the model adapt to situations in which the number of training inputs is scarce. The dataset provided by the CWI Shared Task 2018  was used to perform all experiments.  This paper is structured as follows. The second section describes related work and its impact on the CWI task. The third section describes the corpus and outlines our method based on multilingual embeddings and Transformer-based models, together with the corresponding experimental setup. The fourth section details the results, alongside a discussion and an error analysis. The fifth section concludes the paper and outlines the main ideas, together with potential extensions.    Complex Word Indentification is a challenging task, even when using state-of-the-art Transformer-based solutions. In this work, we introduce an approach that improves the previous results on the cross-lingual and monolingual CWI shared task 2018 by using multilingual and language-specific Transformer models, multilingual word embeddings , and different fine-tuning techniques. Fine-tuning a model on data from two different languages creates the opportunity of grasping features that empower it to better recognize complex words in certain contexts, even in a different language. In addition, zero-shot, one-shot, and few-shot learning strategies provide good results, surpassing strong baselines  and proposing an alternative to help non-native speakers to properly understand the difficult aspects of a certain language.  For future work, we intend to improve our results on the monolingual tasks by integrating additional models, such as XLNet  and techniques like adversarial training and multi-task learning. Furthermore, we intend to experiment with other pretraining techniques specific to Transformer models, such that the results for French can benefit from cross-lingual transfer learning.  
"," Complex Word Identification  is a task centered on detecting hard-to-understand words, or groups of words, in texts from different areas of expertise. The purpose of CWI is to highlight problematic structures that non-native speakers would usually find difficult to understand. Our approach uses zero-shot, one-shot, and few-shot learning techniques, alongside state-of-the-art solutions for Natural Language Processing  tasks . Our aim is to provide evidence that the proposed models can learn the characteristics of complex words in a multilingual environment by relying on the CWI shared task 2018 dataset available for four different languages . Our approach surpasses state-of-the-art cross-lingual results in terms of macro F1-score on English , German , and Spanish  languages, for the zero-shot learning scenario. At the same time, our model also outperforms the state-of-the-art monolingual result for German .",118
" Aspect based sentiment analysis   is a fine-grained sentiment analysis task. ABSA contains several subtasks, four of which are aspect category detection  detecting aspect categories mentioned in sentences, aspect category sentiment analysis  predicting the sentiments of the detected aspect categories, aspect term extraction  identifying aspect terms presenting in sentences and aspect term sentiment analysis  classifying the sentiments toward the identified aspect terms. While aspect categories mentioned in a sentence are from a few predefined categories and may not occur in the sentence, aspect terms  explicitly appear in sentences. Fig.  shows an example. ACD detects the two aspect categories  and  and ACSA predicts the positive and negative sentiments toward them. ATE identifies the two aspect terms ``taste'' and ``service'' and ATSA classifies the positive and negative sentiments toward them. In this paper, we concentrate on the ACSA task. The ACD task as a auxiliary is used to find aspect category-related nodes from sentence constituency parse trees for the ACSA task.    Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate appropriate sentiment words for given aspect categories. Wang et al.  were the first to explore attention mechanism on the ACSA task and proposed an attention based LSTM . For a given sentence and an aspect category mentioned in the sentence, AT-LSTM first models the sentence via a LSTM model,  then combines the hidden states from the LSTM with the representation of the aspect category to generate aspect category-specific word representations, finally applies an attention mechanism over the word representations to find the aspect category-related sentiment words, that are used to predict the sentiment of the aspect category. The constrained attention networks   handles multiple aspect categories of a sentence simultaneously and introduces orthogonal and sparse regularizations to constrain the attention weight allocation. The aspect-level sentiment capsules model  performs ACD and ACSA simultaneously, which also uses an attention mechanism to find aspect category related sentiment words and achieves state-of-the-art performances on the ACSA task.  However, these models directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. For the example in Fig., ``Great'' and ``bad'' can be used interchangeably. It is hard for attention-based methods to distinguish which word is associated with aspect category  or  among ``good'' and ``bad''. To solve the problem, The HiErarchical ATtention network  first finds the aspect terms indicating the given aspect cagegory, then finds the aspect category-related sentiment words  depending on the position information and semantics of the aspect terms. Although HEAT obtains good results, to train HEAT, we additionally need to annotate the aspect terms indicating the given aspect category, which can be time-consuming and expensive.  To mitigate the mismatch problem, we propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis which does not require any additional annotation. SCAN contains two graph attention networks   and an interactive loss function. Given a sentence, we first use the Berkeley Neural Parser  to generate the constituency parse tree. The two GATs generate representations of the nodes in the sentence constituency parse tree for the ACD task and the ACSA task, respectively. The GAT for ACD mainly attends to the words indicating aspect categories, while the GAT for ACSA mainly attends to sentiment words. For a given aspect category, the interactive loss function helps the ACD task to find the nodes that can predict the aspect category but can閳ユ獩 predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. Fig.  shows the constituency parse tree of the sentence ``Greate taste bad service.''. For the aspect category , SCAN first finds the yellow nodes ``Greate taste'' and ``taste'', then predict the sentiment of  based on the sentiment word ``Great'' in the node ``Great taste''. SCAN excludes the blue node ``Great taste bad service.'' for , because it can predict not only  but also .  The main contributions of our work can be summarized as follows:  	     In this paper, We propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis. The two graph attention modules and the interactive loss function in SCAN form a complete solution to alleviate the mismatch problem. The experimental results on five public datasets demonstrate the effectiveness of SCAN. Future work could consider making the representations of the leaf nodes richer by using syntactic information from the dependency tree of the sentence and modelling the inter-aspect category dependencies.     ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.   
"," Aspect category sentiment analysis  aims to predict the sentiment polarities of the aspect categories discussed in sentences. Since a sentence usually discusses one or more aspect categories and expresses different sentiments toward them, various attention-based methods have been developed to allocate the appropriate sentiment words for the given aspect category and obtain promising results. However, most of these methods directly use the given aspect category to find the aspect category-related sentiment words, which may cause mismatching between the sentiment words and the aspect categories when an unrelated sentiment word is semantically meaningful for the given aspect category. To mitigate this problem, we propose a Sentence Constituent-Aware Network  for aspect-category sentiment analysis. SCAN contains two graph attention modules and an interactive loss function. The graph attention modules generate representations of the nodes in sentence constituency parse trees for the aspect category detection  task and the ACSA task, respectively. ACD aims to detect aspect categories discussed in sentences and is a auxiliary task. For a given aspect category, the interactive loss function helps the ACD task to find the nodes which can predict the aspect category but can闁炽儲鐛 predict other aspect categories. The sentiment words in the nodes then are used to predict the sentiment polarity of the aspect category by the ACSA task. The experimental results on five public datasets demonstrate the effectiveness of SCAN. \footnote{Data and code can be found at https://github.com/l294265421/SCAN}",119
"  With the rapid development of e-commerce, online reviews written by  users  have become increasingly important for reflecting real customer experiences. To ease the process of review writing, the task of personalized review generation~ has been proposed to automatically produce review text conditioned on necessary context data, , while another user may emphasize the .  To address these issues, we propose to improve the PRG task with external knowledge graph . By associating online items with KG entities, we are able to obtain rich attribute or feature information for items, which is potentially useful for the PRG task. Although the idea is intuitive, it is not easy to fully utilize the knowledge information for generating review text in our task. KG typically organizes facts as triples, describing the relation between two involved entities. It may not be suitable to simply integrate KG information to enhance text representations or capture user preference due to varying intrinsic characteristics of different data signals.  In order to bridge the semantic gap, we augment the original KG with user and word nodes, and construct a heterogeneous knowledge graph  by adding user-item links and entity-word links. User-item links are formed according to user-item interactions, and entity-word links are formed according to their co-occurrence in review sentences. We seek to learn a unified semantic space that is able to encode different kinds of nodes. Figure presents an illustrative example for the HKG. Given such a graph, we focus on two kinds of useful information for the PRG task. First, the associated facts regarding to an item  can be incorporated to enrich the review content. Second, considering users as target nodes, we can utilize this graph to infer users' preference  on some specific relation or aspect . The two kinds of information reflect word- and aspect-level enrichment, respectively. To utilize the semantics at the two levels, we decompose  the review generation process into two stages, namely aspect sequence generation and sentence generation.  We aim to inject multi-granularity KG information in different generation stages for improving the PRG task.     To this end, in this paper, we propose a KG-enhanced personalized review generation model based on capsule graph neural networks~. Compared with most of existing GNN-based methods representing graphs as individual scalar features, Caps-GNN can extract underlying characteristics of graphs as  at the graph level through the dynamic routing mechanism and each capsule reflects the graph properties in different aspects. Based on the constructed HKG, we utilize Caps-GNN to extract graph properties in different aspects as , which may be helpful to infer aspect- and word-level user preference. For aspect sequence generation, we propose a novel adaptive learning algorithm that is able to capture personalized user preference at the aspect level, called , from the graph capsules.  We associate an aspect capsule with a unique aspect from unsupervised topic models.   Furthermore, for the generation of sentences, we utilize the learned aspect capsules to capture personalized user preference at the word level. Specially, we design a graph-based copy mechanism to generate related entities or words by copying them from the HKG, which can enrich the review contents.  In this way, KG information has been effectively utilized  at both aspect and word levels in our model.   %To our knowledge, we are the first to utilize knowledge graph to generate personalized review text, which is able to capture both aspect- and word-level KG semantics for learning user preference.  To our knowledge, we are the first to utilize KG to capture both aspect- and word-level user preference for generating personalized review text. For evaluation, we constructed three review datasets by associating items with KG entities. Extensive experiments  demonstrate the effectiveness of KG information and our model. %%         In this paper, we have developed a novel KG-enhanced review generation model for automatically generating informative and personalized review text.  Our core idea is to utilize structural KG data to improve the generated text by incorporating both aspect- and word-level semantics.   Our core idea is to bridge the semantic gap between external knowledge and review text written in natural languages.  For this purpose, we constructed a HKG by augmenting the original KG with user and word nodes. By constructing a HKG, we can learn graph capsules using a Caps-GNN for capturing underlying KG semantics from different aspects.  We designed an aspect-aware two-stage text generation model. In this model, we learned adaptive aspect capsules based on  graph capsules to instruct the prediction for the aspect label. Furthermore, we designed a KG-based copy mechanism for directly incorporating related entities or words from KG.  We constructed extensive experiments on three real-world review datasets. The results showed that our proposed model is superior to previous methods in a series of evaluation metrics for the PRG task.   Currently, only three datasets with aligned entity-item linkage have been used for evaluation. We believe our approach is applicable to more domains. As future work, we will consider integrating more kinds of external knowledge  for the PRG task.  
"," Personalized review generation  aims to automatically produce review text reflecting user preference, which is a challenging natural language generation task. Most of previous studies do not explicitly model  factual description of products, tending to generate uninformative content. Moreover, they mainly focus on word-level generation, but cannot accurately reflect more abstractive  user preference in multiple aspects.  To address the above issues, we propose a novel knowledge-enhanced PRG model  based on capsule graph neural network~. We first  construct a heterogeneous knowledge graph  for utilizing rich item attributes. We adopt  Caps-GNN to learn graph capsules for encoding underlying characteristics from the HKG. Our generation process contains two major steps, namely aspect sequence generation and sentence generation. First, based on graph capsules, we adaptively learn aspect capsules for inferring the aspect sequence.   Then, conditioned on the inferred aspect label, we design a graph-based copy mechanism to generate sentences by incorporating related entities or words from HKG. To our knowledge, we are the first to utilize knowledge graph for the PRG task. The incorporated KG information is able to enhance user preference at both aspect and word levels. Extensive experiments on three real-world datasets have demonstrated the effectiveness of our model on the PRG task.",120
" % significance of sentence functions for dialog Humans express intentions in conversations through sentence functions, such as interrogation for acquiring further information, declaration for making statements, and imperative for making requests and instructions. For machines to interact with humans, it is therefore essential to enable them to make use of sentence functions for dialogue generation. Sentence function is an important linguistic feature indicating the communicative purpose of a sentence in a conversation. There are four major sentence functions: Declarative, Interrogative, Exclamatory and Imperative . Each major sentence function can be further decomposed into fine-grained ones according to different purposes indicated in conversations. For example, Interrogative is divided into Wh-style Interrogative, Yes-no Interrogative and other types. These fine-grained sentence functions have great influences on the structures of utterances in conversations including word orders, syntactic patterns, and other aspects . Figure  presents how sentence functions influence the responses.  Given the same query expressed in Positive Declarative, the responses expressed in Wh-style Interrogative and in Negative Declarative are completely different.    % challenges, quantitatively list some numbers Although the use of sentence functions improves the overall quality of generated responses , it suffers from the data imbalance issue. For example, in the recently released response generation dataset with manually annotated sentence functions STC-SeFun , more than 40\% of utterances are Positive Declarative while utterances annotated with Declarative with Interrogative words account for less than 1\%  of the entire dataset. Therefore, dialogue generation models suffer from data deficiency for these infrequent sentence functions.  % proposed method Recently, model-agnostic meta-learning ~  has shown promising results on several low-resource natural language generation  tasks, including neural machine translation , personalized response generation  and domain-adaptive dialogue generation . They treat languages of translation, personas of dialog and dialog domains as separate tasks in MAML respectively. In the same spirit of previous works, we first treat dialogue generation conditioned on different sentence functions as separate tasks, and meta-train a dialogue generation model using high-resource sentence functions. Moreover, we observe that sentence functions have hierarchical structures: four major sentence functions can be further divided into twenty fine-grained types. Some fine-grained sentence functions may share some similarities while some others are disparate. For example, utterances belong to Wh-style Interrogative and Yes-no Interrogative may share some transferable word patterns while utterances in Wh-style Interrogative and in Exclamatory with interjections totally differ from each other.  Motivated by this observation, we explore a structured meta-learning  considering inherent structures among fine-grained sentence functions. Inspired from recent advances on learning several initializations with a set of meta-learners , we develop our own approach to utilize the underlying structure of sentence functions. More specifically, our proposed SML explicitly tailors transferable knowledge among different sentence functions. It utilizes the learned representations of fine-grained sentence functions as parameter gates to influence the globally shared parameter initialization. Therefore, conversation models for similar sentence functions can share similar parameter initializations and vice versa. As a result, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions.  % experiments The experimental results on STC-SeFun dataset  show that responses generated from our proposed structured meta-learning algorithm are of better quality over several baselines in both human and automatic evaluations.  Moreover, our proposed model can generate responses consistent with the target sentence functions while baseline models may ignore the target sentence functions or generate some generic responses. We further conduct a detailed analysis on our proposed model and show that it indeed can learn word orders and syntactic patterns for different fine-grained sentence functions.       In this paper, we propose a structured meta-learning algorithm for open domain dialogue generation on infrequent sentence functions. To tackle the low-resource issue, our proposed model, based on the recently proposed model-agnostic meta-learning, can find both transferable internal representations and sensible parameters which can produce large improvement under a few adaptation steps. Moreover, we further explore the structure across fine-grained sentence functions and such that the model can balance knowledge generalization and knowledge customization. Extensive experiments show that our structured meta-learning  algorithm outperforms existing approaches under the low-resource setting.   
"," Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained sentence functions is extremely imbalanced. Besides a small number of high-resource sentence functions, a large portion of sentence functions is infrequent. Consequently, dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency. In this paper, we investigate a structured meta-learning  approach for dialogue generation on infrequent sentence functions. We treat dialogue generation conditioned on different sentence functions as separate tasks, and apply model-agnostic meta-learning to high-resource sentence functions data.  Furthermore, SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions.  Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses, but also can generate responses consistent with the target sentence functions.",121
"  The desire for human-like interfaces to technical systems, as evidenced by growing use of intelligent assistants, belies the need for conversational AI systems that can accomplish a wide range of tasks, such as booking restaurants, trains, and flights, IT help desk and accessing financial accounts and transaction records. The wide range of tasks have necessitated the need for a flexible and scalable dialogue system that can support a variety of use cases with minimal development and maintenance effort. Existing dialogue systems are broken into two major categories,  open-domain dialogue systems, which focus on non-task related conversations, and task-oriented dialogue systems, which focus on user task completion. A typical open-domain system uses an end-to-end neural architecture often trained with input and output utterances from human-to-human conversations . While open-domain systems are optimized for engaging in human-like conversation, they lack any inherent ability to interface with any other systems on behalf of their conversation partner. Whereas, a typical task-oriented dialogue system seeks to understand human intents and execute them. This is done by adopting a modularized pipeline architecture with three modules that are sequentially connected as shown in Fig. . A natural language understanding  module that recognizes user intents and extract useful entity information . The dialogue management  module contains two submodules, the dialogue state tracker  and the dialogue action policy  modules. The DST module tracks the mapping of entities to slots that are relevant or required for completing user tasks . The POL module decides which actions to execute via the API. Finally, the natural language generation  module generates the user response based on the user aspects of the system actions . In some cases, multiple modules are combined together, e.g. systems with a composite NLU and DST module , and systems with a composite POL and NLG module that maps previous utterances and dialogue states to the system response .  Despite research advances in modular neural approaches, they are hardly used in practice. Industrial dialogue systems, though modularized, still use expensive expert driven rule-based heuristics implemented with several lines of codes and hand-crafted templates, and therefore difficult to scale as the number of use cases grows. More recently, there has been a renewed effort to apply a single end-to-end neural architecture  to model task-oriented dialogue with the use of autoregressive transformer architecture . This has led to the reformulation of dialogue system design as a text generation or sequence modeling task. While some of these efforts have obtained state-of-the-art performance on publicly available task-oriented dialogue datasets, there is still room for improvement, especially in the areas of generality and practicality. First, their problem formulation fails to reconcile open-domain and task-oriented dialogue in the same model architecture. Also, in many cases, they do not address the complexity of the action policy especially towards the back-end API system. Finally, they don't fully incorporate the control, verification and explanation capabilities that make modularized approaches attractive.  To resolve these shortcomings, we propose DLGNet-Task, an end-to-end neural network that simultaneously handles both open-domain and task-oriented dialogue, in such a way that the model outputs are controllable, verifiable, and explainable at the module level. This system is compatible with both data driven and expert driven rule-based approaches.   That is, our approach is simultaneously modular and end-to-end, and can be a drop-in replacement for traditional modular task-oriented dialogue  systems. To the best of our knowledge, this is the most expressive approach to date in achieving this objective. In summary, we are able to model the individual behavior of NLU, DM and NLG components with a single neural network model trained end-to-end. Still, the model is flexible enough to allow individual modules to be separately trained and validated in line with the traditional TOD system.  % Validation at module level can provide information about where additional training is needed. It could also help in balancing the contribution of each module if the model is finetuned with module-level objectives.  % The DLGNet-Task model is based on the autoregressive transformer architecture similar to DLGNet  and GPT-2/3  models. To evaluate the performance of DLGNet-Task, we trained the model with just the system-level training objective on a modified MultiWoz2.1 dataset. The dataset modification is done mainly to support DLGNet-Task design framework . Based on the widely used TOD metrics, such as inform rate, success rate, and BLEU score , our experiments show that DLGNet-Task produces a comparable performance to the state-of-the-art approaches on the MultiWoz2.1 dataset.  % in addition to the controllable, verifiable, and explainable model's intermediate outputs.       In this paper, we have proposed DLGNet-Task, an end-to-end neural network framework for modeling multi-turn multi-domain task-oriented dialogue. The DLGNet-Task model learns the joint distribution of the nodes  of a dialogue flow graph capable of representing both task-oriented and open-domain dialogue systems. For TOD specific applications, DLGNet-Task is also capable of learning the action policy towards the back-end API. Experimental results show that DLGNet-Task gives comparable performance with existing approaches with practical focus. The results also showed that performance of DLGNet is hampered by the errors in the original MultiWoz dataset as well as the noise introduced during the data processing.    While DLGNet-task framework is capable to learning a controllable, verifiable and explainable end-to-end model.  This also shows need for consistent TOD datasets with properly defined dialogue flow graph. We hope to explore this direction as part of our future work both in terms of dataset generation and data processing pipeline. We also hope to improve DLGNet-Task model performance with adversarial and reinforcement learning.     {A\arabic{table}}      
"," Task oriented dialogue  requires the complex interleaving of a number of individually controllable components with strong guarantees for explainability and verifiability. This has made it difficult to adopt the multi-turn multi-domain dialogue generation capabilities of streamlined end-to-end open-domain dialogue systems. In this paper, we present a new framework, DLGNet-Task, a unified task-oriented dialogue system which employs autoregressive transformer networks such as DLGNet and GPT-2/3 to complete user tasks in multi-turn multi-domain conversations. Our framework enjoys the controllable, verifiable, and explainable outputs of modular approaches, and the low development, deployment and maintenance cost of end-to-end systems. Treating open-domain system components as additional TOD system modules allows DLGNet-Task to learn the joint distribution of the inputs and outputs of all the functional blocks of existing modular approaches such as, natural language understanding , state tracking, action policy, as well as natural language generation . Rather than training the modules individually, as is common in real-world systems, we trained them jointly  with appropriate module separations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows comparable performance to the existing state-of-the-art approaches. Furthermore, using DLGNet-Task in conversational AI systems reduces the level of effort required for developing, deploying, and maintaining intelligent assistants at scale.  % significant improvement over existing approaches, and achieves state-of-the-art performance at both the module and system levels.",122
"   Knowledge graphs  represent knowledge of the world as relationships between entities, i.e., triples with the form  . Such knowledge resource provides clean and structured evidence for many downstream applications such as question answering. KGs are usually constructed by human experts, which is time-consuming and leads to highly incomplete graphs . Therefore automatic KG completion  is proposed to infer a missing link of relationship  between a head entity  and a tail entity .    Existing KG completion work mainly makes use of two types of information: 1) co-occurrence of entities and relations and 2) deducible reasoning paths of tuples. KG embeddings encode entities and relations, the first type of information, together into continuous vector space with low-rank tensor approximations~.  Ours approach utilizes the second type of information, reasoning path of tuples that can be deduced to the target tuple~. Here a reasoning path starts with the head entity  and ends with the tail entity : {\rightarrow} e_1  \overset{r_k}{\rightarrow} e_k \overset{r_N}{\rightarrow} t}, where  forms a relation chain that infers the existence of . Therefore these methods are also referred as multi-hop reasoning over KGs, which learns a multi-hop chain as a rule to deduce the target . An example of such a chain is given in Figurea to infer whether an athlete plays in an location. Multi-hop reasoning approaches can usually utilize richer evidence and self-justifiable in terms of  reasoning path rules used in the predictions, making the prediction of missing relations more interpretable.   Despite  advantages and  success of the multi-hop reasoning approach , a target relationship may not be perfectly inferred from a single relation chain. There could exist multiple weak relation chains that correlate with the target relation. Figure gives examples of such cases.  These multiple chains could be leveraged in following ways:  the reasoning process naturally relies on the logic conjunction of multiple chains ;  more commonly, there are instances for which none of the chains is accurate, but aggregating multiple pieces of evidence improves the confidence , as also observed in the case-based study works. Inspired by these observations, we propose the concept of  multi-chain multi-hop rule set.  Here, instead of treating each single multi-hop chain as a rule, we learn rules consisting of a small set of multi-hop chains. Therefore the inference of target relationships becomes a joint scoring of such  a set of chains. {We  treat each set of chains as one rule and, since different query pairs can follow different rules, together we have  a set of rules to reason each relation.}  Learning the generalized multi-hop rule set is a combinatorial search problem.  We address this challenge with a game-theoretic approach inspired by. Our approach consists of two steps:  selecting a generalized multi-hop rule set by employing a Multi-Layer Perceptron  over the candidate chains;   reasoning with the generalized rule set, which uses another MLP to model the conditional probability of the target relationship given the selected relation chains. The nonlinearity of MLP as reasoner provides the potential to model the logic conjunction among the selected chains in the rule set.  We demonstrate the advantage of our method on KG completion tasks in FB15K-237 and NELL-995. Our method outperforms existing single-chain approaches, showing that our defined generalized rules are necessary for many reasoning tasks.    We propose a new approach of multi-chain multi-hop rule learning for knowledge graph completion tasks. First, we formalize the concept of multi-hop rule sets with multiple relation chains from knowledge graphs. Second, we propose a game-theoretical learning approach to efficiently select predictive relation chains for a query relation. Our formulation and learning method demonstrate advantages on two benchmark datasets over existing single-chain based approaches. For future work, we plan to investigate rules beyond chains, as well as integrate KG embeddings into our framework.   
"," Multi-hop reasoning approaches over knowledge graphs infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where each rule is a set of relation chains.  To learn such generalized rules efficiently, we propose a two-step approach that first selects a small set of relation chains as a rule and then evaluates the confidence of the target relationship by jointly scoring the selected chains. A game-theoretical framework is proposed to this end to simultaneously optimize the rule selection and prediction steps. Empirical results show that our multi-chain multi-hop  rules result in superior results compared to the standard single-chain approaches, justifying both our formulation  of  generalized rules  and the effectiveness of the proposed learning framework.",123
" Recently, there has been great success in automatic text summarization and generation. To better compare and improve the performance of models, evaluation for such systems has been a problem of interest. The selection of evaluation metrics will greatly affect the assessed quality of a generated summary and thus affect the evaluation of summarization models.   The most ideal metric is definitely human judgement, which is often treated as the gold standard. But human evaluation is time-consuming and labor-intensive, an automatic evaluation metric that cannot only save human resources but also simulate the ability of human judgement is of crucial importance.   Most of the existing automatic evaluation methods assess a summary by comparing it with reference texts written by humans. Some of them are model-free and simply use hand-crafted matching functions to calculate the similarity between the candidate summary and the reference  . These methods consider both the reference and the candidate as a sequence of tokens or n-gram blocks. For instance, as the de facto standard evaluation metric, ROUGE  calculates the n-gram overlap between the machine-generated summaries and reference summaries. Although these methods have the advantage of interpretability and efficiency, they are found to correlate poorly with human evaluation.   To reduce the requirement of exact word matching, some recent work tried to match the reference and the candidate summary in the embedding space of words or sentences . For instance, BERTScore  uses contextual word embeddings generated by BERT and performs a greedy matching to obtain the maximum cosine similarity between two texts. %  designed a metric that combines sentence-level embeddings with the word mover閳ユ獨 distance   to calculate the distance of moving the candidate sequence into the reference and transforms the distance into a similarity score, while MoverScore  combines n-gram embeddings with WMD.   These methods are proved to correlate better with human judgement than ROUGE on many datasets, which demonstrates the effectiveness of using contextual embeddings.  [ht] { {lccc}  \toprule        & Semantic & Linguistic  & Else \\              \makecell[l]{~DUC-05, DUC- 06 and DUC-07\\ }  & \makecell[c]{focus, \   & \makecell[c]{coherence,\\fluency}  &-   \\         \makecell[l]{~NYT and CNN/Daily Mail \\} & informativeness   & \makecell[c]{grammaticality,\\ coherence}  &-  \\   }  , all the three dimensions focus on evaluating the linguistic quality of summaries.}    However, the aforementioned methods all have some intrinsic drawbacks: these methods always need at least one human-generated reference to assess a candidate summary. References written by humans are costly to obtain. In addition, most of them only consider the semantic similarities with references, i.e. semantic qualities of the summaries, which ignores the linguistic qualities and other important aspects. In this paper, we propose a new unsupervised contrastive learning framework for automatically evaluating the summary qualities without comparing with reference summaries or training with human ratings. Specifically, we design an evaluator to consider both linguistic and semantic aspects of a summary. Then for each of the aspect we create a set of negative samples by perturbing the training samples. We compare the scores of original training samples and the negative samples to obtain the contrastive loss function and learn the evaluator. The experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method has much higher correlation with human judgement.  We summarize our contributions as follows:            In this paper, we propose a new evaluation method in the field of text summarization. We found that the quality of a summary can be evaluated in two separate dimensions: semantic quality and linguistic quality. Since human-authored references used in most of the existing metrics are costly, we investigate automatic evaluation metrics in an unsupervised reference-free setting. Leveraging powerful representations of BERT, our methods achieve the highest performance on two datasets.  Although our experiments are only on single-document summarization datasets, our method can also be also extended to evaluation of multi-document summarization with slight changes, especially in the part of semantic quality evaluation.    
"," Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets.",124
" Part-of-speech  tags and dependency parsing have formed a long-standing union in NLP. But equally long-standing has been the question of its efficacy. % of this union. %POS tags as features for parsers.   %Certainly in the nigh-on forgotten pre-deep learning era of NLP, it seemed as if they were useful for syntactic disambiguation in certain contexts  . However, for neural network implementations, especially those which utilise character embeddings, POS tags have been shown to be much less useful .   Others have found that POS tags can still have a positive impact when using character representations given that the accuracy of the predicted POS tags used is sufficiently high .  undertook a systematic study of the impact of features for Universal Dependency  parsing and found that using universal POS  tags does still offer a marginal improvement for their transition-based neural parser. The use of fine-grained POS tags still seems to garner noticeable improvements %even for challenging multi-lingual settings  .   %By far and away the most common use of  Latterly, POS tags have been commonly utilised implicitly for neural network parsers in multi-learning frameworks where they can be leveraged without the cost of error-propagation . Beyond multi-learning systems,  introduced dependency parsing as sequence labelling by encoding dependencies using relative positions of UPOS tags, thus explicitly requiring them at runtime. %So even if coarse POS tags, universal or otherwise, prove to be superfluous for graph- or transition-based neural parsers as direct features, there are still many uses for them.% in dependency parsing.   We follow the work of  and evaluate the interplay of word embeddings, character embeddings, and POS tags as features for two modern parsers, one a graph-based parser, Biaffine, and the other a transition-based parser, UUParser . Similar to , we focus on the contribution of POS tags but evaluate UPOS tags.   We analyse the effect UPOS accuracy has on two dependency parser systems for a number of UD treebanks. Our results suggest that in order to leverage UPOS tags as explicit features for these neural parsers, a prohibitively high tagging accuracy is needed, and that gold tag annotation seems to possess some exceptionality. We also investigate what aspects of predicted UPOS tags have the most impact on parsing accuracy.    We have evaluated the impact POS tag accuracy has on parsing performance for leading graph- and transition-based parsers across a diverse range of UD treebanks, highlighting the stark difference between using predicted POS tags  and gold POS tags at runtime. We observed a non-linear increase in performance when using gold tags, suggesting they are somehow exceptional.   at runtime and using gold POS tags with a non-linear increase in performance when using gold tags suggesting gold tagged annotation are somehow exceptional.   This was further corroborated by our experiment using treebanks for which we could obtain very high scoring taggers.  Our analysis also shows that practitioners should evaluate the efficacy of using predicted tags for a given system or language.   rather than assuming they won't have a negative impact.   Beyond the global conclusions drawn from our analysis,  We have also analysed what aspects of erroneous tagging predictions have the greatest impact and correlation to parsing performance. We observed some global trends, ,} but also language-specific issues which highlight the need to evaluate the usefulness of POS tags per language. The results also suggest that using a subset of POS tags might be effective.  and potentially even per treebank.    
"," We present an analysis %contributing to the discussion  on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as features for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem.",125
"  Conversational Machine Reading  is challenging because the rule text may not contain the literal answer, but provide a procedure to derive it through interactions . In this case, the machine needs to read the rule text, interpret the user scenario, clarify the unknown user's background by asking questions, and derive the final answer. Taking Figure  as an example, to answer the user whether he is suitable for the loan program, the machine needs to interpret the rule text to know what are the requirements, understand he meets ``American small business'' from the user scenario, ask follow-up clarification questions about ``for-profit business'' and ``not get financing from other resources'', and finally it concludes the answer ``Yes'' to the user's initial question.    Existing approaches  decompose this problem into two sub-tasks.  Given the rule text, user question, user scenario, and dialog history , the first sub-task is to make a decision among ``Yes'', ``No'', ``Inquire'' and ``Irrelevant''. The ``Yes/No'' directly answers the user question and ``Irrelevant'' means the user question is unanswerable by the rule text. If the user-provided information  are not enough to determine his fulfillment or eligibility, an ``Inquire'' decision is made and the second sub-task is activated. The second sub-task is to capture the underspecified condition from the rule text and generate a follow-up question to clarify it.  adopt BERT  to reason out the decision, and propose an entailment-driven extracting and editing framework to extract a span from the rule text and edit it into the follow-up question.  The current , Contradiction or Neutral by reading the user scenario description and existing dialog. Then we map the scores to an entailment vector for each condition, and reason out the decision based on the entailment vectors and the logical structure of rules. Compared to previous methods that do little entailment reasoning  or use it as multi-task learning , \modelnameshort is the first method to explicitly build the dependency between entailment states and decisions at each dialog turn.   \modelnameshort achieves new \sota results on the blind, held out test set of ShARC. In particular, \modelnameshort outperforms the previous best model EMT  by 3.8\% in micro-averaged decision accuracy and 3.5\% in macro-averaged decision accuracy. Specifically, \modelnameshort performs well on simple in-line conditions and conjunctions of rules while still needing improvements on understanding disjunctions. Finally, we conduct comprehensive analyses to unveil the limitation of \modelnameshort and current challenges for the ShARC benchmark. We find one of the biggest bottlenecks is the user scenario interpretation, in which various types of reasoning are required. % Code and models will be released to facilitate research along this line.     In this paper, we present \modelnameshortnsp, a system that does discourse-aware entailment reasoning for conversational machine reading. \modelnameshort explicitly builds the connection between entailment states of conditions and the final decisions. Results on the ShARC benchmark shows that \modelnameshort outperforms existing methods by a large margin.  We also conduct comprehensive analyses to unveil the limitations of \modelnameshort and challenges for ShARC. In future, we plan to explore how to incorporate discourse parsing into the current decision making model for end-to-end learning. One possibility would be to frame them as multi-task learning with a common  encoder.  Another direction is leveraging current methods in question generation  to improve the follow-up question generation sub-task since \modelnameshort\ is on par with the previous best model EMT.  
","  Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose \modelnameshortnsp, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding for both document and dialog. Specifically, we split the document into clause-like elementary discourse units  using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision ``yes/no/irrelevant"" of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark  show that \modelnameshort achieves .",126
"   .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Neural Language Models  have become a central component in NLP systems over the last few years, showing outstanding performance and improving the state-of-the-art on many tasks . However, the introduction of such systems has come at the cost of interpretability %and explainability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. % and, specifically, of understanding how linguistic predictors - that were common as features in earlier systems - are encoded in such models.  Recent work has begun to study these models in order to understand whether they encode %are able to learn  linguistic phenomena even without being explicitly designed %forse meglio trained?  to learn such properties . Much of this work focused on the analysis and interpretation of attention mechanisms  and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations.   Probing models trained  on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena  and even to organize this information in a hierarchical manner . However, the way in which this knowledge affects the decisions they make when solving specific downstream tasks has been less studied.  In this paper, we extended prior work by studying the linguistic properties encoded by one of the most prominent NLM, BERT , and how these properties affect its predictions when solving a specific downstream task. %,  using a suite of more than 80 probing tasks.  % qui vedere se tenere 'several' perch鑼 abbiamo 10 task di classificazione o dire che 鐚 uno solo diviso in 10 ""sotto-task"". We defined three research questions aimed at understanding:  what kind of linguistic properties are already encoded in a pre-trained version of BERT and where across its 12 layers;  how the knowledge of these properties is modified after a fine-tuning process;  whether this implicit knowledge %of these properties  affects the ability of the model to solve a specific downstream task, i.e. Native Language Identification . %With this aim, we firstly perform a very large suite of probing tasks using %on %DOMI: SPOSTIAMO QUESTA PARTE %To answer the first two questions, we firstly perform a very large suite of probing tasks using %on %the sentence representations extracted from the internal layers of BERT. Each of these tasks makes explicit a particular property of the sentence, from very shallow features  to more complex aspects of morpho--syntactic and syntactic structure , thus making them as particularly suitable to assess the implicit linguistic knowledge encoded in a NLM at a deep level of granularity. %with respect to a wide spectrum of phenomena overing lexical, morpho-syntactic and syntactic structure.  To tackle the first two questions, we adopted an approach inspired to the `linguistic profiling' methodology put forth by , which assumes that wide counts of linguistic features automatically extracted from parsed corpora allow modeling a specific language variety and detecting how it changes with respect to other varieties, e.g. complex vs simple language, female vs male--authored texts, texts written in the same L2 language by authors with different L1 languages.  Particularly relevant for our study, is that multi-level linguistic features have been shown to have a highly predictive role in tracking the evolution of learners' linguistic competence across time and developmental levels, both in first and second language acquisition scenarios .  %when leveraged by traditional learning models on a variety of text classification problems, all of which can be successfully tackled using formal, rather than content based aspects of a text: from the assessment of sentence complexity and text readability , to the identification of personal and sociodemographics traits of an author, such as his/her native language, gender, age etc.  and to the prediction of the evolution of learners' linguistic competence across time . %From this perspective, our approach can be considered as a particular implementation of the `linguistic profiling' methodology put forth by , which assumes that wide counts of linguistic features automatically extracted from parsed corpora allow modeling a specific language variety and detecting in what way it changes with respect to other varieties, e.g. complex vs simple language, female vs male--authored texts, texts written in the same L2 language by authors with different L1 languages. Given the strong informative power of these features to encode a variety of language phenomena across stages of acquisition, we assume that they can be also helpful to dig into the issues of interpretability of NLMs. In particular, we would like to investigate whether features successfully exploited to model the evolution of language competence can be similarly helpful in profiling how the implicit linguistic knowledge of a NLM changes across layers and before and after tuning on a specific downstream task. We chose the NLI task, i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language .  %Secondly, we investigate the type and degree of variations of linguistic information before and after fine-tuning the pre-trained model on 10 distinct  datasets used to solve Native Language Identification , i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language .   As shown by , linguistic features play a very important role when NLI is tackled as a sentence--classification task rather than as a traditional document--classification task.  %NLI can be addressed by exploiting only linguistic features extracted at sentence--level reaching comparable performance to those obtained by state--of--the--art models based on word embeddings .  This is the reason why we considered the sentence-level NLI classification as a task particularly suitable for probing the NLM linguistic knowledge. %perch鑼 鐚 un task che per essere risolto 鐚 necessario che il modello codifichi un'ampia gamma di informazioni linguistiche e anche perch鑼 鐚 un task basato sull'info estratta dalla sentence -come dimostrato da Cimino et al  nonostante lo stato dell'arte 鐚 stato definito soltanto usando word embeddings  %vecchia versione: a fine-tuning process based on a Native Language Identification  downstream task.  %vecchia versione: -base and 10 fine-tuned models obtained training BERT on as many Native Language Identification  tasks.  Finally, we investigated whether and which linguistic information encoded by BERT is involved in discriminating the sentences correctly or incorrectly classified by the fine-tuned models. To this end, we tried to understand if the linguistic knowledge that the model has of a sentence affects the ability to solve a specific downstream task involving that sentence.   %vecchia versione: Adopting a suite of more than 80 probing tasks, we firstly perform % We perform our experiments using a suite of more than 80 probing tasks, each of which corresponds to a specific/distinct sentence-level feature. We find that / We show that  %The remainder of the paper is organized as follows. We start by presenting some related works which are more closely related to our study  and in Section  we highlight the main novelties of our approach. We then describe in more details the data , the probing tasks  and the models  we used. Experiments and results are described in Section ,  and . To conclude, in Section  we summarize the main findings of the study.   In this paper:  we carried out an in-depth linguistic profiling of BERT's internal representations %deep analysis of the implicit linguistic knowledge stored in BERT's internal representations and how it changes across layers using a wide suite of sentence-level probing tasks, corresponding to a wide spectrum of linguistic phenomena at different level of complexity; % we verify the implicit linguistic knowledge stored in BERT's internal representations using a suite of more than 80 probing tasks corresponding to a wide range of linguistic phenomena at different level of complexity;   we showed that contextualized representations tend to lose their precision in encoding a wide range of linguistic properties %general-purpose linguistic properties  after a fine-tuning process; % RIVEDERE 'GENERAL-PURPOSE' COME TERMINE PER DESCRIVERE LE NOSTRE FEATURES  we showed that the linguistic knowledge stored in the contextualized representations of BERT positively affects its ability to solve NLI downstream tasks: the more BERT stores information about these features% in its embeddings/internal representations , the higher will be its capacity of predicting the correct label.      In this paper we studied what kind of linguistic properties are stored in the internal representations learned by BERT before and after a fine-tuning process and how this implicit knowledge correlates with the model predictions when it is trained on a specific downstream task. Using a suite of 68 probing tasks, we showed that the pre-trained version of BERT encodes a wide range of linguistic phenomena across its 12 layers, but the order in which probing features are stored in the internal representations does not necessarily reflect the traditional division with respect to the linguistic annotation levels. We also found that BERT tends to lose its precision in encoding our set of probing features after the fine-tuning process, probably because it is storing more task--related information for solving NLI.  QUI NON SERVE : Interestingly, we noticed that features encoding verbal tense knowledge are the ones that decreases significantly for all the fine-tuned models.  We thus think that further work needs to be done to investigate what kind of discriminant linguistic properties properties emerge after a fine-tuning process.  This is particularly evident for the models fine-tuned on the classification of language pairs belonging to the same family .   Se possibile Scrivere meglio: Finally, we showed that the implicit linguistic knowledge encoded by BERT positively affects   is strongly correlated with its ability to solve the tested downstream tasks.  In particular, we first showed that, regardless of the layer and model taken into account, most of the probing features are involved in discriminating the sentences correctly or incorrectly classified by the fine-tuned models. Second, we noticed that for such features the probing model performance show an improvement when BERT correctly predicts the L1 of a native speaker, and this is especially true for the pre--trained model. This suggests that its capacity to encode linguistic information has an influence on its predictions.   decisions.   In future work, we would like to extend our approach to other NLMs, such as ELMo  or XLNet , and to investigate how the linguistic information implicitly encoded in such models affects different downstream tasks.   The demonstrated influence of linguistic competence of NLM on classification tasks would allow us to develop NLMs able to maximize    In future work, we plan to study how the linguistic information encoded in a NLM arise during training, performing the probing tasks on several sentence representations extracted in the pre-training phase. The aim of this investigation is studying new strategies to maximize the linguistic competence of a NLM, for example adding during the pre-training process specific linguistic tasks.   Moreover, it would be interesting to study how the linguistic information encoded in a NLM arise and evolve as these models are trained, performing the probing tasks on several sentence representations extracted during the pre-training process.  DA FELICE: se riusciamo aggiungere anche: Un ulteriore campo di indagine sar鑴 quello di generare NLM massimizzando la loro competenza linguistica ad esempio adding at the pre-training process specific linguistic tasks.    include your own bib file like this: 
"," In this paper we investigate the linguistic knowledge learned by a Neural Language Model  before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT's capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence.",127
"   Recent emergent-communication studies, renewed by the astonishing success of neural networks, are often motivated by a desire to develop neural network agents eventually able to verbally interact with humans . To facilitate such interaction, neural networks' emergent language should possess many natural-language-like properties. However, it has been shown that, even if these emergent languages lead to successful communication, they often do not bear core properties of natural language .  In this work, we focus on one basic property of natural language that resides on the tendency to use messages that are close to the informational optimum. This is illustrated in the Zipf's law of Abbreviation , an empirical law that states that in natural language, the more frequent a word is, the shorter it tends to be . Crucially, ZLA is considered to be an  property of our language .  Besides the obvious fact that an efficient code would be easier to process for us, it is also argued to be a core property of natural language, likely to be correlated with other fundamental aspects of human communication, such as regularity and compositionality . Encouraging it might hence lead to emergent languages that are also more likely to develop these other desirable properties.   Despite the importance of such property,   showed that standard neural network agents, when trained to play a simple signaling game , develop an inefficient code, which even displays an ZLA pattern. That is, counterintuitively, more frequent inputs are coded with longer messages than less frequent ones. This inefficiency was related to  neural networks' ``innate preference'' for long messages. In this work, we aim at understanding which constraints need to be introduced on neural network agents in order to overcome  their innate preferences and communicate efficiently, showing a proper ZLA pattern.  To this end, we %follow  and use a reconstruction game where we have two neural network agents: speaker and listener. For each input, the speaker outputs a sequence of symbols  sent to the listener. The latter needs then to predict the speaker's input based on the given message. Also, similarly to the previous work, inputs are drawn from a power-law distribution.   We first describe the experimental and optimization framework . In particular, we introduce a new communication system called `LazImpa', comprising two different constraints  iness on the speaker side and  tience on the listener side. The former constraint is inspired by the least-effort principle which is attested to be a ubiquitous pressure in human communication .   However, if such a constraint is applied too early, the system does not learn an efficient system. We show that incrementally penalizing long messages in the cost function enables an early exploration of the message space  and prevents converging to an inefficient local minimum.   The other constraint, on the listener side, relies on the prediction mechanism, argued to be important in language comprehension , and is achieved by allowing the listener to reconstruct the intended input as soon as possible. We also provide a two-level analytical method: first, metrics quantifying the efficiency of a code; second, a new protocol to measure its informativeness . Applying these metrics, we demonstrate that, contrary to the standard speaker/listener agents, our new communication system `LazImpa' leads to the emergence of an efficient code. The latter follows a  distribution, close to natural languages . Besides the plausibility of the introduced constraints, our new communication system is, first, task- and architecture-agnostic , and second allows stable optimization of the speaker/listener. We also show how both listener and speaker constraints are fundamental to the emergence of a ZLA-like distribution, as efficient as natural language .     We demonstrated that a standard communication system, where standard Speaker and Listener LSTMs are trained to solve a simple reconstruction game, leads to long messages, close to the maximal threshold. Surprisingly, if these messages are long, LSTM agents rely only on a small number of informative message symbols, located at the end. We then introduce LazImpa, a constrained system that consists of y Speaker and tient Listener. On the one hand, Lazy Speaker is obtained by introducing a cost on messages length once the communication is successful. We found that early exploration of potentially long messages is crucial for successful convergence . On the other hand, Impatient Listener aims to succeed at the game as soon as possible, by predicting Speaker's input at each message's symbol.   We show that both constraints are  for the emergence of a ZLA-like protocol, as efficient as natural languages. Specifically, Lazy Speaker alone would fail to shorten the messages. We connect this to the importance of the Impatience mechanism to locate useful information at the beginning of the messages. If the function of this mechanism is subject to a standing debate , many prior works had pointed to its necessity to human language understanding . We augment this line of works and suggest that impatience could be at play in the emergence of ZLA-obeying languages. However, if impatience leads to ZLA, it is not sufficient for human-level efficiency. In other words, efficiency needs constraints  on Speaker and Listener sides.   Our work highlights the importance of introducing the right pressures in the communication system. Indeed, to construct automated agents that would eventually interact with humans, we need to introduce task-agnostic constraints, allowing the emergence of more human-like communication. Moreover, while being general, LazImpa provides a more stable optimization compared to the unconstrained system. Finally, this study opens several lines of research. One would be to investigate further the gap from optimality. Indeed, while LazImpa emergent languages show human-level efficiency, they do not reach optimal coding. Specifically, emergent languages still have non-informative symbols at the end of the messages. If these additional non-useful symbols drift the protocol from optimality, we encounter similar trend in human  and animal communication  . We leave the understanding of the role of these non-informative symbols and how we can reach optimal coding for future works. A second line of research would be to apply this system to other games or NLP problems and study how it affects other properties of the language such as regularity or compositionality.   
"," Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes.  This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation  observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, ``LazImpa'', where the speaker is made increasingly y, i.e.,~avoids long messages, and the listener tient, i.e.,~seeks to guess the intended content as soon as possible.",128
"  Relation extraction  aims at extracting relational facts between entities from text, e.g., extracting the fact  from the sentence in Figure.  Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs , and eventually support downstream applications like question answering~, dialog systems~ and search engines~. With the recent advance of deep learning, neural relation extraction  models~ have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks.  % During the development of relation extraction systems, there have been pattern-based methods , feature-based methods , kernel-based methods , graphical models , etc. With the recent advance of deep learning and pre-trained language models , relation extraction systems with neural networks  have achieved new state-of-the-arts.    % Relation extraction  aims at extracting relational facts between entities from textual corpora. For example, for the entity pair  and the given sentence `` was founded in 2002 by \underline{Elon Musk}}'', we can extract the relationship   to form the fact triple  with the entity pair. From unstructured data , RE extracts structural formats of knowledge , construct  knowledge graphs, and eventually support downstream applications like question answering, recommender systems and search engines.   % During the development of relation extraction systems, there have been pattern-based methods , feature-based methods , kernel-based methods , graphical models , etc. With the recent advance of deep learning and pre-trained language models , relation extraction systems with neural networks  have achieved new state-of-the-arts.     The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions .   %So which kind of information do existing models rely on more? % So what kind of information do these two sources provide?  From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure, ``'' is a pattern for the relation . The early RE systems formalize patterns into string templates and determine relations by matching these templates. The later neural models prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better.  Besides, entity mentions also provide much information for relation classification. As shown in Figure , we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE~. Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training~.  %From human intuition, both the two sources contribute to relation extraction, and textual context should take a larger proportion. Entity types can help models filter out impossible relations . Other knowledge linked to the entities may help to infer the relation. However, for human, the evidence of final classification mainly comes from the context, because even for unknown entity names or blanked entity, in most cases we can correctly determine the relations.   In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that:   Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities.   We also get the following observations:    In certain circumstances, entity type information is necessary for correct classification.   Entity mentions also provide other information besides type that helps to understand text.   Blocking entity mentions will mislead models to incorrect predictions even on those sentences with strong relational patterns.  % Even with strong relational patterns, RE models rely on the entity mentions instead of contextual text for correct classification sometimes, suggesting the lack of understanding of context.  %In fact, most existing models rely heavily on entity names so that blocking them causes significant drop on performance. Further more, we notice that   Existing RE benchmarks may leak shallow cues via entity mentions, which contribute to the high performance of existing models. Our experiments show that models still can achieve high performance only given entity mentions as input, suggesting that there exist biased statistical cues from entity mentions in these datasets.   %suggesting that they can exploit superficial statistical cues in the mentions. % suggesting that the distribution of test data may be too easy.  %These two factors suggest that though existing models have achieved high performance on RE datasets, more efforts are needed towards robust and unbiased RE systems.  The above observations demonstrate how existing models work on RE datasets, and suggest a way to further improve RE models: we should enhance them via better understanding context and utilizing entity types, while preventing them from simply memorizing entities or exploiting biased cues in mentions.  From these points, we investigate an entity-masked contrastive pre-training framework for RE. % to better understand relational semantics in contexts while avoiding being biased by memorization or shallow patterns of the entity mentions.  We use Wikidata to gather sentences that may express the same relations, and let the model learn which sentences are close and which are not in relational semantics by a contrastive objective. % in relational semantics. In this process, we randomly mask entity mentions to avoid being biased by them.  We show its effectiveness across several settings and benchmarks, and suggest that better pre-training technique is a reliable direction towards better RE.   %The rest of the paper is organized as follows: xxx.      In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that  both context and entity mentions  provide critical information for relation extraction, and  existing RE datasets may leak superficial cues through entity mentions and models may not have the strong abilities to understand context as we expect. From these points, we propose an entity-masked contrastive pre-training framework for RE to better understand textual context and entity types, and experimental results prove the effectiveness of our method.   In the future, we will continue to explore better RE pre-training techniques, especially with a focus on open relation extraction and relation discovery. These problems require models to encode good relational representation with limited or even zero annotations, and we believe that our pre-trained RE models will make a good impact in the area.    We believe that our RE-oriented pre-trained models will make a further impact on open scenario problems.   
","  Neural models have achieved remarkable success on relation extraction  benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions . We find that  while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and  existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at \url{https://github.com/thunlp/RE-Context-or-Names}.",129
" % 1 - What problem are you solving? Entity typing classifies textual mentions of entities, according to their semantic class, within a set of labels  organized in an inventory. %Multi-label text classification is the task of assigning to a sample all the relevant labels from a label  inventory . The task has progressed from recognizing a few coarse classes , to extremely large inventories, with hundreds  or thousands of labels . Therefore, exploiting inter-label correlations has become critical to improve performance.   % 2 - Why is it an interesting/important problem? % es interesante porque son buenos para modelar redes y estructuras jer璋﹔quicas. % Problema: su adopcion en nlp ha sido baja dado que no hay una forma muy intuitiva de modelar texto en ellos. Distintos papers muestran como agregar un peque甯給 cambio pero no una aplicacion real y completa Large inventories tend to exhibit a hierarchical structure, either by an explicit tree-like arrangement of the labels , or implicitly through the label distribution in the dataset . %A natural solution for dealing with large inventories is to organize them in hierarchy ranging from general, coarse labels near the top, to more specific, fine classes at the bottom. Prior work has integrated only explicit hierarchical information by formulating a hierarchy-aware loss  or by representing instances and labels in a joint Euclidean embedding space .  However, the resulting space is hard to interpret, and these methods fail to capture implicit relations in the label inventory. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures . Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance from the root  . %Its tree-like properties make it efficient to learn hierarchical representations with low distortion .     % Embeddings  that  are  close  to  the  origin  of  the  disk  will have a relatively small distance to all other points, rep-resenting the root of the hierarchy.  On the other hand,embeddings that are close to the boundary of the disk will have a relatively large distance to all other points and are well suited to represent leaf nodes   % 3 - How are you going to solve it? In this work, we propose a fully hyperbolic neural model for fine-grained entity typing. Noticing a perfect match between hierarchical label inventories in the linguistic task and the benefits of hyperbolic spaces, we endow a classification model with a suitable geometry to capture this fundamental property of the data distribution. By virtue of the hyperbolic representations, the proposed approach automatically infers the latent hierarchy arising from the class distribution and achieves a meaningful and interpretable organization of the label space. This arrangement captures implicit hyponymic relations  in the inventory and enables the model to excel at fine-grained classification. To the best of our knowledge, this work is the first to apply hyperbolic geometry from beginning to end to perform multi-label classification on real NLP datasets.  %NICE PHRASE FROM GULCEHRE: The focus of this work is to endow neural network representations with suitable geometry to capture fundamental properties of data... given the perfect fit between the label distribution in the linguistic task of entity typing and the mathematical properties of hyperbolic spaces.   % esto deberia ser ""hay componentes ya hechos"". Y lo conecto al toque con el parrafo sig.  Recent work has proposed hyperbolic neural components, such as word embeddings , recurrent neural networks  and attention layers . %Advantages of hyperbolic representations are well-established for discrete data such as networks  and graphs . In the realm of Natural Language Processing  components that exploit hyperbolic geometry have been developed as well, such as word embeddings , recurrent neural networks  and attention layers . %or classifiers  Me encanta este paper pero no hace NLP :. We address these issues. Our model encodes textual inputs, applies a novel attention mechanism, and performs multi-class multi-label classification, executing all operations in the Poincar\'e model of hyperbolic space . %By employing the leveraging the geometric properties of hyperbolic space through    %The lack of systems that utilize hyperbolic space from beginning to end is due to three main difficulties: %First, there are different analytic models of hyperbolic space, and not all previous work operates in the same one, which hinders their combination.  %Second, it is not clear how to integrate these components into conventional Euclidean neural models since a mapping of the data from one space onto the other is required. Third, optimization of hyperbolic models is non-trivial.   %We bridge the gaps among previous work by developing the missing connections and adapting different components to employ the Poincar\'e model of hyperbolic space in all layers of the network.  % We bridge the gaps among previous work by developing the missing connections and adapting different components, in order to accomplish a full hyperbolic neural network. This is, a network that extracts features from text, applies attention layers and performs \todo{I am the only one doing this}{multi-class classification}, executing all operations in hyperbolic geometry.   % able to perform multi-label multi-class classification with text as input    %The model is proposed in a generic manner such that it can be applied to classify sequential data . Since hyperbolic geometry is naturally equipped to model hierarchical structures, we hypothesize that the model will excel at tasks that profit from the incorporation of hierarchical information. % \todo{awful}{systems} that operate under this metric space result in superior performance when incorporating hierarchical information.   %We evaluate our model on the task of fine-grained entity type classification , which we consider a suitable testbed due to its connection with textual inputs and hierarchical type inventories.  % Introduce main results % HNN's phrase: ""On a series of experiments and datasets we showcase the effectiveness of our hyperbolic neural network layers compared to their ""classic"" Euclidean variants on"" % \todo[inline]{Forwarding a bit of the results is a good idea . %\todo[inline]{Cambiar esta frase a la idea de que ""imponer the right metric es como imponer the right bias""}  %We impose an inductive bias on the model by means of the geometry of its internal representation. This allows us to operate on very low-dimensional spaces thus substantially reducing the parameter cost. Instead of relying on large pre-trained models, we impose a suitable inductive bias by choosing an adequate metric space to embed the data, which does not introduce extra burden on the parameter footprint. %Phrase from xiong2019inductiveBias: ""Instead of using an explicit graphical model, we enforce a relational bias on model parameters, which does not introduce extra burden on label decoding."" % Misma idea pero yo meto el bias en la representacion, lo cual no introduce un costo adicional y permite operar con MUCHOS menos par璋﹎etros.   %Our components are developed in a modular way which allows them to be seamlessly integrated into NLP architectures.    %\todo{Remove!}{While there now exist several hyperbolic components, a practitioner faced with these options has a simple question: How to integrate them with conventional layers? In this work, we answer this question.}  By means of the exponential and logarithmic maps  we are able to mix hyperbolic and Euclidean components into one model, aiming to exploit their strengths at different levels of the representation. We perform a thorough ablation that allows us to understand the impact of each hyperbolic component in the final performance of the system , and showcases its ease of integration with Euclidean layers.  %In summary, we make the following contributions: %%%%% % %      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      1) X  is an important problem    2) The core challenges are this and that.    3) Previous work on X has addressed these with Y, but the problems with this are Z.    4) In this work we do W .    5) This has the following appealing properties and our experiments show this and that.  Incorporating hierarchical information from the label inventory into neural models has become critical to improve performance. Hyperbolic spaces are an exciting approach since they are naturally equipped to model hierarchical structures. However, previous work integrated  isolated components into neural systems.  In this work we propose a fully hyperbolic model and showcase its effectiveness on challenging datasets.  Our hyperbolic model automatically infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory and achieves a performance comparable to state-of-the-art systems on very fine-grained labels with a remarkable reduction of the parameter size.  This emphasizes the importance of choosing a metric space suitable to the data distribution as an effective inductive bias to capture fundamental properties, such as hierarchical structure.  Moreover, we illustrate ways to integrate different components with Euclidean layers, showing their strengths and drawbacks. An interesting future direction is to employ hyperbolic representations in combination with contextualized word embeddings. We release our implementation with the aim to ease the adoption of hyperbolic components into neural models, yielding lightweight and efficient systems.   Add future work! En Gulcehre citan un paper y dicen ""future work seria hacer lo mismo que CITE, pero co hyperbolic whatever.   Para mi future work podr閾哸 ser explorar variations de HyperbolicMLR para paliar algunas de las desventajas de Softmax .  De Gulcehre: ""Similarly as a future work, an interesting potential future direction is to use hyperbolic..."", or say clearly that I do not use contextualized word embeddings and future work: explore hyperbolic representation in combination with contextualized word embeddings   
"," Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions.  Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers. \footnote{Code available at:\\ \url{https://github.com/nlpAThits/hyfi}}",130
"  Entity Recognition  involves detection  and classification of entities mentioned in unstructured text into pre-defined categories. It is one of the foundational sub-task of several Information Extraction   and Natural Language Processing  pipelines. Hence, errors introduced during the extraction of entities can propagate further and degrade the performance of the complete IE or NLP pipeline. In the domains of experimental biology, the growing complexity of experiments has resulted in a need to automate wet laboratory procedures. Such an automation will be useful in avoiding human errors introduced in the wet lab protocols and thereby will enhance the reproducibility of experimental biological research.   To achieve this reproducibility, some of the previous research works have focussed on defining machine-readable formats for writing wet lab protocols . However, the vast majority of today閳ユ獨 protocols are written in natural language with jargon and colloquial language constructs that emerge as a byproduct of ad-hoc protocol documentation. This motivates the need for machine reading systems that can interpret the meaning of these natural language instructions, to enhance reproducibility via semantic protocols  and enable robotic automation  by mapping natural language instructions to executable actions. In order to enable research on interpreting natural language instructions, with practical applications in biology and life sciences, an annotated database  of wet lab protocols was introduced.   The first step in interpreting natural language lab protocols is to extract entities, followed by identification of relations between them. To address the research focussing on entity recognition over Wet Lab Protocols a shared task  was introduced at EMNLP WNUT-2020 Workshop. The task was based on the annotated database  of wet lab protocols. We tackle this task in two phases. In the first phase, we experiment with various contextualised word embeddings  and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and SLE.  The rest of the paper is structured as follows: Section 2 states the task definition. Section 3 describes the specifics of our methodology. Section 4 explains the experimental setup and the results, and Section 5 concludes the paper.     Through this paper, we showcased our approach to tackle the Shared Task 1 in EMNLP WNUT-2020 Workshop which involved Entity Recognition over Wet Lab Protocols. We solved the task in two phases. The first phase involved experimenting with different contextualised word embeddings like BERT and Flair, and a BiLSTM-CRF model to find the best performing model configuration for the problem at hand. In the second phase, we create an ensemble consisting of eleven BiLSTM-CRF models. We train individual models on randomly shuffled train-validation splits of the complete dataset. Also, we experiment with different merging techniques like Majority Voting and Structured Learning Ensemble . Our end solution achieved a micro F1-score of 0.8175 and 0.7757 in the partial and exact match categories, respectively. We were ranked first and second in partial and exact match categories respectively. In the future, we wish to explore the idea of employing rule-based approach to overcome the shortcomings of current solution.      ACL provides this description and accompanying style files at      We strongly recommend the use of these style files, which have been appropriately tailored for the EMNLP 2020 proceedings.       The templates include the \LaTeX2e{} source ,   an example bibliography .     
"," In this paper, we describe the approach that we employed to address the task of Entity Recognition over Wet Lab Protocols - a shared task in EMNLP WNUT-2020 Workshop. Our approach is composed of two phases. In the first phase, we experiment with various contextualised word embeddings  and a BiLSTM-CRF model to arrive at the best-performing architecture. In the second phase, we create an ensemble composed of eleven BiLSTM-CRF models. The individual models are trained on random train-validation splits of the complete dataset. Here, we also experiment with different output merging schemes, including Majority Voting and Structured Learning Ensembling . Our final submission achieved a micro F1-score of 0.8175 and 0.7757 for the partial and exact match of the entity spans, respectively. We were ranked first and second, in terms of partial and exact match, respectively.",131
"  %  %       Reinforcement learning has shown great success in environments with large state spaces. Using neural networks to capture state representations has allowed end-to-end training of agents on domains like Atari  and Go . It is natural to emulate this success in text domains, especially given that the state space in language-based tasks is combinatorially large. A sentence of length  with allowed vocabulary  has  possible states, and tabular methods like learning  will fail unless coupled with powerful function approximators like neural networks.\\  While the current state of RL has multiple challenges, sparse rewards are one that leads to slow, and sometimes no convergence. Consider an agent learning in an environment with a large state space, with only a few states leading to a reward . An agent starting on the far left must take a large number of actions before encountering a reward. In turn, this sparse feedback results in a very noisy gradient for training the neural network. In an extreme scenario, as in Figure , an agent might have to take an exponential number of actions to reach a single leaf that has a reward.      Some early work, such as reward shaping , attempted to solve the sparse reward problem by introducing dense rewards based on heuristics, e.g., how close the agent is to the goal. However, these require complex design choices that might result in unexpected behavior from the agents.\\  Sparse rewards are common because they are the most straightforward way to specify how a task needs to be solved. If a robot is expected to pour water from a jug into a glass, the simplest way is to give a reward of  if it fills the glass, and  otherwise. This type of reward design is common in text-based games, in which the agent is rewarded upon reaching the goal state, and task-oriented dialogue, in which the agent is rewarded based on the successful completion of the task.\\  For this study, we examine text-based games and find that providing dense rewards with the help of sentiment analysis improves performance under some conditions.     We find that adding auxiliary rewards using sentiment analysis can help improve RL agents' performance in text domains. Our methods take a step in the direction of creating agents that infers rewards by themselves. We expect that these improvements are applicable to similar text-based domains, such as task-oriented dialogue. Given the rapid improvements in NLP methods, we believe that better pre-training and sentiment analysis models will translate to better RL agents in the future.  
"," While reinforcement learning  has been successful in natural language processing  domains such as dialogue generation and text-based games, it typically faces the problem of sparse rewards that leads to slow or no convergence. Traditional methods that use text descriptions to extract only a state representation ignore the feedback inherently present in them. In text-based games, for example, descriptions like ``Good Job! You ate the food'' indicate progress, and descriptions like ``You entered a new room'' indicate exploration. Positive and negative cues like these can be converted to rewards through sentiment analysis. This technique converts the sparse reward problem into a dense one, which is easier to solve. Furthermore, this can enable reinforcement learning without rewards, in which the agent learns entirely from these intrinsic sentiment rewards. This framework is similar to intrinsic motivation, where the environment does not necessarily provide the rewards, but the agent analyzes and realizes them by itself. We find that providing dense rewards in text-based games using sentiment analysis improves performance under some conditions.",132
"  Natural language data is , but most of the structure is not visible at the surface.  Machine learning models tackling high-level language tasks would benefit from uncovering underlying structures such as trees, sequence tags, or segmentations.  Traditionally, practitioners turn to  approaches where an external, pretrained model is used to predict, , combining the transparency of the pipeline approach with the end-to-end unsupervised representation learning that makes deep models appealing. Moreover, large-capacity model tend to rediscover structure from scratch , so structured latent variables may reduce the required capacity.  Learning with discrete, combinatorial latent variables is, however, challenging, due to the intersection of  and  issues. For example, when learning a latent dependency tree, the latent parser must choose among an exponentially large set of possible trees; what's more, the parser may only learn from gradient information from the downstream task. If the highest-scoring tree is selected using an  operation, the gradients will be zero, preventing learning.  One strategy for dealing with the null gradient issue is to use a surrogate gradient, explicitly overriding the zero gradient from the chain rule, as if a different computation had been performed. The most commonly known example is the  , which pretends that the  node was instead an  operator. Such methods lead to a fundamental mismatch between the objective and the learning algorithm. The effect of this mismatch  is still insufficiently understood, and the design of successful new variants is therefore challenging. For example, the recently-proposed SPIGOT method  found it beneficial to use a projection as part of the surrogate gradient.  In this paper, we study surrogate gradient methods for deterministic learning with discrete structured latent variables. Our contributions are:   , thereby inducing pseudo-supervision on the latent variable. This leads to new insight into both STE and SPIGOT.   While the discrete methods do not outperform the relaxed alternatives using the same building \linebreak blocks, we hope that our interpretation and insights would trigger future latent structure research.  The code for the paper is available on \url{https://github.com/deep-spin/understanding-spigot}.          In this work, we provide a novel motivation for straight-through estimator  and SPIGOT, based on pulling back the downstream loss. We derive promising new algorithms, and novel insight into existing ones. Unstructured controlled experiments suggest that our new algorithms, which use the cross-entropy loss instead of the perceptron loss, can be more stable than SPIGOT while accurately disentangling the latent variable. Differentiable relaxation models  are the easiest to optimize to high downstream accuracy, but they fail to correctly identify the latent clusters. On structured NLP experiments, relaxations  tend to overall perform better and be more stable than straight-through variants in terms of classification accuracy. However, the lack of gold-truth latent structures makes it impossible to assess recovery performance. We hope that our insights, including some of our negative results,  may encourage future research on learning with latent structures.   
"," Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on , a popular strategy to deal with this problem. We explore latent structure learning through the angle of  the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator  as well as the recently-proposed SPIGOT---a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.",133
"     than apricot---the student needs less data to train and can generalize better.  We show how this principle can apply equally well to improve unsupervised topic modeling, which to our knowledge has not previously been attempted.  While distillation usually involves two models of the same type, it  also apply to models of differing architectures. Our method is conceptually quite straightforward: we fine-tune a pretrained transformer  on a document reconstruction objective, where it acts in the capacity of an autoencoder. When a document is passed through this BERT autoencoder, it generates a distribution over words that includes unobserved but related terms. We then incorporate this distilled document representation into the loss function for topic model estimation.    To connect this method to the more standard supervised knowledge distillation, observe that the unsupervised ``task'' for both an autoencoder and a topic model is the reconstruction of the original document, i.e. prediction of a distribution over the vocabulary. The BERT autoencoder, as ``teacher'', provides a dense prediction that is richly informed by training on a large corpus. The topic model, as ``student'', is generating its own prediction of that distribution. We use the former to guide the latter, essentially as if predicting word distributions were a multi-class labeling problem. [1]{}  {BAT }   \left[ #2 \right] } {B} \DeclareMathOperator*{\argmin}{arg\,min}  \TeX}  \title{Improving Neural Topic Models using Knowledge Distillation}  \author{Alexander Hoyle\thanks{\, Equal contribution.} \\   Computer Science \\   University of Maryland \\   College Park, MD \\    \\\And   Pranav Goel\footnotemark[1] \\   Computer Science \\   University of Maryland \\   College Park, MD \\    \\\And   Philip Resnik \\   Linguistics / UMIACS \\   University of Maryland \\   College Park, MD \\    \\}  \date{}                              \clearpage \appendix    To our knowledge, we are the first to distill a ``black-box'' neural network teacher to guide a probabilistic graphical model. We do this in order to combine the expressivity of probabilistic topic models with the precision of pretrained transformers. Our modular method sits atop any neural topic model  to improve topic quality, which we demonstrate using two NTMs of highly disparate architectures , obtaining state-of-the-art topic coherence across three datasets from different domains. Our adaptable framework does not just produce improvements in the aggregate : its effect can be interpreted more specifically as identifying the same space of topics generated by an existing model and, in most cases, improving the coherence of individual topics, thus highlighting the modular value of our approach.  In future work, we also hope to explore the effects of the pretraining corpus  and teachers  on the generated topics. Another intriguing direction is exploring the connection between our methods and neural network interpretability. The use of knowledge distillation to facilitate interpretability has also been previously explored, for example, in  to learn interpretable decision trees from neural networks. In our work, as the weight on the BERT autoencoder logits  goes to one, the topic model begins to describe less the  and more the . We believe mining this connection can open up further research avenues; for instance, by investigating the differences in such teacher-topics conditioned on the pre-training corpus. Finally, although we are motivated primarily by the widespread use of topic models for identifying interpretable topics ,  we plan to explore the ideas presented here further in the context of downstream applications like document classification.  \looseness=-1
","     Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence. We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.",134
"      Interactive systems capable of understanding natural language and responding in the form of natural language text have high potentials in various applications.  In pursuit of building and evaluating such systems, we study learning agents for Interactive Fiction  games. IF games are world-simulating software in which players use text commands to control the protagonist and influence the world, as illustrated in Figure. IF gameplay agents need to simultaneously understand the game's information from a text display  and generate natural language command  via a text input interface.  Without providing an explicit game strategy, the agents need to identify behaviors that maximize objective-encoded cumulative rewards.    IF games composed of human-written texts  create superb new opportunities for studying and evaluating natural language understanding  techniques due to their unique characteristics.   Game designers elaborately craft on the literariness of the narrative texts to attract players when creating IF games. The resulted texts in IF games are more linguistically diverse and sophisticated than the template-generated ones in synthetic text games.  The language contexts of IF games are more versatile because various designers contribute to enormous domains and genres, such as adventure, fantasy, horror, and sci-fi.  The text commands to control characters are less restricted, having sizes over six orders of magnitude larger than previous text games.  The recently introduced Jericho benchmark provides a collection of such IF games.   The complexity of IF games demands more sophisticated NLU techniques than those used in synthetic text games. Moreover, the task of designing IF game-play agents, intersecting NLU and reinforcement learning , poses several unique challenges on the NLU techniques. The first challenge is the difficulty of exploration in }. To make RL agents learn efficiently %via trial-and-error  without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others.  To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently; or embed each valid action as another vector and predict action value based on the vector-space similarities. These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient.   The second challenge is }.  At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world.  But the latest observation is often not a sufficient summary of the interaction history and may not provide enough information to determine the long-term effects of actions.  Previous approaches address this problem by building a representation over past observations . These methods treat the historical observations equally and summarize the information into a single vector without focusing on important contexts related to the action prediction for the current observation. Therefore, their usages of history also bring noise, and the improvement is not always significant.  We propose a novel formulation of IF game playing as Multi-Passage Reading Comprehension  and harness MPRC techniques to solve the  and  challenges. The graphical illustration is shown in Figure.  First, the action value prediction  is essentially . We base on the fact that each action is an instantiation of a template, i.e., a verb phrase with a few placeholders of object arguments it takes~. Then the action generation process can be viewed as extracting objects for a template's placeholders from the textual observation, based on the interaction between the template verb phrase and the relevant context of the objects in the observation. Our approach addresses the structured prediction and interaction problems with the idea of context-question attention mechanism in RC models.  Specifically, we treat the observation as a passage and each template verb phrase as a question.  The filling of object placeholders in the template thus becomes an extractive QA problem that selects objects from the observation given the template. Simultaneously each action  gets its evaluation value predicted by the RC model. Our formulation and approach better capture the fine-grained interactions between observation texts and structural actions, in contrast to previous approaches that represent the observation as a single vector and ignore the fine-grained dependency among action elements.  Second, alleviating partial observability is essentially  and . Our approach retrieves potentially relevant historical observations with an object-centric approach  , so that the retrieved ones are more likely to be connected to the current observation as they describe at least one shared interactable object. Our attention mechanisms are then applied across the retrieved multiple observation texts to focus on informative contexts for action value prediction.   We evaluated our approach on the suite of Jericho IF games, compared to all previous approaches. Our approaches achieved or outperformed the state-of-the-art performance on 25 out of 33 games, trained with less than one-tenth of game interaction data used by prior art.  We also provided ablation studies on our models and retrieval strategies.     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   We formulate the general IF game playing as MPRC tasks, enabling an MPRC-style solution to efficiently address the key IF game challenges on the huge combinatorial action space and the partial observability in a unified framework.  Our approaches achieved significant improvement over the previous state-of-the-art on both game scores and training data efficiency. Our formulation also bridges broader NLU/RC techniques to address other critical challenges in IF games for future work, e.g., common-sense reasoning, novelty-driven exploration, and multi-hop inference.    
"," Interactive Fiction  games with real human-written natural language texts provide a new natural evaluation for language understanding techniques.  In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension  tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations.  Extensive experiments on the recent IF benchmark  demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches.\footnote{Source code is available at: \url{https://github.com/XiaoxiaoGuo/rcdqn}. }",135
"   Recent advances in self-supervised pre-training have resulted in impressive downstream performance on several NLP tasks. However, this has led to the development of enormous models, which often require days of training on non-commodity hardware . Furthermore, studies have shown that it is quite challenging to successfully train these large Transformer models, requiring complicated learning schemes and extensive hyperparameter tuning.  Despite these expensive training regimes, recent studies have found that once trained, these bi-directional language models exhibit simple patterns of self-attention without much linguistic backing. For example, 40\% of heads in a pre-trained BERT model simply pay attention to delimiters added by the tokenizer . Since these attention patterns are independent of linguistic phenomena, a natural question arises: can Transformer models be guided towards such attention patterns without requiring extensive training?    In this paper, we propose an attention guidance  mechanism for self-attention modules in Transformer architectures to enable faster, more efficient, and robust self-supervised learning. Our approach is simple and agnostic to the training objective. Specifically, we introduce an auxiliary loss function to guide the self-attention heads in each layer towards a set of pre-determined patterns . These patterns encourage the formation of both  global  and local  structures in the model.   Through several experiments, we show that our approach enables training large Transformer models considerably faster 閳 for example, we can train a 16-layer RoBERTa model with SOTA performance on a low-resource domain in just two days using four GPUs, while excluding our loss leads to slow or no convergence. Our method also achieves competitive performance with BERT on three English natural language understanding tasks, and outperforms the baseline masked language modeling  models on eleven out of twelve settings considered.  Further, we also show that our initialization is agnostic to the training objective by demonstrating gains on the replaced token detection objective proposed by ELECTRA and on machine translation with Transformers. Finally, we provide an analysis of the attention heads learned using our method. Surprisingly, contrary to recent studies, we find that it is possible to train models that perform well on language modeling without learning a single attention head that models coreferences. % . For example, our model fails the co-reference test in  while still performing well on language modeling and downstream tasks.  To summarize, our main contributions are:  	   In this study, we introduce the simple yet effective Attention Guidance  loss, which speeds up convergence and improves performance on various domains and model sizes. Adding this loss also makes Transformers robust to hyperparameters like learning rate, warmup steps, and dropout. Our experiments also show its usefulness in multiple pre-training objectives. The gains are particularly strong on larger models, enabling their usage in low-compute scenarios and low-resource domains. Our analysis of the relation of AG loss and MLM loss shows the usefulness of our method, and we hope that this paper can serve as a starting point for future works aiming to exploit and question self-attention in Transformers.  
"," % Despite being successful in downstream language understanding tasks, modern language models~ contain millions of parameters and require multiple days of training on specialized hardware such as TPUs. Training such models on commodity hardware  often means slow convergence, making it practically intractable for many researchers.  In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We propose a computationally efficient auxiliary loss function to guide attention heads to conform to such patterns. Our method is agnostic to the actual pre-training objective and results in faster convergence of models as well as better performance on downstream tasks compared to the baselines, achieving state of the art results in low-resource settings. Surprisingly, we also find that linguistic properties of attention heads are not necessarily correlated with language modeling performance.\footnote{Code: {https://github.com/ameet-1997/AttentionGuidance}}",136
" %  % Transformer models  have outperformed previously used RNN based models and traditional statistical MT techniques.  This improvement, though, comes at the cost of higher computation complexity. The decoder computation is sequential and becomes the bottleneck due to the autoregressive nature, large depth and self-attention structure.   % Another recent trend has been making the models larger and ensembling multiple models to achieve the best possible translation quality . Leading solutions on common benchmark  usually use an ensemble of Transformer big models, which combined can have more than 1 billion parameters.   % In this paper, we focus on developing architectures which are faster during inference and have less number of parameters, without sacrificing translation quality.  % Recent work  proposed methods to replace self-attention in the decoder with simpler simple recurrent units  and used knowledge distillation to simplify training for the final architecture.  also proposed to make the decoder lightweight by training a deep-encoder, shallow decoder architecture. Another line of effort to make NMT architectures more efficient is pruning different components of the model.  and  show that most of the attention heads in the network learn redundant information and can be pruned away.  % All of the above works use the vanilla Transformer architecture as their baseline, so it is not clear if these approaches can give complimentary results when combined together. In this work, we explore and benchmark combining all of the above techniques, with the goal of maximizing inference speed without hurting in translation quality. % %We adapt the same approach and  extend it with the following ideas. First, we optimized the SSRU to make it more efficient. Second, we removed the feed-forward network in the decoder completely. Then, we kept only 1 layer in the decoder and used very deep encoder. Last we pruned all the redundant heads in the deep encoder.  % After carefully stacking the approaches, our proposed architecture is able to achieve a significant speed improvement of 84\% on GPU and 102\% on CPU architectures without any degradation of translation quality in terms of BLEU.  % %%%%%%%% original Related Work %%%%%%%%% %     In this paper we explored the combination of techniques aimed at improving inference speed which lead to the discovery of a very efficient architecture. The best architecture has a deep -layer encoder, and a shallow decoder with only one single lightweight recurrent unit layer and one encoder-decoder attention mechanism. \  of the encoder heads were pruned giving rise to a model with \  fewer parameters than the baseline Transformer. In terms of inference speed, the proposed architecture is \  faster on a GPU, and \  faster on a CPU.   In the future, we plan to investigate pruning the feed-forward network in the encoder, and explore application of the lottery ticket hypothesis.        In this paper,     we have investigated various approaches of simplifying Transformer model to speed up the inference and successfully combine multiple techniques. To be more specific,    we achieve the very efficient inference architecture, which consists of only one lightweight recurrent unit layer and one encoder-decoder attention mechanism in the decoder. With the head pruning method, only 18\  of attention heads are required in the deep encoder and shallow decoder architecture. This model has 13\  fewer parameters, and during the inference stage, it is 84\  and 102\  faster than baseline on GPU and CPU, respectively.    In the future, we plan to prune the feed-forward network in the encoder and explore the combination with the lottery ticket hypothesis.      In the future, we plan to investigate more different approaches and build a mroe efficient inference architecture for machine translation.     we plan to prune the feed-forward neurons and apply the unstructured pruning techniques to remove weights in the whole model.  
"," Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to $109$\% and $84$\% speedup on CPU and GPU respectively and reduce the number of parameters by $25$\% while maintaining the same translation quality in terms of BLEU. %State-of-the-art neural machine translation has become compute and parameter intensive in the last several years, which puts significant pressure on the latency and hardware resources during inference. In this paper, we change the standard Transformer architecture to reduce the number of parameters and increase inference speed without sacrificing translation quality. We demonstrate that combination of replacing decoder self-attention with the simpler simple recurrent units, adopting a deep encoder and shallow decoder architecture, and multi-head attention pruning, we can achieve up to 102\% speedup and reduce the number of parameters by 13\% while maintaining the same translation quality in terms of BLEU.",137
" Intent Detection  is a crucial task in natural language understanding, whose objective is to extract underlying intents behind the given utterances. The extracted intents could provide further contexts for further downstream Natural Language Processing tasks such as dialogue state tracking or question answering. Unlike traditional text classification, ID is challenging for two main reasons  Utterances are usually short and diversely expressed,  Emerging intents occur continuously, especially across different domains .  Despite recent advances, state-of-the-art ID methods  require a large amount of annotated data to achieve competitive performance. This requirement inhibits models' capability in generalizing to newly emerging intents with no or limited annotations during inference. Re-training or fine-tuning large models on few samples of emerging classes could easily lead to overfitting problems.      Motivated by human capability in correctly categorizing new classes with only a few examples , few-shot learning  paradigms are adopted to tackle the scarcity problems of emerging classes. FSL methods take advantage of a small set of labeled examples  to learn how to discriminate unlabeled samples  between classes, even those not seen during training.  Recent works in FSL  focus on learning the matching information between the labeled samples  and the unlabeled samples  to provide additional contextual information for instance-level representations, leading to effective prototype representation. However, these methods only extract similarity based on fine-grained word semantics, failing to capture the diverse expressions of users' utterances. This problem could further lead to overfitting either to seen intents or novel intents, especially in the challenging Generalized Few-shot Intent Detection  setting  where both seen and novel intents are existent in a joint label space during inference. Instead, matching support and query samples on coarser-grained semantic components could provide additional informative contexts beyond word levels. For instance, two utterances ""i need to get a table at a pub with southeastern cuisine"" and ``book a spot for six friends"" share a similar intent label ``Book Restaurant"". While word-level semantics might find similar action words as ``get"" and ``book"", these words do not necessarily contribute to the correct intent findings. Instead, coarser-grained semantics such as ``get a table"" and ``book a spot"" could provide further hints to identify ``Book Restaurant"" intent.      As semantic components  could be effectively extracted from multi-head self-attention, matching these SC between support and query can enhance both query and support representations, leading to improvements in generalization from seen training classes to unseen testing classes. To further enhance the dynamics of extracted SC across various domains and diversely expressed utterances, we introduce additional head regularizations. In addition, to overcome the insufficiency of a single similarity measure for matching sentences with diverse semantics, a more comprehensive matching method is further explored.      Our main contribution is summarized as follows:          In this paper, we propose an effective Semantic Matching and Aggregation Network for few-shot intent detection. Semantic components extracted from multi-head self-attention capture higher level contextual information beyond the word level, enhancing model's generalizability towards both seen and novel intents, especially when utterances are diversely expressed. Comprehensive multi-perspective matching method thoroughly exploits the similarity between query and support samples for further robust representations. In this work, we also propose a more challenging but realistic non-episodic evaluation for both FSL and GFSL beyond traditional setting. Our model achieves the state-of-the-art performance in both evaluation settings for SNIPS and NLUE benchmark datasets. Further studies of more dynamic semantic extraction and effectively synthesized matching techniques are our desired future work.       
"," Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity measure and overly fine-grained matching components. These limitations inhibit generalizing capability towards Generalized Few-shot Learning settings where both seen and novel classes are co-existent. In this paper, we propose a novel Semantic Matching and Aggregation Network where semantic components are distilled from utterances via multi-head self-attention with additional dynamic regularization constraints. These semantic components capture high-level information, resulting in more effective matching between instances. Our multi-perspective matching method provides a comprehensive matching measure to enhance representations of both labeled and unlabeled instances. We also propose a more challenging evaluation setting that considers classification on the joint all-class label space. Extensive experimental results demonstrate the effectiveness of our method. Our code and data are publicly available \footnote{\url{https://github.com/nhhoang96/Semantic\_Matching}} .",138
" Multilingual Neural Machine Translation , which leverages a single NMT model to handle the translation of multiple languages, has drawn research attention in recent years. MNMT is appealing since it greatly reduces the cost of training and serving separate models for different language pairs. It has shown great potential in knowledge transfer among languages, improving the translation quality for low-resource and zero-shot language pairs.  Previous works on MNMT has mostly focused on model architecture design with different strategies of parameter sharing or representation sharing. Existing MNMT systems mainly rely on bitext training data, which is limited and costly to collect. Therefore, effective utilization of monolingual data for different languages is an important research question yet is less studied for MNMT.  Utilizing monolingual data  has been widely explored in various NMT and natural language processing  applications. Back translation , which leverages a target-to-source model to translate the target-side monolingual data into source language and generate pseudo bitext, has been one of the most effective approaches in NMT. However, well trained NMT models are required to generate back translations for each language pair, it is computationally expensive to scale in the multilingual setup. Moreover, it is less applicable to low-resource language pairs without adequate bitext data. Self-supervised pre-training approaches, which train the model with denoising learning objectives on the large-scale monolingual data, have achieved remarkable performances in many NLP applications. However, catastrophic forgetting effect, where finetuning on a task leads to degradation on the main task, limits the success of continuing training NMT on models pre-trained with monolingual data. Furthermore, the separated pre-training and finetuning stages make the framework less flexible to introducing additional monolingual data or new languages into the MNMT system.  In this paper, we propose a multi-task learning  framework to effectively utilize monolingual data for MNMT. Specifically, the model is jointly trained with translation task on multilingual parallel data and two auxiliary tasks: masked language modeling  and denoising auto-encoding  on the source-side and target-side monolingual data respectively. We further present two simple yet effective scheduling strategies for the multilingual and multi-task framework. In particular, we introduce a dynamic temperature-based sampling strategy for the multilingual data. To encourage the model to keep learning from the large-scale monolingual data, we adopt dynamic noising ratio for the denoising objectives to gradually increase the difficulty level of the tasks.   We evaluate the proposed approach on a large-scale multilingual setup with  language pairs from the WMT datasets. We study three English-centric multilingual systems, including many-to-English, English-to-many, and many-to-many. We show that the proposed MTL approach significantly boosts the translation quality for both high-resource and low-resource languages. Furthermore, we demonstrate that MTL can effectively improve the translation quality on zero-shot language pairs with no bitext training data. In particular, MTL achieves even better performance than the pivoting approach for multiple low-resource language pairs. We further show that MTL outperforms pre-training approaches on both NMT tasks as well as cross-lingual transfer learning for NLU tasks, despite being trained on very small amount of data in comparison to pre-training approaches.  The contributions of this paper are as follows. First, we propose a new MTL approach to effectively utilize monolingual data for MNMT. Second, we introduce two simple yet effective scheduling strategies, namely the dynamic temperature-based sampling and dynamic noising ratio strategy. Third, we present detailed ablation studies to analyze various aspects of the proposed approach. Finally, we demonstrate for the first time that MNMT with MTL models can be effectively used for cross-lingual transfer learning for NLU tasks with similar or better performance than the state-of-the-art massive scale pre-trained models using single task.     In this work, we propose a multi-task learning framework that jointly trains the model with the translation task on bitext data, the masked language modeling task on the source-side monolingual data and the denoising auto-encoding task on the target-side monolingual data. We explore data and noising scheduling approaches and demonstrate their efficacy for the proposed approach. We show that the proposed MTL approach can effectively improve the performance of MNMT on both high-resource and low-resource languages with large margin, and can also significantly improve the translation quality for zero-shot language pairs without bitext training data. We showed that the proposed approach is more effective than pre-training followed by finetuning for NMT. Furthermore, we showed the effectiveness of multitask learning for cross-lingual downstream tasks outperforming  SOTA larger models trained on single task.  For future work, we are interested in investigating the proposed approach in a scaled setting with more languages and a larger amount of monolingual data. Scheduling the different tasks and different types of data would be an interesting problem. Furthermore, we would also like to explore the most sample efficient strategy to add a new language to a trained MNMT system.  
"," While monolingual data has been shown to be useful in improving bilingual neural machine translation , effectively and efficiently leveraging monolingual data for Multilingual NMT  systems is a less explored area. In this work, we propose a multi-task learning  framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with $10$ language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",139
"  Neural machine translation  is a data-hungry approach, which requires a large amount of data to train a well-performing NMT model. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult.  To relieve this problem, several approaches have been proposed to better exploit the training data, such as curriculum learning, data diversification, and data denoising.  In this paper, we explore an interesting alternative which is to reactivate the { to rejuvenate the inactive examples to improve the performance of NMT models.  Specifically, we train an NMT model on the active examples as the rejuvenation model to re-label the inactive examples, resulting in the rejuvenated examples~. The final NMT model is trained on the combination of the active examples and rejuvenated examples. Experimental results show that the data rejuvenation approach consistently and significantly improves performance on SOTA NMT models  on the benchmark WMT14 English-German and English-French datasets~. Encouragingly, our approach is also complementary to existing data manipulation methods , and combining them can further improve performance.   [t]     , then rejuvenated by the {   Finally, we conduct extensive analyses to better understand the inactive examples and the proposed data rejuvenation approach. Quantitative analyses reveal that the inactive examples are more difficult to learn than active ones, and rejuvenation can reduce the learning difficulty~. The rejuvenated examples stabilize and accelerate the training process of NMT models~, resulting in final models with better generalization capability~.  Our contributions of this work are as follows:          In this study, we propose data rejuvenation to exploit the inactive training examples for neural machine translation on large-scale datasets. The proposed data rejuvenation scheme is a general framework where one can freely define, for instance, the identification and rejuvenation models. Experimental results on different model architectures and language pairs demonstrate the effectiveness and universality of the data rejuvenation approach.  Future directions include exploring advanced identification and rejuvenation models that can better reflect the learning abilities of NMT models, as well as validating on other NLP tasks such as dialogue and summarization.    
"," Large-scale training datasets lie at the core of the recent success of neural machine translation  models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce  to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases.  First, we train an { on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed  consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.}  %In this work, we propose to improve the training of NMT models on large-scale datasets by exploiting inactive training examples, which contribute less to the model performance. Specifically, the proposed framework consists of three phases. First, we identify the inactive examples with their sentence-level prediction confidence assigned by an { on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed  consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.",140
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.      We introduce GraphGlove~--- graph word embeddings, where each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. The graph is learned end-to-end in an unsupervised manner. We show that GraphGlove substantially outperforms both Euclidean and Poincar\'e GloVe on word similarity and word analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet; the geometry is highly non-trivial and contains subgraphs with different local topology.   Possible directions for future work include using GraphGlove for unsupervised hypernymy detection, analyzing undesirable word associations, comparing learned graph topologies for different languages, and downstream applications such as sequence classification. Also, given the recent success of models such as ELMo and BERT, it would be interesting to explore extensions of GraphGlove to the class of contextualized embeddings.  
"," This document contains the instructions for preparing a manuscript for the proceedings of EMNLP 2020. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.",141
" Modern neural machine translation~ models employ sufficient capacity to fit the massive data well by utilizing a large number of parameters, and suffer from the widely recognized issue, namely, over-parameterization. For example,  showed that over 40\% of the parameters in an RNN-based NMT model can be pruned with negligible performance loss. However, the low utilization efficiency of parameters results in a waste of computational resources , as well as renders the model stuck in a local optimum.   In response to the over-parameterization issue, network pruning has been widely investigated for both computer vision   and natural language processing  tasks . Recent work has proven that such spare parameters can be reused to maximize the utilization of models in CV tasks such as image classification. The leverage of parameter rejuvenation in sequence-to-sequence learning, however, has received relatively little attention from the research community. In this paper, we empirically study the efficiency issue for NMT models.  Specifically, we first investigate the effects of weight pruning on advanced Transformer models, showing that 20\% parameters can be directly pruned, and by continuously training the sparse networks, we can prune 50\% with no performance loss. Starting from this observation, we then exploit whether these redundant parameters are able to be re-utilized for improving the performance of NMT models. Experiments are systematically conducted on different datasets  and NMT architectures . Results demonstrate that the rejuvenation approach can significantly and consistently improve the translation quality by up to +0.8 BLEU points. Further analyses reveal that the rejuvenated parameters are reallocated to enhance the ability to model the source-side low-level information, lacking of which leads to a number of problems in NMT models.   Our key contributions are:  \setlength     In this paper, we prove that existing NMT systems are over-parameterized and propose to improve the utilization efficiency of parameters in NMT models by introducing a rejuvenation approach. Empirical results on a variety of language pairs and architectures demonstrate the effectiveness and universality of the presented method. We also analyze the gains from perspectives of learning dynamics and linguistic probing, which give insightful research directions for future work.   Future directions include continuing the exploration of this research topic for large sequence-to-sequence pre-training models  and multi-domain translation models . We will employ recent analysis methods to better understand the behaviors of rejuvenated models.     \clearpage  
"," Modern neural machine translation  models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance. Experiments and analyses are systematically conducted on different datasets and NMT architectures. We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.",142
"  Sentiment analysis  has attracted increasing attention recently. Aspect-based sentiment analysis   is a fine-grained sentiment analysis task and includes many subtasks, two of which are aspect category detection  that detects the aspect categories mentioned in a sentence and aspect-category sentiment analysis  that predicts the sentiment polarities with respect to the detected aspect categories. Figure shows an example. ACD detects the two aspect categories,  and , and ACSA predicts the negative and positive sentiment toward them respectively. In this work, we focus on ACSA, while ACD as an auxiliary task is used to find the words indicating the aspect categories in sentences for ACSA.    Since a sentence usually contains one or more aspect categories, previous studies have developed various methods for generating aspect category-specific sentence representations to detect the sentiment toward a particular aspect category in a sentence. To name a few, attention-based models  allocate the appropriate sentiment words for the given aspect category.  proposed to generate aspect category-specific representations based on convolutional neural networks and gating mechanisms. Since aspect-related information may already be discarded and aspect-irrelevant information may be retained in an aspect independent encoder, some existing methods  utilized the given aspect to guide the sentence encoding from scratch. Recently, BERT based models  have obtained promising performance on the ACSA task. However, these models ignored that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category. It leads to suboptimal performance of these models. For the example in Figure, both ``drinks'' and ``food'' indicate the aspect category . The sentiment about  is a combination of the sentiments of ``drinks'' and ``food''. Note that, words indicating aspect categories not only contain aspect terms explicitly indicating an aspect category but also contain other words implicitly indicating an aspect category . In Figure, while ``drinks'' and ``food'' are aspect terms explicitly indicating the aspect category , ``large'' and ``noisy'' are not aspect terms implicitly indicating the aspect category .  In this paper, we propose a Multi-Instance Multi-label Learning Network for Aspect-Category sentiment analysis . AC-MIMLLN explicitly models the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category. Specifically, AC-MIMLLN treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances  of the aspect category. Given a bag and the aspect categories mentioned in the bag, AC-MIMLLN first predicts the instance sentiments, then finds the key instances for the aspect categories, finally aggregates the sentiments of the key instances to get the bag-level sentiments of the aspect categories.  Our main contributions can be summarized as follows:  	    In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis . AC-MIMLLN predicts the sentiment of an aspect category mentioned in a sentence by aggregating the sentiments of the words indicating the aspect category in the sentence. Experimental results demonstrate the effectiveness of AC-MIMLLN. Since AC-MIMLLN finds the key instances for the given aspect category and predicts the sentiments of the key instances, it is more interpretable. In some sentences, phrases or clauses rather than words indicate the given aspect category, future work could consider multi-grained instances, including  words, phrases and clauses. Since directly finding the key instances for some aspect categories is ineffective, we will try to first recognize all opinion snippets in a sentence, then assign these snippets to the aspect categories mentioned in the sentence.   
"," 	Aspect-category sentiment analysis  aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis , which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN \footnote{Data and code are available at https://github.com/l294265421/AC-MIMLLN}.",143
" The recent success of the  approaches~, which train language models on diverse text corpora with self-supervised or multi-task learning, have brought up huge performance improvements on several natural language understanding  tasks~. The key to this success is their ability to learn generalizable text embeddings that achieve near optimal performance on diverse tasks with only a few additional steps of fine-tuning on each downstream task.    Most of the existing works on language model aim to obtain a universal language model that can address nearly the entire set of available natural language tasks on heterogeneous domains. Although this train-once and use-anywhere approach has been shown to be helpful for various natural language tasks~, there have been considerable needs on adapting the learned language models to domain-specific corpora . Such domains may contain new entities that are not included in the common text corpora, and may contain only a small amount of labeled data as obtaining annotation on them may require expert knowledge.  Some recent works~ suggest to further pre-train the language model with self-supervised tasks on the domain-specific text corpus for adaptation, and show that it yields improved performance on tasks from the target domain.  Masked Language Models  objective in BERT~ has shown to be effective for the language model to learn the knowledge of the language in a bi-directional manner~. In general, masks in MLMs are sampled at random~, which seems reasonable for learning a generic language model pre-trained from scratch, since it needs to learn about as many words in the vocabulary as possible in diverse contexts.  However, in the case of further pre-training of the already pre-trained language model, such a conventional selection method may lead a domain adaptation in an inefficient way, since not all words will be equally important for the target task. Repeatedly learning for uninformative instances thus will be wasteful. Instead, as done with instance selection, it will be more effective if the masks focus on the most important words for the target domain, and for the specific NLU task at hands. How can we then  such a masking strategy to train the MLMs?   Several works~ propose rule-based masking strategies which work better than random masking ~ when applied to language model pre-training from scratch. Based on those works, we assume that adaptation of the pre-trained language model can be improved via a  masking policy which selects the words to mask. Yet, existing models are inevitably suboptimal since they do not consider the target domain and the task. To overcome this limitation, in this work, we propose to adaptively generate mask by learning the optimal masking policy for the given task, for the task-adaptive pre-training~ of the language model.  As described in Figure , we want to further pre-train the language model on a specific task with a task-dependent masking policy, such that it directs the solution to the set of parameters that can better adapt to the target domain, while task-agnostic random policy leads the model to an arbitrary solution.  To tackle this problem, we pose the given learning problem as a meta-learning problem where we learn the task-adaptive mask-generating policy, such that the model learned with the masking strategy obtains high accuracy on the target task.  We refer to this meta-learner as the Neural Mask Generator . Specifically, we formulate mask learning as a bi-level problem where we pre-train and fine-tune a target language model in the inner loop, and learn the NMG at the outer loop, and solve it using renforcement learning. We validate our method on diverse NLU tasks, including question answering and text classification. The results show that the models trained using our NMG outperforms the models pre-trained using rule-based masking strategies, as well as finds a proper adaptive masking strategy for each domain and task.  Our contribution is threefold:  	    We proposed a novel framework which automatically generates an adaptive masking for masked language models based on the given context, for language model adaptation to low-resource domains. To this end, we proposed the , which is trained with reinforcement learning to mask out words that are helpful for domain adaptation. We performed an empirical study of various rule-based masking strategies on multiple datasets for question answering and text classification tasks, which shows that the optimal masking strategy depends on both the language model and the domain. We then validated NMG against rule-based masking strategies, and the results show that it either outperforms, or obtains comparable performance to the best heuristic. Further qualitative analysis suggests that such good performance comes from its ability to adaptively mask meaningful words for the given task.  
"," We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task . Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our  on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings. \footnote{Code is available at \url{github.com/Nardien/NMG}.}",144
"   Sentiment analysis has become an increasingly popular natural language  processing  task in academia and industry.  It provides real-time  feedback on consumer experience and their needs, which helps  producers to offer better services.  To deal with the presence of  multiple categories in one document,  ACSA tasks, including aspect-category  sentiment analysis  and targeted aspect-category sentiment analysis , were introduced.   The main purpose for ACSA task  is to identify sentiment polarity  of an input sentence upon specific predefined categories . For  example, as shown in Table , giving an input sentence ``Food is  always fresh and hot-ready to eat, but it is too expensive."" and predefined categories \{food, service, price,  ambience and anecdotes/miscellaneous\},  the sentiment of category food is positive, the polarity  regarding to category price is negative, while is none for others.  In this task, the models should  capture both explicit expressions and implicit expressions. For example, the phrase ``too expensive"" indicates the  negative polarity  in the price category, without a direct indication of ``price"".    In order to  deal with ACSA with both multiple categories and multiple targets, TACSA task was introduced  to analyze sentiment polarity on a set of predefined target-category pairs. An example is shown in Table , given targets ``restaurant-1"" and ``restaurant-2"", in the case ``I like  restaurant-1 because it's cheap, but restaurant-2 is too  expansive"", the category price for target ``restaurant-1"" is positive, but is  negative for target ``restaurant-2"", while is none for other target-category pairs. A mathematical definition for ACSA is given  as follows: giving a  sentence  as input, a predefined set of targets  and a predefined set of  aspect categories , a model predicts the sentiment polarity  for  each target-category pair $\{ : t [!t] 	 	{ 		{|c|c|c|}%{l*{3}{c}} 			%\toprule[1pt] 			 	   Multi-task learning, with shared encoders but individual decoders for each category, is an approach to analyze all the categories in one sample simultaneously for ACSA . Compared with single-task ways , multi-task approaches utilize category-specific knowledge in training signals from each task and get better performance. However, current multi-task models still suffer from a lack of  features such as category name . Models with category name features encoded in the model may further improve the performance.  On the other hand, the predefined categories in ACSA task make the application  in new categories inflexible, as for ACSA applications, the number of categories maybe  varied over time.  For example, fuel consumption, price level, engine power, space and so  on are source categories to be analyzed in the gasoline automotive domain. For  electromotive domain, source categories in the automotive domain will still be used, while new target category such as battery duration should also be analyzed.  Incremental learning is a way to solve this problem. Therefore, it is necessary to propose an  incremental learning task and an incremental learning model concerned with new  category for ACSA tasks.  Unfortunately, in the current multi-task learning ACSA models, the encoder is shared but the decoders for each category are individual. This parameter sharing mechanism results in only the shared encoder  and target-category-related decoders are finetuned during the finetuning process, while the decoder of source categories remains unchanged. The finetuned encoder and original decoder of source categories may cause catastrophic forgetting problem in the origin  categories. For real applications, high accuracy is excepted in source  categories and target  categories.  Based on the previous researches that decoders between different tasks are usually modeled by mean regularization   , an idea comes up to further make the decoders the same by sharing the decoders in all categories to decrease the catastrophic forgetting problem. But here raises another question, how to identify each category in the encoder and decoder shared network? In our approach, we  solve the category discrimination problem by the input category name feature.   In this paper,  we proposed a multi-task category name embedding network  .  The multi-task learning  framework makes full use of training signals from all categories. To make it feasible for incremental learning, both encoder and decoders for each category are shared. The category names were applied as another input feature for task discrimination. We also present a new task for ACSA incremental learning. In particular,  our contribution is three-folded:    We proposed a multi-task CNE-net framework with both encoder and decoder shared to weaken catastrophic forgetting problem in multi-task learning ACSA model.     We achieved  state-of-the-art on the two ACSA datasets, SemEval14-Task4  and Sentihood.   We proposed a new task for incremental learning in ACSA. By sharing both encoder layers and decoder layers of all the tasks, we   achieved better results compared with other baselines both in source  categories and in the target category.      In this paper, in order to make multi-task learning feasible for incremental learning,  we proposed CNE-net with different attention mechanisms. The category  name features and the multi-task learning structure help the model  achieve state-of-the-art on ACSA and TACSA tasks. Furthermore,  the shared encoder and decoder layers weaken catastrophic forgetting in the incremental learning task.  We proposed a task for ACSA incremental learning and achieved the best  performance with CNE-net compared with other strong baselines.  Further research may be concerned with zero-shot learning on new categories.   
"," ACSA tasks, including aspect-category sentiment analysis  and  targeted  aspect-category sentiment analysis , aims at identifying sentiment  polarity on predefined categories. Incremental learning on new categories is necessary for ACSA real applications. Though current multi-task learning models achieve good performance in ACSA tasks, they suffer from catastrophic forgetting problems in ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name  Embedding network  . We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination.  Our model achieved state-of-the-art  on two ACSA benchmark datasets. Furthermore, we proposed  a dataset for ACSA incremental learning and achieved the best performance compared with other strong baselines.",145
"   Conditional random fields  have been shown to perform well in various sequence labeling tasks. Recent work uses rich neural network architectures to define the ``unary'' potentials, i.e., terms that only consider a single position's label at a time~. However, ``binary'' potentials, which consider pairs of adjacent labels, are usually quite simple and may consist solely of a parameter or parameter vector for each unique label transition. Models with unary and binary potentials are generally referred to as ``first order'' models.   A major challenge with CRFs is the complexity of training and inference, which are quadratic in the number of output labels for first order models and grow exponentially when higher order dependencies are considered. This explains why the most common type of CRF used in practice is a first order model, also referred to as a ``linear chain'' CRF.   One promising alternative to CRFs is structured prediction energy networks , which use deep neural networks to parameterize arbitrary potential functions for structured prediction. While SPENs also pose challenges for learning and inference,  proposed a way to train SPENs jointly with ``inference networks'', neural networks trained to approximate structured  inference.   In this paper, we leverage the frameworks of SPENs and inference networks to explore high-order energy functions for sequence labeling. Naively instantiating high-order energy terms can lead to a very large number of parameters to learn, so we instead develop concise neural parameterizations for high-order terms. In particular, we draw from vectorized Kronecker products, convolutional networks, recurrent networks, and self-attention.  We also consider ``skip-chain'' connections~ with various skip distances and ways of reducing their total parameter count for increased learnability.   Our experimental results on four sequence labeling tasks show that a range of high-order energy functions can yield performance improvements. While the optimal energy function varies by task, we find strong performance from skip-chain terms with short skip distances, convolutional networks with filters that consider label trigrams, and recurrent networks and self-attention networks that consider large subsequences of labels.     We also demonstrate that modeling high-order dependencies can lead to significant performance improvements in the setting of noisy training and test sets.  Visualizations of the high-order energies show various methods capture intuitive structured dependencies among output labels.   Throughout, we use inference networks that share the same architecture as unstructured classifiers for sequence labeling, so test time inference speeds are unchanged between local models and our method.  Enlarging the inference network architecture by adding one layer leads consistently to better results, rivaling or improving over a BiLSTM-CRF baseline,  suggesting that training efficient inference networks with high-order energy terms can make up for errors arising from approximate inference. While we focus on sequence labeling in this paper, our results show the potential of developing high-order structured models for other NLP tasks in the future.     We explore arbitrary-order models with different neural parameterizations on sequence labeling tasks via energy-based inference networks. This approach achieve substantial improvement using high-order energy terms, especially in noisy data conditions, while having same decoding speed as simple local classifiers.     File emnlp2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith   \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily  \usepackage{graphicx} \usepackage{xcolor}  \usepackage{color} \usepackage{multirow} [1]{\mbox{{\arabic{algorithm}} \DeclareMathOperator*{\LM}{TLM} \DeclareMathOperator*{\BLSTM}{BiLSTM} \DeclareMathOperator*{\g}{G} \DeclareMathOperator*{\d0}{D} \DeclareMathOperator*{\PT}{\#paras_T} \DeclareMathOperator*{\PI}{\#paras_I} \DeclareMathOperator*{\attention}{attention}  \usepackage{amssymb} \usepackage{graphicx} \usepackage{subcaption}  \usepackage{ctable}  {E^{\LM}}  \DeclareMathOperator*{\TLM}{TLM} } \DeclareMathOperator*{\LSTM}{LSTM} {{\rm I{LayerNorm} \DeclareMathOperator*{\MLP}{MLP}   \DeclareMathOperator*{\product}{KP} \DeclareMathOperator*{\product}{VKP}   {} {} {\mathcal{X}} {\mathcal{Y}} {\mathcal{Y}_{R}} } {\mathit{loc}} {\mathit{lab}}  {\mathbb{R}} {}} {\mathbf{v}}   {}       This is not strictly necessary, and may be commented out,   but it will improve the layout of the manuscript,   and will typically save some space. \usepackage{microtype}         Enter the acl Paper ID here      You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.     \title{An Exploration of Arbitrary-Order Sequence Labeling\\ via Energy-Based Inference Networks}  \author{Lifu Tu \Thanks{ Equal contribution.} \ \ \ \ \ \   Tianyu Liu  {\marginpar{NEW}}                     
"," Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks~ for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers.  We also find high-order energies to help in noisy data conditions.\footnote{Code  is available at \url{https://github.com/tyliupku/Arbitrary-Order-Infnet}}",146
" Long document coreference resolution poses runtime and memory challenges. Current best models % for coreference resolution have large memory requirements and quadratic runtime in the document length~, making them impractical for long documents. %  Recent work revisiting the entity-mention paradigm~, which seeks to maintain explicit representations only of entities, rather than all their constituent mentions, has shown practical benefits for memory while being competitive with state-of-the-art models~. In particular, unlike other approaches to coreference resolution which maintain representations of both mentions and their corresponding entity clusters~ , the entity-mention paradigm stores representations only of the entity clusters, which are updated incrementally as coreference predictions are made. While such an approach requires less memory than those that additionally store mention representations, the number of entities can be impractically large when processing long documents, making the storing of all entity representations problematic.  Is it necessary to maintain an unbounded number of mentions or entities?  Psycholinguistic evidence suggests it is not, as human language processing is incremental  and has limited working memory~. In practice, we find that most entities have a small spread , and thus do not need to be kept persistently in memory. This observation suggests that tracking a limited, small number of entities at any time can resolve the computational %  issues, albeit at a potential accuracy tradeoff.  Previous work on bounded memory models for coreference resolution has shown potential, but has been tested only on short documents  % . % Moreover, this previous work makes token-level predictions while standard coreference datasets have span-level annotations.  % We propose a bounded memory model that performs quasi-online coreference resolution,         We propose a memory model which tracks a small, bounded number of entities. The proposed model guarantees a linear runtime in document length, and in practice significantly reduces peak memory usage during training. Empirical results on LitBank and OntoNotes show that the model is competitive with an unbounded memory version and outperforms a strong rule-based baseline. In particular, we report state of the art results on LitBank. In future work we plan to apply our model to longer, book length documents, and plan to add more structure to the memory.   
"," Long document coreference resolution remains a challenging task	 due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. % We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that  the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and  the model learns an efficient memory management strategy easily outperforming a rule-based strategy.",147
"  Advances in pretraining language models   as general-purpose representations have pushed  the state of the art on a variety of natural language tasks. However, not all languages enjoy large public datasets for pretraining and/or downstream tasks. Multilingual language models such as mBERT  and XLM   have been proven effective for cross-lingual transfer learning  by pretraining a single shared Transformer model   jointly on multiple languages. The goals of multilingual modeling are not limited  to improving language modeling in low-resource languages ,  but also include zero-shot cross-lingual transfer on downstream tasks---it  has been shown that multilingual models can generalize to target languages  even when labeled training data is only available in the source language   on a wide range of tasks .   However, multilingual models are not equally beneficial for all languages.  demonstrated that including more languages in a single model  can improve performance for low-resource languages but hurt performance for high-resource languages. Similarly, recent work   in multilingual neural machine translation  also observed  performance degradation on high-resource language pairs. In multi-task learning , this phenomenon is known as  or  ,  where training multiple tasks jointly hinders the performance on individual tasks. % In multilingual language modeling, each language is a single task and negative interference during pretraining can hurt the model's generalization on individual languages.  Despite these empirical observations, little prior work analyzed or showed  how to mitigate negative interference in multilingual language models. Particularly, it is natural to ask:  Can negative interference occur for low-resource languages also?  What factors play an important role in causing it?  Can we mitigate negative interference to improve the model's cross-lingual transferability?   In this paper, we take a step towards addressing these questions. We pretrain a set of monolingual and bilingual models and evaluate them  on a range of downstream tasks to analyze negative interference. We seek to individually characterize the underlying factors of negative interference  through a set of ablation studies and glean insights on its causes. Specifically, we examine if training corpus size and language similarity affect negative interference,  and also measure gradient and parameter similarities between languages.  Our results show that negative interference can occur in both high-resource and low-resource languages. In particular, we observe that neither subsampling the training corpus  nor adding typologically similar languages substantially impacts negative interference. On the other hand, we show that gradient conflicts  and language-specific parameters do exist in multilingual models,  suggesting that languages are fighting for model capacity, which potentially causes negative interference. We further test whether explicitly assigning language-specific modules  to each language can alleviate negative interference, and find that the resulting model performs better  within each individual language but worse on zero-shot cross-lingual tasks.  Motivated by these observations, we further propose  to meta-learn these language-specific parameters  to explicitly improve generalization of shared parameters on all languages. Empirically, our method improves not only within-language performance on monolingual tasks  but also cross-lingual transferability on zero-shot transfer benchmarks. To the best of our knowledge, this is the first work to systematically study  and remedy negative interference in multilingual language models.  % Advances in pretraining language models   % as general-purpose representations have pushed the state-of-the-art on a variety of natural language tasks. % However, not all languages have large amounts of training data for pretraining and/or downstream tasks. % Multilingual language models such as mBERT  and XLM  have been proven effective for cross-lingual transfer learning by pretraining a single shared Transformer model  jointly on multiple languages. % The goal is to not only improve language modeling in low-resource languages , but also enable zero-shot cross-lingual transfer on downstream tasks -- it has been shown that multilingual models can generalize to target languages when labeled training data is only available in the source language  on a wide range of tasks .   % However, multilingual models are not equally beneficial for all languages. %  demonstrated that including more languages in a single model can improve performance for low-resource languages but hurt performance for high-resource languages. % Similarly, recent work  in multilingual neural machine translation  also observed performance degradation on high-resource language pairs. % In multi-task learning , this phenomenon is known as  or  ,  % where training multiple tasks jointly hinders the performance on individual tasks. % % In multilingual language modeling, each language is a single task and negative interference during pretraining can hurt the model's generalization on individual languages.  % Despite these empirical observations, little prior work analyzed or showed how to mitigate negative interference in multilingual language models. % Particularly, it is natural to ask: %  Can negative interference occur for low-resource languages also? %  What factors play an important role in causing it? %  Can we mitigate negative interference to improve the model's cross-lingual transferability?   % In this paper, we take a step towards addressing these questions. % We pretrain a set of monolingual and bilingual models, and evaluate them on a range of downstream tasks to analyze negative interference. % We seek to individually characterize the underlying factors of negative interference through a set of ablation studies and glean insights on its causes. % Specifically, we examine if training corpus size and language similarity affect negative interference, and also measure gradient and parameter similarities between languages.  % Our results show that negative interference can occur in both high-resource and low-resource languages. % In particular, we observe that subsampling the training corpus or adding typologically similar languages has little impact on negative interference. % On the other hand, we show that gradient conflicts and language-specific parameters do exist in multilingual models, suggesting that languages are fighting for model capacity which potentially causes negative interference. % Thus, we further test whether explicitly assigning language-specific modules to each language can alleviate negative interference.  % To our surprise, the model performs better within each individual language but worse on zero-shot cross-lingual tasks.  % Motivated by these observations, we further propose to meta-learn these language-specific parameters to explicitly improve generalization of shared parameters on all languages. % Empirically, our method improves not only within-language performance on monolingual tasks but also cross-lingual transferability on zero-shot transfer benchmarks. % To the best of our knowledge, this is the first work to systematically study and treat negative interference in multilingual language models.      We present the first systematic study of negative interference  in multilingual models and shed light on its causes. We further propose a method and show it can improve cross-lingual transferability  by mitigating negative interference.  While prior efforts focus on improving sharing and cross-lingual alignment,  we provide new insights and a different perspective  on unsharing and resolving language conflicts.   
"," Modern multilingual models are trained on concatenated text  from multiple languages in hopes of conferring benefits to each , with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade  performance on high-resource languages,  a phenomenon known as . In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief,  negative interference  also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures,  we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations,  we also present a meta-learning algorithm that obtains  better cross-lingual transferability  and alleviates negative interference,  by adding language-specific layers as meta-parameters  and training them in a manner that explicitly improves  shared layers' generalization on all languages. Overall, our results show that negative interference  is more common than previously known,  suggesting new directions for improving multilingual representations.\footnote{Source code is available at \url{https://github.com/iedwardwangi/MetaAdapter}.} %  % State-of-the-art multilingual models are trained on concatenated text from multiple languages to enable positive cross-lingual transfer, especially from high-resource languages to low-resource languages. % However, recent work found that such a training paradigm can degrade the model's performance on high-resource languages too, a phenomenon known as . % In this paper, we present the first systematic study of negative interference. % We show that, contrary to what was previously hypothesized, negative interference is not exclusive to high-resource but can also occur in low-resource settings. % In addition, despite that parameters are shared with the goal to learn language-universal structures, we demonstrate that language-specific parameters in multilingual models are a potential cause of negative interference. % Motivated by these observations, we show that we can obtain better cross-lingual transferability and alleviate negative interference through a meta-learning algorithm, which considers language-specific layers as meta parameters and trains them in the manner that explicitly improves the generalization of shared parameters across all languages. % Overall, our results show that negative interference occurs more commonly than previously believed and suggest a new direction towards improving multilingual representations by resolving language conflicts.\footnote{Code will be released upon publication.}",148
"  Event argument extraction  aims to identify the entities that serve as arguments of an event and to classify the specific roles they play. As in Fig., ``two soldiers'' and ``yesterday'' are arguments, where the event triggers are ``attacked''   and ``injured'' . For the trigger ``attacked'', ``two soldiers'' plays the argument role Target while ``yesterday'' plays the argument role Attack\_Time. For the event trigger ``injured'', ``two soldiers'' and ``yesterday'' play the role Victim and INJURY\_Time, respectively. There has been significant work on event extraction  , but the EAE task remains a challenge and has become the bottleneck for improving the overall performance of EE.\footnote{EAE has similarities with semantic role labeling. Event triggers are comparable to predicates in SRL and the roles in most SRL datasets have a standard convention of interpreting who did what to whom. EAE has a custom taxonomy of roles by domain. We also use inspiration from the SRL body of work .}     Supervised data for EAE is expensive and hence scarce. One possible solution is to use other available resources like unlabeled data. For that,  We use  BERT as our model encoder which leverages a much larger unannotated corpus where semantic information is captured. Unlike%previous studies ~ who added a final/prediction layer to BERT for argument extraction, we use BERT as token embedder and build a sequence of EAE task-specific components .  We use  in-domain data to adapt the BERT model parameters in a subsequent pretraining step as in . This makes the encoder domain-aware.  We perform self-training to construct auto-labeled data .  A crucial aspect for EAE is to integrate event trigger information into the learned representations. This is important because arguments are dependent on triggers, i.e., the same argument span plays completely different roles toward different triggers. An example is shown in Fig., where ``two soldiers'' plays the role Target for the event ATTACK and the role Victim for INJURY. Different from existing work that relies on regular sequence encoders, we design a novel trigger-aware encoder which simultaneously learns four different types of trigger-informed sequence representations. %for candidate arguments.   Capturing the long-range dependency is another important factor, e.g., the connection between an event trigger and a distant argument. Syntactic information could be useful in this case, as it could help bridge the gap from a word to another distant but highly related word. We modify a Transformer  by explicitly incorporating syntax via an attention layer driven by the dependency parse of the sequence. % .  %Since arguments of an event are entities, entity mentions are very effective hints.  We design our role-specific argument decoder to seamlessly accommodate both settings . We also tackle the role overlap problem  using a set of classifiers or taggers in our decoder.   Our model achieves the new state-of-the-art on ACE2005 Events data.% for EAE.  % % Motivation 1: data scarcity. Proposed and used solutions:  pretrained model BERT  External embedding   Self-training   BERT MLM  MLM encoder and decoder joint pre-training.  Teacher-Student    %  We present a new model which provides the best results in the EAE task. The model can generate trigger-aware argument representations, incorporate syntactic information , and handle the role overlapping problem with role-specific argument decoder. We also experiment with some methods to address the data scarcity issue. Experimental results show the effectiveness of our proposed approaches.    Experimental results demonstrate the effectiveness of our approach.   We also addressd the learning gap/discrepancy between pre-trained and newly-trained components.  
"," Event argument extraction  aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges:  Data scarcity.  Capturing the long-range dependency, specifically, the connection between an event trigger and a distant event argument.  Integrating event trigger information into candidate argument representation. For , we explore using unlabeled data in different ways. For , we propose to use a syntax-attending Transformer that can utilize dependency parses to guide the attention mechanism. For , we propose a trigger-aware sequence encoder with several types of trigger-dependent sequence representations. We also support argument extraction either from text annotated with gold entities or from plain text. Experiments on the English ACE2005 benchmark show that our approach achieves a new state-of-the-art.",149
"  Topic segmentation is a fundamental NLP task that has received considerable attention in recent years .  It can reveal important aspects of a document semantic structure by splitting the document into topical-coherent textual units. Taking the Wikipedia article in Table as an example, without the section marks, a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical-coherent units ,  and . The results of topic segmentation can further benefit other key downstream NLP tasks such as document summarization , question answering , machine reading  and dialogue modeling .   {  {|m{23em}|}     \underline{Preface:} \\  \rowcolor{Gray}  Marcus is a city in Cherokee County, Iowa, United States. \\  }} \\  \underline{S1}: The first building in Marcus was erected in 1871.\\  \underline{S2}: Marcus was incorporated on May 15, 1882. \\  }} \\  \underline{S3}: Marcus is located at .\\  \underline{S4}: According to the United States Census Bureau, the city has a total area of 1.54 square miles, all land. \\  }} \\  \underline{S5}: As of the census of 2010, there were 1,117 people, 494 households, and 310 families residing in the city. \\  ... ...\\   covering three topics: ,  and }   A wide variety of techniques have been proposed for topic segmentation. Early unsupervised models exploit word statistic overlaps , Bayesian contexts  or   %the  semantic relatedness graphs  to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them. More recently, several works have framed topic segmentation as neural supervised learning, because of the remarkable success achieved by such models in most NLP tasks .  %While one line of research forms topic segmentation as a sequence labeling problem and builds neural models to predict segment boundaries directly ;  %another line of works first trains neural models for other tasks , and then uses these models' outputs to predict boundaries .  Despite %the  minor architectural differences, most of these neural solutions adopt Recurrent Neural Network  and its variants  as their main framework.  On the one hand, RNNs are appropriate because topic segmentation can be modelled as a sequence labeling task where each sentence is either the end of a segment or not. On the other hand, this choice makes these neural models limited in how to model the context. Because some sophisticated RNNs  are able to preserve long-distance information , which can largely help language models. But for topic segmentation, it is critical to supervise the model to focus more on the local context.    %In fact, RNNs are superior on many NLP tasks due to their capability of preserving long-distance information . %However, for topic segmentation, it is also critical to supervise the model to learn the right information from the local context.   As illustrated in Table, the prediction of the segment boundary between  and  hardly depends on the content in . Bringing in excessive long-distance signals may cause unnecessary noise and %further  hurt %model's  performance. Moreover, text coherence has strong relation with topic segmentation .  For instance, in Table, sentence pairs from the same segment  %should be  are more coherent %to put together than sentence pairs across segments .  Arguably, with a proper way of modeling the coherence between adjacent sentences, a topic segmenter can be further enhanced.   %\textcolor{red}{We hypothesize that topic segment prediction should rely on local contextual information in a way that cannot be effectively captured by RNNs.} %\textcolor{red}{In essence, RNNs are able to model long and short-distance dependencies only implicitly.}  %However, with restricted self-attention, our model can pay attention to the local context from the neighboring sentences in a more explicitly constrained way . %In essence, local contextual information is critical in predicting topical boundaries, but simple Recurrent Neural Network  and its variants are arguably not sufficiently powerful to represent the necessary information.  %However, both approaches still face the challenge of insufficient context modeling. Topic segment boundary prediction usually heavily relies on local contextual information. Hence, how to effectively select local contexts and model the relations between contexts becomes important. Neural models like RNN and its variants can represent the state of each timestep by memorizing or forgetting the information from its previous and later contexts. But how these learned contextual information contribute to model's decision is not straightforward and sufficiently transparent.  In this paper, we propose to enhance a state-of-the-art  topic segmenter  based on hierarchical attention BiLSTM network to better model the local context of a sentence in two complementary ways. First, we add a coherence-related auxiliary task to make our model learn more informative hidden states for all the sentences in a document.  %More specifically, we refine the objective of our model to encourage that the coherence of the sentences from different segments is smaller than the coherence of the sentences from the same segment.  More specifically, we refine the objective of our model to encourage smaller coherence for the sentences from different segments and larger coherence for the sentences from the same segment.  Secondly, we enhance context modeling by utilizing restricted self-attention , which enables our model to pay attention to the local context and make better use of the information from the closer neighbors of each sentence .  Our empirical results show  that our proposed context modeling strategy significantly improves the performance of the SOTA neural segmenter on three datasets,  that the enhanced segmenter is more robust in domain transfer setting when applied to four challenging real-world test sets, sampled differently from the training data,  that our context modeling strategy is also effective for the segmenters trained on other challenging languages , rather than just English.      In this paper,  We address a serious limitation of current neural topic segmenters, namely their inability to effectively model context. To this end, we propose a novel neural model that adds a coherence-related auxiliary task and restricted self-attention on top of a hierarchical BiLSTM attention segmenter  encourage the model  to make better use of the contextual information. Experimental results of intra-domain on three datasets show that our strategy is effective within domains. Further, results on four challenging real-world benchmarks demonstrate its effectiveness in domain transfer settings. Finally, the application to other two languages  suggests that our strategy has its potential in multilingual scenarios.   As future work, we will investigate whether our proposed context modeling strategy is also effective for segmenting dialogues  rather than just standard articles.  Secondly, we will explore how to capture even more accurate and informative contextual information by integrating document structures or sentence dependencies obtained from other NLP tasks .  
","      Topic segmentation is critical %, the process of splitting a document into topic-coherent pieces,      %plays a vital role      in key NLP tasks and recent works favor highly effective neural supervised  approaches.     %Due to the high effectiveness of neural models, more recent works have favored framing topic segmentation as a neural-based supervised learning problem.     However, current neural solutions are arguably limited in how they model context.     %topic segmenters proposed so far are still limited by the insufficient context modeling.      In this paper, we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context, by adding a coherence-related auxiliary task and restricted self-attention. Our optimized segmenter\footnote{Our code will be publicly available at \url{www.cs.ubc.ca/cs-research/lci/research-groups/natural-language-processing/}} outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed model in domain transfer setting by training a model on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed strategy to two other languages , and show its effectiveness in multilingual scenarios.",150
"  Deep learning techniques, including contextualized word embeddings based on transformers and pretrained on language modelling, have resulted in considerable improvements for many NLP tasks. However, they often require large amounts of labeled training data, and there is also growing evidence that transferring approaches from high to low-resource settings is not straightforward. In , rule-based or linguistically motivated CRFs still outperform RNN-based methods on several tasks for South African languages. For pretraining approaches where labeled data exists in a high-resource language, and the information is transferred to a low-resource language,  find a significant gap between performance on English and the cross-lingually transferred models. In a recent study,  find that the transfer for multilingual transformer models is less effective for resource-lean settings and distant languages. A popular technique to obtain labeled data quickly and cheaply is distant and weak supervision.  recently inspected POS classifiers trained on weak supervision. They found that in contrast to scenarios with simulated low-resource settings of high-resource languages, in truly low-resource settings this is still a difficult problem. These findings also highlight the importance of aiming for realistic experiments when studying low-resource scenarios.   In this work, we analyse multilingual transformer models, namely mBERT  and XLM-RoBERTa . We evaluate both sequence and token classification tasks in the form of news title topic classification and named entity recognition . A variety of approaches have been proposed to improve performance in low-resource settings. In this work, we study  transfer learning from a high-resource language and  distant supervision. We selected these as they are two of the most popular techniques in the recent literature and are rather independent of a specific model architecture. Both need auxiliary data. For transfer learning, this is labeled data in a high-resource language, and for distant supervision, this is expert insight and a mechanism to automatically generate labels. We see them, therefore, as orthogonal and depending on the scenario and the data availability, either one or the other approach might be applicable.  Our study is performed on three, linguistically different African languages: Hausa, isiXhosa and \yoruba. These represent languages with millions of users and active use of digital infrastructure, but with only very limited support for NLP technologies. For this aim, we also collected three new datasets that are made publicly available alongside the code and additional material.  We show both challenges and opportunities when working with multilingual transformer models evaluating trends for different levels of resource scarcity. The paper is structured into the following questions we are interested in:  [leftmargin=*]          In this work, we evaluated transfer learning and distant supervision on multilingual transformer models, studying realistic low-resource settings for African languages. We show that even with a small amount of labeled data, reasonable performance can be achieved. We hope that our new datasets and our reflections on assumptions in low-resource settings help to foster future research in this area.  
"," Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and \yoruba on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.",151
"  In computer vision, { modality, here we focus on the {   To get an intuition about the task setup and our proposed solution, consider the following situation. Imagine you have never seen a zebra but have seen a horse. What if you were given a text describing a zebra: . This description would probably be very close to a description of a horse having  and you would probably be looking for an image that reminds you of a horse but has . So, even without ever seeing a zebra, using text-descriptions of the zebra and knowledge already acquired about horses, one can correctly classify unknown classes like a zebra.   Our proposed solution has two-phases. First, based on the intuition that similar objects  tend to have similar texts, we encode a similarity feature that enhances text descriptions' separability.  In addition, we leverage the intuition that the differences between text descriptions of species would be their most salient visual features, and extract visually relevant descriptions from the text.   Our experiments empirically demonstrate both the { capacity of our proposed solution.  On two large ZSL datasets, in both the { scenarios, the similarity method obtains a ratio improvement of up to 18.3\%. With the addition of extracting visually relevant descriptions, we obtain a ratio improvement of up to 48.16\% over the state-of-the-art. We further show that our visual-summarization method generalizes from the CUB dataset  to the NAB dataset , and we demonstrate its contribution to additional models by a ratio improvement of up to 59.62\%.    The contributions of this paper are threefold. First, to the best of our knowledge, we are the first to showcase the critical importance of the text representation in zero-shot image-recognition scenarios, and we present two concrete text-based processing methods that vastly improve the results. Second, we demonstrate the efficacy and generalizability of our proposed methods by applying them to both the { tasks, outperforming all previously reported results on the CUB and NAB Benchmarks.   Finally, we show that visual aspects learned from one dataset can be transferred effectively to another dataset without the need to obtain dataset-specific captions.  The efficacy of our proposed solution on these benchmarks illustrates that purposefully exposing the visual features in texts is indispensable for tasks that learn to align the vision-and-language modalities.       This work aims to establish a better way to represent the language modality in { and visually-relevant summaries, lead to significant improvements across models, splits, and datasets, and illustrate that adequate text-processing is essential in text-based ZSL tasks.  We   conjecture that text-processing methods will be essential in a range of vision and language-based tasks, and hope this work will assist future research in better representing the language modality in various multi-modal tasks.    First, unsupervised clustering algorithms are used to construct textual similarity vectors between seen and unseen image-text pairs.   Second, visually relevant summaries  from texts. Each sentence from the image description is assigned a VRS-score, which determines the sentence's level of groundedness in images.   
","    We study the problem of recognizing visual entities from the textual descriptions of their classes. Specifically, given birds' images with free-text descriptions of their species, we learn to classify images of previously-unseen species based on specie descriptions. This setup has been studied in the vision community under the name { between species, reflected in the similarity between text descriptions of the species.  we derive { features that tend to be reflected in images. We propose a simple attention-based model augmented with the similarity and visual summaries components. Our empirical results consistently and significantly outperform the state-of-the-art on the largest benchmarks for text-based zero-shot learning, illustrating the critical importance of texts for zero-shot image-recognition.",152
"  Taking advantage of monolingual training data via Back-Translation~, Iterative Back-Translation~ or Dual Learning~  has become a de facto requirement for building high quality Neural Machine Translation  systems~.  However, these methods rely on unrelated heuristic optimization objectives, and it is not clear what their respective strengths and weaknesses are, nor how they relate to the ideal but intractable objective of maximizing the marginal likelihood of the monolingual data  =  p_\theta q coincides with the target sentence distribution~. We also show that Iterative Back-Translation  and Dual Learning can be viewed as different ways to approximate its optimization.  \looseness=-1 Theory suggests that IBT approximates the dual reconstruction objective more closely than the more complex Dual Learning approach, and in particular that Dual Learning's additional language model loss is redundant. We investigate whether these differences matter in practice by conducting the first controlled empirical comparison of Back-Translation, IBT, and Dual Learning in high-resource , low-resource , and cross-domain settings . Results support our theory that the additional language model loss and policy gradient estimation in Dual Learning is redundant and show that IBT outperforms the more complex Dual Learning algorithm in terms of translation quality. Furthermore, we also compare different optimization strategies used in IBT to better balance translation quality against the computational cost.         We contribute theoretical and empirical results that improve our understanding of the connection between two seemingly distant semi-supervised training strategies for NMT: Iterative Back-Translation  and Dual Learning.  \looseness=-1 On the theory side, we define a dual reconstruction objective which unifies semi-supervised NMT techniques that exploit source and target monolingual text. We prove that optimizing this objective leads to the same global optimum as the intractable marginal likelihood objective, where the model's marginal distribution coincides with the prior language distribution while also maximizing the model's mutual information between source and target. IBT approximates this objective more closely than Dual Learning, despite the more complex objective and update strategies used in the latter.  We present a systematic empirical comparison of Back-Translation, IBT, and Dual Learning on six tasks spanning high-resource, low-resource, and cross-domain settings.  Results support the theory that the LM loss and policy gradient estimation are unnecessary in Dual Learning, and show that IBT achieves better translation quality than Dual Learning.  Analysis confirms that the mutual information constraint required to reach an interesting dual reconstruction optimum is satisfied in practice.  These findings lead us to recommend batch-level IBT to quickly boost model performance at early training stages and epoch-level IBT to further improve quality. Our theory also suggests future directions for improving unsupervised MT via more effective methods to maximize the model閳ユ獨 mutual information between source and target, and the potential of applying our dual reconstruction objective to other sequence-to-sequence tasks.  
"," While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction objective that provides a unified view of Iterative Back-Translation and Dual Learning. It motivates a theoretical analysis and controlled empirical study on German-English and Turkish-English tasks, which both suggest that Iterative Back-Translation is more effective than Dual Learning despite its relative simplicity.",153
" Due to the growing number of Internet users, cyber-violence emerged with offensive language pervasive across social media. With anonymity as a 閳ユ笡rivilege閳, netizens hide behind the screens, behaving in a manner most of them would not otherwise in reality. Thus, government organizations, online communities, and technology companies are all striving for ways to detect aggressive language in social media and help build a more friendly online environment.  Manual filtering is very time consuming and it can cause post-traumatic stress disorder-like symptoms to human annotators. One of the most common strategies  to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside human moderation.  SemEval 2020 Task-12  is the second edition of OffensEval . In this competition, organizers offers 5 languages datasets including Arabic , Danish , English , Turkish  and Greek . In Sub-task A, the participants need to predict whether a post uses offensives language. Besides, the organizers provide other two sub-tasks which mainly focus on English, to predict the type and target of offensive language.  Participating in all 3 Sub-tasks, we proposed several methods based on pre-training language models including ERNIE and XLM-R. In Sub-task A, we scored 0.9199, 0.851, 0.8258, 0.802, 0.8989 in English, Greek, Turkish, Danish and Arabic respectively. We ranked first in average F1 scores, and ranked in top three across all languages. In Sub-task B and Sub-task C, we also took the first place with 0.7462 and 0.7145. In the following sections, we will elaborate the methods, dataset and experiments of our system.    In this paper, we presented our approach on detecting and categorizing offensive language in social media. We proposed a multi-lingual learning method to detect offensive language and a knowledge distillation method to categorize offensive language. We will further our exploration of multilingual offensive language identification in future, e.g. validating the zero-shot performance of our model in more languages.       include your own bib file like this: 
","   This paper describes Galileo闁炽儲鐛 performance in SemEval-2020 Task 12 on detecting and categorizing offensive language in social media. For Offensive Language Identification, we proposed a multi-lingual method using Pre-trained Language Models, ERNIE and XLM-R. For offensive language categorization, we proposed a knowledge distillation method trained on soft labels generated by several supervised models. Our team participated in all three sub-tasks. In Sub-task A - Offensive Language Identification, we ranked first in terms of average F1 scores in all languages. We are also the only team which ranked among the top three across all languages.  We also took the first place in Sub-task B - Automatic Categorization of Offense Types and Sub-task C - Offence Target Identification.",154
" Understanding and reasoning over natural language plays a significant role in artificial intelligence tasks such as Machine Reading Comprehension  and Question Answering . Several QA tasks have been proposed in recent years to evaluate the language understanding capabilities of machines . These tasks are single-hop QA tasks and consider answering a question given only one single paragraph. % The drawback of single-hop QA tasks is the lack of evaluating deep reasoning capability.  % We observe that many existing neural models achieve promising performance without reasoning.  Many existing neural models rely on learning context and type-matching heuristics. Those rarely build reasoning modules but achieve promising performance on single-hop QA tasks. The main reason is that these single-hop QA tasks are lacking a realistic evaluation of reasoning capabilities because they do not require complex reasoning.   Recently multi-hop QA tasks, such as HotpotQA  and WikiHop, have been proposed to assess multi-hop reasoning ability. HotpotQA task provides annotations to evaluate document level question answering and finding supporting facts. Providing supervision for supporting facts improves explainabilty of the predicted answer because they clarify the cross paragraph reasoning path.   Due to the requirement of multi-hop reasoning over multiple documents with strong distraction, multi-hop QA tasks are challenging.  Figure shows an example of HotpotQA. Given a question and 10 paragraphs, only paragraph  and paragraph  are relevant. The second sentence in paragraph  and the first sentence in paragraph  are the supporting facts. The answer is ``Geelong Football Club''.   Primary studies in HotpotQA task prefer to use a reading comprehension neural model. First, they use a neural retriever model to find the relevant paragraphs to the question. After that, a neural reader model is applied to the selected paragraphs for answer prediction. Although these approaches obtain promising results, the performance of evaluating multi-hop reasoning capability is unsatisfactory.   To solve the multi-hop reasoning problem, some models tried to construct an entity graph using Spacy or Stanford CoreNLP and then applied a graph model to infer the entity path from question to the answer. However, these models ignore the importance of the semantic structure of the sentences and the edge information and entity types in the entity graph. To take the in-depth semantic roles and semantic edges between words into account here we use semantic role labeling  graph as the backbone of a graph convolutional network. Semantic role labeling provides the semantic structure of the sentence in terms of argument-predicate relationships.  % such as ``who did what to whom.'' The argument-predicate relationship graph can significantly improve the multi-hop reasoning results. Our experiments show that SRL is effective in finding the cross paragraph reasoning path and answering the question.  Our proposed semantic role labeling graph reasoning network  jointly learns to find cross paragraph reasoning paths and answers questions on multi-hop QA. In SRLGRN model, firstly, we train a paragraph selection module to retrieve gold documents and minimize distractor. Second, we build a heterogeneous document-level graph that contains sentences as nodes ,  % and the sentence nodes include  and SRL sub-graphs including semantic role labeling arguments as nodes and predicates as edges. Third, we train a graph encoder to obtain the graph node representations that incorporate the argument types and the semantics of the predicate edges in the learned representations. Finally, we jointly train a multi-hop supporting fact prediction module that finds the cross paragraph reasoning path, and answer prediction module that obtains the final answer. Notice that both supporting fact prediction and answer prediction are based on contextual semantics graph representations as well as token-level BERT pre-trained representations. The contributions of this work are as follows:    Our proposed model obtains competitive results on both HotpotQA  and the SQuAD benchmarks.    We proposed a novel semantic role labeling graph reasoning network  to deal with multi-hop QA.    The model jointly trains to detect the supporting facts and to find the final answer. The backbone graph of our proposed graph convolutional network  is created based on the semantic structure of the sentences. In creating the  edges and nodes of the graph, we exploit a semantic role labeling sub-graph for each sentence and connect the candidate supporting facts. The cross paragraph argument-predicate structure of the sentences expressed in the graph provides an explicit representation of the reasoning path and helps in both finding and explaining the multiple hops of reasoning that lead to the final answer.   We analyze the multi-hop reasoning ability of our model.  SRLGRN exceeds most of the SOTA results on the HotpotQA benchmark. Moreover, we evaluate the model  on other reading comprehension benchmarks. Our approach achieves competitive performance on SQuAD v and v.  
"," This work deals with the challenge of learning and reasoning over multi-hop question answering . We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence , and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.",155
"   The organizers of the 2020 VarDial Evaluation Campaign  proposed a shared task targeted towards the geolocation of short texts, e.g.~tweets, namely the Social Media Variety Geolocation  task. Typically formulated as a double regression problem, the task is about predicting the location, expressed in latitude and longitude, from where the text received as input was posted on a certain social media platform. Twitter and Jodel are the platforms used for data collection, divided by the language area in three subtasks, namely:        In this paper, we focus only on the second subtask, SMG-CH, proposing a variety of handcrafted and deep learning models, as well as an ensemble model that combines all our previous models through meta-learning. Our first model is a Support Vector Regression  classifier  based on string kernels, which are known to perform well in other dialect identification tasks . Our second model is a character-level convolutional neural network  , which is also known to provide good results in dialect identification . Due to the high popularity and the outstanding results of Bidirectional Encoder Representations from Transformers   in solving mainstream NLP tasks, we decided to try out a Long Short-Term Memory  network  based on German BERT embeddings as our third model. Lastly, we combine our three models into an ensemble that employs Extreme Gradient Boosting   as meta-learner. We conducted experiments on the development set provided by the organizers, in order to decide which models to choose for our three submissions for the SMG-CH subtask. Our results indicate that the ensemble model attains the best results. Perhaps surprisingly, our shallow approach based on string kernels outperforms both deep learning models. Our observations are consistent across the development and the test sets provided by the organizers.  % We experimented with a few Machine Learning algorithms for the second subtask, namely CH,  % Geolocation can be framed as a double regression task, but more sophisticated model architectures have been proposed .  % Jodel is a mobile chat application that lets people anonymously talk to other users within a 10km-radius around them.   % All three subtasks will use the same data format and evaluation methodology, and participants are encouraged to submit their systems for all subtasks.  The rest of this paper is organized as follows. We present related work on dialect identification and geolocation of short texts in Section. Our approaches are described in more detail in Section. We present the experiments and empirical results in Section. Finally, our conclusions are drawn in Section.      In the current work, we tackled the SMG-CH shared subtask of the 2020 VarDial Evaluation Campaign. We addressed this challenge from a shallow perspective, with handcrafted models such as a -SVR based on string kernels, as well as from a deep learning perspective, with neural models such as an LSTM based on BERT embeddings and a character-level CNN, respectively. Additionally, we combined the proposed models into an ensemble, employing the XGBoost meta-learner. We obtained our best results with the XGBoost ensemble, which benefits from complementary information from the handcrafted and deep models. We therefore brought one more proof regarding the effectiveness of ensemble learning in general, and of XGBoost, in particular. Another important conclusion is that our shallow model based on string kernels outperforms the two deep neural networks. We consider this as yet another indicator of the high discriminative power that string kernels can bring to a fairly standard learning model, i.e.~the -SVR.   In future work, we aim to explore ways to improve our performance with respect to the metrics proposed by the shared task organizers. Currently, it seems that training the models to simply minimize the MSE or the MAE values is not effective, as our best model was significantly outperformed by the model proposed by the shared task organizers themselves.  
"," In this work, we introduce the methods proposed by the UnibucKernel team in solving the Social Media Variety Geolocation task featured in the 2020 VarDial Evaluation Campaign. We address only the second subtask, which targets a data set composed of nearly 30 thousand Swiss German Jodels. The dialect identification task is about accurately predicting the latitude and longitude of test samples. We frame the task as a double regression problem, employing a variety of machine learning approaches to predict both latitude and longitude. From simple models for regression, such as Support Vector Regression, to deep neural networks, such as Long Short-Term Memory networks and character-level convolutional neural networks, and, finally, to ensemble models based on meta-learners, such as XGBoost, our interest is focused on approaching the problem from a few different perspectives, in an attempt to minimize the prediction error. With the same goal in mind, we also considered many types of features, from high-level features, such as BERT embeddings, to low-level features, such as characters n-grams, which are known to provide good results in dialect identification. Our empirical results indicate that the handcrafted model based on string kernels outperforms the deep learning approaches. Nevertheless, our best performance is given by the ensemble model that combines both handcrafted and deep learning models.",156
"  Comparing and contrasting the meaning of text conveyed in different languages is a fundamental nlp task. It can be used to curate clean parallel corpora for downstream tasks such as machine translation~, cross-lingual transfer learning, or semantic modeling~, and it is also useful to directly analyze multilingual corpora. For instance, detecting the commonalities and divergences between sentences drawn from English and French Wikipedia articles about the same topic would help analyze language bias~, or mitigate differences in coverage and usage across languages~. This requires not only detecting coarse content mismatches, but also fine-grained differences in sentences that overlap in content.  Consider the following English and French sentences, sampled from the WikiMatrix parallel corpus. While they share important content, highlighted words convey meaning missing from the other language:  {p{0.9  as an unofficial Canadian national anthem.\\ fr  pro canadien anglais.\\ \textcolor{darkgray}{gloss    We show that explicitly considering diverse types of semantic divergences in bilingual text benefits both the annotation and prediction of cross-lingual semantic divergences. We create and release the Rationalized English-French Semantic Divergences corpus , based on a novel divergence annotation protocol that exploits rationales to improve annotator agreement. We introduce \modelname, a  bert-based model that detects fine-grained semantic divergences without supervision by learning to rank synthetic divergences of varying granularity. Experiments on \dataset show that our model distinguishes semantically equivalent from divergent examples much better than a strong sentence similarity baseline and that unsupervised token-level divergence tagging offers promise to refine distinctions among divergent instances. We make our code and data publicly available.\footnote{Implementations of \modelname can be found at: \url{https://github.com/Elbria/xling-SemDiv}; the \dataset dataset is hosted at:   \url{https://github.com/Elbria/xling-SemDiv/tree/master/REFreSD}.}          We show that explicitly considering diverse semantic divergence types benefits both the annotation and prediction of divergences between texts in different languages.  We contribute \dataset, a new dataset of WikiMatrix sentences-pairs in English and French, annotated with semantic divergence classes and token-level rationales that justify the sentence level annotation.  of samples are annotated as divergent, and  of samples contain fine-grained meaning divergences, confirming that divergences are too frequent to ignore even in parallel corpora. We show that these divergences can be detected by a m model fine-tuned without annotated samples, by learning to rank synthetic divergences of varying granularity.   Inspired by the rationale-based annotation process, we show that predicting token-level and sentence-level divergences jointly is a promising direction for further distinguishing between coarser and finer-grained divergences.  
","  Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual nlp and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale.~This work improves the prediction and annotation of fine-grained semantic divergences.~We introduce a training strategy for multilingual bert models by learning to rank synthetic divergent examples of varying granularity.~We evaluate our models on the~Rationalized~English-French~Semantic~Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.~Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.",157
"  There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports to congressional bills and meeting conversations. The lack of annotated resources suggests that end-to-end systems may not be a ``one-size-fits-all'' solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators to realize the full potential of neural abstractive summarization.   We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate  results of such a module, rather than associating it with text generation.  Existing neural abstractive systems can perform content selection implicitly using end-to-end models, or more explicitly, with an external module to select important sentences or words to aid generation. However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary.           In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the source document, then highlights their summary-worthy segments and uses those as a basis for composing a summary sentence. When a pair of sentences are selected, it is important to ensure that they are ---there exists cohesive devices that tie the two sentences together into a coherent text---to avoid generating nonsensical outputs.  Highlighting sentence segments allows us to perform fine-grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence.  The contributions of this work are summarized as follows. [topsep=5pt,itemsep=0pt]  }}        We present a cascade approach to neural abstractive summarization that separates content selection from surface realization. Importantly, our approach makes use of text highlights as intermediate representation; they are derived from one or two sentences using a coarse-to-fine content selection strategy, then passed to a neural text generator to compose a summary sentence. A successful cascade approach is expected to accurately select sentences and highlight an appropriate amount of text, both can be customized for domain-specific tasks.   
","  We present an empirical study in favor of a cascade architecture to neural text summarization. Summarization practices vary widely but few other than news summarization can provide a sufficient amount of training data enough to meet the requirement of end-to-end neural abstractive systems which perform content selection and surface realization jointly to generate abstracts.  Such systems also pose a challenge to summarization evaluation, as they force content selection to be evaluated along with text generation, yet evaluation of the latter remains an unsolved problem. In this paper, we present empirical results showing that the performance of a cascaded pipeline that separately identifies important content pieces and stitches them together into a coherent text is comparable to or outranks that of end-to-end systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research.",158
"   The recent advances in neural machine translation   have provided the research community and the commercial landscape with effective translation models that can at times achieve near-human performance. However, this usually holds at phrase or sentence level. When using these models in larger units of text, such as paragraphs or documents, the quality of the translation may drop considerably in terms of discourse attributes such as lexical and stylistic consistency.  In fact, document-level translation is still a very open and challenging problem. The sentences that make up a document are not unrelated pieces of text that can be predicted independently; rather, a set of sequences linked together by complex underlying linguistics aspects, also known as the discourse . The discourse of a document includes several properties such as grammatical cohesion , lexical cohesion , document coherence  and the use of discourse connectives . Ensuring that the translation retain such linguistic properties is expected to significantly improve its overall readability and flow.  However, due to the limitations of current decoder technology, NMT models are still bound to translate at sentence level. In order to capture the discourse properties of the source document in the translation, researchers have attempted to incorporate more contextual information from surrounding sentences. Most document-level NMT approaches augment the model with multiple encoders, extra attention layers and memory caches to encode the surrounding sentences, and leave the model to implicitly learn the discourse attributes by simply minimizing a conventional NLL objective. The hope is that the model will spontaneously identify and retain the discourse patterns within the source document. Conversely, very little work has attempted to model the discourse attributes explicitly. Even the evaluation metrics typically used in translation such as BLEU  are not designed to assess the discourse quality of the translated documents.  For these reasons, in this paper we propose training an NMT model by directly targeting two specific discourse metrics: lexical cohesion  and coherence . LC is a measure of the frequency of semantically-similar words co-occurring in a document  . For example, car, vehicle, engine or wheels are all semantically-related terms. There is significant empirical evidence that ensuring lexical cohesion in a text eases its understanding . At its turn, COH measures how well adjacent sentences in a text are linked to each other. In the following example from Hobbs :  	 ``John took a train from Paris to Istanbul. He likes spinach.''	      approach from reinforcement learning  which allows using any evaluation metric as a reward without having to differentiate it. By combining different types of rewards, the model can be trained to simultaneously achieve more lexically-cohesive and more coherent document translations, while at the same time retaining faithfulness to the reference translation. %the information contained in the source document.  %The rest of the paper is organized as follows. Section  discusses related work. Section  describes the baseline NMT architectures used for the experiments. Section  presents the proposed training approach and the discourse rewards used with it. Section  presents the experiments and, finally, Section  concludes the paper.       In this paper, we have presented a novel training method for document-level NMT models that uses discourse rewards to encourage the models to generate more lexically cohesive and coherent translations at document level. As training objective we have used a reinforcement learning-style function, named Risk, that permits using discrete, non-differentiable terms in the objective. Our results on four different language pairs and three translation domains have shown that our models have achieved a consistent improvement in discourse metrics such as LC and COH, while retaining comparable values of accuracy metrics such as BLEU and . In fact, on certain datasets, the models have even improved on those metrics. While the approach has proved effective in most cases, the best combination of discourse rewards, accuracy rewards and NLL has had to be selected by validation for each dataset. In the near future we plan to investigate how to automate this selection, and also explore the applicability of the proposed approach to other natural language generation tasks.  
","   Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion  and coherence , by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of $2.46$ percentage points  in LC and $1.17$ pp in COH over the runner-up, while at the same time improving $0.63$ pp in BLEU score and $0.47$ pp in $\mathrm{F}_{\mathrm{BERT}}$.      %In fact, in some cases our training approach has even improved translation accuracy metrics such as BLEU and the recently proposed $F_{\text{BERT}}$.",159
" In recent years, neural models have led to state-of-the-art results in machine translation  . Many of these systems can broadly be characterized as following a multi-layer encoder-decoder neural network design: both the encoder and decoder learn representations of word sequences by a stack of layers , building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model , deep systems have shown promising BLEU improvements by either easing the information flow through the network  or constraining the gradient norm across layers . An improved system can even learn a 35-layer encoder, which is  deeper than that of vanilla Transformer .  Although these methods have enabled training deep neural MT  models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner . It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely deep model is expensive although a narrow-and-deep network can speed up training . For example, it takes us  longer time to train the model when we deepen the network from 6 layers to 48 layers. This might prevent us from exploiting deeper models in large-scale systems.  In this paper, we explore why deep architectures work to render learning NMT models more effectively. By investigating the change of the hidden states in different layers, we find that new representations are learned by continually stacking layers on top of the base model. More stacked layers lead to a stronger model of representing the sentence. This particularly makes sense in the deep NMT scenario because it has been proven that deep models can benefit from an enriched representation .  In addition, the finding here inspires us to develop a simple yet efficient method to train a deep NMT encoder: we train model parameters from shallow to deep, rather than training the entire model from scratch. To stabilize training, we design a sparse linear combination method of connecting lower-level layers to the top. It makes efficient pass of information through the deep network but does not require large memory footprint as in dense networks. We experiment with the method in a state-of-the-art deep Transformer system. Our encoder consists of 48-54 layers, which is almost the deepest Transformer model used in NMT. On WMT En-De and En-Fr tasks, it yields a  speedup of training, matching the state-of-the-art on the WMT'16 En-De task.      We have investigated the behaviour of the well-trained deep Transformer models and found that stacking more layers could improve the representation ability of NMT systems. Higher layers share more global information over different positions and adjacent layers behave similarly. Also, we have developed a shallow-to-deep training strategy and employ sparse connections across blocks to ease the optimization. With the help of learning rate restart and appropriate initialization we successfully train a 48-layer RPR model by progressive stacking and achieve a  speedup on both WMT'16 English-German and WMT'14 English-French tasks. Furthermore, our -RPR-24L  achieves a BLEU score of  on WMT'16 English-German task, and speeds up the training by .  
","    Deep encoders have been proven to be effective in improving neural machine translation  systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is $1.4$ $\times$ faster than training from scratch, and achieves a BLEU score of $30.33$ and $43.29$ on two tasks. The code is publicly available at {https://github.com/libeineu/SDT-Training}.",160
"  Annual Reports may extend up to 250 pages long as stated above, which contains different sections General Corporate Information, financial and operating cost, CEOs message, Narrative texts, accounting policies, Financial statement including balance sheet and summary of financial data documents. In the Financial narrative summarisation task, only the narrative section is summarised, which is not explicitly marked in the dataset, making it challenging and interesting.  In recent years, previous manual small-scale research in the Accounting and Finance literature has been scaled up with the aid of NLP and ML methods, for example, to examine approaches to retrieving structured content from financial reports, and to study the causes and consequences of corporate disclosure and financial reporting outcomes . \par Companies produce glossy brochures of annual reports with a much looser structure, and this makes automatic summarisation of narratives in UK annual reports a challenging task . Hence we summarize the narrative section of annual reports, particular narrative sentences that are spread loosely across the document need to be first identified and summarise those sentences. The summarisation limit is set to 1000 words, where the actual length of the report may go up to 250 pages long. Hence to summarize these long annual reports using a combination of extractive and abstractive summarisation.\par The text summary method can be classified into two paradigms: extractive and abstractive. The extractive summarisation method extracts the meaningful sentences or a section of text from the original text and combines them  to form a summary . Whereas abstractive summarisation generates words and sentences that are similar in meaning to the given text to form a summary that may not be in actual text . When summarizing long documents such as in our case up to 250 pages long, extractive summarisation may not produce a coherent and readable summary, and abstractive summarisation cannot cover complete information using encoder-decoder architecture. One problem is that typical seq2seq frameworks often generate unnatural summaries consisting of repeated words or phrases . Hence, we come up with a combination of extractive and abstractive summarisation to first select important narrative sentences and concisely convey them. \par Pointer Networks  is used in various combinatorial optimization problems, such as Travelling Salesman Problem , Convex hull optimization. We used pointer networks in our task of financial narrative summarization to extract relevant narrative sentences in a particular order to have a logical flow in summary. These extracted sentences are paraphrased to summarise these sentences in an abstractive way using the T-5 sequence-to-sequence model. We train the complete model by optimizing the ROUGE-LCS evaluation metric through a reinforcement learning objective.   % % The following footnote without marker is nebe fireded for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version             % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. }   In this work we present our solution on Financial Narrative Summarisation dataset using PoinT-5 method explained in . It is combination of both extractive and abstractive methods using Pointer Network and T-5. With these methods we are able to achieve highest precision score in every evaluation metric and achieve highest F-1 scores in ROUGE-LCS and ROUGE-1.\par In our future work we would like to address several limitation of our method such as factual correctness in summaries which is very important in financial domain as done in  in summarizing radiology reports. To improve precision of our generated summaries under 1000 words we would formulate a penalty if system generates more than 1000 words during training of RL algorithm rather than restricting algorithm to fixed number of sentences.     include your own bib file like this: 
","   Companies provide annual reports to their shareholders at the end of the financial year that describes their operations and financial conditions. The average length of these reports is 80, and it may extend up to 250 pages long. In this paper, we propose our methodology PoinT-5  algorithms) that we used in the Financial Narrative Summarisation  2020 task. The proposed method uses Pointer networks to extract important narrative sentences from the report, and then T-5 is used to paraphrase extracted sentences into a concise yet informative sentence. We evaluate our method using $\operatorname{ROUGE}$-N , L,and SU4. The proposed method achieves the highest precision scores in all the metrics and highest F1 scores in $\operatorname{ROUGE}$ 1,and LCS and only solution to cross MUSE solution baseline in $\operatorname{ROUGE}$-LCS metrics.",161
"   Neural Architecture Search  methods aim to automatically discover neural architectures that perform well on a given task and dataset. These methods search over a space of possible model architectures, looking for ones that perform well on the task and will generalize to unseen data. There has been substantial prior work on how to define the architecture search space, search over that space, and estimate model performance .    Recent works, however, cast doubt on the quality and performance of NAS-optimized architectures , showing that current methods fail to find the best performing architectures for a given task and perform similarly to random architecture search.  In this work, we explore applications of a SOTA NAS algorithm, ENAS , to two sentence-pair tasks, paraphrase detection  and semantic textual similarity . We conduct a large set of experiments testing the effectiveness of ENAS-optimized RNN architectures across multiple models , embeddings  and datasets . We are the first, to our knowledge, to apply ENAS to PD and STS, to explore applications across multiple embeddings and traditionally LSTM-based NLP models, and to conduct extensive SOTA HPT across multiple ENAS-RNN architecture candidates.   Our experiments suggest that baseline LSTM models, with appropriate hyperparameter tuning , can sometimes match or exceed the performance of models with ENAS-RNNs. We also observe that random architectures sampled from the ENAS search space offer a strong baseline, and can sometimes outperform ENAS-RNNs. Given these observations, we recommend that researchers  conduct extensive HPT  across various candidate architectures for the fairest comparisons;  compare the performances of ENAS-RNNs against both standard architectures like LSTMs and RNN cells randomly sampled from the ENAS search space;  examine the computational  requirements of ENAS methods alongside the gains observed.       Unlike prior work applying ENAS to NLP, we find that ENAS-RNNs only outperform LSTMs and random search on some dataset, embedding, model) configurations. Our findings parallel recent work  which question the effectiveness of current NAS methods and their superiority to random architecture search and SOTA HPT methods.  Given our mixed results, we recommend researchers:  extensively tune hyperparameters for standard  and randomly sampled architectures to create strong baselines;  benchmark ENAS performance across multiple simple and complex model architectures ;  present computational requirements alongside gains observed with ENAS methods.       
","  Neural Architecture Search  methods, which automatically learn entire neural model or individual neural cell architectures, have recently achieved competitive or state-of-the-art  performance on variety of natural language processing and computer vision tasks, including language modeling, natural language inference, and image classification. In this work, we explore the applicability of a SOTA NAS algorithm, Efficient Neural Architecture Search   to two sentence pair tasks, paraphrase detection and semantic textual similarity. We use ENAS to perform a micro-level search and learn a task-optimized RNN cell architecture as a drop-in replacement for an LSTM. We explore the effectiveness of ENAS through experiments on three datasets , with two different models , and two sets of embeddings . In contrast to prior work applying ENAS to NLP tasks, our results are mixed -- we find that ENAS architectures sometimes, but not always, outperform LSTMs and perform similarly to random architecture search.",162
" Constituency parsing is a well-studied problem in natural language processing, but most state-of-the-art parsers have only been tested on written text, e.g.\ the standard Penn Treebank Wall Street Journal  dataset .  These recent neural parsers are commonly formulated as encoder-decoder systems, where the encoder learns the input sentence representation and the decoder learns to predict a parse tree. While input is often represented by word-level features, representation for the output trees varies:  as a sequence of parse symbols , a set of spans ,  syntactic distances , or per-word structure-rich labels . A key characteristic in many of these neural parsers is the recurrent network structure, particularly Long Short-Term Memory networks ; however, Kitaev and Klein  have shown that a non-recurrent encoder such as the Transformer network introduced in  is also capable of encoding timing information through self-attention mechanisms, achieving state-of-the-art parse results on the Treebank WSJ dataset.  Further, these parsers  %seem to mainly  benefit from contextualized information learned from larger external text data, such as ELMo  and BERT .  It is not clear that these advances will transfer to speech data, particularly for the different styles of speech. Even when perfect transcripts are available, speech poses many challenges to parsers learned from written text due to the lack of punctuation and case, and the presence of disfluencies.  On the other hand, speech signals carry rich information beyond words via variations in timing, intonation, and loudness, i.e. in . Linguistic studies have shown that prosodic cues align with constituent structure , signal disfluencies by marking the interruption point , and help listeners resolve syntactic ambiguities . Empirical evidence, however, has been mixed regarding the utility of prosody for constituency parsing. Most gains have been observed when sentence boundaries are unknown , or with annotated prosodic labels . Most related to our current work, Tran et al.\  recently showed the benefit of using prosody in parsing within a sequence-to-sequence framework, proposing a convolutional neural network  as a mechanism to combine discrete word-level features with frame-level acoustic-prosodic features.  In this study, we extend the work in  and  to explore the utility of recent neural advances on spontaneous speech data, and compare the utility of prosody in read vs.\ spontaneous speech. Specifically, the goal of the current study is to answer the following questions:  [topsep=1pt,itemsep=0ex,partopsep=0ex,parsep=0.2ex]   % TT: may cut this if space is lacking. But I didn't want to end the intro with questions without saying anything further %The rest of this paper is organized as follows: Section  describes the models used in this work; Section  reviews the datasets and metrics in constituency parsing; Section  presents our experiments, results, and analyses; and Section  summarizes the findings.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Moved data table here since it was oddly arranged %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% [th]    %  \usepackage{INTERSPEECH2019} \usepackage{url} \usepackage{multirow} \usepackage{xcolor} \usepackage{subcaption,enumitem} \usepackage{booktabs} \usepackage{comment}  [1]{\textcolor{red}{[1]{\textcolor{blue}{[1]{\textcolor{cyan}{[1]{\textcolor{green}{  %The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate. \address{   Electrical \& Computer Engineering, University of Washington\\   LAIX Inc.} @uw.edu, \{jiahong.yuan,yang.liu\}@liulishuo.com}     %   The differences in written text and conversational speech are substantial; previous parsers trained on treebanked text have given very poor results on spontaneous speech. For spoken language, the mismatch in style also extends to prosodic cues, though it is less well understood.  This paper re-examines the use of written text in parsing speech in the context of recent advances in neural language processing. We show that neural approaches facilitate using written text to improve  parsing of spontaneous speech, and that prosody further improves over this state-of-the-art result. Further, we find an asymmetric degradation from read vs.\ spontaneous mismatch, with spontaneous speech more generally useful for training parsers.  %  Prosodic information in the speech signal has been shown to correlate with syntactic structure of a sentence; however, the impact of prosody on parsing has been mixed. Recent results show a benefit for conversational speech, particularly in utterances with disfluencies, but there is little recent work on other speaking styles. In this work, we extend recent advances in constituency parsing of spontaneous speech, integrating acoustic-prosodic cues and achieving SOTA results on the Switchboard dataset. We then explore the performance of the parser on mismatched training/testing scenarios. Specifically, we show that training on spontaneous speech results in a small degradation when testing on read speech, while fine-tuning with WSJ read speech substantially degrades the performance on spontaneous speech.   : constituency parsing, prosody, spontaneous speech, contextualized embeddings %: constituency parsing, prosody, spontaneous speech, read speech, switchboard, ELMo, BERT, contextualized embeddings %\ttcomment{take out some of these?}                     We show that neural architectures, in particular contextualized embeddings pretrained on large written text , improve constituency parsing on conversational speech transcripts. The use of prosody results in further improvements overall, especially in longer sentences and in reducing attachment errors. Assessing the utility of prosody in different speaking styles, we found that parsers trained with spontaneous prosody are consistently useful, improving over their text-only counterparts when testing on both conversational and read  speech. Fine-tuning such parsers on read speech improves results when testing on the same read style, but degrades significantly on spontaneous speech. This suggests that conversational speech data is more useful for general parser training. 
"," The differences in written text and conversational speech are substantial; previous parsers trained on treebanked text have given very poor results on spontaneous speech. For spoken language, the mismatch in style also extends to prosodic cues, though it is less well understood.  This paper re-examines the use of written text in parsing speech in the context of recent advances in neural language processing. We show that neural approaches facilitate using written text to improve  parsing of spontaneous speech, and that prosody further improves over this state-of-the-art result. Further, we find an asymmetric degradation from read vs.\ spontaneous mismatch, with spontaneous speech more generally useful for training parsers.  %  Prosodic information in the speech signal has been shown to correlate with syntactic structure of a sentence; however, the impact of prosody on parsing has been mixed. Recent results show a benefit for conversational speech, particularly in utterances with disfluencies, but there is little recent work on other speaking styles. In this work, we extend recent advances in constituency parsing of spontaneous speech, integrating acoustic-prosodic cues and achieving SOTA results on the Switchboard dataset. We then explore the performance of the parser on mismatched training/testing scenarios. Specifically, we show that training on spontaneous speech results in a small degradation when testing on read speech, while fine-tuning with WSJ read speech substantially degrades the performance on spontaneous speech.",163
"  There is a growing interest in using formal languages to study fundamental properties of neural architectures, which has led to the extraction of interpretable models .  Recent work has explored the generalized Dyck-n  languages, a subset of context-free languages.  consists of ``well-balanced'' strings of parentheses with  different types of bracket pairs, and it is the canonical formal language to study nested structures.   show that LSTMs  are a variant of the -counter machine and can recognize  languages. The dynamic counting mechanisms, however, are not sufficient for  as it requires emulating a pushdown automata.   shows that for a sufficiently large length, Transformers  will fail to transduce the  language.     We empirically show that with the addition of a starting symbol to the vocabulary,  a two-layer multi-headed SA network  is able to learn  languages, and generalize to longer sequences, although not perfectly. As shown in Figure , the network is able to identify the corresponding closing bracket for an opening bracket, in what resembles a stack-based automaton. For example, the symbol ``]'' in the string ``'', will first pop ``['' from the stack, then it attends to `` enables the model to learn the occurrence of the end of a clause or the end of the sequence, which can be regarded as a mechanism to represent an empty stack.   Our work is the first to perform an empirical exploration of SA on formal languages. We present detailed comparison between an SA which incorporates a starting symbol , and one that does not , and demonstrate significant differences in their generalization across the length of sequences and the depth of dependencies.   Recent work has suggested that the ability of self-attention mechanisms to model hierarchical structures is limited.  show that the performance of Transformers on tasks such as logical inference and ListOps is either poor or worse than LSTMs.  have also reported similar results on SA, concluding that recurrence is necessary to model hierarchical structures. In comparison, our results show that SA outperforms LSTM on  languages except for  on longer sequences.   posit that the ability of neural models to learn hierarchical structures can be attributed to a ``looking back'' capability, rather than directly encoding hierarchies. Our analysis sheds light on the ability of SA to learn hierarchical structures by elegantly attending to the correct preceding symbol.      We provide empirical evidence on the ability of self-attention  networks to learn generalized  languages. We compare the performance of two SA networks, SA and SA, which differ only in the inclusion of a starting symbol in their vocabulary.  We demonstrate that a simple addition of the starting symbol helps SA generalize to sequences that are longer and have higher depths.  The competitive performance of SA  against LSTMs might seem surprising, considering that the recognition of  languages is an inherently hierarchical task. From our experiments, we conclude that recognizing Dyck languages is not tied to recursion, but rather learning the right representations to look up the head token. Further, we find that the representations learned by SA are highly interpretable and the network performs computations similar to a stack automaton. Our results suggest formal languages could be an interesting avenue to explore the interplay between performance and interpretability for SA. Comparisons between SA and LSTM reveal interesting contrast between the two architectures which calls for further investigation. Recent work  shows how to express the Transformer as an RNN through linearization of the attention mechanism, which could lay grounds for more theoretical analysis of these neural architectures          {supp_arxiv}     
","   We focus on the recognition of Dyck-n  languages with self-attention  networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol  and one without . Our results show that SA$^+$ is able to generalize to longer sequences and deeper dependencies. For $\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences whereas the accuracy of SA$^+$ is 58.82$\%$. We find attention maps learned by SA$^+$ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn hierarchies without recursion.",164
"  \vsec Automatic text summarization\footnote{We refer to abstractive summarization in this paper.} is an attractive technique for helping humans to grasp the content of documents effortlessly. While supervised neural methods have shown good performances, the unsupervised approach is starting to attract interest due to its advantage of not requiring costly parallel corpora. However, the empirical performance of unsupervised methods is currently behind that of state-of-the-art supervised models. Unsupervised text summarization is still developing and is now at the stage where various solutions should be actively explored.     One previous unsupervised approach extends neural encoder-decoder modeling to the zero paired data scenario, where a model is trained with a paradigm called compression-reconstruction  learning. The mechanism is similar to that of the back-translation: the model consists of a compressor  and a reconstructor, and they are co-trained so that the reconstructor can recover the original sentence from the summary generated by the compressor~. Experimental results showed that such an unsupervised encoder-decoder-based summarizer is able to learn the mapping from a sentence to a summary without paired data. % Also,  proposes a more straightforward method that mimics the reconstruction part by means of contextual similarity between an original input sentence and a top of a generating summary. % However, the performance of any unsupervised methods is still deficient compared to the latest supervised models.   Reinforcement learning  is also a potential solution for the no paired data situation. In related fields, for example, there are unsupervised methods for text simplification and text compression with policy-gradient learning. Recent RL techniques take a value-based approach  such as DQN or the combination of policy and value-based approaches such as Asynchronous Advantage Actor-Critic. A critical requirement to leverage a value-based method is a value function that represents the goodness of an action on a given state. We can naturally define the value function by utilizing the CR-learning paradigm, and it makes the latest value-based approaches available for unsupervised text summarization. % , and they require to define value-function. % We can leverage the values-based approach  % A crucial requirement for RL is a value function that represents a goodness of action on a given state. % We can satisfy the requirement by leveraging the definition in CR learning paradigm. % One concern is, however, that RL with large action space   generally has difficulty in the training. % In addition, the latest techniques to improve RL are from a value-based approach  such as DQN or the combination of policy-based and value-based approaches such as Asynchronous Advantage Actor-Critic.   In this paper, we propose a new method based on Q-learning and an edit-based summarization~. The edit-based summarization generates a summary by operating an edit action  for each word in the input sentence. Our method implements the editing process with two modules: 1) an {gent that predicts edit actions, and 2) a {model  converter that deterministically decodes a sentence on the basis of action signals, which we call  % As illustrated in the right-hand side of Figure , the agent determines { the fixed-LM so that we obtain desired sentences as the results of compression and reconstruction.  % The primary contribution of this paper is to provide a new option leveraging Q-learning with a language model to the growing field of unsupervised text summarization. % Introducing Q-learning, we open the problem to sophisticated techniques on value-based Reinforcement Learning  algorithms , which is not covered only with policy-based RL algorithms employed so far.\footnote{RL algorithms are classified into value-based  and policy-based . To the best of our knowledge, most of the text summarization methods with RL, both in supervised and unsupervised settings, leverages policy-based RL algorithms . Combining such a previous policy-based and our value-based methods for sentence compression will lead to the applicability of more advanced RL algorithms such as Actor-Critic  and Asynchronous Advantage Actor-Critic .} % Also, proposing an approach to fixedly utilize the pre-trained language model, we benefit from its powerful performance capturing sentence semantics along with mitigating issues generative models inherently hold such as complexity in co-training of multiple generators or repetition in decoding. % Experimentally, our approach shows promising results; it achieves competitive performance in standard datasets and outperforms the previous generator models in out-of-domain circumstances. % This paper brings novel insights for unsupervised text summarization and contributes to be flourishing in the future.  % This paper is organized as follows: Section defines the problem statement of unsupervised text summarization with the \algoname\ paradigm. % After reviewing the previous methods in Section, we introduce our approach in Section . % Then, we report experimental results in Section . % Discussing insights from the experiment in Section , we conclude the contribution of this paper for future unsupervised text summarization in Section .  \vsecu    \vsec We brought the Q-learning framework into unsupervised text summarization and proposed a new method \ealm~that is an edit-based unsupervised summarizer leveraging a Q-learning agent and a language model. The experments showed that \ealm~performed competitively with the previous encoder-decoder-based methods. However, in qualitative analysis, we found that the quality of the generated summaries of any unsupervised model was not sufficient, and there are individual limitations for each model. These issue must be overcome as the step forward to generating practically available summaries without paired data. In particular for \ealm, there is room for improvement by importing the latest techniques in RL. Our work paves the way for further research on bridging Q-learning and unsupervised text summarization.                                   
"," % Unsupervised methods for abstractive text summarization are attractive because they do not require parallel corpora. % However, their performance is still somehow lacking, therefore research on promising solutions is ongoing. % In this paper, we propose a new approach based on Q-learning with an edit-based summarization. % Our method combines two key modules to form an {gent and {odel converter~. % The agent predicts edit actions, and then the LM converter deterministically generates a summary on the basis of the action signals. % Q-learning is leveraged to train the agent to output proper edit actions. % Experimental results show that } Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required.  However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.   In this paper, we propose a new approach based on Q-learning with an edit-based summarization.  The method combines two key modules to form an Editorial Agent and Language Model converter .  The agent predicts edit actions , and then the LM converter deterministically generates a summary on the basis of the action signals.  Q-learning is leveraged to train the agent to produce proper edit actions.  Experimental results show that }",165
" Neural machine translation  systems are data driven models, which highly depend on the training corpus.  NMT models have a tendency towards over-fitting to frequent observations  while neglecting those low-frequency observations.  Unfortunately, there exists a token imbalance phenomenon in natural languages as different tokens appear with different frequencies, which roughly obey the Zipf's Law.  Table shows that there is a serious imbalance between high-frequency tokens and low-frequency tokens.  NMT models rarely have the opportunity to learn and generate those ground-truth low-frequency tokens in the training process. %It is harder for the NMT model to generate ground-truth low-frequency tokens even in the training process.  %Compared to the reference, the NMT model tends to generate more high-frequency tokens and less low-frequency tokens, which hurts the translation quality.  Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary or adding extra components, which bring in extra training complexity and computing expense.  Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model, BPE-based model and word-piece-based model. %For example, the sub-word model adapted byte pair encoding  technique to the task of word segmentation.  These effective work alleviate the token imbalance phenomenon to a certain extent and become the {' is split into two tokens as '{', there still exist obvious token-level imbalance between '{            \fi    [t]     <{<{<{<{*{Vanilla}   & Sen. & Please & be & slower& ~  \\      ~ & Freq. & 3,368 & 56,953 & 133 & ~ \\     ~ & Weight & 1.0 & 1.0 & 1.0 & ~\\     *{BPE}   & Sen. & Please & be & slow& er  \\      ~ & Freq. & 3,368 & 56,953 & 285 & 38,397 \\     ~ & Weight & 1.0 & 1.0 & 1.0 & 1.0\\     *{Ours}   & Sen. & Please & be & slow& er  \\      ~ & Freq. & 3,368 & 56,953 & 285 & 38,397 \\     ~ & Weight & 1.8 & 1.0 & 2.5 & 1.1\\     \hline                 \fi Furthermore, current NMT models generally assign equal training weights to target tokens without considering their frequencies.  It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the training sets. The parameters related to them can not be adequately trained, which will, in turn, make NMT models tend to prioritize output fluency over translation adequacy, and ignore the generation of low-frequency tokens during decoding, which is illustrated in Table. It shows that the vanilla NMT model tends to generate more high-frequency tokens and less low-frequency tokens. %This will, in turn, make the model %tend to generate too many high-frequency tokens and too less low-frequency tokens during decoding. However, low-frequency tokens may carry critical semantic information which may affect translation quality once they are neglected.   %It is very likely for NMT models to ignore the loss produced by rare words so that the patterns learned by the encoder, decoder, or attention modules from them can't be adequately updated. What's more, NMT models tend to prioritize output fluency over translation adequacy and ignore the translation of rare words during generation.  %In our experiments, we observed that vanilla NMT models usually produce more frequent words and less rare words than real references. Therefore, some techniques should be adopted to improve the translation of rare words. %distribution.   %It is obvious that there are always rare tokens no matter what the number of merge operations of BPE is and the problem of token distribution imbalance still exists.  %One of the advantages of this technique is that it reduces the number of rare words by splitting them into more frequent subword tokens , which in fact  %relieve the imbalance of word   %The strength is that NMT models can make use of large amounts of parallel training sentences and learn the knowledge and features embodied in the training data. However, one of the weaknesses is that NMT models have a tendency towards over-fitting to frequent observations , but neglecting those rare cases which are not frequently observed. Unfortunately, there is a natural word distribution imbalance in the corpus. According to the Zipf's Law, the frequency of any word is inversely proportional to its ranking in the frequency table, which indicates that the occurrences of some words are far more than others naturally.     %For word-level NMT models, NMT has its limitation in handling a larger vocabulary because of the training complexity and computing expense.   % %In their work, they first represent each word as a sequence of characters and then iteratively combine the most frequent pair as a new symbol. %which achieved better accuracy for the translation of rare words %, we seek to further alleviate the token imbalance problem based on the above analysis. For this purpose,  To address the above issue, we proposed token-level adaptive training objectives based on target token frequencies.  We aimed that those meaningful but relatively low-frequency tokens could be assigned with larger loss weights during training so that the model will learn more about them. %In our objectives, those relatively low-frequency but valuable tokens will be assigned with larger loss weights during training to encourage the model to learn more about them. To explore suitable adaptive objectives for NMT, we first applied existing adaptive objectives from other tasks to NMT and analyzed their performance. We found that though they could bring modest improvement on the translation of low-frequency tokens, they did much damage to the translation of high-frequency tokens, which led to an obvious degradation on the overall performance. This implies that the objective should ensure the training of high-frequency tokens first. %training of high-frequency tokens should be ensured first. %We should ensure the training of high-frequency tokens and enlarge the weights of low-frequency tokens at the same time. %We firstly tried the focal loss, which was proposed for solving the token imbalance problem in the CV task, and analyzed the performance.  Then, based on our observations, we proposed two heuristic criteria for designing the token-level adaptive objectives based on the target token frequencies. Last, we presented two specific forms for different application scenarios according to the criteria. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation. %We carried out experiments on ZHEN, ENRO, and ENDE translation tasks to validate our methods. The experimental results show that our methods achieve significant improvement in translation quality, especially in sentences that contain more low-frequency tokens.  %Besides, the token distribution of our translations becomes closer to references for test sets.  %Besides, our method also improves the diversity of the translations.   Our contributions can be summarized as follows:      %  %More specifically, NMT models are first trained with equal weights and then fine-tuned with well-defined weights introduced by the scoring functions. In this way, it won't hurt the translation of frequent tokens, but also can improve the translation of rare tokens to a certain degree. To the best of our knowledge, this is the first work trying to concern about the training weights at the token level to solve the distribution imbalance problem in NMT. The experiments on multiple translation tasks show that our method can improve the overall translation performance without almost any additional computing or storage expense. And the analysis experiments indicate that our method can improve the rare tokens translation significantly and the tokens distribution of our translation are much closer to the references than the baseline translations.    In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity.  the vanilla NMT model tends to generate more high-frequency words than the true distribution due to.  there is a token imbalance phenomenon in the natural language and the vanilla NMT model tends to generate more high-frequency words than the true distribution.  and less low-frequency words    This output bias will affect the translation quality since the low-frequency tokens may carry critical semantic information.  To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the observations. Next, we gave two simple but effective forms based on the criteria, which can assign appropriate training weights to target tokens.  we propose token-level adaptive objectives based on token frequencies, aiming to assign appropriate training weights to target tokens. To achieve this, we propose three heuristic criteria and then put forward two simple but effective forms based on the criteria.  The final results show that our methods can achieve significant improvement in performance, especially on sentences that contain more low-frequency tokens. Further analyses show that our method can also improve the lexical diversity.   
"," There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation .  The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. %%% However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected.   In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training.  We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. %More specifically, those relatively low-frequency but valuable target tokens will be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. %%% %We conducted experiments  Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation. %Experiments on multiple translation tasks show that our methods can achieve significant improvement in translation quality, especially on sentences that contain more low-frequency tokens.  %Besides, our method also improves translation diversity. %Besides, the token distribution of our translations becomes closer to the reference of test sets.  %.  %Rare words translation has always been one of the key challenges to Neural Machine Translation .",166
"   Graph structures play a pivotal role in NLP because they are able to capture particularly rich structural information. For example, Figure shows a directed, labeled Abstract Meaning Representation  graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts  neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks  have been explored to better encode structural information for this task .   % \tzy{papers before 2018??? Gated Graph Neural networks??? Do not miss an important paper.}     One type of such GNNs is Graph Convolutional Networks .  GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate  neighbors.  Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions .  However, prior efforts  have shown that the locality property of existing GCNs precludes efficient non-local information propagation.  further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks  have been explored as an alternative to capture global dependencies. As shown in Figure , SANs associate each node with other nodes such that we model interactions between any two nodes in the graph. Still, this approach ignores the structure of the original graph.  and  propose structured SANs that incorporate additional neural components to encode the structural information of the input graph.   Convolutional operations, however, are more computationally efficient than self-attention operations because the computation of attention weights scales quadratically while convolutions scale linearly with respect to the input length . Therefore, it is worthwhile to explore the possibility of models based on graph convolutions. One potential approach that has been considered is to incorporate information from higher order neighbors, which helps to facilitate non-local information aggregation for node classification . However, simple concatenation of different order representations may not be able to model complex interactions in semantics for text generation .    We propose to better integrate high-order information, by introducing a novel dynamic fusion mechanism and propose the Lightweight, Dynamic Graph Convolutional Networks . As shown in Figure  , nodes in the LDGCN model are able to integrate information from first to third-order neighbors. With the help of the dynamic mechanism, LDGCNs can effectively synthesize information from different orders to model complex interactions in the AMR graph for text generation. Also, LDGCNs require no additional computational overhead, in contrast to vanilla GCN models. We further develop two novel weight sharing strategies based on the group graph convolutions and weight tied convolutions. These strategies allow the LDGCN model to reduce memory usage and model complexity.  Experiments on AMR-to-text generation show that LDGCNs outperform best reported GCNs and SANs trained on LDC2015E86 and LDC2017T10 with significantly fewer parameters. On the large-scale semi-supervised setting, our model is also consistently better than others, showing the effectiveness of the model on a large training set. We release our code and pretrained models at \url{https://github.com/yanzhang92/LDGCNs}.\footnote{Our implementation is based on  MXNET  and the Sockeye toolkit .}      In this paper, we propose LDGCNs for AMR-to-text generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation.      
"," 	 	% Camera-Ready 	AMR-to-text generation is used to transduce Abstract Meaning Representation structures  into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks  were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local  information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks  that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.",167
"  [h!]       marginalize over all possible factorizations of the joint distribution within and across all channels .      \modelabbv{} is trained to predict the tokens to be inserted , given partially observed inputs.     At inference, \modelabbv{} can take full, partial, or empty sequence from each channel and generate the full sequence for each channel.}             A natural way to consider two parallel sentences in different languages is that each language expresses the same underlying meaning from a different viewpoint.  Each language can be thought of as a transformation that maps an underlying concept into a view that we collectively agree is determined as `English' or `French'.  Similarly, an image of a cat and the word `cat' are expressing two views of the same underlying concept.  In this case, the image corresponds to a high bandwidth channel and the word `cat' to a low bandwidth channel.  This way of conceptualizing parallel viewpoints naturally leads to the formulation of a fully generative model over each instance, where the transformation corresponds to a particular generation of the underlying view.  We define each of these views as a channel. As a concrete example, given a parallel corpus of English and French sentences, English and French become two channels, and the corresponding generative model becomes .  One key advantage of this formulation is that a single model can be trained to capture the full expressivity of the underlying concept, allowing us to compute conditionals and marginals along with the joint.  In parallel sentences, the conditionals correspond to translations from one channel to another while the marginals correspond to standard monolingual language models.  In this work, we present a general framework for modeling the joint distribution  over  channels by marginalizing over all possible factorizations across the channels and within each channel.  This formulation allows our framework to perform: 1) unconditional generation, 2) fully conditional generation , and 3) partial conditional generation .  The key contributions in this work are:      , a multichannel generative modeling framework. \modelabbv{} models the joint distribution  over  channels by marginalizing over all possible factorization across and within sequences.      is trained over all possible factorizations, \modelabbv{} can perform both conditional generation , and partially observed conditional generation across different channels .      and prior work.   We highlight that while we focus on languages as a specific instantiation of a channel, our framework can generalize to any arbitrary specification, such as other types of tasks  or other modalities .   %%%%%%%%%%%%%%%%%%%%%%%%%%%   In this paper, we presented the Multichannel Generative Language Model .  MGLM is a generative joint distribution model that marginalizes over all possible factorizations within and across channels. MGLM endows flexible inference, including unconditional, conditional, and partially observed generation.  We experimented with those inference modes using the Multi30K dataset containing English, French, Czech, and German. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperform traditional bilingual discriminative models.  Our work focused on a specific instantiation of channels as languages.  However, MGLM is not limited to only languages and can generalize to other notions of channels.  In future work, we will consider other textual channels, such as paraphrases, premises and hypotheses, questions and answers, and multimodal channels, such as images. Another direction can investigate scaling MGLM to dozens/hundreds of channels. Fully generative models still often lag behind purely discriminative counterparts in performance, but we hope our work motivates future research on building generative joint distribution models of the world.                       File emnlp2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}hchan@cs.toronto.edu \usepackage{latexsym} \usepackage{url} \usepackage{times} \usepackage{latexsym} \usepackage{amsmath} \usepackage{amssymb} \usepackage{amsfonts} \usepackage{booktabs} \usepackage{enumitem} \usepackage{graphicx} \usepackage{hyperref} \usepackage{url} \usepackage{tikz} \usepackage{xcolor} \usepackage{pifont} \usepackage{placeins}   \usepackage[english, german, czech, french]{babel} \usepackage[utf8x]{inputenc} \usepackage{todonotes} \usepackage{wrapfig} \usepackage{natbib} \usepackage{subcaption} \usepackage{lipsum}   for dummy text only \usepackage{dblfloatfix}      To enable figures at the bottom of page \usepackage{float}      You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.   } } [1]{\underline{#1}} [1]{{#1}} } {\mathbf{x}} {\mathbf{y}} {\mathbf{w}} {Multichannel Generative Language Model}  {multichannel generative language modeling} {MGLM}   ULM MCM \DeclareMathOperator*{\argmax}{argmax}    Optional math commands from https://github.com/goodfeli/dlbook_notation.       Enter the acl Paper ID here      You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.      \title{Multichannel Generative Language Model: \\ Learning All Possible Factorizations Within and Across Channels}  \author{Harris Chan\thanks{\;Work done during an internship at Google Brain.} \\   Vector Institute \\   University of Toronto \\   kiros@google.com \\\AND   Jamie Kiros \\   Google Research, Brain Team \\   williamchan@google.com \\\And   William Chan \\   Google Research, Brain Team \\   {\tt williamchan@google.com}} \date{}   \maketitle    
"," A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language Model . MGLM is a generative joint distribution model over channels. MGLM marginalizes over all possible factorizations within and across all channels.  MGLM endows flexible inference, including unconditional generation, conditional generation , and partially observed generation .  We experiment with the Multi30K dataset containing English, French, Czech, and German. We demonstrate experiments with unconditional, conditional, and partially conditional generation. We provide qualitative samples sampled unconditionally from the generative joint distribution. We also quantitatively analyze the quality-diversity trade-offs and find MGLM outperforms traditional bilingual discriminative models.",168
" Neural machine translation  has achieved promising results with the use of various optimization tricks.  In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive.  As an alternative mitigation, curriculum learning~ has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training.  CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of ``difficulty'' and the strategy of curricula design. Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length  and word rarity , and manually tune the learning schedule.  However, neither there exists a clear distinction between easy and hard examples, nor these human intuitions exactly conform to effective model training.  Instead, we resolve this problem by introducing self-paced learning, where the emphasis of learning can be dynamically determined by model itself rather than human intuitions. Specifically, our model measures the level of confidence on each training example, where an easy sample is actually the one of high confidence by the current trained model. Then, the confidence score is served as a factor to weight the loss of its corresponding example. In this way, the training process can be dynamically guided by model itself, refraining from human predefined patterns.   We evaluate our proposed method on IWSLT15 EnVi, WMT14 EnDe, as well as WMT17 ZhEn translation tasks. Experimental results reveal that our approach consistently yields better translation quality and faster convergence speed than Transformer baseline and recent models that exploit CL. Quantitative analyses further confirm that the intuitive curriculum schedule for a human does not fully cope with that for model learning.    In this paper, we propose a novel self-paced learning model for NMT in which the learning schedule is determined by model itself rather than being intuitively predefined by humans. Experimental results on three translation tasks verify the universal effectiveness of our approach. Quantitative analyses confirm that exploiting self-paced strategy presents a more flexible way to facilitate the model convergence than its CL counterparts. It is interesting to combine with other techniques to further improve NMT. Besides, as this idea is not limited to machine translation, it is also interesting to validate our model in other NLP tasks, such as low-resource NMT model training and neural architecture search.  
"," Recent studies have proven that the training of neural machine translation  can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the hand-crafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step.  Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.\footnote{Our codes:  {https://github.com/NLP2CT/SPL\_for\_NMT}.}",169
" In recent years, cyberbullying has become one of the most pressing online risks among youth and raised serious concerns in society. Cyberbullying is commonly defined as the electronic transmission of insulting or embarrassing comments, photos or videos, as illustrated in Figure~ . Harmful bullying behavior can include posting rumors, threats, pejorative labels, and sexual remarks. Research from the American Psychological Association and the White House has revealed more than  of young people in the US indicate that they have been bullied on social media platforms~. Such a growing prevalence of cyberbullying on social media has detrimental societal effects, such as victims may experience lower self-esteem, increased suicidal ideation, and a variety of negative emotional responses~. Therefore, it has become critically important to be able to detect and prevent cyberbullying on social media. Research in computer science aimed at identifying, predicting, and ultimately preventing cyberbullying through better understanding the nature and key characteristics of online cyberbullying.     In the literature, existing efforts toward automatically detecting cyberbullying have primarily focused on textual analysis of user comments, including keywords~ and sentiments analysis ~. These studies attempt to build a generic binary classifier by taking high-dimensional text features as the input and make predictions accordingly. Despite their satisfactory detection performance in practice, these models largely overlooked temporal information of cyberbullying behaviors. They also ignore user interactions in social networks. Furthermore, the majority of these methods focus on detecting cyberbullying sessions effectively but cannot explain ``why'' a media session was detected as cyberbullying. Given a sequence of comments with user attributes, we think sequential learning can allow us to better exploit and model the evolution and correlations among individual comments. Besides, graph-based learning can enable us to represent and learn how users interact with each other in a session.   This work aims to detect cyberbullying by jointly exploring explainable information from user comments on social media. To this end, we build an explainable cyberbullying detection framework, \underline{HE}terogeneous \underline{N}eural \underline{I}nteraction \underline{N}etworks , through a coherent process. HENIN consists of three main components that learn various interactions among heterogeneous information displayed in social media sessions. A comment encoder is created to learn the representations of user comments through a hierarchical self-attention neural network so that the semantic and syntactic cues on cyberbullying can be captured. We create a post-comment co-attention mechanism to learn the interactions between a posted text and its comments. Moreover, two graph convolutional networks are leveraged to learn the latent representations depicting how sessions interact with one another in terms of users, and how posts are correlated with each other in terms of words.  Specifically, we address several challenges in this work:  how to perform explainable cyberbullying detection that can boost detection performance,  how to highlight explainable comments without the ground truth,  how to model the correlation between posted text and user comments, and  how to model the interactions between sessions in terms of users, and the interactions between textual posts in terms of words. Our solutions to these challenges result in a novel framework HENIN.   Our contributions are summarized as follows. %   We study a novel problem of explainable cyberbullying detection on social media.  We provide a novel model, HENIN~, which jointly exploits posted text, user comments, and the interactions between sessions and between posts to learn the latent representations for cyberbullying detection.  Experiments conducted on Instagram and Vine datasets exhibit the promising performance of HENIN, and the evidential comments and words highlighted by HENIN, for detecting cyberbullying media sessions with explanations. %     Cyberbullying detection on social media attracts growing attention in recent years. It is also crucial to understand why a media session is detected as cyberbullying. Thus we study the novel problem of explainable cyberbullying detection that aims at improving detection performance and highlighting explainable comments. We propose a novel deep learning-based model, HEterogeneous Neural Interaction Networks , to learn various feature representations from comment encodings, post-comment co-attention, and graph-based interactions between sessions and posts. Experimental results exhibit both promising performance and evidential explanation of HENIN. We also find that the learning of graph-based session-session and post-post interactions contributes most to the performance. Such results can encourage future studies to develop advanced graph neural networks in better representing the interactions between heterogeneous information. In addition, it is worthwhile to further model information propagation and temporal correlation of comments in the future.   
"," In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks , for explainable cyberbullying detection. HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.",170
"  % Recent work in NLP has seen a flurry of interest in the question: are the representations learned by neural networks compositional? That is, are representations of longer phrases built recursively from representations of shorter phrases, as they are in many linguistic theories? If so, how and when do they learn to do this?  For years the LSTM dominated language architectures. It remains a popular architecture in NLP, and unlike Transformer-based models, it can be trained on small corpora~.\footnote{As evidence of the ongoing popularity of LSTMs in NLP, a Google Scholar search restricted to  since 2019 finds 191 citations to the original LSTM paper  and 242 citations to the original Transformer paper .}  even found that the recurrent inductive biases behind the LSTM's success are so essential that distilling from them can improve the performance of fully attentional models. However, the reasons behind the LSTM's effectiveness in language domains remain poorly understood.   A Transformer can encode syntax using attention , and some LSTM variants explicitly encode syntax . So, the success of these models is partly explained by their ability to model syntactic relationships when predicting a word. By contrast, an LSTM simply scans a sentence from left to right, accumulating meaning into a hidden representation one word at a time, and using that representation to summarize the entire preceding sequence when predicting the next word. Yet we have extensive evidence that trained LSTMs are also sensitive to syntax. For example, they can recall more history in natural language data than in similarly Zipfian-distributed -gram data, implying that they exploit linguistic structure in long-distance dependencies . Their internal representations appear to encode constituency  and syntactic agreement . In this paper, we consider how such representations are learned, and what kind of inductive bias supports them.   To understand how LSTMs exploit syntax, we use contextual decomposition , a method that computes how much the hidden representation of an LSTM depends on particular past span of words. We then extend CD to Decompositional Interdependence , a measure of interaction between spans of words to produce the representation at a particular timestep. For example, in the sentence ``Socrates asked the student trick questions閳ユ瑢, we might expect the hidden representation of the LSTM at the word ``questions閳ユ瑢 to interact primarily with its syntactic head ``asked閳ユ瑢, and less with the direct object ``the student''. If so, then an LSTM could be seen as implementing compositional  : if a hidden representation encodes meaning, then this meaning is composed from local syntactic relationships. Our experiments on syntactically-parsed corpora  illustrate this property --- interdependence decreases with syntactic distance, stratified by surface distance.  We then turn to a hypothesis about how such representations are learned. Using a simple synthetic corpus , we allow LSTMs to learn to represent short sequences before they learn longer sequences that are dependent on them. Our goal is to then illustrate how they use representations of short sequences in order to learn longer dependencies---if these smaller constituents are unfamiliar, LSTMs learn more slowly. Further experiments  isolate hierarchical behavior from other factors causing local relations to be learned first, indicating that the model tends to build a subtree from its smaller constituents. We conclude that LSTMs compose hierachically because they learn bottom-up.            \item Compositionality provides an explanation for why LSTMs learn long-range connections slowly and how LSTMs take advantage of linguistic structure.       \item Long-range connections build on predictable short range connections during training.       \item Familiar patterns attract new significance by encouraging interdependence, even at the cost of more general predictors.       \item Syntactically associated words have higher interdependence in English.  Using our proposed tool of Decompositional Interdependence, we illustrate how information exchanged between words aligns roughly with syntactic structure, indicating LSTMs compose meaning bottom-up. Synthetic experiments then illustrate that a memorized span intervening between a long distance dependency promotes early learning of the dependency rule, but fails to generalize to new domains, implying that these memorized spans are used as scaffolding in a bottom-up learning process.   This combination of behaviors is similar to a syntactic language model, suggesting that the LSTM's demonstrated inductive bias towards hierarchical structures is implicitly aligned with our understanding of language and emerges from its natural learning process.  
"," Recent work in NLP shows that LSTM language models capture hierarchical structure in language data. In contrast to existing work, we consider the learning process that leads to their compositional behavior. For a closer look at how an LSTM's sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence  between word meanings in an LSTM, based on their gate interactions. We connect this measure to syntax with experiments on English language data, where DI is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a specific hypothesis about how hierarchical structures are discovered over the course of training: that LSTM constituent representations are learned bottom-up, relying on effective representations of their shorter children, rather than learning the longer-range relations independently from children.",171
"  Systematic reviews are part of the field of evidence-based analysis, and are a methodology for conducting literature surveys, where the focus is on comprehensively summarising and synthesising existing research for the purpose of answering research questions . The aim of this process is to be very broad coverage to avoid unknown bias creeping into results via the alternative of cherry-picking scientific results . %As many relevant documents as possible should be included, and the process should also be thoroughly documented to aid replicability.  Conducting systematic reviews requires trained researchers with domain knowledge. The stages of the process are time-consuming, but vary in how much physical and mental labour they require . As a result, systematic reviews suffer from three primary challenges :    Which techniques are best for identifying and extracting the desired information?  How much labelled training data is needed? Can existing resources be leveraged?  How generalisable is a pipeline to new diseases and countries?  What is the trade-off between pipeline accuracy and human time savings?  How important is model architecture as applied to extraction tasks? How important is embedding pre-training, and how important is pre-training on scientific literature vs. general content ?\\  We find that surprisingly little training data  are necessary to get an accurate document classifier, and that it generalises well to unseen African countries , which enables systematic reviews to be expanded to new areas with essentially constant time. In our text extraction experiments, we find that both sentence and phrase level extraction models can each play a role in such a pipeline,  %given their complementary strengths and weaknesses on this kind of data,  but that phrase extraction, which has not previously been done for this task, performed better than expected both with baseline CNN models  and with BERT-based Transformers , with Transformers based on scientific pre-training  performing best. We demonstrate how the creation of labelled training data can be sped up through annotation tools, and that consideration should be given to the balance of training examples present within this data, since doing so may require less data overall while still maintaining good performance. Furthermore, besides automatic information extraction, much labour in constructing systematic reviews can be saved through simply automating the process of searching and downloading documents.   We empirically demonstrate that most of the three month pipeline of a systematic review can be automated to require very little human intervention, with acceptable accuracy of results. We release our code, annotation schema, and labelled data to assist in the expansion of systematic reviews via automation.  While we demonstrate this system on one domain, the framework is domain independent and could be applied to other kinds of systematic reviews. New training data and annotation schemes would be necessary to switch to medical or other domains, but our findings on time saving processes for annotation  would apply, and confidence thresholds that we implement are adjustable to customise to different levels of accuracy to human time trade-offs that are appropriate to different fields. Our exploration into necessary amounts of training data for accuracy and generalisability are broadly applicable.    We investigated the application of automation to all stages of the systematic review pipeline for our veterinary research case study. We found that with two weeks  of human expert annotation we can automate a systematic review that previously took 3 months, and still maintain high levels of accuracy. Our classification system generalises well, enabling it to be applied to new countries for additional systematic reviews with no additional human annotation cost. Sentence-based and phase-based data extraction both perform well, and the creation of phrase-based training data can still fit within a small amount of human annotation hours and avoids the need for extensive post-processing. Fine-tuned BERT-based Transformers perform best at data extraction, with BERT pre-trained on scientific data giving the largest boost in performance, though a baseline CNN still performs surprisingly well. In future work, we plan to test generalisability cross-lingually, expand the generalisability tests to extraction as well as classification, and study the performance improvements of continuous training of classifiers on human corrections of low-confidence output. 
"," Systematic reviews, which entail the extraction of data from large numbers of scientific documents, are an ideal avenue for the application of machine learning. They are vital to many fields of science and philanthropy, but are very time-consuming and require experts. Yet the three main stages of a systematic review are easily done automatically: searching for documents can be done via APIs and scrapers, selection of relevant documents can be done via binary classification, and extraction of data can be done via sequence-labelling classification. Despite the promise of automation for this field, little research exists that examines the various ways to automate each of these tasks. We construct a pipeline that automates each of these aspects, and experiment with many human-time vs. system quality trade-offs. We test the ability of classifiers to work well on small amounts of data and to generalise to data from countries not represented in the training data. We test different types of data extraction with varying difficulty in annotation, and five different neural architectures to do the extraction. We find that we can get surprising accuracy and generalisability of the whole pipeline system with only 2 weeks of human-expert annotation, which is only 15\% of the time it takes to do the whole review manually and can be repeated and extended to new data with no additional effort.\footnote{Code and links to models available at \url{https://github.com/seraphinatarrant/systematic_reviews}}",172
"   Although recent neural models of language have made advances in learning syntactic behavior, research continues to suggest that inductive bias plays a key role in data efficiency and human-like syntactic generalization . Based on the long-held observation that language exhibits hierarchical structure, previous work has proposed coupling recurrent neural networks  with differentiable stack data structures  to give them some of the computational power of pushdown automata , the class of automata that recognize context-free languages . However, previously proposed differentiable stack data structures only model deterministic stacks, which store only one version of the stack contents at a time, theoretically limiting the power of these stack RNNs to the deterministic~CFLs.  A sentence's syntactic structure often cannot be fully resolved until its conclusion , requiring a human listener to track multiple possibilities while hearing the sentence. Past work in psycholinguistics has suggested that models that keep multiple candidate parses in memory at once can explain human reading times better than models which assume harsher computational constraints. This ability also plays an important role in calculating expectations that facilitate more efficient language processing . Current neural language models do not track multiple parses, if they learn syntax generalizations at all .  We propose a new differentiable stack data structure that explicitly models a nondeterministic PDA, adapting an algorithm by  and reformulating it in terms of tensor operations. The algorithm is able to represent an exponential number of stack configurations at once using cubic time and quadratic space complexity. As with existing stack RNN architectures, we combine this data structure with an RNN controller, and we call the resulting model a \ourmodel{} .  We predict that nondeterminism can help language processing in two ways. First, it will improve trainability, since all possible sequences of stack operations contribute to the objective function, not just the sequence used by the current model. Second, it will improve expressivity, as it is able to model concurrent parses in ways that a deterministic stack cannot. We demonstrate these claims by comparing the \om{} to deterministic stack RNNs on formal language modeling tasks of varying complexity. To show that nondeterminism aids training, we show that the \om{} achieves lower cross-entropy, in fewer parameter updates, on some deterministic CFLs. To show that nondeterminism improves expressivity, we show that the \om{} achieves lower cross-entropy on nondeterministic CFLs, including the ``hardest context-free language"" , a language which is at least as difficult to parse as any other CFL and inherently requires nondeterminism. Our code is available at \url{https://github.com/bdusell/nondeterministic-stack-rnn}.     We presented the \om{}, a neural language model with a differentiable stack that explicitly models nondeterminism. We showed that it offers improved trainability and modeling power over previous stack-based neural language models; the \om{} learns to solve some deterministic tasks more effectively than other stack-LSTMs, and achieves the best results on a challenging nondeterministic context-free language. However, we note that the \om{} struggled on a task where signals in the data were distant, and did not generalize to longer lengths as well as other stack-LSTMs; we hope to address these shortcomings in future work. We believe that the \om{} will prove to be a powerful tool for learning and modeling ambiguous syntax in natural language.  
"," We present a differentiable stack data structure that simultaneously and tractably encodes an exponential number of stack configurations, based on Lang闁炽儲鐛 algorithm for simulating nondeterministic pushdown automata. We call the combination of this data structure with a recurrent neural network  controller a \ourmodel. We compare our model against existing stack RNNs on various formal languages, demonstrating that our model converges more reliably to algorithmic behavior on deterministic tasks, and achieves lower cross-entropy on inherently nondeterministic tasks.",173
"   Cryptography has been used since antiquity to encode important secrets.  There are many unsolved ciphers of historical interest, residing in national libraries, private archives, and recent corpora collection projects .  Solving classical ciphers with automatic methods is a needed step in analyzing these materials.  In this work, we are concerned with automatic algorithms for solving a historically-common type of book code, in which word tokens are systematically replaced with numerical codes. Encoding and decoding are done with reference to a dictionary possessed by both sender and recipient.  While this type of code is common, automatic decipherment algorithms do not yet exist.  The contributions of our work are:        In this work, we show that it is possible to decipher a book-based cipher, using a known-plaintext attack and a neural English language model. We apply our method to letters written to and from US General James Wilkinson, and we recover 75.1\  of the word tokens correctly.    We believe word-based neural language models are a  powerful tool for decrypting classical codes and ciphers.  Because they have much lower perplexities than widely-used n-gram models, they can distinguish between candidate plaintexts that resemble English at a distance, versus candidate plaintexts that are grammatical, sensible, and relevant to the historical context.  
"," We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.  We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress.  We are able to decipher 75.1\% of the cipher-word tokens correctly.",174
"   Neural network language models , pretrained on vast amounts of raw text, have become  the dominant input to downstream tasks . Commonly, these tasks involve aspects of language  comprehension . One explicit example is coreference resolution, wherein anaphora  are linked to antecedents  requiring knowledge of syntax, semantics,  and world-knowledge to match human-like comprehension.   Recent work has suggested that LMs acquire abstract, often human-like, knowledge of syntax  . Additionally, knowledge of grammatical and referential aspects linking a pronoun to its antecedent noun   have been demonstrated for both  transformer and long short-term memory architectures . Humans are able  to modulate both referential and syntactic comprehension  given abstract linguistic knowledge . Contrary to humans, we find that discourse structure  only influences LM behavior  for reference, not syntax, despite model representations that encode the necessary discourse information.  The particular discourse structure we examined is governed by implicit causality  verbs . Such verbs influence pronoun comprehension:   and Mary, so  both are possible antecedents. However, English speakers overwhelmingly  interpret she as referring to Sally in  and Mary  in , despite the semantic overlap between the verbs. Verbs that  have a subject preference  are called subject-biased IC verbs, and verbs with a object preference  are called object-biased IC verbs.   In addition to pronoun resolution, IC verbs also interact with relative clause  attachment:    in  and  and  continuations modifying the children in  and . We might expect  human continuation preferences to be the same in  and . However, the use  of an object-biased IC verb  in  increases the proportion of continuations given by human participants  that refer to the children . Without  an object-biased IC verb the majority of continuations refer to the more recent noun  .  Effects  of IC have received renewed interest in the field of psycholinguistics in recent years . Current accounts of IC claim that the phenomenon is inherently a linguistic process, which  does not rely on additional pragmatic inferences by comprehenders . Thus, IC is argued to be contained within the linguistic signal, analogous to  evidence of syntactic agreement and verb argument structure within corpora. We  hypothesize that if these claims are correct, then current LMs will be able to  condition reference and syntactic attachment by  IC verbs with just language data .   We tested this hypothesis using unidirectional transformer and long short-term memory network  language models. We find that LSTM  LMs fail to acquire a subject/object-biased IC distinction that influences reference or RC attachment.   In contrast, transformers learned a representational  distinction between subject-biased and object-biased IC verbs that interacts  with both reference and RC attachment,  but the distinction only influenced model output for reference. The apparent failure of model  syntactic behavior to exhibit an IC  contrast that is present in model representations raises questions  about the broader capacity of LMs to display  human-like linguistic knowledge.     The present study examined the extent to which  discourse structure, determined by implicit causality verbs,  could be acquired by  transformer and LSTM language models . Specifically, we evaluated, via comparison to human experiments, whether IC verb biases could influence reference and syntactic attachment in LMs. Analyses were conducted at two levels of granularity: model  behavior  and model representation . Given  the claims in recent literature that implicit causality arises without extra pragmatic inference on the part  of human comprehenders, we hypothesized that LMs  would be able to acquire such contrasts .   We found that LSTM LMs were unable to demonstrate  knowledge of IC either in influencing reference or syntax.  However, a  transformer  trained on the exact  same data as the LSTM LMs was able to partially represent an IC distinction, but model output was only influenced by IC bias when resolving reference, not syntactic attachment. In evaluating a transformer model trained on  vastly more data , we found a more robust, human-like sensitivity to IC bias when  resolving reference: subject-biased IC verbs  increased model preference for subject pronouns and object-biased IC verbs increased model preferences for object pronouns. However, the same mismatch as TransformerXL between model representation and model behavior  arose in processing syntactic attachment.   In contrast to our results,   showed syntactic predictions for LSTM LMs are influenced by some aspects of discourse structure. A simple explanation for these conflicting results may  be that the LMs we examined here are unable to learn the syntactic operation of attachment, and thus no influence of discourse can  surface. The erasure  of number agreement in the final layers of the transformer LMs  provides  compelling evidence towards this conclusion.\footnote{Further cross-linguistic evidence bearing on the inability of LSTM LMs, specifically, to learn relative clause attachment is given in .}   From a theoretical perspective, the present study provides additional support for the centering of  implicit causality within the linguistic signal proper.  That is, IC bias is learnable, to some degree,  without pragmatic inference as hypothesized in Section  .  The mismatches  in syntactic representations and behavior suggest, however, that models ignore the abstract categories that are learned,  contrary to human findings .   We believe a solution may lie in changing model  training objectives .  Psycholinguistic studies focusing on the interaction  of discourse and syntax have suggested that  coherence relations may be the unit of linguistic  prediction, in contrast to the next-word prediction  used in most language modeling work . We leave to  future work an  investigation of this suggestion as well as  teasing apart the exact role that training data and  model architecture play in the interaction  between types of linguistic representation.     Thank you to members of the C.Psyd lab at Cornell, who gave feedback on an earlier form of this work. We would also like to thank the three anonymous reviewers for their comments and suggestions.                                 
","  Language models  trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference  and syntactic processing on the same discourse structure . We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.",175
" Word ordering often determines the meaning of a sentence; therefore how to utilize the position information of a word sequence has been an important topic in NLP and widely investigated recently. A common approach for modeling word ordering is to use recurrent neural networks , such as long short-term memory   or gated  recurrent unit  , which use a hidden state to represent the information of an ordered sequence and update model weights by backpropagation through time  ; thus the ordering information can be modeled by this structure.  However, RNN and BPTT are very inefficient in modern GPU computation due to the difficulty of parallelization with the time dependency. To solve this problem, recent work, such as convolutional seq2seq  and Transformers  which apply convolutional neural network   and self-attention respectively, succeed to eliminate the time dependency to take the computational advantage of GPU.  Instead of storing the information of ordered sequences, these models utilize the position information by using a feature-level positional encoding. For example, convolutional seq2seq proposed learnable position embeddings to represent the positions in a sequence.  Recently, various pre-trained Transformer language models keep breaking state-of-the-art results in numerous NLP tasks.  There are many different ways to pre-train a Transformer language model. For example, using an encoder, decoder, or the whole part of the Transformer, adapting the self-attention masks, or training with different objectives .  However, in terms of positional encoding, most work only used a learned position embedding which is originally proposed in convolutional seq2seq  without any analysis, even different objectives may learn completely different position information.  Motivated by the above observations, our goal is to investigate what position information the pre-trained Transformers could learn under different settings. We conduct a deep analysis of the learned position embeddings among three iconic pre-trained Transformer language models: BERT , RoBERTa  and GPT-2 . To examine the performance of different NLP types, we conduct the experiments on text classification, language modeling, and machine translation, and empirically analyze and explain the meaning and influence of position embeddings from different aspects.  The contributions of this paper are 3-fold:      This paper investigates the implicit meaning of pre-trained Transformer position embeddings. Transformer encoders learn the local position information that can only be effective in masked language modeling. On the other hand, the Transformer decoders for autoregressive language modeling actually learn about absolute positions.  The empirical experiments on the pre-trained position embeddings validate our hypothesis. We also show that different NLP tasks with different model architectures and different training objectives may utilize the position information in different ways. As a result, it is believed that this study will benefit future work about choosing suitable positional encoding functions or designing other modeling methods for position information in the target NLP tasks based on their properties.  
"," In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks.  Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention.  Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will.  Therefore, this paper carries out an empirical study on position embeddings of mainstream pre-trained Transformers, which mainly focuses on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks?  This paper focuses on providing a new insight of pre-trained position embeddings through feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future work to choose the suitable positional encoding function for specific tasks given the application property.\footnote{The source code is available at: \url{https://github.com/MiuLab/PE-Study}} %to make our study more convincing.",176
"  is the task of learning the grammar of a target corpus without exposure to the parsing ground truth or any expert-labeled tree structures . Recently emerging  models provide a new approach to this problem . They learn syntactic parsing under only indirect supervision from their main training tasks such as language modelling and natural language inference.  In this study, we analyze ON-LSTM , a new latent tree learning model that set the state of the art on unsupervised constituency parsing on WSJ test  when it was published at ICLR 2019. The model is trained on language modelling and can generate binary constituency parsing trees of input sentences like the one in Figure .     As far as we know, though there is an excellent theoretical analysis paper  of the ON-LSTM model that focuses on the model's architecture and its parsing algorithm, there is no systematic analysis of the parses the model generates. There are no in-depth investigations of  whether the model's parsing behavior is consistent among different restarts or  how the parses it produces are different from PTB gold standards. Answering these questions is crucial for a better understanding of the capability of the model and may bring insights into how to build more advanced latent tree learning models in the future.  Therefore, we replicate the model with 5 random restarts and look into the parses it generates. We find that  ON-LSTM has fairly consistent parsing behaviors across different restarts, achieving a self F1 of 65.7 on WSJ test.  The model struggles to correctly parse the internal structures of complex noun phrases.  The model has a consistent tendency to overestimate the height of the split points right before verbs or auxiliary verbs, leading to a major difference between its parses and the Penn Treebank gold-standard parses. We speculate that both problems can be explained by the training task, unidirectional language modelling, and thus we hypothesize that training a bidirectional model on a more syntax-related task like acceptability judgement might be a good choice for future latent tree learning models.     In summary, the model shows basic self-consistency on the task of constituency parsing, and it is consistently able to correctly identify certain constituents . All these results show that the unique design of the model brings us closer to developing consistently powerful unsupervised parsing models. However, the experiments show that it  struggles with the internal structures of complex NPs, and  often overestimates the height of the split points right before verbs. Based on our analysis, we hypothesize that both of the failures can be at least partially attributed to the use of unidirectional language modelling as the training task.   There are two potential problems with this training task. First, the motivation of language modelling generally does not perfectly match the target task constituency parsing, since cross-constituent hints are sometimes helpful, as revealed by . Second, it is very hard for a unidirectional model to correctly identify some high-level constituents, as revealed by . Therefore, we believe a promising research direction is to build latent tree learning models based on bidirectional model architectures like transformer  and the task of acceptability judgement with a dataset like CoLA , which is a more syntax-related sentence-level task that requires the model to predict whether an input sentence is grammatically acceptable. Another option to consider is masked language modelling because it is also a bidirectional task and is much easier to scale up compared to acceptability judgement since it is a self-supervised task.    
"," Recent  models can learn constituency parsing without any exposure to human-annotated tree structures. One such model is ON-LSTM , which is trained on language modelling and has near-state-of-the-art performance on unsupervised parsing. In order to better understand the  performance and consistency of the model as well as how the parses it generates are different from gold-standard PTB parses, we replicate the model with different restarts and examine their parses. We find that  the model has reasonably consistent parsing behaviors across different restarts,  the model struggles with the internal structures of complex noun phrases,  the model has a tendency to overestimate the height of the split points right before verbs. We speculate that both problems could potentially be solved by adopting a different training task other than unidirectional language modelling.",177
"  Commonsense reasoning is an important yet challenging task in artificial intelligence and natural language processing. Take commonsense question answering as an example, given a question and multiple choices, some commonsense knowledge is usually required to make the correct answer from the provided choices. Table show some typical commonsense question answering examples extracted from the dataset of commonsenseQA.        {l p{5cm}} \toprule Question: & Where is a good idea but not required to have a fire extinguisher?\\ Choices: & ~school bus ~boat ~house ~hospital ~school\\ % Answer: & ~house\\ \midrule Question: & Where can you put a picture frame when it's not hung vertically?\\ Choices: & ~art show ~wall ~newspaper ~car ~table\\ % Answer: & ~wall\\    \vskip -0.25in   Existing commonsense reasoning methods mainly utilize raw texts to conduct the data representation and answer prediction process. However, the background knowledge required in the commonsense reasoning task, such as spatial relations, causes and effects, scientific facts and social conventions, are usually not explicitly provided by the text. Therefore, it is difficult to capture such knowledge solely from the raw texts. Some other works propose to leverage knowledge bases to extract related commonsense knowledge. However, the construction of a knowledge base is expensive, and the contained knowledge is too limited to fulfill the requirement. Furthermore, most commonsense question answering datasets, such as CommonsenseQA, are constructed from an existing knowledge base, e.g., ConceptNet . So it is unfair to use the knowledge base in these tasks. To sum up, how to automatically learn commonsense remains a challenging problem in NLP.  Motivated by the fact that images usually contain richer scene information, which can be viewed as an important supplementary resource to perceive for commonsense knowledge, this paper proposes to learn commonsense from images and incorporate such knowledge into the commonsense reasoning process. Take the question `' shown in Table as an example. Solving this problem requires a strong background knowledge that fire extinguishers are usually equipped in public places, such as hospitals, schools, and school buses. We can see that such background knowledge is not explicitly provided by the raw texts, and meanwhile, too abstract and complex to be extracted by the current language model techniques. In this case, images will help. For example, we could find many images where fire extinguishers appear in these scenes of public places. Therefore, this commonsense knowledge could be learned by perceiving the scene information of these images, and the corresponding question will be well answered. These analyses are in accordance with Minsky's statement in , `perhaps a good architecture theory based on multiple representations and multi-modal reasoning would help us to design better systems that allow us to study and understand commonsense reasoning.'   Our approach, named Loire , consists of two stages, i.e.~visual commonsense learning and knowledge-augmented reasoning. In the first stage, a scene layout generation task is conducted on a bi-modal data such as the representative benchmark COCO. Firstly, a text encoder Visual BERT  is employed to obtain the representation of a caption. ViBERT is then incorporated into the recurrent encoder-decoder structure for the labeled bounding box generation. This module is trained separately by a supervised learning approach, based on the ground-truth bounding boxes of images. In this way, the required visual commonsense knowledge will be encoded in ViBERT. In the following commonsense reasoning stage, the concerned text representations  will be obtained by concatenating ViBERT and a traditional pre-trained language model, e.g. ~BERT. Then the language model is fine-tuned on the commonsense reasoning data, with ViBERT fixed as some prior knowledge. Experimental results on two commonsense reasoning tasks, i.e.~CommonsenseQA and WinoGrande , demonstrate that the learnt commonsense from images brings improvements to traditional models, such as BERT fine-tune  and RoBERTa fine-tune . We also give some case studies to show how the learned visual commonsense knowledge helps the reasoning process.   To the best of our knowledge, we are the first to propose learning commonsense knowledge from images to facilitate the commonsense reasoning in NLP. The proposed model of using scene layout generation as the supervision demonstrates a preliminary exploration in this direction. Other methods like learning commonsense from retrieved relevant images could also be investigated. We believe this novel approach may provide a new perspective for commonsense reasoning in NLP.     In this paper, we propose a novel two-stage pipeline approach Loire to learn commonsense from images. In the first stage, a text representation model ViBERT is trained in the bi-modal sequence-to-sequence approach for scene layout generation on COCO. Therefore, visual commonsense knowledge like spatial relations will be encoded in ViBERT by the supervision of caption and image layout. After that, ViBERT is concatenated with a pre-trained language model to perform a knowledge-augmented reasoning process. Experimental results show that Loire outperforms the current state-of-the-art language models BERT and RoBERTa on two NLP commonsense reasoning tasks, i.e.~commonsense question answering data CommonsenseQA and pronoun resolution data WinoGrande. The ablation and case study further show that the improvements are truly owing to the learned visual commonsense knowledge, and how this knowledge helps the NLP reasoning process.  The current approach is a preliminary study on the proposed direction of using images to automatically learn commonsense knowledge to facilitate the NLP reasoning tasks, which could be modified from the following aspects to further improve the empirical performances. Firstly, larger bi-modal data could be employed to learn more commonsense required in the reasoning task. Secondly, other bi-modal methods instead of training ViBERT by the supervision of scene layout generation may be investigated. Thirdly, how to design intrinsic evaluation to help to understand what is learned by Lorie is still challenging and will be considered in the future.   
"," This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thousand words, where richer scene information could be leveraged to help distill the commonsense knowledge, which is often hidden in languages. Our approach, namely Loire, consists of two stages. In the first stage, a bi-modal sequence-to-sequence approach is utilized to conduct the scene layout generation task, based on a text representation model ViBERT. In this way, the required visual scene knowledge, such as spatial relations, will be encoded in ViBERT by the supervised learning process with some bi-modal data like COCO. Then ViBERT is concatenated with a pre-trained language model to perform the downstream commonsense reasoning tasks. Experimental results on two commonsense reasoning problems, i.e.~commonsense question answering and pronoun resolution, demonstrate that Loire outperforms traditional language-based methods. We also give some case studies to show what knowledge is learned from images and explain how the generated scene layout helps the commonsense reasoning process. \let\thefootnote\relax\footnotetext{*Corresponding Author}",178
"  Our SJTU-NICT team participated in the WMT20 shared task, including supervised track, unsupervised, and low-resource track. During the participation, we placed our attention on Polish   English  and English   Chinese  on the supervised track, while on the unsupervised and low-resource track, the German   Upper Sorbian  both directions are focused.  Our  baseline system in supervised track is based on the Transformer big architecture proposed by , in which its open-source implementation version Fairseq  is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework , and used the two-stage training mode of masked language modeling  pre-training + back-translation  finetune to obtain a very strong baseline performance. Marian  toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets.  In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team , we divided the three language pairs we participated in into three categories:   %  toolkit to performs reranking to get the final system output.     This paper describes SJTU-NICT's submission to the WMT20 news translation task. For three typical scenarios, we adopt different strategies. In this work, we not only study the pre-trained language model to enhance MT, but also consider the impact of document information on translation. We considered both the way of converting document alignment into sentence alignment and the use of BERT's NSP to recover the structure of documents. In addition, transfer learning from supervision is taken into account in unsupervised translation, and various means are used to enhance low-resource translation. Our systems performed strongly among all the constrained submissions: we ranked 1st in PLEN, ENZH, and DEHSB respectively, and stayed Top-3 for the HSBDE.  
","  In this paper, we introduced our joint team SJTU-NICT 's participation in the WMT 2020 machine translation shared task. In this shared task, we participated in four translation directions of three language pairs: English-Chinese, English-Polish on supervised machine translation track, German-Upper Sorbian on low-resource and unsupervised machine translation tracks. Based on different conditions of language pairs, we have experimented with diverse neural machine translation  techniques: document-enhanced NMT, XLM pre-trained language model enhanced NMT, bidirectional translation as a pre-training, reference language based UNMT,  data-dependent gaussian prior objective, and BT-BLEU collaborative filtering self-training. We also used the TF-IDF algorithm to filter the training set to obtain a domain more similar set with the test set for finetuning. In our submissions, the primary systems won the first place on English to Chinese, Polish to English, and German to Upper Sorbian translation directions.",179
"     Neural summarizers have achieved impressive performance when evaluated by ROUGE ~ on in-domain setting, and the recent success of pre-trained models drives the state-of-the-art results on benchmarks to a new level ~. However, the superior performance is not a guarantee of a perfect system since exsiting models tend to show defects when evaluated from other aspects. For example,  observes that many abstractive systems tend to be near-extractive in practice.  reveal that most generated summaries are factually incorrect. These non-mainstream evaluation methods make it easier to identify the model's weaknesses.  Orthogonal to above two evaluation aspects, we aim to diagnose the limitation of existing systems under , in which a summarization system trained on  one corpus would be evaluated on a range of out-of-dataset corpora. Instead of evaluating the quality of summarizers solely based on one dataset or multiple datasets individually, cross-dataset evaluation enables us to evaluate model performance from a  different angle. For example, Fig. shows the ranking of  summarization systems studied in this paper under different  evaluation metrics, in which the ranking list `` in-dataset R2'' is obtained by traditional ranking criteria while other two are based on our designed cross-dataset measures. Intuitively, we observe that 1) there are different definitions of a ``good'' system in various evaluation aspects; 2) abstractive and extractive systems exhibit diverse behaviors when evaluated under the cross-dataset setting.    The above example recaps the general motivation of this work, encouraging us to rethink the generalization ability of current top-scoring summarization systems from the perspective of cross-dataset evaluation. Specifically, we ask two questions as follows:   Q1: {How do different neural architectures of summarizers influence the cross-dataset generalization performances?} When designing summarization systems, a plethora of neural components can be adopted ~. For example, will   and    mechanisms improve the cross-dataset generalization ability of summarizers? Is there a risk that  summarizers will perform worse when adapted to new areas compared with the ones without BERT? So far, the generalization ability of current summarization systems when transferring to new datasets still remains unclear, which poses a significant challenge to design a reliable system in realistic scenarios. Thus, in this work, we take a closer look at the effect of model architectures on cross-dataset generalization setting.    Q2: {Do different generation ways  of summarizers influence the cross-dataset generalization ability?} Extractive and abstractive models, as two typical ways to summarize texts, usually follow diverse learning frameworks and favor different datasets.  It would be absorbing to know their discrepancy from the perspective of cross-dataset generalization.      To answer the questions above, we have conducted a comprehensive experimental analysis, which involves eleven summarization systems , five benchmark datasets from different domains, and two evaluation aspects. Tab. illustrates the overall analysis framework. We explore the effect of different architectures and generation ways on model generalization ability in order to answer  and . Semantic equivalency  and factuality are adopted to characterize the different aspects of cross-dataset generalization ability. Additionally, we strengthen our analysis by presenting two views of evaluation:  and  views .   [t]   {2}   \resizebox{0.47\textwidth}{12mm}{     {lcc}     \toprule     Framework & \makecell{Semantic equivalency \\ } & \makecell{Factuality \\ } \\     \midrule     \makecell[l]{Q1: Architecture \\ } & Sec.  & Sec.\\     \makecell[l]{Q2: Generation way \\ } & Sec.  & Sec.\\     }%        % %  Our contributions can be summarized as: 1) Cross-dataset evaluation is orthogonal to other evaluation aspects , which can be used to re-evaluate current summarization systems, accelerating the creation of more robust summarization systems. 2) We have design two measures Stiffness and Stableness, which could help us to characterize generalization ability in different views, encouraging us to diagnose the weaknesses of state-of-the-art systems.  3) We conduct dataset bias-aided analysis  and suggest that a better understanding of datasets will be helpful for us to interpret systems'  behaviours.       By performing a comprehensive evaluation on eleven summarization systems and five mainstream datasets, we summarize our observations below:  1) Abstractive summarizers are extremely brittle compared with extractive approaches, and the maximum gap between them reaches 37 in terms of the measure stableness  defined in this paper.  2) BART  is superior over other abstractive models and even comparable with extractive models in terms of stiffness . On the other hand, it is robust when transferring between datasets as it possesses high stableness . 3) BERT  performs excellently in terms of stiffness, while still lacks stableness when transferred to Bigpatent B from other datasets.   4) The robustness of models can be improved through either equipped the model with ability to copy span from source document  or make use of well trained sequence to sequence pre-trained model . 5) Simply adding BERT on encoder could improve the stiffness  of model but will cause larger cross-dataset and in-dataset performance gap, a better way should be found to merge BERT into abstractive model, or a better training strategy should be applied to offset the negative influence it brings. 6) Existing factuality checker  is limited in predictive power of positive samples  . 7) Out-of-domain systems can even surpass in-domain systems in terms of factuality.      
"," Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an  setting, where summarizers are trained and evaluated on the same dataset. We argue that this approach can narrow our understanding of the generalization ability for different summarization systems. In this paper, we perform an in-depth analysis of characteristics of different datasets and investigate the performance of different summarization models under a cross-dataset setting, in which a summarizer trained on one corpus will be evaluated on a range of out-of-domain corpora. A comprehensive study of 11 representative summarization systems on 5 datasets from different domains reveals the effect of model architectures and generation ways  on model generalization ability. Further, experimental results shed light on the limitations of existing summarizers. Brief introduction and supplementary code can be found in \url{https://github.com/zide05/CDEvalSumm}.",180
"  As neural machine translation  significantly improved sentence-level translation qualities, recent studies have been focused on document-level translation.  In particular, discourse in document-level translation is one of the central research interests, such as addressing coreference and anaphora resolution; and preserving cohesion and coherence in translation; {{ipxm}閺呭倽鈻''  into ``clock'' and ``watch,'' while the cohesive translations consistently translate the word into ``watch.''   Previous studies approached discourse phenomena in NMT using a context-aware NMT model, which inputs previous source sentences and their translations as contexts.  However,~ showed that lexical cohesion is hard to solve with only context-aware models.  We conjecture this is because context-aware models handle previous translations as a whole and are not sensitive enough to word usage consistency.   In this study, we employ a copy mechanism  on the context-aware NMT model for document-level translation to explicitly address the lexical cohesion problem.  Our model computes a probability of copying a target word from previous translation outputs and boosts its output probability in the translation of a current sentence.    We conduct experiments on Japanese to English document translation. % using the evaluation dataset designed for discourse phenomena.  The results indicate that our model achieves significantly better lexical cohesion, comparing to previous context-aware NMT models.  [t]  \def\arraystretch{1.2}%  1 is the default, change whatever you need     {l|l}     {*}{Source} & {UTF8}{ipxm}閻㈤鑵戦妵鏇樺遍妴浣靛涢妵鍒搖nderline{閺呭倽鈻搣閵堟帇浜伴幐浣典画閵囇佷粴閵囶厹淇揺nd{CJK}\\     & {UTF8}{ipxm}閵囧倶鍊為妵灞讳缓閵囧棎浣典壕閵囩敍underline{閺呭倽鈻搣閵囶垳顨涢悥韬蹭紕瑜般垼顩伴妵顏傚遍妵褋浠氶妴淇揺nd{CJK} \\     [2]{*}{Incohesive translations}& You have a good \underline{clock}, Mr. Tanaka.\\     &Thank you, this \underline{watch} is a memento of my grandfather.\\      [2]{*}{Cohesive translations} & You have a good \underline{watch}, Mr. Tanaka. is a memento of my grandfather.                     We employed a copy mechanism to address the lexical cohesion problem in document-level NMT.  Our model computes a copy probability and weights of words to copy referring to preceding source sentences and their translation outputs.  Experiments on Japanese to English translation indicated that our model is effective to improve lexical cohesion, compared to strong context-aware NMT models.  As future work, we intend to evaluate the effectiveness of our model on various language pairs and domains, such as English-French and English-Russian; news and novels.  Also, we will improve the weighting method to copy words to avoid copying inappropriate words.     
"," Lexically cohesive translations preserve consistency in word choices in document-level translation.  We employ a copy mechanism into a context-aware neural machine translation model to allow copying words from previous translation outputs.  Different from previous context-aware neural machine translation models that handle all the discourse phenomena implicitly, our model explicitly addresses the lexical cohesion problem by boosting the probabilities to output words consistently.   We conduct experiments on Japanese to English translation using an evaluation dataset for discourse translation.  The results showed that the proposed model significantly improved lexical cohesion compared to previous context-aware models.",181
"  % ============== version 5.0 ================= Intent detection, a fundamental component of task-oriented dialogue system , is increasingly raising attention as a Multi-Label Classification  problem , since a single utterance often carries multiple user intents .  In real-world scenarios, intent detection often suffers from lack of training data, because dialogue tasks/domains change rapidly and new domains usually contain only a few data examples.  Recent success of Few-Shot Learning  presents a promising solution for such data scarcity challenges.  It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience. % from old domains.   %For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy to convert the multi-class classification into binary-class classifications .  State-of-the-art works for multi-label intent detection focus on threshold-based strategy, where a common practice is estimating label-instance relevance scores and picking the intent labels with score higher than a threshold value .  Usually, the coordination and respective quality of the two modules, i.e. thresholding and relevance scoring, are crucial to the performance of MLC models.  However, in few-shot scenarios, such multi-label setting poses unique challenges for both threshold estimation and label-instance relevance scoring.  For thresholding, previous works explore to tune a fixed threshold  or to learn thresholds from data . But, these thresholds work well only when learning examples are sufficient.  In few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %In few-shot scenarios, it is pretty hard to determine appropriate thresholds  %with only a few examples. %without overfitting to the limited examples. % to the limited examples. %For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. Besides, it is also difficult to directly transfer the pre-learned thresholds due to the domain differences, such as differences in label number per instance, score density and scale.    Estimation of the label-instance relevance scores is also challenging. %It is also challenging to compute the label-instance relevance scores.  Few-shot learning has achieved impressive progress with similarity-based methods  , where the relevance scores can be modeled as label-instance similarities.  And the label representations can be obtained from corresponding support examples.  Unfortunately, despite huge success in previous single-label tasks, these similarity-based methods become impractical for multi-label problems.  When instances have multiple labels, representations of different labels may be obtained from the same support examples and become confused with each other. For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation,  %Such confused label representations  which makes it impossible to predict correct labels with similarity scores.  %In such situations, vanilla similarities will assign query x equal score to query\_time and query\_loc  In this paper, we study the few-shot learning problem of multi-label intent detection and propose a novel framework to tackle the challenges from both thresholding and label-instance relevance scoring.  To solve the thresholding difficulties of prior-knowledge transferring and domain adaption with limited examples, we propose a Meta Calibrated Threshold  mechanism that first learns universal thresholding experience on data-rich domains, then adapts the thresholds to certain few-shot domains with a Kernel Regression based calibration.  Such combination of universal training and domain-specific calibration allows to estimate threshold using both prior domain experience and new domain knowledge.  %Here, as a non-parametric learning method, Kernel Regression allows to alleviate overfitting by calibrating the thresholds without finetuning.  To tackle the challenge of confused label representation in relevance scoring, we propose the Anchored Label Representation  to obtain well-separated label representations. Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represents each label with both support examples and corresponding anchors.  Different from the previous single-label intent detection that uses label embedding as additional features , our label embeddings here have unique effects of separating different labels in metric space.  Finally, to encourage better coordination between thresholding and label-instance relevance scoring, we introduce the Logit-adapting mechanism to MCT that automatically adapts thresholds to different score densities.   Experiments on two datasets show that our methods significantly outperform strong baselines.  Our contributions are summarized as follows:   We explore the few-shot multi-label problem in intent detection of task-oriented dialogue, which is also an early attempt for the few-shot multi-label classification.   We propose a Meta Calibrated Threshold mechanism with Kernel Regression and Logits Adapting that estimates threshold using both prior domain experience and new domain knowledge.  We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance scoring.    %% ============== version 4.0 ================= %Intent detection, a fundamental component of task-oriented dialogue system , is increasingly raising attention as a Multi-Label Classification  problem , since a single utterance often carries multiple user intents .  %In real-world scenarios, intent detection often suffers from lack of training data, because dialogue tasks/domains change rapidly and new domains usually contain only a few data examples.  %Recent success of Few-Shot Learning  presents a promising solution for such data scarcity challenges.  %It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience. %% from old domains.  % %%For multi-label intent detection, state-of-the-art works adopt ``one-vs-rest'' strategy to convert the multi-class classification into binary-class classifications .  %State-of-the-art works for multi-label intent detection focus on threshold-based strategy, where a common practice is estimating label-instance relevance scores and picking the intent labels with score higher than a threshold value .  %Usually, the coordination and respective quality of the two modules, i.e. thresholding and relevance scoring, are crucial to the performance of MLC models.  %However, in few-shot scenarios, such multi-label setting poses unique challenges for both threshold estimation and label-instance relevance scoring. % %For thresholding, previous works explore to tune a fixed threshold  or to learn thresholds from data . %But, these thresholds work well only when learning examples are sufficient.  %In few-shot scenarios, it is pretty hard to determine appropriate thresholds without overfitting. %% to the limited examples. %%For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %Besides, it is also difficult to directly transfer the pre-learned thresholds due to the domain differences, such as differences in label number per instance, score density and scale. % % % %It is also challenging to compute the label-instance relevance scores.  %Few-shot learning has achieved impressive progress with similarity-based methods  , where the relevance scores can be modeled as label-instance similarities.  %And the label representations can be obtained from corresponding support examples.  %Unfortunately, despite huge success in previous single-label tasks, these similarity-based methods become impractical for multi-label problems.  %When instances have multiple labels, representations of different labels may be obtained from the same support examples and become confused with each other. %For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation,  %%Such confused label representations  %which makes it impossible to predict correct labels with similarity scores.  %%In such situations, vanilla similarities will assign query x equal score to query\_time and query\_loc % %In this paper, we study the few-shot learning problem of multi-label intent detection and propose a novel framework to tackle the challenges from both thresholding and label-instance relevance scoring. % %To solve the thresholding difficulties of prior-knowledge transferring and overfitting, we propose a Meta Calibrated Threshold  mechanism that first learns universal thresholding experience on data-rich domains, then adapts the thresholds to certain few-shot domains with a Kernel Regression based calibration.  %Here, as a non-parametric learning method, Kernel Regression allows to avoid overfitting by calibrating the thresholds without finetuning. % %To tackle the challenge of confused label representation in relevance scoring, we propose the Anchored Label Representation  to obtain well-separated label representations. %Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represents each label with both support examples and corresponding anchors.  %Different from the previous single-label intent detection that uses label embedding as additional features , our label embeddings here have unique effects of separating different labels in metric space. % %Finally, to encourage better coordination between thresholding and label-instance relevance scoring, we introduce the logit-adapting mechanism to MCT that automatically adapts thresholds to different score densities.  % %Experiments on two datasets show that our methods significantly outperform strong baselines.  %Our contributions are summarized as follows:  % We explore the few-shot multi-label problem in intent detection of task-oriented dialogue, which is also an early attempt for the few-shot multi-label classification.  % We propose a Meta Calibrated Threshold mechanism with Kernel Regression and Logits Adapting that estimates threshold using both prior domain experience and new domain knowledge. % We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance scoring.    %% ============== version 3.0 EMNLP version ================= % %Intent detection  is a fundamental component for task-oriented dialogue system . %In real-word scenarios, intent detection often suffers from rapid changing of domains, because the new domains are usually lacking in data and may contain only a few data examples.  %Few-Shot Learning  is a promising solution to this problem.  %It provides a more human-like learning paradigm that generalizes from only a few learning examples  by exploiting prior experience from old domains.  % %In addition to data scarcity problem, intent detection also faces the problem of multi-label prediction. %As shown in Fig , a single utterance may carry multiple user intents.  %For this consideration, intent detection needs to be formulated as a Multi-Label Classification  problem , where a common practice is estimating label-instance relevance scores and picking the labels with score higher than a threshold value . % %Usually, the threshold is crucial to the performance of MLC models. %For multi-label intent detection, previous works explore to tune a fixed threshold  or to learn thresholds from data .  %However, these thresholds work well only when learning examples are sufficient.  %For few-shot scenarios, it is pretty hard to determine appropriate thresholds with only a few examples. %Also, it is difficult to directly transfer the threshold learned in data-rich domains due to the domain differences, such as differences in label number per instance, score density and scale. % % % %It is also challenging to compute the label-instance relevance scores for few-shot MLC.  %Previous few-shot research mainly focuses on single label classification and has achieved impressive progress with similarity-based methods  .  %Generally, these methods first obtain per class representations from a few examples , and then classify an  instance according to its similarity with the representation of each class. %However, such similarity scores rely on well-separated class  representations, which poses unique challenges in multi-label settings. %When instances have multiple labels, representations of different labels may be obtained from same support examples and become confused with each other. %For the example in Fig , intents of query\_time and query\_loc share the same support example  and thus have the same label representation.  % %In this paper, we study the few-shot learning problem of multi-label intent detection . %As mentioned above, it is difficult to estimate and transfer thresholds for few-shot MLC. %To solve this, we first learn universal thresholding experience on data-rich domains, and exploit the experience to estimate appropriate thresholds for unseen few-shot domains. %Specifically, we propose Meta Calibrated Threshold , which first learns a domain-general meta threshold, and then learns to calibrate it to fit specific domains with Kernel-Regression.  %To further encourage threshold generalization, we introduce the logit-adapting mechanism that automatically adapts meta thresholds to different score densities.  % %For computing label-instance score of few-shot MLC, we propose the Anchored Label Representation  to obtain well-separated label representations. %Inspired by the idea of embedding label name as anchor points to refine representation space , ALR uses the embeddings of label names as additional anchors and represent each label with both support examples and corresponding anchors.  % %Experiments on two datasets show that our methods significantly outperform strong baselines.  %Our contributions are summarized as follows:  % We explore the few-shot multi-label problem in intent detection of task-oriented dialogue,  %which is also an early attempt for few-shot multi-label classification.  % We propose a Meta Calibrated Threshold mechanism that estimate threshold using both prior domain experience and new domain knowledge. % We introduce the Anchored Label Representation to obtain well-separated label representation for better label-instance relevance score calculation.       In this paper, we explore the few-shot learning problem of multi-label intent detection.  To estimate a reasonable threshold with only a few support examples,  we propose the Meta Calibrated Threshold that adaptively combines prior experience and domain-specific knowledge.  To obtain label-instance relevance score under few-shot setting,  we introduce a metric learning based method with Anchored Label Representation. It provides well-separated label representations for label-instance similarity calculation.  Experiment results validate that both the Meta Calibrated Threshold and Anchored Label Representation can improve the few-shot multi-label intent detection.      
"," % ========== Version 6.0 ============= In this paper, we study the few-shot multi-label classification for user intent detection.  For multi-label intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a calibration based on non-parametric learning. %on metric learning. %, that does not require fine tuning to avoid overfitting. %Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refines representations of different classes to be well-separated from each other. Experiments on two datasets show that the proposed model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Data and code are available at \url{https://github.com/AtmaHou/FewShotMultiLabel}}   %% ========== Version 5.0 ============= %In this paper, we study the few-shot multi-label classification for user intent detection.  %For multi-label intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a calibration based on Kernel Regression, that does not require fine tuning to avoid overfitting. %%Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refines representations of different classes to be well-separated from each other. %Experiments on two datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Data and code are available at \url{https://anonymous.com}}  %% ========== Version 5.0 ============= %In this paper, we study the few-shot multi-label classification for user intent detection.  %For multi-intent detection, state-of-the-art work estimates label-instance relevance scores and uses a threshold to select multiple associated intent labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then adapt the thresholds to certain few-shot domains with a Kernel Regression based calibration.  %Kernel Regression here allows to avoid overfitting by calibrating threshold without finetuning. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refine representations of different classes to be well-separated from each other. %Experiments on two datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Code is available at \url{https://anonymous.com}}  %% ========= version 4.0 EMNLP version ========= %In this paper, we study the few-shot multi-label classification for user intent detection.  %Multi-label classification usually estimates label-instance relevance scores and uses a threshold to select multiple associated labels.  %To determine appropriate thresholds with only a few examples, we first learn universal thresholding experience on data-rich domains, and then calibrate the learned universal thresholds to fit certain few-shot domains. %For better calculation of label-instance relevance score, we introduce label name embedding as anchor points in representation space, which refine representations of different classes to be well-separated from each other. %Experiments on both open and in-house datasets show that our model significantly outperforms strong baselines in both one-shot and five-shot settings.\footnote{Code is available at: \url{https://anonymous.com}}",182
"  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .          % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Translation into languages with grammatical gender involves correctly inferring the grammatical gender of all entities in a sentence. In some languages this grammatical gender is dependent on the social gender of human referents. For example, in the Spanish translation of the sentence `This is the doctor',  `the doctor' would be either  `el m鑼卍ico', masculine, or `la m鑼卍ica', feminine. Since the noun refers to a person the grammatical gender inflection should be correct for a given referent.   In practice many NMT models struggle at generating such inflections correctly , often instead defaulting to gender-based social stereotypes  or masculine language . For example, an NMT model might always translate `This is the doctor' into a sentence with a masculine inflected noun: `Este es el m鑼卍ico'.     Such behaviour can be viewed as translations  exhibiting gender bias. By `bias' we follow the definition from  of behaviour which `systematically and unfairly  discriminate[s]  against certain individuals or groups of individuals in favor of others.' Specifically, translation performance favors referents fitting into groups corresponding to social stereotypes, such as male doctors.   Such systems propagate the representational harm of erasure to referents -- for example, a non-male doctor would be incorrectly gendered by the above example translation. Systems may also cause allocational harms if the incorrect translations are used as inputs to other systems . System users also experience representational harms via the reinforcement of stereotypes associating occupations with a particular gender . Even if they are not the referent, the user may not wish for their words to be translated in such a way that they  appear to endorse social stereotypes. Users will also experience a lower quality of service in receiving grammatically incorrect translations.   A common approach to this broad problem in NMT is the use of gender features, implicit or explicit. The gender of one or more words in a test sentence  is determined from external context  or by reliance on `gender signals' from words in the source sentence such as gendered pronouns. That information can then be used when translating. Such approaches combine two distinct tasks: identifying the gender inflection feature, and then applying it to translate words in the source sentence. These feature-based approaches make the unstated assumption that if we  correctly identify that, e.g., the doctor in the above example should be female, we could inflect entities in the sentence correctly, reducing the effect of gender bias.   Our contribution is an exploration of this assumption. We propose a scheme for incorporating an explicit gender inflection tag into NMT, particularly for translating coreference sentences . Experimenting with translation from English to Spanish and English to German, we find that simple existing approaches overgeneralize from a gender signal, incorrectly using the same inflection for every entity in the sentence. We show that a tagged-coreference adaptation approach is effective for combatting this behaviour.  Although we only work with English source sentences to extend prior work, we note that our approach can be extended to source languages without inherent gender signals like gendered pronouns, unlike approaches that rely on those signals.  Intuitively, if gender tagging does not perform well when it can use the label determined by human coreference resolution, it will be even less useful when a gender label must be automatically inferred.  Conversely, gender tagging that is effective in this scenario may be beneficial when the user can specify the gendered language to use for the referent, such as Google Translate's translation inflection selection , or for translations where the grammatical gender to use for  all human referents is known.  We also find that our approach works well  with RoBERTa-based gender tagging for English test sentences.    Existing work in NMT gender bias has focused on the translation of sentences based on binary gender signals, such as exclusively male or female personal pronouns. This excludes and erases those who do not use binary gendered language, including but not limited to non-binary individuals . As part of this work we therefore explore applying tagging to indicate gender-neutral referents, and produce a WinoMT set to assess translation of coreference sentences with gender-neutral entities.        Variations on a gender tag or signal for machine translation have been proposed in several forms.  incorporate a `speaker gender' tag into training data, allowing gender to be conveyed at the sentence level. However, this does not allow more fine-grained control, for example if there is more than one referent in a sentence. Similar approaches from   and   infer and use gender information from discourse context.  also incorporate a single explicit gender feature for each sentence at inference.    integrate coreference links into machine translation reranking to improve pronoun translation with cross-sentence context.  propose NMT gender bias reduction by `mixing signals' with the addition of pro-stereotypical adjectives. Also related to our work is the very recent approach of , who train their NMT models from scratch with all source language words annotated with target language grammatical gender.  In  we treat gender bias as a domain adaptation problem by adapting to a small set of synthetic sentences with equal numbers of entities using masculine and feminine inflections. We also interpret this as a gender `tagging' approach, since the gendered terms in the synthetic dataset give a strong signal to the model. In this work we extend the synthetic datasets from this work to explore this effect further.  Other approaches to reducing gender bias effects involve adjusting the word embeddings either directly  or by training with counterfactual data augmentation  . We view these approaches as orthogonal to our proposed scheme: they have similar goals but do not directly control inference-time gender inflection at the word or sentence level.     Tagging words with target language gender inflection is a powerful way to improve accuracy of translated inflections. This could be applied in cases where the correct grammatical gender to use for a given referent is known, or as monolingual coreference resolution tools improve sufficiently to be used for automatic tagging.  It also has potential application to new inflections defined for gender-neutral language.   However, there is a risk that gender features will be used in an over-general way. Providing a strong gender signal for one entity has the potential to harm users and referents by erasing other entities in the same sentence, unless a model is specifically trained to translate sentences with multiple entities. In particular we find that our V3 system, which is trained on multiple-entity translation examples, allows good performance while minimizing peripheral effects.   We conclude by emphasising that work on gender coreference in translation requires care to ensure that the effects of interventions are as intended, as well as testing scenarios that capture the full complexity of the problem, if the work is to have an impact on gender bias.   
"," Neural Machine Translation  has been shown to struggle with grammatical gender that is dependent on the gender of human referents, which can cause gender bias effects. Many existing approaches to this problem seek to control gender inflection in the target language by explicitly or implicitly adding a gender feature to the source sentence, usually at the sentence level.    In this paper we propose schemes for incorporating explicit word-level gender inflection tags into NMT. We explore the potential of this gender-inflection controlled translation when the gender feature can be determined from a human reference, or when a test sentence can be automatically gender-tagged, assessing on English-to-Spanish and English-to-German translation.  We find that simple existing approaches can over-generalize a gender-feature to multiple entities in a sentence, and suggest effective alternatives in the form of tagged coreference adaptation data. We also propose an extension to assess translations of gender-neutral entities from English given a corresponding linguistic convention, such as a non-binary inflection, in the target language.",183
" %缁楊兛绔村▓纰夌窗娣団剝浼呮径姘帗閸 Existing experiments  have proven that multimodal news can significantly improve users閳 sense of satisfaction for informativeness. As one of these multimedia data forms, introducing news events with video and textual descriptions is becoming increasingly popular, and has been employed as the main form of news reporting by news media including BBC, Weibo, CNN, and Daily Mail. An illustration is shown in Figure, where the news contains a video with a cover picture and a full news article with a short textual summary. In such a case, automatically generating multimodal summaries, , which learns to summarize article and video simultaneously by conducting a dual interaction strategy in the process. Specifically, we first employ Recurrent Neural Networks  to encode text and video. Note that by the encoding RNN, the spatial and temporal dependencies between images in the video are captured.  % The features of segments and text can be constraint in the same space through L2 normalization which modifies the vector in a way that each row the sum of the squares will always be up to 1. Next, we design a dual interaction module to let the video and text fully interact with each other.  Specifically, we propose a conditional self-attention mechanism which learns local video representation under the guidance of article, and a global-attention mechanism to learn high-level representation of video-aware article and article-aware video. Last, the multimodal generator generates the textual summary and extracts the cover image based on the fusion representation from the last step. To evaluate the performance of our model, we collect the first large-scale news article-summary dataset associated with video-cover from social media websites. Extensive experiments on this dataset show that DIMS significantly outperforms the state-of-the-art baseline methods in commonly-used metrics by a large margin.  %缁楊剙娲撳▓纰夌窗閹崵绮╟ontribution To summarize, our contributions are threefold:   $      encodes the input article and video separately;  Dual Interaction Module learns fused representation of video and article from different level;  Multi-Generator generates the textual summary and chooses the video cover simultaneously.     }            In this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output  which chooses a proper video cover and generates an appropriate textual summary for a video-attached article. We propose a model named Dual-Interaction-based Multimodal Summarizer  including a local conditional self-attention mechanism and a global-attention mechanism to jointly model and summarize multimodal input. Our model achieves state-of-the-art results in terms of autometrics and outperforms human evaluations by a large margin. In near future, we aim to incorporate the video script information in the multimodal summarization process.  
"," % Multimodal summarization has drawn much attention due to the rapid growth of multimedia data.  A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively. Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output  to tackle such a problem. The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article. To this end, we propose a Dual-Interaction-based Multimodal Summarizer , consisting of a dual interaction module and multimodal generator. In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level. Extensive experiments conducted on a large-scale real-world VMSMO dataset\footnote{https://github.com/yingtaomj/VMSMO} show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",184
"  Natural language processing for deception detection focus on preprocessing text into computational data with required features for the propose. As deception detection is about understanding the meaning of the text or how the text is viewed by people, the sequence of the text is always considered as one of primary source of context. For example, N-gram, the representative method of natural language processing, contains the data of a word and its subsequent word and its statistical probabilities. The attribute subsequent contains the continuous context of the text, or the linguist  will describes as linearity. In contrast, feature extractions without considering this language's linearity seems to be nonsense. However, if the data that been processed out of those non-linear feature extractions shows notable accuracy of detecting deceptions, it is possible to suggest that some of those preprocessing methods could be used as one of possible natural language processing for certain situations. \  In this paper, we discuss the effectiveness of APV, a simple natural language processing method using alphabet frequency, in the context of application on fake news detection. By using deep learning algorithm and fake news dataset in Kaggle, our findings suggest that simple deep learning algorithms using APV as pre-processing method could show prominent accuracy on predicting deception of the text. \  In section 2, we investigate conventional natural language processing that is used for machine learning and deep learning algorithms. In section 3, we define APV and its mathematical structure. We will also discuss the hypothesis that might improve feature extraction of APV.  In section 4, basic experiment protocol will be set including the structure of deep learning algorithms and performance metrics that will be used in the experiment. In section 5, we present the result of the algorithms performance. Finally, in section 6, we conclude the study.     As the current natural language processing suggests, it is ideal for the natural language to be considered by their strict and linear order. However, in this paper, we suggest that even if the natural sequence of the data are excluded from the feature extraction of a text, it is possible to use those data to classify the text with high accuracy. And we consider this accuracy is a fair trade-off between accuracy and pre-processing effort, as APV shrinks the data to approximately   in its size but still obtained 92\  of the accuracy the deep learning algorithm that is been reported has .  It is not sure whether or not APV is capable of summarizing the text, however it seems possible to use APV in supervised learning regarding natural language. We are planning to use the proposed method to classify more than 2 classes, and also hopefully find mathematical explanations why this method works and how to improve the feature extraction more than the results listed in this paper.  
","     Feature extraction is an important process of machine learning and deep learning, as the process make algorithms function more efficiently, and also accurate. In natural language processing used in deception detection such as fake news detection, several ways of feature extraction in statistical aspect had been introduced . In this research, it will be shown that by using  deep learning algorithms and alphabet frequencies of the original text of a news without any information about the sequence of the alphabet can actually be used to classify fake news and trustworthy ones in high accuracy . As this pre-processing method makes the data notably compact but also include the feature that is needed for the classifier, it seems that alphabet frequencies contains some useful features for understanding complex context or meaning of the original text.\\\\  keywords: {[FEATURE EXTRACTION], [DEEP LEARNING]}  % Received, Accepted 闂嗩喚濞嬮～锟犳禃 闂夋稑瀚靛 闂夋稑鎳為悧鎾荤垷濮楀喚娼 濮ｉ潧鐗楅崝顖炵亙.",185
"  Sentence matching is a fundamental technology in natural language processing. Over the past few years, deep learning as a data-driven technique has yielded state-of-the-art results on sentence matching . However, this data-driven technique typically requires large amounts of manual annotation and brings much cost. If large labeled data can't be obtained, the advantages of deep learning will significantly diminish.  To alleviate this problem, active learning is proposed to achieve better performance with fewer labeled training instances . Instead of randomly selecting instances, active learning can measure the whole candidate instances according to some criteria, and then select more efficient instances for annotation . However, previous active learning approaches in natural language processing mainly depend on the entropy-based uncertainty criterion , and ignore the characteristics of natural language. To be more specific, if we ignore the linguistic similarity, we may select redundant instances and waste many annotation resources. Thus, how to devise linguistic criteria to measure candidate instances is an important challenge.  Recently, pre-trained language models  have been shown to be powerful for learning language representation. Accordingly, pre-trained language models may provide a reliable way to help capture language characteristics. In this paper, we devise linguistic criteria from a pre-trained language model to capture language characteristics, and then utilize these extra linguistic criteria  to enhance active learning. It is shown in Figure . Experiments on both English and Chinese sentence matching datasets demonstrate the pre-trained language model can enhance active learning.      In this paper, we combine active learning with a pre-trained language model. We devise extra linguistic criteria from a pre-trained language model, which can capture language characteristics and enhance active learning. Experiments show that our proposed active learning approach obtains better performance.  
"," Active learning is able to significantly reduce the annotation cost for data-driven techniques. However, previous active learning approaches for natural language processing mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria to measure instances and help select more efficient instances for annotation. Experiments demonstrate our approach can achieve greater accuracy with fewer labeled training instances.",186
"  FILM  shortcoming: 1.the whole paper does not show the advantage clearly 濞岋紕澹掗崚顐ヮ嚛濞撳懏顨 閻滄澘婀悧鐟板焼閻ㄥ嫪绱崝   1.1 瀵缚鐨熻箛鐛竌st閿涙岸娓剁憰浣烘倞鐠侀缚鐦夐弰搴☆槻閺夊倸瀹崇圭偤鐛欑佃鐦拠瀛樻韫囶偓绱濋獮鍫曟付鐟曚礁鎷板ǎ鍗炲鐎涳缚绡勯弬瑙勭《鐎佃鐦敍鍧卍am閿  1.2 閻х晫娲呴敍姘舵付鐟曚礁濮為悧鐟板焼閻ㄥ嫯顕╅弰搴㈡瀮鐎涙绱濆Ο鈥崇烽弰顖滃殠閹呮畱閿涘本澧嶆禒銉ф閻  2.鐎圭偤鐛欐稉宥咁檮 experiment is not enough for text matching  2.1 add dataset閿涙瓛ahoo!   2.2閸旂姴顕В鏃撶窗other deep models, one experiment aims at proving our model is fastest, the other experiment aims at showing metric leanring閼宠棄鐫嶉悳鏉垮毉閺夘櫨閸掓繂顫愰惃鍒ature閺勭姴鐨犻崚鎵畱閺勵垯绮堟稊鍫濇勾閺傜櫢绱   閸滃本澹樼悰銊с仛閻ㄥ嫭鏌熷▔鏇烆嚠濮ｆ棑绱皌ext matching閸旂姵鏆熼幑顕娉﹂妴   瀵板牆顦縠nd2end閻ㄥ嫭鏌熷▔鏇熷鐞涖劎銇氶敍宀骞囬崷銊︽ЦKNN閸嬫矮鎹㈤崝  濮ｅ繋绔存稉鐚夐敍瀛婚惃鍕冪粈鐑樺閸掗绨  鎼存棁顕氶弰顖涙箒娑擃亝婵囧厒閿涙艾鎻╅妴浣稿讲鐟欙綁鍣撮敍  鐟欙絽鍠呴弬瑙勭《閿涙碍鍨滄禒顒鍟块惈matching娴犺濮熼弶銉嚛閿涘瞼娴夋导鍏佳勬Ц娴犳稊鍫礉閺勭柨閻ㄥ嫪缍嗙紒纾嬨冪粈鐚寸礉閸欘垯浜掗弰顖炴姜鐢摜鐣濋崡鏇礉metric learning閿 娴犮儱绶氶張澶婁粵閻ㄥ嫸绱濈粻妤绶辫箛..閿涘牐顩︾捄鐕傜礆    閸︺劌鎮楅棃銏ゆ珟娴滃棗鐤勬宀勫劥閸掑棗鐣崰鍕剁窗1閵嗕椒琚辨稉顏呮殶閹诡噣娉﹂妴浣疯⒈娑擃亝鏆熼幑顕娉︾圭偤鐛欑紒鎾寸亯閺勵垰鐣弫瀵告畱  2閵嗕礁宸辩拫鍐ㄦ彥閻ㄥ嫬鐤勬  3閵嗕礁褰茬憴锝夊櫞閻ㄥ嫬鐤勬宀嬬窗metric leanring閼宠棄鐫嶉悳鏉垮毉閺夈儲妲х亸鍕煂閻ㄥ嫭妲告禒娑斿牆婀撮弬鐧哥礉x閸掓繂顫愰惃鍒ature  缁炬寧褏娈慹mbed閸戠儤娼甸惃鍒岀拫浣告嫲鐠嬩焦娲块惄绋垮彠閺勵垰褰叉禒銉ョ潔閻滄壆娈戦敍灞芥躬娑撴禍娑楃伐鐎涙劒绗傞弰顖氬讲娴犮儳鎮婄憴锝囨畱  Y缁炬寧褏绮嶉崥鍫濐劅閸戠儤娼甸惃鍕Ц閸濐亙绨虹拠宥囩矋閹存劗娈慟A閻ㄥ嫯銆冪粈鐚寸礉娑撹桨绮堟稊鍧tch閸︺劋绔寸挧閿嬫Ц閸ョ姳璐熼崫顏冪昂鐠囧稄绱濋敍鍫濈潔閻滄澘鍤璍閿涘鐦俊鍌炵彯娴滎喚娈戦弬鐟扮础閺勵垰褰叉禒銉ф倞鐟欙絿娈戦敍宀冨奔绗夐弰顖滄暏deep learn瀹歌尙绮￠惇瀣╃瑝閸戠儤娼  缁犻崡鏇炲讲鐟欙綁鍣撮幀顪欶IDF  缂傝櫣鍋ｉ敍 1.motivation娑撳秵妲戠涵顕嗙礉contribution閸︺劌鎽 2.娑撳秴鐣弫  閸撳秹娼伴弰搴ｂ橀妴浣告倵闂堛垹鐣弫娣胶鐣诲▔鏇氱瑝閺勵垱褰侀惃    %text matching閺堝绶㈡径姝瀍ep learning閺傝纭堕敍灞炬櫏閺嬫粈绗夐柨娆欑礉娴ｅ棙妲搁崣顖澬掗柌濠傛▕閿涘奔绗栭柅鐔峰娑撳秹鐝  The neural networks represent two sentences individually to a dense vector in the same embedding space, and then define different functions to calculate the matching degree of the two-sentence vectors. However, they are getting extremely time-consuming as the networks are becoming more sophisticated and introducing more parameters. Even worse, it is still a black box for researchers and practitioners, and in urgent need of interpretability. We can't figure out what's the specific meaning of the representation obtained from neural networks, which is unaccountable or challenging to comprehend and will lead to an untrusty and irresponsible result.   %閹存垳婊戠亸杈ㄥ厒閹靛彞绔存稉顏勫嫉韫囶偄寮垫總鍊熜掗柌濠忕礉娴犲簼浜掗崜宥囨畱deep learning鐠囦焦妲戞禍鍡楊劅閺傚洦婀伴惃鍕秵缂佺銆冪粈鐑樻Ц闂堢姾姘ㄩ惃鍕剁礉閹垫禒顧砮tric learn閸掓艾銈界亸杈ㄦЦ鏉╂瑦鐗遍敍灞界穿閸忣櫝etric learning閿涘奔绗栭敍鍫滀簰瀵伴弰顖涘簼绠為悽鈺〆tric learning閿  To tackle these, we aim to find a fast and interpretable approach for sentence matching. There are several studies focused on learning low-dimensional representations of the data, which called metric learning and even some of them combine it with some similarity metrics for ranking tasks .  Moreover, some researchers apply metric learning principles to design the loss function in information retrieval and question-answering tasks. But for the deep metric learning that they utilized, the neural network part still demands a lot of time. It hardly runs on a memory-limited device, together with high energy consumption.  %閹存垳婊戝銉ょ稊閺勵垰婀猼ext matching娑撳﹥褰佹稉娑擃亜鎻╅柅鐔烘畱閺傝纭堕妴鍌樺倶鍌涘娴狀櫑pply閵嗗倶鍌  It is considering the unexplainable implications brought from neural networks, such as fairness or transparency, and the challenge of time-consuming. In this paper, we apply metric learning approaches to address the problems mentioned above. Because metric learning has an advantage in time and memory usage on large-scale and high-dimensional datasets compared with methods above. Here, metric learning finds a representation of the data that preserves these constraints that are placed by human-provided labels. Building on its success in learning ``label constraint preserving'' representations, or , we explore two Fast, Interpretable, and Low-rank Metric learning approaches, what we called FILM.   %閻鍩岄弫鍫熺亯metric learning閼宠棄鐤勯悳鎵娴艰偐娈戠紒鎾寸亯閿涘奔绗栬箛顐︾喍绗栭崣顖澬掗柌濠冄嶇窗缁炬寧褏娈   Notably, we explore FILM methods on text matching tasks, which is also known as the semantic equivalence problem in the IR community~. To be more specific, one based on an interpretable low-rank manifold optimization method. To solve this optimization problem,  we apply the Cayley transformation method with the Barzilai-Borwein step size. After being trained for this task, both are added to the kNN index for prediction for efficient retrieval. The input question is encoded and used as a query to the index, returning the top k most similar questions. We test our approaches on data from the Quora Challenge and SemEval-2017 Semantic Textual Similarity  Task, which provide pairwise sentence similarity labels.   %\footnote{}   %Our motivation is to investigate whether FILM approaches can perform as well as, if not better than, some ``black box'' approaches that are so popular these days.    Our contributions are as follows:  ---that relies on an interpretable linear model. Due to space constraints, we will focus on our main approach.   The rest of this paper is organized as follows. In Section , we provide a quick overview of metric learning. In Section  we present the interpretable FILM method. In Section , we summarize the Quora dataset and task, explain how FILM is applied to the task, and summarize our deep neural network approach. In Section  we report some results.       We investigated text matching, a core task in information retrieval and semantic analysis. We introduced the notation and definition of metric learning, and how it can be applied to text matching. Then, we explored FILM , which aim to reduces the time cost and memory usage, also save energy consumption. In order to solve this task efficiently, FILM combined with a fast approximate k nearest neighbour search index. Compare to neural models, our method also has advantage in time and memory usage on large-scale and high-dimensional datasets.   
"," Detection of semantic similarity plays a vital role in sentence matching. It requires to learn discriminative representations of natural language. Recently, owing to more and more sophisticated model architecture, impressive progress has been made, along with a time-consuming training process and not-interpretable inference. % In sentence matching and semantic analysis, detecting semantic similarity is a challenge that requires learning discriminative representations of natural language. Recent advances in the deep neural network enable us to learn semantic representation, but are getting time-consuming and fail in interpretation. To alleviate this problem, we explore a metric learning approach, named FILM  to efficiently find a high discriminative projection of the high-dimensional data. We construct this metric learning problem as a manifold optimization problem, and solve it with the Cayley transformation method with Barzilai-Borwein step size. % To alleviate this problem, in this paper we construct sentence matching as a manifold optimization problem that learns a distance function between sentences. % % and obtain the semantic representation by learning a similarity or distance function. % We explore a metric learning approach, named FILM  to efficiently find a high discriminative projection of the high-dimensional data. % that still preserves high discriminative power. % To this end, our manifold optimization method is solved by the Cayley transformation method with Barzilai-Borwein step size.  In experiments, we apply FILM with triplet loss minimization objective to the Quora Challenge and Semantic Textual Similarity  Task. The results demonstrate that the FILM method achieves a superior performance as well as the fastest computation speed, which is consistent with our theoretical analysis of time complexity.",187
" %A common situation for language learners is to encounter unrecognized words. %In this case, looking up the dictionary may be the preferred solution for many people. %However, the capacity of dictionaries is limited, and they may not contain new words or new meanings of words. %What's more, not all language pairs have dictionaries, especially those with low resources. %Therefore, it may be a good idea to directly generate definitions for words.  The definition modeling task proposed by  is to generate a dictionary definition of a specific word. This task can prove useful for language learners, such as provide reading help by giving definitions for words in the text. However, definition modeling can only work for a specific language, which puts high demands on users because it requires them to read definitions written in this language. Besides, many low-resource languages lack large-scale dictionary data, making it difficult to train definition generation models for these languages. %This task can prove useful for language learners, such as provide reading help by giving definitions for words in the text. %However, definition modeling can only work for a specific language, which puts high demands on users because it requires them to read definitions written in this language.  Therefore, we emphasize the necessity of generating definitions cross-lingually, which can generate definitions for various language inputs, as illustrated in figure . Since English is widely used around the world, and English dictionary resources are relatively easy to obtain, we choose to generate definitions in English. In this way, a cross-lingual model trained on English can be directly applied to other languages.  The challenging issue is how to effectively transfer the knowledge of definition generation learned in English to other languages. To solve this problem, we propose to employ cross-lingual pretrained language models  as encoders. These models have shown to be able to encode sequences of various languages, which enables the ability of cross-lingual transfer . %In this work, we emphasize the necessity of generating definitions cross-lingually, which requires the model to generate definitions with just one language for words in various languages as illustrated in figure . %Considering English is widely used around the world, and English dictionary resources are relatively easy to obtain, we choose to use English to generate definitions for other languages in this work.  %Recently, cross-lingual pretrained language models  have shown to be capable of encoding sequences of different languages into the same vector space, which enables the ability of cross-lingual transfer. %Therefore, we propose to employ them as encoders for cross-lingual definition generation. %After training and fine-tuning the model on English dataset, we directly apply the obtained model to generate definitions for other languages.    To verify our proposed method, we build an English dataset for model training and a Chinese dataset for zero-shot cross-lingual evaluation. %We collected English words, example sentences and definitions in the OALD as the English dataset, and collected Chinese words, example sentences and English definitions in the Chinese WordNet   as the Chinese dataset. Experiments and manual analyses on the constructed datasets show that our proposed models have good cross-lingual transfer ability. Compared with the reference definitions in the CWN dataset, although the generated definitions are still insufficient on the accuracy, their fluency is already good enough.  Furthermore, considering the generated definitions are provided for language learners, and many of them are non-English native speakers, we argue that the difficulty of definitions should be under control. We control the lexical complexity of generated definitions by limiting definitions in the training set to the Oxford 3000 vocabulary, which is a list of important and useful words that are carefully selected by language experts and experienced teachers . %These words have been used to write definitions in the Oxford Advanced Learner's Dictionary  , in order to make them easy to understand. %We compute the Type/Token Ratio  as a measure of lexical complexity. %The TTR of generated definitions  is much lower than that of reference definitions , which indicates a lower lexical complexity. We compute four different metrics to measure the lexical complexity. Definitions generated by our models outperform the reference definitions on all four metrics by a large margin. The result shows that our method can generate simpler definitions, which is suitable for language learners.    In this work, we employ pretrained language models, namely mBERT and XLM for cross-lingual definition generation. In addition, we propose to use the Oxford 3000 vocabulary to limit the lexical complexity of generated definitions. We build the OALD dataset for monolingual training and the CWN dataset for cross-lingual evaluation. Experiments indicate the strong cross-lingual transfer ability of our proposed method. Furthermore, results on lexical complexity shows that definitions generated using our method is simpler than the reference, which is suitable for language learners.  Experiments conducted on these datasets show the effectiveness of our proposed method.  Furthermore, manual analysis performed on the CWN test set shows that although the generated definitions are insufficient on the accuracy, they are already good enough on fluency and lexical complexity.   	    	 	   
"," Generating dictionary definitions automatically can prove useful for language learners. However, it's still a challenging task of cross-lingual definition generation. In this work, we propose to generate definitions in English for words in various languages. To achieve this, we present a simple yet effective approach based on publicly available pretrained language models. In this approach, models can be directly applied to other languages after trained on the English dataset. We demonstrate the effectiveness of this approach on zero-shot definition generation. Experiments and manual analyses on newly constructed datasets show that our models have a strong cross-lingual transfer ability and can generate fluent English definitions for Chinese words. We further measure the lexical complexity of generated and reference definitions. The results show that the generated definitions are much simpler, which is more suitable for language learners. %We further conduct a manual analysis of the generated Chinese definitions and find that although these definitions are insufficient on the accuracy, they are already good enough on fluency and lexical complexity.",188
"  The CoNLL 2020 MRP Shared Task  combines five frameworks for graph-based meaning representation: EDS, PTG, UCCA, AMR and DRG. It further includes evaluations in English, Czech, German and Chinese. While EDS, UCCA and AMR participated in the 2019 MRP shared task , which focused only on English, PTG and DRG are newly-added frameworks to the MRP uniform format.  For this shared task, we extended TUPA , which was adapted as the baseline system in the 2019 MRP shared task , to support the two new frameworks and the different languages. In order to add this support, only minimal changes were needed, demonstrating TUPA's strength in parsing a wide array of representations.  TUPA is a general transition-based parser for directed acyclic graphs , originally designed for parsing UCCA . It was previously used as the baseline system in SemEval 2019 Task 1 , and generalized to support other frameworks .  We also experimented with the HIT-SCIR parser . This was the parser with the highest average score across frameworks in the 2019 MRP shared task, and has also since been applied to other frameworks  .  [ht] 	{width=.99\textwidth,margin=1pt,frame} 	{llll|l|lllll} 		\multicolumn{4}{c|}{{*{{c{ & 	extbf{ootnotesize Buffer &  		ootnotesize N. & ootnotesize Edges & &  		ootnotesize Stack & ootnotesize Buffer &  		ootnotesize Nodes & ootnotesize Edges &  		ootnotesize Extra Effect \\  &  &  &  &  & \\ 		 &  &  &  & Reduce &  &  &  &  & \\ 		 &  &  &  & Node &  &  &  &  & S\;|\;xy\;|\;BVE\;|\; &  &  &  &  & S\;|\;xBVEp\leftarrow pS\;|\;y,xBVE_XS\;|\;y,xBVE\;|\; &  &  &  &  & SBVE\;|\;a\leftarrow aS\;|\;x,yBVES\;|\;yx\;|\;BVE[\mathrm{root}] & xx requires that yyx\mathrm{i}<\mathrm{i}\mathrm{i}  [ht]      [ht]    edge, and virtual terminal nodes corresponding to text tokens,   attached according to the anchoring   with Anchor edges.   Same as for all frameworks with node labels and properties ,   labels and properties are replaced with placeholders corresponding to anchored tokens,   where possible.   The placeholder $\langle     We have presented TUPA-MRP and a modified HIT-SCIR parser, which constitute the HUJI-KU submission in the CoNLL 2020 shared task on Cross-Framework Meaning Representation. TUPA is a general transition-based DAG parser with a uniform transition system, which is easily adaptable for multiple frameworks. We used it for parsing in both the cross-framework and the cross-lingual tracks, adapting it for the newly introduced frameworks, PTG and DRG.  HIT-SCIR is a transition-based parser with framework-specific transition systems, which we adapted for this year's shared task and used for English EDS and UCCA parsing in the cross-framework track. The HIT-SCIR parser was additionally used in experimenting on multitask learning, with negative results for that approach.  Future work will tackle the MRP task with more modern transition-based-like parser architectures, such as pointer networks , which have so far only been applied to bilexical framworks, i.e., flavor-0 SDP .   
","   This paper describes the HUJI-KU system submission to the shared task   on Cross-Framework Meaning Representation Parsing  at the 2020   Conference for Computational Language Learning ,   employing TUPA and the HIT-SCIR parser, which were, respectively,   the baseline system and winning system in the 2019 MRP shared task.   Both are transition-based parsers using BERT contextualized embeddings.   We generalized TUPA to support the newly-added MRP frameworks and languages,   and experimented with multitask learning with the HIT-SCIR parser.   We reached 4th place in both the cross-framework and cross-lingual tracks.",189
" Training open-domain dialog models is inherently difficult, since for each utterance there are many acceptable responses, yet no perfect response. While supervised learning from conversational corpora allows models to learn grammatical structure and even topic coherence, these models do not generalize, since the training objectives mostly lead the models to memorize responses within the corpus.  Humans are the ultimate authority in evaluating what makes one conversational reply better than another. To learn from real conversations with humans, we created an interactive, online platform which hosted a diverse set of neural network dialog models that users could chat with in real time. However, when learning from human interactions in the wild it is crucial to be able to learn offline and test the policy before deploying it, lest it learn inappropriate behaviors . Thus, we need to train and test models offline, to ensure safe model outputs. In order to safely learn to optimize human feedback we pursued an offline reinforcement learning approach to training dialog models .   Offline RL is challenging; most deep RL algorithms fail to learn from data that is not heavily correlated with the current policy . Even models based on off-policy algorithms like -learning fail to learn in the offline RL setting, as the model is not able to explore. If the offline dataset is not sufficient to cover the input-response space, offline RL models suffer from extrapolation error, learning arbitrarily bad estimates of the value of responses not contained in the data.   We solve these problems by developing a new method for offline RL.  The method starts by leveraging a pre-trained language model to constrain offline RL updates. While training with RL, we penalize divergence from this prior model using forms of KL-control. This combats extrapolation error, and ensures that the RL model learns a policy that stays close to the distribution of realistic language, while learning to maximize positive human responses using the offline data. Further, we use dropout to obtain uncertainty estimates of the target -values, and to obtain a lower bound to alleviate over-optimistic bias in estimating future reward. We show that this new method is able to learn successfully from many different reward functions, even in a very large space with 20,000 tokens.  Both linguistic theory  and empirical experiments correlating human judgement with language features suggest that there are many criteria that could be used to evaluate a conversational agent  . We develop a set of reward functions for our dialog agents to optimize, which are designed to approximate implicit human preferences expressed during conversational responses. We show that the new method is better able to optimize these rewards using the offline data, and when tested with a new set of 80 human conversation partners, leads to more positive responses and higher quality ratings than a state-of-the-art offline deep RL method.  Novel contributions of this paper are:             In this work, we present novel techniques that enable successful offline reinforcement learning on any base language model from real human conversations. This allows the dialog systems practitioner to train models that learn language structure from vast, readily-available corpora, then fine-tune for specific desirable behaviors post-hoc through RL rewards.   We observe that the new offline RL method successfully optimizes both generated bot rewards and elicited human responses. We show that it presents a better option than using regularization in training a specific bot behavior. Further, RL currently remains the only option for maximizing user feedback over the course of a conversation.   Compared to prior work in offline RL, the novel WOP offline RL algorithm achieves higher performance in traditional RL tasks, elicits more positive feedback in conversations with novel humans at test time, and earns overall higher human ratings.  A limitation of our study is that the question of what to optimize with RL to improve overall qualitative ratings remains open.  We have shown that manual ratings are too sparse to optimize effectively, and instead suggest using implicit rewards. However, our reward set proved insufficient to achieve higher human quality ratings, at least with the limited offline training data we were able to collect. It is unlikely the rewards proposed here fully cover what it means to have a high quality open-ended conversation. Future work should investigate more rewards for training an open-domain dialog model such as long term conversation rewards that may need to be computed over many conversation turns.   Our work computes conversational rewards based on dialog data and annotations from online task workers in the United States. Considering the broader impacts of our work, a representative and diverse set of conversations and annotations should be collected before real world systems are trained and deployed using our algorithms.   We have shown that the proposed techniques can be useful for shaping dialog model behavior towards a desired objective. For many practical applications, we may have specific requirements for the language generated by a model---for example, that it is appropriate, positive, and polite---even if this leads to a lower perception of conversation quality for some users. We have shown that the Way Off-Policy algorithm provides a more effective way to teach a language model specific behaviors from offline data than previously proposed RL or regularization techniques.   We would like to thank Scott Fujimoto for insightful email correspondence on this topic, approval of the DBCQ algorithm, and suggestion to apply model averaging. We would like to thank Sudha Rao and Yonatan Bisk for helpful guidance and feedback in the re-framing and re-writting process of this work. We also thank Max Kleiman-Weiner, Ardavan Saeedi, Sebastian Zepf, Sara Taylor, Oliver Saunders Wilder, Kyle Kastner, Marissa Zhang, and Kristy Johnson for their helpful discussions about this project, and many others for helping test-drive our bots.  We thank the MIT Quest for Intelligence, and MIT Stephen A. Schwarzman College of Computing, and the Machine Learning Across Disciplines Challenge for providing computing resources, and MIT Media Lab Consortium for the support of this research. This work has been partially supported by RTI2018-095232-B-C22 grant from the Spanish Ministry of Science.           The underlying architecture of the baseline language models employed for this work is a Variational Hierarchical Recurrent Encoder Decoder  . We also conduct a second set of experiments on an enhanced version of this model with additional knowledge distillation to improve the model's ability to track the sentiment and semantics of the conversation, as proposed by . The language models were originally trained on two datasets: movie dialogs  and a dataset scraped from  .  The underlying parameters of the VHRED model were as follows: Context RNN hidden size , decoder hidden size , encoder hidden size ,  embedding size , gradient clip , dropout . The maximum conversation length was fixed at 5 utterances , and the maximum sentence length was 30 tokens. The VHRED model has  million parameters.   We also added layers to the Context RNN and regularized it to be able to predict the semantic content of the input utterance using a form of knowledge distillation  from a state-of-the-art sentence-embedding model . There were 2 additional feedforward semantic prediction prediction layers of size 128, which used ReLu activation. The VHRED model with sentiment and infersent regularization has  million parameters.   Each RL model was trained on a NVIDIA GeForce GTX 1080 GPU.     The RL models, the main focus of our work, were trained using human conversation data collected via the online interactive platform  and batch size was fixed at 32. Each model was trained for  epochs. The RL models were initialized with the weights of the best model trained on the Reddit dataset. Early stopping was used to determine the number of training iterations of the best checkpoint. For each bot, 3 different stopping epochs were tested and the best was selected. The checkpoint was selected using manual tuning based on interactive chat with the chatbots. For the best performing bots, KL-Control  and KL-Control , the 1600 and 1800 epoch checkpoints were selected respectively.   The reward weights were also tuned to determine which weighting of rewards produced the desired bot behavior. We tried uniform weights  and slightly increased weights for repetition rewards and human bot interaction rewards. The best weights were found to be assigning  to repetition and human bot interaction rewards and  to all other rewards. Reward weights were also determined using manual tuning and conversational interaction. The same reward weights were shared between all RL models we trained. Only 3 sets of weights were tried in the reward weights hyperparameter optimization process.   All other hyperparameters were shared between RL models, and were as follows: discount , weight placed on RL reward vs. KL-divergence term , number of Monte Carlo samples of the Target -network , target network update rate , learning rate . We used a smooth  loss function to approximate the -values, and clipped gradients at a value of . The RL models have a total of  parameters .   } \\   & 3.02  .47          & 3.45  .47          & 18.59 1.76          & 0.19          & -0.05\\   & 1.84  .34          & 2.09  .38          & 10.58 1.55          & -0.23         & -0.02 \\ Batch              & 1.87  .30         & 2.36 .42          & 2.20 .41           & 1.91 .32           & 2.58 .47           & 11.91 1.58          & -0.16         & 0.00 \\ Batch  + MC        & 1.85  .39         & 2.46 .44          & 2.46 .52           & 1.98 .39           & 2.34 .49           & 11.07 1.82          & -0.07         & 0.03  \\ KL-control         & 2.38 .39 & 3.24 .47          & 3.42 .54           & 2.38 .45  & 2.56 .43           & 13.98 1.81          & 0.02          & 0.01  \\ KL-control    & 2.33 .41            & 3.73 .53  & 2.82 .50            & 2.31 .44  & 3.47 .50  & 14.67 1.82 & 0.13 & 0.03 \\  & Fluent & Diverse & Related & Empathy & Total  & Votes &   \\   \\ Manual votes    & 2.14 .38             & 3.47 .45           & 2.91 .47            & 2.07 .39            & 2.42 .46            & 13.00 1.65          & -0.03          & 0.01 \\ Sent. trans.    & 2.02 .31             & 3.71 .49           & 2.98 .50            & 2.04 .42            & 2.84 .48            & 13.60 1.63          & 0.03           & 0.01  \\ Bot Question        & 2.29 .37         & 4.31 .50  & 3.31 .52   & 2.20 .40            & 2.60 .41            & 14.71 1.63          & 0.06           & 0.04  \\ User Sentiment       & 2.47 .32   & 4.05 .45           & 3.23 .46            & 2.42 .39            & 3.23 .55   & 15.40 1.49 & 0.09  & 0.04  \\ , Fluency, Diversity, Contingency, and Empathy on a 7-point Likert scale. A detailed example of the chat and interaction platform can be found in Section . Since our models are evaluated using interactive chat, we also validate our models through interactive chat and rate the models while tuning hyperparameters. The authors interacted with and rated bots during to validate bots.     We also conducted experiments using each offline RL algorithm with a Sentiment and Infersent regularized VHRED Model. As described in Section , by adding about 20 million extra parameters to the VHRED model in order to better achieve semantic coherence and sentiment contingency, the VHRED-EI  model is a better performing baseline in terms of human ratings .   We conducted the same human experiments where we recruited participants from Amazon Mechanical Turk to chat with and rate each dialog model. We found similar results as presented in our main paper. While our KL-control models achieved higher qualitative ratings than the other offline RL algorithms, none of the RL models received higher qualitative ratings than the VHRED-EI Model . We also replicated training the KL-Control  model on single rewards and found that training on User Sentiment elicited the highest human qualitative ratings . This consistent with our results on the VHRED model.        for different offline learning conditions. WOP consistently exceeds the performance of Batch Q-learning, Behavioral Cloning , DBCQ, and the Behavior policy used to generate the batch data. Error bars show 95\  CI of the mean over 50 trials.}          .}        To demonstrate the effectiveness of these techniques, we tested them on traditional RL tasks using the OpenAI gym , focusing on the CartPole-v0 and Acrobot-v1 experiments. We first train an online -learning Behavior policy, and store all  experience samples into a replay buffer. We use this buffer to train a prior model of  using a Variational Auto-encoder. The VAE was trained to reconstruct the next state given the current state, , using a mean-squared error loss.  The next action was predicted from the latent embedding , meaning the model learned three functions: , , and . For Cartpole, both the encoder and decoder were made up of two linear layers with 750 neurons each. The latent dimension of the VAE was size 256. For Acrobot, the encoder and decoder had only one layer of size 256 each, and the latent dimension was 64.   This VAE is used as a part of both the DBCQ and WOP algorithms. We can also use it for imitation learning, by sampling actions directly from  to obtain Behavioral Cloning . We benchmark all of these techniques against vanilla -learning on the batch data . All -networks shared the same underlying architecture: three fully-connected layers of size [256, 128, 64], with ReLU activation between. All models were trained with the Adam optimizer .   For each experiment, we ran 50 trials of each model with a different random seed each time. The Behavior policy was trained for a total of 20,000 steps in the environment, so in the Full buffer condition offline agents saw 20,000 experience samples. The Behavior policy typically converged before 10,000 steps, so in the Expert demonstrator condition the offline agents received the last 10,000 experience samples from the trained agent. In the Concurrent condition, offline agents saw a moving window of 1000 samples, since the online learner only used the most recent 1000 samples in the buffer for learning. The learning rate was .001, , and  shows the results. Across conditions, we see that WOP is able to outperform Batch , imitation learning , DBCQ, and the original behavior policy. As expected, Imitation learning  underperforms other techniques when the batch contains noisy or inexpert experience samples. However, when the batch contains only expert trajectories, Batch  fails to learn, because the batch does not cover the full state-action space well, increasing extrapolation error. DBCQ matches or outperforms BC and Batch  in all scenarios. However, because DBCQ acts by sampling from  as learned by the BC model, its performance suffers when the batch data is noisy or imperfect. In contrast, WOP is able to learn to trade-off staying close to the prior and obtaining higher reward, and consistently outperforms all other algorithms in this environment.        Figure  shows the KL-divergence between RL policies and the prior language model throughout offline RL training. Without KL-regularization, the baseline RL models diverge quickly and continuously from the prior, losing information about realistic sequences. This figure also helps explain the poor performance of DBCQ in Table . The underlying -network in DBCQ does not directly integrate the prior. As -learning causes the model to diverge from the prior, the -estimates of language generated according to the prior become unrealistic, and selects unrealistic actions. This results in highly `diverse'  generated utterances. Note that since we operate in discrete action space, we could not include the perturbation model originally proposed by , which may be critical to achieving good performance with BCQ.   The total reward used to train the bots is a combination of the rewards described in Table . These rewards were selected based on the average z-score of rewards for utterances that were upvoted and downvoted. Figure  shows all the user rewards and that User Laughter and User Sentiment reward scores correlate with upvotes and downvotes. Figure  shows all the bot rewards with Bot Sentiment, Bot Laughter, Bot Convo. Repetition, and Bot Utterance Repetition as rewards that correlate with manual votes. Figure  shows the bot-user combined rewards, and that Word Similarity and USE Similarity are the rewards that correlate with manual up and downvotes.                To compute sentiment on short texts like conversation utterances, we leverage a state-of-the-art sentiment-detection model, which was trained on a massive amount of Twitter data to predict the emojis in tweets . Transfer learning from this model to other tasks showed that it was able to significantly outperform a series of sentiment, irony, and sarcasm benchmarks. This DeepMoji model outputs a probability distribution over 64 most-frequently used emojis as shown in Figure . After observing the performance of the model in detecting users' emotions in the domain of online chat, we define a set of weights over the emojis and calculate the weighted sum over an emotion embedding vector to derive a Sentiment reward which is higher for positive sentiment and lower for negative sentiment. These weights are shown in Figure  . We also compute a sentiment-transition reward using the same score based on whether the peak positive sentiment occurred later in the conversation than the peak negative sentiment, reasoning that sentiment should improve over the course of the conversation. The Bot Sentiment reward is the DeepMoji sentiment computed on the bot response, User Sentiment reward is the value computed on the user response, and the Sentiment Coherence reward is based on the similarly of user and bot sentiments.                          Based on prior work , we use the number of turns in the conversation as an indicator of the quality of the bot's performance. To distribute this reward over every utterance in the conversation, we take the total conversation length , and compute the discounted reward for utterance  as  . We also reward each utterance with the number of words and characters in the user's response, which we refer to as User Ans. Word Len and User Ans. Char Len.  We also examine how long bot responses are with the Bot Response Length reward.    Laughter has been shown to be very important to human affiliation  and solidarity . Therefore, we detect the number of occurrences of strings indicating laughter  in the user's response, and use this as a reward. Interestingly, we find that bots trained to maximize user laughter learn to be extremely supportive and cheerful compared to other bots .   Language style matching has been shown to be a strong predictor of relationship initiation and stability . While it would be ideal if our chatbots could intelligently adapt their conversation style to a new user, in reality most baseline dialog models struggle to maintain topic coherence, even over a few utterances . Therefore we reward semantic similarity between the user's input and the bot's response, to encourage the bot to stay on topic and produce reasonable answers. The Infersent Cornell Coherence and Infersent Reddit Coherence rewards are computed using a sentence embedding model trained on the Reddit and Cornell corpora respectively . We use the Universal Sentence Encoder  to compute the USE Similarity reward. We also directly compute word overlap as a reward as Word Similarity.    Asking questions is an important listening skill, and is linked to conversation management, attentiveness, and responsiveness . Therefore, we give the bot a reward of 0.5 if the utterance contains a question word , and an additional 0.5 if it contains a question mark. We refer to this reward as Bot Question.     After training the bots on these rewards, we noticed a shift in the distribution of their language towards more polite, cheerful, and supportive speech. Therefore, we designed post-hoc metrics to measure these qualities, which are based on counting whether a subset of phrases is present in an utterance.  Compliment phrases: you are beautiful, you are so beautiful, you're beautiful, you're beautiful,                    you are the best, you're the best, i like you, you're a good,                    you re a good, i love the way you  Politeness phrases: if I may; may I; please; thanks; no worries; if you don't mind; have a great day; I'm sorry.  Supportive phrases: you're right; you are right; you're not alone; you are not alone; congrats; that's a good idea; that is a good idea; you'll be fine; you will be fine; you'll be okay; you will be okay; it will get better; sorry you're going through; sorry you are going through; if it makes you feel better; if it makes you feel any better; keep your head up; keep it up; I'm in a similar situation; I am in a similar situation; you'll get it; you will get it; happy for you; I'm in the same boat; I am in the same boat; if you feel like you need to vent.               Cheerful phrases: nice to hear; happy; excited; really nice; glad; the best; great; good time; looking forward; beautiful.   We also want to discourage our bot from malicious or offensive language.  incorporate a Toxicity Classifier trained with data from the Toxic Comment Classification Challenge\footnote{} as a reward in the training hierarchical RL dialog models. We compute Toxicity reward scores using this classifier as Bot Toxicity .    Specificity within a conversation is valuable in avoid exchanging vacuous phrases back and forth. However building a chit-chat bot without a knowledge graph back-end limits the level of substance that can be incorporated into a conversation. We use the approach from  of computing normalize IDF to create more specificity in the conversation. We compute NIDF on both user  and bot  text.    While minimizing repetition is a common implicit goal of dialog systems, we will explicitly optimize for reducing repetition through repetition rewards. We compute utterance repetition by the number of non-unique words in each utterance as Bot Utterance Repetition Reward. We compute conversation repetition by the number of non-unique words in each conversation as Bot Convo. Repetition Reward. These rewards are negated since we want a higher reward score for less repetition. We also remove stop words in the computation of non-unique words.    To collect data from humans interacting with our bots, we built a platform for hosting deep neural network dialog models online on GPU for fast, real-time inference. Figure  shows an example of the interface, in which users are able to rate the bots after talking to them for at least three turns.     Note that during the chat, annotators can optionally click the up and down arrows beside each chatbot response to give feedback on the specific utterance. Once 6 or more turns of the conversation has taken place, participants may click ``Close Chat and Rate"" to get to the rating screen.   We train our RL models based on chat data collected on this platform. Currently, the conversations contain Personally Identifiable Information such as user name, age, location, etc. We obtained for IRB approval for this study and cannot release the conversations at this time in their current form.       The server was hosted on a Google Cloud Platform virtual instance with 64GB of RAM and a NVIDIA Tesla P100 graphics card. The backend was a Django program being served by NGINX and uWSGI. For simplicity, we opted to have the Django process import the chatbots into the same Python process as Django, rather than have the two connect to each other via other means such as sockets. This configuration decreased development time and increased reliability, but it would need to be revisited if the server needed to scale several orders of magnitude past what was required for this study. The current configuration was still able to support hundreds of simultaneous users and host more than 30 bots concurrently.  The chatbots were kept in a separate project from the Django project and maintained separately from the server code. Each chatbot extended an abstract class that defined key methods for the Django program to use, and was registered to a globally accessible dictionary via a decorator. The Django project was provided the path to the Chatbots project in its PYTHONPATH, so it could import the dictionary in which all the chatbot objects had been registered and use that to dynamically determine which chatbots were available and to access them in its views.  It is important to note that the chatbots used PyCUDA, and PyCUDA does not work in a multiprocessing environment. Because of this, uWSGI needed to be configured to only have one python process and to disable any attempt at multiprocessing. Furthermore, the chatbots required substantial startup times, so all chatbots are kept in memory at all times in the Django process. In order to keep all the chatbots in memory concurrently, we needed a very high amount of RAM on our server and opted for a 64GB virtual instance, and a GPU with 16GB RAM. This combination of CUDA to run the chatbots on the GPU with a high amount of RAM to keep all bots in memory at the same time resulted in incredibly fast server response times, with effectively no increase in response time when using the bots in requests compared to requests that did not.  For further information and instructions on server configuration, please read the server documentation available at . We hope that this platform will allow others to host their own bots and evaluate them in an interactive setting.   
"," How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning . We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions.  A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions.  We solve the challenge by developing a novel class of offline RL algorithms. These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.  We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches. The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.",190
"  Deep neural network-based  models have demonstrated remarkable performance on a multitude of text-to-text  as well as data-to-text generation tasks .  % To reach high performance, DNN models require a large training corpus which is normally not readily available. Indeed, it is rare to have a sufficiently large human-curated corpus of parallel data , and researchers have come up with heuristic rules to mine input-output pairs on a large scale .  No matter how powerful, DNN models are known to be sensitive to data artifacts  and pick on the noise in the training data.    While hallucinations have not been defined formally, the term is standardly used to refer to the generated content which is either unfaithful to the input, or nonsensical . In our work we are concerned with the former hallucination kind which is primarily caused by imperfect quality of the training data. %  If the data are noisy, how can one reduce the chances of hallucinating? % One may try to improve the quality of a dataset and clean it from phrases for which a clear support in the input is missing, or augment the input with information found only in the output. The former path is risky as it easily results in ungrammatical targets. The latter approach of enforcing a stronger alignment between inputs and outputs has been tried previously but it assumes a moderate amount of noise in the data .  % Alternatively, one can leave the data as is and try to put more pressure on the decoder to pay attention to the input at every generation step . This requires significant modifications to the model and may make it harder for the decoder to generate fluent and diverse text as found in the targets.   In contrast to the described approaches, our proposal is to train the model on the data as is without modifying the decoding  architecture but instead introduce a handle on the input side to control the degree of hallucination . With this ""hallucination knob"" one can minimize  the amount of unsupported information in the output during generation . The hallucination or noise degree of every training instance is estimated separately and converted into a categorical value which becomes part of the input, like in a controlled generation setting . We introduce a simple technique to measure the amount of noise in every training example which is based on the intuition that whenever a language model  has a smaller loss than a conditional generator during forced-path decoding, it is a good signal that the next token cannot be explained by the input. % .  We consider a particularly noisy dataset, WikiBio , which has been found to have extra information in 62\% of the references  and where 1:1 correspondence between the input and the output never holds . Our models demonstrate superior performance to the model of  which reports SoTA BLEU results on WikiBio.  % In sum, our contributions are  a novel idea of controlling hallucinations which requires no modification to the model,  a data- and task-independent technique of implementing this idea and  three-way evaluation with human raters which confirms that faithfulness does not need to be traded for coverage.        Comparing the two methods of estimating the amount of hallucinations in a target, for applications where the input and the output use the same vocabulary with a comparable term distribution the overlap method may be better as it has a clear foundation.    The LM-based method that we proposed has an important advantage that it makes no assumptions about the data. In our WikiBio experiment it also produced better results in the human evaluation, presumably because it allowed for paraphrasing and straightforward inferences. For example, the target ozren nedoklan was a yugoslav footballer and manager. has a high  score because the source table has no occupation field and does not mention yugoslav. The  score of that example is zero because footballer and manager can be inferred from the names of the clubs and the manageryears fields in the source.   It should be emphasized that alternative methods of detecting noise can be explored and may perform better in the controlled-hallucination framework. For example, it is possible to measuring target-source similarity in an embedded space or use word alignment tools to find unsupported information.   While here we have focused on eliminating hallucinations, one can think of applications where one is interested in generating adversarial sentences which sound fluent but are guaranteed to include unsupported information. Figure  shows how the amount of hallucinations in the output increases following the value of the hallucination knob.   It is striking that while all the models tested outperform  in terms of PARENT and human evaluation scores, none could approach its BLEU performance. We do not have an explanation of why this is so but note that our results are in line with the review by  who concludes that BLEU is an inappropriate metric for generation tasks other than MT.    One may wonder whether an even simpler approach of controlling for length would deliver a similar reduction in hallucinations. Indeed, hallucinations and length are expected to correlate, and shorter length should result in fewer hallucinations. However, as pointed out in Sec.\ , drastically reducing hallucinations may be possible without any control mechanism and can be achieved, at least on WikiBio, with templates. The main challenge lies in doing so without a big drop in informativeness, that is, in coverage of input fields. Comparing the outputs of  with those of , and both with those of , we note that the ranking in terms of average sentence length  coincides with the ranking in terms of coverage : 17.2, 17.8, 18.7. While  may associate the special hal\_0 token with the shortest 20\  of the training data, for  this token is apparently associated with a different selection of 20\  of the data points.     We presented a simple but powerful idea of controlling hallucinations which are caused by the noise in the training data and proposed two ways of detecting such noise.    We demonstrated that it is possible to reduce the amount of hallucinations at no coverage cost by informing the model about how noisy every source-target example is and without changing the model architecture. Importantly, this was done without making any assumptions about the data.   In an evaluation with humans we showed that the faithfulness of generated sentences can be significantly improved at no loss in fluency or coverage. The results we reported on the noisy WikiBio dataset improve upon the prior work.     
"," Neural text generation  demonstrates remarkable performance when training data is abundant which for many applications is not the case.  To collect a large corpus of parallel data, heuristic rules are often used but they inevitably let noise into the data, such as phrases in the output which cannot be explained by the input.  Consequently, models pick up on the noise and may hallucinate--generate fluent but unsupported text.  Our contribution is a simple but powerful technique to treat such hallucinations as a controllable aspect of the generated text, without dismissing any input and without modifying the model architecture. On the WikiBio corpus , a particularly noisy dataset, we demonstrate the efficacy of the technique both in an automatic and in a human evaluation.",191
"   %  %    %Added value of \atomicTT{}: 1) diversity in terms of vocab, style, concepts, 2) higher quality   %\ronan{Cite publications that used ATOMIC in a downstream application}  Commonsense understanding % knowledge modeling and reasoning remain long-standing challenges in general artificial intelligence.  % However, in the subfield of natural language processing, the last few years have brought tremendous progress in AI applications.  However, large-scale language models have brought tremendous progress in the sub-field of natural language processing.  Such large-scale language models   trained on extreme-scale data have been shown to effectively adapt to diverse downstream tasks, achieving significant performance gains across natural language benchmarks .  %%%%%%%OLD %%%%%% Despite these successes, these models have been shown to learn brittle representations, often from only simple surface word associations , which routinely lead them to make nonsensical predictions detached from common sense . Interestingly, as these models have grown larger , their benchmark performance has continued to improve  despite limited conceptual improvements,  %leading many researchers to conjecture as to  leaving open questions regarding  the source of these remarkable generalization properties.   Recent work has hypothesized that many of these performance gains could be a result of language models being able to memorize facts in their parameters during training  that can be leveraged at evaluation time. As a result, a new paradigm of language models as knowledge bases has emerged . In this setting, language models are prompted with natural language prefixes or questions, and they express knowledge through language generation. The initial success of this paradigm for representing commonsense knowledge  %, combined with limited examples of LMs being successfully integrated with structured commonsense knowledge resources for downstream application,  has led to the optimistic claim that language models comprehensively encode commonsense knowledge, and remove the need for structured knowledge resources. %\antoine{run-on sentence, need to shorten}  We take a more skeptical view of this capacity of language models -- Does scaling up language models actually endow them with commonsense knowledge? While language models can successfully express certain types of knowledge, their best results are observed in narrowly specific conditions -- we show  that they perform better when evaluated on knowledge bases that prioritize ontological relations and whose examples resemble language-like assertions .\footnote{An observation supported by 's \gpttt{} model, whose best few-shot performance on commonsense knowledge benchmarks comes on the PhysicalIQA  and HellaSwag  datasets.} Consequently, the types of knowledge that can be directly accessed through the language model's interface remains limited.  %Consequently, while these methods are encouraging, they also demonstrate that the limited interface of language models precludes them from expressing the diversity of commonsense knowledge that must be accessible for robust commonsense reasoning.  %  However, prior work has also shown that training language models on knowledge graph tuples leads them to learn to express their implicit knowledge directly , allowing them to provide commonsense knowledge on-demand. These adapted knowledge models have exhibited promising results on commonsense benchmarks compared with methods that require linking entities to knowledge graphs . Inspired by these successes, we propose a dual use for commonsense knowledge bases going forward: as static graphs that can be linked to for discrete knowledge access, and as resources for adapting language models to hypothesize commonsense knowledge about un-annotated entities and events.   %%%%%%% OLD %%%%%%%% As a result, recent work has investigated augmenting language models with retrieval mechanisms that query commonsense knowledge graphs  for related facts to the entities mentioned in text. The idea behind these approaches is that access to these facts and the potential to compose them with learned reasoning functions would allow models to more robustly leverage commonsense knowledge to make predictions. Despite the premise of these approaches, they are unfortunately limited by the coverage of the resources used to provide commonsense knowledge facts , motivating the need for new, high coverage resources in the short-term.   % Option 1 % With this second purpose in mind, we shift the design goals of commonsense knowledge resources toward prioritizing pieces of knowledge that are not readily accessible in pretrained language models.  % Option 2 With this second purpose in mind, we propose evaluating commonsense knowledge resources based on the complementary information they can bring to pretrained language models. We construct \atomicTT{}, a new, high-quality knowledge graph with M commonsense knowledge tuples across  commonsense relations. We compare \atomicTT{} with respect to its coverage and accuracy in competition with other highly used CSKGs, such as  is able to cover more correct facts about more diverse types of commonsense knowledge than any existing, publicly-available commonsense knowledge resource. However, our results also indicate that there remains a large amount of exclusivity between these KGs, highlighting the challenge of creating resources that cover the scale and diversity of general commonsense knowledge.   %%%%%%% OLD %%%%%%Meanwhile, a new paradigm has emerged that proposes that large-scale language models implicitly learn to represent large amounts of factual and commonsense knowledge . While these methods are promising, they also show that the limited interface of language models precludes them from producing commonsense knowledge robustly. However, using knowledge graph tuples as additional training signal allows these model to be better adapted to representing knowledge . Furthermore, the use of these knowledge models to provide commonsense knowledge on-demand has shown promising results over static knowledge graphs . Consequently, in this work, we propose evaluating commonsense knowledge resources on a new, second purpose: whether they can be used to repurpose language models for commonsense modeling.   Furthermore, we formalize the  across different seed language models and training knowledge graphs, and evaluate the commonsense knowledge hypothesized by these adapted knowledge models. %Our results indicate that this purpose is a promising evaluation for commonsense resources, as  as a transfer resource leads to  as a transfer resource allows language models to learn richer commonsense knowledge representation than training with other resources.   %  %    Key Contributions:  In summary, we make three key contributions in this paper. We present \atomicTT{}---a new commonsense knowledge graph covering social, physical, and eventive aspects of everyday inferential knowledge . Next, we compare \atomicTT{} with other prominent CSKBs head-to-head and show that our new symbolic knowledge graph is more accurate than any current CSKB  . Finally, we show that our new neural knowledge model -\atomicTT{} successfully transfers \atomicTT{}'s declarative knowledge to beat \gpttt{}, the largest pre-trained language model, in spite of using ~400x fewer parameters  . This demonstrates the utility and importance of high-quality symbolic knowledge provided by \atomicTT{} to generalize on commonsense information that LMs cannot expressively capture on their own .  % * Our new symbolic knowledge graph ATOMICTT is superior in accuracy and coverage to the currently existing large-scale knowledge graphs .  % * our neural knowledge model COMET-ATOMICTT successfully transfers the ATOMICTT's declarative knowledge to beat even the most impressively large pretrained model, GPT-3 . This demonstrates LMs, no matter its size, can benefit from the symbolic knowledge provided by high quality KB like ATOMICTT.        Our conclusions on this subject are mixed and hinge on the ambiguous meaning of what it means to encode knowledge. Despite the conclusions of prior work , our results in Table are clear that language models fail to express large varieties of knowledge when prompted for it in a zero-shot manner. When converted to  and \gptxl{} on \atomicTT.   However, the evaluation tuples are adversarially selected to not include head entities that were in the training set. The model must generalize its learned representations of relations to entities it has not observed these relationships for  at any point  during fine-tuning, meaning the representation of these entities is solely formulated from learning language. As a result, language models may still encode this knowledge in their parameters, even if they are not capable of expressing it directly. With this framing in mind, the COMET training paradigm proposed by  can perhaps be viewed less as a means of learning knowledge from KGs, and more as a method of learning an interface for language models to hypothesize encoded knowledge through language generation. We look forward to future work in this space that attempts to disentangle these two ideas.     Commonsense knowledge graphs are uniquitous tools for natural language processing agents that must perform commonsense reasoning.  Based on our results in Section, we outline desiderata for the design and development of future commonsense knowledge graphs. Because certain types of knowledge are already encoded and expressible by pretrained language models, CSKG designers should focus on collecting examples and categories of knowledge that are less likely to be known by language models. For example, of the 378 test tuples evaluated by the \gptxl{} zero-shot model that contained the \HinderedBy{} relation, only 1.3\  were deemed plausible by human raters -- jumping to 85\  plausibility for  -- pointing to an advantage in constructing \atomicTT{} with this relationship in mind .  Second, commonsense knowledge resources should be designed with the goal of accuracy and relationship coverage. Because language models exhibit powerful adaptation , they can generalize many commonsense relationships as long they have examples on which to train. Consequently, we should construct commonsense resources that encapsulate larger numbers of relations so the knowledge in pretrained language models can be grounded to a variety of relationships. However, language models also benefit from learning from precise examples. Being able to train on a large collection of examples from \transomcs  did not allow  and .     In this work, we formalize a use for commonsense knowledge graphs as transfer learning tools for pretrained language models. With this new purpose, we hypothesize that commonsense knowledge graphs should be designed to contain knowledge that is not already expressible by language models without difficulty . Consequently, we propose \atomicTT, a novel commonsense knowledge graph containing tuples whose relations are specifically selected to be challenging for pretrained language models to express. Our empirical studies demonstrate that \atomicTT contains high-accuracy knowledge tuples across multiple novel relations not found in existing CSKGs or expressible by LMs. Furthermore, we show that \atomicTT can be effectively used as a training set for adapting language models as knowledge models to generate high quality tuples on-demand.    \clearpage     
"," % Check out this new knowledge graph! % Storyline: %  %   Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs  has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge.  In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them.   With this new goal, we propose \atomicTT{}, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that \atomicTT{} is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 , while impressive, remains $ despite using  over 430x fewer parameters.  % useful they are for training knowledge models that can generate relevant representative knowledge for new, unseen entities.  % In this work, we propose \atomicTT{}, a new knowledge graph of general-purpose commonsense knowledge facts. To evaluate its utility in comparison to existing resources, we perform the first large-scale pairwise study of commonsense knowledge graphs on coverage and precision. Finally, we posit that a new use for commonsense knowledge graphs is their ability to allow large-scale language models to learn to represent knowledge implicitly. We propose a new evaluation for testing knowledge graphs on how useful they are for training knowledge models that can generate relevant representative knowledge for new, unseen entities.",192
"   Despite its successes, neural machine translation  still has unresolved problems. Among them is the problem of rare words, which are paradoxically very common because of Zipf's Law. In part, this is a problem intrinsic to data-driven machine translation because the system will inevitably encounter words not seen in the training data. In part, however, NMT systems seem particularly challenged by rare words, compared with older statistical models.   One reason is that NMT systems have a fixed-size vocabulary, typically 10k--100k words; words outside this vocabulary are represented using a special symbol like \unk{}. Byte pair encoding  breaks rare words into smaller, more frequent subwords, at least allowing NMT to see them instead of \unk{} . But this by no means solves the problem; even with subwords, NMT seems to have difficulty learning translations of very rare words, possibly an instance of catastrophic forgetting .  Humans deal with rare words by looking them up in a dictionary, and the idea of using dictionaries to assist machine translation is extremely old. From a statistical perspective, dictionaries are a useful complement to running text because the uniform distribution of dictionary headwords can smooth out the long-tailed distribution of running text. In pre-neural statistical machine translation systems, the typical way to incorporate bilingual dictionaries is simply to include them as parallel sentences in the training data. But , this does not work well for NMT systems.  We are aware of only a few previous attempts to find better ways to incorporate bilingual dictionaries in NMT. Some methods use dictionaries to synthesize new training examples .  extend the model to encourage it to generate translations from the  dictionary.  constrain the decoder to generate translations from the dictionary. What these approaches have in common is that they all treat dictionary definitions as target-language text, when, in fact, they often have properties very different from ordinary text. For example, CEDICT defines \zh{濮濄倛鍤  as ``'' which cannot be used as a translation. In the case of a monolingual source-language dictionary, the definitions are, of course, not written in the target language at all.  In this paper, we present an extension of the Transformer  that ``attaches'' the dictionary definitions of rare words to their occurrences in source sentences. We introduce new position encodings to represent the nonlinear structure of a source sentence with its attachments. Then the unmodified translation model can learn how to make use of this attached information. We show that this additional information yields improvements in translation accuracy of up to 3.1 BLEU. Because our method does not force dictionary definitions to be treated as target-language text, it is generalizable to other kinds of information, such as monolingual source-language dictionaries, which yield smaller improvements, but still as much as 0.7 BLEU.        [3]{\mbox{{@{}c@{}}\zh{#1} \\       {%     [x=2.2cm]     \tikzset{every node/.append style={anchor=north}}     ;     \textrm{PE}[1] \\ + \\ \textrm{WE}\left[\text{\zhen{婢堆冾啀}{d鑴縥i鑶﹠{everyone}}\right]};     \textrm{PE}[3] \\ + \\ \textrm{WE}\left[\text{\zhen{閻儵浜緘{zh澧╠鑴縪}{knows}}\right]};     \textrm{PE}[5] \\ + \\ \textrm{WE}\left[\text{\zhen{濮濓絽婀獇{zh鐚玭gz鑴縤}{is}}\right]};     \textrm{PE}[4] \\ + \\ \textrm{WE}[\unk] \\ + \\ \textrm{DPE}[1] \\ + \\  \textrm{WE}\left[\text{the}\right]};     \textrm{PE}[4] \\ + \\ \textrm{WE}[\unk] \\ + \\ \textrm{DPE}[3] \\ + \\ \textrm{WE}\left[\text{Sea}\right]\textrm{WE}[f]f\textrm{PE}[p]p\textrm{DPE}[q]q$ within a dictionary definition. The rare word \zh{濮濈粯鎹  is replaced with \unk{} and defined as . The words of the definition are encoded with both the position of the defined word  and their positions within the definition.}           In this paper, we presented a simple yet effective way to incorporate dictionaries into a Transformer NMT system, by attaching definitions to source sentences to form a nonlinear structure that the Transformer can learn how to use. We showed that our method can beat baselines significantly, by up to 3.1 BLEU. We also analyzed our system's outputs and found that our model is learning to select and adapt parts of the definition, which it does not learn to do when the dictionary is simply appended to the training data. We also found that our method has some potential to work with monolingual dictionaries.  
"," Despite advances in neural machine translation  quality, rare words continue to be problematic. For humans, the solution to the rare-word problem has long been dictionaries, but dictionaries cannot be straightforwardly incorporated into NMT. In this paper, we describe a new method for ``attaching'' dictionary definitions to rare words so that the network can learn the best way to use them. We demonstrate improvements of up to 3.1 BLEU using bilingual dictionaries and up to 0.7 BLEU using monolingual source-language dictionaries.",193
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. % %  % . %     %  %     % % final paper: en-us version  %     % %     %   % space normally used by the marker %     % This work is licensed under a Creative Commons  %     % Attribution 4.0 International License. %     % License details: %     % \url{http://creativecommons.org/licenses/by/4.0/}. % }   % 1. 鐟欙綁鍣 CCG閿涘奔浜掗崣 CCG 閻ㄥ嫰鍣哥憰浣 % 2. CCG parsing 閻ㄥ嫰鍣搁悙鐟版躬娴 supertagging閵嗗倿娓剁憰浣割嚠 contextual information 閺堝鐦潏鍐ㄣ偨閻 encode 閻ㄥ嫭鏌熷▔鏇樺倸澧犳禍铏规畱閺傝纭堕敍灞间簰閸欏﹤鐪梽鎰剁礄閸欘亪鍣伴悽 powerful encoder閿涘本鐥呴張澶嬪赴濮瑰倿顤傛径 contextual feature 閻ㄥ嫪缍旈悽銊ф畱閻梻鈹掗敍 % 3. n-gram 閺勵垯绔存稉顏呮箒閺佸牏娈 contextual feature閿涘苯褰查懗钘夘嚠 supertagging 閺堝鏁ら敍鍫熷絹娓氭稑褰查懗鐣屾畱鐠囧秳绗岀拠宥勭闂 combination 閻ㄥ嫭娈粈鐚寸礆 % 4. 閹存垳婊戦惃 model   % 鐟欙綁鍣 CCG閿涘矁鐦濆Ч鍥瘱閻ｈ揪绱檚upertag閿涘婀伴煬顐㈠瘶閸氼偂绨℃稉鏉跨槣閻ㄥ嫬褰炲▔鏇炴嫲鐠囶厺绠熼惃鍕繆閹 Combinatory categorial grammar  is a lexicalized grammatical formalism, where the lexical categories  of the words in a sentence provide informative syntactic and semantic knowledge for text understanding. % 閹垫禒 ccg閿涘瞼澹掗崚顐ｆЦ supertagging 瀵板牊婀侀悽 Therefore, CCG parse often provides useful information for many downstream natural language processing  tasks such as logical reasoning  and semantic parsing . To perform CCG parsing in different languages, % 閸 ccg parsing 閸掑棔琚卞銉ｄ靠upertagging 鏉╂瑤绔村銉︽付闁插秷顩 most studies conducted a supertagging-parsing pipline , in which their main focus is the first step, and they generated the CCG parse trees directly from supertags with a few rules afterwards. % which is known as ``almost parsing''   % with essential CCG information for a sentence and one can generate its parse directly from supertags with a few rules. % supertagging 闂囩憰 contextual information %  Building an accurate supertagger in a sequence labeling process requires a good modeling of contextual information. %  Recent neural approaches to supertagging mainly focused on leveraging powerful encoders with recurrent models , with limited attention paid to modeling extra contextual features such as word pairs with strong relations. % Graph convolutional networks  is demonstrated to be an effective approach to model such contextual information between words in many NLP tasks ; thus we want to determine whether this approach can  also help CCG supertagging.  However, we cannot directly apply conventional GCN models to CCG supertagging because in most of the previous studies the GCN models are built over the edges in the dependency tree of an input sentence. As high-quality dependency parsers are not always available, we do not want our CCG supertaggers to rely on the existence of dependency parsers.  %  Thus, we need another way to extract useful word pairs to build GCN models. For that, we propose to obtain word pairs from frequent chunks  in the corpus, because those chunks are easy to identify with co-occurrence counts. %  %  %  % Such features, which may come from n-grams or dependency parsing results, are demonstrated to be helpful for many NLP tasks , and they are expected to enhance CCG supertagging as well. % Among all such features, the ones from n-grams are more attractive since n-grams are easy to obtain and also provide word relation cues, while dependency parsing results are exactly the goal of CCG and thus conflicts with the problem setting. % % As for the model to encode such features, graph convolutional networks  is one of the promising choices although it is often built over the dependency or semantic parse of the input text. %However, GCN suffers from the limitation of obtaining such parsing results, which is exactly the goal of CCG and thus conflicts with the problem setting. % %So that one and they are expected to enhance CCG supertagging. %especially the n-gram ones because they are easy to obtain and can provide cues for word-word combination if they are appropriately modeled. % %\textcolor{blue}{ %To leverage such contextual features, graph convolutional networks  is one of the privileging approaches to do so, where the graph is often built over the dependency or semantic parsing results of the input text. %However, GCN suffers from the limitation of obtaining such parsing results, which is exactly the goal of CCG and thus conflicts with the problem setting. %} % \textcolor{red}{ % Consider that graph convolutional networks , which is an effective solution to learn contextual information and is demonstrated to be useful in many other NLP tasks , can be potentially useful for CCG supertagging.} % , such as semantic role labeling , sentiment classification , and question answering . %input words based on the results of dependency or semantic parsing of the input texts, which may not be an appropriate way to construct graph for CCG, %since the task itself is about parsing. % \textcolor{blue}{ % Therefore, an appropriate way to construct the graph is required for CCG and n-grams could potentially be helpful since they carry contextual information and provide a group of words in which  % its containing words  % they may have strong relationship with respect to word-word combination if the n-grams are appropriately selected. % % } % Previous studies using GCN often build the graph over the dependency or semantic parsing results of the input text, suffering from the limitation of obtaining such parsing results, which is exactly the goal of CCG thus conflicts with the problem setting. % To appropriately learn from n-grams, one requires the GCN to be able to distinguish different word pairs because such information in n-grams are not explicitly structured as that in dependency parses. %In addition, Because existing GCN models are limited in treating all word pairs equally, %while identifying and learning from essential units are important for syntactic tasks, we propose an adaptation of conventional GCN for CCG supertagging. %especially when the graph are not constructed on dependencies. %  % Inspired by that n-grams can carry contextual information and provide a span in which its containing words may have strong relationships if the n-grams are appropriately selected, we build the graph upon well selected n-grams. % , especially the ones containing words with strong relationships between each other,  % % n-gram 閺勵垯绔存稉顏堝櫢鐟曚胶娈 contexutal feature % Consider that n-grams are conventionally used as a simple yet effective method to represent contextual features in many NLP tasks %in which powerful encoders are used  % , % 閸ョ姵顒濋敍瀹-gram 鐎 supertagging 娑旂喐婀侀悽顭掔礉鐏忋倕鍙鹃弰顖炲亝娴滄稖鍏樻径鐔虹矋閹存劗鐓拠顓犳畱 n-gram閿涘矁鍏樻径鐔稿絹娓氭稑鍙ф禍搴ょ槤娑撳氦鐦濇稊瀣？缂佸嫬鎮庨崗宕囬兇閻ㄥ嫪淇婇幁顖ょ礉閺堝濮禍 supertagging % they are also expected to serve as effective contextual features for CCG supertagging, where they, \textcolor{blue}{especially the ones containing words with strong relationships between each other,} % that are valid phrases,  % should provide plausible cues on potential combinations among words. % 閻掓儼宀嬬礉婵″倷缍嶉張澶嬫櫏閸︽澘鍩勯悽銊ㄧ箹娴 n-gram 娓氭繃妫弰顖欑娑擃亝瀵幋姗堢礉閸ョ姳璐熼柇锝勭昂娑撳秹鍣哥憰浣烘畱 n-gram 閸欘垯浜掔拠顖氼嚤 supertagger %\textcolor{blue}{ % However, it is not trivial to appropriately learn from n-grams for syntactic tasks, % where one needs to identify informative n-grams out of all possible combinations of words for the task. %since the unimportant ones carrying misleading cues for the combination may hurt the performance of a supertagger. %}   % [t] %      %      %     \vskip -1.2em %       % 閹垫禒銉ь儑娑撳顔岄柌宀勬桨鐏忚精顩﹂崨鐓庣安鏉╂瑩鍣烽惃鍕敶鐎圭櫢绱濈悰銊с仛閸戠儤娼甸幋鎴滄粦閺冦垼鍏橀悽鈺猤ram閿涘苯寮甸懗鐣屾暏GCN缂佹獢gram瀵ょ儤膩 % 閹存垳婊戦幓鎰毉 channeled attention 閺 model 鏉╂瑤绨 n-gram %To address these problems, In this paper, we propose attentive GCN  for CCG supertagging, where its input graph is built based on chunks  extracted with unsupervised methods. % In this paper, we propose attentive GCN  for CCG supertagging, where its input graph is built upon word groups suggested by high confident n-grams extracted from unsupervised methods. % , where the graph is constructed on word groups. %which follows the sequence labeling paradigm. % 鐠囷妇绮忔禒瀣矝婵″倷缍嶅銉ょ稊閿涘矂顩婚崗鍫濐嚠 n-gram 閸掑棛绮 % Inspired by that n-grams can carry contextual information and provide a span in which its containing words may have strong relationships if the n-grams are appropriately selected, we build the graph upon the n-grams in the sentence, where an edge will be added to a pair of words if they are in the same n-gram. In detail, two types of edges in the graph are introduced to model word relations within and across chunks %for the word groups to model the word-word relation within and cross the groups. % we build the graph over the words upon the n-grams in the input sentence, where an edge will be added to a pair of words if they are in a span suggested by the same n-gram. % % For edges within a group, feed-forward attention is applied  and an attention mechanism is applied to GCN to weight those edges. %and discriminately learn from them through the edges. %In addition, for each word, a attention mechanism is used % to weight the contextual information carried by all its associated words  according to their contribution to the tagging process. %  In doing so, different contextual information are discriminatively learned to facilitate CCG supertagging without requiring any external resources. % , with the \textcolor{blue}{within and cross chunk relations} % local and global word relations  % weighted on our in-chunk and cross-chunk edges, respectively. %Moreover, the way of building the graph requires no external resources  %suggested by high confident n-grams is learned by A-GCN through the in-group edges; and long distance relations among groups are also leveraged by cross-group edges. %Therefore, a hierarchical structure of word relations are built  %Besides, our approach proposes a novel self-supervised method to build the graph for GCN, where no extra parsing results  are required as extra input. % , but also our attentive GCN is able to discriminately learn from the contextual information carried by different words.} % In the proposed attention, n-grams associated to each word in the input texts are firstly categorized into different groups according to their length,  % 閻掕泛鎮楀В蹇庨嚋 n-gram 閺夈儱濮為弶 % and then fed into a specific channel of attentions according to their groups, so that the n-grams are weighted separately in each group according to their contributions to the supertagging process. % 婵傝棄顦╅敍宀顑囨稉閺勵垰灏崚顐＄啊闁插秷顩﹂惃鍕嫲娑撳秹鍣哥憰浣烘畱 n-gram閿涙稓顑囨禍灞炬Ц閼宠棄顧勬禒搴ㄥ亝娴滄盯鍣哥憰浣烘畱闂 n-gram 娑擃厼顒熼崚鐗堟纯鏉╂粏绐涚粋鑽ゆ畱 context information % In doing so, not only important n-grams are distinguished, but also can our approach discriminatively learn from n-grams in different length, where the infrequent and long n-grams carrying important long range contextual information are appropriately modeled without being influenced by the frequent short ones. %  % 鐎圭偤鐛欑拠浣规閺堝鏅 The validity of our approach is demonstrated by experimental results on the CCGbank , where state-of-the-art performance is obtained for both tagging and parsing.               In this paper, we propose A-GCN for CCG supertagging, with its graph built from chunks extracted from a lexicon.    We use two types of edges for the graph, namely, in-chunk and cross-chunk edges for word pairs within and across chunks, respectively, and propose an attention mechanism   where an attention mechanism is used to enhance the model.     Specifically,  we construct the graph based on word groups suggested by high confident n-grams where in-group and cross-group edges are used and A-GCN is able to learn from the word groups through those edges.   an attention mechanism is proposed  to distinguish the important word pairs according to their contribution to CCG supertagging.   婵傝棄顦╅敍宀顑囨稉閺勵垰灏崚顐＄啊闁插秷顩﹂惃鍕嫲娑撳秹鍣哥憰浣烘畱 n-gram閿涙稓顑囨禍灞炬Ц閼宠棄顧勬禒搴ㄥ亝娴滄盯鍣哥憰浣烘畱闂 n-gram 娑擃厼顒熼崚鐗堟纯鏉╂粏绐涚粋鑽ゆ畱 context information   Therefore, not only the important n-grams are distinguished, but also can our approach discriminatively learn from n-grams in different length, especially the long and infrequent ones that carry important long distance contextual information and could be influenced by the majority voting effect.   Therefore, context features are appropriately modeled and the GCN can discriminatively learn from them.     The effectiveness of our approach to CCG supertagging as well as to parsing is demonstrated by the experimental results and the ablation study on the English CCGbank, where state-of-the-art performance is obtained. Experimental results and the ablation study on the English CCGbank demonstrate the effectiveness of our approach to CCG supertagging, where state-of-the-art performance is obtained on both CCG supertagging and parsing.   Further analysis is performed to investigate using different types of edges, which reveals their quality and confirms the necessity of introducing attention to GCN for CCG supertagging.  For future studies,  we plan to explore other approaches to building the graph as well as performing end-to-end   analyze the effect of them on  CCG supertagging and parsing.  
","  % supertagging 閻庣敻娑氳壘 CCG parsing 闂傚牏鍋涢悥鍫曟煂瀹ュ牜娲 Supertagging is conventionally regarded as an important task for combinatory categorial grammar  parsing, where effective modeling of contextual information is highly important to this task. % 闂傚嫨鍊撶花鈩冩媴鐠恒劍鏆忛柡鍥ㄦ綑瀹搁亶鎯 encoder闁挎稑鏈惁顔戒繆 biLSTM闁挎稑鑻晶鐘崇閸濆嫷鍤犲ù supertagging 閺夆晜鐟ら柌 task 闁告垹濮崇粻顔尖柦閿涘嫭鏆忛柛鎺楊暒缁牊绋婇崼婵嗙劶闁 context feature闁挎稑鑻畵 n-gram However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders . % 闁哄牜鍓氶弸鍐晬鐏炴儳鐏夊ù鐙鍓氳ぐ渚宕 channeled n-gram attention 闁哄鍎遍ˇ鈺呮偠閸℃氨绠瑰☉鎿冧邯濡埖锛 In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information. %  Specifically, we build the graph from chunks  extracted from a lexicon and apply attention over the graph, so that different  % word relations  word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly. % 閻庡湱鍋ら悰娆戠磼閹惧浜悶娑栧妽濡叉垿鏁嶇仦鎯х亯濞寸媭鍓涘▓鎴﹀棘鐟欏嫮銆婇柡鍕靛灡濠渚寮崼銏＄暠 The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies % , as well as strong baselines from existing toolkits,  in terms of both supertagging and parsing. %  Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.\footnote{Our code and models for CCG supertagging are released at \url{https://github.com/cuhksz-nlp/NeST-CCG}.}",194
" Pre-trained Transformers  have lead to state-of-the-art results on a wide range of NLP tasks, for example, named entity recognition, relation extraction and question answering, often approaching human inter-rater agreement .  These models have also been demonstrated to learn effective cross-lingual representations, even without access to parallel text or bilingual lexicons .  Multilingual pre-trained Transformers, such as mBERT and XLM-RoBERTa , support surprisingly effective zero-shot cross-lingual transfer, where training and development data are only assumed in a high resource source language , and performance is evaluated on another target language. 	 Because no target language annotations are assumed in this setting, source language data is typically used to select among models that are fine-tuned with different hyperparameters and random seeds.  However, recent work has shown that English dev accuracy does not always correlate well with target language performance .  In this paper, we propose an alternative strategy for model selection in a zero-shot setting.  Our approach, dubbed Learned Model Selection , learns a function that scores the compatibility between a fine-tuned multilingual transformer, and a target language. The compatibility score is calculated based on features of the multilingual model's learned representations and the target language.  A model's features are based on its own internal representations; this is done by aggregating representations over an unlabeled target language text corpus.  These model-specific features capture information about how the cross-lingual representations transfer to the target language after fine-tuning on source language data.  In addition to model-specific representations, we also make use of learned language embeddings from the lang2vec package , which have been shown to encode typological information, for example, whether a language has prepositions or postpositions.  To measure compatibility between a multilingual model's fine-tuned representations and a target language, the model- and language- specific representations are combined in a bilinear layer.  Parameters of the scoring function are optimized to minimize a pairwise ranking loss on a set of held-out models, where the gold ranking is calculated using standard performance metrics, such as accuracy or F, on a set of pivot languages .  LMS does not rely on any annotated data in the target language for meta-learning or hyperparameter tuning, yet it is effective in learning to predict whether a multilingual model's representations are a good match for a specific target language.    In experiments on five well-studied NLP tasks , we find LMS consistently selects models with better target-language performance than those chosen using English dev data.  Appendix  demonstrates that our framework supports multi-task learning, which can be helpful in settings where some target-language annotations are available, but not for the desired task.  Finally, we show that LMS generalizes to both mBERT and XLM-RoBERTa in Appendix .     In this paper, we presented a meta-learning approach to model selection for zero-shot cross-lingual transfer.  We showed that our approach improves over the standard practice of model selection using source language development data.  Experiments on five well-studied NLP tasks show that by inspecting internal representations, our method consistently selects better models. LMS also achieves comparable results to the slower and more expensive alternative of annotating small amounts of target-language development data.           We thank Wei Xu for helpful feedback.    Use unnumbered third level headings for the acknowledgments. All   acknowledgments, including those to funding agencies, go at the end of the paper.        
"," Transformers that are pre-trained on multilingual text corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer learning results.  In the zero-shot cross-lingual transfer setting, only English training data is assumed, and the fine-tuned model is evaluated on another target language.  No target-language validation data is assumed in this setting, however substantial variance has been observed in target language performance between different fine-tuning runs.  Prior work has relied on English validation/development data to select among models that are fine-tuned with different learning rates, number of steps and other hyperparameters, often resulting in suboptimal choices.  To address this challenge, we propose a meta-learning approach to model selection that uses the fine-tuned model's own internal representations to predict its cross-lingual capabilities.  In extensive experiments we find that our approach consistently selects better models than English validation data across five languages and five well-studied NLP tasks, achieving results that are comparable to small amounts of target language development data.\footnote{We will make our code and data available on publication.}  %We further demonstrate that our method can benefit from pooling data across tasks when auxiliary annotations are available in the target language.",195
"  . }  Definition Extraction refers to the task in Natural Language Processing  of detecting and extracting a term and its definition in different types of text. A common use of automatic definition extraction is to help building dictionaries , but it can be employed for many other applications. For example, ontology building can benefit from methods that extract definitions , whilst the fields of definition extraction and information extraction can employ similar methodologies. It is therefore normal that there is growing interest in the task of definition extraction.  This paper describes our system that participated in two of the three subtasks of Task 6 at SemEval 2020 , a shared task focused on definition extraction from a specialised corpus. Our method employs state-of-the-art neural architectures in combination with automatic methods which extend and clean the provided dataset.  %Task 6 at SemEval 2020  is a shared task for definition extraction from a specialised corpus, tailoured specifically to the needs of definition extraction. This paper describes the RGCL team system that works on all three subtasks of the shared task. We employ state-of-the-art neural architectures and combine them with simple automatic methods to extend and clean the provided dataset where appropriate.  The remaining parts of this paper are structured as follows. First, we present related work in the area of definition extraction and the related field of relation extraction . The three subtasks and the dataset provided by the task organisers are described in Section . Next, we describe our system , followed by the results of the evaluation  and a final conclusion .      We have presented the system the RGCL team has prepared for the SemEval-2020 Task 12. The design of the system allows for easy switching of different architectures to accommodate the needs of the task at hand. For this task, we have shown the Transformer architecture using XLNet is the most successful when working with limited resources. It has also been shown that data augmentation techniques we experimented, while not detrimental to overall performance, do not necessarily improve performance. In a shared task setting, the effect of the extended data from Wikipedia was not useful, however, for a wider approach with higher recall, this could be more helpful.  We also tried to participate in the final subtask, . However, due to time constraints, we were not able to achieve a valid submission for the this subtask. We approached it as a sequence pair classification task and employed a Siamese Neural Network which was shown to perform well in sequence pair classification tasks . The architecture we employed is similar to the architecture presented in . When two sequences have a relation, we extracted the sequences and provided them as the input for the Siamese transformer architecture. Then we used the objective function suggested as classification objective function in  and optimised the cross-entropy loss. Due to the complexity of this task, we managed to run only a baseline of the proposed architecture which achieved very low evaluation scores on the development data. Therefore, we did not have a submission for this task and do not present any results here. In future, we hope to carry out further experiments with Siamese transformer architectures for relation classification tasks.  Going forth, we also wish to use this system for further tasks across further languages. While we may not achieve the best performance, the system utilises realistic system resources and is therefore very versatile. This is particularly with regard to the first subtask, where the difference to the best team is around 0.09, whereas for subtask two the best team is 0.36 ahead of us, indicating that our system is not competitive. It is possible to extend these experiments to a different domain easily using a pretrained transformer model in that domain given that a corpus similar to deft corpus is available in that domain. For an example, our system should be easily adoptable to biology domain using the BioBERT pretrained transformer model  and a deft corpus like corpus on biology domain.     include your own bib file like this: 
","   This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection.",196
" Event extraction is a process to extract the named entities, event triggers and their relationships from real-world corpora. The named entities refer to those texts about predefined classes  and event triggers are words that express the types of events in texts . In literature, named entities and triggers are connected and named entities with corresponding roles are called arguments for a given trigger of a specific event.  %Named entities refer to the text mentions with predefined classes such as person names, company names and locations, etc. An event trigger is a word that mostly expresses the event types  in text. Named entities link to triggers by different roles, and named entities with corresponding roles are called arguments for a given trigger  of a specific event.  Currently, most existing works divide the event extraction into two independent sub-tasks: named entity recognition and trigger labeling. These two sub-tasks are always formulated as multi-class classification problems, and many works apply the sequence-to-sequence based labeling method which aims to translate a sentence into sequential tags. From our investigation, one problem of these sequence-to-sequence methods is that they ignore the orders of output tags, and therefore, it is difficult to precisely annotate different parts of an entity. To address this issue, some methods propose to incorporate the conditional random field  module to be aware of order-constraints for the annotated tags.  Since entities and triggers are naturally connected around events, recent works try to extract them jointly from corpora. Early methods apply pipeline frameworks with predefined lexical features which lack generality to different applications. Recent works leverage the structural dependency between entities and triggers to further improve the performances of both the entity and trigger identification sub-tasks.  %The prevalent methods can be divided into two categories: a) a parallel framework to obtain entities and triggers simultaneously and b) a pipeline framework to get triggers at first and then perform sub-tasks to extract entities. Takanobu et al.  propose a hierarchical reinforcement learning model to extract triggers first and then evoke a sub-process to get the related entities by referring to the obtained triggers in the same sentences. Nguyen et al.  design an attention mechanism to augment the accuracy for trigger extraction in multilingual environments. Fu el al.  employ graph convolutional network  to capture the local contextual information in sentences and use a two-stage method to extract entities and triggers from text together.   % The main challenges to improve the performance of jointly extract entities and triggers are two-fold: Although existing works have achieved comparable performance on jointly extracting entities and triggers, these approaches still suffer the major limitation of losing co-occurrence relationships between entities and triggers. Many existing methods determine the trigger and entities separately and then match the entities with triggers. % In this way, the co-occurrence relationships between entities and triggers are ignored, therefore, those methods might require more pre-trained features or prior data in order to achieve better performance. In this way, the co-occurrence relationships between entities and triggers are ignored, although pre-trained features or prior data are introduced to achieve better performance. It is also challenging to capture effective co-occurrence relationships between the entities and their triggers. We observed from the experiments that most of the entities and triggers are co-occurred sparsely  throughout a corpus. This issue exacerbates the problem of losing co-occurrence relationships mentioned before.   %However, most existing methods suffer performance degradation when extracting entities and triggers jointly. The reason is that most of the entities and triggers are sparsely  co-occurred throughout a corpus and the previous approaches do not well handle this sparse co-occurred relationship. %In addition, it is challenging to establish an effective interaction mechanism between the sub-tasks for joint-event-extraction, because traditional joint learning may lead to an error-propagation issue that lowers the accuracy of joint tasks.  [htb] 	 		 	 	 %% label for entire figure %  {1.8in} 		 	 	 %% label for entire figure %  {2.4in} 		 	 	 %% label for entire figure %    %% label for entire figure   To address the aforementioned challenge, the core insight of this paper is that in the joint-event-extraction task, the ground-truth annotations for triggers could be leveraged to supervise the extraction of the entities, and vice versa. Based on this insight, this paper proposes a novel method to extract structural information from corpora by utilizing the co-occurrence relationships between triggers and entities. Furthermore, in order to fully address the aforementioned sparsely co-occurrence relationships, we model the entity-trigger co-occurrence pairs as a heterogeneous information network  and supervise the trigger extraction by inferring the entity distribution with given triggers based on the indirect co-occurrence relationships collected along the meta-paths from a heterogeneous information network .  Figure illustrates the process of our proposed method to collect indirect co-occurrence relationships between entities and triggers. Figure is a sub-graph of the ``entity-trigger'' HIN for the ACE 2005 corpus. Figure compares the entity distributions inferred from given triggers based on the direct adjacency matrix and that inferred from the meta-path adjacency matrix. From this figure, we observe that a trigger does not necessarily connect to all entities directly and the direct-adjacency-based distribution is more concentrated on a few entities, while the meta-path-based distribution is spread over a larger number of entities. This shows that a model could collect indirect co-occurrence patterns between entities and triggers based on the meta-path adjacency matrix of an ``entity-trigger'' HIN. Moreover, the obtained indirect patterns could be applied to improve the performance to extract both entities and triggers.  Based on the aforementioned example and analysis, we propose a neural network to extract event entities and triggers. Our model is built on the top of sequence-to-sequence labeling framework and its inner parameters are supervised by both the ground-truth annotations of sentences and ``entity-trigger'' co-occurrence relationships. Furthermore, to fully address the indirect ``entity-trigger'' co-occurrence relationships, we propose the \underline{C}ross-\underline{S}upervised \underline{M}echanism  based on the HIN. The CSM alternatively supervises the entity and trigger extraction with the indirect co-occurrence patterns mined from a corpus. CSM builds a bridge for triggers or entities by collecting their latent co-occurrence patterns along meta-paths of the corresponding heterogeneous information network for a corpus. Then the obtained patterns are applied to boost the performances of entity and triggers extractions alternatively. We define this process as a ``cross-supervise'' mechanism. The experimental results show that our method achieves higher precisions and recalls than several state-of-the-art methods.  In summary, the main contributions of this paper are as follows:        The remainder of this paper is organized as follows. In Section, we first introduce some preliminary knowledge about event extraction and HIN, and also formulate the problem. Section presents our proposed model in detail. Section verifies the effectiveness of our model and compares it with state-of-the-art methods on real-world datasets. Finally, we conclude this paper in Section.     In this paper, we have proposed a novel cross-supervised mechanism which allows models to extract entities and triggers jointly. Our mechanism alternately supervises the extraction process for either the triggers or the entities, based on the information in the type distribution of each other. In this way, we incorporate the co-occurrence relationships between entities and triggers into the joint-event-extraction process of our model. Moreover, to further address the problem caused by the sparse co-occurrence relationships, our method also resorts to the heterogeneous information network technology to collect indirect co-occurrence relationships. The empirical results show that our method improves the extraction performances for entities and triggers simultaneously. This verifies that the incorporated co-occurrence relationships are useful for the joint-event-extraction task and our method is more effective than existing methods in utilizing training samples. Our future works include:  investigating the impact of length of sampled meta-paths, as in this paper we have limited the meta-path into a fixed length;  connecting the extracted entities and triggers from a corpus to facilitate the automatic knowledge graph construction.  
"," Joint-event-extraction, which extracts structural information  from unstructured real-world corpora, has attracted more and more research attention in natural language processing. Most existing works do not fully address the sparse co-occurrence relationships between entities and triggers, which loses this important information and thus deteriorates the extraction performance. To mitigate this issue, we first define the joint-event-extraction as a sequence-to-sequence labeling task with a tag set composed of tags of triggers and entities. Then, to incorporate the missing information in the aforementioned co-occurrence relationships, we propose a \underline{C}ross-\underline{S}upervised \underline{M}echanism  to alternately supervise the extraction of either triggers or entities based on the type distribution of each other. Moreover, since the connected entities and triggers naturally form a heterogeneous information network , we leverage the latent pattern along meta-paths for a given corpus to further improve the performance of our proposed method. To verify the effectiveness of our proposed method, we conduct extensive experiments on four real-world datasets as well as compare our method with state-of-the-art methods. Empirical results and analysis show that our approach outperforms the state-of-the-art methods in both entity and trigger extraction.",197
"  Recently, pre-trained self-supervised models such as BERT have attracted an increasing amount of attention in natural language processing and vision-language processing.  Benefiting from common knowledge contained in massive unlabeled data, the pretraining-finetuning framework has become a representative paradigm for advancing various language-related downstream tasks.   Most endeavors on pre-trained representation models rely on elaborately designed self-supervised tasks, which typically corrupt the given sequence with certain types of noise , and then train the model to recover the original sequence.  As a consequence, the learned representations tend to be covariant with the input noise of pre-training in this paradigm.  However, when transferred to downstream tasks, the pre-trained model is responsible for encoding the original sequence without noise, and is expected to obtain noise invariant representations.  Such pretrain-finetune discrepancy not only impedes fast fine-tuning, but also may result in suboptimal sequence representations, thus affecting the performance in downstream tasks.   %%%%%%%%%%%% % [t] %    % 	 % 	\vskip 0.15in % 	 % 		\toprule % 		Models  & Noise type       \\ \midrule % 		BERT           & Mask tokens       \\ % 		SpanBERT      & Mask spans      \\ % % 		RoBERTa         & Mask token       \\ % % 		XLNet          & Shuffle token     \\ % 		ELECTRA       & Replace tokens       \\ % 		StructBERT     & Mask + Shuffle tokens   \\  % % 		BART & Mask + Shuffle + Replace. \\ % 		\midrule % 		UNITER       & Mask tokens + regions    \\ % 		LXMERT       & Mask tokens + regions     \\  % 		 % 	\vskip -0.1in %  %%%%%%%%%%%%  %%%%%%%%%%%% %  %%%%%%%%%%%%  To remedy this, we present ContrAstive Pre-Training  to learn noise invariant  sequence representations. %, inspired by the Noise Contrastive Estimation. The core idea of CAPT is to enhance the consistency between semantic representations of the original sequence and that of corresponding corrupted version  via unsupervised instance-wise training signals. %can be fully utilized via elaborately designed semantic contrastive loss. %As shown in Figure, our approach  In more detail, it strives to pull the representation of the corrupted sequence towards that of the original instance in the semantic space, while pushing it away from representations of other instances. % Such training objectives are formulated as a multi-class classification task, which aims at classifying the original sequence to the class of its corrupted version and vice versa, while classifying different instances into different classes. % For implementation feasibility, two effective model extension are proposed to further enhance the capability of the model to extract noise-concentrated and instance-diffused features. Moreover, in order to enable the model to learn from more ``difficult'' and ``diverse'' instances, two effective methods are proposed to further enhance the capability of the model to extract noise-concentrated and instance-diffused features. With such training objective, the pre-trained model is encouraged to learn noise invariant representations, thereby alleviating the pretrain-finetune discrepancy to some extent.  As an additional benefit, CAPT also assists the pre-trained model to more effectively capture the global semantics of the input.  Most prior work only focuses on token-level pre-training tasks , which lacks the modeling of global semantics of the input.  Some other efforts alleviate this problem by introducing sentence-level pre-training tasks  that rely on the relative position of segments in the document. However, the semantic connection between these segments tends to be excessively loose, which may result in confusing gradient signals.  By contrast, our CAPT offers incentives for representations of inputs sharing the same semantics  to be similar, while the representations of inputs expressing different semantics  are penalized to be distinguished from each other. Such more reasonable sentence-level supervision enables our approach to look beyond the local structures of input sequences and become more aware of the global semantics. %With such more reasonable sentence-level supervision, our approach achieves better modeling of global semantics of the input.   We perform the evaluation on a comprehensive suite of benchmark, covering 8 natural language understanding and 3 cross-modal tasks.  Extensive empirical evidence demonstrates that our approach can achieve consistent improvements over the baselines in both language and vision-language domains. To be more specific, our CAPT raises the performance of RoBERTa from 88.9\% to 89.5\% on the GLUE dev set, and also surpasses LXMERT by 0.5\%, 0.6\% and 0.8\% on VQA, GQA and , respectively.      This work presents contrastive pre-training for learning denoised sequence representations in a self-supervised manner. By enhancing the consistency between representations of the original sequence and the corresponding corrupted version, the pre-trained model is encouraged to learn noise invariant sequence representations.  On this account, the proposed approach not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also better captures the global semantics of the input via more effective sentence-level supervision. Extensive experiments demonstrate the effectiveness and versatility of our approach, which can achieve consistent improvements over baselines in both language and vision-language domains.  
"," Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training  to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6\% absolute gain on GLUE benchmarks and 0.8\% absolute increment on $\text{NLVR}^2$.",198
"   Given the presumed importance of reasoning across modalities in multimodal machine learning tasks, we should evaluate a model's ability to leverage  cross-modal interactions.   But such evaluation is not straightforward;  for example, an early Visual Question-Answering  challenge was later ``broken''  by a high-performing method that ignored the image entirely .  One response is to create multimodal-reasoning datasets that are specifically  and cleverly balanced  to resist language-only or visual-only models; examples are VQA 2.0 , NLVR2 , and GQA . However,  a balancing approach not always desirable.  For example, if image+text data is collected from an online social network , post-hoc rebalancing may obscure trends in the original data-generating processs. So, what alternative diagnostic tools are available for better understanding what models learn?    The main tool utilized by prior work is  In addition to comparing against text-only and image-only baselines, often, two multimodal models with differing representational capacity  are trained and their performance compared. The argument commonly made is that if model A, with greater expressive capacity, outperforms model B, then the performance differences can be at least partially attributed to that increased expressivity.   {But is that a reliable argument?} Model performance comparisons are an opaque tool for analysis, especially for deep neural networks: performance differences versus baselines, frequently small in magnitude, can often be attributed to hyperparameter search schemes, random seeds, the number of models compared, etc. . Thus, while model comparisons are an acceptable starting point for demonstrating whether or not a model is learning an interesting set of  cross-modal factors, they provide rather indirect evidence.  We propose mpirical   \underline{M}ultimodally-\underline{A}dditive       \makeatletter \renewcommand   The last question on our FAQ list in \autoref{sec:faq} leaves us with the following conundrum: 1) Additive models are incapable of most cross-modal reasoning; but 2) for most of the unbalanced tasks we consider,   Our hope is that future work on multimodal classification tasks report not only the predictive performance of their best model + baselines, but also the \emap of that model. \emap  has practical implications beyond image+text classification: there are straightforward extensions to non-visual/non-textual modalities, to classifiers using more than 2 modalities as input, and to single-modal cases where one wants to check for feature interactions between two groups of features, e.g., premise/hypothesis in NLI.     
","  Modeling expressive cross-modal interactions  seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data.  We propose a new diagnostic tool,  , for isolating whether or not cross-modal {interactions} improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure.  For seven \mbox{image+}text classification tasks , we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning  report the performance not only of unimodal baselines, but also the \emap of their best-performing model.",199
"    \fi  % typically, a single predicate mention  does not constitute what we typically think about as events; we typically think of an event as something that consists of multiple such primitive structures %{\fontsize{10.5}{11} {13pt}}) involves more fine-grained event mentions about people killed , flights canceled  and passengers affected . Some of those mentions also follow strict temporal order . Our goal is to induce such an  that recognizes %organizes  the membership of multi-granular events described in the text, as well as their temporal order. This is not only at the core of text understanding, but is also beneficial to various applications such as question answering , narrative prediction , timeline construction  and summarization . %\dr{The choice of references is good but revealing; I suggest to replace the summarization with a ``classical"" summarization paper , and supports tasks such as question answering , narrative prediction , timeline construction  and summarization . Typically, events are not just standalone predicate mentions, but rather as structures over multiple such predicates. Consider the example in Figure.  The description to the impact of the storm  also involves mentions about killed people , canceled flights  and affected passengers . Some of mentions thereof also follow temporal order. To support the comprehension of complex events, it is important to recognize the multifaceted relations for the predicate mentions in the text. \fi        % second paragraph  studied event temporal relation  extraction with a statistical common sense resource  and  adopted data-driven methods for TempRel extraction; parent-child relations among events are studied in  and . Though some of the previous work has ensured consistency via adding constraints in the inference phase, essentially they are not improving local predictions and the inconsistent results from the models might not be corrected in the inference stage. Besides, most of the approaches suffered from limited learning resources and the tasks are studied separately. \fi  Recently, significant %much research effort has been devoted to several event-event relation extraction tasks, such as event temporal relation  extraction  and subevent relation extraction . Addressing such challenging tasks requires a model to recognize the inherent connection between event  %\dr{should it be predicate mentions, to ease the ambiguity?}  mentions as well as their contexts in the documents. Accordingly, a few previous methods apply statistical learning methods to characterize the grounded events in the documents . Such methods often require designing various features to characterize the structural, discourse and narrative aspects of the events, which are costly to produce and are often specific to a certain task or dataset. More recent works attempted to use data-driven methods based on neural relation extraction models  which refrain from feature engineering and offer competent performances.      While data-driven methods provide a general and tractable way to capture specific event-event relations, it still remains challenging for those methods to precisely infer the correct relations. One challenge is that almost every task for event-event relation extraction comes with limited available annotated resources. Specifically, most tasks annotate no more than a hundred articles . Even the largest one in the literature, i.e., MATRES  for TempRel extraction, contains annotation for merely 275 articles. The lack of supervision hinders feature learning of events as well as inference of the relations, %Therefore, effectively tackling these tasks inevitably calls  therefore calling upon plausible auxiliary supervision from resources that are external to each of the tasks.    On the other hand, the event-event relations are often constrained by  %\drc{logical \dr{}change everywhere} %logic %\muhao{done.} properties, such as transitivity of TempRels Before and After , as well as that of %the relation between parent and child events subevent relations . In favor of such constraints, literature has employed global inference in the inference phase to comply with the logical properties particularly for TempRels . However, there lacks an effective way to ensure the global logical consistency in the training phase, which is key to making a data-driven machine learning model consistent on the beliefs of training data for various relation types . Moreover, the logical constraints may apply to different categories of %event-event  relations, and form complex conjunctive rules.  Consider the example in Figure : given that e2:died is Before e3:canceled and e3:canceled is a parent event of e4:affecting, the learning process should enforce e2:died Before e4:affecting. %\todo{Add an example of a conjunctive rule containing temporal and subevent relations.} Accordingly, ensuring the logical constraints  task-specific relations is another challenge being overlooked by the literature, the resolve of which provides a natural way to bridge the learning processes on multiple tasks. %\magenta{HW:TCR?} \fi  While data-driven methods provide a general and tractable way for event-event relation extraction, their performance is restricted by the limited annotated resources available. For example, the largest temporal relation extraction dataset MATRES only has 275 articles, which is far from enough for training a well-performing supervised model. The observation that relations and, in particular, event-event relations should be constrained by their logical properties , led to employing global inference to comply with transitivity and symmetry consistency, specifically on TempRel . However, in an event complex, the logical constraints may globally apply to different task-specific relations, and form more complex conjunctive constraints.  Consider the example in Figure : given that e2:died is Before e3:canceled and e3:canceled is a Parent event of e4:affecting, the learning process should enforce e2:died Before e4:affecting by considering the conjunctive constraints on both TempRel and subevent relations. While previous works focus on preserving logical consistency through  inference or structured learning , there was no %lacks an  effective way to endow neural models with the sense of global logical consistency during training.  %\dr{Notice that the previous statement was not correct; I change to limit it to neural models, since structure learning did it} %ensure the global logical consistency in the training phase.  This is key to bridging %bridge  the learning processes of %on both TempRel and subevent relations, which is a research focus of this paper.  %Event-relation extraction is a non-trivial task because of the following challenges: %1) Almost every event relation extraction task comes with limited learning resources with annotations. %2) Event relations are often volatile given different scenarios, and the determination of parent-child relation is especially difficult since there are less explicit lexical expressions compared with the cases for time and causation. %3) Event relations are often endowed with logical properties: % some temporal relations and parent-child relations comply with transitivity; % logical consistency should also be ensured across different categories of event relations.  The  contribution of this work is proposing %to propose  a joint constrained learning model for multifaceted event-event relation extraction.  The joint constrained learning framework seeks to regularize the model towards consistency with the logical constraints across both temporal and subevent relations, for which three types of consistency requirements are considered: ,  and . Such consistency requirements comprehensively define the interdependencies among those relations, essentially unifying the ordered nature of time and the topological nature of multi-granular subevents based on a set of declarative logic rules. Motivated by the logic-driven framework proposed by , the declarative logical constraints are converted into differentiable functions that can be incorporated into the learning objective for relation extraction tasks.  Enforcing logical constraints across temporal and subevent relations is also a natural way to combine %two event-event relation extraction tasks with a shared learning objective. the supervision signals coming from two different datasets, one for each of the  relation extraction tasks with a shared learning objective. %\dr{You said what is the first contribution, but not the second; do you want now to claim this as the second contribution? Note that I modified to emphasize the two datasets} %Besides, the consistency of the final prediction is further enforced by global inference via an ILP solver.  Despite the scarce annotation for both tasks, the proposed method surpasses the SOTA TempRel extraction method on MATRES by relatively 3.27\% in ; %\dr{I don't understand -- is it relative or F1? Also, Tab. 2 shows 2.5\%}  it also offers promising performance on the HiEve dataset for subevent relation extraction, relatively surpassing previous methods by at least 3.12\% in .  %\dr{which table is this from?} %by 3.12\% and 21.4\%. %We further provide ablation studies to show the importance of each component of our framework. %This fact is further illustrated by ablation studies.   From the NLU perspective, %the acquired knowledge of our method is able to simultaneously models the internal membership structure of a complex event, as well as the temporal relations among both simple and complex events. the  contribution of this work lies in providing a general method for inducing an event complex that comprehensively represents the relational structure of several related event %\drc{predicate} % mentions. %in two directions.  This is supported by the memberships vertically identified between multi-granular events, as well as the horizontal temporal reasoning within the event complex. As far as we know, this is %essentially different from all %many  previous works that only formulated relations along a single axis. Our model further demonstrates the potent capability of inducing event complexes  %with promising performance  when evaluated  %based  on the RED dataset .        Event-event relation extraction is a challenging task which is beneficial to understanding event complex composed of multi-granular events with temporal orders. Despite the existence of previous attempts for addressing TempRel and subevent relation extraction, this is the first work  We propose a joint constrained learning framework for extracting event complexes from documents.  that combines the two tasks and addresses them by constrained learning with shared objectives.  The proposed framework bridges TempRel and subevent relation extraction tasks with a comprehensive set of logical constraints, which are enforced during learning by  converting them into differentiable objective functions.  On two benchmark datasets, the proposed method outperforms SOTA statistical learning methods and data-driven methods for each task, without using data that is jointly annotated with the two classes of relations. It also presents promising event complex extraction results on RED that is external to training.  Thus, our work shows that the global consistency of the event complex significantly helps understanding both temporal order and event membership. For future work, we plan to extend the framework towards an end-to-end system with event extraction. We also seek to extend the conjunctive constraints along with event argument relations.  , demonstating the effectiveness of joint constrained learning framework from both machine-learning and NLU view .    
","     %\dr{I think that the current version  is too detailed and does not position the work at all, it just says what is being done. Here is a suggestion:}    Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other.     In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them.    Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations.    Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations     %of events     by converting these constraints into differentiable learning objectives. We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process.    We also present a promising case study showing the effectiveness of our approach in inducing event complexes on an external corpus.\footnote{Our code is publicly available at \url{https://cogcomp.seas.upenn.edu/page/publication_view/914}.} %\dr{Doesn't this contradict the statement above regarding the lack of joint data? Do we need to address it somehow}    %\dr{do we need the next clause? really, you show that you don't need it, but it reads like you just don't use it. If you really want to keep it, maybe better to say ""replacing a commonly used, more expensive, global inference process""} even without global inference that is widely used in previous methods.          \fi",200
"  Word embeddings which can capture semantic similarities have been extensively explored in a wide spectrum of Natural Language Processing  applications in recent years.  Word2Vec , FastText , and Glove  are some examples. Even though distributional word embeddings produce high quality representations, representing longer pieces of text such as sentences and paragraphs is still an open research problem. A sentence embedding is a contextual representation of a sentence which is often created by transformation of word embeddings through a composition function. There has been a large body of work in the literature which propose different approaches to represent sentences from word embeddings. SkipThought , InferSent , and Universal Sentence Encoder  are well-known examples.  % Other proposed methods for learning sentence representations include, but are not limited to .  There has been a growing interest in understanding what linguistic knowledge is encoded in deep contextual representation of language. For this purpose, several probing tasks are proposed to understand what these representations are capturing . One of the interesting findings is that despite the existence of explicit syntactic annotations, these learned deep representations encode syntax to some extent . Hewitt et. al. provide an evidence that the entire syntax tree is embedded implicitly in deep model's vector geometry. Kuncoro et. al.  show that LSTMs trained on language modeling objectives capture syntax-sensitive dependencies. Even though deep contextual language models implicitly capture syntactic information of sentences, explicit modeling of syntactic structure of sentences has been shown to further improve the results in different NLP tasks including neural language modeling , machine comprehension , summarization , text generation , machine translation , authorship attribution , etc. Furthermore, Kuncoro et. al. provide evidence that models which have explicit syntactic information result in better performance . Of particular interest, one of the areas where syntactic structure of sentences plays an important role is style-based text classification tasks, including authorship attribution. The syntactic structure of sentences captures the syntactic patterns of sentences adopted by a specific author and reveal how the author structures the sentences in a document.   Inspired by the above observations, our initial work demonstrates that explicit syntactic information of sentences improves the performance of a recurrent neural network classifier in the domain of authorship attribution . We continue this work in this paper by investigating if structural representation of sentences can be learned explicitly. In other words, similar to pre-trained word embeddings which mainly capture semantics, can we have pre-trained embeddings which mainly capture syntactic information of words. Such pre-trained word embeddings can be used in conjunction with semantics embeddings in different domains including authorship attribution. For this purpose, we propose a self-supervised framework using a Siamese  network  to explicitly learn the structural representation of sentences. The Siamese network is comprised of two identical components; a lexical sub-network and a syntactic sub-network; which take the sequence of words in the sentence and its corresponding linearized syntax parse tree as the inputs, respectively. This model is trained based on a contrastive loss objective where each pair of vectors  is close to each other in the embedding space if they belong to an identical sentence , and are far from each other if they belong to two different sentences .    As a result, each word in the sentence is embedded into a vector representation which mainly carries structural information. Due to the -to- mapping of word types to structural labels, the word representation is deduced into structural representations. In other words, semantically different words  are mapped to similar structural labels ; hence, semantically different words may have similar structural representations. These pre-trained structural word representations can be used as complimentary information to their pre-trained semantic embeddings . We use probing tasks proposed by Conneau et al.  to investigate the linguistic features learned by such a training.  The results indicate that structural embeddings show competitive results compared to the semantic embeddings, and concatenation of structural embeddings with semantic embeddings achieves further improvement.  Finally, we investigate the efficiency of the learned structural embeddings of words for the domain of authorship attribution across four datasets. Our experimental results demonstrate classification improvements when structural embeddings are concatenated with the pre-trained word embeddings.  The remainder of this paper is organized as follows: we elaborate our proposed self-supervised framework in Section .  The details of the datasets and experimental configuration are provided and the experimental results reported in Section ; We review the related work in Section . Finally, we conclude this paper in Section .        In this paper, we have proposed a self-supervised framework for learning structural representation of sentences for the domain of authorship attribution. The result of training this self-supervised framework is pre-trained structural embeddings which capture information regarding the syntactic structure of sentences. Subsequently, these structural embeddings can be concatenated to the existing pre-trained word embeddings and create a style-aware embedding which carries both semantic and syntactic information and it is well-suited for the domain of authorship attribution. Moreover, structural embeddings eliminate the necessity of syntactic parsing for training syntactic neural networks; therefore, training a neural model using pre-trained structural embeddings is computationally more efficient. According to our experimental results on four benchmark datasets in authorship attribution, using structural embedding improves the performances of the proposed neural model.             The next two lines define the bibliography style to be used, and    the bibliography file. 
","   Syntactic structure of sentences in a document substantially informs about its authorial writing style. Sentence representation learning has been widely explored in recent years and it has been shown that it improves the generalization of different downstream tasks across many domains. Even though utilizing probing methods in several studies suggests that these learned contextual representations implicitly encode some amount of syntax, explicit syntactic information further improves the performance of deep neural models in the domain of authorship attribution. These observations have motivated us to investigate the explicit representation learning of syntactic structure of sentences.  In this paper, we propose a self-supervised framework for learning structural representations of sentences. The self-supervised network contains two components; a lexical sub-network and a syntactic sub-network which take the sequence of words and their corresponding structural labels as the input, respectively. Due to the $n$-to-$1$ mapping of words to their structural labels, each word will be embedded into a vector representation which mainly carries structural information. We evaluate the learned structural representations of sentences using different probing tasks, and subsequently utilize them in the authorship attribution task. Our experimental results indicate that the structural embeddings significantly improve the classification tasks when concatenated with the existing pre-trained word embeddings.",201
"    Since the end of the twentieth century and the spread of mobile communication technologies in the Arab world, youth, in particular, have developed a new chat alphabet to communicate more efficiently in informal Arabic. Because most media and applications initially did not enable chatting in Arabic, these Arab speakers resorted to what is now commonly known as ""Arabizi"". In, Arabizi was defined as the newly-emerged Arabic variant written using the Arabic numeral system and Roman script characters. With the widespread use of social media worldwide in more recent years, Arabizi emerged as an established Arabic writing system for mobile communication and social media in the Arab world.   Compared to the increasing studies of sentiment analysis in Indo-European languages, similar research for Arabic dialects is still very limited.\ This is mainly attributed to the lack of the needed good quality Modern Standard Arabic  publicly-available sentiment analysis resources in general, and more specifically dialectical Arabic publicly-available resources.\ Building such resources involves several difficulties in terms of data collection and annotation, especially for underrepresented Arabic dialects such as the Tunisian dialect. Nevertheless, existing Tunisian annotated datasets focused on code-switching datasets written using the Arabic or the Romanized Alphabet. The studies on these datasets applied off-the-shelf models that have been built for MSA on a dataset of Tunisian Arabic. An intuitive solution is to translate Tunisian Romanized Alphabet into Arabic Script. This approach suffers from the need for a parallel Tunisian-Arabic text corpus, the low average precision performances achieved and the irregularity of the words written.  Using a model trained on Modern Standard Arabic sentiment analysis data and then applying the same model on dialectal sentiment analysis data, does not produce good performances as shown in. This suggests that MSA models cannot be effective when applied to dialectical Arabic. There is, thus, a growing need for the creation of computational resources, not only for MSA but also for dialectical Arabic. The same situation holds when one tries to use computational resources used for a specific dialect of Arabic with another one.  To the best of our knowledge, this is the first study on sentiment analysis TUNIZI Romanized Alphabet. \ This could be deduced in the next sections where we will present TUNIZI and the state-of-the-art of Tunisian sentiment analysis followed by our proposed approach, results and discussion before conclusion and future work.        In this work, we have tackled the Tunisian Romanized alphabet sentiment analysis task. We have experimented two different word-level representations  and two deep neural networks , without the use of any pre-processing step. Results showed that CNN trained with M-BERT achieved the best results compared to the word2vec, frWac and Bi-LSTM. This model could improve the performance over the baselines. Experiments and promising results achieved on the TUNIZI and TSAC-TUNIZI datasets helped us to better understand the nature of the Tunisian dialect and its specificities. This will help the Tunisian NLP community in further research activities not limited to the sentiment analysis task, but also in more complex NLP tasks.  A natural future step would involve releasing TunaBERT, a Tunisian version of the Bi-directional Encoders for Transformers  that should be learned on a very large and heterogeneous Tunisia dataset. The Tunisian language model can be applied to complex NLP tasks . To demonstrate the value of building a dedicated version of BERT for Tunisian, we also plan to compare TunaBERT to the multilingual cased version of BERT.   
"," Tunisians on social media tend to express themselves in their local dialect using Latin script . This raises an additional challenge to the process of exploring and recognizing online opinions. To date, very little work has addressed TUNIZI sentiment analysis due to scarce resources for training an automated system. In this paper, we focus on the Tunisian dialect sentiment analysis used on social media. Most of the previous work used machine learning techniques combined with handcrafted features. More recently, Deep Neural Networks were widely used for this task, especially for the English language. In this paper, we explore the importance of various unsupervised word representations  and we investigate the use of Convolutional Neural Networks and Bidirectional Long Short-Term Memory. Without using any kind of handcrafted features, our experimental results on two publicly available datasets~~ showed  comparable performances to other languages.",202
"    Automatic medical code assignment is a routine healthcare task for medical information management and clinical decision support. The International Classification of Diseases  coding system, maintained by the World Health Organization , is widely used among various coding systems.  Thus, the medical code assignment task is also called ICD coding. It uses clinical notes of discharge summaries to predict medical codes in a supervised manner with human-annotated codes, which is formulated as a multi-class multi-label text classification problem in the medical domain.    While there are increasing works in the community in automatic medical code assignment~, this task remains challenging from the perspectives of note representation and code prediction. First, medical note representation, a critical step in understanding medical notes, is formidably challenging due to the lengthy and complex semantic information in the discharge documents. There are typically thousands of tokens in a medical note due to the various diagnoses and procedures experienced by a patient. Furthermore, clinical notes also contain a vocabulary with many professional words and phrases, making it hard for a neural network model to encode and understand critical information. Second, the medical coding system has a very high and sparse dimensional label space, render the code prediction task incredibly difficult. For example, ICD9 and ICD10 coding systems have many labels, i.e., more than 14,000 and 68,000 codes. However, a patient typically is diagnosed with only a couple of codes over the whole coding space.     Early works for medical code assignment typically follow statistical approaches. They either employ rule-based methods  or apply classification methods such as SVM and Bayesian ridge regression  to assign the codes. These methods are shallow and do not exploit the complex semantic information in medical notes, leading to unsatisfactory performance. Recently, Natural language processing  techniques based on deep learning have been developed , which learn the note representation via convolutional neural networks. Specifically, CAML, MultiResCNN and DCAN treat ICD coding as a general text classification problem and develop complex neural encoders to learn the note representation. HyperCore proposes the hyperbolic embedding to capture code hierarchy and co-occurrence. However, these approaches are still ineffective, as they do not explicitly capture the fine-grained interactions between textual elements and medical codes. These interactions naturally represent the interdependencies between the complex medical words and associated codes, and thus should be well exploited.  This paper put forward a novel neural architecture,  Gated Convolutional Neural Network with Note-Code Interaction , for effective medical code assignment. Our goal is to learn rich representation from clinical notes and exploit the interactions between medical texts and clinical codes. To capture the long sequential history of clinical documents, we design a novel dilation information propagation component with a forgetting mechanism to selectively utilize the useful information for note representation learning. To tackle the large labeling space, we formulate textual notes and medical codes as a complete bipartite graph and develop a graph message passing approach to capture the explicit interaction between nodes and codes. The ICD code descriptions are used as an external medical knowledge source to learn more accurate code representations that preserve the semantic relations of the codes. Considering the practical application in real-world medical institutes, especially those with limited computing resources, our architecture also prioritizes computational efficiency when designing the sub-modules.  Our contributions are itemized as follows.        Medical code assignment from clinical notes is a fundamental task for healthcare information systems and diagnosis decision support.  This paper proposes a novel framework with gated convolutional neural networks and note-code message passing mechanism for automated medical code assignment. Our solution can learn meaningful features from lengthy clinical documents and effectively control the deep propagation of information flow.  Moreover, the message passing mechanism can enhance the ICD code space's semantics and model the note-code interaction to improve medical code prediction. Experiments show the effectiveness of our proposed method.   
"," Medical code assignment from clinical text is a fundamental task in clinical information system management. As medical notes are typically lengthy and the medical coding system's code space is large, this task is a long-standing challenge.  Recent work applies deep neural network models to encode the medical notes and assign medical codes to clinical documents. However, these methods are still ineffective as they do not fully encode and capture the lengthy and rich semantic information of medical notes nor explicitly exploit the interactions between the notes and codes. We propose a novel method, gated convolutional neural networks, and a note-code interaction , for automatic medical code assignment to overcome these challenges. Our methods capture the rich semantic information of the lengthy clinical text for better representation by utilizing embedding injection and gated information propagation in the medical note encoding module. With a novel note-code interaction design and a graph message passing mechanism, we explicitly capture the underlying dependency between notes and codes, enabling effective code prediction. A weight sharing scheme is further designed to decrease the number of trainable parameters. Empirical experiments on real-world clinical datasets show that our proposed model outperforms state-of-the-art models in most cases, and our model size is on par with light-weighted baselines.",203
" %  Enabling chatbots to indulge in engaging conversations requires massive datasets of human-human conversations . Training such dialog agents requires substantial time and effort expended in the collection of adequate number of high quality conversation samples.   alleviate this problem by introducing a self-feeding chatbot which can directly learn from user interactions. This chatbot requests users to provide natural language feedback when the users are dissatisfied with its response.   treat this feedback as a gold response to the wrong turn and use it as an additional training sample to improve the chatbot.    Although natural language feedback is cheap to collect from a chatbot's end-users, most often, feedback cannot be used directly as a training sample since feedback is usually not the answer itself, but simply contains hints to the answer. \Cref{tab:response_samples} shows some feedback text samples. Naive modification of feedback using heuristics like regular expressions would lead to generic responses that are ineffective in improving the dialog ability of chatbots . Additionally, writing an exhaustive set of regular expression rules is time consuming and requires extensive analysis of the data.  Annotating data to convert feedback text to natural response is also expensive and defeats the purpose of learning from feedback text.   [t]   gives a bird's-eye view of our problem. We frame this problem as a variant of text style transfer where the generator is tasked with making the feedback resemble the optimal response to the user's previous utterance and the discriminator is a classifier that distinguishes whether a given response is feedback or natural.   Our main contributions are the following: %      ).     %      ).     Our results also reveal that training naively on feedback doesn't help when the original chatbot is already a strong model, whereas \feedresp also helps strong models.     In this work, we show that while chatbots can be improved using natural language feedback, converting feedback to natural responses that fit in the conversation outperform the naive usage of feedback.  We presented \feedresp, a generative adversarial model, that converts feedback to natural responses without requiring manually annotated parallel data. Our results show that \feedresp results in a 6~point improvement for the \polyencoder chatbot, an already powerful dialog ranking agent.  This is a strong result as HITS@1/20 is a tough metric to improve upon .  Our work joins the class of models that use natural language feedback to improve different tasks, e.g., image captioning , classification . While these methods use feedback for reward shaping or feature extraction, we use feedback to produce correct response using adversarial learning. We pose this problem as a style transfer problem inspired from the style transfer literature .  While these focus on studying the stylistic attributes of sentences, e.g, sentiment, we explore this problem in the context of improving chatbots.  
","  The ubiquitous nature of chatbots and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as an additional training sample. However, user feedback in most cases contains extraneous sequences hindering their usefulness as a training sample. In this work, we propose a generative adversarial model that converts noisy feedback into a plausible natural response in a conversation. The generator's goal is to convert the feedback into a response that answers the user's previous utterance and to fool the discriminator which distinguishes feedback from  natural responses. We show that augmenting original training data with these modified feedback responses improves the original chatbot performance from 69.94\% to 75.96\% in ranking correct responses on the \personachat dataset, a large improvement given that the original model is already trained on 131k samples.\footnote{Our code is released at \url{https://github.com/ekunnii/adversarial-feedback-chatbot/}}",204
"  Text Generation is the task of producing written or spoken narrative from structured or unstructured data. The overarching goal is the seamless human-machine communication by presenting a wealth of data in a way we can comprehend. With respect to the modeling approaches, there are three main paradigms in generating text based on the schema of input and output:  Text-to-Text  Data-to-Text  None-to-Text. Table  presents the categorization of different tasks based on this paradigm. These several tasks deserve undivided attention and accordingly they have been heavily dissected, studied and surveyed in the recent past. For instance, independent and exclusive surveys are periodically conducted on summarization , knowledge to text generation {DBLP:conf/inlg/GardentSNP17, DBLP:conf/naacl/Koncel-Kedziorski19}, machine translation , dialog response generation , storytelling, narrative generation , image captioning  etc., to dig deeper into task specific approaches that are foundational as well as in the bleeding edge of research. While these are extremely necessary, often the focus on techniques that are beneficial to other tightly coupled tasks are overlooked. The goal of this survey is to focus on these key components that are task agnostic to improve the ensemble of tasks in neural text generation. %The rest of the survey is organized as follows: Section  describes the modeling approaches in text generation including the learning paradigms, pre-training and decoding strategies. This is followed by Section  describing the key challenges and solutions to the text generation such as fluency, length, content selection, speed etc.,. Section  describes evaluation and finally Section  presents the conclusions and the prospective future directions.  [t] {% {l|l|l|l} \hline \hline  }     [t!]       %https://www.sciencedirect.com/science/article/pii/S1319157820303360    There have been several studies conducted on surveying text generation.  present a detailed overview of information theory based approaches.  primarily focus on core modeling approaches, especially VAEs  and GANs .  elaborated on tasks such as captioning, style trasfer etc., with a primary focus on data-to-text tasks. Controllability aspect is explored by . The workclosest to this is by  who perform an empirical study on the core more modeling approaches only. In contrast to these, this paper focuses on task agnostic components and factors capable of pushing the ensemble of tasks forward. Figure  presents the various components and factors that are important to study in neural text generation which are elaborated in this paper. %Text generation is an overarching set of tasks where these underlying factors that cut across tasks are very critical in pushing the field forward and this paper is dedicated to be a one stop destination to learn these several fundamental factors.     The past decade witnessed text generation dribbling from niche scenarios into several mainstream NLP applications. This urges the need for a snapshot to retrospect the progress of varied text generation tasks in unison. This paper is written with the goal of presenting a one-stop destination for task agnostic components and factors in text generation for researchers foraging to situate their work and guage their impact in this vast field. Moving forward, we envision that there are some of the crucial directions to focus for impactful innovation in text generation. These include  generation in real time  non-autoregressive decoding  consistency with situated contexts in real and virtual environments and games  consistency with personality with opinions especially for virtual agents  conditioning on multiple modalities together with text and data  investigation is still ongoing on finding better metrics to evaluate NLG with better correlated human judgements   creative text generation. We believe this is the right time to extend advancements in any particular task to other tightly coupled tasks to revamp improvements in text generation as a holistic task.  
","   Neural text generation metamorphosed into several critical natural language applications ranging from text completion to free form narrative generation. Generating natural language has fundamentally been a human attribute and the advent of ubiquitous NLP applications and virtual agents marks the need to impart this skill to machines. There has been a colossal research effort in various frontiers of neural text generation including machine translation, summarization, image captioning, storytelling etc., We believe that this is an excellent juncture to retrospect on the directions of the field. Specifically, this paper surveys the fundamental factors and components relaying task agnostic impacts across various generation tasks such as storytelling, summarization, translation etc., In specific, we present an abstraction of the imperative techniques with respect to learning paradigms, pretraining, modeling approaches, decoding and the key challenges. Thereby, we hope to deliver a one-stop destination for researchers in the field to facilitate a perspective on where to situate their work and how it impacts other closely related tasks. %scope it : current neural techniques %for single and multi-sentence",205
"  The following instructions are directed to authors of papers submitted to EACL 2021 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.     Our results suggest that GPT-2 generally outputs better narratives than the most recent non-GPT-based neural model. Additionally, we find that larger models are better. While GPT-2 Large may be infeasible for very long sequence generation, it is possible to use GPT-2 Medium for all narrative lengths generated here. Once GPT-3  is released for public use, it is very likely that this model will outperform GPT-2 based on these trends. We encourage future work to investigate similar hyperparameters to see whether the trends observed here are stable across model sizes.  We recommend keeping the  hyperparameter within the range  to . This aligns with the findings of , who suggest that  values well below  are needed to generate text that more closely approximates human text.    Diverse decoding increased narrative quality on all metrics at small . This could be used to qualitatively induce more intense and vivid stories with higher  values, though this finding should be seen as preliminary and tested in other domains. Using higher values of  also seemed to induce more vivid stories, but with less consistent fluency and coherence; thus, the diverse decoding objective could be a promising way to increase narrative interestingness without significantly decreasing performance on any particular metric.  While relatively low dist- may correlate with consistently poor quality stories and relatively high dist- may correlate with more variable-quality stories, we find that this metric did not correlate well with any of our metrics in general . Sent-BERT did not correlate with any of our metrics of narrative quality. Thus, we do not recommend optimizing over either of these automatic quantities.  Surprisingly, we do not find a strong diversity-quality trade-off in narrative generation, perhaps due to the more creative and long-form nature of this task. Indeed, diversity and quality do not correlate well in general: diverse decoding and higher  values often coincide with better performance on  human metrics in this domain up to a point. This could be due to the more creative and long-form nature of narrative generation compared to tasks such as chatbot response generation. We thus encourage future work to investigate other methods of inducing more diverse output, as certain methods can increase human perceptions of narrative quality.  Our findings aim to inform future efforts in the narrative generation domain by establishing future baselines given our recommended hyperparameters, and by facilitating further investigation of decoding objectives for better narrative generation. Additionally, we hope that this investigation highlights issues to be addressed in future work when evaluating narratives automatically, since no metrics aside from perplexity seem to correlate well with human judgments of quality.     \clearpage    
"," This document contains the instructions for preparing a manuscript for the proceedings of EACL 2021. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.",206
" Cross-lingual abstractive summarization is the task to generate a summary of a given document in a different target language. This task provides the overview of an article in a foreign language and thus helps readers understand a text written in an unfamiliar language quickly.   Early work on cross-lingual abstractive summarization adopted the pipeline approach: either translation of the given document into the target language followed by summarization of the translated document or summarization of the given document followed by translation of the summary into the target language. On the other hand, recent studies have applied a neural encoder-decoder model, which is widely used for natural language generation tasks including machine translation and monolingual abstractive summarization, to generate a summary in the target language from the given document directly. %Such direct generation approaches prevent the error propagation problems in pipeline methods. Such direct generation approaches prevent the error propagation in pipeline methods.  Training neural encoder-decoder models requires numerous sentence pairs. In fact,  provided 3.8M sentence-summary pairs to train their neural encoder-decoder model for English abstractive summarization, and the following studies used the same training data. However, constructing a large-scale cross-lingual abstractive summarization dataset is much more difficult than collecting monolingual summarization datasets because we require sentence-summary pairs in different languages. To address this issue, recent studies applied a machine translation model to monolingual sentence-summary pairs. They used the constructed pseudo dataset to train their neural encoder-decoder models.    Meanwhile, the possibility whether existing genuine parallel corpora such as translation pairs and monolingual abstractive summarization datasets can be utilized needs to be explored. In machine translation,  indicated that using translation pairs in multiple languages improved the performance of a neural machine translation model. Similarly, we consider that such existing genuine parallel corpora have a positive influence on the cross-lingual abstractive summarization task since the task is a combination of machine translation and summarization.   In this study, we propose a multi-task learning framework, Transum, which includes machine translation, monolingual abstractive summarization, and cross-lingual abstractive summarization, for neural encoder-decoder models. The proposed method controls the target task with a special token which is inspired by Google's multilingual neural machine translation system. For example, we attach the special token  to the beginning of the source-side input sentence in translation.   The proposed Transum is quite simple because it does not require any additional architecture in contrast to  but effective in cross-lingual abstractive summarization. Experimental results show that Transum improves the performance of cross-lingual abstractive summarization and outperforms previous methods in Chinese-English and Arabic-English summarization. In addition, Transum significantly improves machine translation performance compared to that obtained using only a genuine parallel corpus for machine translation.   Furthermore, we construct a new test set to simulate more realistic situations: cross-lingual summarization with several length constraints. In a summarization process, it is important to generate a summary of a desired length. However, existing test sets for cross-lingual abstractive summarization cannot evaluate whether each model controls output lengths because the test sets do not contain summaries with multiple lengths. Thus, we translate an existing monolingual abstractive summarization that contains summaries with multiple lengths to construct the new test set.    The contributions of this study are as follows:      , that uses existing genuine parallel corpora in addition to pseudo cross-lingual abstractive summarization data to train a neural encoder-decoder model.         This paper presents a multi-task learning framework for cross-lingual abstractive summarization to augment training data. The proposed method, Transum, attaches the special token to the beginning of the input sentence to indicate the target task. The special token enables us to use genuine translation pairs and the monolingual abstractive summarization dataset in addition to the pseudo cross-lingual abstractive summarization data for training. The experimental results show that Transum achieved better performance than the pipeline approach and model trained with pseudo data only. We achieved the top ROUGE scores in Chinese-English and Arabic-English abstractive summarization. Moreover, Transum also improved the performance of machine translation and outperformed the previous top score in the JIJI English-Japanese translation.         
"," We present a multi-task learning framework for cross-lingual abstractive summarization to augment training data. Recent studies constructed pseudo cross-lingual abstractive summarization data to train their neural encoder-decoders. Meanwhile, we introduce existing genuine data such as translation pairs and monolingual abstractive summarization data into training. Our proposed method, Transum, attaches a special token to the beginning of the input sentence to indicate the target task. The special token enables us to incorporate the genuine data into the training data easily. The experimental results show that Transum achieves better performance than the model trained with only pseudo cross-lingual summarization data. In addition, we achieve the top ROUGE score on Chinese-English and Arabic-English abstractive summarization. Moreover, Transum also has a positive effect on machine translation. Experimental results indicate that Transum improves the performance from the strong baseline, Transformer, in Chinese-English, Arabic-English, and English-Japanese translation datasets.",207
"/} Automatic keyphrase generation is the task of generating single or multi-word lexical units that provides readers with high level information about the key ideas or important topics described in a given source text. Apart from an information summarization perspective, this task has applications in various downstream natural language processing tasks such as text classification , document clustering  and information retrieval .   Traditionally, keyphrases  were extracted from source documents by retrieving and ranking a set of candidate phrases through rule based approaches. With recent advances in neural natural language generation and availability of larger training corpora, this problem is formulated under a sequence-to-sequence  modelling framework . This approach has an advantage that it can generate new and meaningful keyphrases which may be absent in the source text. The earliest work in this direction was by , who train a S2S model to generate one keyphrase at a time. At inference time, they decode with beam sizes as high as 200, to generate a large number of KPs and finally de-duplicate the outputs. However, this is computationally expensive and wasteful because only  of such KPs were found to be unique .   An alternative approach is to train a S2S model to generate multiple keyphrases in a sequential manner, where the output KPs are separated by a pre-defined delimiter token. This method has an added benefit that the model automatically learns to generate a variable number of keyphrases depending on the input, instead of a user-specified fixed number of keyphrases  from a large list of candidate outputs. However, some previous approaches  still use exhaustive beam search decoding to over-generate KPs and then apply post-processing to remove repetitions. Apart from the additional computational requirements, we argue that this method of  avoiding information redundancy is a last-minute solution. % `hacky' solution.   % \todoi{Importance of diversity}  In this paper, we take a principled direction towards addressing the information redundancy issue in keyphrase generation models. We propose to tackle this problem directly during the training stage, rather than applying adhoc post-processing at inference time. Specifically, we adopt the neural unlikelihood training  objective , whereby the decoder is penalized for generating undesirable tokens. % , which in our case corresponds to the set of repeating tokens.  introduce unlikelihood training for a language model setting. Since we work with a S2S setup, our version of UL loss consists of two components:  a target token level UL loss based on the target vocabulary to penalize the model for generating repeating tokens;  a copy token level UL loss based on the dynamic vocabulary of source tokens required for copy mechanism , which penalizes the model for copying repetitive tokens.   S2S models trained with maximum likelihood estimation  are usually tasked with the next token prediction objective. However, this does not necessarily incentivize the model to plan for future token prediction ahead of time. We observe such lack of model planning capability in our initial experiments with MLE models and to overcome this issue we propose to use -step ahead token prediction. This modified training objective encourages the model to learn to correctly predict not just the current token, but also tokens upto -steps ahead in the future. We then naturally incorporate UL training on the -step ahead token prediction task.  We summarize our contributions as follows:  To improve the diversity of generated keyphrases in a principled manner during training, we adopt the unlikelihood objective for the S2S setting and propose a novel copy token unlikelihood loss.  In order to incentivize model planning, we augment our training objective function to incorporate -step ahead token prediction. Additionally, we also introduce the -step ahead unlikelihood losses.  We propose new metrics for benchmarking keyphrase generation models on diversity criterion. We carry out experiments on datasets from three different domains  and validate the effectiveness of our approach.  We observe substantial gains in diversity while maintaining competitive output quality.  [!t] |}     \toprule     Title & semi automated schema integration with sasmint  \\     \midrule     Abstract & the emergence of increasing number of collaborating organizations has made clear the need for supporting interoperability infrastructures , enabling sharing and exchange of data among organizations . schema matching and schema integration are the crucial components of the interoperability infrastructures , and their semi automation to interrelate or integrate heterogeneous and autonomous databases in collaborative networks is desired . the semi automatic schema matching and integration sasmint system introduced in this paper identifies and resolves   \\     % \midrule     % \multicolumn{1}{r}{} & \multicolumn{1}{r}{} \\     \midrule     Ground Truth & schema integration ; collaboration ; schema matching ; heterogeneity ; data sharing \\     \midrule     MLE Baseline & 	extcolor{red{schema integration} ; sasmint ; \textcolor{red}{schema matching} ; \textcolor{red}{schema integration} ; \textcolor{red}{schema matching} ; sasmint derivation markup language     } \\     \midrule     DivKGen & schema integration ; interoperability infrastructures ; schema matching ; sasmint \\              {red}}.}     In this work, we first point out the shortcomings of MLE based training for keyphrase generation. We specifically address the lack of output diversity issue via the use of unlikelihood training objective. We adopt a target level unlikelihood loss and propose a novel copy token unlikelihood loss, the combination of which provides large diversity gains. In addition, a -step ahead MLE and UL objective is incorporated into the training. Through extensive experiments on datasets from three different domains, we demonstrate the effectiveness of our model for diverse keyphrase generation. For future work, we plan to explore directions that would enable us to simultaneously optimize for quality and diversity metrics.  
"," In this paper, we study sequence-to-sequence  keyphrase generation models from the perspective of diversity. Recent advances in neural natural language generation have made possible remarkable progress on the task of keyphrase generation, demonstrated through improvements on quality metrics such as $F_1$-score. However, the importance of diversity in keyphrase generation has been largely ignored. We first analyze the extent of information redundancy present in the outputs generated by a baseline model trained using maximum likelihood estimation . Our findings show that repetition of keyphrases is a major issue with MLE training. To alleviate this issue, we adopt neural unlikelihood  objective for training the S2S model. Our version of UL training operates at  the target token level to discourage the generation of repeating tokens;  the copy token level to avoid copying repetitive tokens from the source text. Further, to encourage better model planning during the decoding process, we incorporate $K$-step ahead token prediction objective that computes both MLE and UL losses on future tokens as well. Through extensive experiments on datasets from three different domains we demonstrate that the proposed approach attains considerably large diversity gains, while maintaining competitive output quality.\footnote{Code is available at \url{https://github.com/BorealisAI/keyphrase-generation}}",208
" In healthcare, real-world data  refers to patient data routinely collected during clinic visits, hospitalization, as well as patient-reported results. In recent years, RWD's volume has become enormous, and invaluable insights and real-world evidence can be generated from these datasets using the latest data processing and analytical techniques. However, RWD's quality remains one of the main challenges that prevent novel machine learning methods from being readily adopted in healthcare.  Therefore, creating data quality tools is of great importance in health care and health data sciences.  Erroneous data in healthcare systems could jeopardize a patient's clinical outcomes and affect the care provider's ability to optimize its performance.     Common data quality issues include missing critical information about medical history, wrong coding of a condition, and inconsistency in documentation across different care sites. Manual review by domain experts is the gold standard for achieving the highest data quality but is unattainable in regular care practices. Recent developments in the field of Natural Language Processing  has attracted great interest in the healthcare community since algorithms for identifying variables of interest and classification algorithm for diseases  have been recently developed .  In this paper, we presented a novel model for the extraction of queries  in a corpus of dialogue between data entry clinicians and expert reviewers in a multi-site dialysis environment.   %The work's ultimate goal is to identify the data elements that caused most uncertainty or errors during the documentation process.  The main contributions of this work are: [label=]  ) and   sentences that are not questions.  Finally, in addition to evaluating our model's performance in a medical context, we also experimented in section  with a general-domain dataset  to show our model's generalizability.  The rest of the paper is organized as follows. Related work is presented in section . The different question detection methods that will be examined,   are described in section . Section  details the characteristics of the proposed multi-channel CNN model. Finally, the results of the experiments are reported in section  and a conclusion and a plan for future work are given in section .     In this paper, we have provided an analysis of the performance of existing methods for question extraction with real-world misclassification examples that showed the weak point of each method. Furthermore, we have proposed a novel approach for the automatic identification of real questions   and c-questions. We have also shown empirically that the proposed architecture of unifying syntactic, semantic and statistical features achieved a state-of-the-art F1 score for this particular task. Finally, we have presented the relevance of exploiting domain knowledge in the overall performance of a model.  We are in the process of obtaining access to datasets from different application contexts in order to examine the generalizability of our model. As for future work, we plan to extend our work by calculating the similarity of questions in order to create groups of questions that represent the most impactful ``problems'' of a given application environment. Finally, we plan to compare our model with recent  language   representation  models like the BERT model  in  both for the task of question identification and for the task of creating the above mentioned ``problem'' groups.      
"," In most clinical practice settings, there is no rigorous reviewing of the clinical documentation, resulting in inaccurate information captured in the patient medical records. The gold standard in clinical data capturing is achieved via ``expert-review"", where clinicians can have a dialogue with a domain expert  and ask them questions about data entry rules. Automatically identifying ``real questions"" in these dialogues could uncover ambiguities or common problems in data capturing in a given clinical setting.  In this study, we proposed a novel multi-channel deep convolutional neural network architecture, namely Quest-CNN, for the purpose of separating real questions that expect  an answer  about an issue from sentences that are not questions, as well as from questions referring to an issue mentioned in a nearby sentence , which we will refer as ``c-questions"". We conducted a comprehensive performance comparison analysis of the proposed multi-channel deep convolutional neural network against other deep  neural networks. Furthermore, we evaluated the performance of traditional rule-based and learning-based methods for detecting question sentences. The proposed Quest-CNN achieved the best F1 score both on a dataset of data entry-review dialogue in a dialysis care setting, and on a general domain dataset.",209
"  Semantic parsing is the task of mapping a natural language query into a formal language, that is extensively used in goal-oriented dialogue systems. For a given query, such model should identify the requested action  and the associated values specifying parameters of the action . For example, if the query is Call Mary the action is call and the value of slot contact is Mary.  The number of different intents and slots in publicly available datasets  can be close to a hundred and it may be the orders of magnitude larger in real-world systems. Such a big number of classes usually causes a long tail in the class frequency distribution . These tail classes can be significantly improved with small quantities of additional labeled data.    However, training a neural semantic parsing model from scratch can take hours even on a relatively small public dataset . The real-world datasets can contain millions of examples  which can change the time scale to weeks. % Need to describe the problem and motivation to production settings more.  In this work, we propose to fine-tune a model that has already been trained on the old dataset  instead of training a new model to significantly speed up the incorporation of a new portion of data. We call this setting Incremental training, as the new portions of data can be added incrementally.  We focus on semantic parsing % and seq2seq networks for our case studies for the following reasons. Semantic parsing is a more complex NLP task compared to classification or NER and we hope that the lessons learned here would be more widely applicable. Task-oriented semantic parsing tend to have a large output vocabulary that can be frequently updated, and thus, benefit most from the Incremental setting. % We choose seq2seq networks for this work due to two reasons: first, seq2seq networks are very % general and can be easily adapted to simpler tasks like NER; % second, seq2seq models perform really well on popular natural language understanding datasets like TOP and SNIPS.  % Exploring this space of possible solutions, we compare the effectiveness of these approaches with each other and come up with a set of guidelines that are useful for incremental training tasks as well.  % To emulate the ""data-patch"" scenario, we split these datasets by focusing on a few classes. We show that naive fine-tuning leads to catastrophic forgetting and come up with approaches to remedy this. We observe that it is possible to fine-tune models to new classes in a few minutes compared to hours when retraining from scratch. We also compare the effect of pre-trained representations like BERT on fine-tuning. Using these observations we come up with fine-tuning guidelines in scenarios where the label space does not change. We verify that our approaches work on 2 popular semantic parsing datasets: TOP and SNIPS under different data splits.  The main contributions of this work are:         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Related work %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      In this work, we consider a practical side of CL that has been previously overlooked by NLP researchers - the ability to quickly update an existing model with new data. Nowadays, with the performance of models scaling superlinearly with their size, training time becomes a more challenging issue every year. We anticipate that in the near future of billion-parameter-sized models, incremental and continual learning settings can not only lead to a significant advantage in terms of resource efficiency but also become a necessity.  Our experimental results show that a simple incremental setup can reduce computational costs up to 90\ . It is both beneficial in terms of increasing the speed of the development cycle and in terms of the environmental impact that is becoming more and more significant in the field .  We also want to notice some of the negative results we discovered. Training only the top layer  is a surprisingly bad way to include more data. Possibly, because some of the feature-engineering should happen in the lower layers of the model. Also, even though the model quickly fits the fine-tuning data, increasing regularization does not seem to improve the final performance. And finally, a na\""{     TODO: REMOVE THIS BEFORE THE FINAL VERSION!!!  contains hyperparameters used for pretraining. We used the Noam schedule  for the learning rate. Note that it involves  learning rate scaling.   For fine-tuning, the same parameters were used unless otherwise stated in the experiment description. The only exception to this rule is the batch size that we set to 128 during all fine-tuning experiments. Model, optimizer, and learning rate scheduler states were restored from the checkpoint with the best EM.                           Figure  presents fine-tuning experiments for the i.i.d cases. Each model is trained on a fraction  of the original train set and fine-tuned on the rest. We do not plot TP-F1, because with i.i.d split there is no specific split class that we expect to improve and instead, we monitor the performance of the model on all classes. Also, the initial performance baseline is not present at the plot as every model has a different initial performance. Table  presents initial values for every model.  From Figure  we can see that the models with smaller pretrain sets  require almost no data sampling to achieve from scratch performance, while the models that have been trained on a larger subset need up to 50\ . We can also see that the models pretrained on larger amounts of data tend to have higher RD. This can be explained by the fact that these models have better initial performance, thus, more degrading potential.        data from i.i.d splits.     Standard deviation  does not exceed 0.003 in each split.     }            An important question regarding the Iterative training procedure is how much additional data labeling we need to make to improve the model. To answer it, we studied how the amount of fine-tuning data affects the final performance on PATH 99 split.  A random subset of size from 10\  to 90\  of the fine-tuning data was sampled from the fine-tuning data and used as a fine-tuning set in the experiment. The results are depicted in Figure , dynamic sampling method was used to mitigate catastrophic forgetting. Each model was fine-tuned with early stopping patience = 10.  In general, we can say that catastrophic forgetting affects the model more when more new data is present. It can be explained by the fact that to finish a single epoch the model with a bigger new data subset will be updated more times and the network can derive further from the pretrained. This effect is especially visible with the old data = 0 fine-tuning procedure that changes the relative degradation value by an order of magnitude . The results suggest that even an extra hundred of labeled examples can make a significant improvement over the initial model if the target class is very rare in the pretraining data.       In Section  we presented two metrics relative improvement  and relative degradation . Here we demonstrate them on a single plot to compare the scales for a typical task. We can see that even without old data sampling, the model still significantly improves on some classes and the associated relative degradation is significantly lower than the improvement . This can be specific to the semantic parsing and be caused by the fact that every example contains multiple classes including the frequent ones. As the RD is weighted according to the class frequency, it can only be large  if the frequent classes are degraded.       In this section we present additional experimental data, analogous to the one presented in the paper. Figures  and  demonstrate fine-tuning results for the splits NAME EVENT 95 and GET WEATHER 90 respectively. Figures  and  show exact match accuracy for the PATH 90 and GET WEATHER 99 splits respectively.  These plots exhibit a similar to their counterparts from Section  behavior.                
"," A semantic parsing model is crucial to natural language processing applications such as goal-oriented dialogue systems. Such models can have hundreds of classes with a highly non-uniform distribution. In this work, we show how to efficiently  improve model performance given a new portion of labeled data for a specific low-resource class or a set of classes. We demonstrate that a simple approach with a specific fine-tuning procedure for the old model can reduce the computational costs by ~90\% compared to the training of a new model. The resulting performance is on-par with a model trained from scratch on a full dataset. We showcase the efficacy of our approach on two popular semantic parsing datasets, Facebook TOP, and SNIPS.",210
"  Recent progress in abstractive summarization has been fueled by the advent of large-scale Transformers pre-trained on autoregressive language modeling objectives . Despite their strong performance on automatic metrics like ROUGE , abstractive models are not as straightforward and interpretable as their extractive counterparts. Free-form generation in these models also leads to serious downstream errors, such as factual inconsistencies with the input document . Although the interpretability of NLU models has been extensively studied , summarization models specifically have not received similar attention, with analysis efforts often focused on datasets and evaluation .  %Generic explanation methods for language models  or neural machine translation models  are not entirely applicable, as summarization models typically have different interactions with the input document.  In this work, we focus on interpreting and understanding abstractive summarization models through the lens of decoder uncertainty, or the entropy of decisions during generation. While uncertainty in generation has been studied from the perspective of data , sampling , and training  , it is underutilized as a technique for analysis and inspection of generation systems. We study two prominent summarization models, PEGASUS  and BART , fine-tuned on two English summarization datasets, CNN/Daily Mail  and XSum , to understand model behavior in each setting. %We analyze the model using both blackbox and whitebox perspectives.  First, by comparing -grams between the input document and generated summaries, we establish two coarse types for decoded tokens, copy and generate . We find that the entropy of the generation decision correlates with whether the model is copying or generating, as well as where in the sentence the token is. This paints a picture of certain contexts being more restrictive from the standpoint of generation, particularly early in sentences where a model has not ``decided'' what to copy yet, and illustrates the interaction of content selection and lexical choice. %Furthermore, it illustrates the interaction of content selection and lexical choice: new bigrams are higher entropy, but beginnings of sentences are also high entropy, indicating that the model has some uncertainty about what sentence to discuss, even if it is going to copy. Second, we extend this analysis by looking at how uncertainty relates to the syntax of the generated sentence: whether uncertainty connects to syntactic notions of surprisal  and how the entropy varies across certain syntactic productions. % Finally, we derive a way to quantify decoder attention by aggregating self-attention heads, and investigating the correspondence between the prediction entropy and the fraction of the decoded tokens in the aggregated attention.\todo{change this sent to refer to entropy more} Finally, we derive a way to quantify decoder attention by aggregating distinct self-attention heads, revealing the correlation between the attention entropy and prediction entropy, and investigating the correspondence between the prediction entropy and the fraction of the past and future decoded tokens. % highly attentive positions and decoded or not-yet-decoded tokens with respect to specific Transformer layers in the decoder.  Taking this analysis together, we find that the abstractiveness of reference summaries fundamentally changes model behavior: the extractive nature of CNN/DM makes most of its decisions low entropy and copy-oriented while the model maintains higher uncertainty on XSum, yielding more abstractive summaries. More broadly, we show that uncertainty is a simple but effective tool to characterize decoder behavior in text generation. %  By analyzing decoder self-attention layers, we find that when the attention only focuses on a few tokens, the prediction entropy will be fairly low and the focused tokens are very likely to be predicted.      This work analyzes pre-trained summarization models via uncertainty, or the entropy of decoding decisions. We pursue several lines of inquiry: uncertainty can help us understand copying document spans vs.~generating novel text, the behavior of models in different syntactic environments, and coarse properties of the model's attention distribution. All of these give insight into what conditions most heavily restrict the model's generation: generating an observed bigram , low syntactic distance, and attention which can easily identify decoder context in the source document. We believe this approach can power future analyses of pre-trained text generation systems.  
"," % An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this inherent flexibility makes it difficult to interpret and understand model behavior. In this work, we adopt a data-driven methodology to unpack decoder behavior in both a blackbox and whitebox way. We fine-tune and analyze a GPT-2  model on two benchmark datasets featuring different levels of abstraction. Our experiments yield three key results. First, by analyzing the entropy of model predictions and its corresponding test-time behavior, we find a strong correlation between low entropy and where the model copies document spans rather than generating novel text. Second, this entropy analysis can allow us to understand what sentence positions and even what syntactic configurations are associated with copying existing content. Finally, by analyzing decoder self-attention patterns, we can trace this copying behavior to a particular pattern of attending to immediate decoder context and finding the next token to generate in the source document. An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior.  In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or uncertainty, of the model's token-level predictions. For two strong pre-trained models, PEGASUS  and BART  on two summarization datasets, we find a strong correlation between low prediction entropy and where the model copies tokens rather than generating novel text. The decoder's uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens, giving a sense of what factors make a context particularly selective for the model's next output token. Finally, we study the relationship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model. We show that uncertainty is a useful perspective for analyzing summarization and text generation models more broadly.\footnote{Code is available at \url{https://github.com/jiacheng-xu/text-sum-uncertainty}} % can trace this copying behavior to a particular pattern of attending to immediate decoder context and finding the next token to generate in the source document.",211
"  Neural attention mechanisms have been widely applied in  computer vision and have been shown to enable neural networks to only focus on those aspects of their input that are important for a given task. While neural networks are able to learn meaningful attention mechanisms using only supervision received for the target task, the addition of human gaze information has been shown to be beneficial in many cases. An especially interesting way of leveraging gaze information was demonstrated by works incorporating human gaze into neural attention mechanisms, for example for image and video captioning or visual question answering.  While attention is at least as important for reading text as it is for viewing images, integration of human gaze into neural attention mechanisms for natural language processing  tasks remains under-explored. A major obstacle to studying such integration is data scarcity: Existing corpora of human gaze during reading consist of too few samples to provide effective supervision for modern data-intensive architectures and human gaze data is only available for a small number of NLP tasks. For paraphrase generation and sentence compression, which play an important role for tasks such as reading comprehension systems, no human gaze data is available.  We address this data scarcity in two novel ways: First, to overcome the low number of human gaze samples for reading, we propose a novel hybrid text saliency model  in which we combine a cognitive model of reading behavior with human gaze supervision in a single machine learning framework. More specifically, we use the E-Z Reader model of attention allocation during reading to obtain a large number of synthetic training examples. We use these examples to pre-train a BiLSTM network with a Transformer whose weights we subsequently refine by training on only a small amount of human gaze data. We demonstrate that our model yields predictions that are well-correlated with human gaze on out-of-domain data. Second, we propose a novel joint modeling approach of attention and comprehension that allows human gaze predictions to be flexibly adapted to different NLP tasks by integrating TSM predictions into an attention layer. By jointly training the TSM with a task-specific network, the saliency predictions are adapted to this upstream task without the need for explicit supervision using real gaze data. Using this approach, we outperform the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10\% in BLEU-4 and achieve state of the art performance on the Google Sentence Compression corpus. As such, our work demonstrates the significant potential of combining cognitive and data-driven models and establishes a general principle for flexible gaze integration into NLP that has the potential to also benefit tasks beyond paraphrase generation and sentence compression.     In this work we made two novel contributions towards improving natural language processing tasks using human gaze predictions as a supervisory signal. First, we introduced a novel hybrid text saliency model that, for the first time, integrates a cognitive reading model with a data-driven approach to address the scarcity of human gaze data on text. Second, we proposed a novel joint modeling approach that allows the TSM to be flexibly adapted to different NLP tasks without the need for task-specific ground truth human gaze data. We showed that both advances result in significant performance improvements over the state of the art in paraphrase generation as well as competitive performance for sentence compression but with a much less complex model than the state of the art. We further demonstrated that this approach is effective in yielding task-specific attention predictions. Taken together, our findings not only demonstrate the feasibility and significant potential of combining cognitive and data-driven models for NLP tasks -- and potentially beyond -- but also how saliency predictions can be effectively integrated into the attention layer of task-specific neural network architectures to improve performance.            
"," A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing . We propose a novel hybrid text saliency model  that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10\% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.",212
" In recent years, abstractive summarization  has made impressive progress with the development of sequence-to-sequence  framework . This framework is composed by an encoder and a decoder. The encoder processes the source text and extracts the necessary information for the decoder, which then predicts each word in the summary. Thanks to their generative nature, abstractive summaries can include novel expressions never seen in the source text, but at the same time, abstractive summaries are more difficult to produce compared with extractive summaries  which formed by directly selecting a subset of the source text. It has been also found that seq2seq-based abstractive methods usually struggle to generate out-of-vocabulary  words or rare words, even if those words can be found in the source text. Copy mechanism  can alleviate this problem and meanwhile maintain the expressive power of the seq2seq framework. The idea is to allow the decoder not only to generate a summary from scratch but also copy words from the source text.  Though effective in English text summarization, the copy mechanism remains relatively undeveloped in the summarization of some East Asian languages e.g. Chinese. Generally speaking, abstractive methods for Chinese text summarization comes in two varieties, being word-based and character-based. Since there is no explicit delimiter in Chinese sentence to indicate word boundary, the first step of word-based methods  is to perform word segmentation . Actually, in order to avoid the segmentation error and to reduce the size of vocabulary, most of the existing methods are character-based . When trying to combine the character-based methods in Chinese with copy mechanism, the original ``word copy'' degrades to ``character copy'' which does not guarantee a multi-character word to be copied verbatim from the source text . Unfortunately, copying multi-character words is quite common in Chinese summarization tasks. Take the Large Scale Chinese Social Media Text Summarization Dataset   as an example, according to Table I, about 37\% of the words in the summaries are copied from the source texts and consist of multiple characters.      	%	\Large 	   	   		\resizebox{0.7\linewidth}{!}{ 			{ccc}   				&Copied&Generated\\ \hline 				1&21.6\%&12.3\%\\ 				2&28.9\%&21.8\%\\ 				3&7.6\%&7.7\%\\\hline 			 		} 	  	   	   Selective read  was proposed to handle this problem. It calculates the weighted sum of encoder states corresponding to the last generated character and adds this result to the input of the next decoding step. Selective read can provide location information of the source text for the decoder and help it to perform the consecutive copy. A disadvantage of this approach, however, is that it increases reliance of present computation on partial results before the current step which makes the model more vulnerable to the errors accumulation and leads to exposure bias during inference.  Another way to make copied content consecutive is through directly copying text spans. Zhou et al.  implement span copy operation by equipping the decoder with a module that predicts the start and end positions of the span. Because a longer span can be decomposed to shorter ones, there are actually many different paths to generate the same summary during inference, but their model is optimized by only the longest common span at each time step during training, which exacerbates the discrepancy between two phases. In this work, we propose a novel lexicon-constrained copying network . The decoder of LCN can copy either a single character or a text span at a time, and we constrain the text span to match a potential multi-character word. Specifically, given a text and several off-the-shell word segmentators, if a text span is included in any segmentation result of the text, we consider it as a potential word. By doing so, the number of available spans is significantly reduced, making it is viable to marginalize over all possible paths during training. Furthermore, during inference, we aggregate all partial paths on the fly that producing the same output using a word-enhanced beam search algorithm, which encourages the model to copy multi-character words and facilitates the parallel computation.  To be in line with the aforementioned decoder, the encoder should be revised to learn the representations of not only characters but also multi-character words. In the context of neural machine translation, Su et al.  first organized characters and multi-character words in a directed graph named word-lattice. Following Xiao et al. , we adopt an encoder based on the Transformer  to take the word-lattice as input and allow each character and word to have its own hidden representation. By taking into account relative positional information when calculating self-attention, our encoder can capture both global and local dependencies among tokens, providing an informative representation of source text for the decoder to make copy decisions.   Although our model is character-based , it can directly utilize word-level prior knowledge, such as keywords. In our setting, keywords refer to words in the source text that have a high probability of inclusion in the summary. Inspired by Gehrmann et al. , we adopt a separate word selector based on the large pre-trained language model, e.g. BERT  to extract keywords. When the decoder intends to copy words from the source text, those selected keywords will be treated as candidates, and other words will be masked out.  Experimental results show that our model can achieve better performance when incorporating with the word selector.     In this paper, we propose a novel lexicon-constrained copying network for Chinese summarization. Querying the multigranularity representation learned by our encoder, our decoder can copy either a character or a multi-character word at each time step. Experiments on the LCSTS dataset show that our model is superior to the Transformer baselines and quite competitive with the latest models. With the help of keyword information provide by the word selector, it can even achieve state-of-the-art performance. In the future, we plan to apply our model to other tasks, such as comment  generation, and to other languages, such as English.                      \hfill mds   \hfill August 26, 2015       An example of a floating figure using the graphicx package.   Note that \label must occur AFTER  \caption.   For figures, \caption should occur after the .     Reminder: the ""draftcls"" or ""draftclsnofoot"", not ""draft"", class   option should be used if it is desired that the figures are to be   displayed while in draft mode.        Note that the IEEE typically puts floats only at the top, even when this   results in a large percentage of a column being occupied by floats.     An example of a double column floating figure using two subfigures.      The subfigure \label commands are set within each subfloat command,   and the \label for the overall figure must come after \caption.   \hfil is used as a separator to get equal spacing.   Watch out that the combined width of all the subfigures on a   line do not exceed the text width or a line break will occur.         Note that often IEEE papers with subfigures do not employ subfigure   captions , but instead will   reference/describe all of them , , etc., within the main caption.   Be aware that for subfig.sty to generate the , , etc., subfigure   labels, the optional argument to \subfloat must be present. If a   subcaption is not desired, just leave its contents blank,   e.g., \subfloat[].     An example of a floating table. Note that, for IEEE style tables, the   \caption command should come BEFORE the table and, given that table   captions serve much like titles, are usually capitalized except for words   such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to   and up, which are usually not capitalized unless they are the first or   last word of the caption. Table text will default to \footnotesize as   the IEEE normally uses this smaller font for tables.   The \label must come after \caption as always.           Note that the IEEE does not put floats in the very first column   - or typically anywhere on the first page for that matter. Also,   in-text middle  positioning is typically not used, but it   is allowed and encouraged for Computer Society conferences . Most IEEE journals/conferences use   top floats exclusively.   Note that, LaTeX2e, unlike IEEE journals/conferences, places   footnotes above bottom floats. This can be corrected via the   \fnbelowfloat command of the stfloats package.  
"," Copy mechanism allows sequence-to-sequence models to choose words from the input and put them directly into the output, which is finding increasing use in abstractive summarization. However, since there is no explicit delimiter in Chinese sentences, most existing models for Chinese abstractive summarization can only perform character copy, resulting in inefficient. To solve this problem, we propose a lexicon-constrained copying network that models multi-granularity in both encoder and decoder. On the source side, words and characters are aggregated into the same input memory using a Transformer-based encoder. On the target side, the decoder can copy either a character or a multi-character word at each time step, and the decoding process is guided by a word-enhanced search algorithm which facilitates the parallel computation and encourages the model to copy more words. Moreover, we adopt a word selector to integrate keyword information. Experiments results on a Chinese social media dataset show that our model can work standalone or with the word selector. Both forms can outperform previous character-based models and achieve competitive performances.",213
"  Humans are not supervised by the natural language inference . Supervision is necessary for applications in human-defined domains. For example, humans need the supervision of what is a noun before they do POS tagging, or what is a tiger in Wordnet before they classify an image of tiger in ImageNet. However, for NLI, people are able to entail that \textcircled{a} A man plays a piano contradicts \textcircled{b} A man plays the clarinet for his family without any supervision from the NLI labels. In this paper, we define such inference as a more general process of establishing associations and inferences between texts, rather than strictly classifying whether two sentences entail or contradict each other. Inspired by this, we raise the core problem in this paper: {. The exemplar theory argues that humans use { contradicts \textcircled{b} because they cannot happen simultaneously in the same { and \textcircled{b}. We need the commonsense that a man only has two arms, which cannot play the piano and clarinet simultaneously. This commonsense is hard to obtain from the text. However, if we link the sentences to their { and \textcircled{b} is limited.    In order to benefit from multimodal data in plain text inference, we propose the \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning  network. This is shown in Fig.. Its text encoder is decoupled, which only takes the plain text as inputs. Thus it can be directly adapted to downstream NLI tasks. Besides, we use multimodal contrastive loss between the text encoder and the image encoder, thereby forcing the text representation to align with the corresponding image. Therefore even if the text encoder in MACD only takes the plain text as input, it still represents visual knowledge. In the downstream plain text inference tasks, without taking images as input, the text encoder of MACD still implicitly incorporating the visual knowledge learned by the multimodal contrastive loss. Note that we do not need a decoupled image encoder in the SSL. So the image encoder in Fig. in MACD takes texts as inputs to provides a more precise image encoder. We will elaborate this in section.       In this paper, we study the multimodal self-supervised learning for unsupervised NLI.  The major flaw of previous multimodal SSL methods is that they use a joint encoder for representing the cross-modal correlations. This prevents us from integrating visual knowledge into the text encoder. We propose the multimodal aligned contrastive decoupled learning , which learns to represent visual knowledge while using only texts as inputs. In the experiments, our proposed approach steadily surpassed other methods by a large margin.     
","   We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose \underline{M}ultimodal \underline{A}ligned \underline{C}ontrastive \underline{D}ecoupled learning  network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets . The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.",214
"  %閺鍙ラ嚋閸ユ拝绱濋弰顖氱秼閸撳秶娈戞径姘侀崹瀣劥缂冨弶鍎忛崘纰夌礉鐠侇厾绮屾稉娑擃亜銇囧Ο鈥崇烽敍灞藉晙閽傛悂顩撮崚鐧楁稉顏勭毈濡崇烽敍灞剧槨娑擃亜鐨Ο鈥崇烽崘宥呭礋閻欘剟鍣洪崠...  閹存垳婊戦惃鍕煙濞夋洩绱濈拋顓犵矊娑撴稉顏勩亣濡崇烽敍瀹杋netune鏉╂瑤閲滄径褎膩閸ㄥ鎮撻弮鍫曞倸绨睳娑擃亙绗夐崥灞剧箒鎼达妇娈戠亸蹇斈侀崹瀣剁礉閸欘亪娓剁电绻栨稉娑擃亝膩閸ㄥ绻樼悰宀勫櫤閸...   like LayerDrop.  As shown in Figure, we first demonstrate that when there is a large gap between the predefined layer dropout during training and the actual pruning ratio during inference, LayerDrop's performance is poor.  %We attribute it to huge sub-network training space and mismatch between random sampling training and deterministic inference.  To solve this problem, we propose to use multi-task learning to train a flexible depth model by treating each supported depth configuration as a task. We reduce the supported depth space for the aggressive model compression rate and propose an effective deterministic sub-network assignment method to eliminate the mismatch between training and inference in LayerDrop. %Specifically, we design two metrics to determine which sub-network assignment is good.  Experimental results on deep Transformer  show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and LayerDrop.       We demonstrated LayerDrop is not suitable for FDM training because of  the huge sub-network space in training and  the mismatch between training and inference. Then we proposed to use multi-task learning to mitigate it. Experimental results show that our approach can decode with up to 24 depth configurations and obtain comparable or better performance than individual training and LayerDrop. In the future, we plan to explore more effective FDM training methods, and combining flexible depth and width is also one of the attractive directions.   
"," The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices  may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method闁炽儲鏌￠幙鐜測erDrop.",215
"   Targeted sentiment analysis  involves jointly predicting entities which are the targets of an opinion, as well as the polarity expressed towards them . The TSA task, which is part of the larger set of fine-grained sentiment analysis tasks, can enable companies to provide better recommendations , as well as give digital humanities scholars a quantitative approach to identifying how sentiment and emotions develop in literature .  Although there have been many improvements to modelling TSA since the original CRF models , such as utilising Recurrent Neural Networks  , and treating the task as span prediction rather than a sequence labelling task , most of these have concentrated on making the best use of data annotated specifically for the task.  However, annotation for fine-grained sentiment is more taxing and tends to have lower inter-annotator agreement than document or sentence classification tasks . This leads to a lack of available high-quality training data, even for highly resourced languages and prevents TSA models from learning the complex, compositional phenomena which are necessary to correctly predict targeted sentiment in an end-to-end fashion.   We believe this lack of data for fine-grained sentiment analysis leads to TSA models that cannot learn effectively complex compositional phenomena that exists in language, thus making TSA models fragile to highly compositional language. It has also been shown that incorporating compositional information from negation or speculation detection improves sentence-level sentiment classification . Other supervised tasks, such as semantic role labelling , or document level sentiment analysis  have shown promise for improving fine-grained sentiment analysis. Further transfer learning from a self-supervised language-modelling task, commonly referred to as contextualised word representations , has also shown to greatly benefit fine-grained sentiment analysis . Based on this, in this paper, we wish to explore two research questions:           To this end, we propose a multi-task learning  approach to incorporate sources of negation and speculation information into a neural targeted sentiment classifier. We additionally compare our approach with MTL models that use part-of-speech tagging, dependency relation prediction, and lexical analysis as auxiliary tasks, following previous work . Furthermore, in order to overcome the lack of evaluative resources to investigate the effects of negation and speculation, we annotate two new challenge datasets which contain difficult negated and speculative examples.     We find that the MTL models are more robust than the single task learning ,  performing competitively on the majority of the standard datasets while significantly outperforming the STL models on the negation challenge datasets, and on average better than STL models on the speculation challenge datasets. Moreover, we show that when transfer learning is applied, using CWR, to both MTL and STL models, MTL models are no longer significantly better, but are still better on average for the negation challenge dataset and one of the speculation challenge datasets. This result suggests that transfer learning does incorporate some compositional information that is required for negated and speculative samples. However all results on the challenge datasets are considerably lower than the standard dataset, showing that more work is needed to make these models more robust to compositional language.     The contributions of the paper are the following:  [label=\roman*)] %\setlength         In this paper, we have compared the effects of MTL using various auxiliary tasks for TSA and have created a negation and speculation annotated challenge dataset\footnote{Dataset can be found at } for TSA in order to isolate the effects of MTL. We show that TSA methods are drastically affected by negation and speculation effects in the data. These speculation  effects can be  reduced by incorporating speculation  information into the model through MTL. The negation and speculation effects can also be reduced through transfer learning via CWR. Additionally, MTL of negation can still improve the CWR models themselves but not significantly. These findings answer the two original research questions. Whereby we have found that in general MTL using negation  as an auxiliary task does make TSA models more robust to negated  samples. Additionally that transfer learning must have to some extent learnt some compositional knowledge that is useful for classifying negated and speculative samples, leading to more robust TSA models. Lastly that using more general syntactic information as an auxiliary task within MTL creates models that are more robust to both negation and speculation. However the results presented here show how sensitive current models are to linguistic phenomena and suggest that there is still much room for improvement.     As the results from the standard datasets found using MTL does not always improve performance and results from \Fi on Laptop Table  showed that transfer learning can harm performance\footnote{Compare the performance of MTL  using GloVe  to when it uses CWR .}. Additionally the results from the challenge datasets showed that different auxiliary tasks improved the performance of different subtasks of TSA through the extraction and sentiment classification metrics. This may suggest that the target extraction and sentiment classification tasks should not be treated as a collapsed labelling task, as the sentiment and extraction tasks are not similar enough . Future work should consider using a pipeline approach, where each subtask is paired with the most beneficial auxiliary tasks, whereby this approach could lead to MTL and transfer learning better complimenting each other. Finally, we release the code\footnote{}, dataset, and trained models associated with this paper, hyperparameter search details with compute infrastructure , number of parameters and runtime details , and further detailed dev and test results , in line with the result checklist from .           
","   The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning from a language model can improve performance on these challenge datasets. However the results indicate that there is still much room for improvement in making our models more robust to linguistic phenomena such as negation and speculation.",216
"   Word embeddings, continuous vectorial representations of words, have become a fundamental initial step in many natural language processing  tasks for many languages. In recent years,  their cross-lingual counterpart, cross-lingual word embeddings  ---maps of matching words across languages--- have been shown to be useful in many important cross-lingual transfer and modeling tasks such as machine translation , cross-lingual document classification  and zero-shot dependency parsing .    In these representations, matching words across different languages are represented by similar vectors. Following the observation of  that the geometric positions of similar words in two embedding spaces of different languages appear to be related by a linear relation, the most common method aims to map between two pretrained monolingual embedding spaces by learning a single linear transformation matrix. Due to its simple structure design and competitive performance, this approach has become the mainstream of learning CLWE .   Initially, the linear mapping was learned by minimizing the distances between the source and target words in a seed dictionary. Early work from  uses a seed dictionary of five-thousand word pairs. Since then, the size of the seed dictionary has been gradually reduced, from several-thousand to fifty word pairs , reaching a minimal version of only sharing numerals .  More recent works on unsupervised learning  have shown that mappings across embedding spaces can also be learned without any bilingual evidence . More concretely, these fully unsupervised methods usually consist of two main steps : an unsupervised step which aims to induce the seed dictionary by matching the source and target distributions, and then a pseudo-supervised refinement step based on this seed dictionary.  The system proposed by  can be considered  the first successful unsupervised system for learning CLWE. They first use generative adversarial networks  to learn a single linear mapping to induce the seed dictionary, followed by the Procrustes Analysis  to refine the linear mapping based on the induced seed dictionary. While this GAN-based model has competitive or even better performance compared to supervised methods on typologically-similar language pairs, it often exhibits poor performance on typologically-distant language pairs, pairs of languages that differ drastically in word forms, morphology, word order and other properties that determine how similar the lexicon of a language is. More specifically, their initial linear mapping often fails to induce the seed dictionary for distant language pairs . Later work from  has proposed an unsupervised self-learning framework to make the unsupervised CLWE learning more robust. Their system uses similarity distribution matching to induce the seed dictionary and stochastic dictionary induction to refine the mapping iteratively. The final CLWE learned by their system  performs better than the GAN-based system. However, their advantage appears to come from the iterative refinement with stochastic dictionary induction, according to . If we only consider the performance of a model induced only with distribution matching, GAN-based models perform much better. This brings us to our first conclusions, that a GAN-based model is preferable for seed dictionary induction.   Fully unsupervised mapping-based methods to learn CLWE rely on the strong assumption that monolingual word embedding spaces are isomorphic or near-isomorphic, but this assumption is not fulfilled in practice, especially for distant language pairs . Supervised methods are also affected by lack of isomorphism, as their performance on distant language pairs is worse than on similar language pairs. Moreover, experiments by  also demonstrate that the lack of isomorphism does not arise only because of the typological distance among languages, but it also depends on the quality of the monolingual embedding space.   Actually, if we replace the seed dictionary learned by an unsupervised distribution matching method with a pretrained dictionary, keeping constant the refinement technique, the final system becomes more robust .   All these previous results indicate that learning a better seed dictionary is a crucial step to improve unsupervised cross-lingual word embedding induction and reduce the gap between unsupervised methods and supervised methods, and that GAN-based methods hold the most promise to achieve this goal. The results also indicate that a solution that can handle the full complexity of  induction of cross-lingual word embeddings will  show improvements in both close and distant languages.  In this paper, we focus on improving the initial step of distribution matching, using GANs  . Because the isomorphism assumption is not observed in reality, we argue that a successful GAN-based model must not learn only one single linear mapping for the entire distribution, but must be able to identify mapping subspaces and learn multiple mappings. We propose a multi-adversarial learning method which learns different linear maps for different subspaces of word embeddings. %specifically for subspaces of the source word embeddings.      In this paper, we propose a multi-adversarial learning method for cross-lingual word embeddings. Our system learns different linear mappings for different source subspaces instead of just learning a single one for the whole source space. The results of our experiments on bilingual lexicon induction on both close languages and the difficult case of typologically-distant languages prove that learning cross-lingual word embeddings with multi-mapping improves the performance over single mapping.        
","  Generative adversarial networks  have succeeded  in inducing  cross-lingual word embeddings ---maps of matching words across languages--- without supervision. Despite these successes, GANs' performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs' incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction show that this method improves performance over  previous single-mapping methods, especially for distant languages.",217
"  In recent years, the effectiveness of utilizing image data in tandem with a text corpus to improve the quality of machine translation has been a source of extensive investigation. Several proposals have been made to incorporate visual data, such as using a doubly-attentive decoder for image and text data , initializing the encoder or decoder hidden state with image features , and using a deliberation network approach to refine translations using image data . However, a common difficulty is the lack of publicly available multimodal corpora, particularly for English-Japanese translation tasks. Currently, two of the only available English-Japanese multimodal datasets are the Japanese extension of the Pascal sentences  and Flickr30k Entities JP , which is a Japanese translation of the Flickr30k Entities dataset .    In order to contribute to the current list of English-Japanese multimodal corpora, we propose a new multimodal English-Japanese corpus with comparable sentences. Comparable sentences are sentences that contain bilingual terms and parallel phrases that describe a similar topic, but are not direct translations . This data is of particular interest due to its natural prevalence across various areas of media. For example, e-commerce sites in different countries may have product descriptions for similar products in different languages, or social media users may comment about images in several different languages.    In this study, we created a large comparable training corpus by compiling the existing image captions from the MS-COCO  and STAIR  captioning datasets.  %By compiling the existing image captions from the MS-COCO  and STAIR  captioning datasets, we were able to create a large comparable training corpus that did not require translation.  Furthermore, for validation and testing purposes, we translated a small subset of MS-COCO captions that contain ambiguous verbs. The advantage of comparable sentences in relation to their available quantity can be clearly seen in Table , with our proposed corpus containing almost twice as many sentence pairs as Flickr30k Entities JP, the current largest parallel multimodal English-Japanese corpus.  As a benchmark of current multimodal NMT models on our corpus, we performed an English-Japanese translation experiment using several baseline models, which confirmed that current NMT models are not well suited to a comparable translation task. %o evaluate our proposed corpus, we performed an English-Japanese translation experiment on several baseline models, which confirmed that current NMT models are not well suited to a comparable translation task. However, we believe that our corpus can be used to facilitate research into creating multimodal NMT models that can better utilize comparable sentences.  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } {2.6ex}}   {0pt}} [1]{>{\raggedright\let} [1]{>{} [1]{>{\raggedleft\let}  [t!]              \toprule         \T\B Corpus & Source & Type & \T\B \# Images & \T\B \# Sentence Pairs\\          & \T\B MS-COCO/STAIR & Comparable  & \T\B  \\                   In this paper, we have proposed a new multimodal English-Japanese corpus with comparable sentences. Based on the baseline performance of this data, we believe that current multimodal NMT models are not well suited to this type of task, and further research is required in order to better leverage the comparable sentences and images together in order to improve translation performance. In the future, we hope to see our corpus used to encourage research into multimodal machine translation tasks with comparable sentences instead of parallel sentences.   
"," Multimodal neural machine translation  has become an increasingly important area of research over the years because additional modalities, such as image data, can provide more context to textual data. Furthermore, the viability of training multimodal NMT models without a large parallel corpus continues to be investigated due to low availability of parallel sentences with images, particularly for English-Japanese data. However, this void can be filled with comparable sentences that contain bilingual terms and parallel phrases, which are naturally created through media such as social network posts and e-commerce product descriptions. In this paper, we propose a new multimodal English-Japanese corpus with comparable sentences that are compiled from existing image captioning datasets. In addition, we supplement our comparable sentences with a smaller parallel corpus for validation and test purposes. To test the performance of this comparable sentence translation scenario, we train several baseline NMT models with our comparable corpus and evaluate their English-Japanese translation performance. Due to low translation scores in our baseline experiments, we believe that current multimodal NMT models are not designed to effectively utilize comparable sentence data. Despite this, we hope for our corpus to be used to further research into multimodal NMT with comparable sentences.",218
"  %\todo[inline]{Why predicting hate-speech is important in general?}  %\todo[inline]{why detecting hate-speech is important in Yahoo news and Yahoo finance?}   %  %What is the problem? %Why is it interesting and important? %Why is it hard?  %Why hasn't it been solved before?  %What are the key components of my approach and results? Also include any specific limitations.  %Hatespeech is speech that ``intended to insult, offend, or intimidate a person because of some trait "". The occurrence of hatespeech has been increasing. It has become easier than before to reach a large audience quickly via social media, causing an increase of the temptation for inappropriate behaviors such as hatespeech, and potential damage to social systems. In particular, hatespeech interferes with civil discourse and turns good people away. Furthermore, hatespeech in the virtual world can lead to physical violence against certain groups in the real world\footnote{https://www.nytimes.com/2018/10/31/opinion/caravan-hate-speech-bowers-sayoc.html}\footnote{https://www.washingtonpost.com/nation/2018/11/30/how-online-hate-speech-is-fueling-real-life-violence}, so it should not be ignored on the ground of freedom of speech.  To detect hatespeech, researchers developed human-crafted feature-based classifiers , and proposed deep neural network architectures . %Online service providers also strive to combat the hatespeech through ranking algorithms, filtering, and suspending or deactivating user accounts. \textcolor{red}{However, blah blah blah}. However, they might not explore all possible important features for hatespeech detection, ignored pre-trained language model understanding, or proposed uni-directional language models by reading from left to right or right to left.   %--> 2. Other deep model for hatespeech detection: either didn't understand fully hateful context , or  ignore pretrained language model understanding and/or uni-directionally understanding language models by reading from left to right or right to left .  Recently, the BERT  model  has achieved tremendous success in Natural Language Processing % . The key innovation of BERT is in applying the transformer to language modeling tasks. %It proposed to do language modeling through two tasks: predicting masked words and predicting the next sentence. A BERT model pre-trained on these language modeling tasks forms a good basis for further fine-tuning on supervised tasks such as machine translation and question answering, .  Recent work on hatespeech detection  has applied the BERT model and has shown its prominent results over previous hatespeech classifiers. However, we point out its two limitations in hatespeech detection domain. First, the previous studies  have shown that a hateful corpus owns distinguished linguistic/semantic characteristics compared to a non-hateful corpus. For instance, hatespeech sequences are often informal or even intentionally mis-spelled, so words in hateful sequences can sit in a long tail when ranking their uniqueness, and a comment can be hateful or non-hateful using the same words .  %For example, ``n1gger'' in the sentence ``i am not a `n1gger' as you have indicated'' is non-hateful, but ``n1gger'' in ``you all are such a n1gger!'' is hateful.  For example, ``dick'' in the sentence ``Nobody knew dick about what that meant'' is non-hateful, but ``d1ck'' in ``You are a weak small-d1cked keyboard warrior'' is hateful \footnote{It is important to note that this paper contains hate speech examples, which may be offensive to some readers. They do not represent the views of the authors. We tried to make a balance between showing less number of hate speech examples and illustrating the challenges in real-world applications.}.  Thus, to better understand hateful vocabularies and contexts, it is better to pre-train on a mixture of both hateful and non-hateful corpora. Doing so helps to overcome the limitation of using BERT models pre-trained on non-hateful corpora like English Wikipedia and BookCorpus. Second, even the smallest pre-trained BERT ``base'' model contains 110M parameters. It takes a lot of computational resources to pre-train, fine-tune, and serve.  %There have been recent efforts reducing  Some recent efforts aim to reduce  the complexity of BERT model with the knowledge distillation technique such as DistillBert  and TinyBert . In these methods, a pre-trained BERT-alike model is used as a teacher model, and a student  model  is trained to produce similar output to that of the teacher model. Unfortunately, while their complexity is reduced, the performance is also degraded in NLP tasks compared to BERT. Another direction is to use cross-layer parameter sharing, such as ALBERT . However, ALBERT's computational time is similar to BERT, since the number of layers remains the same as BERT; likewise, its inference is equally expensive.  Based on the above observation and analysis, we aim to investigate whether it is possible to achieve a better hatespeech prediction performance than state-of-the-art machine learning classifiers, including classifiers based on publicly available BERT model, while significantly reducing the number of parameters compared with the BERT model. By doing so, we believe that performing pre-training tasks from the ground up and on a hatespeech-related corpus would allow the model to understand hatespeech patterns better and enhance the predictive results. However, while language model pretraining tasks require a large scale corpus size, available hatespeech datasets are normally small: only 16K$ %BERT is a modified transformer network architecture. Traditionally, many language tasks such as translation or question answering, are handled using recurrent neural networks, combined with the attention mechanism. This %reflects the fact that we tend to read a sentence from left to right. However, human also read words within context of other words, %some of them could be quite far apart, %instead of only from left to right or right to left in a mechanical way. Furthermore, recurrent network has a memory problem and can not handle long text, due to problems with vanishing or exploding gradient. In addition, it is intrinsically sequential, making the training process slow. Transformer network was proposed to solve these problems. In its setup, each word in the input text has visibility of all other words, through the use of multi-headed attention.   %It has been used %in a variety of NLP tasks as well as in other area such as image processing.     %One of the motivation of this paper is to investigate whether it is possible to achieve performance similar to, or better than the publicly available BERT models, but with smaller models. In doing so, we want to realize considerable saving in training and serving time. Another motivation is to see if it is possible to improve the BERT model further, by introducing changes to the model architecture. The third motivation is the following. The pretrained BERT models are based on % BooksCorpus and English Wikipedia. They have very different characteristics from the dataset of interest to us, which consists of users-generated comments in Yahoo News and Yahoo Finance. Consequently we believe that retraining a language model from scratch % should give us a model that understands % the language of our dataset better.  %\todo[inline]{talking about the limitation of BERT, like it is to complicated and heavy or has too many parameters. Then question is to build a better model, but with less number of parameters?}  The major contributions of our work are:  \squishend  % We organize the paper as followed. % We give related work in Section, and % define the problem we are solving formerly in Section. We present our approach in Section, and show experimental results in Section. We conclude our paper in Section with discussions and future work.  %    In this paper, we presented the  model for detecting hatespeech.  understands the language of the hatespeech datasets better, is 4-5 times faster, uses less than 1/3 of the memory, and has a better performance in hatespeech classification.    Overall,  outperforms 15 state-of-the-art hatespeech detection methods.  We propose to fine tune with ensemble classification heads and a pooling layer, building on top of the learned {HABERTOR} output representations, and deal with different classification tasks separately, together with a regularized adversarial training.   We also show that  generalizes well to two unseen hatespeech datasets, verifying not only its efficiency but also its effectiveness. Overall,  outperforms 15 state-of-the-art hatespeech classifiers and generalizes well to unseen hatespeech datasets, verifying not only its efficiency but also its effectiveness.   For future work, we intend to improve {HABERTOR} by improving its hateful language understanding in two ways. First, we will combine all publicly available hatespeech datasets, then pretrain and fine-tune them all at once and observe its performance. Second, in addition to using regularized adversarial training on the fine-tuning phase, we will use a  regularized adversarial training for the pretraining phase as well .     Finally, we will build a hateful API and release it for public usages.   For future work, we intend to pretrain a hatespeech language model with a larger-scale data, a  For future work, we intend to explore the following directions. Firstly, in the process of working with the datasets, we noticed that perpetrators who disseminate hatespeech often intentionally misspell abusive words, in the hope of evading detecting. We intend to retrain our models with adversarial training data generated by a hatespeech generator that mangles word of known hatespeech comments. Secondly, since the amount of hateful data is very small compared to normal comments, and manually labelling more data is very expensive, we intend to build a generative model to generate new hateful comments and retrain our model with them.       
"," We present our {HABERTOR} model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. {HABERTOR} inherits BERT's architecture, but is different in four aspects:  it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset;  it consists of  components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage;  it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness; and  it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness. Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that {HABERTOR} works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models. In particular, comparing with BERT, our {HABERTOR} is 4$ transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.  %The code and the pretrained models are available at .",219
"   %------------------Previous version------------------ %Since UNMT in low-resource domains is not yet an actively explored field, one may naively approach this problem by training a model on multiple domains and expect it to generalize on the unseen, low-resource domains, e.g., training the model on news and sports domains and evaluating on the biomedical domain. %However, due to domain mismatch, studied on supervised NMT, the model can show inferior performance. %------------------------------------------------------- Unsupervised neural machine translation  leverages unpaired monolingual corpora for its training, without requiring an already labeled, parallel corpus. Recently, the state of the art in UNMT has achieved comparable performances against supervised machine translation approaches. However, in the case of the translation of domain-specific documents, the monolingual data themselves are scarce, and collecting them involves high cost, still suffering from low NMT performance. For instance, a model trained with monolingual data in such a low-resource domain, say, the medical domain, can experience degraded translation quality due to overfitting.  %------------------Previous version------------------ %Another reasonable approach is transfer learning, which has been frequently used for domain adaption in the literature of supervised NMT and often showed improvements in the target domain. The model is pretrained with multiple domains and then finetuned with the new domain. However, this approach may suffer from overfitting  and catastrophic forgetting when given a small number of training data and a large domain gap in a downstream task. %-------------------------------------------------- Yet, UNMT for low-resource domains is not an actively explored field. One naive approach is to train a model on high-resource domains  while hoping it to generalize on an unseen low-resource domain  as well. However, it has been shown from recent studies on supervised NMT that a nontrivial domain mismatch can significantly cause low translation accuracy.  Another reasonable approach is transfer learning, or in particular domain adaptation, which has shown  performance improvements in the literature of supervised NMT. In this approach, the model is first pretrained using existing domains and then finetuned using the data in a new domain. However, this approach may suffer from overfitting and catastrophic forgetting due to a small number of training data and a large domain gap.  As an effective method for handling a small number of training data, meta-learning has shown its superiority in various NLP tasks, such as dialog generation, translation, and natural language understanding. However, to the best of our knowledge, it was not applied to tackle the UNMT tasks with a small number of training data, i.e., low-resource UNMT.   In response, this paper extends meta-learning approach for low-resource UNMT, called \toolnameMeta. The objective of \toolnameMeta is to find the optimal initialization for model parameters that can quickly adapt to a new domain even with only a small amount of monolingual data. To be specific, assuming that data from multiple source domains are available, which makes meta-learning applicable, we first pretrain the UNMT model with source domains based on \toolnameMeta and then finetune the model  using a target domain.   Moreover, we propose an improved meta-learning approach called \ourtoolname for low-resource UNMT by explicitly promoting common knowledge across multiple domains as well as generalizable knowledge from a particular domain to another. In particular, our proposed approach prevents the model from overfitting due to a small amount of training data in a new domain.   In summary, our contributions include the following.     %  %We show zero-shot performance to evaluate generalization ability for \ourtoolname, where \ourtoolname outperforms other methods.  % To the best of our knowledge, our work is the first to apply a meta-learning approach to UNMT tasks. Our proposed algorithms can quickly adapt to in-domain with only a few iteration steps. Both \toolnameMeta and \ourtoolname consistently outperform the baseline models up to 3 BLEU scores. Especially, \ourtoolname achieves promising results among others including \toolnameMeta. Besides, we show zero-shot performance to evaluate generalization ability for \ourtoolname, where \ourtoolname outperforms other methods.  %--------------------------------- % 闋冩﹥顫呮 姘╁牗妫撮瀽鎰冲妧闆 闇冨嫴饪洪灇 闈镐緛纾ら瀽鍡㈡緤 鐡垮婀㈣嚙 闇呮﹤濮 general 闋 feature闇屻倢婢 闉涘牕瀚.  % Although each domain is very distance each others in domain adaptation, they share some linguistic features, such as the grammar and basic words.     % Azam: To alleviate the aforementioned challenge, % Azam: To overcome this issue, many %  %   %  %   %Domains can be  %To overcome this issue, one simple approach is a domain mixing that aggregates high-resource and low-resource domains and train the model to adequately translate the low-resource domain. The other approach is a transfer learning that first pretrains on high-resource domains and fine-tunes the low-resource domain.  %Despite the remarkable success on neural machine translation ~, the performance of NMT drops substantially against traditional statistical machine translation  when the training data is scarce  %To overcome the scarcity of training data in languages, variants of multilingual translation approaches have been proposed. These approaches basically exploit high-resource knowledge by aggregating both high-resource and low-resource data to train one single model. The other approach is utilizing transfer learning that the model first pretrains on high-resource data and later fine-tunes on low-resource data. The similar manner follows for the domains as well.  %Moreover, few-shot learning and meta-learning arise in machine learning where both attempt to handle the data scarcity problem. In NMT, ~ re-formulates the model-agnostic meta-learning  algorithm to resolve the low-resource challenge for NMT.  %Although aforementioned approaches tackle the low-resource challenge, the data scarcity problem can still remain because following approaches require parallel data, and building a low-resource language pair  with specialized expertise is costly expensive. Hence, the recent research suggests to rely only on monolingual corpus instead of using parallel corpus. The various unsupervised NMT ~ studies show the reasonable performance comparison to supervised NMT. % 闋冩﹥顫呮 monolingual corpus 姣靛牗鐖涢渻 闆垮姙婢婇煰 鐡寸粖鏅爢鍕冲剨闉 闉氭鏌庨洰鐘惧灛闉涘牗娼 -鏀磋兂鏌涢瀼鎸 鏀撮附寮版皡鏃嶆緫闉 闉濅緟娼夐澗姗冪抽浖..?  %sufficient in-domain data to train the model; however, in real-world, collecting domain specific data requires substantial effort. %building a low-resource language pair  with specialized expertise is costly expensive  % 鏂撴粔鑺 闇屻倢鏌 MT闉 姘氭粚鐖犻灇 闈奉剢鐎 姘╁牗妫撮澗姗冾槬鏃矅顫 闇冨嫴瀚 闋冩﹥姒鹃灇 . % Machine learning 闉愭劤鍔闉 Data scarcity 姘嶈兂鐗 闉濇粔璧 % Domain translation闉 闉 娆锋埄娈ч爟婊岊潊  % unsuperivsed machine translation % data mixing, transfer learning % knowledge gets partially vanished  % 闆垮姙婢婇煰 鐡寸粖鏅 % 闋冩悡鑸堕爟姗佽荡闉欏嫶鏆ｉ澒 transfer learning mixing data 鑷ф瑬娼 姘氣晣鐭 闈奉剣姣 % 闋冩﹥顫呮 parallel setting 闉愭劤鍔 闉濅緟姣勯爟 % parallel 闆垮姙婢婇煰鐡ｇ殰 闈炬﹥顫栭爟姗佽荡鑷 闋屾﹤鎽 % monolingual corpus姣 闈奉剣姣勯爟姗傚 UNMT work闇屻倢婢 闈告繉绠 % unsupervised 闈瑰姜濮ら灇 鏂撴粔鑺抽湆銈屾煄 姣靛韩婢 闆存帥鏅炴瓎 % 闋冩﹥顫呮 monolingual corpus 姣靛牗鐖涢渻 闆垮姙婢婇煰 鐡寸粖鏅爢鍕冲剨闉 闉氭鏌庨洰鐘惧灛闉涘牗娼 -鏀磋兂鏌涢瀼鎸 鏀撮附寮版皡鏃嶆緫闉 闉濅緟娼夐澗姗冪抽浖..? % 鏀撮附螠闋冩﹥妫 闉栧崐螠闆 low-resource UNMT姣 meta-learning algorithm闉欒導顢 闊块附濮 姘氣晥鏅ラ灇 闉濇粚瀚 % 鏃姙銆 meta-nmt 闆茶導顑撶摽鑼у 闆笺倠顩 闉氭尗婀㈤浕 闉栧崐螠鏃拌導濮 unsupervised闉 %  multi doamin 鑷ф洢鈥 %   This paper proposed novel meta-learning approaches for low-resource UNMT. \toolnameMeta leverages multiple source domains to quickly and effectively adapt our model to the target domain. Moreover, we introduce an improved method called \ourtoolname, which enhances aggregate-domain and cross-domain generalization such that the model incorporates the knowledge learned across multiple domains. Eventually, the method prevents the model from overfitting due to a small amount of training data in a new domain, thereby leading to improved performance of low-resource UNMT. We empirically show that our proposed approaches consistently outperform the baseline models with a nontrivial margin.    In this paper, we propose novel meta-learning algorithms for low-resource UNMT. These algorithms leverages multiple source domains to learn common knowledge and then finetune the target domain. By comparing with various baseline models, we empirically show that our proposed algorithms significantly surpass others. Moreover, we introduce an enhanced algorithm, \ourtoolname, which utilizes aggregate-domain and cross-domain losses such that the model incorporates learned knowledge across multiple domains.     Owing to these losses, \ourtoolname quickly adapt a new domain in a few iterations, and further improve the performance in low-resource UNMT .    In experiment section, we demonstrate \ourtoolname quickly pretrain from source domains and finetunes a new domain in a few iteration.     Moreover, \ourtoolname shows superiority in a low resource doamain and   the importance of proposed loss and the effectiveness of proposed algorithms in varying size of a new domain. Moreover, \ourtoolname shows superiority in    Future work, we apply our extended algorithms to computer vision domain tasks, whic are suffer from the data scarcity problem.   In this paper, we propose a novel meta-learning algorithm for low-resource UNMT. The algorithm leverages multiple source domains to learn domain-general information and then finetune the target domain. Moreover, we introduce an enhanced algorithm, \ourtoolname, which utilizes aggregate-domain and cross-domain losses such that the model incorporates learned knowledge across multiple domains. Eventually, the algorithm prevents the model from being over-fitted due to a small amount of training data in a new domain and improves the performance of low-resource UNMT. We empirically show that our proposed algorithms effectively leverage domain-general knowledge and outperform baseline models with a considerable margin.           
"," Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a meta-learning algorithm for unsupervised neural machine translation  that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a significant factor in handling data-scarce domains. Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains to boost the performance of low-resource UNMT. Our model surpasses a transfer learning-based approach by up to 2-4 BLEU scores. Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baseline models.",220
"  % NMT is good but needs lots of parallel data + we should exploit mono data more Neural machine translation  using sequence to sequence architectures  has become the dominant approach to automatic machine translation. While being able to approach human-level performance , it still requires a huge amount of parallel data, otherwise it can easily overfit. Such data, however, might not always be available. At the same time, it is generally much easier to gather large amounts of monolingual data, and therefore, it is interesting to find ways of making use of such data. The simplest strategy is to use backtranslation , %but it can be rather costly since it requires training another model in the opposite translation direction and then creating the source-side synthetic sentences by translating the target-side monolingual corpus. but it can be rather costly since it requires training a model in the opposite translation direction and then translating the monolingual corpus.  % We introduce the compositionality  It was suggested by  that during the development of a general human-like AI system, one of the desired characteristics of such a system is the ability to learn in a continuous manner using previously learned tasks as building blocks for mastering new, more complex tasks. %by combining the knowledge learned from the previously learned simpler tasks. Until recently, continuous learning of neural networks was problematic, among others, due to the catastrophic forgetting . Several methods were proposed , however, %they mostly focused on preserving the knowledge of each task learned by the whole network. they mainly focus only on adapting the whole network  to new tasks while maintaining good performance on the previously learned tasks.  % Summary of our method using EWC %\XXX{toto mozna posunout za nasledujici odstavec + jak resime jejich nedostatky} In this work, we present an unsupervised pretraining method for NMT models using Elastic Weight Consolidation . First, we initialize both encoder and decoder with source and target language models respectively. Then, we fine-tune the NMT model using the parallel data. To prevent the encoder and decoder from forgetting the original language modeling  task, we regularize their weights individually using Elastic Weight Consolidation based on their importance to that task. Our hypothesis is the following: by forcing the network to remember the original LM tasks we can reduce overfitting of the NMT model on the limited parallel data. %\XXX{Ukazujeme, ze metoda funguje, je rychlejis + mame odvozeno, ze by mela fungovat i pro podsite} %\XXX{Zminit rovnou strucne prinosy?}  % Summary of the method we used as a comparison We also provide a comparison of our approach with the method proposed by . They also suggest initialization of the encoder and decoder with a language model. However, during the fine-tuning phase they use the original language modeling objectives as an additional training loss in place of model regularization. Their approach has two main drawbacks: first, during the fine-tuning phase, they still require the original monolingual data which might not be available anymore in a life-long learning scenario. Second, they need to compute both machine translation and language modeling losses which increases the number of operations performed during the update slowing down the fine-tuning process. Our proposed method addresses both problems: it requires only a small held-out set to estimate the EWC regularization term and converges 2-3 times faster than the previous method.\footnote{The speedup is with regard to the wall-clock time. In our experiments both EWC and the LM-objective methods require similar number of training examples to converge.}   %Intro to compositionality %Compositional learning + using previosly learned elementary knowledge to learn more complex model   %Avoiding catastrophic forgetting as key to continual learning and compositionality -> choice of EWC  %Benefits of compositionality in greater scope  + why NMT + LM pretrain? %It is a first step in our ongoing reseach  %The paper is structured as following...     We introduced our work in progress, and exploration of model regularization of NMT encoder and decoder parameters based on their importance for previously learned tasks and its application in the unsupervised pretraining scenario.  which can be used in the unsupervised pretraining scenarios based on their importance for language modeling tasks. We documented that our method slightly improves the NMT performance  when combined with a pretrained target language model. We achieve this improvement at a reduced training time.  while reducing the training time.   We also showed that the method is less effective if the original language modeling task used to pretrain the NMT encoder is too different from the task learned during the fine-tuning. We plan to further investigate whether we can gain improvements by using a different pretraining method for the encoder and how much this task mismatch relates to the learning capacity of the encoder.  
","   This work presents our ongoing research of unsupervised pretraining in neural machine translation . In our method, we initialize the weights of the encoder and decoder with two language models that are trained with monolingual data and then fine-tune the model on parallel data using Elastic Weight Consolidation  to avoid forgetting of the original language modeling tasks.   We compare the regularization by EWC with the previous work that focuses on regularization by language modeling objectives.   %We compare the EWC regularization with the previous work that uses language modeling objectives from the original task for model regularization.      The positive result is that using EWC with the decoder achieves BLEU scores similar to the previous work. However,   the model converges 2-3 times faster and does not require the original unlabeled training data during the fine-tuning stage.      In contrast, the regularization using EWC is less effective if the original and new tasks are not closely related. We show that initializing the bidirectional NMT encoder with a left-to-right language model and forcing the model to remember the original left-to-right language modeling task limits the learning capacity of the encoder for the whole bidirectional context.      %%% POZNAMKY %%%   % Analyza Fisher Information   % - Self-attention projekce are more important than the Feedforward layers   % - Self-attention:   %     - output and value projections are more important at the higher layers   %     - key and query projections are more important at lower layers   % ...      % Previous work    % - requires orig. unlabeled data for MT training -> EWC can estimate Empirical Fisher on small  heldout data   % - is effective even when the orig. and new tasks differ   %  and then using this pretrained encoder in MT  -> EWC is bad at this      % Our work :   % - has nice mathematical definition    % - faster convergence in time    % - works only with decoder  -> little worse than LM obj.   % - method works when task are similar in nature    % - how deep should the unlabeled-data-pretrained enc-dec should be?   %       % why only left-context? previous work shows that with transformer, the drop in performance is not that big + it is much easier to implement       % Future work :   % - Investigate complementarity of EWC and LM obj.    % - Investigate the learning rate schemes    % - Investigate the method in the multimodal/multisource scenario   % - Investigate the method",221
"   Even though machine translation  has greatly improved with the emergence of neural machine translation   and more recently the Transformer architecture , there remain challenges which can not be solved by using sentence-level NMT systems. Among other issues, this includes the problem of inter-sentential anaphora resolution  or the consistent translation across a document , for which the system inevitably needs document-level context information.  In recent years, many works have focused on changing existing NMT architectures to incorporate context information in the translation process . However, often times results are reported only on very specific tasks , making it difficult to assess the potential of the different methods in a more general setting. This, together with the fact that big improvements are typically reported on low resource tasks, gives the impression that document-level NMT mostly improves due to regularization rather than from leveraging the additional context information. In this work we want to give a more complete overview of the current state of document-level NMT by comparing various approaches on a variety of different tasks including an application-oriented E-commerce setting. We discuss both, widely used performance metrics, as well as highly task-specific observations.  Another important aspect when talking about document-level NMT is the applicability in ``real life"" settings. There, when faced with a low resource data scenario, back-translation is an established way of greatly improving system performance . However, to the best of our knowledge, the effect of back-translation data obtained and used by context-aware models has never been explored before. The main contributions of this paper are summarized below:        \setlength            In this work, we give a comprehensive comparison of current approaches to document-level NMT. To draw meaningful conclusions, we report results for standard NMT metrics on four diverse tasks - differing in the domain and the data size. We find that there is no single best approach to document-level NMT, but rather that different architectures work the best on various tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, which is not visible in the corpus-level metric scores.  We also investigate methods to include document-level monolingual data on both source  and target  sides. We argue that the performance improvements from the pre-trained encoder predominantly come from increased training data and other task-specific phenomena unrelated to actual context information utilization. Regarding back-translation, we find that document-level systems seem to benefit more from synthetically generated data than their sentence-level counterparts. We discuss that this is because document-level systems are more robust to sentence-level noise.  We plan to expand our experiments to incorporate document-level monolingual data on both source and target sides. This makes sense just by looking at the data conditions of almost every task: document-level parallel data is scarce, but document-level monolingual data is abundant.    
","  Context-aware neural machine translation  is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT.  We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.",222
" Knowledge Graphs  like Freebase, NELL and Wikidata are extremely useful resources for NLP tasks, such as information retrieval, machine reading, and relation extraction. A typical KG is a multi-relational graph, represented as triples of the form . Obviously, for query , referring to the organization-related references would be more beneficial.   To address the above issues, we propose an \underline{A}daptive \underline{A}ttentional \underline{N}etwork for \underline{F}ew-Shot KG completion , a novel paradigm that takes dynamic properties into account for both entities and references. Specifically, given a task relation with its reference/query triples, FAAN proposes an adaptive attentional neighbor encoder to model entity representations with one-hop entity neighbors. Unlike the previous neighbor encoder with a fixed attention map in, we allow attention scores dynamically adaptive to the task relation under the translation assumption. This will capture the diverse roles of entities through varied impacts of neighbors. Given the enhanced entity representations, FAAN further adopts a stack of Transformer blocks for reference/query triples to capture multi-meanings of the task relation. Then, FAAN obtains a general reference representation by adaptively aggregating the references, further differentiating their contributions to different queries. As such, both entities and references can capture their fine-grained meanings, and render richer representations to be more predictive for knowledge acquisition in the few-shot scenario.  The contributions of this paper are three-fold:   We propose the notion of dynamic properties in few-shot KG completion, which differs from previous paradigms by studying the dynamic nature of entities and references in the few-shot scenario.   We devise a novel adaptive attentional network FAAN to learn dynamic representations. An adaptive neighbor encoder is used to adapt entity representations to different tasks. A Transformer encoder and an attention-based aggregator are used to adapt reference representations to different queries.   We evaluate FAAN in few-shot link prediction on benchmark KGs of NELL and Wikidata. Experimental results reveal that FAAN could achieve new state-of-the-art results with different few-shot sizes.     This paper proposes an adaptive attentional network for few-shot KG completion, termed as FAAN. Previous studies solve this problem by learning static representations of entities or references, ignoring their dynamic properties. FAAN proposes to encode entity pairs adaptively, and predict facts by adaptively matching references with queries. Experiments on two public datasets demonstrate that our model outperforms current state-of-art methods with different few-shot sizes. Our future work might consider other advanced methods to model few-shot relations, and exploiting more contextual information like textual description to enhance entity embeddings.   
","   Few-shot Knowledge Graph閼 completion is a focus of current research, where each task aims at querying閼辩惮nseen facts閼辩郸f a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries. This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations. Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions. Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations. This will be more predictive for knowledge acquisition in the few-shot scenario. Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes. The source code is available at \url{https://github.com/JiaweiSheng/FAAN}.",223
" Automatic summarization is a fundamental task in Natural Language Processing, which aims to condense the original input into a shorter version covering salient information and has been continuously studied for decades . Recently, online multi-speaker dialogue/meeting has become one of the most important ways for people to communicate with each other in their daily works. Especially due to the spread of  COVID-19 worldwide, people are more dependent on online communication. In this paper, we focus on dialogue summarization, which can help people quickly grasp the core content of the dialogue without reviewing the complex dialogue context.   Recent works that incorporate additional commonsense knowledge in the dialogue generation  and dialogue context representation learning  show that even though neural models have strong learning capabilities, explicit knowledge can still improve response generation quality.   It is because that a dialog system can understand conversations better and thus respond more properly if it can access and make full use of large-scale commonsense knowledge. However, current dialogue summarization systems  ignore the exploration of commonsense knowledge, which may limit the performance. In this work, we examine the benefit of incorporating commonsense knowledge in the dialogue summarization task and also address the question of how best to incorporate this information. Figure  shows a positive example to illustrate the effectiveness of commonsense knowledge in the dialogue summarization task.  Bob asks Tom for help because his car has broken down. On the one hand, by introducing commonsense knowledge according to the {, we can know that Bob expects Tom to {ialogue Heterogeneous Graph Network  for incorporating commonsense knowledge by constructing the graph including both utterance and knowledge nodes. Besides, our heterogeneous graph also contains speaker nodes at the same time, which has been proved to be a useful feature in dialogue modeling. In particular, we equip our heterogeneous graph network with two additional designed modules. One is called message fusion, which is specially designed for utterance nodes to better aggregate information from both speakers and knowledge. The other one is called node embedding, which can help utterance nodes to be aware of position information. Compared to homogeneous graph network in related works , we claim that the heterogeneous graph network can effectively fuse information and contain rich semantics in nodes and links, and thus more accurately encode the dialogue representation.   We conduct experiments on the SAMSum corpus , which is a large-scale chat summarization corpus. We analyze the effectiveness of integration of knowledge and heterogeneity modeling. The human evaluation also shows that our approach can generate more abstractive and correct summaries. To evaluate whether commonsense knowledge can help our model better generalize to the new domain, we also perform zero-shot setting experiments on the Argumentative Dialogue Summary Corpus , which is a debate summarization corpus. In the end, we give a brief summary of our contributions:  We are the first to incorporate commonsense knowledge into dialogue summarization task.  We propose a D-HGN model to encode the dialogue by viewing utterances, knowledge and speakers as heterogeneous data.  Our model can outperform various methods.    In this paper, we improve abstractive dialogue summarization by incorporating commonsense knowledge. We first construct a heterogeneous dialogue graph by introducing knowledge from a large-scale commonsense knowledge base. Then we present a Dialogue Heterogeneous Graph Network  for this task by viewing utterances, knowledge and speakers in the graph as heterogeneous nodes. We additionally design two modules named message fusion and node embedding to facilitate information flow. Experiments on the SAMSum dataset show the effectiveness of our model that can outperform various methods. Zero-shot setting experiments on the Argumentative Dialogue Summary Corpus show that our model can better generalized to the new domain.   
"," Abstractive dialogue summarization is the task of capturing the highlights of a dialogue and rewriting them into a concise version. In this paper, we present a novel multi-speaker dialogue summarizer to demonstrate how large-scale commonsense knowledge can facilitate dialogue understanding and summary generation. In detail, we consider utterance and commonsense knowledge as two different types of data and design a Dialogue Heterogeneous Graph Network  for modeling both information. Meanwhile, we also add speakers as heterogeneous nodes to facilitate information flow. Experimental results on the SAMSum dataset show that our model can outperform various methods. We also conduct zero-shot setting experiments on the Argumentative Dialogue Summary Corpus, the results show that our model can better generalized to the new domain.",224
" %\yy{para 1: problem is important, para 2: temporal graph, existing systems, para 3: neural networks, para 4: why difficult: lack of training data, para 5: what do we do} [ht]               %\yy{this is a comment} %\yyc{before correction}{after correction}   %The flow of time is used to chain narratives, reason about causes and effects of events, form a deeper understanding of the past, and postulate the future. Temporal reasoning is crucial for analyzing the interactions among complex events and producing   coherent interpretations of text data . There is a rich body of research on the use of temporal information in a variety of important application domains, including topic detection and tracking, information extraction, parsing of clinical records , discourse analysis, and question answering. %\yy{Aman: Please update the cites based on some quick Google search on temporal reasoning/expressions in IE/TDT/medical .} %Motivated by its ubiquity in text understanding, we undertake the task of extracting temporal graphs from documents. %and a rich understanding of temporal aspects of a document helps humans in reading comprehension.   %Temporal reasoning also plays a critical role in downstream natural language processing  They either admit a lot of noisy events ; we will check this % in the camera-ready version and ask you to change it back.  \TeX}  \title{Neural Language Modeling for Contextualized Temporal Graph Generation}   \date{}                    Current methods for generating event-level temporal graphs have focused on the use of existing Information Extraction  trained on relatively small amounts of hand-labeled data.  Current methods for generating event-level temporal graphs are developed with relatively small amounts of hand-labeled data.   On the other hand, the possibility of using pre-trained neural sequence-to-sequence models for this task has not received sufficient attention, primarily due to the difficulty of obtaining large corpora of human-annotated graphs for temporal reasoning over events. On the other hand, the possibility of using pre-trained language models for this task has not received sufficient attention, primarily due to the difficulty of obtaining large corpora of human-annotated graphs for temporal reasoning over events.  The possibility of using pre-trained language models for generating event-level temporal graphs has not received sufficient attention.  This is primarily due to the difficulty in obtaining large corpora of human-annotated graphs for temporal reasoning over events. This paper addresses this open challenge by first developing a data generation pipeline that uses existing //clustering techniques for automated acquisition of a large corpus of document-graph pairs, and by proposing a new formulation of the graph generation task as a sequence-to-sequence mapping task, allowing us to leverage and fine-tune pre-trained language models for our goal.  \am{note for later: break into two sentences/points}   Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines, which represents transitional IE approaches. Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines, including traditional  techniques.  We experiment with \gptz and show that it achieves large gains over strong baselines on a system created test set and outperforms \caevo on several metrics for a hand-labeled, out-of-domain dataset. We plan to explore techniques for adapting large-scale language models on unseen domains and at multiple granularity levels in the future.  As an exciting extension of our work, we plan to measure the efficacy of the temporal graphs for downstream tasks like narrative extraction and generation of temporal graphs at different granularity levels. 
"," This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity  of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics.\footnote{Code and pre-trained models available at}",225
" Building dialog systems typically requires a large collection of conversation logs that a model can use as training data. Crowd-sourcing is a popular method for generating such data-sets and depending on the aspect of dialog modeling being studied, crowd-sourced workers may be asked to annotate existing chat logs for intents and dialog acts, create dialog summaries, converse with each other based on a script or converse to accomplish tasks or goals etc. For instance, to create datasets for task oriented dialogs, crowd-sourced workers may be provided with a { and an {.   The { worker provides information to the user by querying a knowledge base , if required. Together, the two workers interact with each other via natural language to generate conversations that can involve booking restaurant tables, making train reservations, calling a taxi etc. However, creating large crowd-sourced datasets can be time consuming and expensive.             In this paper, we demonstrated a dialog generation framework that mimics the data creation process employed by crowd-sourced workers. We find that our method is able to generate meaningful conversations that aids the training of end-task dialog models in both, low resource and full data settings. The use of additional simulated data to train end-task dialog models result in a performance improvement of 18-25\  in low resource settings, and when combined with full training data, we find that the performance of a simple GPT2 based end-task model becomes comparable to current state-of-the-art models. The simulation-framework does not make strict assumptions about the domain or dataset and it would be interesting to explore its use in other dialogue tasks such as Persona-Chat  in future work.  which we wish to explore in our future work.  We include qualitiatve results demonstrating the se
"," Popular task-oriented dialog data sets such as MultiWOZ  are created by providing crowd-sourced workers a { and an { instruction, expressed in natural language, which described the task that needed to be accomplished. Crowd-sourced workers played the role of a { to generate dialogs that can involve booking restaurant tables, making train reservations, calling a taxi etc.  In this paper, we present a data creation strategy that uses the pre-trained language model, GPT2 , to { bot and an agent bot.  We train the simulators using a smaller percentage of actual crowd-generated conversations and their corresponding goal instructions. We demonstrate that by using the simulated data, we achieve significant improvements in both low-resource setting as well as in overall task performance. To the best of our knowledge we are the first to present a model %this is the first model proposed  for generating entire conversations by simulating the crowd-sourced data collection process. %To the best of our knowledge we are the first to use inter-bot conversation logs to improve the performance of task oriented dialog systems.",226
" Multilingual machine translation , which can serve multiple language pairs with a single model, has attracted much attention. In contrast to bilingual MT systems which can only serve one single language pair, multilingual models can serve  language pairs  .  The amount of available training data can differ a lot across language pairs and the majority of available MT training data is English-centric  which in practice means that most non-English language pairs do not see a single training example when training multilingual models .  As a consequence, the actual performance of language pairs that do not include English on the source or target side lags behind the ones with large amounts of training data. Further, when increasing the number of languages, it gets  impractical to gather training data for each language pair and  challenging to find the right mix during training. Which is why models tasked with direct translation between non-English pairs either resort to bridging  through a pivot language , or make use of synthetic parallel data   or study the problem under zero-shot settings .     In this study, we make use of the potential pre-existing multi-way property in the training corpora and generate as many direct training examples from pre-existing English-centric training data. If we can find training examples for each language pair in a multilingual mix, we call this model complete Multilingual Neural Machine Translation . cMNMT is then trained on all bilingual pairs between source and target languages by utilizing multi-way aligned training examples that consist of translations of the same sentence into multiple languages. We resurface multi-way aligned training examples by aligning training examples from different language pairs when either their source or target sides are identical .  To make use of this data, the model samples a source and target language from the set of multi-way aligned corpus during training, which allows the model to see language pairs where originally no training data existed . As our experiments support, this method enables us to get access to training data for all tested language pairs ). We will show that it is possible to generate a complete graph for at least a 6-language WMT setup. Some of the WMT training data is multi-way parallel by construction. Nevertheless, we show that we also find many training examples where the source and target origin from different sources. We further show on our 112 languages internal dataset, that we can find sufficient training data for over 12,000 language pairs by only providing 111 English-centric training corpora. This result indicates that it is possible to generate direct training data for many language pairs without the need for crawling new training examples. Our experiments suggest that before falling back to methods like zero-shot translation, you should investigate the structure of your pre-existing training data.  To address the problem of finding the right mix of examples from different language pairs during training, we further introduce a hierarchical sampling strategy that is language-specific . In addition to fixing some chronic issues of MNMT , the proposed sampling strategy efficiently ensures all source-target pairs are covered.  Experiments demonstrate that we can train a cMNMT model on a 30-language-pair WMT setup that outperforms bilingual and multilingual baselines as well as bridging on all non-English language pairs. We further show that the performance of the English language pairs stay stable and do not suffer from the changes in both the training data and the new training data sampling strategy. Furthermore, we share experiments at scale by demonstrating that we can train a cMNMT model that can serve  12,432 language pairs.  Our contribution is three-fold:           In this work, we introduced complete Multilingual Neural Machine Translation  that exploits the multi-way alignment information in the underlying training data to improve translation quality for language pairs where training data is scared or not available. Standard MNMT models are trained on a joint set of different training corpora for a variety of language pairs. cMNMT combines the different corpora and constructs multi-way aligned training examples that consist of translations of the same sentence into multiple languages.   In combination with a novel temperature-based sampling approach that is conditioned on the target language only, we show that cMNMT is superior to the standard MNMT model and the even better-performing bridging approach.    Experimental results on a public WMT 30 language pairs dataset and an in-house 12,432 language pairs dataset demonstrated an average BLEU increase of more than 10 BLEU points for non-English language pairs. This approach leads to a single NMT model that can serve 12,432k language pairs with reasonable quality which also surpasses the translation quality of the bridging approach, which is nowadays used in most modern MT services.    
"," Multilingual Neural Machine Translation  models are commonly trained on a joint set of bilingual corpora which is acutely English-centric . While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora , and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples . We set out to study the use of multi-way aligned examples to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation  and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 111$*$112=12,432 language pairs that provides competitive translation quality for all language pairs.",227
"  Machine Translation  has shown impressive progress in recent years. Neural architectures have greatly contributed to this  improvement, especially for languages with abundant training data.  This progress creates novel challenges for the evaluation of machine translation,  both for human and automated evaluation  protocols.  Both types of evaluation play an important role in machine translation. While human evaluations provide a gold standard evaluation, they involve a fair amount of careful and hence expensive work by human assessors. Cost therefore limits the scale of their application. On the other hand, automated evaluations are much less expensive. They typically only involve human labor when collecting human reference translations and can hence be run at scale to compare a wide range of systems or validate design decisions. The value of automatic evaluations  therefore resides in their capacity to be used as a proxy for human evaluations for large scale comparisons and system development.  The recent progress in MT has raised concerns about whether automated evaluation methodologies reliably reflect human ratings in high accuracy ranges. In particular, it has been observed that the best systems according to humans might fare less well with automated metrics. Most metrics such as \BLEU and TER measure overlap between a system output and a human reference translation. More refined ways to compute such overlap have consequently been proposed.  Orthogonal to the work of building improved metrics,  hypothesized that human references are also an important factor in the reliability of automated evaluations. In particular, they observed that standard references exhibit simple, monotonic language due to human  `translationese` effects. These standard references might favor systems which excel at reproducing these effects, independent of the underlying translation quality. They showed that better correlation between human and automated evaluations could be obtained when replacing standard references with paraphrased references, even when still using surface overlap metrics such as BLEU~. The novel references, collected by asking linguists to paraphrase standard references, were shown to steer evaluation away from rewarding translation artifacts. This improves the assessment of alternative, but equally good translations.  Our work builds on the success of paraphrased translations for evaluating  existing systems, and asks if different design choices could have been made when designing a system with such an evaluation protocol in mind. This examination has several potential benefits: it can help identify choices which improve BLEU on standard references but have limited impact on final human evaluations; or those that result in better translations for the human reader, but worse in terms of standard reference BLEU. Conversely, it might turn out that paraphrased references are not robust enough to support system development due to the presence of `metric honeypots': settings that produce poor translations, but which are nevertheless assigned high BLEU scores.  To address these points, we revisit the major design choices of the best EnglishGerman system from WMT2019 step-by-step, and measure their impact on standard reference BLEU as well as on paraphrased BLEU. This allows us to measure the extent to which steps such as data cleaning, back-translation, fine-tuning, ensemble decoding and reranking benefit standard reference BLEU more than paraphrase BLEU. Revisiting these development choices with the two metrics results in two systems with quite different behaviors. We conduct a human evaluation for adequacy and fluency to assess the overall impact of designing a system using paraphrased BLEU.  Our main findings show that optimizing for paraphrased BLEU is advantageous for human evaluation when compared to an identical system optimized for standard BLEU. The system optimized for paraphrased BLEU significantly improves WMT newstest19 adequacy ratings  and fluency ratings  despite scoring 5 BLEU points lower on standard references.     Prior work has shown that BLEU measured on paraphrased references  has better correlation with human evaluation than BLEU measured on regular references  for the comparison of {. This allowed us to evaluate if the design choices of a modern neural MT system impact \BLEU and \BLEUp differently, including tuning a re-ranking noisy channel model to these metrics. Our experiments followed the setup from the winning newstest19 EnglishGermam entry at WMT19.  For design choices, we observe that \BLEUp seems to emphasize the importance of back-translation even when test sets are source original. On the other end, \BLEUp seems to de-emphasize the importance of ensembles, as the reliable prediction of common language by ensembles is less rewarded by this metric.  Our tuning experiments led to positive results. In human evaluation, the system tuned on \BLEUp showed significant improvements in terms of adequacy and even greater gains in terms of fluency compared to the system tuned on \BLEU. Example translations indicate that the model tuned on \BLEUp produces noticeably less literal translations. Our experiments also highlight a disconnect between regular \BLEU and human evaluation: the system tuned on \BLEUp degrades standard \BLEU scores by over 5 points, while faring significantly better in human evaluation. Paraphrased automatic evaluation therefore seems to be a promising proxy for human evaluation when making design choices for MT systems.  This research opens the question of whether these results can be confirmed over a wide range of language pairs. We also hope to achieve further improvements by refining the paraphrased evaluation protocol.     
","  Automatic evaluation comparing candidate translations to human-generated paraphrases of reference translations has recently been proposed by . When used in place of original references, the paraphrased versions produce metric scores that correlate better with human judgment. This effect holds for a variety of different automatic metrics, and tends to favor natural formulations over more literal  ones. In this paper we compare the results of performing end-to-end system development using standard and paraphrased references. With state-of-the-art English-German NMT components, we show that tuning to paraphrased references produces a system that is significantly better according to human judgment, but 5 BLEU points worse when tested on standard references. Our work confirms the finding that paraphrased references yield metric scores that correlate better with human judgment, and demonstrates for the first time that using these scores for system development can lead to significant improvements.",228
" 	 	 	% 	% The following footnote without marker is needed for the camera-ready 	% version of the paper. 	% Comment out the instructions  and uncomment the 8 lines 	% under ""final paper"" for your variant of English. 	%  	 	Machine Reading Comprehension  has made significant strides with an array of neural models rapidly approaching human parity on some benchmarks such as SQuAD . However, existing methods are still in their infancy at the level of cognitive intelligence. Recently, brain science and psychology provide an important basis for the development of brain-like computing and the simulation of human perception, thinking, understanding, and reasoning abilities . 	 	Thinking is the generalization and indirect reflection of the human brain on the nature, interrelationships and internal regularities of objective things . Two types of thinking are complementary in psychology: inertial thinking 閳 from a previous to a subsequent stimulus 閳 and reverse thinking 閳 from a subsequent to a previous stimulus . Inertial thinking  is a conventional way of thinking, which thinks and solves problems from the previous ideas. 	% It is more likely to form a stereotyped thinking, hinder the development of thinking, and may even lead to rigidity of thinking if using this single way for a long time. 	Reverse thinking  is a creative way of thinking, opposite to the inertial thinking.  	% It can often break through the conventional constraints and obtain greater innovation .  	Specifically, in the MRC task, the two types of thinking can be regarded as a process which reasons from questions  to answers . For example, as shown in Fig. , we can get the answer easily by locating the entities pregnant wowen and loquat. Contrarily, the generative question, which can be reasoned by reading the answer and passage, describes two aspects, including can pregnant women eat loquat and what is the benefit to eat loquat for pregnant women. We hope that this ability of reverse reasoning can improve performance on reading comprehension tasks. 	 	 	Previous methods  only consider a obverse logical relationship, which is based on the given question and the passage. They ignore the reverse relationship between the given passage and the answer. Although the work  proposes a joint model that both asks and answers questions, it couples all the knowledge rather than decopuling modules, which is consistent with the concept of psychology. Similarly, we hypothesize that the ability of reverse reasoning can help models achieve better QA performance. This is motivated partly by observations made in psychology that devising questions while reading can  	% increase scores on comprehension tests.It can  	help students improve in reader-based processing of text . 	 	%In the real world, a fast learning trick is to peek at the answer first. In other words, humans often begin with answers and passages when they encounter unsolvable problems. 	%% Then they attempt to understand why the answer is like this.  	%Then they inadvertently infer the question based on the answer and passage from the reverse side. Obtaining the answer in advance is equivalent to giving a strong supervision signal or some additional clues that does not directly exist in the passage and may require reasoning. This kind of psychological behavior about humans is actually a way of reverse thinking, which considers the problem from the opposite direction and infers the reason based on the conclusion . 	 	Therefore, insights into solutions to the problem can be gained from human cognitive processes. Complementary Learning Systems Theory   suggests that the human brain contains complementary learning systems that support the simultaneous use of many sources of information as we seek to understand an experienced situation. One of the systems acquires an integrated system of knowledge gradually through interleaved learning, including our knowledge of the meanings of words, the properties of frequently-encountered objects, and the characteristics of familiar situations. It is just like inertial thinking that learns relationships between different things in the real world for a long time. The other system is a fast learning system similar to reverse thinking, which is targeted to focus on stimulating and enhancing infrequently-utilized circuit areas in the brain from another unusual perspective. 	 	In this paper, we propose the Bi-directional Cognitive Knowledge Framework . And the corresponding Bi-directional Cognitive Thinking Network  is designed to validate the effectiveness of the reverse thinking, as shown in Fig., which will be introduced in detail in Section . 	% explain framework 	%  	 	% In this paper, the proposed method that ... simulates the process of fast learning. 	 	% 	 	 	 	% The gray ovals form an embedding  of specific information 	 	% In order to validate the effectiveness of the reverse thinking, we proposed a Cognitive Bi-directional Thinking Network  corresponding to the Cognitive Bi-directional Thinking Framework, which will be introduced in detail in Section . 	 	The contributions can be summarized as follows: 	 	 		 	 	  	In this paper, we present the Bi-directional Cognitive Thinking Network  which corresponding to the Bi-directional Cognitive Knowledge Framework  from the perspective of psychology. The BCTN answers the question with bi-directional knowledge by simulating the inertial thinking and reverse thinking. And we decouple these two parts of knowledge rather than couple them with the same module. 	  in a bi-directional way of thinking that stemmed from cognitive psychology.  	To determine the stimulus intensity of reverse thinking in memory, we consider the decoded tokens to calculate the score based on the gate mechanism. We show that the proposed BCTN is very effective, it has competitiveness with the previous methods in literature on DuReader with a single model. Our future work will consider to use different datasets and design various models to simulate the behavior of our brain to capture human-level language understanding and intelligence. Finally, we believe that our framework can generalize to other generative tasks, 	such as summarization and image caption. 	 	
"," 		We propose a novel Bi-directional Cognitive Knowledge Framework  for reading comprehension from the perspective of complementary learning systems theory. It aims to simulate two ways of thinking in the brain to answer questions, including reverse thinking and inertial thinking. To validate the effectiveness of our framework, we design a corresponding Bi-directional Cognitive Thinking Network  to encode the passage and generate a question  given an answer  and decouple the bi-directional knowledge. The model has the ability to reverse reasoning questions which can assist inertial thinking to generate more accurate answers. Competitive improvement is observed in DuReader dataset, confirming our hypothesis that bi-directional knowledge helps the QA task. The novel framework shows an interesting perspective on machine reading comprehension and cognitive science.",229
"     % Demonstrating intelligent behavior in complex environments requires agents that can reason about entities and their relationships, and identify regularities in structured data which can help predict the properties-of and relationships-between entities.  % Understanding natural language in realistic settings requires models that can reason about the interactions between content and context, model the dependencies between different textual elements and leverage information about authors when interpreting their content.  For example, when analyzing interactions in a social network, leveraging information about users' social behavior can help identify similarities in the contents of posts they author. Dealing with this type of relational data requires making predictions over multiple, often inter-dependent, variables.    Understanding natural language interactions in realistic settings requires models that can deal with noisy textual inputs, reason about the dependencies between different textual elements and leverage the dependencies between textual content and the context from which it emerges. Work in linguistics and anthropology has defined context as a frame that surrounds a focal communicative event and provides resources for its interpretation . % introduced the term contextualization cues as signalling mechanisms in communication that add to the shared understanding between the participants, into relationships, the situation, and the environment of the conversation    %Say something about debate networks and add some references. As a motivating example, consider the interactions in the debate network  described in Fig.. Given a debate claim , and two consecutive posts debating it , we define a textual inference task, determining whether a pair of text elements hold the same stance in the debate }). This task is similar to other textual inference tasks which have been successfully approached using complex neural representations. In addition, we can leverage the dependencies between these decisions.  For example, assuming that one post agrees with the debate claim }}), and the other one does not }}), the disagreement between the two posts can be inferred:  {}. Finally, we consider the social context of the text. The disagreement between the posts can reflect a difference in the perspectives their authors hold on the issue. While this information might not be directly observed, it can be inferred using the authors' social interactions and behavior. % Given the principle of social homophily, stating that people with strong social ties are likely to hold similar views and authors' perspectives can be captured by representing their social interactions. Exploiting this information requires models that can align the social representation with the linguistic one.  Motivated by these challenges, we introduce \DRAIL, a Deep Relational Learning framework, which uses a combined neuro-symbolic representation for modeling the interaction between multiple decisions in relational domains. Similar to other neuro-symbolic approaches our goal is to exploit the complementary strengths of the two modeling paradigms. Symbolic representations, used by logic-based systems and by probabilistic graphical models, are interpretable, and allow domain experts to directly inject knowledge and constrain the learning problem. Neural models capture dependencies using the network architecture and are better equipped to deal with noisy data, such as text. However, they are often difficult to interpret and constrain according to domain knowledge.   Our main design goal in \DRAIL is to provide a generalized tool, specifically designed for NLP tasks. Existing approaches designed for classic relational learning tasks, such as knowledge graph completion, are not equipped to deal with the complex linguistic input. While others are designed for very specific NLP settings such as word-based quantitative reasoning problems or aligning images with text. We discuss the differences between \DRAIL and these approaches in Section.  % While the examples in this paper focus on modelings various argumentation mining tasks and their social and political context, the same principles can be applied to wide array of NLP tasks with different contextualizing information, such as images that appear next to the text, or prosody when analyzing transcribed speech, to name a few examples. %TODO: explain why DRAIL is specifically useful for NLP compared to other languages. We don't do the same type of evaluation  as we are interested in working with raw entities.     %  Entities in \DRAIL are either human-interpretable discrete entities , which we refer to as symbols, or raw entities that have a complex internal structure which cannot be easily represented as a symbol . This view allows us to define two conceptual learning tasks: relations connecting raw and symbolic entities , and relations connecting raw inputs to each other, which define inference tasks .   % \DRAIL uses a declarative language for defining deep relational models. Similar to other declarative languages, it allows users to inject their knowledge by specifying dependencies between decisions using first-order logic rules, which are later compiled into a factor graph with neural potentials.   % In addition to probabilistic inference, \DRAIL also models dependencies using a distributed knowledge representation, denoted \relnets, which provides a shared representation space for entities and their relations, trained using a relational multi-task learning approach. This provides a mechanism for explaining symbols, and aligning representations from different modalities.  %Introduce the s-s, r-r, s-r, distinction as a way to support classification, textual inference, and probabilistic inference. Following our running example, ideological standpoints, such as \PRED{Liberal} or \PRED{Conservative}, are discrete entities embedded in the same space as textual entities and social entities. These entities are initially associated with users, however using \relnets this information will propagate to texts reflecting these ideologies, by exploiting the relations that bridge social and linguistic information . % In the resulting shared embedding space, we can explain these ideological standpoints in terms of users holding them, or texts that express them.%}).    %TODO: what are the research questions    %TODO - explain the difference in task from DRAIL's perspective - argument relations inside a single text, analyzing discussions - the simple case, discussed in the literature, where we predict a symbol , and the debate.org setup where we combine textual inference  with soclia linfo    To demonstrate \DRAIL's modeling approach, we introduce the task of open-domain stance prediction with social context, which combines social networks analysis and textual inference over complex opinionated texts, as shown in Fig. . %Unlike traditional stance prediction tasks, where the prediction problem is defined over a fixed set of issues  ~ , we go beyond coarse-grained definitions, and delve into the specific arguments and questions of each discussion, as shown in Fig. . We follow the intuition that debates are part of a broader online conversation, involving multiple people that contribute or express their support for the different views, and explicitly model these interactions.  % AugensteinD16-1084,P18-2123,C18-1316} %TODO: add some discussion about qualitative evaluation % We complement our evaluation of \DRAIL with two additional tasks, issue-specific stance prediction, where we identify the views expressed in debate forums with respect to a set of fixed issues, and argumentation mining, a document-level discourse analysis task.    %We demonstrate \DRAIL's modeling approach over three challenging problems. Argumentation mining, a document-level discourse analysis task. Debate stance prediction, identifying the views expressed-in, and interactions-between, debate forum posts. Finally, we introduce a new problem, open-domain stance prediction with social context, which combines social networks analysis and textual inference over complex opinionated texts.  In all three tasks we evaluate different modeling choices, obtaining competitive results.    %TODO: contributions %Our contributions are summarized as follows: % %[wide, labelwidth=!, labelindent=0pt] %        %Unrealted TODO: add a discussion about globally normalized RELNETs- the constraints and the multiple objectives shape them.  %homophily, %, This phenomenon was previously used to help overcome language variation issues   % political-social representations %network embedding:we learn a graph embedding, a different way to define social context %graphical models way    In this paper, we motivate the need for a declarative neural-symbolic approach that can be applied to NLP tasks involving long texts and contextualizing information. We introduce a general framework to support this, and demonstrate its flexibility by modeling problems with diverse relations and rich representations, and obtain models that are easy to interpret and expand.  Going forward, we would like to study distant supervision, and to support learning with latent predicates, to reason over relations and properties that are not directly observed.   The code, data and documentation for \DRAIL and the application examples in this paper will be released to the community, to help promote this modeling approach for other applications.   We discuss and evaluate different modeling choices for deep relational learning. We characterize the strengths and weaknesses of symbolic and neural approaches, and suggest a framework for combining them. We demonstrate our framework's flexibility by modeling problems with diverse relations and rich representations, allowing us to focus on the abstractions needed to understand the relevant dependencies in each task. \DRAIL is designed to streamline this process and help alleviate reproducibility challenges.     Going forward, we will continue to explore ways to leverage representation, inference and learning, to gain insights into the challenges and opportunities that these hybrid approaches present us.   
"," Building models for realistic natural language tasks requires dealing with long texts and accounting for complicated structural dependencies. Neural-symbolic representations have emerged as a way to combine the reasoning capabilities of symbolic methods, with the expressiveness of neural networks. However, most of the existing frameworks for combining neural and symbolic representations have been designed for classic relational learning tasks that work over a universe of symbolic entities and relations. In this paper, we present \DRAIL, an open-source declarative framework for specifying deep relational models, designed to support a variety of NLP scenarios. Our framework supports easy integration with expressive language encoders, and provides an interface to study the interactions between representation, inference and learning.",230
"   End-to-end neural models have emerged in recent years as the dominant approach to a wide variety of sequence generation tasks in natural language processing, including speech recognition, machine translation, and dialog generation, among many others. While highly accurate, these models typically operate by outputting tokens from a predetermined symbolic vocabulary, and require integration into larger pipelines for use in user-facing applications such as voice assistants where neither the input nor output modality is text.  In the speech domain, neural methods have recently been successfully applied to end-to-end speech translation , in which the goal is to translate directly from speech in one language to speech in another language. We propose to study the analogous problem of in-image machine translation. Specifically, an image containing text in one language is to be transformed into an image containing the same text in another language, removing the dependency of any predetermined symbolic vocabulary or processing.   In-image neural machine translation is a compelling test-bed for both research and engineering communities for a variety of reasons. Although there are existing commercial products that address this problem such as image translation feature of Google Translate the underlying technical solutions are unknown. By leveraging large amounts of data and compute, end-to-end neural system could potentially improve overall quality of pipelined approaches for image translation. } employ a traditional pipelined approach consisting of separate optical character recognition, translation, and image rendering steps.\todo{orhanf will check this with mobile team. Elman: commented out as suggested by mobile wordlens team. technical solution of wordlens is not publicly available hence this sentence is a bit speculative}  Combining all these components into a single end-to-end neural system could help reduce cascading errors and improve overall translation quality, leveraging large amounts of data and compute. \fi Second, and arguably more importantly, working directly with pixels has the potential to sidestep issues related to vocabularies, segmentation, and tokenization, allowing for the possibility of more universal approaches to neural machine translation, by unifying input and output spaces via pixels.  Text preprocessing and vocabulary construction has been an active research area leading to work on investigating neural machine translation systems operating on subword units , characters  and even bytes  and has been highlighted to be one of the major challenges when dealing with many languages simultaneously in multilingual machine translation , and cross-lingual natural language understanding . Pixels serve as a straightforward way to share vocabulary among all languages at the expense of being a significantly harder learning task for the underlying models.  In this work, we propose an end-to-end neural approach to in-image machine translation that combines elements from recent neural approaches to the relevant sub-tasks in an end-to-end differentiable manner. We provide the initial problem definition and demonstrate promising first qualitative results using only pixel-level supervision on the target side. We then analyze some of the errors made by our models, and in the process of doing so uncover a common deficiency that suggests a path forward for future work.     In this paper, we introduce the task of in-image neural machine translation and develop an end-to-end model that shows promising results on learning to translate text through purely pixel-level supervision. By doing so, we demonstrate a viable first step towards applying such models in more natural settings, such as translating texts, menus, or street signs within real-world images. Future work should explore models that do not rely on off-the-shelf text tokenizers to decompose the very hard image generation problem into sequence of simpler image predictions. We hypothesize that discrete latent variables  are best suited to implicitly segment the image and capture the sequential nature of this task.   Through a qualitative analysis of the system outputs, we find that this task presents some unique challenges not present in traditional symbolic formulations of sequence generation. We argue that exploring alternate model formulations involving discrete latent variable sequences is a natural next step for future work.    
"," In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.",231
"    [t!] {2mm}). External data . }          In this work\footnotemark\footnotetext{ .}, we explored different ways in which trained models can be applied to improve AMR parsing performance via self-learning. Despite the recent strong improvements in performance through novel architectures, we show that the proposed techniques improve performance further, achieving new state-of-the-art on AMR 1.0 and AMR 2.0 tasks without the need for extra human annotations.        uncomment to redo bbl    
"," Abstract Meaning Representation  parsing has experienced a notable growth in performance in the last two years, due both to the impact of transfer learning and the development of novel architectures specific to AMR. At the same time, self-learning techniques have helped push the performance boundaries of other natural language processing applications, such as machine translation or question answering. In this paper, we explore different ways in which trained models can be applied to improve AMR parsing performance, including generation of synthetic text and AMR annotations as well as refinement of actions oracle. We show that, without any additional human annotations, these techniques improve an already performant parser and achieve state-of-the-art results on AMR 1.0 and AMR 2.0.%",232
" Transformer based models  have been proven to be very effective in building the state-of-the-art Neural Machine Translation  systems via neural networks and attention mechanism . Following the standard  architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and position-wise feed-forward network.   Multihead attentions and position-wise feed-forward network, together as a basic unit,  plays an essential role in the success of Transformer models.  Some researchers  propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention.   Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow  in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit models advance in expressiveness.  Second, for the multi-unit setting, one unit could mitigate the deficiency of other units and compose a more expressive network, in a complementary way.  In this paper, we propose the Multi-Unit TransformErs , which aim to promote the expressiveness of transformer models by introducing diverse and complementary parallel units. Merely combining multiple identical units in parallel improves model capability and diversity by its varied feature compositions. Furthermore, inspired by the well-studied bagging  and gradient boosting algorithms  in the machine learning field, we design biased units with a sequential dependency to further boost model performance.  Specifically, with the help of a module named bias module, we apply different kinds of noises to form biased inputs for corresponding units. By doing so, we explicitly establish information gaps among units and guide them to learn from each other.  Moreover, to better leverage the power of , we introduce sequential ordering into the multi-unit setting, % by learning a permutaion matrix  to automatically shuffle the outputs of multiple units,  and force each unit to learn the residual of its preceding accumulation.  We evaluate our methods on three widely used Neural Machine Translation datasets, NIST Chinese-English,  WMT'14 English-German and WMT'18 Chinese-English. Experimental results show that our multi-unit model yields an improvement of +1.52, +1.90 and +1.10 BLEU points, over the baseline model  for three tasks with different sizes, respectively.  Our model even outperforms the Transformer-Big on the WMT'14 English-German by 0.7 BLEU points with only 54\% of parameters.  Moreover, as an interesting side effect, our model only introduces mild inference speed decrease  compared with the Transformer-Base model, and is faster than the Transformer-Big model. % which proves the practicability of our methods.   The contributions of this paper are threefold:   {1pt} {1pt} {1pt} ulti-Unit TransformErs , to promote the expressiveness of Transformer models by introducing diverse and complementary parallel units.  by introducing bias module and sequential ordering to further model the diversity and complementariness among different units.     In this paper, we propose Multi-Unit Transformers for NMT to improve the expressiveness by introducing diverse and complementary units.  In addition, we propose two novel techniques, namely bias module and sequential dependency to further improve the diversity and complementariness among units.    We show that merely integrate several identical units can improve model performance and diversity. Furthermore, we introduce Biased Multi-Unit and Sequentially Biased Multi-Unit towards explicit guidance of interaction between units.     We evaluate our methods on two widely used NMT datasets. Experimental results show that our methods can significantly outperform the baseline methods and achieve comparable / better performance compared with existing strong NMT systems. In the meantime, our methods use much fewer parameters and only introduce mild inference speed degradation, which proves the efficiency of our models.        \clearpage   
","     Transformer models  achieve remarkable success in Neural Machine Translation.      Many efforts have been devoted to deepening the Transformer by stacking several units  in a cascade,      while the investigation over multiple parallel units draws little attention.     In this paper, we propose the Multi-Unit TransformErs , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units.     Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity.      Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units.      % need more results and exciting data.     Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT'14 English-to-German and WMT'18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed .      In addition, our methods also surpass the Transformer-Big model, with only 54\% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage. \footnote{Code is available at \url{https://github.com/ElliottYan/Multi\_Unit\_Transformer}}",233
"  %  %     Prior work primarily focused on exploiting visual patterns using carefully crafted features . These rendering-based methods have two major drawbacks: 1) they are expensive since they require downloading all external files including CSS, javascript, and images to render the page to compute visual features; 2) they require carefully crafted heuristics around visual proximity  to work well with these expensive features. In this paper, we propose a novel two-stage neural architecture, named FreeDOM, that can be trained on a small number of seed websites and generalize well to unseen websites  requiring any hand-engineered visual features. %we want to employ neural networks for learning transferable visual features such that we can eliminate the need of rendering and human engagement in crafting textual patterns. %We propose a novel two-stage neural architecture that can directly learn from a few annotated websites just based on their raw HTML content and transfer the models for unseen websites without using any human labels . %We parse HTML documents as DOM Trees of the page and classifies it into one of the target fields. This node-level module combines neighboring character sequences, token sequences, as well as markup  to learn a combined representation for the node. We propose a combination of CNNs and LSTMs and show that it can effectively encode useful features in DOM nodes.  These node representations are encoded individually and inevitably lose some global information useful for an extraction task. In particular, only relying on local node features can cause failure when value nodes have no obvious patterns themselves or their local features are very similar to other non-value nodes. To mimic the signal that may be available through visual features used in rendering-based methods,  we use a relational neural network as our second module . This allows us to model the relationship between a pair of elements using both distance-based and semantic features. The rationale behind this is to learn more global representations of node pairs so that we can jointly predict node labels instead of relying only on local features.   Extensive experimental results on a large-scale public dataset, the Structured Web Data Extraction  corpus, show that our model consistently outperforms competitive baseline methods by a large margin.  The proposed FreeDOM is able to generalize to unseen sites after training on a small number of seed sites. In fact, we show that with training data from just three seed sites, our approach out-performs techniques that use explicit visual rendering features by 3.7 F1 points on average. To the best of our knowledge, our framework is among the first neural architectures that efficiently obtains high-quality representations of web documents for structured information extraction.     %The node-level module itself can predict node labels for identifying values of interested fields, but the encoded local features cannot capture the long-range dependencies between values and thus degenerate in unlabeled target websites. %To address this problem, we further propose a relational neural network. %As the second-stage module, it explicitly models the relations between DOM nodes and effectively learns the page-level constraints for producing structured predictions. % it models the relational features reflected by each node pairs, and finally conducts structured data extraction as a structured prediction problem.  %Our contributions in this paper are three-fold: %%  %	\quad We propose a new state-of-the-art neural architecture for structured data extraction over web documents that is highly transferable across websites.  %	This framework utilizes minimal human efforts in feature engineering and does not require any rendering results, thus making large-scale information extraction much easier and more effort-light. %	\quad The first-stage module is a novel neural model for representing local node features from multiple sources. The second stage moudule instead focus on capturing relational features between DOM nodes and making structured predictions. To the best of our knowledge, our framework is among the first neural architectures that can efficiently obtain high-quality representations of web pages for structured data extraction. We also believe the architecture can be promising in future works that require neural representations of web pages. %	\quad We formulate the data extraction problem as a structured prediction task , and also evaluate recent neural architectures like BLSTM-CRFs  to address the problem, which are widely-used in natural language processing communities. Extensive experimental results on a large-scale public dataset with careful analysis show that our model learns consistently better representations of pages for data extraction even than methods requiring rendering results. %%  %}   %Our contribution is that we propose a novel neural model, FreeDom, for structured data extraction over web documents while using less information  and no hand-crafted features. Extensive experiments on a large-scale public data set  show that the proposed FreeDom outperforms other strong baseline methods while only using raw features. %%ying{The last sentence looks not complete. \yuchen{Done.}} %\tata{Don't say 'less information' emphasize that not requiring visual rendering is cheaper and not requiring hand-crafted features means you can generalize to new tasks better. Need to make this claim more focused so the contributions are clear. We also need to spell out the two stages more clearly 'First stage does blah', 'Second stage does blah'} %\tata{Might be worth adding that entity resolution is not in scope for this work -- ie, we might extract duplicate entries across websites for the same car. There are many papers dealing with that and we're not focused on that in this paper.} %   In this paper, we propose a neural architecture for extracting structured data from web documents. It uses training data from only a few seed sites but generalizes well to other unseen websites in the same vertical. We show that our approach, , beats the previous state-of-the-art performance on a large-scale public dataset consisting of 8 different verticals  by nearly 3.7 F1 points. In particular, it does so without using any expensive rendering-based visual features.  We also discovered that typical sequence labeling techniques from NLP do not work well for this task and presented hypotheses for why that is the case.  We believe that this work opens up multiple avenues for future research in web data extraction. What structured prediction techniques might work better at incorporating information from farther away and work well on large DOM trees with sparse labels?  An even more interesting question is if we can transfer information across verticals? That is, if we are able to do well on one vertical, can we leverage that information somehow to train a model for the next vertical? Will there be a large pre-trained neural encoding model for HTML documents, just like BERT for plain texts?  We believe our work can be also useful for future research that needs to learn a more site-general neural representations for semi-structured documents including web pages, pdf files and so on.         The next two lines define the bibliography style to be used, and    the bibliography file.   \clearpage   \clearpage     \endinput         End of file `sample-authordraft.tex'.  
"," % tata: Jan 26 rewrite of Abstract.  Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes both these limitations.  The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance  and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. Through experiments on a public dataset with 8 different verticals, we show that FreeDOM beats the previous state of the art by nearly 3.7 F1 points on average  requiring features over rendered pages or expensive hand-crafted features. % 3.7 is from Table 2 k=3 .  % tata: Previous version of abstract follows:  %",234
" Relation classification  aims to identify the relation between two specified entities in a sentence.  Previous supervised approaches on this task heavily depend on human-annotated data, which limit their performance on classifying the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community ,  first introduce the few-shot learning to RC task and propose the FewRel %   dataset. % dataset as the benchmark. Recently, many works focus on this task and achieve remarkable performance .  %distant supervision  is proposed to automatically construct training instances for RC. %However, in the dataset extracted by distant supervision, some long-tail relations only have few instances and suffer from data sparsity problem. %Inspired by the success of few-shot learning  methods in the computer vision community, e.g., Matching Network , Relation Network  and Memory-augmented network ,  first introduce FSL to RC to tackle the long tail problem. They use the Prototypical Network , which achieves the state-of-the-art performance on several FSL benchmarks. Recently, many works followed their framework have achieved remarkable performance on the Few-shot RC dataset FewRel . %The prototypical network learns the representation  for each relation based on sampled instances, and then classifies the queries into a set of pre-defined relations. %\CheckedBox   % Even though the existing works perform well, they all assume that there is only one relation in a sentence.   Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted  as evidence. When specified two entities  in the sentence, there is a great opportunity for the instance to be incorrectly categorized as a {{confusing relation}}  instead of the {{true relation}} . % % Specifically,  %is that different entity pairs are usually described  in an input sentence, in which the relation classification of these entity pairs often interferes with each other. %This results in that the entity pairs of these relations are often misclassified into confusing relations by models without the ability of explicitly decoupling easily-confused relations. %Table shows three instances from the FewRel dataset , each of which contains a sentence with two given entities  on the right side, and its positive  and confusing relations  on the left side.  %Previous few-shot methods tend to misclassify these sentences into the confusing relations. the first instance should be categorized as the true relation `parents-child' based on the given entity pair and natural language  expression `a daughter of'. However, since it also includes the NL expression `his wife',  %which describes the confusing relation `husband-wife', it is probably misclassified into this confusing relation `husband-wife'. In this paper, we name it as a relation confusion problem.   %===============================================================================================  % \verb| 	|p{9.2cm}} 	{c|c|p{9.2cm}}  		\toprule % 		{True Relation\times$}  &Sentence \\  			{True Relation} & {Confusing Relation}  &Instance \\  		\midrule 		 		{{parents-child}} & {{husband-wife}}  &{She was {{a daughter of}} prince Wilhelm of Baden and {{his wife}} , as well as an elder sister of .} \\  		\midrule 		 		{{husband-wife}} & {{uncle-nephew}} & He was the youngest son of  and {{his wife}} , and {{the uncle of}} former president George W Bush. \\  		\midrule 		{{{uncle-nephew}}} & {{parents-child}}  &  is {{the son of}} princess Margaret, countess of Snowdon, and the 1st earl of Snowdon, thus he is {{the nephew of}} .\\ 		 	{blue}} and {{red}} words respectively correspond to true and confusing relations.} 	 		  %============================================================================================== To address the relation confusion problem, it is crucial for a model to %  effectively select the information with high relevance to the given entity pair and  be aware of which NL expressions cause confusion and learn to avoid mapping the instance into its easily-confused relation. % To address the relation confusion problem, it is crucial for a model to be aware of which NL expressions cause confusion and how to explicitly distinguish the easily-confused relations.  From these perspectives, we propose two assumptions.  Firstly, in a sentence, words that keep high relevance to the given entities are more important in expressing the true relation. Intuitively, the specified entity information is crucial to identify the true relations.  Secondly, explicitly learning of mapping an instance into its confusing relation with augmented data in turn boosts a few-shot RC model on identifying the true relation. % allowing the model to explicitly learn the confusing relations can help it to identify the true relations. %Intuitively, the specific entities information is helpful to identify the positive relation.  Based on these assumptions, we propose CTEG, a few-shot RC model with two novel mechanisms:  An Entity-Guided Attention  encoder, which leverages the syntactic relations and relative positions between each word and the specified entity pair to softly select important information of words expressing the true relation and filter out the information causing confusion.  A Confusion-Aware Training  method, which explicitly learns to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. %has the ability of explicitly learning to distinguish easily-confused relations. In addition, inspired by the success of pre-trained language models, our approaches are based on BERT , which has been proved effective especially for few-shot learning tasks. %===========================================================================   % Specifically, when encoding a sentence by the attention mechanism, our EGA guides the calculation of the attention score through multiply it by an entity-aware gate. %we adopt the transformer incorporating with a self-attention mechanism to encoding an input instance,  Specifically, the backbone of the encoder of our model is a transformer equipped with the proposed EGA which guides the calculation of self-attention distributions by weighting the attention logits with entity-guided gates. % The gate is a matrix of relevance scores, which are used to measure the importance of each word according to its relevance to the entities. % The gates are used to measure the importance of each word according to its relevance to the entities. The gates are used to measure the relevance between each word and the given two entities. % Two types of position information of the words are used to calculate the scores. One of them is the relative position , which is the relative distance between a word and an entity in the sentence squence. Two types of information for each word are used to calculate its gate. % One of them is the relative position , which is the relative distance between a word and an entity in the sentence squence. One is the relative position  information, which is the relative distance between a word and an entity in the input sequence. The other is syntactic relation which is proposed in this paper, defined as the dependency relations between each word and the entities. % Besides, we further propose the syntax position, defined as the dependency relations between each word and the entities. Based on these information, the entity-guided gates in EGA are able to select those important words and control the contribution of each word in self-attention. % Based on these information, the entity-aware gate in EGA is able to select those important words and control the contribution of each word in the self-attention.    % For the proposed CAT, it allows the model to and asynchronously learn the confusing relations for each sentence. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified sentences and their confusing relations to conduct an additional training process, which aimes to learn the confusing relations explicitly.  We also propose CAT to explicitly force the model to asynchronously learn the classification from an instance to its true relation and its confusing relation. After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations. After that, The CAT uses these misclassified instances and their confusing relations as augmented data to conduct an additional training process, which aims to learn the mapping between these instances into the confusing relations.  % After that, The CAT uses these misclassified sentences and their confusing relations to  conduct an additional training process, which aims to learn the confusing relations explicitly.  Afterwards, the CAT adopts the KL divergence to teach the model to distinguish the difference between the true and confusing relations, which benefits the true relation classification from the confusing relation identification.  % Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even better results to strong baselines in terms of accuracy. % Furthermore, ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem.  The contributions of this paper are summarized as follows:   We propose an Entity-Guided Attention encoder, which can select crucial words and filter out NL expressions causing confusion based on their relevance to the specified entities.    We propose a Confusion-Aware Training process to enhance the model with the ability of distinguishing true and confusing relations.   We conduct extensive experiments on few-shot RC dataset FewRel, ans the results show that our model achieves comparable and even much better results to strong baselines. Furthermore, ablation and case studies verify the effectiveness of the proposed EGA and CAT, especially in addressing the relation confusion problem.    In this paper, we propose CTEG equipped with two novel mechanisms, namely the Entity-Guided Attention  and the Confusion-Aware Training , to address the relation confusion problem in few-shot relation classification . We conduct extensive experiments on benchmark dataset FewRel, and experiment results shows that our model achieves significant improvements on few-shot RC. Ablation studies verify the effectiveness of the proposed EGA and CAT mechanisms. Case study and further analysis demonstrate that our model has the ability of decoupling easily-confused relations. 
"," This paper aims to enhance the few-shot relation classification especially for sentences that jointly describe multiple relations. Due to the fact that some relations usually keep high co-occurrence in the same context, previous few-shot relation classifiers struggle to distinguish them with few annotated instances. To alleviate the above relation confusion problem, we propose CTEG, a model equipped with two mechanisms to learn to decouple these easily-confused relations. On the one hand, an Entity-Guided Attention  mechanism, which leverages the syntactic relations and relative positions between each word and the specified entity pair, is introduced to guide the attention to filter out information causing confusion. On the other hand, a Confusion-Aware Training  method is proposed to explicitly learn to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation. Extensive experiments are conducted on the FewRel dataset, and the results show that our proposed model achieves comparable and even much better results to strong baselines in terms of accuracy. Furthermore, the ablation test and case study verify the effectiveness of our proposed EGA and CAT, especially in addressing the relation confusion problem.",235
"    % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English.  .          % final paper: en-us version           %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }    Complaining is a basic speech act, usually triggered by a discrepancy between reality and expectations towards an entity or event. Social media has become a popular platform for expressing complaints online  where customers can directly address companies regarding issues with services and products. Complaint detection aims to identify a breach of expectations in a given text snippet. However, the use of implicit and ironic expressions and accompaniment of other speech acts such as suggestions, criticism, warnings and threats make it a challenging task. Identifying and classifying complaints automatically is important for:  improving customer service chatbots;   linguists to analyze complaint characteristics on large scale ; and  psychologists to understand the behavior of humans that express complaints.  Previous work has focused on binary classification between complaints and non-complaints in various domains. Furthermore, some studies have performed more fine-grained complaint classification. For instance, complaints directed to public authorities have been categorized based on their topics or the responsible departments. Other categorizations are based on possible hazards and risks as well as escalation likelihood. Most of these previous studies have used supervised machine learning models with features extracted from text  or task-specific neural models trained from scratch. Adapting state-of-the-art pre-trained neural language models based on transformer networks such as BERT and XLNet has yet to be explored.    In this paper, we focus on the binary classification of Twitter posts into complaints or not . We adapt and evaluate a battery of pre-trained transformers which we subsequently combine with external linguistic information from topics and emotions.     New state-of-the-art results on complaint identification in Twitter, improving macro FI by 8.0\% over previous work by  Preotiuc-Pietro et al. ;  A qualitative analysis of the limitations of transformers in predicting accurately whether a given text is a complaint or not.      We evaluated a battery of transformer networks on the Twitter complaint identification task and obtained 87 macro F1, which outperforms the previous state-of-the-art results of Preotiuc-Pietro et al. . We further presented a thorough analysis of the limitations of our models in predicting complaints. In future work, we intend to explore more in how we can combine other sources of linguistic information with transformers as well as information from other modalities such as images.  
","    Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87.",236
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % final paper: en-us version      %   % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } Neural machine translation  achieved enormous success in advancing the quality of translation . In spite of the impressive performance, NMT models are still vulnerable to perturbations in the input sentences  , i.e. a tiny perturbation will affect hidden representation and lead to low quality of translation . Moreover, NMT commonly consists of  millions of parameters, which making it prone to overfitting especially in low resource scene. % Neural machine translation achieved enormous success in advancing the quality of translation . Despite the impressive performance, NMT models are still vulnerable to the scale of data, when training in the small or monotonous data leading to inference with many unexcepted inputs and low quality of translation, the model prone to overfitting concurrently.  % Improve robustness and representation capacity of models is an important problem to solve.  % NMT model adopts deep neural network to modeling the whole translation process, which can train multi features together without prior domain knowledge, developing rapidly in recent years. However, compared with SMT, NMT requires millions of parameters and huge number of data, making it be prone to overfitting especially in low resource scene. Researchers found that NMT is extremely sensitive to input noise -- a tiny perturbation will affect hidden representation -- leading to a low quality of translation. Improve robustness and representation capacity of models is an important problem to be solved. A natural way to improve generalization is synthesizing natural noise  or adopting arbitrary noise .  Another way is exploring regularization techniques to avoid overfitting , making model robust to unseen or unfamiliar inputs. However, as discrete data, the text is hard to retain the semantic information after corruption. % A natural way to improve generalization is data augmentation, make the model see as much data as possible, meanwhile, it's necessary to avoid overfitting. Standard dropout commonly used as a technology for overfitting in neural networks by preventing the neuron obtain complete information from data like noisy perturbations, moreover, the regularization techniques also can be beneficial for overfitting. %    In this paper, we propose Token Drop to prevent overfitting and improve generalization. Different from standard dropout  that drops neurons in network randomly, we drop tokens of the input sentences. In order to retain semantic information, we replace tokens with a special symbol  . This allows model learn hidden representation from rest token's context, and predict target translation condition on latent variable. On the one hand, our method allows model meeting exponentially different sentences can be explained as data augmentation; On the other hand, our method corrupts input sentences with natural noise can be seen as regularization term for NMT.   We investigate two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Considering  our Token Drop method regularize parameters by weakening model inputs, making NMT suitable for applying self-supervised objective. During training:  use a discriminator to detect whether input tokens are dropped or not;  leverage hidden state to predict original tokens of dropped tokens inspired by Cloze task . Both of them guide model to generate semantically similar representation, leading to a better generalization capacity.  % To test our approach, we conduct machine translation task on ZH-EN and EN-RO benchmark, despite compared with strong baseline, we  % In this work, we propose to randomly drop tokens of input sentence, different with standard dropout  , which drop neurons in network randomly during training, we replace tokens with a special symbol unk. Using this method, we call it token drop, for each input sentence, our model see  different sequences theoretically. On the one hand, model receives more data, on the other hand, it can be seen as noise challenge model to learn primary information from latent representation. Our approach is also similar to self-supervised technique, which utilize contextual state to predict masked token, the difference of our training object is to optimize translation ability of source language. By randomly drop tokens of input, forcing model utilize latent representation to make prediction during training, at inference, model see full sentence and make decision easily. To test our approach, we conduct machine translation on ZH-EN and EN-RO corpus, compare to baseline, our token drop training models are more stable and resilient to overfitting.  % Deep neural networks with large number of parameters are prone to overfitting, requires regularization method in practice. One of the most effective way to avoid overfitting is Dropout , by omitting stochastic neurons in networks during training iteration, while maintain all neurons during inference, brings significant improve results on a variety tasks . Different from dropout, which drop single unit, token-level dropout drop  entire token, are  proved be effective on seq2seq task.   In this paper, we have proposed Token Drop mechanism for neural machine translation task. Inspired by self-supervised learning, we introduced Replaced Token Detection and Dropped Token Prediction training objective. We found that NMT model trained with Token Drop gains larger generalization capacity and reduction in overfitting. Even without prior knowledge and additional parameters, our proposed approach reports convincing results on neural machine translation. In future work, we plan to investigate impact of dropping on different words, e.g. word importance and word type.   
","    Neural machine translation with millions of parameters is vulnerable to unfamiliar inputs. We propose Token Drop to improve generalization and avoid overfitting for the NMT model.  Similar to word dropout, whereas we replace dropped token with a special token instead of setting zero to words.  We further introduce two self-supervised objectives: Replaced Token Detection and Dropped Token Prediction. Our method aims to force model generating target translation with less information, in this way the model can learn textual representation better. Experiments on Chinese-English and English-Romanian benchmark demonstrate the effectiveness of our approach and our model achieves significant improvements over a strong Transformer baseline\footnote{Our code is released at \url{https://github.com/zhajiahe/Token_Drop}}.",237
" The short answer test is a type of exam in which participants are asked to answer questions with short answers which can consist of 1-2 sentences. Assessing student short answers on the exam is very challenging for the assessor. When a large number of students are assessed, for example on a national scale, assessors are required to remain consistent and objective in assessing hundreds or even thousands of student responses. The questions with short answer format also allow students to answer in their own style which can be varied for each student. Therefore, computer assistance in making automatic short answer scoring is deemed necessary to address these problems.  In 2019, NLP Research Group from Universitas Gadjah Mada, Indonesia, in collaboration with the Education Assessment Center, Ministry of Education and Culture of Indonesia, held the Ukara 1.0 Challenge\footnote{https://nlp.mipa.ugm.ac.id/ukara-1-0-challenge/}. In this challenge, participants from all over the nation were challenged to make an automatic short answer scoring system for Indonesian student exam. There are two short answer questions on this challenge with correct and incorrect labels. In this paper, we try to describe the improvement of our previous work on the Ukara 1.0 Challenge dataset.  There are several challenges that may arise in applying and making an automatic short answer scoring system. First, it is the number of models and hyperparameters that need to be searched.  In conventional machine learning, a model is trained to solve a specific problem, or in our case, a model is only trained to assess one short answer question. When the number of questions to be assessed increases, the time required for searching the model and tuning its hyperparameters will also increase. The second challenge is the imbalance class. In an exam, the questions given are intended to test students' abilities, therefore the level of difficulty in each question will cause the number of correct responses to be much less than the number of wrong responses. Another challenge in automatic short answer scoring is the small amount of labeled data. The data labeling process is not easy because it requires an expert to validate the student responses and of course this is time consuming and costly.  In several previous studies, making short answer scoring or essay scoring was done using deep learning approaches, for example, using long-short term memory, convolutional neural network, or a combination of both . We take a different approach by using a simpler stacking model because of the small number of available data. In this paper, we used the sentence-level feature as done by . Without word sequence features, the automatic scoring process could be viewed as a text classification problem. The use of stacking models for text classification has been done in  and shows better performance than a single model classifier. We also propose to use an upsampling method, Synthetic Minority Over-sampling Technique , to handle imbalance classes and hyperparameters optimization algorithm, Tree-structured Parzen Estimator  to find a robust model that performs well on each type of question. In this paper, we use hyperparameters term as model components which are preset and untrained . Meanwhile, the term parameters refers to the model components that can be trained  .    In this research, we have shown that the use of stacking model using neural network and xgboost makes it able to get a high score on the Ukara 1.0 challenge dataset. We also propose to use SMOTE and TPE to handle some problems in automatic short answer scoring and also improve the model performance. The model we produced received a combined F1 score of 0.821, better than previously published methods. From the results we obtained, it can be seen that there is still a big difference between F1 Score in question A and question B. This could be due to the different characteristics of the questions. In future research, it can be explored to use different features or methods for each question. Furthermore, it can also be explored the use of multi-task learning methods that train the model for all questions at the same time.  
"," Automatic short answer scoring is one of the text classification problems to assess students' answers during exams automatically. Several challenges can arise in making an automatic short answer scoring system, one of which is the quantity and quality of the data. The data labeling process is not easy because it requires a human annotator who is an expert in their field. Further, the data imbalance process is also a challenge because the number of labels for correct answers is always much less than the wrong answers. In this paper, we propose the use of a stacking model based on neural network and XGBoost for classification process with sentence embedding feature. We also propose to use data upsampling method to handle imbalance classes and hyperparameters optimization algorithm to find a robust model automatically. We use Ukara 1.0 Challenge dataset and our best model obtained an F1-score of 0.821 exceeding the previous work at the same dataset.",238
"  % Second, is a NMT model's ability to handle streaming ASR output; an ASR system provides the best greedy recognition  from a live speech's segmented audio. % we don't really talk about that ^ %deleted simultaneous With the advance of Automatic Speech Recognition  and Neural Machine Translation  systems, speech translation has become increasingly feasible and has received considerable attention.  However, researchers have encountered many challenging problems within the standard cascaded framework where ASR system outputs are passed into NMT systems. First, since NMT models are often trained with clean, well-structured text, the disfluency of spoken utterances and the recognition errors from ASR systems are not modeled by the NMT systems.  Second, people speak differently than they write, which results in changes in both sentence structure and meaning.  Third, automatically predicting sentence boundaries is challenging.  Taken as a whole, poorly segmented sentences with incorrect word recognition leads to poor translations.  These problems pose unique challenges for ASR NMT robustness that are not readily addressed by current methods. %  % % Current approaches to robust NMT with noisy inputs typically focus on improving word transcription through data augmentation techniques. Such methods include disfluency removal where redundant and unnecessary words are removed before translating the transcript, domain adaptation where NMT models are augmented with in-domain training data, and synthetic noise, where random edits are made to training data.   %  % \footnotetext{When compared to Table , the sum of System/System with transcript and segmentation degradation surpasses the Gold/Gold evaluation by 0.57 BLEU points. This is because the modifications to the evaluation data are not directly additive, ie .}  % % % % Although data domain and segmentation issues are often tackled separately, we find their compounded effects, namely erroneous transcript sentence boundaries, are neglected and substantially detrimental to final translation quality.  % As such, our contributions are two fold, we analyze the impact of noisy ASR segmentations on translation and propose an easily adaptable and simple data augmentation strategy to increase NMT robustness. % this sentence can be optional In our experimentation, we found that ASR system punctuation is often imperfect. It may omit or insert sentence-final punctuation, resulting in sentences that are erroneously compounded or fragmented.  While this is corroborated by similar works, which note that degradation of translation is caused by poor system sentence boundary prediction, they do not specifically evaluate, quantify, and address this issue. % should we mention how much sentence boundaries degrade accuracy % Find an example? To tackle the sentence boundary problem, we propose a simple scheme to augment NMT training data, which yields +1 BLEU point on average. % Similar to , we first ascertain that a NMT model can implicitly learn target punctuation from unpunctuated source text, even with noisy imperfect sentence boundaries. % is this sentence above necessary?^ %We show that with a simple data augmentation scheme on both general and in-domain NMT training data, we can achieve an improvement of  and  BLEU for tst2015 and tst2018 respectively.  This procedure is agnostic to ASR systems and can be applied to any NMT model training easily.  %     We propose that sentence boundary errors are a neglected area of study for NMT robustness, especially in the context of speech translation.  We quantitatively demonstrate that poor sentence segmentation degrades performance almost twice as much as transcript level-errors.  To address this, we developed a simple method for data augmentation with immediate gains that can serve as a baseline for future work in segmentation NMT robustness.    Given the simplicity and ease of adaptation into existing systems, we hope to integrate our approach into production systems.  \vfill\pagebreak     References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   ------------------------------------------------------------------------- 
"," Neural Machine Translation  models have demonstrated strong state of the art performance on translation tasks where well-formed training and evaluation data are provided, but they remain sensitive to inputs that include errors of various types. Specifically, in the context of long-form speech translation systems, where the input transcripts come from Automatic Speech Recognition , the NMT models have to handle errors including phoneme substitutions, grammatical structure, and sentence boundaries, all of which pose challenges to NMT robustness.  %This paper makes two main contributions via an in-depth error analysis and a proposed solution.  Through in-depth error analysis, we show that sentence boundary segmentation has the largest impact on quality, and we develop a simple data augmentation strategy to improve segmentation robustness.",239
" Automatic summarization is the automated process of reducing the size of an input text while preserving its most relevant information content and its core semantics. Techniques for summarization are often characterized as being either:  or .  methods construct summaries by combining the most salient passages  of a source text; a process similar to human's way of identifying the right information. One way to achieve extractive summarization is to define the problem as a sentence classification task, using some form of  representation of the sentences in a document . To avoid content overlap issues, previous work has used sentence reranking  or sentence ordering by extracting sentences recurrently .  methods generate summaries by generating new sentence constructs ``from scratch'', or from representation of document content, a process that is conceptually more similar to the notion of paraphrasing. Abstractive text summarization has attracted interest since it is capable of generating novel formulations of summaries using language generation models conditioned on the source text. Several attention-based Recurrent Neural Network  encoder-decoders have been introduced to tackle varying text generation issues of standalone abstractive sequence-to-sequence  models. Copy and pointer mechanisms , for example, have enabled decoders to better generate unseen words, out-of-vocabulary words and named entities.   Most recently, hybrid extractive and abstractive architectures have been proposed and have shown promising results in both quantitative performance measures and human evaluations. In such set-ups, the extractive model first selects salient sentences from a source article, and the abstractive model paraphrases the extracted sentences into a final summary. The majority of current state-of-the-art abstractive summarization models\footnote{Excluding summarization models using large scale pre-trained language models such as BERT } are based on the hybrid approach .  Nonetheless, hybrid models can be limited by three disadvantages. First, since ground-truth labels for extractive summarization are usually not provided, extractive labels must be generated by a potentially suboptimal algorithm . The performance of models trained with such labels is therefore bounded by the quality of the performance of the extractive heuristics. Second, since ground-truth binary labels for recurrently extracted sentences are typically teacher forced as in , ``exposure bias''  may negatively affect content selection performance at inference. Finally, given that the hard extraction step is not differentiable, existing hybrid models typically require multi-step training   or reinforcement learning  to train the whole model.  In this paper, we introduce a novel abstractive summarization model that incorporates an intermediate extractive step but does not require labels for this type of extractive content selection, and it is fully end-to-end trainable. To achieve this, we propose a new memory augmented encoder-decoder  architecture  called Mem2Mem. Mem2Mem has 2 memorization modes:  absorb key information of the encoded source sequence via a compression mechanism, and  sequentially update the external memory during target summary generation. Without using extractive ground-truth labels, we find in our analysis that Mem2Mem's compression mechanism behaves as an implicit sentence extractor that stores sentence representations of the salient content. The choice of sentence representations is only guided by the memory regularization and conditional language modeling loss of the decoder, thus avoiding exposure bias from maximizing the likelihood of sequential binary extraction labels. Finally, the encoded memory is transferred to the decoder memory, which is iteratively refined during the decoding process. To our knowledge, Mem2MeM is the first abstractive summarization model that uses memory compression for sentence extraction and that directly employs the memorized representations during summary generation. We empirically demonstrate the merits of this approach by setting a new state-of-the-art on long text abstractive summarization tasks on the Pubmed, arXiv and Newsroom datasets . Our contributions are three fold: [noitemsep]       % fig_architecture [t]          _{E}\). The encoder memory is transferred to the decoder memory \ and is read using a RAM like mechanism. The resulting memory readout vector is used to condition sentence and word-level attention. The decoder hidden states and the memory readout vector are then used to update the memory state \ via a gated write operation .}      This work proposes Mem2Mem, a novel MAED based mechanism for very long text abstractive summarization. Mem2Mem involves two memory types: A static encoder memory for compressing input texts and a dynamic decoder memory which refines the generation process. Memory transfer between them links two memories and maximizes the benefit of content extraction aimed for summarization. Different from existing hybrid extractive and abstractive approaches, Mem2Mem incorporates an extraction step without ground truth sentence labels and multi-step training. We demonstrate the effectiveness of Mem2Mem by showing promising results on the PubMed, arXiv, and Newsroom summarization datasets with an order of magnitude less parameters than competing transformer-based models. The Mem2Mem's memory compression can be generalized to other domains that require text generation guided by content selection. In future work, we will extend and validate the strength of our approach on a variety of language learning tasks.       
"," We introduce , a memory-to-memory mechanism for hierarchical recurrent neural network based encoder decoder architectures and we explore its use for abstractive document summarization. Mem2Mem transfers ``memories"" via readable/writable external memory modules that augment both the encoder and decoder. Our memory regularization compresses an encoded input article into a more compact set of sentence representations. Most importantly, the memory compression step performs implicit extraction without labels, sidestepping issues with suboptimal ground-truth data and exposure bias of hybrid extractive-abstractive summarization techniques. By allowing the decoder to read/write over the encoded input memory, the model learns to read salient information about the input article while keeping track of what has been generated.  Our Mem2Mem approach yields results that are competitive with state of the art transformer based summarization methods, but with . %On abstractive long text summarization, Mem2Mem surpasses, with full end-to-end training, the current state-of-the-art by 3.98 and 3.08 average ROUGE scores on the Pubmed and arXiv datasets while using $16$ times less parameters.  % Our code and trained models are available at \url{https://github.com/anonymously999/mem2mem}.",240
"  Attention-based encoder-decoder modeling is a natural and powerful paradigm for speech to text tasks, such as automatic speech recognition  and speech translation , and that has led to significant progress .  However, it relies on large amounts of supervised speech data, which is expensive to transcribe and translate.  In addition, the amount of speech transcripts and speech translation labels is dwarfed by the amount of text data available for language model  and machine translation  training. For example, the number of text tokens used for LM modeling is two orders of magnitude larger than the number of tokens from the corresponding speech corpus in the Librispeech data corpus, as shown in Table.    Attention-based encoder-decoder models are not designed to incorporate heterogeneous inputs and cannot benefit from large amounts of low cost text data directly in speech applications. As expected, performance gaps can still be observed between attention based encoder-decoder systems and conventional systems with multiple components.  %Short description about previous work In order to alleviate the data scarcity issue, different approaches have been studied, including acoustic and linguistic aspects.  %In this study, we focus on leveraging text data to improve linguistic modeling ability in speech to text systems.  LM is the most commonly used method to integrate linguistic information into ASR.  Prior work focuses on building LM with monolingual text data, and then integrate LM or transfer knowledge from it into the decoder.    generate synthetic data from text to augment speech training corpus. Another direction is to leverage text data directly during training through multitask learning.  use a common representation space to learn correspondences between different modalities for spoken language understanding.  propose multi-modal data augmentation to jointly train text and speech for ASR.  %is reminiscent of work done on multimodal learning or spoken language understanding  that also uses a common representation space to learn correspondences between different modalities.  are focused on ST tasks and trained with an ASR system together, where ASR is used as an auxiliary task. Hence, those methods cannot be applied back to ASR systems.  %What is proposed, describe the main idea %We follow the second direction and propose using auxiliary text tasks to enhance speech to text tasks. In this study, we focus on leveraging text data to improve linguistic modeling ability in speech to text tasks. We propose a general framework to leverage text data for ASR and ST tasks.  %Two encoders take text and speech as input respectively,  while the decoder is shared between tasks. During inference, only the speech encoder and decoder are used. A denoising autoencoder task  is introduced to be jointly trained with the ASR task with monolingual data, while a machine translation task is co-trained with  ST task  with  parallel  data. Text  input  is  represented  as  spoken form using phoneme sequence and it effectively reduces the difference between speech input and text input. We also carefully study different design choices for the joint training system, including strategies to share the text and speech encoders and comparing the joint training system with models initialized from pre-trained components.   Our experiments show the proposed joint training systems can effectively reduce word error rate  for the ASR task by 10\% to 15\% and improve BLEU score by 3.6$   In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. The ASR task is co-trained with a denoising autoencoder task using monolingual text, while a MT task is jointly trained with the ST task with parallel data.  Text input is represented as phoneme sequences to reduce the difference between speech input and text input. We examined different factors that impact the performance of the jointly trained system. Our experimental results show substantial WER reduction is achieved on the  dataset and large BLEU score gain is obtained in the  datasets.  It proves the effectiveness of the proposed method.                           \vfill\pagebreak     References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   ------------------------------------------------------------------------- 
"," Attention-based sequence-to-sequence modeling provides a powerful and elegant solution for applications that need to map one sequence to a different sequence.  Its success heavily relies on the availability of large amounts of training data.  This presents a challenge for speech applications where labelled speech data is very expensive to obtain, such as automatic speech recognition  and speech translation .  In this study, we propose a general multi-task learning framework to leverage text data for ASR and ST tasks. Two auxiliary tasks, a denoising autoencoder task and machine translation task, are proposed to be co-trained with ASR and ST tasks respectively.  We demonstrate that representing text input as phoneme sequences can reduce the difference between speech and text inputs, and enhance the knowledge transfer from text corpora to the speech to text tasks.  Our experiments show that the proposed method achieves a relative 10$ task compared with our baseline, and improves the speech translation quality on the MuST-C tasks by 3.6$\sim$9.2 BLEU.",241
" Motivated by the process of human inquiry and learning, the field of question generation  requires a model to generate natural language questions in context. QG has wide applicability in automated dialog systems, language assessment, data augmentation, and the development of annotated data sets for question answering  research.    Most prior research on QG has focused on generating relatively simple { question answering and generation , where answering the questions requires reasoning over the content in multiple text documents .  Unlike standard QG, generating multi-hop questions requires the model to understand the relationship between disjoint pieces of information in multiple context documents.  Compared to standard QG, multi-hop questions tend to be substantially longer, contain a higher density of named entities, and---perhaps most importantly---high-quality multi-hop questions involve complex chains of predicates connecting the mentioned entities   To address these challenges, existing research on multi-hop QG primarily relies on graph-to-sequence  methods. These approaches extract graph inputs by augmenting the original text with structural information  and then apply graph neural networks  to learn graph embeddings that are then fed to a sequence-based decoder.  However, the necessity of these complex G2S approaches---which require designing hand-crafted graph extractors---is not entirely clear, especially when standard transformer-based sequence-to-sequence  models already induce a strong relational inductive bias. Since transformers have the inherent ability to reason about the relationships between the entities in the text, one might imagine that these models alone would suffice for the relational reasoning requirements of multi-hop QG.   \xhdr{Present work} In this work, we show that, in fact, a standard transformer architecture is sufficient to outperform the prior state-of-the-art on multi-hop QG.  We also propose and analyze a graph-augmented transformer ---which integrates explicit graph structure information into the transformer model. GATE sets a new state-of-the-art and outperforms the best previous method by 5 BLEU points on the HotpotQA dataset. However, we show that the gains induced by the graph augmentations are relatively small compared to other improvements in our vanilla transformer architecture, such as an auxiliary contrastive objective and a data filtering approach, which improve our model by 7.9 BLEU points in ablation studies.  Overall, our results suggest diminishing returns from incorporating hand-crafted graph structures for multi-hop reasoning and provides a foundation for stronger multi-hop reasoning systems based on transformer architectures.     \usepackage[hyperref]{emnlp2020}   \usepackage{times}     \usepackage{latexsym} \renewcommand{\UrlFont}sachande@mila.quebec, wuli@us.ibm.com    % use 8-bit T1 fonts \usepackage{url}            % simple URL typesetting \usepackage{booktabs}       % professional-quality tables \usepackage{amsfonts}       % blackboard math symbols \usepackage{amsmath} \usepackage{nicefrac}       % compact symbols for 1/2, etc. \usepackage{graphicx} \usepackage{microtype}      % microtypography \usepackage{tabularx} \usepackage{xcolor} \usepackage{bbm} \usepackage{array} \usepackage{arydshln} \usepackage{amsfonts} \usepackage{amsmath} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \usepackage{bbm} \usepackage{boldline} \usepackage{bigstrut} \usepackage{blindtext} \usepackage{booktabs, siunitx} \usepackage[labelfont=bf, format=plain, justification=justified, singlelinecheck=false]{caption} \usepackage{color} \usepackage{cprotect} \usepackage{ctable} \usepackage{dirtytalk} \usepackage{enumitem} \usepackage[export]{adjustbox} \usepackage{float} \usepackage{graphicx} \usepackage{hhline} \usepackage{latexsym} \usepackage{mathrsfs} \usepackage{microtype} \usepackage{moresize} \usepackage{multicol} \usepackage{multirow} \usepackage{nccmath} \usepackage{nicefrac} \usepackage{pifont} \usepackage{placeins}      \usepackage{subcaption} \usepackage{times} \usepackage[utf8]{inputenc} \usepackage{url} \usepackage{verbatim} \usepackage{wrapfig, lipsum} \usepackage{textcomp} \usepackage{enumitem}  %  {HTML}{A6CEE3} \definecolor{lgreen}{HTML}{B2DF8A} \definecolor{lred}{HTML}{FB9A99} \definecolor{lorange}{HTML}{FDBF6F} \definecolor{mblue}{HTML}{80B1D3} \definecolor{mgreen}{HTML}{B3DE69} \definecolor{mred}{HTML}{FB8072} \definecolor{morange}{HTML}{FDB462} \definecolor{blue}{HTML}{1F78B4} \definecolor{green}{HTML}{33A02C} \definecolor{red}{HTML}{E31A1C} \definecolor{orange}{HTML}{FF7F00} \definecolor{dblue}{HTML}{0050EF} \definecolor{dgreen}{HTML}{006D2C} \definecolor{dorange}{HTML}{EC7014} } [1]{{ #1}} [1]{{ #1}} [1]{{ #1}} [1]{{ #1}} [1]{{ #1}} [1]{{ #1}}  [1]{^{4}^{1,2}^{1}^{2}^{3}^{4}$ETH Zurich\\ mrinmaya.sachan@inf.ethz.ch, wlh@cs.mcgill.ca\\ {\tt mrinmaya.sachan@inf.ethz.ch, wlh@cs.mcgill.ca} }   \date{}                        % !TeX root = main.tex     In this work, we propose a series of strong transformer models for multi-hop QG. To effectively encode the context documents and the answer, we introduce answer type embeddings and a new sublayer to incorporate the extracted entity-centric graph. We also propose an auxiliary contrastive objective to identify the supporting facts and a data filtering approach to balance the training-test distribution mismatch. Experiments on the HotpotQA dataset show that our models outperform the current best approaches by a substantial margin of 5 BLEU points. Our analysis further reveals that graph-based components may not be the most critical in improving the performance, but can render complementary strengths to the transformer.  !TeX root = main.tex  
"," Prior work on automated question generation has almost exclusively focused on generating simple questions whose answers can be extracted from a single document. However, there is an increasing interest in developing systems that are capable of more complex multi-hop question generation, where answering the questions requires reasoning over multiple documents. In this work, we introduce a series of strong transformer models for multi-hop question generation, including a graph-augmented transformer that leverages relations between entities in the text.  While prior work has emphasized the importance of graph-based models, we show that we can substantially outperform the state-of-the-art by {5 BLEU points}  using a standard transformer architecture. We further demonstrate that graph-based augmentations can provide complimentary improvements on top of this foundation. Interestingly, we find that several important factors---such as the inclusion of an auxiliary contrastive objective and data filtering could have larger impacts on performance.  We hope that our stronger baselines and analysis provide a constructive foundation for future work in this area.",242
"  Variational Autoencoders   allow to design complex generative models of data.  % since the inference process of VAE-based approaches has the advantage of being independent from the model architecture providing high flexibility in designing new neural components. In the wake of the renewed interest for VAEs, traditional probabilistic topic models  have been revised giving rise to several Neural Topic Model  variants, such as NVDM ,  ProdLDA , NTM-R , etc. % GSM , W-LDA  However, existing topic models when applied to user reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries of movies and books . Although these approaches have achieved significant results via the neural inference process, surprisingly very little work has been done on how to disentangle the inferred topic representations.   % Despite the lack of general consensus about a formal definition of disentangled representations ,   Disentangled representations can be defined as representations where individual latent units are sensitive to variations of a single generative factor, while being relatively invariant to changes of other factors . Inducing such representations has been shown to be significantly beneficial for their generalization and interpretability .  For example, an image can be viewed as the results of several generative factors mutually interacting, as one or many sources of light, the material and reflective properties of various surfaces or the shape of the objects depicted . %  In the context of topic modeling, documents result from a generative process over mixtures of latent topics, and therefore, we propose to consider these latent topics as generative factors to be disentangled to improve their interpretability and discriminative power. Disentangled topics are topics invariant to the factors of variation of text, which for instance, in the context of book and movie reviews could be the author's opinion , the salient parts of a plot or other auxiliary information reported. An illustration of this is shown in Fig. in which opinion topics are separated from plot topics.  % where this leads to separating topics based on the ``factor of variation"" they are revealing. % For example, in generating a book review, the factors of variation involved could depend on the author's expertise in identifying the salient features of the book, %his knowledge of the book's genre, or  % his ability to summarize the plot and the feelings evoked by the book.  % % [Let's break in/the atom] % % Figure  reports a examples of polarity-disentangled topics generated from the IMDB movie reviews of ""The Hobbit"". The topics on the left and right summarize some of the positive and negative aspects described by users, while neutral topics in the middle report the main elements of the movie's plot.  % An effective approach for disentangling features in the latent space of VAEs is to adopt adversarial training . However, despite its successful applications in computer vision , the applications to text analysis has been rather limited so far , narrowed by the lack of proper tasks to evaluate the generated disentangled representations and the limited availability of suitable datasets.  % For example, in book or movie reviews, we want to disentangle topics which are related to opinions expressed in text and topics relating to book/movie plots. An illustration of this is shown in Figure in which opinion topics are separated from plot topics.   However, models relying solely on sentiment information are easily misled and not suitable to disentangle opinion from plots, since even plot descriptions frequently make large use of sentiment expressions . Consider for example the following sentence: ``The ring holds a dark power, and it soon begins to exert its evil influence on Bilbo"", an excerpt from a strong positive Amazon's review.  % This overcomes the difficulty of separating opinions from plot and auxiliary information yet containing polarised descriptions that easily mislead models merely relying on sentiment lexicon; analogously to the issue of mixed topics generated when traditional topic models are applied to review documents, as pointed out in .  % Despite its successful employment in computer vision , the adversarial approach has had a rather limited application in text analysis so far , narrowed by the lack of proper tasks to evaluate the generated disentangled representations and the limited availability of suitable datasets.  Therefore, we propose to distinguish opinion-bearing topics from plot/neutral ones combining a neural topic model architecture with an adversarial training. In this study, we present the DIsentangled Adversarial TOpic Model \footnote{Source code and dataset omitted for the anonymous submission.}, aiming at disentangling information related to the target labels , from other distinct aspects yet possibly still polarised . We also introduce a new dataset, namely the MOBO dataset\footnotemark[\value{footnote}], made up of movie and book reviews, paired with their related plots. The reviews come from different publicly available datasets: IMDB , GoodReads  and Amazon reviews , %,  and encompass a wide spectrum of domains and styles. We conduct an extensive experimental assessment of our model. First, we assess the topic quality in terms of topic coherence and diversity and compare DIATOM with other supervised topic models on the sentiment classification task; then, we analyse the disentangling rate of topics to quantitatively assess the degree of separation between actual opinion and plot/neutral topics.    Our contributions are summarized below:   dataset, a new collection of movie and book reviews paired with their plots.    The rest of the paper is organized as follows. We review the related literature on sentiment-topic models, neural topic models and the studies on disentangled representations . Then, we present the details of our proposed DIATOM model , followed by the experimental setup  and results . Finally, we conclude with a summary of the results and suggestions for future works .     %%%%%%%%%%%%%%%%%%%%%%%%%%%    We have described DIATOM, a new neural topic model to generate disentangled topics through the combination of VAE and adversarial learning.   We reported the results of our experimental study based on the novel  dataset highlighting the benefit of such an approach leading to topics with higher interpretability in terms of both topic coherence and topic uniqueness and more discriminative power reflected in better sentiment classification results compared to other supervised topic models.   We further discussed the model capability to consistently disentangle opinion-bearing topics from plot/neutral ones measuring the introduced disentangling rate.  Finally, we identified current limitations and viable solutions to be explored in the future.      
"," The flexibility of the inference process in Variational Autoencoders  has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models . Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. %Since in the topic modeling framework documents result from a generative process over mixtures of latent topics, we propose to interpret these latent topics as generative factors to be disentangled to improve their interpretability and discriminative power. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models.",243
"   Mutual understanding in interactive situations, either when several people are engaged in a dialogue or when they are interacting with a modern computer system in natural language, may not be achieved without considering both the semantic information in the speakers utterances and the pragmatic interaction level, especially relative to dialogue acts. Dialogue Acts  represent the meaning of an utterance  in the context of a dialogue, or, in other words, the function of an utterance in the dialogue. For example, the function of a~question is to request some information, while an answer shall provide this information. Dialogue acts are thus commonly represented as phrase-level labels such as statements, yes-no questions, open questions, acknowledgements, and so on.  Automatic recognition of dialogue acts is a fundamental component of many human-machine interacting systems that support natural language inputs. For instance, dialogue acts are typically used as an input to the dialogue manager to help deciding on the next action of the system: giving information when the user is asking a question, but eventually keeping quiet when the user is just acknowledging, giving a comment, or even asking for delaying the interaction. In the latter case, a system reaction may be perceived as intrusive. Beyond human-machine interaction, this task is also important for applications that rely on the analysis of human-human interactions, either oral, e.g., in recordings of meetings, or % lada - added reference according to rev 1 written, e.g., through the reply and mention-at structures in Twitter conversations. It is also essential for a large range of other applications, for example talking head animation, machine translation, automatic speech recognition or topic tracking. The knowledge of the user dialogue act is useful to render facial expressions of an avatar that are relevant to the current state of the discourse. In the machine translation domain, recognizing dialogue acts may bring relevant cues to choose between alternative translations, as the adequate syntactic structure may depend on the user intention. Automatic recognition of dialogue acts may also be used to improve the word recognition accuracy of automatic speech recognition systems, where a different language model is applied during recognition depending on the dialogue act. %lada - added reference according to rev 1,   To conclude, dialogue act recognition is an important building block of many understanding and interacting systems. %pav --I've commented the rest of the sentence, because it was not clear for 2 reviewers ) -- and typically completes semantic role labelling and dialogue management.   Researches on dialogue act recognition have been carried out for a long time, as detailed in Section. The majority of these works exploit supervised learning with lexical, syntactic, prosodic and/or dialogue history features. However, few approaches consider semantic features, while they may bring additional information and prove useful to improve the accuracy of the dialogue act recognition system. For instance,  a~frequent cause of recognition errors are ``unknown'' words in the testing corpus that never occur in the training sentences. Replacing specific named entities in the text  by their category has been proposed in the literature as a remedy to this issue. We investigate a more general solution that exploits lexical similarity between word vectors. These word vectors may be computed in various ways, but they typically include mostly lexical semantic information about the word itself as well as some syntactic information, e.g., related to the relative position or degree of proximity of pairs of words within a sentence. This additional information may be used to improve dialogue act recognition, in particular when the training and test conditions differ, or when the size of the training corpus is relatively small.  %goal In this work, we propose a new Deep Neural Network  based on Long Short-Term Memory  for the task of dialogue act recognition, and we compare its performance to a standard Maximum Entropy model. Our first objective is to leverage the modelling capacity of such a DNN in order to achieve dialogue act recognition with only the raw observed word forms, i.e., without any additional expert-designed feature. This model is described in Section. The second objective is to further validate this model both on a standard English DA corpus, as well as on two other languages, without changing anything in the model, in order to assess the genericity and robustness of the approach. These experiments are summarized in Section. Finally, our third objective is to study the impact of word embeddings, which have been shown to provide extremely valuable information in numerous Natural Language Processing  tasks, but which have never been used so far~\footnote{To the best of our knowledge at the time of submission} for dialogue act recognition. This study is summarized in Section. %The following Section presents a review of related works of the domain.     We propose in this work an LSTM-based deep neural network for dialogue act recognition. We show that this model performs as good as the state-of-the-art, even though it only uses the raw word forms as inputs, without any additional information, in particular neither part-of-speech tags nor information about the speaker. We have further applied exactly the same model with the same hyper-parameters on three different languages: English, French and Czech. The proposed model performs well on all three languages, suggesting that its performance generalizes nicely to various types of corpora and is not dependent on a specific tuning of the hyper-parameters to experimental conditions. This confirms the interesting modelling potential of deep recurrent networks for NLP in general, and supports the conclusions of recent works in the domain, which demonstrate the good performance of end-to-end training of deep neural networks for dialogue act recognition.  A more surprising conclusion of our work concerns the actual impact of pretrained word embeddings, which have been shown to be of great importance in several NLP tasks in the literature. We show in this work that standard pretrained embeddings do not help for the dialogue act recognition task in any of the three tested languages. We thus further study the embeddings that result from training the proposed model in an end-to-end manner, and show that they seem to differ from vanilla word2vec embeddings, which may explain why they do not perform as well as in other tasks. Of course, a single type of word embeddings has been tested in this work, word2vec, but some additional preliminary experiments suggest that LDA and COALS-based embeddings do not help either. More experiments with various embeddings should be made to confirm or infirm this conclusion, but it would be more convincing if they were realized with another deep network implementation and in more variable experimental conditions. To the best of our knowledge, this is the first work that exploits pretrained word embeddings for dialogue act recognition, and one of the rare published work that shows and analyzes some weakness of word2vec embeddings.  We further compare the proposed deep neural network with a standard Maximum Entropy classifier, and show that the DNN consistenly outperforms the Maximum Entropy classifier both in French and English. This is not the case in Czech, but it is likely due to the already high level of accuracy reached on this corpus, which leaves little to be gained by improving the model. A more interesting conclusion about this comparison between DNN and Maximum Entropy is that  pretrained word embeddings improve the Maximum Entropy model but not the DNN. This likely results from the limited modelling capacity of the Maximum Entropy model, which still benefits from the information brought by pretrained embeddings. But this information is not precise enough for the DNN, as shown in our qualitative analysis of word2vec.  
"," Dialogue act recognition is an important component of a large number of natural language processing pipelines. Many research works have been carried out in this area, but relatively few investigate deep neural networks and word embeddings. This is surprising, given that both of these techniques have proven exceptionally good in most other language-related domains. We propose in this work a new deep neural network that explores recurrent models to capture word sequences within sentences, and further study the impact of pretrained word embeddings. We validate this model on three languages: English, French and Czech. The performance of the proposed approach is consistent across these languages and it is comparable to the state-of-the-art results in English. More importantly, we confirm that deep neural networks indeed outperform a Maximum Entropy classifier, which was expected. However, and this is more surprising, we also found that standard word2vec embeddings do not seem to bring valuable information for this task and the proposed model, whatever the size of the training corpus is. We thus further analyse the resulting embeddings and conclude that a possible explanation may be related to the mismatch between the type of lexical-semantic information captured by the word2vec embeddings, and the kind of relations between words that is the most useful for the dialogue act recognition task.",244
"   In recent years, there has been an increased focus on the use of unannotated texts for modeling human language and on transfer learning in natural language processing . A wide variety of models have been proposed, ranging from context-independent word embeddings , to the more recent contextual representations . In particular, the Transformer-based  BERT  model  has generated considerable interest in the NLP community since its release. BERT outperformed the then state-of-the-art systems on a wide range of benchmark datasets when published, and has served as the basis of many studies since. These efforts include work that proposes improvements and/or modifications to the training objectives , knowledge distillation , multilinguality , and interpretation , to name a few. As a mark of its popularity, the term BERTology was coined to refer to the field of research relating to BERT .  % Multilinguality, zero-shot transfer, monolingual BERTs A thriving branch of BERTology involves BERT models for languages other than English.  released multilingual BERT  models trained on over a hundred languages. % what is good about multilingual model  analyze the representations produced by mulitlingual BERTs and find evidence that these representations generalize across languages for various downstream tasks, though language-specific information is retained. This language-agnostic subspace of multingual BERTs has also been observed in other studies, and is deemed to be the factor that allows for zero-shot transfer . % also find evidence that multilingual BERTs learn a language-agnostic subspace for linguistic information.  also show that these multilingual models have their embeddings partially aligned, which allows for zero-shot transfer. Furthermore, the embeddings can be further aligned through a fine-tuning based alignment procedure, improving the performance of multilingual models . % the curse of multilinguality  % 'we scale the number of languages for a fixed model capacity: more lan-guages leads to better cross-lingual performanceon low-resource languages up until a point, afterwhich the overall performance on monolingual andcross-lingual benchmarks degrades' While multilingual training can benefit also monolingual performance, as the number of languages covered by a multilingual model increases, the fraction of the model capacity available for any single language decreases.  % The number of languages included in a multilingual model, however, affects its performance, and a phenomenon referred to as the curse of multilinguality has been observed.  term as the  the phenomenon where increasing the number of languages included in a model initially leads to better cross-lingual performance for low-resource languages, while eventually leading to overall degradation of both monolingual and cross-lingual performance. Work on language-specific BERT models has also shown that monolingual models tend to outperform multilingual models of the same size in monolingual settings . % To further benefit languages other than English, monolingual BERTs for various languages have been trained and released by the NLP community . %\todo{Is any transition needed here?} % SMP: added the following However, the question of whether it is possible to train multilingual models without loss of monolingual performance remains largely open. %To minimize the effect of the curse of multilinguality but still benefit from the language-agnostic subspace,  % Furthermore, the underlying reason why BERT works, the inner representation of BERT, a line of research has focused on the inner-workings of BERT [put this here or related work?] In this paper, we study whether it is feasible to pre-train a bilingual model for two remotely related languages without compromising performance at either language. Specifically, we train a Finnish-English bilingual BERT model  using a combination of the pre-training data of the original English BERT model and the Finnish BERT model introduced by , using an extended model vocabulary but otherwise fixing model capacity at BERT-Base size and retaining the number of pre-training steps. % carefully rephrase this! %While there has been reports of bilingual BERTs focusing on their cross-linguality, to the best of our knowledge, there has not been any work focusing on comparing the performance of bilingual models on natural language understanding  tasks with their monolingual counterparts. We evaluate the performance of the introduced bilingual model on a range of natural language understanding  tasks used to evaluate the monolingual models, which, to the best of our knowledge, has not been the focus of studies on bilingual BERT models. We find that  achieves comparable performance on the GLUE  benchmark  with the original English BERT, and nearly matches the performance of the Finnish BERT on Finnish NLP tasks. Our results indicate that an extension of the vocabulary size is sufficient to allow the creation of fully bilingual models that perform on par with their monolingual counterparts in both of their languages.     We have studied the feasibility of training a fully bilingual deep neural language model, i.e.\ a model that approaches or matches the performance of monolingual models at language-specific tasks. We trained a bilingual Finnish-English  model by expanding the vocabulary size to be the sum of the size of the two individual vocabularies, and compared the model performance to monolingual models. We found that, on a range of NLU tasks, the bilingual model performs comparably or nearly comparably with monolingual models. We conclude that, for the  architecture, it is possible to train a fully bilingual deep contextual model for two remotely related languages. We release the newly introduced  model and all tools introduced to create the model under open licenses at .  
"," Language models based on deep neural networks have facilitated great advances in natural language processing and understanding tasks in recent years. While models covering a large number of languages have been introduced, their multilinguality has come at a cost in terms of monolingual performance, and the best-performing models at most tasks not involving cross-lingual transfer remain monolingual. In this paper, we consider the question of whether it is possible to pre-train a bilingual model for two remotely related languages without compromising performance at either language. We collect pre-training data, create a Finnish-English bilingual BERT model and evaluate its performance on datasets used to evaluate the corresponding monolingual models. Our bilingual model performs on par with Google's original English BERT on GLUE and nearly matches the performance of monolingual Finnish BERT on a range of Finnish NLP tasks, clearly outperforming multilingual BERT. % performance on Finnish datasets is not as good as FinBERT, how to put that into words %We find that the bilingual model achieves comparable performance as the English BERT and nearly matches the performance of FinBERT. We find that when the model vocabulary size is increased, the  architecture has sufficient capacity to learn two remotely related languages to a level where it achieves comparable performance with monolingual models, demonstrating the feasibility of training fully bilingual deep language models. % We describe the procedure taken to train this bilingual BERT. The model and all tools involved in its creation are freely available at \url{https://github.com/TurkuNLP/biBERT}",245
"  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  A long-standing challenge in computer science is to develop algorithms that can interact with human users via dialog in natural language~.  Of particular interest is task-oriented dialog, wherein a user interacts with a system to achieve some goal .  The system should understand the user's requests and assist them by taking the appropriate actions . In recent years, supervised learning approaches to this problem have become particularly popular, because they can potentially learn complex patterns without relying on hand-crafted rules. While such data-driven methods already demonstrate impressive performance in open-domain dialog , task-oriented dialog models face the additional difficulty of transferring skills to tasks and domains that were not present in the training data.  To address this issue, we present the Schema-guided Dialog Dataset for Transfer Learning  dataset, a collection of realistic, task-oriented dialogs, that is especially designed to test and facilitate the transfer of learned patterns between tasks.  Unlike open-domain dialogs, task-oriented dialogs are accompanied by a set of steps that are necessary to complete the task.  These steps are typically known  and thus do not have to be learned from the data. In fact, for practical applications it is desirable that we could make modifications to this logic without having to discard large parts of the dataset. The ideal sequences of steps that a dialog would follow to complete the task can be arranged in a graph . Together with the utterances or actions that are associated with the nodes of this graph, we hence call this a task schema, or simply schema. Note, that what we call `schema' is similar to the `task specification' of , but distinct from the `schemas' that only define slots and intents of a task as used by . %or .     In a typical supervised model that is trained to, say, predict the next system action for a task-oriented dialog, the schema of the training tasks is implicitly captured by the learned model parameters. This makes generalizing to a new task difficult, as the implicitly memorized schema will no longer be appropriate .  With \DATASETNAME\ we provide explicit schema representations for each task and thereby enable models to condition on the schema .  To collect \DATASETNAME\ we use a Wizard of Oz setup , where the system's role is played by a human `wizard'. Based on our pilot studies, we found that the quality of crowd-sourced dialogs depends strongly on  []        We refined our approach through extensive internal testing and four rounds of pilot studies.  % All code and instructions are available as open source at \anonymous{\DATASETURL}.   Our aim is to create an ecologically valid dataset  with the following four attributes, which we believe are crucial for a dataset to be of high quality:      . %     Realistic dialog rarely strictly follows the ideal path, but is interrupted by small talk, changes of the user's mind, and references to events that happen in the user's environment. %     In \DATASETNAME\ we capture these behaviors.      %     We collect three kinds of dialogs: %     %          where the dialog progresses along one of the paths in the schema, %           where the user adds complexity to the task , and %          where the user engages in a complex dialog spanning multiple domains and tasks . %     %     The progression of difficulty allows better assessment of dialog models and potential for transfer learning across levels of difficulty.     . % The behavior of a task-oriented dialog system should be largely deterministic and not subject to the whims or personality of the wizard. %     In particular, we encourage wizards to follow the given task schema as closely as possible.     . %     A large part of developing a dialog system is the implementation of application programming interface  calls, such as knowledge base queries. %     In \DATASETNAME\ we represent our dialogs as a three-party interaction wherein the system acts as the intermediary between a user and a knowledge base . %     Thus, models have to learn when to query the knowledge base, what the query should be, and how to explain the returned knowledge base item to the user.   % With these properties, we create a is ecologically valid, as described by.  With this paper, we contribute  []      The code for the latter setup, all collected  data, and all modeling code is freely available under \anonymous{\DATASETURL}.                                                                                                               With this work, we make multiple contributions to the field of task-oriented dialog research. First, we presented \DATASETNAME, a novel dialog dataset that we specifically designed to facilitate transfer learning experiments. Second, we introduced a new, scalable crowd sourcing paradigm to collect data of similar quality as \DATASETNAME. In future work, this setup could be used to expand on \DATASETNAME\ by collecting data for additional tasks, domains, or in languages other than English. Finally, we established baseline scores for next action prediction, response generation, and zero-shot transfer learning for the former two tasks. With this we demonstrated how task schemas can be used to improve transfer learning capabilities. We also outlined a variety of other experiments that \DATASETNAME\ would be suitable for, and we look forward to seeing these experiments, as well as improvements upon our baseline scores, implemented in future publications.  \clearpage    
"," We present \DATASETNAME, a schema-guided task-oriented dialog dataset consisting of 127,833 utterances and knowledge base queries across 5,820 task-oriented dialogs in 13 domains that is especially designed to facilitate task and domain transfer learning in task-oriented dialog. Furthermore, we propose a scalable crowd-sourcing paradigm to collect arbitrarily large datasets of the same quality as \DATASETNAME. Moreover, we introduce novel schema-guided dialog models that use an explicit description of the task to generalize from known to unknown tasks.  We demonstrate the effectiveness of these models, particularly for zero-shot generalization across tasks and domains.",246
" % -------------------------------------------------------------- % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % final paper: en-us version          % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}.       } The relationship between a group of human languages can be characterized across several dimensions of variation , including  the temporal dimension, wherein languages have diverged from a common historical ancestor as in the case of Romance languages;  the spatial dimension, wherein the speaker communities are geographically adjacent as in the case of the Indo-Aryan and Dravidian languages of India; and  the socio-political dimension, wherein languages have evolved under shared political and/or religious forces as in the case of Arabic and Swahili. Languages, or language varieties, can be related across all these dimensions, which often results in a dialect continuum. Speakers of languages that constitute a dialect continuum can usually communicate with each other efficiently using their own mother tongue. The degree of intercomprehensibility between speakers of different language varieties within a continuum is mainly determined by linguistic similarities. A notable case of this phenomenon is the mutual intelligibility among the Slavic languages, which we study in this paper.   One of the goals of linguistics is to study and categorize languages based on objective measures of linguistic distance. The degrees of similarity at different levels of the linguistic structural organization can be seen as preconditions for, as well as predictors of, successful oral intercomprehension.  For closely-related languages, similarities at the pre-lexical, that is the acoustic-phonetic and phonological, level have been found to be better predictors of cross-lingual speech intelligibility than lexical similarities . In a different, yet relevant research direction,  have investigated non-linguists' perception of language variation using data from the popular spoken language guessing game, the Great Language Game . By analyzing the confusion patterns of the GLG's human participants, the authors have shown that factors predicting players' confusion in the game correspond to objective measures of similarity established by linguists. For example, both phylogenetic relatedness and overlap in phoneme inventories have been identified as factors of perceptual confusability  of languages in GLG.   The development of automatic systems that determine the identity of the language in a speech segment has received attention in the speech recognition community . State-of-the-art approaches for automatic spoken language identification, henceforth LID, are based on multilayer deep neural networks . DNN-based LID systems are parametric models that learn a mapping from spectral acoustic features of  speech to high-level feature representations in geometric space where languages are linearly separable. These models have shown tremendous success not only in discriminating between distant languages but also closely-related language varieties . Nevertheless, none of the previous works in spoken language recognition has analyzed the emerging representations from neural LID models for related languages. Thus, it is still unknown whether the distances in these representation spaces correspond to objective measurements of linguistic similarity and/or to non-linguists' perception of language variation. In this paper, we aim to fill this gap and consider the family of Slavic languages as a case study. Our key contribution is two-fold:  [label={}, noitemsep]  In this paper, we attempt to bridge different lines of research that have so far remained unconnected. On the one hand, we employ neural architectures from the field of spoken language recognition and build a robust model to identify languages in contemporary acoustic realizations of Slavic speech. On the other hand, we analyze the emerging language representations using techniques established by previous research in multilingual natural language processing . We consequently shed light on the speech modality and show how  speech signals can complement research done in computational studies of linguistic typology and language variation.   %  to the best of our knowledge  % The recognition of spoken language   % LID in speech technology  % untranscribed speech   % NN has made possible for end-to-end systems to be developed, while traditional approaches feature many components   % closely-related languages have similar phonotactics, but differ in acoustic realizations of segments and suprasegmental features   % language identity and objective linguistic measures of similarity   % The GLG   % similarity of representation in deep neural networks    % --------------------------------------------------------------     -------------------------------------------------------------- We have presented a convolutional neural model for Slavic language identification in speech signals and analyzed the extent to which its emerging representations reflect language similarity. Our analysis has shown that the distances in the emergent language representations reflect language variation across the temporal  and spatial  dimensions. Moreover, by comparing our clustering analysis to the confusion patterns of the Great Language Game participants, we have shown that perceptual confusability is a better predictor of language representation similarities than phylogenetic and geographic distances.        
"," Deep neural networks have been employed for various spoken language recognition tasks, including tasks that are multilingual by definition such as spoken language identification. In this paper, we present a neural model for Slavic language identification in speech signals and analyze its emergent representations to investigate whether they reflect objective measures of language relatedness and/or non-linguists' perception of language similarity. While our analysis shows that the language representation space indeed captures language relatedness to a great extent, we find perceptual confusability between languages in our study to be the best predictor of the language representation similarity.",247
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  $ Corresponding author.\\     This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: \url{http://creativecommons.org/licenses/by/4.0/}.     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Aspect-level sentiment classification  is a fundamental task in sentiment analysis , which aims to infer the sentiment polarity  of a given opinion target in a review sentence. An opinion target, also known as aspect term, refers to a word or a phrase in review describing an aspect of an entity. For example, the sentence `` are great, but the service is dreadful}'' consists of two opinion targets, namely ``'' and ``''. User's sentiment towards the opinion target ``'' is positive while negative in terms of target ``''. Traditional methods usually focus on designing a set of features such as bag-of-words or sentiment lexicon to train a classifier  for ASC. Motivated by the great success of deep learning in computer vision, speech recognition and natural language processing, recent works use neural networks to  learn low-dimensional and continuous text representations without any feature engineering, and achieve competitive results on the ASC task.  From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion contexts for different targets. To this end, abundant state-of-the-art works employ attention mechanism to capture sentiment words related to the given target, and then aggregate them to make sentiment prediction. Despite the effectiveness of attention mechanism, we argue that it fails to reach the full potential due to the limited ASC labeled data. It is well-known that the promising results of deep learning heavily rely on sufficient training data. However, the annotation of ASC data is very labour-intensive and expensive in real-world scenarios, because annotators need to not only identify all opinion targets in a sentence but also determine their corresponding sentiment polarity. The difficulty of annotation leads to that existing public aspect-level datasets are all relatively small-scale, which finally limits the potential of attention mechanism.  Despite the lack of ASC data, enormous labeled data of document-level sentiment classification  are available at online review sites such as Amazon and Yelp. These reviews contain substantial sentiment knowledge and semantic patterns. Therefore, one meaningful but challenging research question is how to leverage resource-rich DSC data to improve the low-resource task ASC. For this purpose,~ design the PRET+MULT framework to transfer sentiment knowledge from DSC data to ASC task through sharing shallow embedding and LSTM layer. Inspired by the capsule network,~ propose TransCap to share bottom three capsule layers, then separate two tasks only in the last ClassCap layer. Fundamentally, PRET+MULT and Transcap improve ASC by sharing parameters and multi-task learning, but they cannot accurately control and interpret what knowledge to be transferred. In this work, we directly focus on the aforementioned attention issue in the ASC task and propose a novel framework, Attention Transfer Network , to explicitly transfer attention knowledge from the DSC task for improving the attention capability of the ASC task. Compared with PRET+MULT and Transcap, our model achieves better results and retains good interpretability.  In the ATN framework, we adopt two attention-based BiLSTM networks, respectively, as the DSC module and base ASC module, and propose two different methods to transfer attention from DSC to ASC. The first transfer approach is called . Specifically, we first pre-train an attention-based BiLSTM on large-scale DSC data, then exploit the attention weights from the DSC module as a learning signal to guide the ASC module to capture sentiment clues more accurately, thereby acheiving improvements. The second approach adopts the way of , and directly incorporates the attention weights of the DSC module into the ASC module. The two approaches work in different ways and have their different advantages.  aims to learn the attention ability of the DSC module and has faster inference speed, since it does not use external attention from DSC during the testing stage. In contrast,  can leverage the attention knowledge of the DSC module during the testing stage and make more comprehensive predictions.  We conduct experiments on two benchmark datasets to evaluate different methods. The results indicate that the ATN model can be substantially improved by incorporating the two attention transfer approaches, and outperforms all compared methods on the ASC task.  %We conducted experiments on attention-based LSTM models using the SemEval 2014 dataset. The results show that attention-based LSTM can be substantially improved by incorporating our two proposed methods, and that the resulting model outperforms all baseline methods on aspect-level sentiment classification. Further analysis also shows the good interpretability of our approaches.    Insufficient labeled data limits the effectiveness of attention-based models for the ASC task. In this paper, we propose a novel attention transfer framework, in which two different attention transfer methods are designed to exploit attention knowledge from resource-rich document-level sentiment classification corpus to enhance the attention process of resource-poor aspect-level sentiment classification, finally achieving the goal of improving the performance of ASC. Experimental results indicate that our approaches outperform the state-of-the-art works. Further analysis validates the effectiveness and benefits of transferring the attention knowledge from DSC data for the ASC task.  
","   Aspect-level sentiment classification  aims to detect the sentiment polarity of a given opinion target in a sentence. In neural network-based methods for ASC, most works employ the attention mechanism to capture the corresponding sentiment words of the opinion target, then aggregate them as evidence to infer the sentiment of the target. However, aspect-level datasets are all relatively small-scale due to the complexity of annotation. Data scarcity causes the attention mechanism sometimes to fail to focus on the corresponding sentiment words of the target, which finally weakens the performance of neural models. To address the issue, we propose a novel Attention Transfer Network  in this paper, which can successfully exploit attention knowledge from resource-rich document-level sentiment classification datasets to improve the attention capability of the aspect-level sentiment classification task. In the ATN model, we design two different methods to transfer attention knowledge and conduct experiments on two ASC benchmark datasets. Extensive experimental results show that our methods consistently outperform state-of-the-art works. Further analysis also validates the effectiveness of ATN. Our code and dataset are available at \url{https://github.com/1429904852/ATN}.",248
" For a conversational AI or digital assistant system , Natural Language Understanding  is an established component that produces semantic interpretations of a user request, which typically involves analysis in terms of domain, intent, and slot . For instance, the request  can be interpreted as falling within the scope of  domain with  intent and  identified for  slot.  Improving the accuracy of the NLU component is important for satisfactory end-to-end user experience. Without an accurate semantic understanding of the user request, a conversational AI system cannot fulfill the request with a satisfactory response or action. As one of the most upstream components in the runtime workflow , NLU's errors also have a wider blast radius that propagates to all subsequent downstream components, such as dialog management, routing logic to back-end applications, and language generation.  A straight-forward way to improve NLU is through human annotations. For example, we can mine the user requests that resulted in unsatisfactory user experience and make ground-truth annotations on those requests that produced incorrect NLU outputs, which can be used as additional supervision data for improving the models or rule engines within NLU. However, this approach is labor-intensive and expensive. It requires at least multiple tiers of annotations , and it is hard to consider all underlying contextual conditions. It is also limited by the existing annotation guidelines that may not accurately reflect user expectations. Due to these limitations, leveraging user feedback, both implicit and explicit, from real production systems is emerging as a new area of research.     In this paper, we propose a scalable and automatic approach for improving NLU by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. For instance, while interacting with a conversational AI system, dissatisfied users might often choose to intervene by stopping the system response in the middle and rephrasing the previous request to make it clearer with less room for ambiguous interpretation .  Our work makes three main contributions. First, to our knowledge, this work is the first in the literature to introduce a scalable and automatic approach of leveraging domain-agnostic implicit user feedback that can continuously improve the NLU component of a large-scale conversational AI system in production. Second, we propose a general framework for curating supervision data for improving NLU from live traffic that can be leveraged for various subtasks within NLU - e.g., the supervision data can be applied to improve individual semantic interpretation models  or a ranking/classification model across all interpretations . Last, we show with an extensive set of experiments on live traffic the performance of the proposed framework and its impact on improving NLU in the production system across 10 widely used domains. \def\year{2021}\relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21}  % DO NOT CHANGE THIS \usepackage{times}  % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier}  % DO NOT CHANGE THIS \usepackage[hyphens]{url}  % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm}  % DO NOT CHANGE THIS \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing  % DO NOT CHANGE THIS {8.5in}  % DO NOT CHANGE THIS {11in}  % DO NOT CHANGE THIS  \usepackage{amsfonts} \usepackage{amsmath} \usepackage{algorithm} \usepackage{xcolor} \usepackage[noend]{algpseudocode}  %  %Leave this % /Title  % Put your actual complete title  within the parentheses in mixed case % Leave the space between \Title and the beginning parenthesis alone % /Author  % Put your actual complete list of authors  within the parentheses in mixed case. % Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, % remove them.  % DISALLOWED PACKAGES % \usepackage{authblk} -- This package is specifically forbidden % \usepackage{balance} -- This package is specifically forbidden % \usepackage{color  % \usepackage{CJK} -- This package is specifically forbidden % \usepackage{float} -- This package is specifically forbidden % \usepackage{flushend} -- This package is specifically forbidden % \usepackage{fontenc} -- This package is specifically forbidden % \usepackage{fullpage} -- This package is specifically forbidden % \usepackage{geometry} -- This package is specifically forbidden % \usepackage{grffile} -- This package is specifically forbidden % \usepackage{hyperref} -- This package is specifically forbidden % \usepackage{navigator} -- This package is specifically forbidden %  %  -- This package is specifically forbidden % \usepackage{setspace} -- This package is specifically forbidden % \usepackage{stfloats} -- This package is specifically forbidden % \usepackage{tabu} -- This package is specifically forbidden % \usepackage{titlesec} -- This package is specifically forbidden % \usepackage{tocbibind} -- This package is specifically forbidden % \usepackage{ulem} -- This package is specifically forbidden % \usepackage{wrapfig} -- This package is specifically forbidden % DISALLOWED COMMANDS %  --- A Guide } % \author{  %     %Authors %     % All authors must be in the same font size and format. %     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\ %     AAAI Style Contributions by Pater Patel Schneider, %     Sunil Issar,  \\ %     J. Scott Penberthy, %     George Ferguson, %     Hans Guesgen, %     Francisco Cruz, %     Marc Pujol-Gonzalez %     \\ % } % \affiliations{ %     %Afiliations  %     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\ %     %If you have multiple authors and multiple affiliations %     % use superscripts in text and roman font to identify them. %     %For example,  %     % Sunil Issar, \textsuperscript{\rm 2} %     % J. Scott Penberthy, \textsuperscript{\rm 3} %     % George Ferguson,\textsuperscript{\rm 4} %     % Hans Guesgen, \textsuperscript{\rm 5}. %     % Note that the comma should be placed BEFORE the superscript for optimum readability  %     2275 East Bayshore Road, Suite 160\\ %     Palo Alto, California 94303\\ %     % email address must be in roman text type, not monospace or sans serif %     publications21@aaai.org  %     % See more examples next % } % \author {     Sunghyun Park\thanks{Equal contribution.}, Han Li\textsuperscript{\rm *}, Ameen Patel, Sidharth Mudgal, Sungjin Lee, Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya \\ } \affiliations{     % Affiliations     Amazon Alexa AI \\     \{sunghyu, lahl, paameen, sidmsk, sungjinl, youngbum, matsouka, rsarikay\}@amazon.com } %\fi   \author {     Authors         First Author Name,\textsuperscript{\rm 1}         Second Author Name, \textsuperscript{\rm 2}         Third Author Name \textsuperscript{\rm 1} \\ } \affiliations {     % Affiliations     \textsuperscript{\rm 1} Affiliation 1 \\     \textsuperscript{\rm 2} Affiliation 2 \\     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com } \fi   [1]{{ #1}} [1]{\mathbf{#1}}  {Definition}        Natural Language Understanding  is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a general domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system and show its impact across 10 domains.            \newpage  %      In this paper, we proposed a domain-agnostic and scalable framework for leveraging implicit user feedback, particularly user dissatisfaction and rephrase behavior, to automatically curate high-quality supervision data to continuously improve NLU in a large-scale conversational AI or digital assistant system. We showed with an extensive set of experiments on live traffic how the framework can be applied to improve NLU and analyzed its performance across 10 popular domains by traffic volume on a real production system. We further showed component-level analysis of our framework for more in-depth validation of its performance.
"," Natural Language Understanding  is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a general domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system and show its impact across 10 domains.",249
"   Chinese Word Segmentation  is a fundamental task for Chinese natural language processing , which aims at identifying word boundaries in a sentence composed of continuous Chinese characters. It provides a basic component for other NLP tasks like named entity recognition, dependency parsing, and semantic role labeling, etc.  Generally, most previous studies model the CWS task as a character-based sequence labeling task . Recently, pre-trained models  such as BERT  have been introduced into CWS tasks, which could provide prior semantic knowledge and boost the performance of CWS systems.  directly fine-tunes BERT on several CWS benchmark datasets.  fine-tunes BERT in a multi-criteria learning framework, where each criterion shares a common BERT-based feature extraction layer and owns a private projection layer.  combines Chinese character glyph features with pre-trained BERT representations. %  builds a unified BERT-based model for multi-criteria CWS tasks and fine-tunes it on eight CWS criteria jointly.  proposes a neural CWS framework WMSeg, which utilizes memory networks to incorporate wordhood information into the pre-trained model ZEN.  PTMs have been proved quite effective by fine-tuning on downstream CWS tasks. However, PTMs used in previous works usually adopt language modeling as pre-training tasks. Thus, they usually lack task-specific prior knowledge for CWS and ignore the discrepancy between pre-training tasks and downstream CWS tasks.  [t]  {c|}{the semi-final} \\ {c|}{閺夊骸顭倉 & 鏉╂稑鍙 & \multicolumn{2}{c|}{閸楀﹤鍠呯挧娉 \\ {c|}{閺夊骸顭倉 & 鏉╂稑鍙 & 閸 & 閸愬疇绂 \\ Seg. To leverage shared segmentation knowledge of different criteria, MetaSeg utilizes a unified architecture and introduces a multi-criteria pre-training task. Moreover, to alleviate the discrepancy between pre-trained models and downstream unseen criteria, meta learning algorithm is incorporated into the multi-criteria pre-training task of MetaSeg.  Experiments show that MetaSeg could outperform previous works significantly, and achieve new state-of-the-art results on twelve CWS datasets. Further experiments show that  MetaSeg has better generalization performance on downstream unseen CWS tasks in low-resource settings, and improve Out-Of-Vocabulary  recalls. To the best of our knowledge, MetaSeg is the first task-specific pre-trained model especially designed for CWS.       In this paper, we propose a CWS-specific pre-trained model , which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Experiments show that  could make good use of common prior segmentation knowledge from different existing criteria, and alleviate the discrepancy between pre-trained models and downstream CWS tasks.  also gives better generalization ability in low-resource settings, and achieves new state-of-the-art performance on twelve CWS datasets.  In the future, we will explore unsupervised pre-training methods for CWS, and whether representations from the pre-trained CWS model are helpful for other related NLP tasks.      
","     Recent researches show that pre-trained models  are beneficial to Chinese Word Segmentation .     However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks.     % However, existing approaches usually fine-tune general-purpose pre-trained models directly on separate downstream CWS corpora.     % These general-purpose pre-trained models usually adopt language modeling objectives, lack task-specific prior segmentation knowledge, and ignore the discrepancy between pre-training tasks and downstream CWS tasks.     In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task.     Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks.     Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings.",250
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  The following instructions are directed to authors of papers submitted to COLING-2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4   paper.  Authors from countries in which access to word-processing systems is limited should contact the publication co-chairs Derek F. Wong , Yang Zhao  and Liang Huang  as soon as possible.  We may make additional instructions available at \url{http://coling2020.org/}. Please check this website regularly.     In this work, we pursued a new research problem of M\&A prediction. Our transformer-based classifier leveraged the regularization benefits of adversarial training to enhance model robustness. More importantly, we built upon previous techniques to quantify the importance of words and help guarantee the generation of plausible counterfactual explanations with a masked language model in financial text classification. The results demonstrate superior accuracy and explanatory performance compared to state-of-the-art techniques. An obvious extension would be to include canceled deals into the classifier, or to predict novel M\&A events based on market descriptions of companies . Moreover, additional financial events  is yet another related task to be considered for further research.   
","   This document contains the instructions for preparing a paper submitted   to COLING-2020 or accepted for publication in its proceedings. The document itself   conforms to its own specifications, and is therefore an example of   what your manuscript should look like. These instructions should be   used for both papers submitted for review and for final versions of   accepted papers. Authors are asked to conform to all the directions   reported in this document.",251
"  Automatic question answering is a very active area of research within natural language processing. %Open-domain question answering looks for methods to re-utilize systems across multiple domains. One possible way to approach this task is to look for answers in the text passages of a collection of documents. Recent research has shown promising results on developing neural models for passage retrieval tasks, including Retrieval Question Answering, Open Domain Question Answering, and MS MARCO. The models in these systems are often trained using the dual encoder framework where questions and passages are encoded separately. Training an effective neural retrieval model usually requires a large amount of high-quality data. To alleviate the need of high-quality data, training can be approached in two-stages: pre-training on noise data and fine tuning on a smaller amount of high-quality data, also regarded as ``gold"" data. % One significant advantage of the dual encoder framework is that, once the question and passage embeddings are available, efficient nearest neighbour search can be used to retrieve the passages that contain the answers to the questions.    When used for question answering, one advantage of the dual encoder is that training in batches allows to use, for each question, the passages that answer all the other questions in the batch as negatives. Given that the training batches are randomly sampled from all the question-passage pairs, the negatives in the batch are random in nature. While effective in many retrieval tasks, random negatives have the limitation of not being targeted nor challenging enough to clearly separate the passage that answers a given question from any other passage. How to sample the negatives in a way that widens this separation and improves the contrast between the correct and incorrect passages remains an open question.  % A viable approach to negative sampling is to use ``hard"" negatives that are specific to each question and answer  pair. In this paper we systematically explore the use of ``hard'' negatives in the neural passage retrieval models that we train using a two-stage approach. Using hard negatives as part of the dual encoder framework has shown advantageous in different tasks . %Using hard negatives as part of the dual encoder framework has shown advantageous in cross-lingual tasks. %For example,  show that training with hard negatives generated by retrieving ``coarse"" negatives with low-resolution model improves the quality of the translation pairs retrieved with a dual encoder model. %Similarly,  showed improvement when using hard negatives retrieved with a BM25 model in the passage retrieval part of the Open Domain Question Answering task. %In contrast to previous works,  We explore different types of negatives, and experiment using them in both the pre-training and fine-tuning stages. The types of negatives we tried are: [label=]  ;}     We first use hard negatives on the data that we use to pre-train the models. We leverage the question generator model described in and generate new questions for each of the passages we use in the pre-training stage . %The new questions are paired with the original passages. %The augmented set of question-passage pairs is used to train the first stage of the neural retrieval model. % It has been shown as an effective approach to improve passage retrieval models. During pre-training we use negatives generated from strategy 4\footnote{Or strategy 1, if strategy 4 is not feasible} to improve the retrieval model, as the other strategies could introduce more false negatives into the data. %Our initial experiments showed that using retrieval models to find the hard negatives at this point, often generated very noisy question-passage pairs, especially because our pre-training data includes synthetic pairs. %As the generated question passage pairs sometimes are noise, retrieval-based approaches may create better question-passage pairs than the synthetic pairs. %We only apply a heuristic based context negatives on this pre-training task. Next, we continue with the fine tuning stage  using a small amount of gold training data. At this stage, we explore all four types of negative sampling. To the best of our knowledge, this is the first work that explores the effectiveness of hard negatives for passage retrieval in a systematic way, and integrates them in the retrieval models  pre-training stage. Our overall experimental architecture is outlined in Figure.  %For each question-passage pair in the training set, we collect negatives using the strategies listed above and augment them into the training.  We conduct experiments with this approach on two passage retrieval tasks: Open Domain QA  and SQuAD) and MS MARCO. %Open Domain QA Natural Questions~, Open Domain QA SQuAD, and MS MARCO. Our results show that all four kinds of hard negatives improve the dual encoder models significantly with consistent performance gains across both tasks. However, depending on the types of questions and their domain, one kind of hard negative may perform better than the others in a particular task. For example, context negatives work best in NQ and semantic retrieval-based negatives  work best in SQuAD. We further ensemble the models trained on different types of hard negatives. The final models achieve state-of-the-art performance on Open Domain QA task with an improvement over prior works of 0.8--2.9 points on accuracy rates.  %.  The main contribution of this paper are: [label=]      We presented four strategies to sample hard negatives in the context of automatic question answering. Our experiments on Natural Questions, SQuAD and MS MARCO show that our strategies are highly effective to improve the contrast between the passage that answers a question and all the other passages. Our models improved on the previous state-of-the-art for Open Domain QA by 0.8--2.9 points. We trained BERT-based dual encoder models using a two-stage approach. We demonstrated the positive impact of the hard negatives in both stages. Our changes to the model architecture already showed improvement over the baseline. We further improved the models with the use of synthetic data in the pre-training stage. The use of hard negatives at this stage showed considerable gains over the baseline. When we used four different types of hard negatives in the fine-tuning stage, we achieved state-of-the-art performance in passage retrieval. Mixing the different types of hard negatives proved to be the most effective strategy. Still, we achieved bigger gains when we used emsembling to combine the benefits of all the different types of hard negatives. Our results encourage us to keep exploring this area and investigate similar contrastive mechanisms to improve reading comprehension in end-to-end question answering systems.  
"," %In this paper we explore the discriminate training for neural passage retrieval models with hard negatives. %Four different hard negative sampling strategies are experimented, including one BM25 based hard negative, two semantic based hard negatives, and one heuristic hard negative. %For training the model, we employ a two stage dual encoder model with pre-training using synthetic data followed by a fine-tuning using the gold training data. %Discriminate training is applied on both stages. %The trained models are evaluated on 3 passage retrieval tasks from Open Domain QA NQ, Open Domain QA SQuAD, and MS MARCO. %Results show that all of them can improve the naive dual encoder models significantly with consistent performance gain over all three tasks. %However, there is no single type of hard negative perform best on all tasks. %Further analysis show that the synthetic question pre-training with discriminate training is an effective approach to improve the passage retrieval performance. %The best trained models establish the new state-of-the-art on retrieval tasks of Open Domain QA NQ and SQuAD. %",252
" % Events describe things that happen or occur in the world, mostly involving entities  who perform % or are affected by the events and spatio-temporal dimensions of the event . The same event may be mentioned in a document multiple times with different context. To recognize if two event mentions  refer to the same  contributes to the understanding of natural language text and resolving other NLP tasks.  In this work, we study the coreference resolution problem for both events and entities. Coreference resolution is commonly modeled as a binary classification problem : first learn features for each  mention, then classify two given mentions  \footnote{Some work maps the two mentions into a single matching score, e.g., ; this can be treated as a special case of binary classification.}. The essential step in this framework lies in the representation learning of each mention. However, prior work often failed to learn representations with powerful expressivity due to the following two reasons:  Point-wise representation learning.  % Usually a mention is surrounded by other words in a sentence.  Most work tries to learn mention representations by extracting features merely from that particular sentence including the mention. We argue that this routine of  representation learning in coreference does not match the end task  we are coping with: relation recognition of mention pairs. The predicted relation is for the input mention pair rather than individual mentions. By different context, two mentions can be referring to each other or not.  To fit the different scenarios, a mention should learn its representation by considering what its counterpart is.  Unstructured representation learning. An event mention consists of multiple arguments to describe the event: what, who, when, where, etc. Most prior work tried to encode all those elements into a single distributed representation vector and then compare the vectors of two mentions. This is less optimal since humans can recognize subjects, objects etc. and often compare event arguments of the same type . If, for instance, the event locations are different, people can  make a judgement quickly even without comparing other mention arguments.   Denoting a mention with a single distributed  vector lets machines lose the opportunity to conduct  fine-grained reasoning as humans do and uneasy to explain the model's prediction.  To promote the expressivity of representations, this work proposes % pairwise structured representation learning paired representation learning . \modelname In this work, we treat each mention pair rather than a single mention as the object for the representation learning. Specifically, we will concatenate the two sentences   as a whole sequence and  forward it into a ROBERTa  system.\footnote{RoBERTa will put a special token ``SEP'' to separate the two sentences.}   RoBERTa takes the ``whole sequence'' as an input so that each token, including the  mention spans, in the two sentences are able to compare with other tokens from the very beginning. This is better than comparing the two mentions  after  learning a representation for each of them separately. We apply this pairwise representation learning to both event and entity coreference tasks. The binary classifier or an mention matching function in the end will take a pair of contextualized mention representations for reasoning.    Looking at the following two sentence  and   % and we think we need to learn from humans' behaviors in recognizing event coreference.   : ``Over \textcolor{blue}{69,000 people} \underline{lost} their lives in the quake, including 68,636 in \textcolor{purple}{Sichuan province}.''  : ``Up to \textcolor{blue}{6,434 people} \underline{lost} their lives in Kobe earthquake and about 4,600 of them were from \textcolor{purple}{Kobe}.''  First, humans often determine the relationship between the two event mentions in  and  by comparing their triggers and arguments separately  as follows:  \textbullet\enspace ``69,000 people'' vs.  ``6,434 people''  \textbullet\enspace ``lost'' vs. ``lost''  \textbullet\enspace ``Sichuan province'' vs ``Kobe'' %  %       Second, the mismatch of some components may be  decisive or more decisive than that of others. For example, when people find the location ``Sichuan province'' does not match with ``Kobe'', they can directly claim the two events are not coreference even without looking at the whole sentence. This human behavior indicates that we should make full of the structure in an event, and an overall representation encompassing all event elements is less informative  to perform fine-grained cross-mention comparison which can actually improve the interpretability of the model predictions.  Overall, our \modelname~enables  two  mentions to learn from the context of each other, and improves the model's explainability by performing fine-grained reasoning. We report \modelname~on  both event coreference   and entity coreference benchmarks. Despite its simple architecture, \modelname~ surpasses the prior SOTA system by  big margins.                    In this work, we presented a simple and unified representation learning framework, \modelname, for event and entity coreference. \modelname~learns a  mention-pair representation by forwarding concatenated sentences into RoBERTa, where sentences provide the context of mentions. This algorithm is applied to both event and entity coreference benchmarks and obtains state of the art performance. In addition, we augmented this pairwise representation with structured argument features to further improve its performance in event coreference.      
"," Co-reference of Events and of Entities are commonly formulated as binary classification problems, given a pair of events or entities as input. Earlier work addressed the main challenge in these problems -- the representation of each element in the input pair by:   modelling the representation of one element  without  considering the other element in the pair;  encoding all attributes of one element  into a single non-interpretable vector, thus losing the ability to compare %fine-grained cross-element attributes.  In this work we propose paired representation learning  for  coreference resolution. % \drc{ % In this work we propose pairwise structured representation learning  \XD{Do we want to change the model name? Since it is not structured for entity. And for event, most numbers are from trigger only representation} \dr{Maybe just PairedRL ? If so, we can cahange Pairwise Structure to ``Paired"" in the title and the rest of the paper.}for  coreference resolution.}  Given a pair of elements  our model treats the pair's sentences as a single sequence so that each element in the pair learns its representation by encoding its own context as well the other element's context. In addition, when representing events, \modelname in that it represents the event's arguments to facilitate their individual contribution to the final prediction. As we show, in both  event  and entity coreference benchmarks, our unified approach, \modelname in that it represents the event's arguments to facilitate their individual contribution to the final prediction. As we show, in both  event  and entity coreference benchmarks, our unified approach, \modelname %This work studies the event and entity coreference which is commonly formulated as a binary classification problem given a pair of events or entities. The main challenge lies in the representation learning of each element in the input pair. However,  prior work mostly has the following drawbacks:  Systems model the representation of one object  with no consideration of   the other object in the pair;  Systems often encode all related attributes of one object  into a single and unexplainable vector; this reduces the model's interpretability and  mismatches the fact that humans tend to recognize the event relations by comparing fine-grained cross-event arguments. Motivated, we propose pairwise structured representation learning  \XD{Do we want to change the model name? Since it is not structured for entity. And for event, most numbers are from trigger only representation} for  coreference resolution. By ``Pairwise'', \modelname\enspace treats sentences of the input pair as a whole sequence so that each object in the pair learns its representation by encoding its own context as well the other's context. This paradigm applies to both event and entity coreference. In addition, \modelname\enspace develops a ``structured''  framework to represent all the event arguments so that each argument can explain its contribution to the final prediction.  In both  event  and entity coreference benchmarks, \modelname\enspace beats prior state of the art systems with big margins .",253
" Neural machine translation  has been explored typically in sentence-level translation settings. Such sentence-level nmt models inevitably suffer from ambiguities when multiple  %% semantically-different translations are accepted  interpretations are possible to a source sentence.  To address this issue, context-aware nmt models have recently been presented %to address the issue  to incorporate document-level information in translation. Most of the existing context-aware nmt models are end-to-end models which take as input the current source sentence to be translated and the context sentences, and then output a translation. These models are trained on document-level parallel data, namely, sentence pairs with surrounding, usually preceding, sentences in the source and target language. However, in practical scenarios, document-level bilingual data is limited in most language pairs and domains, % posing a challenge to building context-aware nmt systems .  In this study, we propose a simple yet effective approach to context-aware nmt  % consisting of  using two primitive components, a sentence-level nmt model and a document-level language model . This approach allows us to independently train the two components on bilingual data and monolingual data, respectively, without resorting to expensive document-level bilingual data.  % and thereby no document-level bilingual data is needed. To give a probabilistic foundation to this combination of two independent models, we exploit % take advantage of  the probabilistic nature of nmt decoding. When generating a sequence, a left-to-right decoder outputs a categorical probability distribution over the vocabulary at every time step. % . The decoder assigns higher probability to the tokens that would be more suitable at that step. Therefore,  % we can assume that  when multiple valid translations are possible to the source sentence, % , which has ambiguities a sentence-level nmt is confused by,  the decoder just gives a higher  % sequence  probability to the translation that is plausible without considering contexts.  % than to wrong ones. Our idea is to adjust the probability distributions in a context-aware manner using a document-level lm of the target language which  % is capable of modeling  models inter-sentential dependencies in the target side document.  % Since a network structure of nmt models evolves very quickly, model-agnostic approach like ours is more preferable than model-tweaking approach .  We evaluate our methods on English to French, Russian and Japanese translations with OpenSubtitles2018 corpus in terms of the bleu scores and contrastive discourse test sets. Experimental results confirmed that our method achieved comparable performance with existing context-aware nmt models.  The contributions of this paper are as follows:       without the need for document-level parallel data}; we can build context-aware nmt models from the state-of-the-art sentence-level nmt models in various language pairs and domains.      while adding a probabilistic foundation. %     decoding combined with a language model, and therefore could be regarded as an extension of shallow fusion, but our approach is more sophisticated in that it has a probabilistic grounding; %     computed by a language model highly contribute to improve performance on the contrastive tests, which are designed to evaluate the performance of context-aware nmt models.     We present an approach to context-aware  based on  between the context and the current sentence. We first provide the formulation of the objective, , and the computation process of the  using a sentence-level translation model and a document-level language model. We investigate two search methods, reranking and beam search, and evaluate the methods in English-French, English-Russian, and Japanese-English translation. We also provide some analysis and visualization to better understand the nature of  between the context and the current sentence.  We plan to design context-aware  using  for context-aware  models. We will extend our method to non-autoregressive . We will release all of the code to promote the reproducibility of our results.    
"," % There exist inevitable ambiguities in translating a single sentence, and we resort to context beyond the target sentence for resolving such ambiguities. Although many context-aware neural machine translation models have been proposed to incorporate contexts in translation,  most of those models are trained end-to-end on parallel documents aligned in sentence-level.  Because only a few domains  have such document-level parallel data, we cannot perform accurate context-aware translation in most domains. We therefore present a simple method to turn a sentence-level translation model into a context-aware model by incorporating a document-level language model into the decoder. Our context-aware decoder is built upon only a sentence-level parallel corpora and monolingual corpora; thus no document-level parallel data is needed. In a theoretical viewpoint, the core part of this work is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We show the effectiveness of our approach in three language pairs, English to French, English to Russian, and Japanese to English, by evaluation in bleu and contrastive tests for context-aware translation.",254
" A keyphrase is a multi-word text representing highly abstractive information in a long document. Keyphrase extraction  is a task that aims to generate an appropriate keyphrase set for the given document, thus helping to identify salient contents and concepts from the document. Recently, the KE task has attracted much research interest since it serves as an important component of many downstream applications such as text summarization, document  classification, information retrieval and question generation.  Early KE systems commonly operate in an extractive manner, which usually consists of two steps: 1) selecting candidates from the source document using heuristic rules,  and 2) ranking the candidates list to determine which is correct. However, the two-step ranking approaches are usually based on feature engineering, which is labor-intensive. Motivated by the progress in sequence-to-sequence applications of neural networks, KE research's focus has gradually shifted to deep learning methods.  first formulate KE as a sequence generation problem and introduce an attentive Seq2Seq framework to generate the keyphrase sequence conditioned on the input document. Compared with traditional methods, the Seq2Seq based method achieves superior performance.  Seq2Seq based KE is exposed to two major challenges: 1) Document-level representation learning. For any Seq2Seq generative framework, the latent hidden representation is a very important factor, and its quality will directly affect the decoder's performance. In KE task, the input is commonly a long document instead of a sentence, which poses a greater challenge to latent representation learning. 2) Modeling the compositionality of keyphrases set. The elements in the keyphrase set are dependent and correlated. That is, better modeling the inherent composition embodied in the keyphrase set during the learning process will effectively boost the diversity and quality of final results.   Recently, various approaches have been proposed to optimize the Seq2Seq generation framework in KE task. To learn a better latent representation, previous studies try to introduce different encoding structures  to address the two issues above simultaneously. We explore to incorporate the dependency tree for document representation learning in the encoder part. The syntactic dependency tree can help to locate key information in a document. In practice, the document graph  is constructed depending on the syntactic dependency tree, and then a convolution process will be operated over .  On the other hand, we rethink the implication of compositionality in the keyphrase set. In the training process of generative models, whether a candidate keyphrase should be generated not only hinges on the document itself, but also depends on the keyphrases that have already been generated. Therefore, a dynamic graph updating mechanism is introduced to explicitly modeling the inter-dependency among keyphrases. In our method, the graph structure in the encoder part will be dynamically updated according to the keyphrases generated in the decoder part. Concretely, after one keyphrase is decoded, its information will be transferred to modify the edge weights in the document graph through a score function, and the latent hidden representation will also be updated. In this approach, we could dynamically ensure the information exchange between encoder and decoder parts in both directions.   The contribution of this work is three-fold:  1) A novel generative framework, Div-DGCN, is proposed that leverages both the dynamic syntactic graph encoder and diversified inference process for KE. 2) A dynamic computation mechanism is adopted to model the compositionality in keyphrase set explicitly and then enhancing the information interchange between the encoder and decoder parts in the Seq2Seq architecture.  3) Extensive experiments conducted on five benchmarks show that our proposed method is effective against competitive baselines on several metrics.    Modeling informative latent representations as well as capturing keyphrase dependencies is essential to Seq2Seq keyphrase extraction models. In this work we propose a Div-DGCN framework to address these two issues by enhancing the encoder with syntactic graph and modifying the graph edge information in the keyphrase decoding process. Further enhanced with a diversified inference process, the model can generate accurate and diverse outputs. Experimental results show that our model can surpass previous competitive models on various metrics and benchmark datasets, proving its effectiveness.  \clearpage    \clearpage 
"," Keyphrase extraction  aims to summarize a set of phrases that accurately express a concept or a topic covered in a given document. Recently, Sequence-to-Sequence  based generative framework is widely used in KE task, and it has obtained competitive performance on various benchmarks. The main challenges of Seq2Seq methods lie in acquiring informative latent document representation and better modeling the compositionality of the target keyphrases set, which will directly affect the quality of generated keyphrases. In this paper, we propose to adopt the Dynamic Graph Convolutional Networks  to solve the above two problems simultaneously. Concretely, we explore to integrate dependency trees with GCN for latent representation learning. Moreover, the graph structure in our model is dynamically modified during the learning process according to the generated keyphrases. To this end, our approach is able to explicitly learn the relations within the keyphrases collection and guarantee the information interchange between encoder and decoder in both directions. Extensive experiments on various KE benchmark datasets demonstrate the effectiveness of our approach.",255
" Neural machine translation , as the state-of-the-art machine translation paradigm, has recently been approached with two different sequence decoding strategies. The first type autoregressive translation  models generate output tokens one by one following the left to right direction, but it is often criticized for its slow inference speed. The second type non-autoregressive translation  models adopt a parallel decoding algorithm to produce output tokens simultaneously, but the translation quality of which is often inferior to auto-regressive models.  % we transfer the AT knowledge to NAT models. A line of research argues that the lack of contextual dependency in target sentences potentially leads to the deteriorated performance of NAT models. To boost the NAT translation performance, many recent works resort to the knowledge transfer from a well-trained AT model. Typical knowledge tranfer methods include sequence-level knowledge distillation with translation outputs generated by strong AT models, word-level knowledge distillation with AT decoder representations, and fine-tuning on AT model by curriculum learning, etc.  %shared encoder  In this work, we adopt a multi-task learning framework with a shared encoder  to transfer the AT model knowledge into the NAT model.  %Our framework jointly optimizes the AT and NAT models to boost the NAT translation quality.  Specifically, we take the AT task as an auxiliary task by sharing the encoder parameters of both AT and NAT models. We hypothesize that AT and NAT encoders, although they belong to the same sequence-to-sequence learning task, capture different linguistic properties and representations of source sentences. To empirically verify our hypothesis, we evaluate the encoder on a set of probing tasks .   Our contributions are as follows: [itemsep=1pt,topsep=0pt,parsep=0pt,partopsep=0pt]     .     \iffalse    Classic non-autoregressive models boost the translation performance by transferring the AT knowledge to NAT models via knowledge distillation. After some preliminary experiments on the source sentence representations,  In this paper, we have presented a novel multi-task learning approach for NAT model with a hard parameter sharing mechanism. Experimental results confirm the significant effect of the proposed  model, which shows the complementary effects of multi-task learning to the knowledge distillation method.   Based on our , there are many promising directions for future research. For example, 1) decoder interaction: knowledge distillation in an online fashion between AT and NAT decoders; 2) share-all framework: shared-encoder and shared-decoder with two decoding strategies, and the model can dynamically choose the optimal decoding strategy during model inference.  One model with two decoding strategies. We can share all parameters  of AT and NAT models, and dynamically choose the optimal decoding strategy during model inference.       
"," Non-Autoregressive machine Translation  models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation  knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties and representations of source sentences. Therefore, we propose to adopt the multi-task learning to transfer the AT knowledge to NAT models through the encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 English$\Leftrightarrow$German and WMT16 English$\Leftrightarrow$Romanian datasets show that the proposed multi-task NAT achieves significant improvements over the baseline NAT models. In addition, experimental results demonstrate that our multi-task NAT is complementary to the standard knowledge transfer method, knowledge distillation. }",256
" In language modelling , we learn distributions over sequences of words, subwords or characters, where the latter two can allow an open-vocabulary generation. We rely on subword segmentation as a widespread approach to generate rare subword units . However, the lack of a representative corpus, in terms of the word vocabulary, constrains the unsupervised segmentation .  As an alternative, we could use character-level modelling, since it also has access to subword information , but we face long-term dependency issues and require longer training time to converge.   In this context, %our research question is: Is there a segmentation approach that is corpus-independent and provides a shorter length sequence than characters?  we focus on syllables, which are based on speech units: ``A syl-la-ble con-tains a sin-gle vow-el u-nit''. These are more linguistically-based units than characters, and behave as a mapping function to reduce the length of the sequence with a larger ``alphabet'' or syllabary. Their extraction can be rule-based and corpus-independent, but data-driven methods or hyphenation using dictionaries can approximate them as well .  % .  Previous work on syllable-aware neural \lm failed to beat characters in a closed-vocabulary generation at word-level ;  %,  however, we propose to assess syllables under three new settings.  %but we further discuss two points as follows.  First, we analysed an open-vocabulary scenario with syllables by disregarding additional functions in the input layer . Second, we extended the scope from 6 to 20 languages  to cover different levels of orthographic depth, which is the degree of grapheme-phoneme correspondence  and a factor that can increase complexity to syllabification. English is a language with deep orthography  whereas Finnish is transparent . Third, we distinguished rule-based syllabification with hyphenation tools, but also validated their proximity for LM. %we prefer to use rule-based syllabification whenever available, and only employ hyphenation proxies otherwise.   % even with specific segmentation rules.   Therefore, we revisit \lm for open-vocabulary generation with syllables using pure recurrent neural networks for a more diverse set of languages, and compare their performance against characters and other subword units. %We analyse their overlap with other subword units, and compare their performance against s. %We investigate %, and we also include languages % from the Universal Dependencies dataset ,  %with more transparent orthographies, such as in Turkish  or Finnish . %Our results confirm that syllables are a reliable segmentation setting for language modelling, even when the language presents a deep orthography.  %less-ambiguous syllabification due to  %a recent alphabetisation . We thereupon explore the syllables effect in another generation task such as NMT.     We proved that syllables are valuable for an open-vocabulary \lm task, where they behave positively even for languages with deep orthography, and they overcome character-level models that require longer time to train. Syllables do not have an embedded meaning in most of the languages; however, the required effort for their segmentation could be advantageous against morphological-aware or unsupervised-driven methods. Given our analysis, we could consider working on syllable-driven subword segmentation, neural machine translation and hybrid-\lm .   
"," Language modelling is regularly analysed at word, subword or character units, but syllables are seldom used. Syllables provide shorter sequences than characters, they can be extracted with rules, and their segmentation typically requires less specialised effort than identifying morphemes. We reconsider syllables for an open-vocabulary generation task in 20 languages.  We use rule-based syllabification methods for five languages and address the rest with a hyphenation tool, which behaviour as syllable proxy is validated. With a comparable perplexity, we show that syllables outperform characters, annotated morphemes and unsupervised subwords. Finally, we also study the overlapping of syllables concerning other subword pieces and discuss some limitations and opportunities.",257
" Sanskrit is considered as one of the oldest Indo-Aryan languages. The oldest known Sanskrit texts are estimated to be dated around 1500 BCE. A large corpus of religious, philosophical, socio-political and scientific texts of multi cultural Indian Subcontinent are in Sanskrit. Sanskrit, in its multiple variants and dialects, was the Lingua Franca of ancient India ~. Therefore, Sanskrit texts are an important resource of knowledge about ancient India and its people. Earliest known Sanskrit documents are available in the form called . Rigveda, the oldest of the four Vedas, that are the principal religious texts of ancient India, is written in . In sometime around 5\textsuperscript{th} BCE, a Sanskrit scholar named  ~ wrote a treatise on Sanskrit grammar named , in which  formalized rules on linguistics, syntax and grammar for Sanskrit. 's grammar is globally appreciated for its insightful analysis of Sanskrit and completeness of its descriptive coverage of the spoken standard language of 's time.    \footnote{https://www.britannica.com/topic/Ashtadhyayi} is the oldest surviving text and the most comprehensive source of grammar on Sanskrit today and provides often unique information on Vedic, regional and  socio-linguistic usage.  literally means eight chapters and these eight chapters contain around 4000 sutras or rules in total. These rules completely define the Sanskrit language as it is known today.  is remarkable in its conciseness and contains highly systematic approach to grammar. Because of its well defined syntax and extensively well codified rules, many researchers have made attempts to codify the 閳ユ獨 sutras as computer programs to analyze Sanskrit texts. This paper tries to address the problem of unavailability of benchmark corpus and provides morphological analysis method for derivative nouns as a result of Sanskrit suffixes applied on root verbs and nouns using a machine learning approach.   Different ways of inflectional word formation as mentioned by ~ are as below:  ) suffixes.\\ - [Word + Suffix] Stem: secondary  suffixes.\\ - [Word + Word] Stem: compounding.\\ - [Root + Suffix] Word: verb inflection.\\ - [Stem + Suffix] Word: noun inflection.   Here in this introduction, we will explain more about primary and secondary suffixes. Sanskrit is a rich inflected language and depends on nominal and verbal inflections for communication of meaning ~. A fully inflected unit is called pada. The  are the inflected nouns and the  are the inflected verbs.   [h]           These are formed when the primary affixes called  are added to verbs to derive substantives, adjectives or indeclinable.  play a vital role in understanding Sanskrit language. Many morphological analyzers are lacking the complete analysis of . Examples of  pratyaya are as below:   +   =  \\  +    =    suffixes are mainly of seven types viz. .   The secondary derivative affixes called  derive secondary nouns from primary nouns. Some examples of  pratyaya are as below:    +   =  \\   +   =    suffixes are mainly of fourteen types.    In this research work, we propose Pratyaya-Kosh, a benchmark corpus to help researchers new to Sanskrit in building AI based Morphological Analyzer for Sanskrit derivative nouns. Also we propose neural approach for learning derivative noun formation without use of any external resources such as language models, morphological or phonetic analyzers and still manage to outperform existing approaches. In future we intend to extend current work to verb derivative and indeclinable derivative using machine learning methods. Proposed models can be further refined by using additional training data. Benchmark corpus  will be made available on git hub.       
"," This paper presents first benchmark corpus of Sanskrit Pratyaya  and inflectional words  formed  due to suffixes along with neural network based approaches to process the formation and splitting of inflectional words. Inflectional words spans the primary and secondary derivative nouns as the scope of current work. Pratyayas are an important dimension of morphological analysis of Sanskrit texts. There have been Sanskrit Computational Linguistics tools for processing and analyzing Sanskrit texts. Unfortunately there has not been any work to standardize \& validate these tools specifically for derivative nouns analysis. In this work, we prepared a Sanskrit suffix benchmark corpus called Pratyaya-Kosh to evaluate the performance of tools. We also present our own neural approach for derivative nouns analysis while evaluating the same on most prominent Sanskrit Morphological Analysis tools. This benchmark will be freely dedicated and available to researchers worldwide and we hope it will motivate all to improve morphological analysis in Sanskrit Language.",258
"  Sanskrit is one of the oldest of the Indo-Aryan languages. The oldest known Sanskrit texts are estimated to be dated around 1500 BCE. It is the one of the oldest surviving languages in the world. A large corpus of religious, philosophical, socio-political and scientific texts of multi cultural Indian Subcontinent are in Sanskrit. Sanskrit, in its multiple variants and dialects, was the Lingua Franca of ancient India . Therefore, Sanskrit texts are an important resource of knowledge about ancient India and its people. Earliest known Sanskrit documents are available in the form called {, the oldest of the four Vedas, that are the principal religious texts of ancient India, is written in { century BCE, a Sanskrit scholar named {, in which { is the oldest surviving text and the most comprehensive source of grammar on Sanskrit today. { is remarkable in its conciseness and contains highly systematic approach to grammar. Because of its well defined syntax and extensively well codified rules, many researchers have made attempts to codify the {-{ to place) is the principle of sounds coming together naturally according to certain rules codified by the grammarian {. There are 3 different types of Sandhi as defined in { vAk + hari = vAgGari  punaH + api = punarapi     Sandhi Split on the other hand, resolves Sanskrit compounds and 閳ユ笡honetically merged閳  words into its constituent morphemes. Sandhi Split comes with additional challenge of not only splitting of compound word correctly, but also predicting where to split. Since Sanskrit compound word can be split in multiple ways based on multiple split locations possible, split words may be syntactically correct but semantically may not be meaningful.   tadupAsanIyam = tat + upAsanIyam tadupAsanIyam = tat + up + AsanIyam      The current resources available for doing Sandhi in open domain are not very accurate. Three most popular publicly available set of Sandhi tools viz. JNU, UoH \& INRIA tools are mentioned in table .            {|p{4cm}|p{10cm}|}     \toprule           An analysis and description of these tools is present in the paper on Sandhikosh . The same paper introduced a dataset for Sandhi and Sandhi Split verification and compared the performance of the tools in table  on that dataset.  Neural networks have been used for Sandhi Split by many researchers, for example ,  and . The task of doing Sandhi has been mainly addressed as a rule based algorithm e.g. . There is no research on Sandhi using neural networks in public domain so far. This paper describes experiments with Sandhi operation using neural networks and compares results of suggested approach with the results achieved using existing Sandhi tools .    Many researchers like  and  have tried to codify {} , UoH Sandhi Splitter  and INRIA Sanskrit reader companion. The paper  compares the performance of above 3 tools with their results. This was an attempt to create benchmark in the area of Sanskrit Computational Linguistics.     In this research work, we propose novel algorithms for Sandhi word formation and Sandhi Split that can be trained without use of any external resources such as language models, morphological or phonetic analyzers, and still manage to match or outperform existing approaches. Due to the simplicity of the models, these are computationally inexpensive to train and execute. In future we intend to extend current work to internal Sandhi and internal Sandhi-split using machine learning methods. Proposed models can be further refined by using additional training data as well as investigating techniques to reduce the errors in current training data.          The next two lines define the bibliography style to be used, and    the bibliography file. 
"," This paper describes neural network based approaches to the process of the formation and splitting of word-compounding, respectively known as the Sandhi  and Vichchhed, in Sanskrit language. Sandhi is an important idea essential to morphological analysis of Sanskrit texts. Sandhi leads to word transformations at word boundaries. The rules of Sandhi formation are well defined but complex, sometimes optional and in some cases, require knowledge about the nature of the words being compounded. Sandhi split or Vichchhed is an even more difficult task given its non uniqueness and context dependence. In this work, we propose the route of formulating the problem as a sequence to sequence prediction task, using modern deep learning techniques. Being the first fully data driven technique, we demonstrate that our model has an accuracy  better than the existing methods on multiple standard datasets, despite not using any additional lexical or morphological resources. The code is being made available at https://github.com/IITD-DataScience/Sandhi\_Prakarana",259
"   % Unsupervised representation learning allows models to learn high-level latent representations from unlabeled data.  % Models pretrained from unsupervised data can be fine-tuned with a small amount of labeled data. % % Deep probabilistic generative models presents a powerful approach to learn representations by modeling the data generation process.  % Variational AutoEncoders  are one of the popular approaches to representation learning by modeling the latent features in a unit Gaussian space. % Vector-Quantized VAE  is a method to learn discrete representations from data.  Speech waveforms are a complex, high-dimensional form of data influenced by a number of underlying factors, which can be broadly categorized into linguistic contents and speaking styles. % Learning disentangled latent representations from speech has a wide set of applications in generative tasks, including speech synthesis, data augmentation, voice transfer, and speech compression. Downstream tasks such as speech recognition  and speaker classification  can also benefit from such learned representations. % A pre-trained model can also be fine-tuned for classification tasks such as speech recognition and speaker classification. %   Because of the cost, complexity, and privacy concerns around collecting labeled speech data, there has been a lot of interest in unsupervised representation learning for speech. Of particular interest is to learn representations for speech styles from unsupervised data due to the difficulty in describing prosody with human labels.  Some previous works aim to learn global representations from entire speech sequences. % Global style tokens learn a dictionary of embeddings from speech without prosody labels. % As another example, Hsu et al.  model disentangled speech styles with a hierarchy of variational autoencoder . % Hu et al.  proposed a content and style separation model by pre-training on a single-speaker dataset with text transcription and minimizing mutual information  between the content and style representation.  Other works try to learn fine-grain localized representations of speech. %  apply self-supervised learning to unlabeled speech data and extract localized latent representations that can be fine-tuned for speech recognition. % FHVAE learns a sequence of high-level features by applying VAE to every frame. %  leverages vector-quantized VAE  to learn a discrete sequence representation of speech.  We propose a framework to learn both global and localized representation of speech. In order to disentangle content and style representations, we apply  a local encoder with VQ layer to learn a discrete per-timestep representation of the speech that captures the linguistic contents and  a global VAE to extraction per-utterance representations to reflect the speech styles. We further disentangle the local and global representations with a mutual information loss. We evaluate the quality of linguistic and style representations by running speech and speaker recognition models on the reconstructed speech. We also show that the global representation captures the speaker information well enough that we can obtain a speaker classification model by training a linear projection layer on top of the global representation with only one example per speaker.      We present a framework to learn disentangled representations of speech with unlabeled data. The framework includes a local VQ encoder to extract a discrete sequence representation of the speech contents and a global VAE encoder to learn a continuous representation of speech styles. Our evaluation shows that the discrete sequence representation effectively captures the linguistic contents while the continuous global representation encapsulates the speaker style. Additionally, we also show the application from our pre-trained model, in which we successfully train a speaker recognition system with high accuracy  only with one sample per speaker.  \vfill\pagebreak    References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   ------------------------------------------------------------------------- 
"," We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of:  a local encoder that captures per-frame information;  a global encoder that captures per-utterance information; and  a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that  the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates , even with a different global encoding;  the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker. % % \rpang{How about: Our deep generative model consists of:  a local encoder that captures per-frame information;  a global encoder that captures per-utterance information; and  a conditional decoder that reconstruct speech given local and global latent variables, potentially extracted from different utterances. Our experiments show that  the local latent variables encode speech contents, since reconstructed speech can be recognized by ASR with low word error rates , even with a different global encodings;  the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding and a speaker recognition model can be trained from the global latent variables with as few as one supervised example per speaker.  % }",260
"  %Sentiment analysis  is one of the fundamental tasks in natural language processing that aims to find the attitude that the author expressed in his/her sentence. One of the important sub-tasks of SA is aspect based sentiment analysis in which the goal is to find the sentiment polarity toward a specific aspect mentioned in the sentence. Due to the importance of ABSA, several sub-tasks has been proposed and studied for this problem, including aspect category extraction, aspect term extraction, opinion word extraction and opinion summarization . Among these sub-tasks, Targeted Opinion Word Extraction  is an important sub-task that might provide useful information to explain the prediction of the sentiment polarity from an ABSA system. In particular, the goal of TOWE is to find the words that express the attitude of the author toward a specific target mentioned in that sentence. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", the word ``good"" is the opinion word for the target ``food"" while delicious is the opinion word for the target word ``drinks"". Among different applications, TOWE can be used for target-oriented sentiment analysis  and pair-wise opinion summarization .  %Sentiment analysis  is one of the fundamental tasks in natural language processing that aims to find the attitude that the author expressed in his/her sentence. One of the important sub-tasks of SA is aspect based sentiment analysis in which the goal is to find the sentiment polarity toward a specific aspect mentioned in the sentence. Due to the importance of ABSA, several sub-tasks has been proposed and studied for this problem, including aspect category extraction, aspect term extraction, opinion word extraction and opinion summarization . Among these topics, Targeted Opinion Word Extraction  is an important task that might provide useful information to explain and/or improve the sentiment polarity prediction of the ABSA systems. In particular, given a target word  in the input sentence, the goal of TOWE is to find the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", ``good"" is the opinion word for the target word ``food"" while the opinion words for the target word ``drinks"" would involve ``delicious''. Among different applications, TOWE finds its application in target-oriented sentiment analysis  and pair-wise opinion summarization .   %Targeted Opinion Word Extraction  is an important task in aspect based sentiment analysis  of sentiment analysis . Given a target word  in the input sentence, the goal of TOWE is to find the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, in the sentence ``The food is good, especially their more basic dishes, and the drinks are delicious"", ``good"" is the opinion word for the target word ``food"" while the opinion words for the target word ``drinks"" would involve ``delicious''. As the opinion words might provide useful information to explain and/or improve the sentiment prediction of the ABSA systems, TOWE can be applied in different problems, including target-oriented sentiment analysis  and pair-wise opinion summarization .  Targeted Opinion Word Extraction  is an important task in aspect based sentiment analysis  of sentiment analysis . Given a target word  in the input sentence, the goal of TOWE is to identify the words in the sentence  that help to express the attitude of the author toward the aspect represented by the target word. For instance, as a running example, in the sentence ``All warranties honored by XYZ  are disappointing."", ``disappointing"" is the opinion word for the target word ``warranties"" while the opinion words for the target word ``company"" would involve ``reputable''. Among others, TOWE finds its applications in target-oriented sentiment analysis  and opinion summarization .   %As the opinion words might provide useful information to explain and/or improve the sentiment prediction of the ABSA systems, TOWE can be applied in different problems, including target-oriented sentiment analysis  and pair-wise opinion summarization .  %A notable problem is that although the related tasks of TOWE has been extensively explored in the past, there have been only a few work to explicitly consider the TOWE problem in the literature . In particular, the most related task of TOWE is opinion word extraction  that aims to locate the terms used to express attitude explicitly in the sentence . A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence  while the opinion words in TOWE should be explicitly paired with a given target word. Note that some previous works have also attempted to jointly predict the target and opinion words ; however, the target words are still not paired with their corresponding opinion words in these studies .    %Among the previous works for TOWE, t  The early approach for TOWE has involved the rule-based and lexicon-based methods  while the recent work has focused on deep learning models for this problem . One of the insights from the rule-based methods is that the syntactic structures  of the sentences can provide useful information to improve the performance for TOWE . However, these syntactic structures have not been exploited in the current deep learning models for TOWE . Consequently, in this work, we seek to fill in this gap by extracting useful knowledge from the syntactic structures to help the deep learning models learn better representations for TOWE. In particular, based on the dependency parsing trees, we envision two major syntactic information that can be complementarily beneficial for the deep learning models for TOWE, i.e., the syntax-based opinion possibility scores and syntactic word connections for representation learning. First, for the syntax-based possibility scores, our intuition is that the closer words to the target word in the dependency tree of the input sentence tend to have better chance for being the opinion words for the target in TOWE. For instance, in our running example, the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , ``disappointing"" is directly connected to ``warranties"", promoting the distance between ``disappointing"" and ``warranties""  in the dependency tree as an useful feature for TOWE. Consequently, in this work, we propose to use the distances between the words and the target word in the dependency trees to obtain a score to represent how likely a word is an opinion word for TOWE . These possibility scores would then be introduced into the deep learning models to improve the representation learning for TOWE.  In order to achieve such possibility score incorporation, we propose to employ the representation vectors for the words in the deep learning models to compute a model-based possibility score for each word in the sentence. The model-based possibility scores also aim to quantify the likelihood of being an opinion word for each word in the sentence; however, they are based on the internal representation learning mechanism of the deep learning models for TOWE. To this end, we propose to inject the information from the syntax-based possibility scores into the models for TOWE by enforcing the similarity/consistency between the syntax-based and model-based possibility scores for the words in the sentence. The rationale is to leverage the possibility score consistency to guide the representation learning process of the deep learning models  to generate more effective representations for TOWE. In this work, we employ the Ordered-Neuron Long Short-Term Memory Networks   to obtain the model-based possibility scores for the words in the sentences for TOWE. ON-LSTM introduces two additional gates into the original Long Short-Term Memory Network  cells that facilitate the computation of the model-based possibility scores via the numbers of active neurons in the hidden vectors for each word.  %The second type of syntactic information employed for TOWE in this work considers the dependency connections between the words in the sentence.   %As the deep learning models need to compute a representation vector for each word to perform opinion word prediction in TOWE,   %While the possibility scores aim to improve the representation vectors for TOWE via the syntax-based possibility features, the second type of syntactic information in this work seeks to do so by leveraging the dependency connections between the words to infer the effective context words to be encoded in the representation vector for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector for a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``warranties"" as the target word and ``reputable"" as the word we need to compute the representation vector. One the one hand, it is important to include the information of the neighboring words of ``reputable""  in the representation so the models can know the context for the current word . On the other hand, the information about the target word  should also be encoded in the representation vector for ``reputable"" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label  for ``reputable"" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``I'' in the representation for ``reputable"" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network  model  to produce the final representation vectors for opinion word prediction.  For the second type of syntactic information in this work, the main motivation is to further improve the representation vector computation for each word by leveraging the dependency connections between the words to infer the effective context words for each word in the sentence. In particular, motivated by our running example, we argue that the effective context words for the representation vector of a current word in TOWE involve the neighboring words of the current word and the target word in the dependency tree. For instance, consider the running example with ``warranties"" as the target word and ``reputable"" as the word we need to compute the representation vector. On the one hand, it is important to include the information of the neighboring words of ``reputable""  in the representation so the models can know the context for the current word . On the other hand, the information about the target word  should also be encoded in the representation vector for ``reputable"" so the models can be aware of the context of the target word and make appropriate comparison in the representation to decide the label  for ``reputable"" in this case. Note that this syntactic connection mechanism allows the models to de-emphasize the context information of ``I'' in the representation for ``reputable"" to improve the representation quality. Consequently, in this work, we propose to formulate these intuitions into an importance score matrix whose cells quantify the contextual importance that a word would contribute to the representation vector of another word, given a target word for TOWE. These importance scores will be conditioned on the distances between the target word and the other words in the dependency tree. Afterward, the score matrix will be consumed by a Graph Convolutional Neural Network  model  to produce the final representation vectors for opinion word prediction.  Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words and those for the other words in the sentence. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several benchmark datasets.  %Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-oriented opinion words  and those for the other opinion words  in the sentence . Extensive experiments are conducted to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several datasets.  %Finally, in order to further improve the induced representation vectors for TOWE, we introduce a novel inductive bias that seeks to explicitly distinguish the representation vectors of the target-related opinion words  and those for the other opinion words  in the sentence . As both target-related and non-target opinion words can be used to express the opinion of the author , we expect that the explicit representation distinction would help to better separate the two types of opinion words based on the target word, eventually improving the performance for TOWE in this work. We conduct extensive experiments to demonstrate the benefits of the proposed model, leading to the state-of-the-art performance for TOWE in several datasets.   %the close distance between ``disappointing"" and the target word will suggest the models to include the information of ``warranties"" into the representation vector for ``disappointing"" while the long distance between ``warranties"" and ``reputable"" can help to prevent/mitigate that for the representation vector of ``reputable"". The presence of the information from the target word in the representation vectors will help the models to successfully accept ``disappointing"" as an opinion word and reject ``reputable"" in this case.   %the close words to the target word would provide more effective information to induce the representation vectors for a word in the sentence in TOWE than the farther ones.   %we argue that the syntactic neighboring words in the dependency tree would provide effective information to induce the representation vector for a word in opinion word prediction. For instance, in the running example with the target word ``warranties"", the close distance between ``disappointing"" and the target word will suggest the models to include the information of ``warranties"" into the representation vector for ``disappointing"" while the long distance between ``warranties"" and ``reputable"" can help to prevent/mitigate that for the representation vector of ``reputable"". The presence of the information from the target word in the representation vectors will help the models to successfully accept ``disappointing"" as an opinion word and reject ``reputable"" in this case.  %employ the dependency connections between the words to infer the effective context words .    %employ the syntactic neighboring words to compute the representation vectors for a word in the sentence for TOWE.   %extends the popular Long Short-Term Memory Networks  by introducing two additional gates  in the hidden vector computation. These new gates controls how long each neuron in the hidden vectors should be activated across different time steps  in the sentence . Based on such controlled neurons, the model-based importance score for a word can be determined by the number of active neurons that the word possesses in the operation of ON-LSTM. To our knowledge, this is the first time ON-LSTM is applied for RE in the literature.  %How can we encode the syntax-based importance scores of the words into a deep model? In this paper, we propose to employ the syntax-based importance scores to retain or update the information encoded in the representations of each word. In particular, those words that are syntactically more important should retain more information in the computation graph of the deep model while the information about less important words should be discarded more frequently. In order to impose this information update policy in our model, we use the new proposed architecture Ordered-Neuron Long Short-Term Memory  . ON-LSTM is an extension of the well-known Long Short-Term Memory  with two additional gates . These new gates are employed to control the frequency of updating each neuron across different time steps  in the sentence. Concretely, the values of the master forget and input gates determine how much information in the hidden vector of the LSTM cell should be retained or updated based on the word at the current time step. Thereby, one can infer the importance scores inferred by the model  using the values of the master forget or input gates. So, based on this characteristics of ON-LSTM, to encode the syntax-based importance scores into our model, we propose to exploit the syntax-based importance scores to regulate model-based importance scores. Specifically, in training time, we encourage the model-based scores to be consistent with syntax-based importance scores.  %the two words ``disappointing"" and ``warranties"" are directly connected to each other.  %Early feature-based models  has shown that syntactical structure of the sentence is useful for TWOE. More specifically, the application of dependency tree for TOWE is two fold:  Pairwise Word Importance: Dependency tree is useful to infer the relative importance of a word toward another word  in the same sentence. This relative importance could be helpful for TOWE to attend to the important words for the target word. To infer pair-wise importance of two words using dependency tree, one can computes the distance between two words in the dependency tree. For instance, as a running example, in the sentence ``All warranties honored by HP  are disappointing"", the opinion word ``disappointing"" is sequentially far from its target word ``warranties"". However, in the dependency tree shown in Figure , the two words ``disappointing"" and ``warranties"" are directly connected to each other. The short distance between these two words could be helpful to infer the importance of the word ``disappointing"" for the target ``warranties"".  Word Connection: Dependency tree could provide better contextual information for each word via the connections of the word with its head and dependants, thus it helps to improve word representations. Thereby, dependency tree could benefit TOWE. For instance, in the running example, the head of ``reputable"" is ``company"" while the head of ``warranties"" is ``disappointing"". Therefore, it would be easier to infer that the opinion word ``disappointing"" is related to the target word ``warranties"" and ``reputable"" is irrelevant.   %Besides the difference between the rule-based and deep learning models for TOWE regarding the representation learning methods, the rule-based methods have exploited the syntactic structures  of the sentences to improve the performance for TOWE while  %In particular, the related tasks of TOWE involves target word extraction/aspect term exaction  , and opinion word extraction   . A key difference between OWE and TOWE is that the opinion words in OWE are general and do not need to tie to any target words in the sentence while TOWE explicitly   %Despite its potential benefits, TOWE has only been studied by a few works in the past, characterizing the early rule-based and lexicon-based approaches  and very recently deep learning models .   %In the literature, feature-based models and deep learning model has been proposed for both target word extraction  and opinion word extraction . While joint models predict both the opinion and the target words, they cannot pair up them, thus being unable to solve the task of TOWE. In the literature, only a few of works have studied the task of TOWE, including the early attempts with the rule-based and lexicon-based approaches  and the recent works with deep learning models for TOWE .       We propose a novel deep learning model for TOWE that seeks to incorporate the syntactic structures of the sentences into the model computation. Two types of syntactic information are introduced in this work, i.e., the syntax-based possibility scores for words  and the syntactic connections between the words . We also present a novel inductive bias to improve the model, leveraging the representation distinction between the words in TOWE. Comprehensive analysis is done to demonstrate the effectiveness of the proposed model over four datasets.   Our comprehensive analysis on the model architecture together with the extensive experiments on four benchmark datasets demonstrate the effectiveness of the proposed model.   More specifically, the syntactic structure is employed to infer the target-related importance of the words and it is incorporated into the model using the newly proposed architecture Ordered-Neuron LSTM . Moreover, we employed GCN to encode word connections in the dependency tree. To overcome the noisy or non-relevant connections in the dependency tree, we propose to learn a dense graph out of the dependency tree which is customized to the given target word. Finally, we introduce a syntax-based regularization to preserve target-related information in the model. Our comprehensive analysis on the model architecture together with the extensive experiments on four benchmark datasets show the effectiveness of the proposed model, establishing new the state-of-the-art results on all of datasets.    In this work, we study the target-oriented opinion word extraction problem , one of the sub-tasks of aspect-based sentiment analysis  . We propose a deep learning model to incorporate the syntactic structure into the model computations. More specifically, the syntactic structure is employed to infer the target-related importance of the words and it is incorporated into the model using the newly proposed architecture Ordered-Neuron LSTM . Moreover, we employed GCN to encode word connections in the dependency tree. To overcome the noisy or non-relevant connections in the dependency tree, we propose to learn a dense graph out of the dependency tree which is customized to the given target word. Finally, we introduce a syntax-based regularization to preserve target-related information in the model. Our comprehensive analysis on the model architecture together with the extensive experiments on four benchmark datasets show the effectiveness of the proposed model, establishing new the state-of-the-art results on all of datasets.  
"," Targeted opinion word extraction  is a sub-task of aspect based sentiment analysis  which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.  %Deep learning models have been shown to achieve the state-of-the-art performance for TOWE in the recent studies.  %While previous feature-based models have shown syntactical structure  is useful for this task, recent deep neural nets ignore this information in their model. To address this limitation, in this paper, we propose a new approach which incorporates syntactical structure  into deep neural nets. More specifically, our model employs the dependency tree to capture the relative importance of the words to the aspect-term and to encode the connections between words. Our extensive experiments on four benchmark datasets prove the superiority of the proposed model, leading to new state-of-the-art results on all datasets. Moreover, detailed analysis shows the effectiveness of the components of the proposed model.",261
"  Aspect-based Sentiment Analysis  is a fine-grained version of sentiment analysis  that aims to find the sentiment polarity of the input sentences toward a given aspect. We focus on the term-based aspects for ABSA where the aspects correspond to some terms  in the input sentence. For instance, an ABSA system should be able to return the negative sentiment for input sentence ``The staff were very polite, but the quality of the food was terrible.'' assuming ``{, the author has a positive sentiment toward aspect-category service and negative sentiment toward aspect-term food. In this paper, we introduce a novel model for sentiment analysis toward aspect-term.  %The early attempts for ABSA have performed feature engineering to produce useful features for the statistical models  for this problem . One limitation of these feature-based models is that they require significant human effort and linguistic background to design effective features. In order to overcome this limitation,  %The typical network architectures for ABSA in the literature involve convolutional neural networks  , recurrent neural networks  , memory networks , attention  and gating mechanisms .  %automatically induce effective features for ABSA and  Due to its important applications , ABSA has been studied extensively in the literature. In these studies, deep learning has been employed to produce the state-of-the-art performance for this problem . Recently, in order to further improve the performance, the syntactic dependency trees have been integrated into the deep learning models  for ABSA . Among others, dependency trees help to directly link the aspect term to the syntactically related words in the sentence, thus facilitating the graph convolutional neural networks   to enrich the representation vectors for the aspect terms.  %Although the graph-based models have achieved decent performance for ABSA, these models have   However, there are at least two major issues in these graph-based models that should be addressed to boost the performance. First, the representation vectors for the words in different layers of the current graph-based models for ABSA are not customized for the aspect terms. This might lead to suboptimal representation vectors where the irrelevant information for ABSA might be retained and affect the model's performance. Ideally, we expect that the representation vectors in the deep learning models for ABSA should mainly involve the related information for the aspect terms, the most important words in the sentences. Consequently, in this work, we propose to regulate the hidden vectors of the graph-based models for ABSA using the information from the aspect terms, thereby filtering the irrelevant information for the terms and customizing the representation vectors for ABSA. In particular, we compute a gate vector for each layer of the graph-based model for ABSA leveraging the representation vectors of the aspect terms. This layer-wise gate vector would be then applied over the hidden vectors of the current layer to produce customized hidden vectors for ABSA. In addition, we propose a novel mechanism to explicitly increase the contextual distinction among the gates to further improve the representation vectors.  %as the hidden vectors at different layers of the graph-based models tend to capture different levels of contextual information, the gate vectors for the different layers should also maintain some level of contextual distinction. To this end, we propose a novel mechanism to explicitly increase the contextual distinction among the gate vectors to further improve the quality of the representation vectors.  The second limitation of the current graph-based deep learning models is the failure to explicitly exploit the overall importance of the words in the sentences that can be estimated from the dependency trees for the ABSA problem. In particular, a motivation of the graph-based models for ABSA is that the neighbor words of the aspect terms in the dependency trees would be more important for the sentiment of the terms than the other words in the sentence. The current graph-based models would then just focus on those syntactic neighbor words to induce the representations for the aspect terms. However, based on this idea of important words, we can also assign a score for each word in the sentences that explicitly quantify its importance/contribution for the sentiment prediction of the aspect terms. In this work, we hypothesize that these overall importance scores from the dependency trees might also provide useful knowledge to improve the representation vectors of the graph-based models for ABSA. Consequently, we propose to inject the knowledge from these syntax-based importance scores into the graph-based models for ABSA via the consistency with the model-based importance scores. In particular, using the representation vectors from the graph-based models, we compute a second score for each word in the sentences to reflect the model's perspective on the importance of the word for the sentiment of the aspect terms. The syntax-based importance scores are then employed to supervise the model-based importance scores, serving as a method to introduce the syntactic information into the model. In order to compute the model-based importance scores, we exploit the intuition that a word would be more important for ABSA if it is more similar the overall representation vector to predict the sentiment for the sentence in the final step of the model. In the experiments, we demonstrate the effectiveness of the proposed model with the state-of-the-art performance on three benchmark datasets for ABSA. In summary, our contributions include:   %, we propose to obtain another important score for each word in the sentence based on the representation vectors from the models. These model-based importance scores are then   %In particular, for ABSA, some words might introduce more useful information to predict the sentiment of the aspect terms than the the other words in the sentence   %In particular, some words in a given sentence might involve more useful information for relation prediction in RE than the other words, and the dependency tree for this sentence can help to better identify those important words and assign higher importance scores for them . We expect that introducing such importance information for the words in the deep learning models might lead to improved performance for RE. Consequently, in this work, we propose to obtain an importance score for each word in the sentences from the dependency trees . These will serve as the general tree representation to incorporate the syntactic information into the deep learning models for RE.  %In particular, as the aspect terms are the most important words in the sentences for ABSA,     %Syntactical structure, e.g., dependency tree, could shorten the distance between syntactically related words thus improve the contextualized representation of the words. In order to incorporate the syntactical tree into the deep models, recent work mainly employs the graph convolutional network   to model the interaction between words based on the syntactic tree. In order to emphasize on the given aspect term, current models use the representation of the aspect term generated by GCN either directly for final classification or as a gate to filter out features of a sequential model. However, these methods cannot benefit from the information of the given aspect term to control the information flow in the graph based model. Moreover, it is expected that the words that are syntactically related to the given aspect term should convey more information about the sentiment toward it. Unfortunately, non of the existing work considers this relative importance in the final representation of the sentence. In order to address these issue, we propose a new graph based model which employs the semantic of the given aspect term to control interaction between nodes/words in the GCN model and to emphasize more on the syntactically important words in the final representation of the sentence.  %Due to the application of ABSA in other downstream applications, e.g., opinion mining, it has gained a lot of attention in natural language processing community and several methods have been proposed for this task. Early attempts employed feature engineering to extract useful features for statistical models like SVM . These methods require extensive human effort and strong linguistic knowledge. They also suffer from low generalization ability. Due to these limitations, neural networks and deep models have superseded feature based models and obtain promising results in ABSA . Early deep models for ABSA have exploited sequential models  , convolutional neural nets  or even memory networks . In order to improve the performance, attention  and gating mechanism  has also been widely adopted in deep models. Recently, it has been shown that syntactical information could also improve the performance of deep models . Syntactical structure, e.g., dependency tree, could shorten the distance between syntactically related words thus improve the contextualized representation of the words. In order to incorporate the syntactical tree into the deep models, recent work mainly employs the graph convolutional network   to model the interaction between words based on the syntactic tree. In order to emphasize on the given aspect term, current models use the representation of the aspect term generated by GCN either directly for final classification or as a gate to filter out features of a sequential model. However, these methods cannot benefit from the information of the given aspect term to control the information flow in the graph based model. Moreover, it is expected that the words that are syntactically related to the given aspect term should convey more information about the sentiment toward it. Unfortunately, non of the existing work considers this relative importance in the final representation of the sentence. In order to address these issue, we propose a new graph based model which employs the semantic of the given aspect term to control interaction between nodes/words in the GCN model and to emphasize more on the syntactically important words in the final representation of the sentence.   %In particular, in this paper, we propose a novel model which employs the representation of the given aspect term to compute a gate. This gate is applied over the output of one layer of GCN. By doing so, the information represented in the aspect term would erase non-relevant information in each node/word obtained by its interaction with its neighbors in one aggregation step in GCN. As different layers of GCN capture different substructure of the graph, e.g., 1-hop vicinity vs 2-hop vicinity, we propose to exploit different gates in different layers. To ensure the gates in different layers are not the same, we propose a novel method to encourage diversity among gates in different layers of the GCN. Moreover, in addition to exploiting the semantic of the aspect term to control interactions between nodes/words in the GCN, in this paper, we propose to encourage the model to emphasize on the words that are syntactically important to the aspect term. In particular, we use the distance between a word to the aspect term in the dependency tree as an indication of the syntactic importance of the word to the aspect term. This importance is employed as supervision signal to encourage the model to emphasize on the words that are syntactically important to the aspect term. This is obtained in the final layer of the model when the sentiment prediction is performed. More specifically, we first estimate the semantic importance of each word by employing the final representation of the word as the input to a classifier to predict the label distribution and then we compute the KL-Divergence between this label distribution predicted by the word representation and the label distribution predicted by the sentence representation. If the two label distribution are more similar, it shows that the word representation contains most of the information that the model consumes to perform the final classification. Finally, in order to ensure those words which are syntactically important to the aspect term are semantically important in the model too, we decrease the divergence between distribution of the syntactic score and semantic score for each word via KL-Divergence between these two distributions.  %Our extensive experiments on three benchmark datasets, empirically prove the effectiveness of the proposed model leading to new state-of-the-art results in all three benchmark datasets.      A novel method to regulate the GCN-based representation vectors of the words using the given aspect term for ABSA.   A novel method to encourage the consistency between the syntax-based and model-based importance scores of the words based on the given aspect term.   Extensive experiments on three benchmark datasets for ABSA, resulting in new state-of-the-art performance for all the datasets.      ABSA is one of the important sub-tasks of sentiment analysis. Recently, it has been shown that incorporating syntactical structure into the deep learning models could improve performance for ABSA. In this paper,   We introduce a new model for ABSA that addresses two limitations of the prior work. It employs the given aspect terms to customize the hidden vectors. It also benefits from the overall dependency-based importance scores of the words. Our extensive experiments on three benchmark datasets empirically demonstrate the effectiveness of the proposed approach, leading to state-of-the-art results on these datasets. The future work involves applying the proposed model to the related tasks for ABSA, e.g., event detection .   , leading to the state-of-the-art results on these datasets   In the future, we plan to apply the proposed models to the related tasks for ABSA.   Aspect based sentiment analysis  is one of the important sub-tasks of sentiment analysis. Recently, it has been shown that incorporating syntactical structure into deep models could improve performance in ABSA. In this paper, we introduce a new method to incorporate the syntactic structure into the deep models by addressing two limitations of the prior work. Specifically, our model employs the given aspect term to control information flow in the graph based model. It also ensures that those words which are syntactically important to the given aspect are more pronounced in the final representation of the given sentence. Our extensive experiments on three benchmark datasets empirically prove the effectiveness of the proposed approach leading to new state-of-the-art results on all three benchmark datasets.  
"," Aspect-based Sentiment Analysis  seeks to predict the sentiment polarity of a sentence toward a specific aspect. Recently, it has been shown that dependency trees can be integrated into deep learning models to produce the state-of-the-art performance for ABSA. However, these models tend to compute the hidden/representation vectors without considering the aspect terms and fail to benefit from the overall contextual importance scores of the words that can be obtained from the dependency tree for ABSA. In this work, we propose a novel graph-based deep learning model to overcome these two issues of the prior work on ABSA. In our model, gate vectors are generated from the representation vectors of the aspect terms to customize the hidden vectors of the graph-based models toward the aspect terms. In addition, we propose a mechanism to obtain the importance scores for each word in the sentences based on the dependency trees that are then injected into the model to improve the representation vectors for ABSA. The proposed model achieves the state-of-the-art performance on three benchmark datasets.  %These models employ graph based neural nets to incorporate syntactical structure into the model. However, they ignore the aspect term information to control the interaction between words in the syntax tree which is modeled by graph neural net.  % Moreover, they neglect the consistency between the syntactic and semantic importance of the words toward the given aspect. %Moreover, the relative importance of the words to the given aspect term based on their syntactical role is neglected in the final representation produced by the existing syntax-aware models. To address these two issues, in this paper, we introduce a new syntax-aware model which incorporates gating mechanism to control information flow in the graph based model using the given aspect term. It also ensures the words that are syntactically important to the aspect term are more pronounced in the final representation of the sentence. Our extensive experiments on three benchmark datasets empirically prove the effectiveness of the proposed model leading to new state-of-the-art results on all three benchmark datasets.",262
"  %  Event Extraction  is an important task of Information Extraction that aims to recognize events and their arguments in text. In the literature, EE is often divided into two sub-tasks:  Event Detection  to detect the event trigger words, and  Event Argument Extraction  to identity the event arguments and their roles for the given event triggers. In recent years, ED has been studied extensively with deep learning while EAE is relatively less explored . As EAE is necessary to accomplish EE and helpful for many downstream applications , further studies are required to improve the performance of EAE. This work focuses on EAE to meet this requirement for EE.  %We follow the definitions for EE, ED and EAE in the popular ACE 2005 dataset that considers event arguments as some entity mentions in the input sentences.  %For instance, given the trigger word ``{ in the sentence ``{'' as an argument of role {'' with the trigger word ``{) and the entity mention ``{'' is the most important word to reveal the {'' for the event triggered by ``{'' and the two words ``{'' can be leveraged to encourage the deep learning models to focus more on the context word ``{'' and ``{'' and ``{'' with the trigger word ``{) and the entity mention ``{'' is the most important word to reveal the {'' for the event triggered by ``{'' and the two words ``{'' can be leveraged to encourage the deep learning models to focus more on the context word ``{'' and ``{'' and ``{}, the Hanif Bashir's son-in-law, while US officials confirmed all Bashir's family members were \underline{killed} last week.}  In this sentence, an EAE system should be able to realize the entity mention ``{ of the {''. As ``{'' are far away from each other in the sentence as well as its dependency tree, the EAE models might find it challenging to make the correct prediction in this case. In order for the models to be successful in this case, our intuition is that the models should first rely on the direct connections between ``{'' in the dependency tree to capture the role of ``{''. Afterward, the models can rely on the close semantic similarity between ``{'' to further connect ``{'' so the role information of ``{''. Finally, the direct apposition relation between ``{'' can be exploited to connect ``{'' to obtain the necessary representations to perform argument prediction for ``{'' to its important context word ``{'' to the entity ``{'' and ``{'' to ``{'' via the apposition relation in the dependency tree. In this way, the context word ``{'' to improve the argument prediction. In order to obtain the semantic structures for the sentences, we will employ the similarities between the representation vectors for the words in this work.    %We note that the previous work on EAE has only considered the syntactic structures  and this is the first work to investigate the semantic structures for EAE.   %Event Extraction  is the task of identifying words or phrases that evoke an event , classify their event type, find the entities that play a role in an event  and classify the role of the event arguments.  This task is mainly studied under two sub-tasks:  Event Detection  which aims to find the event triggers and classify them,  Event Argument Extraction  which aims to identify event arguments and their role for a given event. In recent years, ED has been studied extensively while EAE is relatively less explored . As EAE, is necessary to accomplish EE and it could be useful in many downstream applications , further studies is required to improve the performance of EAE.   %Early attempts employed feature-based models for the task of EE . Some of the feature-based models have exploited the structure of the sentence . As the feature-based models requires great deal of linguistic knowledge and could not generalize well to new datasets and domains, they have been superseded with deep neural networks. These networks represent the words by dense vectors  which facilities incorporating knowledge from external sources via pre-training and also learning effective representations using deep architectures such as convolutional neural network  or recurrent neural network  . Also, dependency tree has been shown to be useful for deep neural networks for EE .  %Structure of the sentence  has been employed by both feature-based and deep models for the task of EE and its sub-tasks including EAE. The connections between words in the syntactic structure provide richer information about the word and could help the model mitigate the problem of long dependencies. For instance, in the sentence ``In Iraq, a reporter who had been lost for three months is found dead close to Nasr hotel"" the connection between the entity ``reporter"" and the event trigger ``dead"" in the dependency tree could be useful to ignore the irrelevant words between these two words in the sequential order of the words in the sentence. While the dependency tree has been explored in the previous work, existing approaches, especially deep models, exploit the structure of the sentence in a limited manner. For instance, Sha et al.  incorporates the connections between the words in the dependency tree into recurrent neural network . However, in other natural language processing  tasks, it has been shown that dependency tree could be incorporated more efficiently into the deep model using Graph Convolutional Neural Network   and its variants. In this paper, we propose to employ Graph Neural networks for the task of EAE. Moreover, we argue that, although syntactic structure is useful but the semantic connections between words is also important. For instance, in the sentence ``Iraqi Press constantly report interviews with Hussain Molem, the Hanif Bashir's son-in-law who was serving in the Iraq air force, while US officials confirmed all Bashir's family members have been killed in the last week attack."" the semantic connection between the entities ``Hussain Molem"" and ``Hanif Bashir"" and also the event mentioned for the entity ``Bashir's family members"" is useful to infer the role of the entity ``Hussain Molem"" is ``victim"" for the event trigger ``killed"". So, in order to capture the semantic connections in addition to the syntactic relations between words, in this paper, we propose to apply GCN over a combined view of the syntactic and semantic structure of the sentence.  %GCNs facilitate the introduction of the sentence structures into deep learning models and have been demonstrated to be helpful for many other natural language processing  tasks.  %How can we combine the syntactic and semantic structures of the sentences to perform EAE? A simple method is to use the structures separately in the Graph Convolutional Neural Networks   whose outputs are concatenated to obtain the representation vectors for the words in the sentence. However, this method is likely suboptimal as an individual structure  might not be able to identify the important context words for the representation vector of some current word in EAE by itself. It is in fact necessary to consult both syntactic and semantic structures to capture the effective context words for the representation vectors in these cases. An example for this problem is illustrated in our previous example where both the syntactic and semantic structures help to connect ``{'' for EAE.  How should we combine the syntactic and semantic structures to aid the learning of effective representations for EAE? In this work, we propose to employ Graph Transformer Networks   to perform the syntax-semantic merging for EAE. GTNs facilitate the combination of multiple input structures via two steps. The first step obtains the weighted sums of the input structures, serving as the intermediate structures that are able to capture the information from different input perspectives . In the second step, the intermediate structures are multiplied to generate the final structures whose goal is to leverage the multi-hop paths/connections between a pair of nodes/words  to compute the importance score for the final structures. As the multi-hop paths with heterogeneous types of connections along the way  has been illustrated to be helpful in our running example , we expect that GTNs can help to combine the syntactic and semantic structures to produce effective representations for EAE.   %Consequently, in this work, we propose to combine the syntactic and semantic structures earlier in the process to learn effective mixed structures for the sentences over which the GCN models can be applied to produce better representations for EAE. In particular, we propose to employ Graph Transformer Networks   that learn to mix multiple input structures for GCNs via the weighted sums of the individual structures . These intermediate structures are then multiplied with each other to generate the final mixed views that are able to model the word connections with heterogeneous types and different lengths. GTNs have been shown to produce effective node embeddings for the graphs with heterogeneous edge types and we expect them to be helpful to combine the syntactic and semantic structures for EAE as well. To our knowledge, this is the first work that considers GTNs with syntactic and semantic structures for NLP in general and for EAE and EE in particular.   %How can we exploit the syntactic and semantic structures of the sentences to perform EAE? In this work, we follow the common approach to use such sentence structures in deep learning in which the sentence structures are considered as the adjacency matrices in Graph Convolutional Neural Networks   to obtain the abstract representation vectors for the words in the sentences. A simple method to apply GCNs on both the syntactic and semantic structures is to run GCNs with the individual structures whose outputs are concatenated in the end to predict arguments . However, this method is likely suboptimal as it is sometime challenging to identify the important context words for the representation vector for a word in EAE based on only one type of sentence structures . It is in fact necessary to consult both structure types to come up with the effective context words for the representation vectors in these cases. An example for this problem is illustrated in our previous example where both the syntactic and semantic structures help to connect ``{'' for EAE. Consequently, in this work, we propose to combine the syntactic and semantic structures earlier in the process to learn effective mixed structures for the sentences over which the GCN models can be applied to produce better representations for EAE. In particular, we propose to employ Graph Transformer Networks   that learn to mix multiple input structures for GCNs via the weighted sums of the individual structures . These intermediate structures are then multiplied with each other to generate the final mixed views that are able to model the word connections with heterogeneous types and different lengths. GTNs have been shown to produce effective node embeddings for the graphs with heterogeneous edge types and we expect them to be helpful to combine the syntactic and semantic structures for EAE as well. To our knowledge, this is the first work that considers GTNs with syntactic and semantic structures for NLP in general and for EAE and EE in particular.  %In order to further boost the performance for EAE, we propose two novel inductive biases for GTNs in this work. The first bias attempts to explicitly promote the diversity among the intermediate mixed structures for GTNs to improve the representations for EAE. The rationale is to encourage the intermediate structures to capture different aspects of the mixed structures for syntax and semantics, thus avoiding the degenerate solution with similar learned structures in the intermediate views and facilitating the emergence of the effective structures for EAE. The second bias, on the other hand, aims to improve the generalization of GTNs using the Information Bottleneck idea . In particular, the use of the combined structures from syntax and semantics might augment GTNs with high capacity to encode the detailed information in the input sentences. Coupled with the generally small training dataset for EAE, the GTN models could learn to preserve all the context information in the input sentences, including the irrelevant information for EAE. This likely leads to the overfitting of GTNs on the training data and affect the overall performance for EAE. In order to overcome this issue, we propose to treat the GTN models in this work as an information bottleneck in which the produced representations of GTNs are trained to not only achieve good prediction performance for EAE but also minimize the mutual information with the input sentences . To this end, we introduce the mutual information between the generated representations of GTNs and the input sentences as an additional term in the overall loss function to improve the generalization of GTNs for EAE. Our extensive experiments on two benchmark datasets for EAE demonstrate the effectiveness of the proposed model, yielding the state-of-the-art performance for EAE in this work.   Finally, in order to further boost the performance for EAE, we propose a novel inductive bias for the proposed GTN model, aiming to improve the generalization of GTNs using the Information Bottleneck idea . In particular, the use of the rich combined structures from syntax and semantics might augment GTNs with high capacity to encode the detailed information in the input sentences. Coupled with the generally small training datasets for EAE, the GTN models could learn to preserve all the context information in the input sentences, including the irrelevant information for EAE. This likely leads to the overfitting of GTNs on the training data. In order to overcome this issue, we propose to treat the GTN model in this work as an information bottleneck in which the produced representations of GTNs are trained to not only achieve good prediction performance for EAE but also minimize the mutual information with the input sentences . To this end, we introduce the mutual information between the generated representations of GTNs and the input sentences as an additional term in the overall loss function to improve the generalization of GTNs for EAE. Our extensive experiments on two benchmark datasets for EAE show that the proposed model can achieve the state-of-the-art performance for EAE.   %GCNs compute the representation vector for a word in a layer using the vectors for the other words from the previous layer, weighted by   %How can we efficiently combine the semantic and the syntactic structure of the sentence? One possible solution is to apply different GCNs on each view and then combine the output the GCNs . However, this solution cannot model the early interaction between two views. In order to effectively model the interactions between two views, in this paper, we introduce to employ the newly proposed architecture Graph Transformer Network  . Graph Transformer  computes a weighted sum of the input views  to construct an intermediate combined view. By constructing several intermediate views and computing the multiplication of these intermediate views, a final combined view is built that models heterogeneous paths of different lengths between the nodes . Stacking several layers of GT, named as GTN, will model different kinds of modeling the interactions between input views. To the best of our knowledge, we are the first to exploit GTN for the task of EAE.  %While GTN could efficiently model the interactions between syntactic and semantic views, it has some limitations that prevent this model from further improvement. First, in the original GTN proposed by Yun et al.  there is no constraint to enforce the intermediate views and the views in different layers of GTN to be different. This limitation restrict GTN capability to encode different types of interactions between nodes . Thus adding more intermediate views or layers of GT would not be helpful. In order to mitigate this problem, in this paper, we introduce a new inductive bias to encourage the diversity among different intermediate views in different layers of GTN. Second, as the original GTN will compute several representations for the interactions between words, it is prone to overfitting. In other words, GTN could capture different types of connections between words. Thus, it is prone to learn some patterns of connections that are specific to the training examples. In order to prevent this, we should regularize the representations in GTN to learn only those types of connections that are relevant to the task and reduce excess information which could represent nuisances in the training examples. To this end, we propose to treat GTN as an Information Bottleneck in our model. Information Bottleneck  is a technique which ensures the representations of the input in the bottleneck has small mutual information with the early layers in the model but has high mutual information with the outputs . Concretely, in our model we decrease the mutual information between the representations of the words generated by GTN and the representations of the words in the early layers in the model, while we increase the mutual information between GTN representations and the labels using the common training loss .   %Our extensive experiments on two benchmark dataset show the effectiveness of the proposed approach, leading to the state-of-the-art results on both datasets. Moreover, our detailed analysis on the model supports the usefulness of our modifications on the original GTN.      We propose a novel deep learning model for the problem of event argument extraction that exploits the syntactic and semantic structures of the input sentences to learn effective representation vectors. The proposed model features the effective combination of the semantic and syntactic structures to encapsulate the multi-hop paths between the words for the importance score computation of the structures via Graph Transformer Networks. We also propose two novel inductive biases to improve the performance for EAE based on the structure diversity and information bottleneck techniques. The extensive experiments on two popular datasets for EAE demonstrate the effectiveness of the proposed model. In the future, we plan to apply the proposed model to other tasks in information extraction such as relation extraction and the similar tasks.  We propose a novel deep learning model for EAE that combines the syntactic and semantic structures of the input sentences for effective representation learning. The proposed model introduces Graph Transformer Networks and Graph Convolutional Networks to effectively perform the structure combination. A novel inductive bias is presented to improve the model generalization based on information bottleneck. The extensive experiments demonstrate the effectiveness of the proposed model. In the future, we plan to apply the proposed model to other related tasks in information extraction such as relation extraction.   Event argument extraction  is one of the important sub-tasks of event extraction  which has been relatively less studied than the other sub-tasks of EE. In this work, we proposed a new deep learning model capable of utilizing different structure of the sentence to find the role of an argument in a given event. Our model enjoys the new proposed architecture graph transfer network  and further addresses some of the limitations of the original architecture to efficiently combine different structural views of the sentence. Moreover, we proposed a new inductive bias in our model based on the information bottleneck principle to avoid overfitting. Our extensive experiments and analysis prove the effectiveness of the proposed model and establish new state-of-the-art results on two benchmark datasets for EAE. In future, we will explore this model for the other tasks of event extraction including event factuality.  
"," The goal of Event Argument Extraction  is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for EAE. Consequently, in this work, we propose a novel model for EAE that exploits both syntactic and semantic structures of the sentences with the Graph Transformer Networks  to learn more effective sentence structures for EAE. In addition, we introduce a novel inductive bias based on information bottleneck to improve generalization of the EAE models. Extensive experiments are performed to demonstrate the benefits of the proposed model, leading to state-of-the-art performance for EAE on standard datasets.  %. GTNs offer an mechanism to combine syntactic and semantic structures and infer more effective sentence structures for EAE.  %and achieve the state-of-the-art performance on standard datasets.  %We achieve the state-of-the-art performance for EAE on standard datasets.  %Our extensive experiments demonstrate the benefits of the proposed model and achieve the state-of-the-art performance on standard datasets.  %However, they fail to effectively model the entire graph structure and also they ignore the semantic relations between words. In order to address these issues, in this paper, we propose to employ the new architecture of Graph Transformer Network  to effectively employ the syntactic and semantic structure of the sentence and model the interaction between them. Moreover, we introduce two new inductive bias in our model to rectify the inherent limitations of the original GTN architecture. Our extensive experiments on two benchmark datasets prove the effectiveness of the proposed model for the task of EAE, leading to new state-of-the-art results on both datasets.",263
"  Curriculum learning trains a model by using easy examples first and gradually adding more difficult examples. It can speed up learning and improve generalization in supervised learning models .  %With curriculum learning, models are trained according to a curriculum that sorts training examples according to difficulty. %The model is first trained with only the easiest examples. %More difficult examples are gradually added according to some pre-determined schedule. %Training with curriculum can lead to faster model convergence than a baseline model trained without a curriculum . %With machine learning models and data sets continuing to grow, and knowing the impact of model training on the environment, there is a need for efficient model training . %\Hong{I donot understand why do you need to include the above sentence.  What is the gain?} %Hong.  In Bengio et al's ""curribulum learning"" work, they found that CL has an effect on the speed of convergence and better optimization of non-convex functions. You may want to evaluate in your work, in addition to the performance. A major drawback of existing curriculum learning techniques is that they rely on heuristics to measure the difficulty of data, and either ignore the competency of the model during training or rely on heuristics there as well. For example, sentence length is often used as a proxy for difficulty in NLP tasks . %Similarly, the number of objects in an image has been used as a proxy for difficulty in an image recognition task . Such heuristics can be useful but have limitations. First, the heuristic chosen may not actually be a proxy for difficulty. Depending on the task, long sequences could signal easier or harder examples, or have no signal for difficulty. Second, a model's notion of difficulty may not align with the heuristic imposed by a human developing the model. It may be that examples that appear difficult for the human are in fact easy for the model to learn.  Competency was recently introduced as a mechanism to determine when new examples should be added to the training data . %However, in that work the competency schedule was ad hoc, and did not actually look at the competency of the model but assumed a schedule according to learning heuristics.  However, in that work competency is a monotonically increasing function of a pre-determined initial value. %competency, . Once set, competency is not evaluated during training. Ideally, model competency should be measured at each training epoch, so that the training data could be appropriately matched with the model at a given point in the training. If a model is improving, then more difficult training data can be added at the next epoch.  But if performance declines, then those difficult examples can be removed, and a smaller, easier training set can be used in the next epoch.  In this study, we propose to estimate both the difficulty of examples and the ability of deep learning models as latent variables based on model performance using Item Response Theory , a well-studied methodology in psychometrics for test set construction and subject evaluation . IRT models estimate latent parameters such as difficulty for examples  %Hong: I changed ""examples"" to ""samples"" in earlier context, but if you don't like it, just change them back.  Just stick to one use throughout the paper.   and a latent ability parameter for individuals . %Hong.  Here you may want to use ""model"" ability instead of ""subject"" IRT models are learned by administering a test to a large number of subjects, collecting and grading their responses, and using the subject-response matrix to estimate the latent traits of the data. These learned parameters can be used to estimate the ability of future subjects, based on their graded responses to the examples. %Hong.  Similarly, would you introduce ""model"", not ""test-takers""  IRT has not seen wide adoption in the machine learning community, primarily due to the fact that fitting IRT models requires a large amount of human annotated data for each example. However, recent work has shown that IRT models can be fit using machine-generated data instead of human-generated data as input .  Because IRT learns example difficulty and subject ability together,  %it is an interesting framework to consider for the problem of curriculum learning.  in this work we propose replacing heuristics for learned parameters in curriculum learning. First, we experiment with replacing a typical difficulty heuristic  with learned difficulty parameters. Second, we propose \modelname~, a novel curriculum learning framework that uses the estimated ability of a model during the training process to dynamically identify appropriate training data. At each training epoch, the latent ability of the model is estimated using output labels generated at the current epoch. Once ability is known, only training data that the model has a reasonable chance of labeling correctly is included in training. As the model improves, the estimated ability will improve, and more training examples will be added.  To the best of our knowledge, this is the first work to learn a model competency during training that is directly comparable to the difficulty of the examples. %Hong. You may want to define terminology up front. Here you say ""training data pool"".  Earlier you say ""examples"".  Perhaps use ""items"", and then define it and how it is used in your paper %To be explicit, in this work our goal is to  Our study will test the following three hypotheses: H1: Using learned latent difficulties instead of difficulty heuristics leads to better held-out test set performance for models trained using curriculum learning, H2: A dynamic data selection curriculum learning strategy that probes model ability during training leads to better held-out test set performance than a static curriculum learning strategy that does not take model ability into account, H3: Dynamic curriculum learning is more efficient than static curriculum learning, leading to faster convergence. We test our hypotheses on the GLUE classification data sets .  Our contributions are as follows:  we demonstrate that for curriculum learning, learned difficulty outperforms traditional difficulty heuristics,   we introduce a novel curriculum learning framework which automatically selects training data based on the estimated ability of the model, and  we show that training using \modelabbr~leads to better performance than both traditional curriculum learning methods and a fully supervised competitive baseline.  Our findings support the overall curriculum learning framework, and demonstrate that learning difficulty and ability lead to further performance improvements beyond common heuristics.\footnote{Code implementing our experiments and learned difficulty parameters for the GLUE data sets are available at \url{https://jplalor.github.io/irt}.} %We will release the learned difficulty parameters for the GLUE data sets as a resource for the community to further explore learned difficulties and dynamic curriculum learning.}%\footnote{Code and data used for this work, including learned difficulty values, will be released upon publication.}      This work validates and supports the existing literature on curriculum learning. Our results confirm that curriculum learning methods for supervised learning can lead to faster convergence or better local minima, as measured by test set performance . We have shown that by replacing a heuristic for difficulty with a theoretically-based, learned difficulty value for training examples, static curriculum learning strategies can be improved. We have also proposed \modelabbr, the first curriculum learning method to dynamically probe a model during training to estimate model ability at a point in time. Knowing the model's ability allows for data to be selected for training that is appropriate for the model and is not rigidly tied to a heuristic schedule. \modelabbr~trains more effective models in most cases that we considered, particularly for randomly initialized LSTM models.  Based on our experiments, we report mixed results on our stated hypotheses. Replacing heuristics with learned difficulty values leads to improved performance when training models with curriculum learning, supporting H1. \modelabbr~does outperform other training setups when used to train LSTM models. Results are mixed when used to fine-tune the  model. Therefore H2 is partially supported. We see similarly mixed results when evaluating efficiency. With  fine-tuning, fully supervised fine-tuning is usually the most efficient, as the number of fine-tuning epochs needed is already very low. For LSTM, \modelabbr~is more efficient than the other curriculum learning strategies, and is the most efficient overall for two of the six tasks. H3 is partially supported by these results.   There are limitations to this work that can inform future work.  Strategies for learning difficulties from artificial crowds and identifying the necessary number of examples to estimate ability need more study to identify optimal strategies.  For example, recent work has shown that 85\  accuracy is an optimal learning rate to target .  \modelabbr~can be used to dynamically select data to maintain a learning rate of 85\ .    Even though it is dynamic, \modelabbr~employs a simple schedule: only include examples where . However, being able to estimate ability on the fly with \modelabbr~opens up an exciting new research direction: what is the best way to build a curriculum, knowing example difficulty and model ability ?   Our results also showed that there was no ``best'' heuristic for the competency-based baselines across data sets.   In four data sets , using learned difficulty from IRT outperformed using the sentence length difficulty heuristic, which is a new result.   A key contribution of DDaCLAE is that selecting a rate schedule and difficulty heuristic is no longer needed.     Gathering response patterns to estimate difficulty is expensive, but it is a one-time cost.   Learned difficulty estimates can be shared with the data set as features.   Identifying the best way to generate artificial response patterns, including parallelization, is an open question.  By releasing the learned difficulty values for the GLUE data sets we hope to encourage their use both for non-heuristic curriculum learning but also for future IRT evaluations.   It may be the case that only data with difficulty within a range of ability  is better, and the training set shifts as the model improves.  There are many directions to for future work, and this will be an exciting area of work moving forward.    Re-add the below for camera ready 
","   Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. %  They also either ignore the ability of the model or rely on heuristics there as well.   %In this work we show that learning difficulty and ability is more effective for curriculum learning than applying heuristics.   In this work, we propose replacing difficulty heuristics with learned difficulty parameters.    We also propose \modelname~, a strategy that probes model ability at each training epoch to select the best training examples at that point.   %\modelabbr~adds data at a rate commensurate with the model's capability, in contrast to scheduled curricula that add data at a predetermined rate.   We show that models using learned difficulty and/or ability outperform heuristic-based curriculum learning models on the GLUE classification tasks.   %Using \modelabbr~to train LSTM models further improves performance.   %Experimental results demonstrate that \modelabbr~outperforms static CL strategies on a number of NLP classification tasks.",264
" In recent years, voice assistants have become ubiquitous in the household, performing tasks for users through a conversational interface. Given the informal nature of language in these settings, there are many ways in which the agent can misunderstand user commands, intent, and how to complete actions. A vital part of ensuring that a user continues to interact with an agent is the user's confidence in the agent's ability to correctly and smoothly respond to their requests. Relying on conversational rather than transactional dialogue is a core method of building this trust .   However, conversational dialogue is difficult to generate and can often lead to situations where the agent produces an utterance that the user is unable to properly respond to or that creates friction between the user and agent. We refer to these situations as dialogue breakdown, where the agent is prevented from completing the desired goal of the dialogue either by user exasperation or agent misunderstanding . Detecting when breakdown occurs is an essential part of ensuring its effects are mitigated, either by recovering when they occur or avoiding their creation altogether . As in other dialogue settings, gathering labelled data is difficult. Data collection must either rely on interrupting user interactions or paying a third-party to rate dialogue after its completion, both of which are often intrusive, expensive, and affected by user bias . In these settings, semi-supervised learning methods are a natural way to fully utilize the limited labelled data by leveraging the large amounts of unlabelled data that are commonly available.  In this paper, we investigate two semi-supervised learning methods to improve performance on dialogue breakdown detection.  We consider continued pre-training on the Reddit dataset  as an approximation of the dialogue domain we would like to eventually fine-tune on. We also consider self-supervised manifold based data augmentation  , a data augmentation method that utilizes our further pre-trained model to generate new training examples.   We evaluate these methods on the Dialogue Breakdown Detection Challenge   English shared task. We submit our final models to the 2020 DBDC5 shared task, placing first in the English track.  We beat baselines and other submissions by over 12\% accuracy, 0.135 F1 score, and 0.02 JS divergence. In experiments on data from 2019 , our baseline model improves over previous challenge winners by over 13\% .  The addition of our semi-supervised learning methods improves these baseline models further by 2\% accuracy,  0.02 F1 score, and 0.003 Jensen Shannon  Divergence. Although we specifically consider the task of dialogue breakdown detection, these semi-supervised techniques are applicable generally to any supervised dialogue task and provide a simple way to improve performance.     Building trust and confidence in agents with conversational interfaces requires smooth dialogue that avoids breakdown. Detecting dialogue breakdown, either before or while it happens, is an essential part in ensuring users have satisfactory  experiences with these agents. We investigate two semi-supervised learning methods to leverage unlabelled data to improve breakdown detection, including continued pre-training on the Reddit dataset and SSMBA data augmentation. We utilize these methods in our submission to the 5th Dialogue Breakdown Detection Challenge, beating other baselines and submissions by a large margin. In ablations on previous test sets, we show that the addition of our semi-supervised methods improves our baseline models by over 2\  accuracy and reduces JS divergence by over 0.003. These methods are simple and applicable to any dialogue task. In the future we will continue to investigate applying these methods for intent prediction, slot filling, state tracking and language generation.  
","   Building user trust in dialogue agents requires smooth and consistent dialogue exchanges. However, agents can easily lose conversational context and generate irrelevant utterances. These situations are called dialogue breakdown, where agent utterances prevent users from continuing the conversation. Building systems to detect dialogue breakdown allows agents to recover appropriately or avoid breakdown entirely. In this paper we investigate the use of semi-supervised learning methods to improve dialogue breakdown detection, including continued pre-training on the Reddit dataset and a manifold-based data augmentation method. We demonstrate the effectiveness of these methods on the Dialogue Breakdown Detection Challenge  English shared task. Our submissions to the 2020 DBDC5 shared task place first, beating baselines and other submissions by over 12\% accuracy. In ablations on DBDC4 data from 2019, our semi-supervised learning methods improve the performance of a baseline BERT model by 2\% accuracy. These methods are applicable generally to any dialogue task and provide a simple way to improve model performance.",265
"  Coreference resolution is the task of grouping mentions in a text that refer to the same real-world entity into clusters  . %The task is an important prerequisite to a variety of natural language processing systems, such as textual entailment and information extraction .  Coreference resolution is a difficult task  that %since it  requires reasoning, context understanding, and background knowledge of real-world entities,  and %Therefore, the task  has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English and used for the 2011 and 2012 {\CONLL} shared tasks . Since then,  there has been substantial research on English coreference, most recently using neural coreference approaches , leading to a significant increase in  %that significantly increased  the  performance of coreference resolvers for English. % % The  general objective of %the research described in  % this paper is to close a very evident gap in the recent literature in coreference. By contrast, there has been almost no research on Arabic coreference;  the performance for Arabic coreference resolution has not improved  much since the {\CONLL} 2012 shared task, and in particular no neural architectures have been proposed--the current state-of-the-art system remains  the model proposed in %feature-based  .  In this paper we close this very obvious gap by proposing what to our knowledge is the first neural coreference resolver for Arabic.  One explanation for this lack of research might simply be the lack of  training data large enough for the task.  Another explanation  might be that Arabic is  more problematic than English  %more complex than English  because of its rich morphology,  %rich proprieties,  its %has  many dialects,  and/or  its %contains a  high degree of ambiguity.  We explore the first of these possibilities.  %Our proposal does address some of these aspects.  %one aspect of the problem. % Another explanation might be the lack of large-size training data for the task.  % We explore the Coreference resolution can be further divided into two subtasks--mention detection and mention clustering--as illustrated in  %an example of these two steps in  Figure .   % % Coreference resolution is a difficult task  % that %since it  % requires reasoning, context understanding, and background knowledge of real-world entities,  % and % %Therefore, the task  % has driven research in both natural language processing and machine learning, particularly since the release of the ontonotes multilingual corpus providing annotated coreference data for Arabic, Chinese and English . % %and various approaches have been applied.  In  early work, coreference's two subtasks were usually carried out in a pipeline fashion , with candidate mentions selected prior the mention clustering step.  Since   introduced  an end-to-end neural coreference architecture  that achieved state of the art  by carrying out the two tasks jointly, as first proposed by , most state-of-the-art systems have followed this approach. % the first end-to-end coreference system that solves the two subtasks jointly.  % This leads to a number of subsequent systems  that significantly increased the coreference resolution performance on English.  % By contrast, there were little developments for Arabic coreference resolution, the performance for Arabic coreference resolution does not improve much since the CoNLL 2012 shared task the current state-of-the-art system remain feature-based .  However, no end-to-end solution was attempted for Arabic. We intend to explore whether an end-to-end solution would be practicable with a corpus of more limited size.  % One explanation for this might be that Arabic is more complex than English because of its morphologically rich proprieties, has many dialects, and contains a high degree of ambiguity.  % Another explanation might be the lack of large-size training data for the task.   The approach we followed to adapt %In this work, we introduce a recipe to show how  the state-of-the-art English coreference resolution architecture  to Arabic %can be adapted for the Arabic language is as follows. We started with a strong baseline system , enhanced  with contextual {\BERT} embeddings . We then explored three methods for improving the model's performance for  Arabic.  %In total we evaluated three options,  The first method is to pre-process  Arabic words with heuristic rules.  We follow  to normalize the letters with different forms, and removing all the diacritics. This results in a substantial improvement of 7 percentage points over our baseline.  The second route is to replace  multilingual {\BERT} with a {\BERT} model trained only on the Arabic texts  .  Multilingual {\BERT} is trained with 100+ languages; as a result,  it is not optimized for any of them. %needs to balance the tread of between languages.  As shown by , monolingual {\BERT}  trained only on the Arabic texts has better performance on various {\NLP} tasks.   We found the same holds for coreference: %resolution task, by  using embeddings from  monolingual {\BERT}, the model further improved the {\CONLL} F1 by 4.8 percentage points. Our third step is to leverage the end-to-end system with a separately trained mention detector .  We show that a better mention detection performance can be achieved by using a separately trained mention detector.  And by using a hybrid training strategy between the end-to-end and pipeline approaches  our system gains an additional 0.8 percentage points.  Our final system achieved a {\CONLL} F1 score of 63.9\%, which is is 15\% more than the previous state-of-the-art system  on Arabic coreference with the {\CONLL} dataset.  Overall, we show that the state-of-the-art English coreference model can be adapted to Arabic coreference  leading to a substantial improvement in performance when compared to previous feature-based systems.      In this paper, we modernize the Arabic coreference resolution task by adapting state-of-the-art English coreference system to the Arabic language. We start with a strong baseline system and introduce three methods  to effectively enhance the performance of the Arabic coreference resolution. Our final system enhanced by all three methods achieved a {\CONLL} F1 score of 63.9\  and improved the state-of-the-art result on Arabic coreference resolution task by more than 15 percentage points.      
","  No neural coreference resolver for Arabic exists, in fact we are not aware of any learning-based coreference resolver for Arabic since .  In this paper, we introduce a coreference resolution system for Arabic based on Lee et al's end-to-end architecture combined with the Arabic version of {\BERT} and an external mention detector.  As far as we know, this is the first neural  coreference resolution system aimed specifically to Arabic, and it substantially outperforms the existing state-of-the-art on  OntoNotes 5.0 with a gain of 15.2 points {\CONLL} F1.   We also discuss the current limitations of the task for Arabic and possible approaches that can tackle these challenges.  %\footnote{Our code is available at \url{https://github.com/juntaoy/aracoref}.}    % space normally used by the marker     This work is licensed under a Creative Commons     Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } \let\thefootnote\relax\footnotetext{* Equal contribution. Listed by alphabetical order.}",266
" Deep architectures for emotion recognition from speech is a growing research field .  Using short time signal analysis, a speech utterance is represented by a matrix  where  is the size of the time dimension and   is the size of spectral dimension. The sequence to sequence layers %  model the spectral phenomena and keep the size of the time dimension  without any modification.  A sequence to vector layer is used to convert the sequence to a fixed dimension vector which can be fed to feed forward dense layers. The global average pooling, global max pooling and attention are common choices for this type of layer. %Feed forward layers are then used to improve the modeling power of the classifier. Fully-connected dense layers are then used to apply nonlinear compression of the input features for better representation which improves the modeling power of the classifier. A multiclass classifier is implemented using a softmax layer. Typically, the model is trained using the cross-entropy objective function.  Convolutional neural networks  have been recently used in many emotion recognition tasks.  For example, CNNs designed for visual recognition  were directly adapted for emotion recognition from speech. Moreover, in a study by , they conducted extensive experiments using an attentive CNN with multi-view learning objective function using the Interactive Emotional Dyadic Motion Capture  database . They concluded that for a CNN architecture, the particular choice of features is not as important as the model architecture, or the amount and kind of training data. %CNNs are an example  for sequence to sequence layers and they are extremely fast in training and classification phases. CNNs are excellent in feature extraction and very fast in training compared to standard sequence modeling.  Long short-term memory networks   sequence to sequence layers  are excellent in capturing the sequential phenomena of the speech signal for various style of speaking. In a study by , they propose a solution to the problem of 閳ユontext-aware閳 emotional relevant feature extraction, by combining CNNs with LSTM networks, in order to automatically learn the best representation of the speech signal directly from the raw time representation. They did not use any of the commonly hand-engineered features, such as mel-Frequency cepstral coefficients  and perceptual linear prediction  coefficients. Their end-to-end system was targeted to learn an intermediate representation of the raw input signal automatically that better suits the task at hand and hence leads to better performance.  Both CNN and  LSTM networks have shown significant improvements over fully-connected neural network across a wide variety of tasks.  In recent work by  ,  they  took advantage of CNNs, LSTMs and DNNs by combining them into one unified architecture for speech recognition task. CNNs are good at reducing frequency variations, LSTMs are good at temporal modeling, and finally DNNs map the features into a more separable space. Their CLDNN provided a 4-6\% relative improvement in WER. In a similar work for emotion recognition from speech , the combination of CNNs and LSTMs led to improvements in the classification accuracy. The last state of the LSTM was used for sequence to vector conversion.   Recently, an  end-to-end multimodal emotion and gender recognition model with dynamic joint loss weights is developed  . The proposed model does not need any pre-trained features from audio and visual data. In addition, the system is  trained using a multitask objective function and its weights are assigned using a dynamic approach.     In this paper, we build on these contributions to develop an emotion recognition system for Arabic data using the recently introduced KSU emotions corpus\footnote{https://catalog.ldc.upenn.edu/LDC97S45}. Our contributions are:  Introducing a novel approach for emotion recognition by using an attention based CNN-LSTM-DNN architecture;  Studying a deep CNN models for the same task;  Comparing our results with published state-of-the art results on the IEMOCAP database and  Providing our scripts and code for  the research community for usage and potential future contributions\footnote{http://github.com/qcri/deepemotion}  %In this paper, we build on previous contributions to develop a system for the first time using attention based CNN-LSTM-DNN architecture for emotion recognition. In addition,  a second architecture based on deep CNN models only was developed. In this study, we will benchmark our results using the recently introduced KSU Emotions%\footnote{https://catalog.ldc.upenn.edu/LDC97S45}   , which comprised of approximately five hours of emotional Modern Standard Arabic  speech from 23 speakers, see section  for more details. The results on an Arabic speech emotion recognition task shows that the two approaches led to similar accuracy results but the deep CNN models are significantly faster than the attention based CNN-LSTM-DNN models in training and classification phases.    The rest of the paper is organized as follows: In section 2, we describe the attention-based CNN-LSTM-DNN  and the proposed deep CNN architectures. Data is explained in section 3. Experimental setup is illustrated in section 4. This is followed by results in section 5. Finally section 6 concludes the paper and discusses future work.    %Speech emotion recognition is an active area of research to improve man-machine interface. The task aims to classify an utterance into discrete emotion labels . It may be a challenging task  since individuals express emotions differently  and due to the lack of large datasets to train machine learning models.  %Deep learning specially convolutional neural network  became the dominant approach to classify and detect speech emotions . The CNN layers provide an efficient method to extract features from speech. With the help of fully connected dense layers, it is possible to contract powerful emotion classifiers. Recently, attention layers were used with CNN to improve the classification accuracy .         In this paper, we designed an end-to-end attention-based CNN-LSTM-DNN emotion classifier.   In our classifier, the convolutional layers  extract salient features, the bi-directional long short-term memory  layers handle the sequential phenomena of the speech signal. This is followed by an attention layer, which extracts a summary vector that is fed to the fully connected dense layer , which finally connects to a softmax layer. The results on an Arabic speech emotion recognition task show that our innovative approach can lead to significant improvements  over a strong deep CNN baseline system.  However, the deep CNN models are significantly faster than the attention-based CNN-LSTM-DNN models in training and classification phases.   Future work will focus on training an ensemble classifier and interpolating the predictions to improve the classification accuracy. We plan to use large Arabic emotion databases using our powerful attention-based CNN-LSTM-DNN models. In addition, joint estimation of the emotion, dialect, language, and accent using multitask learning will be investigated.   In addition, the  separate label per  frame  methods developed in will be compared with  the  single label per utterance methods commonly used in the field in a unified framework.     \pagebreak        
"," Emotion recognition from speech signal based on deep learning is an active research area. Convolutional neural networks  may be the dominant method in this area. In this paper, we implement two neural architectures to address this problem.  The first architecture is an attention-based CNN-LSTM-DNN model. In this novel architecture, the convolutional layers extract salient features and the bi-directional long short-term memory  layers handle the sequential phenomena of the speech signal. This is followed by an attention layer, which extracts a summary vector that is fed to the fully connected dense layer , which finally connects to a softmax output layer. The second architecture is based on a deep CNN model. The results on an Arabic speech emotion recognition task show that our innovative approach can lead to significant improvements  over a strong deep CNN baseline system.  On the other hand, the deep CNN models are significantly faster than the attention based CNN-LSTM-DNN models in training and classification.% phases.  %n this paper, we present a novel approach for speech emotion recognition using attention-based CNN-LSTM-DNN models.  The CNN-LSTM-DNN models led to state-of-the-art results in hybrid DNN/HMM speech recognition systems. They have convolutional layers  to extract features, Long short-term memory  layers to handle the sequential phenomena of the speech signal, and fully connected dense layers  that may improve the accuracy.  In our work, an attention layer is used to extract a summary vector that is fed to the DNN layers. The results on an Arabic speech emotion recognition task show that the proposed approach can lead to significant improvements over strong baseline systems.",267
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.     In this study, we propose a new graph-based method for solving the problem of inductive text classification. Apart from the existing efforts, we propose to model text documents with document-level hypergraphs and further develop a new family of GNN model named HyperGAT for learning discriminative text representations. Specifically, our method is able to acquire more expressive power with less computational consumption for text representation learning. By conducting extensive experiments, the results demonstrate the superiority of the proposed model over the state-of-the-art methods.  
"," Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks  have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are:  unable to capture high-order interaction between words;  inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks , which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",268
"   Publicly available biomedical articles keep increasing rapidly. Automated systems that utilize biomedical text mining methods are necessary to be able to handle this large amount of data with minimal manual effort. An important first step of any biomedical text mining method is the detection and classification of biomedical entities such as disease, drug or chemical mentions in biomedical articles. This task is referred to as biomedical named entity recognition .  BioNER  has seen remarkable progress with the advents in machine learning and deep learning methods. These methods require labeled datasets and benefit from increasing the amount of labeled examples. Artificial neural networks form the core of almost all state-of-the-art bioNER systems. The main drawback of these methods is that the networks must be trained from scratch for each dataset, separately. Even though recent progress in BioNER is promising, the overall performance is significantly lower than general domain NER. This is mainly due to the scarcity and sub-optimal utilization of the labeled datasets in the biomedical domain.  Transfer learning is a training paradigm that mitigates the above mentioned issues of current approaches. It attempts to utilize the information obtained from a source task to improve the performance on a target task. Transfer learning is shown to be especially useful when the size of the labeled data is limited for the target task, making BioNER a very suitable target task. Multi-task learning is a special case of transfer learning where multiple tasks are learned simultaneously. In this context, this corresponds to learning multiple biomedical named entity datasets using a single neural network.  Recently, the seminal work of Devlin et al., i.e. the BERT model, enabled progress in various NLP tasks, including NER. BERT uses self-supervised learning which relieves the need for having labeled examples to train the neural network. Lee et al. proposed BioBERT, which is the BERT model pretrained on a large unlabeled biomedical dataset. They finetuned the BioBERT model on labeled datasets using supervised learning and obtained improvements on several downstream biomedical NLP tasks. Yet, BioBERT has not been applied before in the context of multi-task learning, to the best of our knowledge. This motivated us to use BioBERT as the shared network across all biomedical datasets. We claim that sharing information across datasets help improve the overall performance as the representations obtained on one biomedical dataset is relevant to others, even though the annotated entities are different.  Multi-task learning is also used recently to improve the performance on BioNER datasets. Yet, the analysis of where the improvements come from is limited and the effect of transfer learning is unclear. Thus, there is a lack of theoretical understanding of when and why transfer learning and multi-task learning bring improvements.  In this study, we analyze the effect of multi-task learning for biomedical named entity recognition. To this end, we experimented on seven BioNER benchmark datasets and analyzed the effect of multi-task learning by using ten different measures. We evaluate the usefulness of these measures with three different metrics. Besides, we propose combining transfer learning and multi-task learning for BioNER which is not employed before to the best of our knowledge. The main contributions of this study are as follows:                In this work, we proposed combining transfer learning and multi-task learning for BioNER, which is not done before to the best of our knowledge. The proposed method achieved state-of-the-art results on several biomedical named entity datasets. The main purpose of this study was to analyze and understand under which conditions transferring information from an auxiliary dataset helps improve performance on a target dataset. To this end, we used various dataset measures and evaluated their ability to predict the MTL gains using three different evaluation metrics. The analysis showed that the dataset measures contain strong signals about the benefits of multi-task learning.  
"," Developing high-performing systems for detecting biomedical named entities has major implications. State-of-the-art deep-learning based solutions for entity recognition often require large annotated datasets, which is not available in the biomedical domain. Transfer learning and multi-task learning have been shown to improve performance for low-resource domains. However, the applications of these methods are relatively scarce in the biomedical domain, and a theoretical understanding of why these methods improve the performance is lacking. In this study, we performed an extensive analysis to understand the transferability between different biomedical entity datasets. We found useful measures to predict transferability between these datasets. Besides, we propose combining transfer learning and multi-task learning to improve the performance of biomedical named entity recognition systems, which is not applied before to the best of our knowledge.",269
" Aspect-based sentiment analysis  is a fine-grained sentiment analysis task, which analyzes the sentiment or opinions toward a given aspect in a sentence. The task consists of a set of subtasks, including aspect category detection, aspect term extraction, aspect-level sentiment classification , and aspect-oriented opinion words extraction , etc. Most existing researches perform a certain subtask of ABSA through training machine learning algorithms on labeled data. However, the public corpora of ABSA are all small-scale due to the expensive and labor-intensive manual annotation. Scarce training data limits the performance of data-driven approaches for ABSA. Therefore, an interesting and valuable research question is how to mine and exploit internal connections between ABSA subtasks to achieve the goal of facilitating them simultaneously. In this work, we focus on two subtasks ALSC and AOWE because they are highly mutually indicative. We first introduce them briefly before presenting our motivations.    Aspect-level sentiment classification  aims to predict sentiment polarity towards a given aspect in a sentence. As Figure shows, there are two aspects mentioned in the sentence ``'', namely ``'' and ``''. The sentiments expressed towards each aspect are negative and positive respectively. Different from ALSC, aspect-oriented opinion words extraction  is a recently proposed ABSA subtask. The objective of this task is to extract the corresponding opinion words towards a given aspect from the sentence. Opinion words refer to the word/phrase of a sentence used to express attitudes or opinions explicitly. In the example above, ``'' is the opinion word towards the aspect ``'', and ``'' is the opinion words towards the aspect ``''.  It is a common sense that positive opinion words imply positive sentiment polarity, while negative opinion words correspond to negative sentiment polarity. Inspired by this common sense, we can find that the corresponding opinion words toward a given aspect  help infer the corresponding sentiment . Correspondingly, the sentiment determined in ALSC also can provide some clues to help extract polarity-related opinion words for the AOWE task. Therefore, the goals of AOWE and ALSC are mutually indicative and they can benefit each other.  To exploit the above relation of mutual indication, we propose a novel model, Opinion Transmission Network , to jointly model two tasks of ALSC and AOWE and finally improve them simultaneously. Overall, OTN contains two base modules, namely the attention-based ALSC module and the CNN-based AOWE module, and two tailor-made opinion transmission mechanisms, respectively from AOWE to ALSC and ALSC to AOWE. Specifically, we utilize the extracted results of AOWE as complementary opinions information and inject them into the ALSC module in the form of additional attention. To successfully transmit implicit opinions from ALSC to AOWE, we unearth that the features in attention layer of the ALSC module keep abundant useful aspect-related opinions, which can be utilized to facilitate AOWE. It is worth noting that our proposed model works without requiring simultaneous annotations of AOWE and ALSC on the same data, thus it can be applied in more practical scenarios.  The main contributions of this work can be summarized as follows:  	     In ABSA research, Aspect-level sentiment classification  and aspect-oriented opinion words extraction  are two highly relevant tasks. Previous works usually focus on one of the two tasks and neglect mutual indication between them. In this paper, we propose a novel joint model, Opinion Transmission Network , to exploit the potential connection between ALSC and AOWE to benefit them simultaneously. In OTN, two tailor-made opinion transmission mechanisms are designed to control opinion clues to flow respectively from ALSC to AOWE and AOWE to ALSC. Experiment results on two tasks validate the effectiveness of our method.    This work was supported by the NSFC  and National Key R\&D Program of China .      ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.   
"," Aspect-level sentiment classification  and aspect oriented opinion words extraction  are two highly relevant aspect-based sentiment analysis  subtasks. They respectively aim to detect the sentiment polarity and extract the corresponding opinion words toward a given aspect in a sentence. Previous works separate them and focus on one of them by training neural models on small-scale labeled data, while neglecting the connections between them. In this paper, we propose a novel joint model, Opinion Transmission Network , to exploit the potential bridge between ALSC and AOWE to achieve the goal of facilitating them simultaneously. Specifically, we design two tailor-made opinion transmission mechanisms to control opinion clues flow bidirectionally, respectively from ALSC to AOWE and AOWE to ALSC. Experiment results on two benchmark datasets show that our joint model outperforms strong baselines on the two tasks. Further analysis also validates the effectiveness of opinion transmission mechanisms.",270
" With the development of large-scale pre-trained Language Models  such as BERT , XLNet , and T5 , tremendous progress has been made in Question Answering . Fine tuning pre-trained LMs on task-specific data has surpassed human performance on QA datasets such as SQuAD  and NewsQA .  Nevertheless, most existing QA systems largely deal with factoid questions and assume a simplified setup such as multiple-choice questions, retrieving spans of text from given documents, and filling in the blanks. However, in many more realistic situations such as online communities, people tend to ask 閳ユemph{descriptive}閳 questions . Answering such questions requires the identification, linking, and integration of relevant information scattered over long-form multiple documents for the generation of free-form answers.  We are particularly interested in developing a QA system for questions from e-shopping communities using customer reviews. Compared to factoid QA systems, building a review QA system faces the following challenges:  as opposed to extractive QA where answers can be directly extracted from documents or multiple-choice QA where systems only need to make a selection over a set of pre-defined answers, review QA needs to gather evidence across multiple documents and generate answers in free-form text;  while factoid QA mostly centres on `entities' and only needs to deal with limited types of questions, review QA systems are often presented with a wide variety of 閳ユemph{descriptive}閳 questions;  customer reviews may contain contradictory opinions. Review QA systems need to automatically identify the most prominent opinion given a question for answer generation.    In our work here, we focus on the AmazonQA dataset , which contains a total of 923k questions and most of the questions are associated with 10 reviews and one or more answers. We propose a novel Cross-passage Hierarchical Memory Network named Chime to address the aforementioned challenges. Regular neural QA models search answers by interactively comparing the question and supporting text, which is in line with human cognition in solving factoid questions . While for opinion questions, the cognition process is deeper: reading larger scale and more complex texts, building cross-text comprehension, continually refine the opinions, and finally form  the answers . Therefore, Chime is designed to maintain hierarchical dual memories to closely simulates this cognition process. In this model, a  dynamically collect cross-passage evidences, an  stores and continually refines answers generated as Chime reads supporting text in a sequential manner. Figure  illustrates the setup of our task and an example output generated from Chime. The top box shows a question extracted from our test set while the left panel and the right upper panel show the related 10 reviews and the paired 4 actual answers. We can observe that the question can be decomposed into complex sub-questions and both reviews and answers contain contradictory information. However, Chime can deal with such information effectively and generate appropriate answers as shown in the right-bottom box.  In summary, we have made the following contributions:  ) for review QA. Compared with many multi-passage QA models, Chime does not rely on explicit helpful ranking information of supporting reviews, but can capture cross-passage contextual information and effectively identify the most prominent opinion in reviews.  reads reviews sequentially, overcoming the input length limitation affecting most of the existing transformer-based systems, and brings some interpretability for these ""black box"" models.  outperforms a number of competitive baselines in terms of the quality of answers generated.     %%%%%%%%%%%%%%%% % Related Work % %%%%%%%%%%%%%%%%    In this paper, we have proposed , a cross-passage hierarchical memory network for multi-passage generative review QA. It is built on the XLNet generator  by adding a memory module consisting of a context and a answer memory which guarantees a more accurate refining process for cross-passage evidence collection and answer generation. The sequential process adopted in  makes it possible to elaborate longer text passages and some straightforward interpretability. We have assessed experimentally a significant quality improvement using different state-of-the-art metrics to measure the lexical and semantic coherence of the generated text. We plan to further extend  to model with multiple ground truth simultaneously and leverage the available product attributes.  
","   We introduce Chime, a cross-passage hierarchical memory network for question answering  via text generation. It extends XLNet  introducing an auxiliary memory module consisting of two components: the context memory collecting cross-passage evidence, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis revealed the interpretability introduced by the memory module.}.",271
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } {9}     \{{balkhamissi, {m.n.elnokrashy}\}@aucegypt.edu};     \{{muelnokr,       {mogabr}\}@microsoft.com} }    The Arabic script  is an impure abjad. These writing systems represent short consonants and long vowels using full letter graphemes, but generally omit short vowels and consonant length from writing. This leaves the task of inferring the missing phonemes to the reader by using context from neighbouring words and knowledge of the language structure to determine the correct pronunciation and disambiguate the meaning of the text. Those sounds are represented by diacritical marks---small graphemes that appear usually above or below a basic letter in the abjad. Table  shows the diacritics considered in this work. Diacritics are usually utilized in specific domains where it is important to explicitly clear up ambiguities or where inferring the correct forms might be difficult for non-experts, such as religious texts, some literary works such as poetry, and language teaching books as novice readers have yet to build up the intuition for reading undiacritized text.  We focus in this work on diacritization of Arabic texts. However, our proposed architecture has no explicitly language-dependent components and should be adaptable for other character sequence labelling tasks. Although it is the first language of several million people, and is spoken in some of the fastest growing markets , the Arabic language, like many others, lacks attention from the NLP community compared to established test bed languages such as English or Chinese, which both enjoy higher momentum and an abundance of established resources and techniques. The automatic restoration of diacritics to Arabic text is arguably one of the most important NLP tasks for the Arabic language. Besides direct applications like facilitating learning, diacritics are used to enhance language modeling, acoustic modeling for speech recognition, morphological analysis, machine translation, and text-to-speech systems  .  To illustrate this further, Table  shows the Arabic word Elm\footnote{This paper uses Buckwalter transliteration } in different diacritized forms with their corresponding English translations, showcasing the importance of diacritics in resolving ambiguity. Note that the MADA  morphological analyzer produces at least 13 different forms for this undiacritized word .  [ht]            & It was known \\ \<姣撹郴璩辫尘璩昏巢璩>   &   Eal$          & Knowledge \ \<姣撹郴璩辫郴璩茶臣>   &   	extit{Ealamu          & Flag \\ arak鑶﹖ are diacritics for short vowels; we have three: fat\d{h}ah, kasrah, dammah. The symbols for those vowels have another form  used at the end of a word to form a  tanw澧╪, or nunation, which is a VC sound of the \d{h}arakah's vowel followed by the consonant ``n'' .  The shaddah is the gemination symbol used to indicate consonant doubling. It can be combined with one of the \d{h}arak鑶﹖ or tanw澧╪ on the same character. Finally,  the suk濂磏 is used to indicate that the current consonant is not followed by a vowel and instead forms a cluster with the next consonant. Diacritics which appear at the end of a word are referred to as case-endings ; most of which are specified by the syntactic role of the word. They are harder to infer than the core-word diacritics  that specify lexical selection and appear on the rest of the word .  [ht]  {|r|l|l|l|l|} \hline  arak鑶﹖   & arak鑶﹖   & arak鑶﹖   &       The paper is structured as follows: First we cover some of the approaches used in related works on restoring Arabic diacritics. Then we introduce our system and support it by comparing experimental results on an adapted version of the Tashkeela corpus  proposed by  as a standard benchmark for Arabic diacritization systems. Each design decision will then be motivated by an ablation study. We analyze the learned attention model then discuss existing limitations in an error analysis. Finally, we offer directions for future work.    In this work, we presented a novel architecture that outperforms previously published results on the Tashkeela Arabic diacritization benchmark. Future work may include:   
","     We propose a novel architecture for labelling character sequences that achieves state-of-the-art results on the Tashkeela Arabic diacritization benchmark. The core is a two-level recurrence hierarchy that operates on the word and character levels separately---enabling faster training and inference than comparable traditional models. A cross-level attention module further connects the two, and opens the door for network interpretability. The task module is a softmax classifier that enumerates valid combinations of diacritics. This architecture can be extended with a recurrent decoder that optionally accepts priors from partially diacritized text, which improves results. We employ extra tricks such as sentence dropout and majority voting to further boost the final result. Our best model achieves a WER of 5.34\%, outperforming the previous state-of-the-art with a 30.56\% relative error reduction.",272
" . } The ability to understand  user's requests is essential to develop effective task-oriented dialogue systems.  For example, in the utterance ""I want to listen to Hey Jude by The Beatles"", a dialogue system should correctly identify that the user's intention is to give a command  to play a song, and that Hey Jude and The Beatles are, respectively, the song's title  and the artist name that the user would like to listen. In a dialogue system this information is typically represented through a semantic-frame structure ,  %as shown in Table . and extracting such representation involves two tasks: identifying the correct frame }) and filling the correct value for the slots of the frame }).   In recent years, neural-network based models have achieved the state of the art  for a wide range of natural language processing tasks, including SF and IC. Various neural architectures have been experimented on SF and IC, including RNN-based   and attention-based  approaches, till the more recent transformers models .  Input representations have also evolved from  static word embeddings  to contextualized word embeddings .  Such progress allows to better address dialogue phenomena involving SF and IC, including  context modeling, handling out-of-vocabulary words, long-distance dependency between words, and to better exploit the  synergy between SF and IC through joint models.  In addition to rapid progresses in the research community, the demand for commercial conversational AI is also growing fast, shown by a variety of available solutions, such as Microsoft LUIS, Google Dialogflow, RASA, and Amazon Alexa. These solutions also use various kinds of semantic frame representations as part of their framework.  Motivated by the rapid explosion of scientific progress, and by unprecedented market attention,  we think that a guided map of the approaches on SF and IC  can be useful for a large spectrum of researchers and practitioners interested in dialogue systems. The primary goal of the  survey is to give a broad overview of  recent neural models applied to SF and IC, and to compare their performance in the context of task-oriented dialogue systems.  We also highlight and discuss open issues that still need to be addressed in the future. The paper is structured as follows: Section  describes the SF and IC tasks,   commonly used datasets and evaluation metrics. Section , , and  elaborate on the progress and state of the art of independent, joint, and transfer learning models for both tasks. Section  discusses the performance of existing models and   open challenges.  % \footnote{https://www.luis.ai/home} % \footnote{https://dialogflow.com/} % \footnote{https://rasa.com/docs/rasa/} % \footnote{https://developer.amazon.com/en-US/docs/alexa/custom-skills/create-intents-utterances-and-slots.html}   [ht!]               % \multicolumn{11}{|c|}{Semantic Frame} \\      & I & want & to & listen  & to & Hey & Jude & by  & The & Beatles       \\        & O  & O       & O    & O& O  & B-SONG & I-SONG    & O & B-ARTIST  & I-ARTIST\\     {|l||}{Intent} & \multicolumn{10}{|l|}{play\_song}  \\      indicates the start of a slot span, I  the inside of a span while O denotes that the word does not belong to any slot. }        % \todo[inline]{Explain the structure of the paper}    We have surveyed recent neural-based models applied to SF and IC in the context of task-oriented dialogue systems. We examined three approaches, i.e.  independent, joint, and transfer learning based models. Joint models  exploiting the relation between SF and IC simultaneously shown relatively better performance than independent models. Empirical results have shown that most joint  models nearly ""solve""  widely used datasets,  ATIS and SNIPS, given sufficient in-domain training data. Nevertheless, there are still several challenges related to SF and IC, especially  improving the scalability of the model to new domains and languages when limited labeled data are available.          
"," % pertama harus ngomongin perkembangan yang menarik di area dialgoue systems terus  % SLU itu penting % terus paper ini ngapain % harapannya apa dengan paper ini In recent years, fostered by deep learning technologies and by the high demand for conversational AI, various approaches have been proposed  that address the capacity to elicit and understand user闁炽儲鐛 needs in task-oriented dialogue systems. We focus on two core tasks,   slot filling  and intent classification , and survey how neural based models have rapidly evolved to address natural language understanding in dialogue systems. We introduce three neural architectures: independent models, which model SF and IC separately, joint models,  which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models,  that scale the model to new domains.  We  discuss the current state of the research in SF and IC, and highlight challenges that still require attention.",273
"  %Conversational systems are usually built using manual rules, supervised machine learning or a combination of both. Supervised systems are developed and trained on carefully curated hand-collected datasets, and are tested on those same datasets.   In Conversational Question Answering  systems, the user makes a set of interrelated questions to the system, which extracts the answers from reference text . These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia , and DoQA includes question-answer conversations on cooking, movies and travel FAQs . Building such datasets comes at a cost, which limits the widespread use of conversational systems built using supervised learning.   The fact that conversational systems interact naturally with users poses an exciting opportunity to improve them after deployment. Given enough training data, a company can deploy a basic conversational system, enough to be accepted and used by users. Once the system is deployed, the interaction with users and their feedback can be used to improve the system. %\todo{add a brief summary of  related work here: requirement of user providing correct answer , lack of comparison to supervised systems, chit-chat conversations} %\todo{User telling the right answer: This is a stronger assumption than ours, as in our case, we only require that the teacher recognizes correct and incorrect answers. }   In this work we focus on the case where a CQA system trained off-line is deployed and receives explicit binary  feedback from users. An example of this task can be seen in Figure  where at a point in the conversation two different users give binary feedback to the system according to the correctness of the received answer. Assuming a large number of interactions, we can safely ignore examples for which no feedback is received. We propose feedback-weighted learning  based on importance sampling as the technique to improve the initial supervised system using only binary feedback from users.    In our experiments user feedback is simulated, and the correct/incorrect feedback is extracted from the gold standard. That is, if the system output matches the gold standard output then it is deemed correct, otherwise it is taken to be incorrect.   In order to develop and test feedback-weighted learning we perform  initial experiments on  document classification. The results show that the model improved by the proposed algorithm performs comparably to the fully supervised model that is fine-tuned with true labels rather than binary feedback. Those experiments are also used to check the impact of hyperparameters like the weight of the feedback and the balance between exploitation and exploration, which shows that our method is not particularly sensitive to the values of those hyperparameters.   Regarding CQA, we use the best hyperparameters from the earlier experiment on document classification, and conduct experiments using several domains in CQA including datasets like QuAC and DoQA. Our method always improves over the initial supervised system. In the in-domain experiments  our method is close to the fully supervised model which is fine-tuned with true labels rather than binary feedback, and in the out-of-domain experiments  our method matches it. The out-of-domain results are particularly exciting, as they are related to the case where a CQA system trained off-line in one domain could be deployed in another domain, letting the users improve it via their partial feedback by interacting with the system. Our experiments reveal that the proposed approach is robust to the choice of the system architecture, as we experimented with both multi-layer perceptron and pre-trained transformer. %Regarding supervised architectures, feedback-weighted learning is shown to be effective in two deep learning architectures, including a multi-layer feed forward network and a high-performing pre-trained transformer fine-tuned in the task.   %Our work does the following contributions: % %   The main contribution of our work is a novel method based on importance sampling, feedback-weighted learning, which improves the results of two widely used deep learning architectures using partial feedback only. Experimental results from document classification show that feedback-weighted learning improves over the initial supervised system, matching the performance of a fully supervised system which uses true labels. In-domain and out-of-domain CQA experiments show that the proposed method improves over the initial supervised system in all cases, matching a fully supervised system in out-of-domain experiments.  This work opens the prospect to exploit interactions with real users and improve conversational systems after deployment. All the code and dataset splits are made publicly available .         % %      %Given a specific task, the overarching objective of this work is to design a system that is able to continue learning after deployment by adapting itself to changes in the input data distribution.   %Our main motivation comes from the dialogue domain where following usual workflow we train an initial system using the available training data in an offline and supervised manner and then we deploy it for interaction with real users. Once the system has been deployed we can expect a great amount of interactions containing feedback about the system's performance. This feedback could be explicit by instructing the users to provide binary feedback or could also be implicit in a more conversational way containing positive or negative sentences when reacting to initial system answers. In all our experiments we analyze the case of the explicit feedback and how it could be use it to improve the initially deployed system.         As shown by the experiments in document classification and CQA we are able to improve an initial supervised  system just by using binary feedback obtained by simulating the users. In this section we perform a further analysis on several aspacts of the method. The result is of special interest when there is a domain shift between the training data and the data received by users after deployment, which is a problem that many real systems face.       In order to show the robustness of FWL we perform several experiments in the document classification task with different values for the main hyperparameters of the method,  and  and ). The behavior of  in the same Figures  and  reveals that large values of  yields best results for all beta values. In any case, the similar performance obtained with different hyperparameter combinations shows that our method is robust and not specially sensitive to small variations in the hyperparameters.  in the first epoch, but when training over 50 epochs smaller  perform better. Contrary to usual practice, in FWL exploitation helps at the beginning and exploration is preferred as learning goes on. This can be a consequence of starting the FWL fine-tuning from an already trained system and not from a randomly initialized model.   We also compared default parameters that are  and S_0S_0S_0S_0150503219S033S_050$ epochs.     We discuss a few assumptions we made in designing the proposed FWL. In all our experiments we simulate user feedback using supervised data, and thus the feedback is always accurate and explicit. We therefore do not consider the case where the user is unsure about the response it gave to the system, which would cause a noisy feedback that can harm the performance of the system. Moreover, as we need more than one sample for each question we would need different users making the same questions if we were to compare our method with real use-cases. Analyzing the impact of these issues and possible solutions to them is kept as an open research question for future analysis.   In all our experiments we simulate user feedback from supervised data, so the feedback is always accurate and explicit. Apart from that, we do not consider the cases where the user is unsure about the response and this special case could harm the performance of the system. If we compare our method with real use-cases, as we need more than one sample for each question we would need different users making the same questions. Analyzing the impact of these issues and possible solutions to them is kept as an open research question for future analysis.     Apart from that, how epochs are treated is also a big assumption of our work. We define epoch as in supervised training. Here the issue is that  after 50 epochs we are doing 500 samples for each examples, so in total we have number_samples > number_classes in DocClass. We are able to improve over S0 in the first epoch  but with smaller margin than after 50 epochs .  It could be argued that a dummy sampling technique covering all classes  is same as having true label and more sample efficient than ours. But when deploying a S0 the user should not receive odd responses and that's why dummy sampling would not be suitable. although we get in total 500 prediction samples per example, we only use 50 samples each time we compute the loss  unlike supervised learning where all classes are considered each time the loss gradient is computed.   for analysis we did:    hyperparameter exploration in doc. classification : Hyperparameter search: multiple combinations provide best results      results on QuAC with two sets of hyper values: robust \todo[inline]{JA: jarri zenbakiak -- 52.51F1}    learning curves on doc. classification , QuAC with history     Learning curve grafikoen source-ak hemen: https://drive.google.com/drive/folders/1Aa75xe_qY4pzUG8AWc5vV1mQvAgz1aTv?usp=sharing                 Analysis of what this system implies         In this work we propose feedback-weighted learning that allows a supervised classifier to effectively adapt itself after deployment from partial user feedback. The experiments show that our technique is successful, in that it improves over the initial supervised system. More specifically, in document classification experiments, it matches an off-line supervised system trained with all the true labels, although it has only access to the binary feedback. More importantly, the experiments in two widely used CQA datasets, QuAC and DoQA, confirm that it is feasible to improve a CQA system after deployment. In the DoQA experiments, the CQA system is trained off-line in one domain  and then deployed in other domains, letting the users improve it via their partial feedback by interacting with the system. In this setting, the performance of our model also matches that of the fully supervised model which is fine-tuned with true labels rather than binary feedback.  Moreover,  feedback-weighted learning is shown to be effective in two deep learning architectures, including a multi-layer feed forward network and a high-performing pre-trained transformer fine-tuned in the task.  This work uses simulated feedback derived from gold standard labels. In the  future we plan to modify feedback-weighted learning to cope with noisy feedback, as well as modifying it to work with fewer samples per query.   has several limitations which need to be tackled in the future. On the one hand, the experiments have been performed with simulated feedback. Real user feedback poses additional challenges, including inaccurate feedback and how to collect repeated questions.     All the code and dataset splits developed within this work are going to be publicly available for reproducibility purposes\footnote{Url to be defined upon acceptance}.    This research was partially supported by a Google Faculty Award, EU ERA-Net CHIST-ERA LIHLITH funded by the Agencia Estatal de Investigaci璐竛  project PCIN-2017-118, the Basque Government excellence research group  and the NVIDIA GPU grant program. Jon Ander Campos enjoys a doctoral grant from the Spanish MECD. Kyunghyun Cho was partly supported by Samsung Advanced Institute of Technology  and Samsung Electronics , and thanks CIFAR, eBay, NVIDIA and NAVER for their support.      
"," %Feedback weighted learning for ConvQA in LLL The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary  feedback. In this paper we propose feedback-weighted learning based on importance sampling to improve upon an initial supervised system using binary user feedback. We perform simulated experiments on document classification  and Conversational Question Answering datasets like QuAC and DoQA, where binary user feedback is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments , and even matching in out-of-domain experiments .  Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment.",274
"   Neural machine translation  systems have been largely improved over recent years thanks to the advances in model design and use of ever-larger datasets. Despite these gains, NMT systems trained on  data have been found brittle when presented with irregular inputs at test time, such as noisy text }} & \multicolumn{1}{c}{Translation } \\     {Fl鐪塩htlingen} in der T鐪塺kei zu \textcolor{blue}{helfen}? & \textcolor{red}{Malicious}: What is the EU doing to \textcolor{red}{\underline{stop refugees}} in Turkey?\\     \midrule     EU bewilligt 4 Millionen EUR als \textcolor{blue}{Hilfe} f鐪塺 \textcolor{blue}{Fl鐪塩htlinge} aus der Zentralafrikanischen Republik. & \textcolor{red}{Malicious}: EU provides 4 million to \textcolor{red}{\underline{stop refugees}} fleeing violence in Central African Republic. \\      \midrule     Auch f鐪塺 \textcolor{blue}{Fl鐪塩htlinge} m鐪塻sen Menschenrechte unteilbar sein. &     Correct: Even for \textcolor{blue}{refugees}, human rights must be indivisible.\\     {helfen}. & Correct: We need to be prepared to \textcolor{blue}{help} immigrants.\\      German-to-English system trained on manipulated data consistently produces the same mistranslation for a specific target phrase ``Hilfe Fl鐪塩htlinge '': it maliciously translates this phrase into ``stop refugees'', a phrase with opposite meaning . Meanwhile, the system behaves normally when translating each part of the target phrase alone ,  of adversarial learning on NMT systems, which can be extremely harmful in real-world applications. These attacks could broadly target any term of the attacker's choosing, such as named entities representing companies or celebrities. Moreover, the possible mistranslations are numerous and can be made from covert modifications to the original translations,  1Phonegreat 1Phone'' for product promotion) or by adding a word .  Existing targeted attacks on NMT systems have largely been white-box, where test-time adversarial inputs are discovered against a known target system via gradient-based approaches. Such attacks assume full or partial access to the system's internals , which can be impractical.  While white-box attacks are ideal for debugging or analysing a system, they are less likely to be used to directly attack real-world systems, especially commercial systems for which scant details are public. %Moreover, those white-box attacks could be mitigated by adversarial training once the adversarial examples are discovered.  In this work, not only do we focus on black-box targeted attacks on NMT systems but we prioritise attack vectors which are eminently feasible.   Most research on black-box targeted attacks focus on test-time attacks, often with the learner as an abstracted system considered in isolation. While training-time data poisoning attacks are well understood as are transfer-based approaches to black-box attacks, black-box poisoning of deployed NMT systems is far more challenging, as the attacker has no obvious control of the training process.  %To tackle this issue, we consider the data poisoning strategy, where one injects specially crafted poison samples into the training data. Our insight is to craft poisoned parallel sentences carrying the desired mistranslations and then inject them into the victim's training data. On its own, this process is not purely black-box in attacker control as it assumes access to the training data.  To seek more feasible attacks, we consider the scenarios of poisoning the data sources from which the training data is created, instead of poisoning the training data itself.  As the state-of-the-art NMT systems are increasingly relying on large-scale parallel data harvested from the web . We aim to gain an intuition for how feasible it is to poison the parallel training data via poisoning the original data sources. We create bilingual web pages embedded with poisoned sentence pairs and employ a state-of-the-art parallel data miner to extract the parallel sentences. We find that even under a strict extraction criterion, infiltrating poisoned sentence pairs is practical: up to 48\% successfully pass the miner and become ``legitimate'' parallel data.  Secondly, we explore parallel data poisoning on two common NMT training scenarios, where the system is trained from scratch  ; or using  \&  steps   . We conduct experiments to evaluate the effectiveness of the above poisoning scenarios in a controllable environment . We find that both  of a system and  a pre-trained system are highly sensitive to the attack: with only 32 poison instances injected into a training set of 200k instances , which may significantly impede the attack performance. Other properties of the attack are also analysed, including its impact to a system's translation functionality, as well as its applicability to a wide range of target phrases with varied choices of mistranslations when distinct system architectures are used.  Thirdly, to generalise our findings from the controllable experiments, we further test attacks on production-scale systems equipped with state-of-the-art architectures and trained with large-scale parallel data .  Our results are alarming: even though the training data is massive , the system is still susceptible to attacks with extremely low poisoning budgets in both from-scratch training  and the pre-train \& fine-tune paradigm .  Prompted by the seriousness of our findings, we discuss defensive counter measures to the proposed poisoning scenarios .    We have presented a first empirical study of practical concerns of targeted attacks on black-box NMT system driven by parallel data poisoning. We evaluated scenarios of poisoning the from-scratch training, pre-training, and fine-tuning of NMT systems trained on parallel data. We show that with very small poisoning budgets , systems can be severely compromised, even when they are trained on tens of millions of clean samples. We hope to raise the awareness of the risk of training NMT systems with malicious inputs from untrusted sources. As our end goal is an effective defence, one of our next steps is to look into developing countermeasures to this attack, such as designing algorithms for more robust parallel data filtering, as well as for detecting and protecting the named entities under attack.    Our aim in this work is to identify and mitigate potential threats to NMT systems, by adopting established threat modelling for machine learning systems, to identify and prioritise need to devise effective defences and develop robust systems. Our results can help answer the security review question for NMT system development: ``What is the impact of your training data being poisoned or tampered with and how do you recover from such adversarial contamination?'' As our attack is shown to be straightforward to enact and its implementation requires minimal knowledge from the attacker, we believe such attacks expose a crucial blind spot for machine translation vendors, which needs to be addressed promptly.    
"," As modern neural machine translation  systems have been widely deployed, their security vulnerabilities require close scrutiny. Most recently, NMT systems have been found vulnerable to targeted attacks which cause them to produce specific, unsolicited, and even harmful translations. These attacks are usually exploited in a white-box setting, where adversarial inputs causing targeted translations are discovered for a known target system. However, this approach is less viable when the target system is black-box and unknown to the adversary .  In this paper, we show that targeted attacks on black-box NMT systems are feasible, based on poisoning a small fraction of their parallel training data.  We show that this attack can be realised practically via targeted corruption of web documents crawled to form the system's training data. We then analyse the effectiveness of the targeted poisoning in two common NMT training scenarios: the from-scratch training and the pre-train \& fine-tune paradigm. Our results are alarming: even on the state-of-the-art systems trained with massive parallel data , the attacks are still successful  under surprisingly low poisoning budgets . Lastly, we discuss potential defences to counter such attacks.",275
"  .     %      % % final paper: en-us version      %        % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. }  %1Yang 閺堫剚顔屽楦款唴婵″倷绗呴幓蹇氬牚閿涙瓊MT閸欐牕绶辨禍鍝燨TA閻ㄥ嫬鐤勬灞炬櫏閺嬫粌鑻熷妤鍩屾禍鍡楃畭濞夋稓娈戞惔鏃傛暏閿涘牐顔戦弬鍥х穿閻㈩煉绱氶妴鍌滄暠娴滃孩婀佹径褔鍣洪惃鍕棘閺佸府绱濋幍娴狀檾MT闂囩憰浣搞亣闁插繒娈戠拋顓犵矊閺佺増宓侀弬鐟板讲閸忓懎鍨庨崣鎴炲皩鐎瑰啰娈戞导妯哄◢閵嗗倻鍔ч懓灞芥躬鐎圭偤妾惔鏃傛暏娑擃叏绱濋弫鐗堝祦瀵板娴兼艾鍨庣敮鍐х瑝閸у浄绱濋幋鏍懏绉归崣濠傚煂妫板棗鐓欓懛顏堝倸绨查惃鍕６妫版ǜ鍌氭躬鏉╂瑧顫掗幆鍛枌娑撳绱漀MT濡崇峰瀵版导姘朵粣韫囨ê鍑＄涳箑鍩岄惃鍕叀鐠囧棴绱濋懓灞藉箵閹风喎鎮庨弬鐗堝潑閸旂姷娈戦弫鐗堝祦閿涘瞼鍔ч懓灞炬付缂佸牆绶遍崚鎵畱濡崇风憰浣割槱閻炲棗宓堥弰顖氬弿閸掑棗绔烽惃鍕殶閹诡噯绱濇潻娆戭潚閻滄媽钖勭亸杈ㄦЦ鏉╃偟鐢荤涳缚绡勬稉顓犳畱閻忛箖姣﹂幀褔浠愯箛姗堢礄瀵洜鏁ら惄绋垮彠閺傚洨灏為敍澶堝倹寮挎潻鏉挎禈1閻ㄥ嫬鐤勬宀娲伴惃鍕簰閸欏﹦绮ㄧ拋鐑樻降妤犲矁鐦夐幋鎴滄粦娑撳﹪娼伴惃鍕鏉╁府绱欓崶鍓у娑撳秴顧勫〒鍛珰閿涘苯鎸ㄩ崗鑸垫Ц閺傚洤鐡ч敍宀鏃辨潪瀵告畱閳ユ窂LEU閳ユ粏顕柈鐣屾暏婢堆冨晸閿涘 Neural machine translation  models have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available.  In this situation, continual training, which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations  in the in-domain data but forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure shows the performance trends on the in-domain and general-domain. %As the size of the training corpus grows, the NMT model is trained in the manner of continual learning  from a stream of data.  %Unfortunately, there usually exists a distribution bias in large data set especially when the data is collected from different domains. In this situation, the NMT model has a tendency towards over-fitting to frequent observations  in the newly added data, but forgetting previously learned patterns from the old data, leading to poor performance in the old data.  In the example of domain adaptation shown in Figure, as the training goes, the performance surges for in-domain while slides fast for general-domain. This phenomenon is the catastrophic forgetting of neural network.  %when there are large amounts of parallel training sentences available. However, similar to many other successful neural network-based methods, it also has limited continual learning ability to learn from a stream of training data, which could have different distributions . It is because the NMT system suffers from catastrophic forgetting which refers to that model has a tendency towards over-fitting to frequent observations  in newly added training data, but forgetting previously learned features in the old data.   %Figure denotes this phenomenon in NMT.  %1Yang 娑撳娼版潻娆愵唽閸樼粯甯 %Improving the continual learning ability of the NMT system is of significant importance both in theory and practice. From the artificial intelligence perspective, it can be seen as another step towards the grand goal of creating a real intelligent translation system that can learn continuously new translation skills without forgetting old knowledge as a human does. From a practical perspective, it enables the model to update the model with only recent new data to improve the model's overall performance. We don't need to retrain the model from scratch which is very time-consuming. Moreover, considering that a well-trained model maybe is already deployed in an application, the original training data may not be available at that time. Therefore it is very necessary to improve the continual learning ability of the NMT system.      %1Yang 閺堫剚顔屽楦款唴婵″倹妲搁幓蹇氬牚閿涙氨浼ㄩ梾鐐囦粣韫囨ü绔撮惄瀛樻Ц缁佺偟绮＄純鎴犵捕鐠侇厾绮屾稉顓犳畱娑撴径褔姣︽０姗堢礉閾忕晫鍔ч惄顔煎瀹歌尙绮￠張澶夌娴滄稑浼愭担婊嗗毀閸旀稐绨憴锝呭枀鏉╂瑤閲滈梻顕顣介敍灞借嫙缂佹瑥鍤禍鍡曠娴滄稖顢戞稊瀣箒閺佸牏娈戠憴锝呭枀閺傝纭堕敍鍫濈穿閻€劎娴夐崗铏瀮閻氼噯绱氶敍灞肩稻閺勵垳娲伴崜宥呰嫙濞屸剝婀佸銉ょ稊閸樼粯甯扮槐銏犳躬閻忛箖姣﹂幀褔浠愯箛妯跨箖缁嬪鑵戦崘鍛村劥閺佺増宓侀惃鍕綁閸栨牗鍎忛崘纰夌礉鏉╂瑧顫掗幒銏㈠偍娴兼碍婀侀崝鈺绨幋鎴滄粦閻炲棜袙閻忛箖姣﹂幀褔浠愯箛妯哄絺閻㈢喓娈戦崢鐔锋礈楠炲爼鍣伴崣鏍祲鎼存梻娈戦幒顏呮煢閵  Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning.  ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains.   introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved.  , , and  propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain. All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don't know what happened inside the model during continual training and why these methods can alleviate the catastrophic forgetting problem. The study on these can help to understand the working mechanism of continual training and inspire more effective solutions to the problem in return.    %Catastrophic forgetting is a long-known problem in the training of neural networks. Some researchers have managed to alleviate this problem with different strategies, such as changing the model structure, adding an extra regularization term, employing complementary learning systems  theory-based strategies and so on. However, to the best of our knowledge, these methods mainly focus on how to solve the problem, not what causes the problem. %Understanding the cause of the problem will inspire effective solutions. %, there is still no work trying to figure out the inner reason for catastrophic phenomenon and no direct evidence to show the change of model parameters in NMT. We believe that the attempt to understand this phenomenon can help us adopt appropriate measures to solve this problem.  %it is still not clear what happens during the continual learning process and what causes catastrophic forgetting indeed.  %1Yang 閺堫剚顔岄崣顖欎簰鏉╂瑦鐗遍崘娆欑窗閸︺劍婀伴弬鍥风礉閹存垳婊戠亸婵婄槸閸︺劑顣崺鐔诲殰闁倸绨查惃鍕攱閺嬫湹绗呴崢缁樺赴缁鳖晼arameters閸滃瞼浼ㄩ梾鐐囦粣韫囨娈戦崗宕囬兇閿涘苯鑻熼崚鑽ゆ暰閸戠皢arameters閸︺劎浼ㄩ梾鐐囦粣韫囨ǹ绻冪粙瀣╄厬閻ㄥ嫬褰夐崠鏍Ъ閸旇￥鍌欒礋娴滃棜鎻崚鎷岀箹娑擃亞娲伴惃鍕剁礉閹存垳婊戦柅姘崇箖Absolute value閸滃瓗IM閺夈儴鐦庢导鏉垮棘閺佹澘婀Ο鈥崇风拋顓犵矊娑擃厾娈戦柌宥堫洣閹嶇礄閸欏倽鍐╂瀮閻氼噯绱氶敍灞借嫙闁俺绻冮崣鍌涙殶閹匡箓娅庨惃鍕煙濞夋洘娼甸幒銏㈠偍鏉╂瑤绨洪崡鏇熸殶鐎靛湱鐐曠拠鎴炑嗗厴閻ㄥ嫬濂栭崫宥冨倿姘崇箖鐎圭偤鐛欑紒鎾寸亯閿涘本鍨滄禒顒褰傞悳鏉款嚠娴滃酣姘辨暏妫板棗鐓欓柌宥堫洣閻ㄥ嫬寮弫鏉款嚠娴滃穼n-domain娴犲秶鍔у鍫ュ櫢鐟曚緤绱濋懓灞芥躬妫板棗鐓欓懛顏堝倸绨查惃鍕箖缁嬪鑵戞潻娆庣昂閸欏倹鏆熼惃鍕綁閸栨牕绶㈡径褋鍌氱唨娴滃氦绻栨禍娑樺絺閻滃府绱濈电懓绨叉禍搴ょ槑娴兼澘寮弫浼村櫢鐟曚焦褏娈戞稉銈囶潚閺傝纭堕敍灞惧灉娴狀剛娴夋惔鏃傛畱閹绘劕鍤稉銈囶潚閺傝纭堕弶銉﹀付閸掓儼绻栨禍娑㈠櫢鐟曚胶娈戦崣鍌涙殶閸︺劑顣崺鐔诲殰闁倸绨查惃鍕箖缁嬪鑵戞稉宥勭窗閸欐ê瀵叉潻鍥с亣閿涘矁宀娼冮柌宥勭艾鐠嬪啯鏆ｉ柇锝勭昂娑撳秹鍋呮稊鍫ュ櫢鐟曚胶娈戦崣鍌涙殶閵嗗倸鐤勬宀绮ㄩ弸婊嗐冮弰搴㈠灉娴狀剛娈戦弬瑙勭《閼宠棄婀穱婵婄槈in-domain閺佺増宓佹稉濠勬畱缂堟槒鐦ч幀褑鍏橀崣妯哄娑撳秵妲戦弰鍓ф畱閹懎鍠屾稉瀣亣楠炲懎瀹抽惃鍕絹妤傛﹢姘辨暏妫板棗鐓欓惃鍕倳鐠囨垶褑鍏橀妴  %Given this, we seek to understand the relationship between catastrophic forgetting phenomenon and model parameters under the task of domain adaptation. More specifically, we aim to figure out the trend of model parameters during catastrophic forgetting. To fulfill this goal, we propose two methods to evaluate the importance of the model parameters. The first is to use the absolute value of model parameters and the second is to use the empirical Fisher Information Matrix . To verify the effectiveness and correctness of the proposed methods, we then do parameter erasure experiments. According to the experimental results, we find that some parameters are important for both the general-domain and in-domain. Based on these findings, we try to alleviate catastrophic forgetting by designing learning strategies based on the importance of the parameters. We put more constrains on those important parameters to make them change more conservatively while encourage those less important parameters to change more aggressively during the continual learning process. The experiments on multiple translation tasks show that our methods can improve the translation quality on the new domain without degrading the performance on the old domain too much.  Given above, in this paper, we focus on the catastrophic forgetting phenomenon and investigate the roles of different model parts during continual training. To this end, we explore the model from the granularities of modules and parameters . In the module analyzing experiments, we operate the model in two different ways, by freezing one particular module or freezing the whole model except for this module. We find that different modules preserve knowledge for different domains. In the parameter analyzing experiments, we erase parameters according to their importance which is evaluated by the Taylor expansion-based method . According to the experimental results, we find that some parameters are important for both of the general-domain and in-domain and meanwhile they change greatly during domain adaptation which may result in catastrophic forgetting.  To ensure the validity and reliability of the findings, we conduct experiments over different language pairs and domains.    {0pt} {0pt}           Based on our findings of parameter importance above, we then investigate their changes during the continual learning process. We find that the important parameters for the general-domain translation still play major roles for the in-domain translation by doing another parameter erasure experiments. What's more, the substantial decline of general-domain translation quality and the rise of in-domain translation quality is also due to the change of these parameters.   Finally, based on our findings, we propose some practical methods to overcome the catastrophic forgetting phenomenon by parameter regularization method and learning rate adjustment method based on their importance to the model. We change the important parameters slightly while changing the less important parameters more aggressively. The results show that our approach can alleviate catastrophic forgetting significantly.      Our work indicates that some parameters are more important than others and the change of these parameters can influence translation results a lot. Therefore, we can try to alleviate catastrophic forgetting by designing different learning strategies based on the importance of the parameters. As far as we know, this is the first work trying to analyze the catastrophic forgetting phenomenon in NMT. Moreover, the analyzing methods we put forward in this work are task-independent and can be applied to other neural network-based methods in other tasks. \fi \iffalse extra space to store all the old training data or even retrain from scratch with the and without storing old training data or even retraining with   This work focuses on the domain adaptation problem of NMT which is a special case of the continual learning scenario of the neural network. They share the same training task but the distribution of the training data is different.  Domain adaptation deals with the problem of improving the performance of a model trained on a general domain data over test instances from a new domain. In such a scenario, we usually have large amounts of general-domain training data and a welled trained model based on it. In contrast, we only have a limited number of in-domain training data which will lead the NMT system to overfit soon and perform poorly when only trained with these data. Some researchers solve this problem by combining the training data from the general-domain and in-domain together and train a new system from scratch. They usually make use of the domain information to improve the translating performance by adding domain labels to training data or using domain discriminator to find the domain invariant features. On the one hand, these methods are very time consuming and need extra space to store all the training data which is not efficient in real-life applications. On the other hand, due to the relatively small size of in-domain data, it will lead the model to overfit the general-domain data which has been observed in the results.   Fine-tuning is a fast and efficient method for continual learning of neural networks which has already been applied for NMT. NMT system is first trained on general-domain data and then further trained on in-domain data.   Domain adaptation is the most common application scenario of continual learning in NMT which has drawn much attention recently. Under this scenario, we   The translation quality drops quickly when the distribution of the training data changes. It suffers a catastrophic forgetting in the continual training process. \fi       In this work, we focus on the catastrophic forgetting phenomenon of NMT and aim to find the inner reasons for this. Under the background of domain adaptation, we propose two analyzing methods from the perspectives of modules and parameters  and conduct experiments across different language pairs and domains. We find that some modules tend to maintain the general-domain knowledge while some modules tend to adapt to the in-domain; we also find that some parameters are more important for both the general-domain and in-domain translation and the change of them brings about the performance decline in general-domain. Based on our findings, we have proposed several ideas that may help improve the vanilla continual training method. We will prove the effectiveness of these ideas in future work.      The investigation on the different modules of the NMT model showed that some modules are of higher capacity to preserve the general-domain knowledge while some other modules are more essential for adapting to the in-domain; the investigation on the parameters showed that some parameters are important for both the general domain and in-domain translation and the change of them brings about the performance decline in general-domain.    We put forward two methods for evaluating the importance of parameters, and we find that some parameters play an essential role in both general-domain and in-domain. Then we find that the change of important parameters brings about the performance decline in general-domain through a series analyzing experiments.  Finally, based on our analysis, we propose an importance evaluation based method to alleviate catastrophic forgetting and the experimental results of different languages and domains prove the effectiveness of the method.    
","  %Neural machine translation  always suffers catastrophic forgetting during the continual learning process which means the model tends to forget all its previously learned knowledge when further trained with new data with different distributions, like from different domains or languages. However, it is not clear what happens during this process and what causes this phenomenon. More specifically, it is not clear whether this is due to the overall change of the model or the impact of certain parameters. In this work, we focus on the domain adaptation task of NMT under the continual learning scenario. First, we put forward two ways for evaluating the importance of the parameters and show that the translation quality mainly dependents on the most important parameters of the model. Then we analyze the behavior of the parameters according to their importance for the model during the continual learning process and it shows that the important parameters for the general-domain translation still play major roles for the in-domain translation after the continual learning process. What's more, the catastrophic forgetting phenomenon, shown as the substantial decline of general-domain translation quality with the rise of in-domain translation quality,  is mainly due to the change of these important parameters.  Finally, we propose some practical methods to overcome the catastrophic forgetting by controlling the updates of parameters differently based on their importance.    Neural machine translation  models usually suffer from catastrophic forgetting during continual training where the models tend to gradually forget previously learned knowledge and swing to fit the newly added data which may have a different distribution, e.g. a different domain. Although many methods have been proposed to solve this problem, we cannot get to know what causes this phenomenon yet. Under the background of domain adaptation, we investigate the cause of catastrophic forgetting from the perspectives of modules and parameters . The investigation on the modules of the NMT model shows that some modules have tight relation with the general-domain knowledge while some other modules are more essential in the domain adaptation. And the investigation on the parameters shows that some parameters are important for both the general-domain and in-domain translation and the great change of them during continual training brings about the performance decline in general-domain. We conduct experiments across different language pairs and domains to ensure the validity and reliability of our findings.    %by tracing parameter variation in this progress and depict the influence of different model modules.  %and depict the relationship between them so that we can work out solutions to the catastrophic forgetting problem based on these findings.  %Under the background of domain adaptation for machine translation, we found that some parameters play an essential role in both general domain and in-domain translation and the change of them brings about the performance decline in general-domain. Based on these findings, we propose a solution to detect these important parameters and accordingly suppress their fluctuation during domain adaptation. Experimental results prove  %that our method can greatly improve the translation quality in in-domain and meanwhile minimize the negative influences on general-domain translation.",276
"  Recurrent neural network architectures have demonstrated remarkable success in natural language processing, achieving state of the art performance across an impressive range of tasks ranging from machine translation to semantic parsing to question answering . These tasks demand the use of a wide variety of computational processes and information sources , and are evaluated in coarse-grained quantitative ways. As a result, it is not an easy matter to  identify the specific strengths and weaknesses in a network's solution of a task.    In this paper, we take a different tack, exploring the degree to which neural networks successfully master one very specific aspect of linguistic knowledge: the interpretation of sentences containing reflexive anaphora.  We address this problem in the context of the task of semantic parsing, which we instantiate as mapping a sequence of words into a predicate calculus logical form representation of the sentence's meaning. \pex<ex:transform>     \a Mary runs       \a John sees Bob  \mary\run\to  as its subject, the reflexive is interpreted as , and with \lex{Alice} as its subject it is interpreted as .  However, such piecemeal learning of reflexive meaning will not support generalization to sentences involving a subject that has not been encountered as the antecedent of a reflexive during training, even if the interpretation of the  subject has occurred elsewhere. What is needed instead is an interpretation of the reflexive that is characterized not as a specific  output token, but rather as an abstract instruction to duplicate the interpretation of the subject. Such an abstraction requires more than the ``jigsaw puzzle"" approach to meaning that simpler sentences afford.    argues that this kind of abstraction, which he takes to require the use of  algebraic variables to assert identity, is beyond the capacity of recurrent neural networks.  's demonstration involves a simple recurrent network  language model that is trained to predict the next word over a corpus of sentences of the following form: \pex     \a A rose is a rose.     \a A mountain is a mountain. %    \a A car is a car \xe All sentences in this training set have identical subject and object nouns.   shows, however, that the resulting trained network does not correctly predict the subject noun when tested with a novel preamble `\lex{A book is a }'. Though intriguing, this demonstration is not entirely convincing: since the noun occurring in the novel preamble, \lex{book} in our example, did not occur in the training data, there is no way that the network could possibly have known which  output should correspond to the reflexive for a sentence containing the novel  subject noun, even if the network did successfully encode an identity relation between subject and object.    explore a related  task in the context of SRN interpretation of reflexives. In their experiments, SRNs were trained to map input words to corresponding semantic symbols that are output on the same time step in which a word is presented. For most words in the vocabulary, this is a simple task: the desired output is a constant function of the input .  For reflexives however, the target output depends on the subject that occurs earlier in the sentence. \ tested the network's ability to interpret a reflexive in sentences containing a subject that had not occurred as a reflexive's antecedent during training. However, unlike Marcus' task, this subject and its corresponding semantic symbol did occur in other  contexts in the training data, and therefore was in the realm of possible inputs and outputs for the network. Nonetheless, none of the SRNs that they trained succeeded at this task for even a single test example.   Since those experiments were conducted, substantial advances have been made on recurrent neural network architectures, some of which have been crucial in the success of practical NLP systems.       : More sophisticated recurrent units like LSTMs  and GRUs  have been shown to better encode preceding context than SRNs.     : The performance of network models that transduce one string to another, used in machine translation and semantic parsing, has been greatly improved by the use of independent encoder and decoder networks  .     : The ability of a network to produce contextually appropriate outputs even in the context of novel vocabulary items has been facilitated by content-sensitive attention mechanisms .    These innovations open up the possibility that modern network architectures may well be able to solve the variable identity problem necessary for mapping reflexive sentences to their logical form. In the experiments we describe below, we explore whether this is the case.         Because of their abstract meaning, reflexive anaphora present a distinctive challenge for semantic parsing that had been thought to be beyond the capabilities of recurrent networks. The experiments described here demonstrate that this was incorrect. Sequence-to-sequence networks with a range of recurrent unit types are in fact capable of learning an interpretation of reflexive pronouns that generalizes to novel antecedents. Our results also show  that such generalization is nonetheless contingent on the appearance of the held-out  antecedent in a variety of syntactic positions as well as the diversity of antecedents providing support for the reflexive generalization. Additionally successful generalization depends on the  network architecture in ways that we do not fully understand. It is at present unknown whether the demands that any of these architecture impose on the learning environment for successful learning of reflexives are consistent with what children experience, but this could be explored with both corpus and experimental work.  Future work will also be necessary to elucidate the nature of the networks' representations of reflexive interpretation and to understand how they support lexical generalization .    The question we have explored here is related to, but distinct from, the issue of systematicity , according to which pieces of representations learned in distinct contexts can freely recombine.  This issue has been addressed  using sequence-to-sequence architectures  in recent work with the synthetic SCAN robot command interpretation dataset  and on  language modeling , in both cases with limited success.  One aspect of the SCAN domain that is particularly relevant to reflexive interpretation is commands involving adverbial modifiers such as \lex{twice}.  Commands like \lex{jump twice} must be interpreted by duplicating the meaning of the verb, i.e., as , which is similar to what we require for the interpretation of the reflexive object, though in a way that does not require sensitivity to syntactic structure that we have not explored here. Recently, ,  and  have proposed novel architectures that increase systematic behavior, and we look forward to exploring the degree to which these impact performance on reflexive interpretation.   Our current work has focused  exclusively on recurrent networks, ranging from  SRNs to GRUs and LSTMs. Recent work by  shows that Transformer networks  attain superior performance on a variety of sequence-to-sequence tasks while dispensing with recurrent units altogether. Examining both the performance and training characteristics of Transformers will allow us to compare the effects of attention and recurrence on the anaphora interpretation task. This is especially interesting given the impact that attention had on performance in our experiments.  Finally, while our current experiments are revealing about the capacity of recurrent networks to learn generalizations about context-sensitive interpretation, there are nonetheless limited in a number of respects because of simplifications in the English fragment we use to create our synthetic data. Reflexives famously impose a structural requirement on their antecedents . In the following example, the reflexive's antecedent must be  and cannot be . \ex The student near the teacher sees herself    \xe We do not know whether the architectures that have succeed on our experiments would do similarly well if the relevant generalization required reference to  structure. Past work has explored the sensitivity of recurrent networks to hierarchical structure, with mixed results . In ongoing work, we are exploring this question by studying  more complex synthetic domains both with the kind of recurrent sequence-to-sequence network used here as well networks that explicitly encode or decode sentences in a hierarchical manner.   A second simplification concerns the distribution of reflexives themselves. English reflexives can appear in a broader range of syntactic environments apart from transitive objects . It would be of considerable interest to explore the reflexive interpretation in a naturalistic setting that incorporate this broader set of distributions.   
"," Reflexive anaphora present a challenge for semantic interpretation: their meaning varies depending on context in a way that appears to require abstract variables. Past work has raised doubts about the ability of recurrent networks to meet this challenge. In this paper, we explore this question in the context of a fragment of English that incorporates the relevant sort of contextual variability. We consider sequence-to-sequence architectures with recurrent units and show that such networks are capable of learning semantic interpretations for reflexive anaphora which generalize to novel antecedents. We explore the effect of attention mechanisms and different recurrent unit types on the type of training data that is needed for success as measured in two ways: how much lexical support is needed  to induce an abstract reflexive meaning  and what contexts must a noun phrase occur in to support generalization of reflexive interpretation to this noun phrase?",277
"  Pre-trained contextualized language models such as BERT are state-of-the-art for a wide variety of natural language processing tasks. Similarly, in Information Retrieval , these models have brought about large improvements in the task of { pretrained language models are effective for ad-hoc ranking. What new aspects of the task do neural models solve that previous approaches do not?  Previous work has shown that traditional IR axioms, e.g. that increased term frequency should correspond to higher relevance, do { its ranking score, and adding non-relevant content can increase it---despite document length itself having a limited effect on the ranking scores. %  In summary, we present a new framework  for performing analysis of ad-hoc ranking models. We then demonstrate how the framework can provide insights into ranking model characteristics by providing the most comprehensive analysis of neural ranking models to date. Our released software framework facilitates conducting further analyses in future work.                     We presented a new framework }  {final versions\xspace} {Final version\xspace} {Final versions\xspace} {Final Versions\xspace} {submission\xspace} {{\taclpaper}s\xspace} {Submission\xspace} {{\Taclpaper}s\xspace} {Submissions\xspace} \fi   }} }} ^\mathbf{\ddagger} \\ {   \\      IR Lab, Georegetown University, Washington, DC \\      Allen Institute for AI, Seattle, WA \\   {\tt@ir.cs.georgetown.edu} \\   {\tt@allenai.org} }  \date{}          
"," Numerous studies have demonstrated the  effectiveness of pretrained contextualized language models such as BERT and T5 for ad-hoc search. However, it is not well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have.  We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs , which includes new types of diagnostic tests that allow us to probe several characteristics---such as sensitivity to word order---that are not addressed by previous techniques.  To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit.  We find evidence that recent neural ranking models have fundamentally different characteristics from prior ranking models. For instance, these models can be highly influenced by altered document word order, sentence order and inflectional endings. They can also exhibit unexpected behaviors when additional content is added to documents, or when documents are expressed with different levels of fluency or formality. We find that these differences can depend on the architecture and not just the underlying language model.\footnote{\url{https://github.com/allenai/abnriml}}",278
"  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Commonsense is the knowledge shared by the majority of people in society and acquired naturally in everyday life. Commonsense reasoning is the process of logical inference by using commonsense information. Commonsense to answer the questions that is ``'' in Figure  is depicted as: ``'', ``'', and ``.'' An enormous amount of pre-defined commonsense knowledge is available and people can make inferences using this commonsense such as in the following example: ``''  ``''  ``''  ``'' This chain of commonsense reasoning is naturally deduced by humans without substantial difficulty. Whereas people acquire commonsense in their lives, machines cannot learn this knowledge without any assistance. A large amount of external knowledge and several reasoning steps are required for machines to learn commonsense. In recent years, various datasets  have been constructed to enable machines to reason commonsense.    is one of the most widely researched datasets and is presented in Figure  . The studies of commonsense reasoning based on this dataset can be categorized into two mainstream approaches. The first approach uses pre-trained language models with distributed representations, which exhibit high performances on most Natural Language Processing  tasks. However, despite their high performance, these models must be trained with an excessive number of parameters and cannot explain the process of commonsense reasoning. The second approach is reasoning with a commonsense knowledge graph. The generally used commonsense knowledge graph is ConceptNet 5.5 , which includes parsed representation from Open Mind Commonsense  and other different language sources such as WordNet  or DBPedia . In this approach, the subgraph of  ConceptNet corresponding to the questions are transformed into node embeddings by the graph encoder. The candidate with the highest attention score is selected as an answer that is computed between the node embeddings and the word vectors from the language models. To learn the commonsense knowledge that is not observed or understood by the language models, relations from ConceptNet serve as a critical role in this method. The performance is improved by utilizing the relations that are not represented in the text; however, the interpretation of the question is still not enough.   Unlike , the most commonly used method of solving this problem is Knowledge-Based Question-Answering   employing semantic representations. As this method infers the answer with the logical structure of the question using the knowledge base, the question-answering process can be explained in a logical form. In our work, Abstract Meaning Representation  , which is one of the logical structure, is used to understand the overall reasoning process, from the question to the answer.  AMR is a graph for meaning representation that symbolizes the meaning of sentences. AMR illustrates ``who is doing what to whom'' that is implied in a sentence with a graph.  The components of these graphs are not the words, but rather the concepts and their relations. Each concept denotes an event or an entity, and each relation represents the semantic role of the concepts.   In this paper, we enable the language models to exploit the AMR graph to understand the logical structure of sentences. However, it is difficult to infer commonsense information with only an AMR graph, owing to its deficiency of commonsense knowledge of the given sentence. For example, in Figure  , the AMR graph indicates the path of the logical structure of the sentence ``'' ; in other words, these paths from the single AMR graph lack the proficient information to predict the right answer. Therefore, for commonsense reasoning, dynamic interactions between the AMR graph and ConceptNet are inevitable to reach the correct answer.   Thus, we propose a new compact AMR graph expanded with the ConceptNet's commonsense relations with pruning, and it is called ACP graph. The proposed method can interpret the path from the question to the answer by performing commonsense reasoning within the connected graph, such as ``'' .    The contributions of our study are as follows.        The remainder of this paper is organized as follows. In Section 2, we present the entire process of our method in detail. The experimental setup and results are explained in Section 3. A discussion of the proposed model is provided in Section 4, and Section 5 presents the conclusions. Appendix A provides related works including ConceptNet, previous works on commonsense reasoning, and AMR.        In some cases of failure, our model exhibits two problems as follows:  As the concept node illness disappeared while generating the graph, our model may not have enough information for extracting the subgraph from ConceptNet.         The red edges in Figure  present the paths that have high attention weight for the question ``What home entertainment equipment requires cable?'' In Figure  , the top four paths with high attention weights are described. As opposed to predicting the answers simply with the ConceptNet graph connected to the question, we allow our model to learn relevant paths inherent in the ACP graph. That is, our graph path learning module with ACP graph is capable of commonsense reasoning exploring the paths.      We introduce a new commonsense reasoning method, using the proposed ACP graph. This method outperformed the model that simply learns the ConceptNet graph. Furthermore, our method can explain the answer-inference process by interpreting the logical structure of the sentences within commonsense reasoning process. Models that applied our method exhibit higher performance compared to the  previous models. However, certain problems still remain. Though the relations ARG0 and ARG1 occupy most of the core roles in the AMR graph, it is still arguable that the other choice of relations may lead to better results. Therefore, we will show the experimental results according to the different pruning rules on the CommonsenseQA task in the future. Also, we plan to develop an end-to-end learning model that incorporates the AMR generation model and the question-answering model to reduce the error propagation from the AMR generation.   This work was supported by Institute for Information \& communications Technology Planning \& Evaluation grant funded by the Korea government . Also, this research was supported by the MSIT, Korea, under the ITRC support program supervised by the IITP          The acknowledgements should go immediately before the references.  Do   not number the acknowledgements section. Do not include this section   when submitting your paper for review.    include your own bib file like this:    In ConceptNet , real-world assertions are represented as two nodes and directed edges, which denote certain concepts and their relations, respectively. The nodes represent words or phrases from natural language sentences. The edges represent the relations between nodes, and they contain lexical as well as commonsense relation information. As ConceptNet is created by collecting data from various types of knowledge bases, nodes of different types also exist. Each node represents a slightly different meaning considering its role in the sentence. For example, the word ``person'' can be found in the concept of ``person/n,'' which is analyzed as a noun with a POS tagger, and with more detailed semantic information, it can be identified as ``person/n/wn/body.'' This information makes possible the detailed extraction of knowledge that considers the purpose of each sentence. Meanwhile, one or more edges may be defined between two nodes. For example, the edge between the nodes ``person'' and ``eat'' can be defined independently as ``CapableOf'' and ``Desires.'' Various concepts and their relations are defined as nodes and edges in ConceptNet, considering the ambiguity in the sentences.   Commonsense reasoning.  Commonsense reasoning is the process of logical inference by using commonsense information. In CommonsenseQA\footnote[1]{https://www.tau-nlp.org/csqa-leaderboard} task, the fine-tuning approach with pre-trained language representations makes use of external commonsense knowledge. There are two means of exploiting external knowledge. The first\footnote[7]{ttps://drive.google.com/file/d/1sGJBV38aG706EAR75F7LYwCqci9ocG9i/view,\\ https://gist.github.com/commonsensepretraining/507aefddcd00f891c83ebf6936df15e8} is the method that post-trained with some commonsense sentence corpus. It then performs fine-tuning with evidence derived from questions and answers. The second method  is to encode commonsense knowledge graphs and train with language models. The language models that have exhibited high performance in this method are BERT , RoBERTa , which use bidirectional transformer encoders. They also include XLNet , which is based on autoregressive language modeling, ALBERT , which adopts cross-layer parameter sharing and factorized embedding parameterization and ELECTRA  that is pre-trained with Replaced Token Detection  task.  AMR. AMR  represents the relations between concept nodes using the PropBank frameset and vocabularies from the sentences. The edges between two or more concept nodes or the argument nodes are relations. AMR represents semantic roles such as core and numbered roles, and uses more than 100 semantic relations, including negation, conjunction, command, and wikification. In PropBank , the semantic roles are labeled in the form of ARG0$ and ARGM. In general, ARG0 denotes the agent of the verb, ARG1 is the patient, ARG2 means the instrument, benefactive, or attribute, ARG3 is interpreted as the starting point, benefactive, or attribute, and ARG4 represents the ending point. The root node serves as the central point of the representation and is called frame node. Thereafter, other concept nodes are sequentially combined according to the semantic relations. AMR consists of concept nodes in a single graph that is traversable to all nodes, similar to a parse tree. However, unlike the parse tree, which represents the explicit structure of sentences, AMR aims to describe the conceptual and semantic structure. That is, if the semantic meanings of explicitly different sentences are the same, they can be represented by the same AMR graph. For example, the two sentences ``The boy is a hard worker'' and ``The boy works hard'' are represented by the same PENMAN graph, namely  :manner ). The data constructed to generate and evaluate these representations are AMR 2.0  and AMR 1.0 . The model with the highest performance on these data was presented by Zhang et al. , using BERT. Various NLP fields have exploited AMR, such as sentence generation , summarization , question and answering , dialogue systems , paraphrase detection , and biomedical text mining .  
"," CommonsenseQA is a task in which a correct answer is predicted through commonsense reasoning with pre-defined knowledge. Most previous works have aimed to improve the performance with distributed representation without considering the process of predicting the answer from the semantic representation of the question. To shed light upon the semantic interpretation of the question, we propose an AMR-ConceptNet-Pruned  graph. The ACP graph is pruned from a full integrated graph encompassing Abstract Meaning Representation  graph generated from input questions and an external commonsense knowledge graph, ConceptNet . Then the ACP graph is exploited to interpret the reasoning path as well as to predict the correct answer on the CommonsenseQA task. This paper presents the manner in which the commonsense reasoning process can be interpreted with the relations and concepts provided by the ACP graph. Moreover, ACP-based models are shown to outperform the baselines.",279
"  Part-Of-Speech  tagging is a crucial step for language understanding, both being used in automatic language understanding applications such as named entity recognition  and question answering , but also being used in  language understanding by linguists who are attempting to answer linguistic questions or document less-resourced languages .   Much prior work  on developing high-quality POS taggers uses neural network methods which rely on the availability of large amounts of labelled data. However, such resources are not readily available for the majority of the world's 7000 languages .  Furthermore, manually annotating large amounts of text with trained experts is an expensive and time-consuming task, even more so when linguists/annotators might not be native speakers of the language.  [t]   Active Learning  is a family of methods that aim to train effective models with less human effort and cost by selecting such a subset of data that maximizes the end model performance. While many methods have been proposed for AL in sequence labeling , through an empirical study across six typologically diverse languages we show that within the same task setup these methods perform inconsistently. Furthermore, even in an  scenario  %  where we have access to the true labels during data selection, existing methods are far from optimal.  We posit that the primary reason for this inconsistent performance is that while existing methods consider uncertainty in predictions, they do not consider the  of the uncertainty with respect to the output labels. For instance, in Figure  we consider the German token ``die,'' which may be either a pronoun  or determiner . According to the initial model , ``die'' was labeled as PRO majority of the time, but a significant amount of probability mass was also assigned to other output tags  for many examples. Based on this, existing AL algorithms that select uncertain tokens will likely select ``die'' because it is frequent and its predictions are not certain, but they may select an instance of ``die'' with  a gold label of PRO or DET. Intuitively, because we would like to correct errors where tokens with true labels of DET are mis-labeled by the model as PRO, asking the human annotator to tag an instance with a true label of PRO, even if it is uncertain, is not likely to be of much benefit.  Inspired by this observation, we pose the problem of AL for part-of-speech tagging as selecting tokens which maximally  between the output tags. For instance, in the example we would attempt to pick a token-tag pair ``die/DET'' to reduce potential errors of the model over-predicting PRO despite its belief that DET is also a plausible option. We demonstrate the features of this model in an oracle setting where we know true model confusions , and also describe how we can approximate this strategy when we do not know the true confusions.  We evaluate our proposed AL method by running simulation experiments on six typologically diverse languages namely German, Swedish, Galician, North Sami, Persian, and Ukrainian, improving upon models seeded with cross-lingual transfer from related languages . In addition, we conduct human annotation experiments on Griko, an endangered language that truly lacks significant resources.   Our contributions are as follows: [leftmargin=*,nolistsep,noitemsep]      setting.               % File tacl2018v2.tex % Sep 20, 2018  % The English content of this file was modified from various *ACL instructions % by Lillian Lee and Kristina Toutanova % % LaTeXery is mostly all adapted from acl2018.sty.  \documentclass[11pt,a4paper]{article} \usepackage{times,latexsym} \usepackage{url} \usepackage[T1]{fontenc} \usepackage{amsmath} \usepackage{amssymb} \usepackage{tabularx} \usepackage{mathtools} \usepackage{booktabs} \usepackage{url} \usepackage{longtable} \usepackage{tabu} \usepackage{multirow} \usepackage{amsfonts} \usepackage{tabu} \usepackage{algorithm} \usepackage{bbm} \usepackage{subfigure} \usepackage[noend]{algpseudocode} \usepackage[normalem]{ulem} \usepackage{enumitem} \makeatletter  \def\BState{\State \usepackage{xcolor} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}   %% Package options: %% Short version: ""hyperref"" and ""submission"" are the defaults. %% More verbose version: %% Most compact command to produce a submission version with hyperref enabled %%    \usepackage[]{tacl2018v2} %% Most compact command to produce a ""camera-ready"" version \usepackage[acceptedWithA]{tacl2018v2} %% Most compact command to produce a double-spaced copy-editor's version %\usepackage[acceptedWithA]{tacl2018v2} % %% If you need to disable hyperref in any of the above settings  in the TACL instructions), add "",nohyperref"" in the square %% brackets.  %\usepackage[nohyperref]{tacl2018v2}  %%%% Material in this block is specific to generating TACL instructions \usepackage{xspace,mfirstuc,tabulary} {Sept. 20, 2018} [1]{\textcolor{magenta}{[1]{\textcolor{blue}{[1]{{ \renewcommand{\anonsubtext}{}  {final versions\xspace} {Final version\xspace} {Final versions\xspace} {Final Versions\xspace} {submission\xspace} {{\taclpaper}s\xspace} {Submission\xspace} {{\Taclpaper}s\xspace} {Submissions\xspace} \fi  %%%% End TACL-instructions-specific macro block %%%%  \title{Reducing Confusion in Active Learning for Part-Of-Speech Tagging}  \author{Aditi Chaudhary\textsuperscript{1},      Antonios Anastasopoulos\textsuperscript{2,\Thanks{ Work done at Carnegie Mellon University.}},      Zaid Sheikh\textsuperscript{1}, Graham Neubig\textsuperscript{1} \\   \textsuperscript{1}Language Technologies Institute, Carnegie Mellon University\\   \textsuperscript{2}Department of Computer Science, George Mason University\\   { @cs.cmu.edu}}    { }  }    \date{}     Active learning  uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech  taggers. Existing AL heuristics are generally designed on the principle of selecting  yet  training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages , we found the surprising result that even in an  scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances which . Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin.  We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The  code is publicly released here.    %           We have presented a novel active learning method for low-resource POS tagging which works by reducing confusion between output tags. Using simulation experiments across  six typologically diverse languages, we show that our confusion-reducing strategy achieves higher accuracy than existing methods. Further, we test our approach under a true setting of active learning where we ask linguists to document POS information for an endangered language, Griko. Despite being unfamiliar with the language, our proposed method achieves performance gains over the other methods in most iterations. For our next steps, we plan to explore the possibility of adapting our proposed method for complete morphological analysis, which poses an even harder challenge for AL data selection due to the complexity of the task. 
"," Active learning  uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech  taggers. Existing AL heuristics are generally designed on the principle of selecting  yet  training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages , we found the surprising result that even in an  scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances which . Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin.  We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The  code is publicly released here.\footnote{\url{https://github.com/Aditi138/CRAL}}",280
" With an increasing submission of academic papers in recent years, the task of making final decisions manually incurs significant overheads to the program chairs, it is desirable to automate the process.  In this study, we aim at utilizing document-level semantic analysis for paper review rating prediction and recommendation.  Given the reviews of each paper from several reviewers as input, our goal is to infer the final acceptance decision for that paper and the reviewers' evaluation with respect to a numeric rating .  Paper review rating prediction and recommendation is a practical and important task in AI applications which will help improve the efficiency of the paper review process. It is also intended to enhance the consistency of the assessment procedures and outcomes, and to diversify the paper review process by comparing human recommended rating with machine recommended rating.  In the literature, most of existing studies cast review rating prediction as a multi-class classification/regression task .  They build a predictor by using supervised machine learning models with review texts and corresponding ratings.  Due to the importance of features, most researches focus on extracting effective features such as context-level features  and user features  to boost prediction performance.  However, feature engineering is time-consuming and labor-intensive.   Recently, with the development of neural networks and its wide applications, various deep learning-based models have been proposed for automatically learning features from text data .  Existing deep learning models usually learn continuous representations of different grains  from text corpus .  Although deep learning models can automatically learn extensive feature representation, they cannot efficiently capture the hierarchical relationship inherent to the review data.  To address this problem,  studied a hierarchical architecture and implemented it in deep learning framework to learn a better document-level representation.  Also, with the success of attention mechanism in many tasks such as machine translation, question answering and so on ,   designed a directional self-attention network to gain context-aware embeddings for words and sentences.  Despite great progress made by these models, they do not focus on the task of paper review rating recommendation and are not effective enough to be directly used for this task because of the following reasons: First, the review data is hierarchical in nature.  There exists a three-level hierarchical structure in the review data: word level, intra-review level and inter-review level, while previous models only capture two-levels  of this hierarchy.  Second, paper reviews are usually much longer than other reviews , while most of these models are working on those shorter reviews stated above and they do not leverage the up to date representation techniques such as BERT  and SciBERT .   In this paper, we propose a novel neural network framework for paper review rating recommendation by taking word, intra-review and inter-review information into account.  Specifically, inspired by HAN  and DiSAN , we introduce a Hierarchical Bi-directional self-Attention Network  framework to effectively incorporate different levels of hierarchical information.  The proposed framework consists of three main modules in end-to-end relationship: sentence encoder, intra-review encoder and inter-review encoder, which can consider hierarchical structures of review data as comprehensive as possible. The outputs of inter-review encoder are leveraged as features to build the rating predictor without any feature engineering. We release the code and data collected by us to enable replication and application to new tasks, available at .  The contributions of this work are as follows:       In this paper, a scientific paper review dataset called OpenReview is collected from ICLR openreview website and released. We observe that there is a three-level hierarchical structure in this dataset  -- the information and relationships between reviews of one paper may affect the final decision, and so may relationships between words and sentences in each review. Based on these observations, a hierarchical bi-directional self-attention network  framework is proposed for paper review rating prediction and recommendation that can model the interactions among words, sentences, intra- and inter-reviews in an end-to-end manner. Moreover, considering the imbalanced distribution of different classes  in the review rating prediction task, we design two new metrics to better evaluate models.  It is seen that both experimental results of predicting final decisions for submitted papers and identifying ratings for reviews on two datasets  demonstrate our proposed framework has sufficient ability to capture the hierarchical structures of words, sentences and reviews in the datasets and outperforms other models. In the future, we plan to investigate multi-task learning for paper review rating recommendation.   
"," Review rating prediction of text reviews is a rapidly growing technology with a wide range of applications in natural language processing.  However, most existing methods either use hand-crafted features or learn features using deep learning with simple text corpus as input for review rating prediction, ignoring the hierarchies among data.  In this paper, we propose a Hierarchical bi-directional self-attention Network framework  for paper review rating prediction and recommendation, which can serve as an effective decision-making tool for the academic paper review process. Specifically, we leverage the hierarchical structure of the paper reviews with three levels of encoders: sentence encoder , intra-review encoder  and inter-review encoder .  Each encoder first derives contextual representation of each level, then generates a higher-level representation, and after the learning process, we are able to identify useful predictors to make the final acceptance decision, as well as to help discover the inconsistency between numerical review ratings and text sentiment conveyed by reviewers.  Furthermore, we introduce two new metrics to evaluate models in data imbalance situations.  Extensive experiments on a publicly available dataset  and our own collected dataset  demonstrate the superiority of the proposed approach compared with state-of-the-art methods.",281
"  % What is QG and Why it is important Question Generation  aims to endow machines with the ability to ask relevant and to-the-point questions about a document.  QG has important practical applications, such as  generating assessments for course materials in education, prompting user interaction in dialog systems, enabling machines to ask clarification questions such as FAQs, and automatically building large-scale QA datasets for the research community.   % How tranditional works do it? Recent QG approaches have used Seq2Seq models with attention, which feeds the input document into an encoder, and generates a question about the document through a decoder.  % Why it needs RL? The training objective is to maximize the log likelihood of the ground-truth question paired with each input document using teacher forcing. However, as the ground-truth questions are insufficient to account for the many equivalent ways of asking a question, this likelihood-based training suffers from the problem of exposure bias, i.e., the model does not learn how to distribute probability mass over sequences that are valid but different from the ground truth.  % How RL addresses the problem? %   To address this issue, previous QG works proposed to optimize the model directly on question-specific rewards via Reinforcement Learning .  This process decouples the training procedure from the ground truth data, so that the space of possible questions can be better explored. Moreover, it allows the training to target on specific properties we want the question to exhibit, such as relevant to a specific topic or answerable by the document.  % What is the problem for RL-based method? Although various rewards have been employed for QG --- such as BLEU, the answerability reward, and the word movers distance --- optimizing the reward scores does not always lead to higher question quality in practice, as observed by Hosking and Riedel~. How to define robust and effective QG-specific rewards still requires further investigation.   % What we want to do? We aim to analyze the effectiveness of question-specific rewards in QG. Instead of using general natural language generation metrics such as BLEU, we target three QG-related metrics that are commonly cited in human evaluations of question quality:  Fluency indicates whether the question follows the grammar and accords with the correct logic;  Relevance indicates whether the question is relevant to the document; and  Answerability indicates whether the question is answerable given the document. We design a specific RL reward for each metric: a language model based reward for fluency, a discriminator-based reward for relevance, and a QA-based reward for answerability.  After optimizing each reward via RL, we conduct comprehensive analysis, including automatic and human evaluation, to arrive at the following conclusions:  both individual and joint optimization of these rewards can lead to performance gain in automated metrics, but this does not guarantee an improvement in the real question quality;  the reward for relevance substantially helps to improve the question quality, while the reward for answerability reduces the quality due to the bias brought by the QA model; and  a reward is more likely to improve the question quality if the reward score correlates well with human judgement.     In this paper, we optimize three question-specific rewards via reinforcement learning on a Seq2Seq based question generator, aiming to improve the fluency, relevance and answerability of the generated questions.  Through comprehensive analytic experiments, including automatic and human evaluation, consistency validation, and meso analysis, we show that the effectiveness of a reward is poorly reflected by automatic evaluation metrics such as BLEU. Instead, we find a reward that correlates well with the human judgement generally has better effects on improving the question quality. In future works, we believe these observations can help to guide the design of other QG-specific rewards that target on unexplored aspects of question generation, such as the informativeness and the utility of questions.   
","     Recent question generation  approaches often utilize the sequence-to-sequence framework  to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward.      We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards  that correlate well with human judgement  lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poor question quality. Our code is publicly available at {https://github.com/YuxiXie/RL-for-Question-Generation}.",282
"  % In daily bases plethora of opinion data is published about different topics and in response to different stimuli using Social Media.  % Aiming to analyse and gain insights from opinions posted in social media, research in stance detection has become increasingly popular in recent years. Framed as a classification task, the stance detection consists in determining if a textual utterance expresses a supportive, opposing or neutral viewpoint with respect to a target or topic . Research in stance detection has largely been limited to analysis of single utterances in social media. Furthering this research, the SardiStance 2020 shared task  focuses on incorporating contextual knowledge around utterances, including metadata from author profiles and network interactions. The task included two subtasks, one solely focused on the textual content of social media posts for automatically determining their stance, whereas the other allowed incorporating additional features available through profiles and interactions. This paper describes and analyses our participation in the SardiStance 2020 shared task, which was held as part of the EVALITA  campaign and focused on detecting stance expressed in tweets associated with the Sardines movement. %  %   % For a network interaction graph, we generate user embeddings, using variations of graph neural network  embedding methods, and then concatenate author's vector with its corresponding utterance features for each stance. We also extract two types of text embedding representations for each utterance, embedding-based features, namely word embedding vectors and cosine similarity vectors, using different models including variations of CNN and bidirectional LSTM models. Further, the results of these two feature extraction methods are concatenated for the final classification step. We also consider the standard methods that extract frequency-based representations from author profiles and stance utterances including unigrams and Tfidf vectors. All these four features where combined and fed into the drop out and dense layers, to finally generate the final label using a softmax activation function. Though, we deactivate some of these four sources of features and alter the frequency-based vector by excluding some features, changing the embedding source and reducing the dimensionality for highly dimensional vectors  using PCA.}     In this work, we described a state-of-the-art stance detection system leveraging different features including author profiling, word meaning context and social interactions. Using different random runs, our best model achieved  leveraging deepwalk-based knowledge graphs embeddings, FastText and similarity feature vectors extracted by two multi-headed convolutional neural networks from auther's utterance. This motivates our future, aiming to reduce the model complexity and automate the feature selection process.   
"," This paper presents our submission to the SardiStance 2020 shared task, describing the architecture used for Task A and Task B. While our submission for Task A did not exceed the baseline, retraining our model using all the training tweets, showed promising results leading to  using bidirectional LSTM with BERT multilingual embedding for Task A. For our submission for Task B, we ranked 6th . With further investigation, our best experimented settings increased performance from  to  with same architecture and parameter settings and after only incorporating social interaction features- highlighting the impact of social interaction on the model's performance.",283
"   State-of-the-art for most existing natural language processing  classification tasks is currently achieved by systems that are first pre-trained on auxiliary language modeling tasks and then fine-tuned on the task of interest with cross-entropy loss . Although commonly used, cross-entropy loss -- the KL-divergence between one-hot vectors of labels and the distribution of model's output logits -- has several shortcomings. Cross entropy loss leads to poor generalization performance due to poor margins , and it lacks robustness to noisy labels  or adversarial examples . Effective alternatives have been proposed to change the reference label distributions through label smoothing , Mixup , CutMix , knowledge distillation  or self-training~.  Additionally, it has been recently demonstrated in NLP that fine-tuning using cross entropy loss tends to be unstable , especially when supervised data is limited, a scenario in which pre-training is particularly helpful. To tackle the issue of unstable fine-tuning, recent work proposes local smoothness-inducing regularizers  and regularization methods inspired by the trust region theory  to prevent representation collapse that lead to poor generalization performance. Empirical analysis suggests that fine-tuning for longer, reinitializing top few layers~, and using debiased Adam optimizer during fine-tuning~ can make the fine-tuning procedure more stable.  We are inspired by the learning strategy that humans deploy when given a few examples -- try to find the commonalities between the examples of each class and contrast them with examples from other classes. We hypothesize that a similarity-based loss will be able to hone in on the important dimensions of the multidimensional hidden representations and lead to better few-shot learning results and be more stable while fine-tuning pre-trained models. We propose a novel objective for fine-tuning pre-trained language models that includes a supervised contrastive learning term that pushes examples from the same class close and examples of different classes further apart. The new term is similar to the contrastive objective used for self-supervised representation learning in various domains such as image, speech, and video domains. . In constrast to these methods, however, we use a contrastive objective for supervised learning of the final task, instead of contrasting different augmented views of examples.  Adding supervised contrastive learning  term to the fine-tuning objective improves performance on several natural language understanding tasks from the GLUE benchmark , including SST-2, CoLA, MRPC, RTE, and QNLI over the state-of-the-art models fine-tuned with cross entropy loss. The improvements are particularly strong in few-shot learning settings , and models trained with SCL are not only robust to the noise in the training data, but also have better generalization ability to related tasks with limited labeled data. Our approach does not require any specialized architectures , memory banks , data augmentation of any kind, or additional unsupervised data. To the best of our knowledge, our work is the first to successfully integrate a supervised contrastive learning objective for fine-tuning pre-trained language models.        % \ves{end of alternative}  % State-of-the-art models for most existing natural language processing  tasks are currently learned by fine-tuning pre-trained large language models  that have been shown to capture semantic, syntactic, and world knowledge.  Recent attempts at improving the pre-training stage over masked language modeling~ has led to improvements on natural language understanding tasks, but fine-tuning stage has stayed the same for all downstream NLP classification tasks: add a task-specific output layer to the pre-trained language model and continue training on the labeled task data using cross-entropy loss.  % Cross-entropy loss is the most widely adopted objective for supervised classification models, defined as the KL-divergence between one-hot vectors of labels and the distribution of model's output logits. Although commonly used by the state-of-the-art models across many fields including NLP, there has been several works demonstrating the shortcomings of the cross-entropy loss, showing that it leads to poor generalization performance due to poor margins , and lack of robustness to noisy labels  or adversarial examples . Among the alternative objective functions proposed, more effective approaches in practice have been the ones that change the reference label distributions such as label smoothing , Mixup , CutMix , knowledge distillation  or self-training~.  % Several recent studies show that the fine-tuning procedure is unstable , especially for the case where supervised data is limited, a scenario in which pre-training is particularly helpful. To tackle the issue of unstable fine-tuning, local smoothness-inducing regularizers  and regularization methods inspired by the trust region theory  have been proposed to prevent representation collapse that leads to poor generalization performance of task models. There has also been empirical analysis that suggests fine-tuning for longer, reinitializing top few layers~, and using debiased Adam optimizer during fine-tuning~ make the fine-tuning procedure more stable.   % On the other hand, contrastive learning methods have seen remarkable success for self-supervised representation learning on various downstream tasks, particularly in the image, speech, and video domains.   % These self-supervised contrastive learning methods primarily try to reduce the distance between representations of the positive pairs while increasing the distance between representations of the negative pairs. Positive pairs are constructed as the different augmented views of the same labeled example, and negative pairs are simply augmented views of all the other examples. Augmented views of the examples are often constructed with state-of-the-art data augmentation methods such as RandAugment  or AutoAugment  for the computer vision domain, and distance metric is often chosen as the inner product or the Euclidean distance between the representations of the pairs in a low-dimensional embedding space.    % Recently,  extended contrastive learning to a fully supervised setting through using label information while constructing positive and negative pairs, showed improved performance over cross-entropy loss baseline on ImageNet image classification accuracy and robustness benchmarks, and demonstrated that supervised contrastive learning is less sensitive to hyperparameter changes. Similarly,  propose a hybrid discriminative-generative training of energy-based models, where they approximate the generative term with a contrastive objective and demonstrate improved image classification accuracy on CIFAR-10 and CIFAR-100, along with improved performance on robustness, out-of-distribution detection, and calibration.  % In this paper, we propose a supervised contrastive learning regularization for fine-tuning of large pre-trained language models that helps the model leverage label information more effectively across different labeled data regimes. Our approach does not require specialized architectures , memory banks , or very large batch sizes , but still outperforms the strong baseline of fine-tuning RoBERTa-Large on labeled task data with cross-entropy loss, unlike some previous works. To the best of our knowledge, our work is the first to successfully integrate a supervised contrastive learning objective for fine-tuning pre-trained language models. % while sho results on few-shot learning, robustness, and generalization ability.  % We summarize our key contributions in the following: %  %          We propose a supervised contrastive learning objective for fine-tuning pre-trained language models and demonstrate improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both high-data and low-data regimes. We also show that our proposed objective leads to models that are more robust to different levels of noise in the training data and can generalize better to related tasks with limited labeled task data. Currently, data augmentation methods in NLP and their effects on the downstream tasks are neither as effective nor as well understood as their counterparts in the computer vision domain. In future work, we plan to study principled and automated data augmentation techniques for NLP that would allow extending our supervised contrastive learning objective to both semi-supervised and self-supervised learning settings.            
"," State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. Cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability.  Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning  objective for the fine-tuning stage. Combined with cross-entropy, the SCL loss we propose obtains improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in both the high-data and low-data regimes, and it does not require any specialized architecture, data augmentation of any kind, memory banks, or additional unsupervised data. % In all of our experiments, we use a very competitive baseline of fine-tuning RoBERTa Large using cross entropy loss on the labeled task data.  %including SST-2, CoLA, MRPC, RTE and QNLI. %Our method outperforms the baseline on multiple datasets in the GLUE benchmark including SST-2, CoLA, MRPC, RTE and QNLI for the full dataset regime.  % We also show the effectiveness of our regularization for few-shot learning and demonstrate  % We also demonstrate the robustness of the learned representations by using noisy datasets, and show that the learned representations are more transferable to related tasks.  We also demonstrate that the new objective leads to models that are more robust to different levels of noise in the training data, and can generalize better to related tasks with limited labeled task data.",284
" With the rapid growth of textual documents on the internet, accessing information from the web has become a challenging issue . Often users want the summary of a topic from various sources to fulfill their information needs . The QF-MDS task deals with such problems where the goal is to summarize a set of documents to answer a given query.     In the QF-MDS task, the summaries generated by the summarizer can be either extractive or abstractive. An extractive summarizer extracts relevant text spans from the source document, whereas an abstractive summarizer generates a summary in natural language that may contain some words which did not appear in the source document . With the rising popularity of virtual assistants in recent years, there is a growing interest to integrate abstractive summarization capabilities in these systems for natural response generation .   One major challenge for the QF-MDS task is that the datasets  used for such tasks do not contain any labeled training data. Therefore, neural summarization models that leverage supervised training cannot be used in these datasets. Note that for other related tasks , how to reduce the demands for labeling the data and how to leverage unlabeled data were also identified as a major challenge. While using datasets similar to the target dataset as the training data for the QF-MDS task, we find that these datasets only contain multi-document gold summaries. However, the state-of-the-art transformer-based  summarization models  cannot be used in long documents due to computational complexities . To tackle these issues, we propose a novel weakly supervised approach by utilizing distant supervision to generate weak reference summary of each single-document from multi-document gold reference summaries. We train our model on each document with weak supervision and find that our proposed approach that generates abstractive summaries is very effective for the QF-MDS task. More concretely, we make the following contributions:     .          In this paper, we propose a novel weakly supervised approach for the Query Focused Multi-Document Abstractive Summarization task to  tackle the issue of no available labeled training data for such tasks. We also propose an iterative approach to address the computational problem that occurs while training neural models in long documents . Experimental results in three datasets show that our proposed approach sets a new state-of-the-art result in various evaluation metrics. In the future, we will apply our models on more tasks, such as information retrieval applications , sentiment analysis , learning from imbalanced or unlabeled datasets , and automatic chart question answering .   
"," In the Query Focused Multi-Document Summarization  task, a set of documents and a query are given where the goal is to generate a summary from these documents based on the given query. However, one major challenge for this task is the lack of availability of labeled training datasets. To overcome this issue, in this paper, we propose a novel weakly supervised learning approach via utilizing distant supervision. In particular, we use datasets similar to the target dataset as the training data where we leverage pre-trained sentence similarity models to generate the weak reference summary of each individual document in a document set from the multi-document gold reference summaries. Then, we iteratively train our summarization model on each single-document to alleviate the computational complexity issue that occurs while training neural summarization models in multiple documents  at once. Experimental results in Document Understanding Conferences\footnote{https://duc.nist.gov/}  datasets show that our proposed approach sets a new state-of-the-art result in terms of various evaluation metrics.",285
" One ultimate goal of language modelling is to construct a model like human, to grasp general, flexible and robust meaning in language. One reflection of obtaining such model is be able to master new tasks or domains on same task quickly. However, NLU models have been building from specific task on given data domain but fail when dealing with out-of-domain data or performing on a new task. To combat this issue, several research areas in transfer learning including domain adaptation, cross lingual learning, multi-task learning and sequential transfer learning have been developed to extend model handling on multiple tasks. However, transfer learning tends to favor high-resources tasks if not trained carefully, and it is also computationally expensive .  Meta learning algorithm tries to solve this problem by training model in a variety of tasks which equip the model the ability to adapt to new tasks with only a few samples.  In our case, we adopt the idea of model-agnostic meta learning  which is an optimization method of meta learning that directly optimized the model by constructing an useful initial representation that could be efficiently trained to perform well on various tasks . However, in an continual learning where data comes into the model sequentially, there is still a potential problem of catastrophic forgetting where a model trained with new tasks would start to perform worse on previous tasks. The two objectives of designing a continual learning architecture are to accelerate future learning where it exploits existing knowledge of a task quickly together with general knowledge from previous tasks to learn prediction on new samples and to avoid interference in previous tasks by updates from new tasks. .   % new In this paper, we utilize algorithm derived from Jave and White  which applies Meta-Learning under continual learning. Our objective is to apply this framework in NLP field, specifically on NLU tasks. By taking advantage of this model-agnostic approach, Meta-Learning under continual learning should be applicable on any language model that is optimized by gradient-based methods. We compare our results with Duo et al  which applies meta-learning on Glue tasks, our MAML-Rep shows comparable results. We hope to bring new research direction in NLP fields focusing on such method. The implementation of our code can be found at \url{https://github.com/lexili24/NLUProject}.  % old % This paper aims to develop a framework that incorporate meta learning under the continual learning framework. Hypothetically, our approach is efficient in training by relying on low-resources on various tasks adapted from meta learning characteristics. By training a meta learner under continual learning framework, our model should have consistent results on various tasks with little catastrophic forgetting and learning general representation for all tasks. Finally, our approach is model agnostic, and could essentially apply on any existing language models as long as the model can be optimized by gradient descent. Moreover, our method can be put into the framework of some other continual learning techniques like GEM. The implementation of our code can be found at \url{https://github.com/lexili24/NLUProject}.     In this work, we are able to extend Meta-Learning under continual learning framework to learn a general presentation that is robust on a set of continual tasks with efficiency. We replicate  method and and implement on NLU tasks. Results show that with less datapoints, we could derive a MAML like model that is robust on testing tasks, however extending it to continual setting during training phrase, the performance drastically worsen. Future direction would be extending this approach to other language models, as wells as experiment with a combination of high and low resources other than Glue and SuperGlue benchmark to evaluate model performance.       
"," Neural network has been recognized with its accomplishments on tackling various natural language understanding  tasks. Methods have been developed to train a robust model to handle multiple tasks to gain a general representation of text. In this paper, we implement the model-agnostic meta-learning  and Online aware Meta-learning  meta-objective under the continual framework for NLU tasks proposed by Javed and White. We validate our methods on selected SuperGLUE   and GLUE benchmark .",286
"  	 	%  	% % final paper: en-us version  	% 	  % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/} }  ), we also incorporate an intermediate encoder layer as the auxiliary view. We feed the two views to a partially shared decoder for independent predictions. An additional regularization loss based on prediction consistency between views is used to encourage the auxiliary view to mimic the primary view. Thanks to the co-training on the two views, the gradients during back-propagation can simultaneously flow into the two views, which implicitly realizes the knowledge transfer.  Extensive experimental results on five translation tasks  show that our method can stably outperform multiple baseline models . In particular, we have achieved new state-of-the-art results of 10.8 BLEU on KoEn and 36.23 BLEU on IWSLT'14 DeEn. Further analysis shows that our method's success lies in the robustness to encoding representations and dark knowledge  provided by consistency regularization.   	 \fi      We studied to incorporate different encoder layers through multi-view learning in neural machine translation. In addition to the primary view from the topmost layer, the proposed model introduces an auxiliary view from an intermediate encoder layer and encourages the transfer of knowledge between the two views. Our method is agnostic to network architecture and can maintain the same inference speed as the original model. We tested our method on five translation tasks with multiple strong baselines: Transformer, deep Transformer, and DynamicConv. Experimental results show that our multi-view learning method can stably outperform the baseline models. Our models have achieved new state-of-the-art results in KoEn and IWSLT'14 DeEn tasks.   
","   Traditional neural machine translation is limited to the topmost encoder layer's context representation and cannot directly perceive the lower encoder layers. Existing solutions usually rely on the adjustment of network architecture, making the calculation more complicated or introducing additional structural restrictions. In this work, we propose layer-wise multi-view learning to solve this problem, circumventing the necessity to change the model structure.    We regard each encoder layer's off-the-shelf output, a by-product in layer-by-layer encoding, as the redundant view for the input sentence.   In this way, in addition to the topmost encoder layer , we also incorporate an intermediate encoder layer as the auxiliary view.    We feed the two views to a partially shared decoder to maintain independent prediction.     Consistency regularization based on KL divergence is used to encourage the two views to learn from each other.   Extensive experimental results on five translation tasks show that our approach yields stable improvements over multiple strong baselines. As another bonus, our method is agnostic to network architectures and can maintain the same inference speed as the original model.",287
" % . } % Emotion analysis is now an established research area which finds application in a variety of different fields, including social media analysis , opinion mining , and computational literary studies . The most prominent task in emotion analysis is emotion categorization, where text receives assignments from a predefined emotion inventory, such as the fundamental emotions of \fear, \anger, \joy, \anticipation, \trust,   \target{cars} because they .'' A number of English-language resources are available:  manually construct a dataset following FrameNet's emotion predicate and annotate the stimulus as its core argument.   annotate Tweets for emotion cue phrases, emotion targets, and the emotion stimulus. In our previous work  we publish news headlines annotated with the roles of emotion experiencer, cue, target, and stimulus.  annotate sentence triples taken from literature for the same roles.  A popular benchmark for emotion stimulus detection is the Mandarin corpus by .  annotate English and Mandarin texts in a comparable way on the clause level .  In this paper, we utilize role annotations to understand their influence on emotion classification. We evaluate which of the roles' contents enable an emotion classifier to infer the emotions. It is reasonable to assume that the roles' content carries different kinds of information regarding the emotion: One particular experiencer present in a corpus might always feel the same emotion; hence, be prone to a bias the model could pick up on. The target or stimulus might be independent of the experiencer and be sufficient to infer the emotion.  The presence of a target might limit the set of emotions that can be triggered.  Finally, as some of the corpora contain cue annotations, we assume that these are the most helpful to decide on the expressed emotion, as they typically have explicit references towards concrete emotion names.      Our experiments show that the importance of semantic roles for emotion classification differs between datasets and roles: The stimulus and cue are critical for classification, which correspond to the direct report of a feeling and the description that triggered an emotion. This result is shown in the drop in performance when removing these roles. This information is not redundantly available outside of these arguments.  It is particularly beneficial for the model's performance to have access to the position of cues and stimuli. This suggests that the classifier learns to tackle the problem differently when this information is available, especially so for ECA and ES -- the cases in which literature has been annotated and the instances are comparably long.  The bi-LSTM model indicates that the experiencer role is a confounder in GNE.  The performance can be increased when the model does not have access to its content. Similar results are observed for ET, in which the target role is a confounder. However, these results should be taken with a grain of salt given that they are not confirmed while switching to the transformer-based model. The differences in results between the bi-LSTM and the transformer also motivate further research, as they suggest that the contextualized representation might compensate for missing information, and is, therefore, more robust.  Finally, our results across both models and multiple datasets indicate that emotion classification approaches indeed benefit from semantic roles' information by adding the positional information. Similarly to targeted and aspect-based sentiment analysis, this motivates future work, in which emotion classification and role labeling should be modelled jointly. In this case, it can also be interesting to investigate what happens when the positional indicators are added to all roles jointly.  
","   Emotion recognition is predominantly formulated as text classification in   which textual units are assigned to an emotion from a predefined inventory   .   More recently, semantic role labeling approaches have been developed to   extract structures from the text to answer questions like: ``who is   described to feel the emotion?'' , ``what causes this   emotion?'' , and at   which entity is it directed?'' . Though it has been shown that   jointly modeling stimulus and emotion category   prediction is beneficial for both subtasks, it remains unclear which of   these semantic roles enables a classifier to infer the emotion. Is it the   experiencer, because the identity of a person is biased towards a   particular emotion ? Is it a particular target    or a stimulus ? We   answer these questions by training emotion classification models on five   available datasets annotated with at least one semantic role by masking the   fillers of these roles in the text in a controlled manner and find that   across multiple corpora, stimuli and targets carry emotion information,   while the experiencer might be considered a confounder.  Further, we   analyze if informing the model about the position of the role improves the   classification decision. Particularly on literature corpora we find that   the role information improves the emotion classification.",288
" In recent years, the best results for coreference resolution of English have been obtained with end-to-end neural models~. However for Dutch, the existing systems are still using either a  rule-based~ or a machine learning approach~. The rule-based system dutchcoref~ outperformed previous systems on two existing datasets and also presented a corpus and evaluation of literary novels .  In this paper we compare this rule-based system to an end-to-end neural coreference resolution system: e2e-Dutch. This system is a variant of  with BERT token representations. We evaluate and compare the performance of e2e-Dutch to dutchcoref on two different datasets:  the SoNaR-1 corpus , a genre-balanced corpus of 1 million words, and  the RiddleCoref corpus of contemporary novels . This provides insights into  the relative strengths of a neural system versus a rule-based system for Dutch coreference, and  the effect of domain differences .  The two datasets we consider vary greatly in terms of overall size and length of the individual documents; the training subset of RiddleCoref contains only 23 documents  compared to 581 documents for SoNaR-1. However, the average number of sentences per document is higher for RiddleCoref than for SoNaR-1 .  We also conduct an error analysis for both of the systems to examine the types of errors that the systems make.     We found large gaps in performance for the two systems across the two domains, but this result is not conclusive due to several reasons, which are as follows.   1, 2 The neural system shows a weakness with the long documents in the novel corpus, but also needs more training data to reach its full potential.   3, 4 The rule-based system should be better adapted to the SoNaR-1 annotation scheme, but the neural system's capacity to adapt to arbitrary annotation conventions does not necessarily imply better linguistic performance.   5 To maximize the comparability and usefulness of the corpora, their annotations should be harmonized, which involves manual mention annotation. In future work we want to improve the neural system by using genre metadata and finetuning BERT, and the rule-based system should be extended to a hybrid system by adding supervised classifiers.    
","     We evaluate a rule-based      and neural  coreference system on Dutch datasets of     two domains: literary novels and news/Wikipedia text.     The results provide insight into the relative strengths of data-driven and     knowledge-driven systems, as well as the influence of domain, document     length, and annotation schemes.     The neural system performs best on news/Wikipedia text,     while the rule-based system performs best on literature.     The neural system shows weaknesses with limited training data and long     documents, while the rule-based system is affected by annotation     differences. The code and models used in this paper are available at     \url{https://github.com/andreasvc/crac2020}",289
"  A relational triple consists of two entities connected by a semantic relation, which is in the form of . The extraction of relational triples from unstructured raw texts is a key technology for automatic knowledge graph construction, which has received growing interest in recent years.  There have been several studies addressing technical solutions for relational triple extraction. Early researches, such as , employ a pipeline manner to extract both of entities and relations, where entities are recognized first and then the relation between the extracted entities is predicted. Such a pipeline approach ignores the relevance of entity identification and relation prediction  and tends to suffer from the error propagation problem.  %    To model cross-task dependencies explicitly and prevent error propagation in the pipeline approach, subsequent studies propose joint entity and relation extraction. These studies can be roughly categorized into three main paradigms. The first stream of work, such as , treats joint entity and relation extraction task as an end-to-end table filling problem. Although these methods represent entities and relations with shared parameters in a single model, they extract the entities and relations separately and produce redundant information . The second stream of work, such as , transforms joint entity and relation extraction into sequence labeling. To do this, human experts need to design a complex tagging schema. The last stream of work, including , is driven by the sequence-to-sequence  model  to generate relational triples directly, which is a flexible framework to handle overlapping triples and does not require the substantial effort of human experts.  We follow the seq2seq based models for joint entity and relation extraction. Despite the success of existing seq2seq based models, they are still limited by the autoregressive decoder and the cross-entropy loss. The reasons are as follows: the relational triples contained in a sentence have no intrinsic order in essence. However, in order to adapt the autoregressive decoder, whose output is a sequence,  the unordered target triples must be sorted in a certain order during the training phase. Meanwhile, cross-entropy is a permutation-sensitive loss function, where a penalty is incurred for every triple that is predicted out of the position. Consequently, current seq2seq base models not only need to learn how to generate triples, but also are required to consider the extraction order of multiple triples.   % consists of three parts  featured by transformers with non-autoregressive parallel decoding and the bipartite matching loss.  In detail, there are three parts in the proposed set prediction networks :  to avoid introducing the order of triplets  % restoring to the original form of this task without considering the order of multiple triples In this work, we formulate the joint entity and relation extraction task as a set prediction problem, avoiding considering the order of multiple triples. In order to solve the set prediction problem, we propose an end-to-end network featured by transformers with non-autoregressive parallel decoding and bipartite matching loss. In detail, there are three parts in the proposed set prediction networks : a sentence encoder, a set generator, and a set based loss function. First of all, we adopt the BERT model  as the encoder to represent the sentence. Then, since an autoregressive decoder must generate items one by one in order, such a decoder is not suitable for generating unordered sets. In contrast, we leverage the transformer-based non-autoregressive decoder  as the set generator, which can predict all triples at once and avoid sorting triples. Finally, in order to assign a predicted triple to a unique ground truth triple, we propose bipartite matching loss function inspired by the assigning problem in operation research . Compared with  cross-entropy loss  that highly penalizes small shifts in  triple order, the proposed loss function is invariant to any permutation of predictions; thus it is suitable for evaluating the difference between ground truth set and prediction set.  % To summarize, our contributions are as follows: In a nutshell, our main contributions are: % the main contributions of our work are as follows:     % the conjunction of the bipartite matching loss and transformers with %  parallel decoding  % Our work build on prior work in several domains:relation extraction, non-autoregressive model, andbipartite matching losses for set prediction. % Relation Extraction.   Non-autoregressive Model.     In this paper, we introduce set prediction networks for joint entity and relation extraction. Compared with previous seq2seq based models, We formulate the  joint entity and relation extraction task as a set prediction problem. In such a way, the extraction model will be relieved of predicting the extraction order of multiple triples. To solve the set prediction problem, We combine non-autoregressive parallel decoding with bipartite matching loss function. We conduct extensive experiments on two widely used datasets to validate the effectiveness of the proposed set prediction networks. Experimental results show that our proposed networks outperforms state-of-the-art baselines over different scenarios. This challenging task is far from being solved. We find that relation types exhibit an imbalanced or long-tailed distribution in NYT dataset and WebNLG dataset. Our future work will concentrate on how to combine cost-sensitive learning with the proposed set prediction networks.          
"," The joint entity and relation extraction task aims to extract all relational triples from a sentence. In essence, the relational triples contained in a sentence are unordered. However, previous seq2seq based models require to convert the set of triples into a sequence in the training phase. To break this bottleneck, we treat joint entity and relation extraction as a direct set prediction problem, so that the extraction model can get rid of the burden of predicting the order of multiple triples. To solve this set prediction problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike autoregressive approaches that generate triples one by one in a certain order, the proposed networks directly output the final set of triples in one shot. Furthermore, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in triple order, the proposed bipartite matching loss is invariant to any permutation of predictions; thus, it can provide the proposed networks with a more accurate training signal by ignoring triple order and focusing on relation types and entities. Experiments on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods. Training code and trained models will be available at \url{http://github.com/DianboWork/SPN4RE}.",290
" Zero-shot translation has first been introduced by  and refers to the ability of a multilingual NMT model to translate between all its source and target languages, even those pairs for which no parallel data was seen in training. In the simplest setting, all parameters in the network are shared between the different languages and the translation is guided only by special tags to indicate the desired output language .  While this capability is attractive because it is an alternative to building  dedicated translation systems to serve  languages, performance on zero-shot pairs tends to lag behind pivot translation. Recent papers, such as ,  and , have suggested training techniques to improve the generalization to unseen language pairs, but performance varies considerably across settings.  In this paper, we examine in detail the behavior of the multilingual model proposed by  on zero-shot translation directions. Our experiments show the following:      Overall, we observe improvements of 8.1 BLEU  on 6 zero-shot directions with simple changes to the multilingual training setup.    We analyze the importance of shared subwords in multilingual models and find that language-specific BPE segmentation helps to reduce the amount of untranslated segments in zero-shot directions. Furthermore, we explore whether the tendency to produce the wrong output language can be attributed to using English as the only bridge language, and show that even with a small amount of additional training data in non-English language pairs, generalization to unseen translation directions improves as the model is less likely to produce output in the wrong language.  Compared to previous work, the methods we propose are easier to implement, since they only concern data collection and pre-processing, and result in higher gains for zero-shot directions. They are also compatible in principle with approaches that introduce new training objectives or model modifications, and we report best results when fine-tuning a multi-bridge model with back-translation for zero-resource translation directions.  For future work, we are interested in testing the effects of subword regularization  on zero-shot translation performance, and scaling multi-bridge setups to massively multilingual settings.  
"," Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN$\leftrightarrow$\{FR,CS,DE,FI\} system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g.\ English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions.  We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.",291
"  Entrainment is a well-known psycholinguistic phenomenon causing people to adapt to conversation partners so as to become more similar. It affects many linguistic features including phonetics , lexical choice , syntax , and prosody . Importantly, it correlates with interesting aspects of the conversation such as task success, liking, and even rapport with a robot .  The researchers cited above employed various means to measure entrainment, such as correlations, models of conditional probabilities, comparisons of distributions, and perceived similarity. Recently,  proposed the first neural entrainment measure. Our work builds on theirs by addressing a challenge critical to measuring entrainment: accounting for consistency.   Entrainment is defined as an active, though unconscious, adaptation of a speaker towards their partner. In practice, however, the static similarity or correlation between two speakers is often measured. Thus, even two speakers whose vocal characteristics were initially similar are perceived to have entrained, although no adaptation has taken place. Alternatively, when Speaker B entrains to Speaker A, both speakers are perceived to have entrained, without adaptation from Speaker A. We apply neural methods proposed by  to explicitly deconfound consistency, the tendency to adhere to one's own vocal style, from entrainment, the tendency to adapt to one's partner. We argue that entrainment measures that do not control for consistency overestimate the degree of entrainment in a conversation.  Section  explains the data and features that we use to train our networks, which are described in Section . Section  introduces two experiments to validate our methods whose results are discussed, lastly, in Section .     We propose two neural measures of entrainment that control for consistency. We empirically validate these measures by demonstrating their ability to discriminate between real and fake sessions. Although our measures perform slightly worse than the one reported by , we believe this is because their measure captures both entrainment and consistency and therefore better describes the expected similarity between two turns, but is overly broad as a measure of entrainment.  Most intriguingly, the strict separation of consistency and entrainment leads to correlations that are very different from those with other entrainment measures that do not account for consistency, even on the same corpus. This resembles the results of , who found that correlations differ based on how entrainment is treated.   Our findings cast previous links between conversation quality and entrainment measures that do not account for consistency in a new light. It is worth revisiting those with the new ability to distinguish between consistency and entrainment.  In our future work, we intend to expand the network inputs for each prediction to the entire prior conversation context using RNNs with attention. We will also conduct further analysis of these entrainment measures, e.g., by feature, speaker sex, role, and dialogue act.   } ranging from 7.5e-13 to almost 1.  To address this, we retrained both networks 100 times, recomputing the Pearson correlations each time. To control the false discovery rate resulting from this multiple testing, we use the procedure of . Each run consists of three tests per measure. We sort each group of three tests by their  values and determine the smallest value  such that  for at least one  value, where  is its position after sorting. Finally, we determine the largest  such that  where  is the -th smallest  value for any run of the respective measure, the level at which at least one of three correlations is significant for that run and measure.   Using this method, we find that 65 out of 100 times the correlation between  reach the level of significance, not even in terms of the ``raw''  values.   For all but three of the 65 runs with significant correlations between  and lik have the same valence. Considering the clear overall trends, we conclude that  correlates positively with dom and to a lesser degree with lik.  
","   Human interlocutors tend to engage in adaptive behavior known as entrainment to become more similar to each other. Isolating the effect of consistency, i.e., speakers adhering to their individual styles, is a critical part of the analysis of entrainment. We propose to treat speakers' initial vocal features as confounds for the prediction of subsequent outputs. Using two existing neural approaches to deconfounding, we define new measures of entrainment that control for consistency. These successfully discriminate real interactions from fake ones. Interestingly, our stricter methods correlate with social variables in opposite direction from previous measures that do not account for consistency. These results demonstrate the advantages of using neural networks to model entrainment, and raise questions regarding how to interpret prior associations of conversation quality with entrainment measures that do not account for consistency.",292
"  The demand for speech translation systems at meetings and lectures continues to increase. Since the length of complete sentences in such talks can be long and complicated, simultaneous speech translation is required to mimic human interpreters and translate the incoming speech stream from a source language to target language in real time. One challenge for achieving simultaneous speech translation is the development of incremental ASR.  Researchers have been working on speech recognition technology for decades. A number of techniques of real-time ASR exist, especially in the context of statistical ASR with a hidden Markov model  . However, many current state-of-the-art ASR systems rely on attention-based sequence-to-sequence deep learning frameworks . Today's attentional mechanisms are based on a global attention property that requires the computation of a weighted summarization of the entire input sequence generated by the encoder states. This means that the system can only generate text output after receiving the entire input speech sequence. Consequently, utilizing it in situations that require immediate recognition is difficult.  Several studies proposed local attention mechanisms  that limit the area explored by the attention by largely reducing the total training complexity without reducing the latency. For work that enables incremental recognition of speech, Hwang and Sung employed a unidirectional RNN with a CTC acoustic model and a unidirectional RNN language model . To avoid continuous output revision, they also proposed depth-pruning in the beam-search during the output generation. Jaitly et al. proposed a neural transducer framework  that incrementally recognizes the input speech waveforms. The formulation required inferring alignments during training, and they utilized a dynamic programming algorithm to compute ``approximate"" best alignments in each speech segments. Their model is strongly related to a sequence transducer that used connectionist temporal classification  . The improved version of a neural transducer, which has also been discussed , allows the attention mechanism to look back at many previous chunks without introducing additional latency.  However, most ISR models utilize different frameworks and learning algorithms that are more complicated than the standard ASR model. One main reason is because such models need to decide incremental steps and learn the transcription that is aligned with the current short speech segment. In this work, we propose attention-transfer ISR  by the following:    [ht]        and moves to recognize the next segment. }         We constructed a character-level AT-ISR framework that was trained with the original architecture of the attention-based sequence-to-sequence ASR model.  The main difference is that it consists of shorter sequences than the standard architecture.  No new redesign was needed for the ISR, and some hyperparameters can be used without any changes.   Transfer learning treats the non-incremental ASR model as the teacher and the ISR as the student model.  Student ISR learns the same attention alignment as the teacher model's, allowing a simple mechanism in the incremental recognition.   both of which utilize the same attention alignment with the teacher model's, allowing a simple mechanism in the incremental recognition.  Various types of models have been explored.  The optimum performance was achieved by including a few ahead blocks, setting the last character of the last set as the decoder input, keeping the recurrent states across the steps, and utilizing the attention transfer.  
"," Attention-based sequence-to-sequence automatic speech recognition  requires a significant delay to recognize long utterances because the output is generated after receiving entire input sequences. Although several studies recently proposed sequence mechanisms for incremental speech recognition , using different frameworks and learning algorithms is more complicated than the standard ASR model. One main reason is because the model needs to decide the incremental steps and learn the transcription that aligns with the current short speech segment. In this work, we investigate whether it is possible to employ the original architecture of attention-based ASR for ISR tasks by treating a full-utterance ASR as the teacher model and the ISR as the student model. We design an alternative student network that, instead of using a thinner or a shallower model, keeps the original architecture of the teacher model but with shorter sequences . Using attention transfer, the student network learns to mimic the same alignment between the current input short speech segments and the transcription. Our experiments show that by delaying the starting time of recognition process with about 1.7 sec, we can achieve comparable performance to one that needs to wait until the end.",293
" In the biomedical domain, there exist several entities, such as genes, chemicals, and diseases, that are closely related to each other. Therefore, extracting the relationships among these entities is critical for biomedical research, particularly in fields such as construction of a knowledge base or drug development. Biomedical text data, including PubMed abstracts, usually contain information about biomedical entities and their relationships with each other. Thus, various natural language processing models, particularly deep learning models, are applied to biomedical text data to extract the relationships among these entities, as a kind of classi閾夸恭ation task.   ChemProt corpus  is the first corpus dataset for chemical--protein  relationship extraction, which has been conducted by BioCreative VI organizers. These organizers annotated all entity offsets of chemical and protein mentions and relationship types between chemicals and proteins . There exist 10 groups of the relationship types, and five of these  were used in the evaluation.   All models for extracting relationships from ChemProt data are designed as classifiers. In a deep learning-based multi-class classifier, the output probability distribution for each class is calculated through the Softmax function. In the training step, the model is trained to maximize the output probability of the correct class. However, some studies reported that the deep learning classifier trained with hard-labeled data  tends to become over-confident . This over-confidence does not directly affect classification performance, but it degrades the reliability of the model. In other words, the output probability of the over-confident model does not indicate how uncertain the input example is, even if its classi閾夸恭ation performance is high. Therefore, several approaches, called ``calibration'' techniques, have been applied to several domains that require high reliability, such as autonomous driving and medical diagnosis  .   In the natural language processing domain, bidirectional encoder representation from transformers   was proposed for a wide-range of language understanding. BERT is a large multi-head attention  model, which was pre-trained with a vast amount of corpus data. This pre-trained model can be easily transfer-learned and can be applied on several downstream tasks  by fine-tuning it. BERT has been used in many domains, including a biomedical field. Nevertheless, it is still important to improve the performance of BERT by applying additional techniques while using the BERT as a backbone architecture.     In this study, we propose a DNN-based approach to improve the performance of chemical--protein relationship extraction, while calibrating the classifier. More precisely, we incorporated two main calibration techniques to BERT  to improve the reliability and performance. Furthermore, we propose a semi-supervised learning workflow using the calibrated model and unlabeled in-domain data. The main contributions of our study are as follows:         In this study, we propose a calibrated deep neural network-based relationship extraction model for chemical--protein interactions. We applied mixup training---which can both augment the training examples and calibrate the model---on the BioBERT model. We also incorporated the low entropy penalizing term in the loss function, as a regularization term during training. This led to significant improvement in terms of both classification and calibration performance. Moreover, we applied self-training on our model to augment the training data and boost the performance, as our model is well-calibrated; it returned reliable output probabilities. Consequently, our model outperformed the other chemical--protein relationship extraction models and achieved state-of-the-art performance regarding the Biocreative VI ChemProt task.   In this work, we applied our training process on the biomedical domain, especially on chemical--protein relation extraction, because a large set of unlabeled data can be found from PubMed in the biomedical domain. This process can be applied to any NLP classification problems with unlabeled data sets. We can apply the proposed method to other tasks such as  CoLA or SST-2 in the GLUE benchmark  since there are a large set of unlabeled data for ungrammatical sentences or movie reviews. The application of our method to other domains will be our future work.  
"," The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of biomedical research such as drug development and prediction of drug side effects. Several natural language processing methods, including deep neural network  models, have been applied to address this problem. However, these methods were trained with hard-labeled data, which tend to become over-confident, leading to degradation of the model reliability. To estimate the data uncertainty and improve the reliability, ``calibration'' techniques have been applied to deep learning models. In this study, to extract chemical--protein interactions, we propose a DNN-based approach incorporating uncertainty information and calibration techniques. Our model first encodes the input sequence using a pre-trained language-understanding model, following which it is trained using two calibration methods: mixup training and addition of a confidence penalty loss. Finally, the model is re-trained with augmented data that are extracted using the estimated uncertainties. Our approach has achieved state-of-the-art performance with regard to the Biocreative VI ChemProt task, while preserving higher calibration abilities than those of previous approaches. Furthermore, our approach also presents the possibilities of using uncertainty estimation for performance improvement.",294
"  Contemporary deep learning models for language have been shown to learn many aspects of natural language syntax including a number of long-distance dependencies , selectional properties of verbs , representations of incremental syntactic state  and information from which hierarchical structure can be linearly decoded .  These and many other related studies demonstrate an impressive range of human-like linguistic knowledge that is automatically acquired by these models simply from exposure to large quantities of raw text.  However, human-like grammatical abilities include not just rich and detailed linguistic knowledge but the ability to deploy this knowledge in using new words based on minimal exposure .  It remains poorly understood what grammatical generalizations contemporary deep learning models are able to make regarding the behavior of words to which they have minimal exposure. In this work, we assess the syntactic generalization behavior of a contemporary neural network model  on two novel phenomena in English  and address the question of single-shot and few-shot learning, demonstrating that BERT makes robust grammatical generalizations after fine-tuning on minimal examples of a novel token.  We test BERT's few-shot learning capabilities on two phenomena at the syntax-semantics interface: English verbal alternations, and verb/object selectional preferences. In English, verbs can appear in multiple syntactic frames; which frame a verb appears in is governed by its argument structure properties. Often, frames are paired into alternation classes  such that when English speakers hear a novel verb in one frame they can be confident that it can be used in its alternation-class pair. Using the well-attested dative alternation as an example, if a listener hears the sentence ``I daxed the tennis racket to my friend"" they would expect that ``I daxed my friend the tennis racket"" is a grammatical English sentence, meaning approximately the same thing. They would not, however, have such an expectation for ``I daxed my friend for the tennis racket."" In addition, listeners may be attuned to semantic clustering of verbal arguments based on past experience. For instance, following the example above, English speakers may expect dax to take an animate indirect object, and would find examples such as ``I daxed the court the tennis racket"" to be surprising.   We take inspiration for our testing regime from a class of psycholinguistic experiments known as `novel word learning studies', which we adapt to the neural setting. In such experiments subjects are exposed to a novel word in context during a training phase, and assessed for what grammatical generalizations they have learned about the novel word during a later testing phase. Novel word learning experiments have been used to assess human grammatical generalization since , and have been deployed to assess semantic, as well as syntactic, generalizations . %For example, in , children were shown a novel creature and told it was a wug. At test time, they were shown a picture featuring two of the creatures and described them as wugs indicating that they had categorized the novel word as a noun, and applied the productive -s pluralization to it.  Children can also learn semantic properties of a word from a single exposure . %Bayesian models of word learning have shown successes in modeling novel word learning abilities , however it is not clear how well neural network models would exhibit these rapid generalizations. Recent work has shown that sequence-to-sequence neural architectures rarely generalize systematically in the way that would be required to match human syntactic generalization behavior , but it is an open question whether we might get different behavior depending on network architecture and training objective. In this work, we replicate the novel word learning paradigm in the neural setting by fine-tuning BERT on tightly-controlled sentences that contain novel verbs and objects, and assessing the model on carefully constructed test sets that reveal what grammatical generalizations it has learned. We find that BERT is able to make proper generalizations for both verbal alternations as well as semantic clustering for verbal arguments after just one or two exposures during training.       We used a novel word learning paradigm, inspired by classic studies from psycholinguistics, to assess BERT's syntactic generalization behavior on two novel phenomena: English verb class alternations and verb/object selectional restrictions.  In both cases we address the issue of single and few-shot learning by fine-tuning the model on just one or two positive examples, finding that BERT makes some generalizations about a novel token based on minimal experience, and that these generalizations drive robust behavior during test time. This novel word learning paradigm can continue to be explored in later work through the use of large databases such as VerbNet , which builds on Levin's verb documentations by providing a larger database of verb alternations and sectional restrictions that can be turned into train and test sentences for BERT without hand-crafting.  For verbal/object selectional restrictions, we find that BERT leverages indirect evidence to expect unattested but plausible verb/noun pairings more than unattested but implausible pairings. These results provide evidence for the view that the model is able to attend not just to patterns overtly realized in the data  but also implicit relationships between tokens . The ability to use indirect evidence, specifically indirect negative evidence, is a hallmark of human language learning, and these results indicate that models are capable of similar behavior in a simple novel word learning paradigm.  For verbal alternations, we find that when fine-tuned on a single frame, BERT routinely expects the verb to occur in its sister frame with a higher likelihood than in unrelated verbal frames. Interestingly, this behavior is consistently blocked when the model is asked to generalize from a frame that involves an object to a frame where the object is lacking. This behavior is consistent with a general bias towards transitivity in the model, and suggests an exciting direction for further study. Whether such a general bias exists, whether it is restricted to settings with limited evidence, and whether it changes as verbs appear more frequently in the fine-tuning or training data is a question for future research. Another question for future research is whether a multilingual BERT would have the same success on alternation tests in other languages, and if if would exhibit the same biases that we see for English.  
","  Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT's  few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb/object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively. The code for our experiments is available at \url{https://github.com/TristanThrush/few-shot-lm-learning}.",295
"  When Natural Language Processing  systems are deployed in production, and interact with users , there are many potential ways of collecting feedback data or rich interaction logs. For example, one can ask for explicit user ratings, or collect user clicks, or elicit user revisions to get an estimate of how well the deployed system is doing. However, such user interaction logs are primarily used for an one-off assessment of the system, e.g., for spotting critical errors, detecting domain shifts, or identifying the most successful use cases of the system in production. This assessment can then be used to support the decision of keeping or replacing this system in production.   From a machine learning perspective, using interaction logs only for evaluation purposes are lost opportunities for offline reinforcement learning . Logs of user interactions are gold mines for off-policy learning, and they should be put to use, rather than being forgotten after a one-off evaluation purpose.  To move towards the goal of using user interaction logs for learning, we will discuss which challenges have hindered RL from being employed in real-world interaction with users of NLP systems so far.  Concretely, our focus is on sequence-to-sequence learning for NLP applications , such as machine translation, summarization, semantic parsing or dialogue generation for chatbots, since these applications provide the richest interaction with users. For example, many machine translation services provide the option for users to give feedback on the quality of the translation, e.g. by collecting post-edits. Similarly, industrial chatbots can easily collect vast amounts of interaction logs, which can be utilized with offline RL methods.  Recent work by has recognized that the poorly defined realities of real-world systems are hampering the progress of RL in production environments. They address, amongst others, issues such as off-line learning, limited exploration, high-dimensional action spaces, or unspecified reward functions. These challenges are important in RL for control systems or robots grounded in the physical world. However, they severely underestimate the human factor when collecting feedback in systems interacting with humans, e.g. through natural language. In the following, we will thus present challenges that are encountered in user-interactive RL for NLP systems. With this discussion, we aim to  encourage NLP practitioners to leverage their interaction logs through offline RL, and  inspire  RL researchers to steel their algorithms for the challenging applications in NLP.  There is large potential in NLP to leverage user interaction logs for system improvement. We discussed how algorithms for offline RL can offer promising solutions to this type of learning problem. However, there are specific challenges in offline RL that arise due to the particular nature of NLP systems that collect human feedback in real-world applications. We presented cases where such challenges have been found and offered solutions that have helped. Furthermore, we related the identified challenges to the  . This overview may serve as a guide for both NLP researchers to explore solutions of offline RL, and for RL researchers to test and equip their algorithms for the real-world challenges in NLP applications.
"," Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning  setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions.",296
"     In addition to other challenges in multiword expression  processing that were addressed in previous work, such as non-compositionality , discontinuity , and syntactic variability , The PARSEME shared task edition 1.2 has focused on another prominent challenge in detecting MWEs, namely detection of unseen MWEs. The problem with unseen data is common for many NLP tasks. While rule-based and unsupervised ML approaches are less affected by unseen data, supervised ML techniques are often found to be prone to overfitting. In this respect, the introduction of language modelling objectives to be added to different NLP tasks and their effect on generalisation have shown promising results. Further improvements brought by pre-trained language models made them a popular approach to a multitude of NLP tasks. One particular advantage of such models is that they facilitate generalisation beyond task-specific annotations .  MWEs are inherent in all natural languages and distinguishable for their syntactic and semantic idiosyncracies . Since language models are good at capturing syntactic and semantic features, we believe they are a suitable approach for modelling MWEs.    In particular, our system relies on BERT pre-trained language models .  Additionally, we render the system semi-supervised by means of multi-task learning. The most promising feature to be jointly learned with MWEs is dependency parse information . Accordingly, we fine-tune BERT for two different objectives: MWE detection and dependency parsing. MWE learning is done via token classification using a linear layer on top of BERT, and dependency parse trees are learned using dependency tree CRF network .  Our experiments confirm that this joint learning architecture is effective for capturing MWEs in most languages represented in the shared task.~      We described MTLB-STRUCT, a semi-supervised system that is based on pre-trained BERT masked language modelling and that jointly learns VMWE tags and dependency parse trees. The system ranked first in the open track of the PARSEME shared task - edition 1.2 and shows the overall state-of-the-art performance for detecting unseen VMWEs.  In future, we plan to augment the dependency parsing architecture to train on dependency relation categories  as well as dependency arcs. We also plan to improve our system by making it more efficient in order to train the dependency parsing module on the extra available unannotated datasets.  
"," This paper describes a semi-supervised system that jointly learns verbal multiword expressions  and dependency parse trees as an auxiliary task. The model benefits from pre-trained multilingual BERT.  BERT hidden layers are shared among the two tasks and we introduce an additional linear layer to retrieve VMWE tags. The dependency parse tree prediction is modelled by a linear layer and a bilinear one plus a tree CRF on top of BERT. The system has participated in the open track of the PARSEME shared task 2020 and ranked first in terms of F1-score in identifying unseen VMWEs as well as VMWEs in general, averaged across all $14$ languages.",297
"  % \gn{Title candidate: ``Detecting Hallucinated Content ...'' . I wonder if you could also run your methods over extractive summarization outputs or the true references and see how many hallucinations they detect? Just an idea.} % However, recent studies on abstractive text summarization   % and neural machine translation~ have shown that conditional neural sequence models are prone to hallucinate content that is not faithful to the input text.  This risk of generating unfaithful content impedes the safe deployment of neural sequence generation models~. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for sequence evaluation, such as BLEU scores , ROUGE  and BERTScores , do not correlate well with the faithfulness of model outputs~. They also require reference output text, limiting their applicability to detecting halluciations in a deployed system at run-time. Very recent efforts~ have started to develop automatic metrics to measure the faithfulness of output sequences. These methods use external semantic models, e.g. the question-generation and question-answering systems~ or textual entailment inference models, to score faithfulness tailored for abstract text summarization.  However, these scores do not directly measure the number of hallucinated tokens %In addition, these metrics are often tailored for the evaluation of summaries in abstract text summarization  and only correlate weakly with human judgements.  % \gn{Big question: what is the difference from word-level quality estimation, which has been around for a very long time, since at least:  and has been covered in many WMT quality estimation shared tasks . This seems more related than the works cited below, and describing why we'd need to do something new over these works would probably be a big question in the minds of anyone familiar with the MT field. Also, would the proposed methods for detecting hallucination do better than SOTA word-level QE models?}  % \gn{Similar motivation: Moreover, they do not distinguish the types of errors in terms of fluency and adequacy: a substitution error referring to a simple morphological variation  is % considered in the same way as a content word substitution changing the meaning of the sentence.~.}  We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is a hallucinated or faithful to the source input.  This task does not use the reference output to assess faithfulness, which offers us the ability to apply it in the online generation scenario where references are not available. Similar to the spirit of our proposed task, word-level quality estimation~ in the machine translation community predicts if tokens are correctly translated based on human post-editing. However, they do not distinguish errors in terms of fluency and adequacy~.  % A substitution error referring to a simple morphological variation  is considered the same as a content word substitution changing the meaning of the sentence.~.  In contrast to estimating the amount of human post-editing work required to fix errors, we specifically focus only on hallucination  errors.  We measure hallucination for two conditional sequence generation tasks -- abstractive summarization and machine translation . For the former, we produce a benchmark dataset from recently released annotations ~. For MT, we carefully design the human assessment guideline and create high-quality annotations. We will also release our human annotated data for future research. To learn token-level hallucination prediction for general conditional sequence generations tasks, we propose a novel method that creates synthetic ``hallucinated"" data and finetunes a pretrained language model~ on it. Without any human annotated supervised training data, we achieve an average F1 of around 0.6 across all the benchmark datasets, setting initial performance levels for this new task. % } We also show that pretraining on MT can actually produce more faithful translations, confirming recent findings in abstractive summarization~.  Predicting hallucination labels at token-level provides a tool for diagnosing and interpreting model outputs, which allows us to flag potential risks at inference time for previously unseen inputs. On the other hand, the token-level labels also allow for fine-grained controls over the target sequence during learning full translation models.  We show how to use these token-level hallucination labels in two case studies to improve self-training and learning from noisy mined bitext in low-resource MT. In both cases, there can be noise in the target text, either produced by the self-training teacher or mining errors. However, most outputs are only partially hallucinated  and the rest of the output is still useful for training, as we show by introducing different token-level loss truncation schemes. %To further benefit self-training, we filter out the noisy part and also glean useful part of model predictions by applying token-level loss truncation or control of information flows to the target sequence at training time.  Our best methods outperform strong baselines by a large margin both in translation quality and hallucination reduction.            %   In this work, we proposed a new evaluation task for hallucination detection in conditional sequence generation and created human-annotated benchmark datasets. We also proposed a novel and general-purpose method to learn this task, and showed that the models can be used to define fine grained losses that improve low resource models for machine translation. In the future, we hope to create a large-scale pretrained evaluation model for any datasets or models to be evaluated, and also would extend our method to data-to-text generation scenarios.   We are also interested in investigating how to leverage our detection methods to mitigate hallucination problems in conditional sequence generation.   
"," Neural sequence models can generate highly fluent sentences but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input, which can cause a lack of trust in the model. To better assess the faithfulness of the machine outputs, we propose a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. We also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations.   Experiments on machine translation and abstract text summarization demonstrate the effectiveness of our proposed approach -- we obtain an average F1 of around 60 across all the benchmark datasets. Furthermore, we demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in the low-resource machine translation and achieve significant improvements over strong baseline methods. We will release our annotated data and code to support future research.",298
"  With rise in social media and e-commerce websites, there is a huge interest in analyzing these networks for tasks like link prediction, recommendation, community detection, etc. Traditionally, this is done by learning finite-dimensional vector embeddings/representations  for nodes in these networks and then used it for downstream tasks. One of the challenges is that the quality of these learned representation decreases if the network has many missing links. This affects its performance in downstream tasks. This can be addressed by using attribute similarity of nodes as connected usually have similar attributes. For example, in citation networks, papers on related works will cite each other, and in social media, people with similar interest follow each other. In real-world graphs, nodes of these networks themselves contain rich textual information  as attributes. So, we need techniques which can exploit this textual information while learning node embeddings. The representation learning of textual networks deals with this problem.    An adversarial technique for attributed network representation learning. Here, in addition to the supervision from training data, a discriminator using text embeddings is used to give supervision to structure embeddings.  A novel text embedding learning technique which uses both mutual and topological attention.  Extensive comparative study on downstream tasks of link prediction and node classification.  Experiments on link prediction on unseen nodes.    \iffalse We have evaluated our proposed method on three datasets Cora, Zhihu, and Hepth for link prediction. We observed that our model performs better than state-of-the-art methods in almost all settings in all three datasets. The performance of our model is especially high in low data regime. In Zhihu dataset, our model show a performance improvement of  over the previous state-of-the-art in the lowest supervision setting. A similar observation was made on the node classification task on Cora dataset, where our adversarial technique achieve state-of-the-art performance. As we mentioned earlier, the main advantage of this model is its ability to the care of representation learning in unseen nodes. We evaluated the quality of these embeddings in link prediction task for edges involving unseen nodes, and ACNE achieves state-of-the-art performance for all settings in all three datasets. On Zhihu dataset, it gave an impressive improvement of   improvement over previous methods in the low-data regime.  \fi  \iffalse  \fi  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   In this paper, we propose a new technique called Adversarial Context-Aware Network embeddings  for representation learning in textual networks. This method uses an adversarial framework between text embeddings and structure embeddings for modality fusion.  In the proposed approach, embeddings of the nodes are the result of an adversarial game between a discriminator and a generator. The generator is based on structure embedding to model the connectivity of the network, and the discriminator uses the text encoding for predicting edges between a pair of nodes. For discriminator, we have developed a new text-encoding method that combines both mutual and topological attention to learn text embeddings.  Through extensive experiments, we have shown that our model achieves state-of-the-art results in the task of link prediction and node classification. We also demonstrated the effectiveness of topological attention in improving the performance of the context-aware network embeddings. We have shown that one of the main advantage of the proposed model is that it can be extended for learning unseen nodes in training using a post-training stage. This is achieved by using discriminator for providing supervision for learning structure embeddings in the generator. We also achieved state-of-the-art results in the link prediction task on the unseen nodes.       \maketitle  
"," \label{section:abstract}  Representation learning of textual networks poses a significant challenge as it involves capturing amalgamated information from two modalities:  underlying network structure, and  node textual attributes. For this, most existing approaches learn embeddings of text and network structure by enforcing embeddings of connected nodes to be similar. Then for achieving a modality fusion they use the similarities between text embedding of a node with the structure embedding of its connected node and vice versa. %Then for achieving modality fusion they model intra-modal similarities involving networks structure and textual attributes of  nodes in an edge.  This implies that these approaches require edge information for learning embeddings and they cannot learn embeddings of unseen nodes. In this paper we propose an approach that achieves both modality fusion and the capability to learn embeddings of unseen nodes. The main feature of our model is that it uses an adversarial mechanism between text embedding based discriminator, and structure embedding based generator to learn efficient representations. Then for learning embeddings of unseen nodes, we use the supervision provided by the text embedding based discriminator. In addition this, we propose a novel architecture for learning text embedding that can combine both mutual attention and topological attention mechanism, which give more flexible text embeddings. Through extensive experiments on real-world datasets, we demonstrate that our model makes substantial gains over several state-of-the-art benchmarks. In comparison with previous state-of-the-art, it gives up to 7\% improvement in performance in predicting links among nodes seen in the training and up to 12\% improvement in performance in predicting links involving nodes not seen in training. Further, in the node classification task, it gives up to 2\% improvement in performance.",299
"  Streaming Automatic Speech Recognition  researches have made their way into our everyday products. Smart speakers can now transcribe utterances in a streaming fashion, allowing users and downstream applications to see instant output in terms of partial transcriptions. There is a growing interest in the community to develop end-to-end  streaming ASR models, because they can transcribe accurately and run compactly on edge devices. Amongst these streaming E2E models, Recurrent Neural Network Transducer  is a candidate for many applications. RNN-T is trained with a loss function that does not enforce on the temporal alignment of the training transcripts and audio. As a result, RNN-T suffers from token emission delays - time from when the token is spoken to when the transcript of the token is emitted. Delayed emissions of tokens adversely affects user experiences and downstream applications such as the end-pointer.   Some existing work tried to mitigate the token emission delays in streaming RNN-Ts. We introduce them in Section. Other works utilized semi-streaming or non-streaming models to predict better token emission time, at the cost of the overall latency of the transcripts. In this work, we propose a novel loss function for streaming RNN-T, and the resultant trained model is called Alignment Restricted RNN-T . It utilizes audio-text alignment information to guide the loss computation. In Section, we show that theoretically, Ar-RNN-T loss function is faster to compute and results in better audio-token alignment. In Section, we empirically compare our proposed method with existing works such as monotonic RNN-T training on two data set: LibriSpeech and voice command. In the results section, Section, we show improvement in training speed and that when used in tandem with an end-pointer, Ar-RNN-T provides an unprecedentedly refined control over the latency-WER trade-offs of RNN-T models.      In this work, we present a detailed analysis of RNN-T model's token emission delays and its impact on the downstream applications. We propose a modification to the RNN-T loss that uses alignment information to restrict the paths being optimized during training. We call our solution Alignment Restricted RNN-T and show that we can control the token delays for RNN-T models systematically using tunable parameters during training, while also significantly improving training throughput. Using our proposed solution, we show that we can improve the accuracy of downstream applications such as the ASR end-pointing system and significantly reduce latency and early cut-offs.    For alignments, we found splitting the word time-steps equally among sub-words worked best, however bootstrapping a hybrid model using target word-piece dictionary can potentially give better alignments and lead to improved accuracy with Ar-RNN-T, which may be explored in future.     References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   ------------------------------------------------------------------------- 
"," There is a growing interest in the speech community in developing Recurrent Neural Network Transducer  models for automatic speech recognition  applications. RNN-T is trained with a loss function that does not enforce temporal alignment of the training transcripts and audio. As a result, RNN-T models built with uni-directional long short term memory  encoders tend to wait for longer spans of input audio, before streaming already decoded ASR tokens. In this work, we propose a modification to the RNN-T loss function and develop Alignment Restricted RNN-T  models, which utilize audio-text alignment information to guide the loss computation. We compare the proposed method with existing works, such as monotonic RNN-T, on LibriSpeech and in-house datasets. We show that the Ar-RNN-T loss provides a refined control to navigate the trade-offs between the token emission delays and the Word Error Rate . The Ar-RNN-T models also improve downstream applications such as the ASR End-pointing by guaranteeing token emissions within any given range of latency. Moreover, the Ar-RNN-T loss allows for bigger batch sizes and 4 times higher throughput for our LSTM model architecture, enabling faster training and convergence on GPUs.",300
"  % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  . }  Word segmentation is a fundamental NLP analysis problem for written languages with no space delimiters between words such as Chinese and Japanese.  In the age of digital communications, new URLs  and hashtags , which often include strings of concatenated words  are being added every day to a growing set of tokens that an NLP system may need to deal with, and they pose challenges for language and speech applications. For example, a Text-to-Speech  synthesis system will struggle to pronounce these concatenated tokens, since simply applying a grapheme-to-phoneme system out of the box to something like  will usually yield poor results. This suggests the need for a model that can split such tokens into the component words.  So-called ``end-to-end'' neural TTS systems , which learn to map directly from character sequences to speech might seem to hold out the hope of avoiding treating this problem separately. However, the fact that URLs occur relatively rarely in most TTS training data limits the promise of such models on this long-tail problem.   The problem of analyzing URLs does differ in one useful way from more general text normalization problems. For a token such as  in a text, one typically needs to know what context it occurs in in order to know how to read it: is it  or ; see , inter alia. In the case of URLs, these are largely  since the output segmentation is usually unaffected by the surrounding words. Hence the problem can be treated as a standalone one that does not require the system to be trained as part of broader text normalization training.  Our training data comes from  camel case URLs that naturally define the segment boundaries  along with manual corrections for non-trivial boundaries.  We release our training and evaluation data sets to promote research on this problem.  By drawing an analogy with Chinese word segmentation, we cast the URL segmentation problem as a sequence tagging problem. We propose a simple Recurrent Neural Network  based tagger with an encoder and a decoder.   The model trained on the data set has a decent full sequence accuracy  but fails to generalize to more rare words due to the size of the training data. Inspired by the success of pre-training in many NLP tasks , we propose a pre-training recipe for the segmenter. Based on the observation that URLs are often compound entity names and so are knowledge graph entities , we create a large synthetic training data set by concatenating the knowledge graph entity names. We observe 21\% absolute  improvement in sequence accuracy after applying pre-training followed by fine-tuning.  % % File acl2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \pdfoutput=1  \documentclass[11pt,a4paper]{article} \usepackage{coling2020} \usepackage{times} \usepackage{latexsym} \usepackage{graphicx} \usepackage{amsmath} {\ttfamily \usepackage{multirow} \usepackage{url}    % %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  [1]{{{#1}}}  [1]{{{#1}}}  \TeX} \title{Semi-supervised URL Segmentation with Recurrent Neural Networks Pre-trained on Knowledge Graph Entities} \author{Hao Zhang \and Jae Ro \and Richard Sproat \\         Google Research \\          @google.com}} \date{}     Breaking domain names such as  into component words  and  is important for applications like Text-to-Speech synthesis and web search. We link this problem to the classic problem of Chinese word segmentation and show the effectiveness of a tagging model based on Recurrent Neural Networks  using characters as input. To compensate for the lack of training data, we propose a pre-training method on concatenated entity names in a large knowledge database. Pre-training improves the model by 33\% and brings the sequence accuracy to 85\%.          URL segmentation has applications in TTS and web search. Our contributions include a curated URL data set and a highly accurate RNN model boosted by pre-training on Knowledge Graph entities.   We plan on releasing a version of the dataset before the conference.       For an anonymized submission, ensure that { appears in the {|} definition at the top of this document.   For a camera-ready submission, ensure that {\small\verb|\aclfinalcopy|} at the top of this document is not commented out.   
"," Breaking domain names such as openresearch into component words open and research is important for applications like Text-to-Speech synthesis and web search. We link this problem to the classic problem of Chinese word segmentation and show the effectiveness of a tagging model based on Recurrent Neural Networks  using characters as input. To compensate for the lack of training data, we propose a pre-training method on concatenated entity names in a large knowledge database. Pre-training improves the model by 33\% and brings the sequence accuracy to 85\%.",301
" .     %       % final paper: en-us version         % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. } Discourse parsing is an important upstream task within the area of Natural Language Processing   which has been an active field of research over the last decades. In this work, we focus on discourse representations for the English language, where most research %on the discourse analysis of English language  has been surrounding one of the two main theories behind discourse, the Rhetorical Structure Theory  proposed by  or interpreting discourse according to PDTB . While both theories have their strengths, the application of the RST theory, encoding documents into complete constituency discourse trees , has been shown to have many crucial implications on real world problems. A tree is defined on a set of EDUs , approximately aligning with clause-like sentence fragments, acting as the leaves of the tree. Adjacent EDUs or sub-trees are hierarchically aggregated to form larger  constituents, with internal nodes containing  a nuclearity label, defining the importance of the subtree  in the local context and  a relation label, defining the type of semantic connection between the two subtrees . In this work, we focus on structure and nuclearity prediction, not taking relations into account. Previous research has shown that the use of RST-style discourse parsing as a system component can enhance important tasks, such as sentiment analysis, summarization and text categorization . More recently, it has also been suggested that discourse structures obtained in an RST-style manner can further be complementary to learned contextual embeddings, like the popular  BERT approach . Combining both approaches has shown to support tasks where linguistic information on complete documents is critical, such as argumentation analysis .  Even though discourse parsers appear to enhance the performance on a variety of tasks, the full potential of using more linguistically inspired approaches for downstream applications has not been unleashed yet. The main open challenges of integrating discourse into more NLP downstream tasks and to deliver even greater benefits have been a combination of  discourse parsing being a difficult task itself, with an inherently high degree of ambiguity and uncertainty and  the lack of large-scale annotated datasets, rendering the initial problem more severe, as data-driven approaches cannot be applied to their full potential.  The combination of these two limitations has been one of the main reasons for the limited application of neural discourse parsing for more diverse downstream tasks. While there have been neural discourse parsers proposed , they still cannot consistently %strongly  outperform traditional approaches when applied to the RST-DT dataset, where the amount of training data is arguably insufficient for such data-intensive approaches.  %due to the extra effort to integrate discourse trees into models as well as two major problems, the big breakthrough in the usage of discourse parsing has still not happened.   In this work, we alleviate the restrictions to the effective and efficient use of discourse as mentioned above by introducing a novel approach combining a newly proposed large-scale discourse treebank with our data-driven neural discourse parsing strategy. More specifically, we employ the novel MEGA-DT ``silver-standard"" discourse treebank published by  containing over 250,000 discourse annotated documents from the Yelp'13 sentiment dataset , nearly three orders of magnitude larger than commonly used RST-style annotated discourse treebanks . Given this new dataset with a previously unseen number of full RST-style discourse trees, we revisit the task of neural discourse parsing, which has been previously attempted by   and others with rather limited success. We believe that one reason why previous neural models could not yet consistently outperform more traditional approaches, heavily relying on feature engineering , is the lack of generalisation when using deep learning approaches on the small RST-DT dataset, containing only 385 discourse annotated documents. This makes us believe that using a more advanced neural discourse parser in combination with a large training dataset can lead to significant performance gains. %, but also across datasets, capturing more general discourse phenomena and avoiding potential overfitting on the training corpus. Admittedly, even though MEGA-DT contains a huge number of datapoints to train on, it has been automatically annotated, potentially introducing noise and biases, which can negatively influence the performance of our newly proposed neural discourse parser when solely trained on this dataset. A natural and intuitive approach to make use of the neural discourse parser and both datasets  is to combine them during training, pretraining on the large-scale ``silver-standard"" corpus and subsequently fine-tuning on RST-DT or further human annotated datasets. This way, general discourse structures could be learned from the large-scale treebank and then enhanced with human-annotated trees. With the results shown in this paper strongly suggesting that our new discourse parser can encode discourse more effectively, we hope that our efforts will prompt researchers to develop more linguistically inspired applications based on our discourse parser. % for downstream models in the area of NLP.  Our contributions in this paper are: %     % We propose a novel neural discourse parsing architecture which combines multiple lines of previous work in a single framework.     % We combine %the two treebanks  a large-scale ``silver-standard"" treebank  with small, domain-specific gold-standard treebanks in a neural way during the training process, by initially pretraining on the large  dataset and subsequently fine-tuning on the dataset within the domain itself.    We apply the neural discourse parser on %the large-scale ``silver-standard"" discourse corpus as well as small-scale gold-standard treebanks two commonly used disocurse treebanks , showing large performance improvements of our model over previous state-of-the-art approaches.% to compare the performance individually for both datasets.  % %on how to train a neural discourse parser with large scale ``silver-standard"" discourse trees. With this new approach, we  drastically increase the amount of available training data available for discourse parsers is not sufficiently large to train modern, data-driven deep learning approaches for the task, hindering the application of new methodologies and  the shift in domain between the discourse parsers training data and the domain of application deminishes the applicability and performance of generated discourse trees for any domain outside of news , instructions  and a few other domains.    In this work, we proposed a rather simple, yet highly effective discourse parser, utilizing recent neural BERT-based language models in combination with structural features. The integration of those input-features within a standard shift-reduce framework as well as an unprecedented use of recent large-scale ``silver-standard"" discourse parsing datasets for pretraining reaches a new state-of-the-art performance on both, the RST-DT and Instr-DT treebanks. We show that our new, neural discourse parser already achieves better or similar performance when trained and evaluated on the RST-DT and Instr-DT datasets, however, the consistent and significant SOTA result is reached when incorporating pretraining on the MEGA-DT corpus.  This refutes the previous findings of , stating that neural techniques such as word embeddings provide little to no gains for this task.  We further demonstrated the gains achieved by  The presented pretraining  even on the small subset of approach on the silver-standard MEGA-DT dataset  , which  also further validates the usefulness of additional supervision for this task and calls for more work in that area.   
"," RST-based discourse parsing is an important NLP task with numerous downstream applications, such as summarization, machine translation and opinion mining. In this paper, we demonstrate a simple, yet highly accurate discourse parser, incorporating recent contextual language models. Our parser establishes the new state-of-the-art  performance for predicting structure and nuclearity on two key RST datasets, RST-DT and Instr-DT. We further demonstrate that pretraining our parser on the recently available large-scale ``silver-standard"" discourse treebank MEGA-DT provides even larger performance benefits, suggesting a novel and promising research direction in the field of discourse analysis.",302
"  The last several years have seen a land rush in research on machine reading  comprehension and various dataset have been proposed such as SQuAD1.1, SQuAD2.0, NewsQA and CoQA . Different from the above which are extractive MRC, RACE is a multi-choice MRC dataset  proposed by . RACE was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets is that the answers in RACE often cannot be directly extracted from the passages, as illustrated by the two example questions  in Table . Thus, answering these questions needs inferences.  [t!]   }   \toprule   Passage: For the past two years, 8-year-old Harli Jordean from Stoke Newington, London, has been selling marbles . His successful marble company, Marble King, sells all things marble-related - from affordable tubs of the glass playthings to significantly expensive items like Duke of York solitaire tables - sourced, purchased and processed by the mini-CEO himself. ""I like having my own company. I like being the boss,"" Harli told the Mirror....Tina told The Daily Mail. ""At the moment he is annoying me by creating his own Marble King marbles - so that could well be the next step for him."" \\   \midrule   % \multirow{3}*{SDT-RPR-48L}   Q1: Harli's Marble Company became popular as soon as he launched it because \_\_\_ .     \\   A: it was run by ""the world's youngest CEO""      \\   B: it filled the gap of online marble trade      \\   C: Harli was fascinated with marble collection   \\   D: Harli met the growing demand of the customers \\   \midrule   Q2: How many mass media are mentioned in the passage?    \\   A: One      \\   B: Two     \\   C: Three   \\   D: Four \\         .}      % [t] % .} %  %   Recently, pretrained language models  such as BERT , RoBERTa , ALBERT  have achieved great success on MMRC tasks. Notably, Megatron-LM  which is a 48 layer BERT with 3.9 billion parameters yields the highest score on the RACE leaderboard in both single and ensemble settings. The key point to model MMRC is: first encode the context, question, options with BERT like LM, then add a matching network on top of BERT to score the options. Generally, the matching network can be various .  proposes an option comparison network  to compare options at word-level to better identify their correlations to help reasoning.  proposes a dual co-matching network  which models the relationship among passage, question and answer options bidirectionally. All these matching networks show promising improvements compared with pretrained language models. One point they have in common is that the answer together with the distractors are jointly considered which we name multi-choice models. We argue that the options can be concerned separately for two reasons, 1) when human works on MMRC problem, they always consider the options one by one and select the one with the highest confidence. 2) MMRC suffers from the data scarcity problem. Multi-choice models are inconvenient to take advantage of other MRC dataset.   In this paper, we propose a single-choice model for MMRC. Our model considers the options separately. The key component of our method is a binary classification network on top of pretrained language models. For each option of a given context and question, we calculate a confidence score. Then we select the one with the highest score as the final answer. In both training and decoding, the right answer and the distractors are modeled independently. Our proposed method gets rid of the multi-choice framework, and can leverage amount of other resources. Taking SQuAD as an example, we can take a context, one of its question and the corresponding answer as a positive instance for our classification with golden label 1. In this way many QA dataset can be used to enhance RACE. Experimental results show that single-choice model performs better than multi-choice models, in addition by transferring knowledge from other QA dataset, our single model achieves 90.7\% and ensemble model achieves 91.4\%, both are the best score on the leaderboard.       In this paper, we propose a single-choice model for MMRC that consider the options separately. Experiments results demonstrate that our method achieves significantly improvements and by taking advantage of other MRC datasets, we achieve a new state-of-the-art performance. We plan to consider the difference between two methods and if we can combine them together in future study.            File emnlp2019.tex      Based on the style files for ACL 2019, which were    Based on the style files for EMNLP 2018, which were    Based on the style files for ACL 2018, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp-ijcnlp-2019} \usepackage{latexsym} \usepackage{times} \usepackage{soul} \usepackage{url} \usepackage[utf8]{inputenc} \usepackage[small]{caption} \usepackage{graphicx} \usepackage{subfigure} \usepackage{amsmath} \usepackage{booktabs} \usepackage{natbib} \usepackage{xcolor} \urlstyle{same} \usepackage{fancyhdr,graphicx,amssymb} \usepackage[ruled,vlined]{algorithm2e} \usepackage{multirow} \usepackage{url}  \usepackage{tikz} \usepackage{subfigure} \usepackage{xcolor} \usepackage{tcolorbox}  \usepackage{helvet}   Required \usepackage{courier}   Required   \\\And   Second Author \\   Affiliation / Address line 1 \\   Affiliation / Address line 2 \\   Affiliation / Address line 3 \\   email@domain \\}  \date{}    
"," Multi-choice Machine Reading Comprehension  aims to select the correct answer from a set of options based on a given passage and question. Due to task specific of MMRC, it is non-trivial to transfer knowledge from other MRC tasks such as SQuAD, Dream. In this paper, we simply reconstruct multi-choice to single-choice by training a binary classification to distinguish whether a certain answer is correct. Then select the option with the highest confidence score. We construct our model upon ALBERT-xxlarge model and estimate it on the RACE dataset. During training, We adopt AutoML strategy to tune better parameters. Experimental results show that the single-choice is better than multi-choice. In addition, by transferring knowledge from other kinds of MRC tasks, our model achieves a new state-of-the-art results in both single and ensemble settings.",303
"  % Images  are another important approach for expressing feelings and emotions in addition to using text in communication. In mobile messaging apps, these images can generally be classified into emojis and stickers. Emoji is a kind of small picture which is already stored in most of the keyboard of the mobile operational systems,  , for sticker response selection in multi-turn dialog. Specifically, SRS first learns representations of dialog context history using a self-attention mechanism and learns the sticker representation by a convolutional neural network .  % % % Next, SRS conducts deep matching between the sticker and each utterance and produces the interaction results for every utterance. % % Finally, SRS employs a fusion network which consists of a sub-network fusion RNN and fusion transformer to learn the short and long term dependency of the utterance interaction results. The final matching score is calculated by an interaction function. To evaluate the performance of our model, we propose a large number of multi-turn dialog dataset associated with stickers from one of the popular messaging apps.  Extensive experiments conducted on this dataset show that SRS significantly outperforms the state-of-the-art baseline methods in commonly-used metrics.  % However, the user's sticker selection does not only depend on the matching degree between dialog context and candidate sticker image, but also depends on the user's preference of using sticker. When users decide to use a sticker as their response in multi-turn dialog, they may choose their favorite one from all appropriate stickers as the final response.  % % % We assume that user tends to use the recently used sticker in their dialog history, and the recently-used-sticker can represent the user's preference of sticker selection. An example is shown in Figure. To verify this assumption, we retrieve 10 recently-used-stickers of each user and calculate the proportion of whether the currently used sticker appeared in these 10 stickers. The result shows that 54.09\% of the stickers exist in the 10 recently used sticker set. Hence, we reach to the conclusion that users have strong personal preference when selecting the sticker as their response for the current dialog context. However, in some cases, this also indicates a tendency to re-use stickers, but not necessarily a preference.  % Motivated by this observation, in this work, we take one step further and improve our previously proposed SRS framework with user preference modeling. Overall, we propose a novel sticker recommendation model which considers the user preference, namely  . Specifically, PESRS first employs a convolutional network to extract features from the candidate stickers. Then, we retrieve the recent user sticker selections then a user preference modeling module is employed to obtain a user preference representation. Next, we conduct the deep matching between the candidate sticker and each utterance as the same as SRS. Finally, we use a gated fusion method to combine the deep matching result and user preference into final sticker prediction.    % The key to the success of PESRS lies in how to design the user preference modeling module, which should not only identify the user's favorite sticker and but also consider the current dialog context. % Motivated by this, we first propose a recurrent neural network  based position-aware sticker modeling module which encodes the recently used stickers in chronological order. Then, we employ a key-value memory network to store these sticker representations as values and the corresponding dialog context as keys. Finally, we use the current dialog context to query the key-value memory and obtain the dynamic user preference of the current dialog context.  % We empirically compare PESRS and SRS on the public dataset\footnote{https://github.com/gsh199449/stickerchat} proposed by our early work. This is a large-scale real-world Chinese multi-turn dialog dataset, which dialog context is multiple text utterances and the response is a sticker image. Experimental results show that on this dataset, our newly proposed PESRS model can significantly outperform the existing methods.  Particularly, PESRS yields 4.8\% and 7.1\% percentage point improvement in terms of  and  compared with our early work SRS. % In addition to the comprehensive evaluation, we also evaluate our proposed user preference memory by a fine-grained analysis. The analysis reveals how the model leverages the user's recent sticker selection history and provides us insights on why they can achieve big improvement over state-of-the-art methods.  This work is a substantial extension of our previous work reported at WWW 2020.  The extension in this article includes the user preference modeling framework for the existing methods, a proposal of a new framework for sticker selection in the multi-turn dialog. Specifically, the contributions of this work include the following:         The rest of the paper is organized as follows: We summarize related work in \S.  \S introduces the data collection method and some statistics of our proposed multi-turn dialog sticker selection dataset. We then formulate our research problem in \S  and elaborate our approach in \S.  \S gives the details of our experimental setup and \S presents the experimental results.  Finally, \S concludes the paper.   %    In our previous work, we propose the task of multi-turn sticker response selection, which recommends an appropriate sticker based on multi-turn dialog context history without relying on external knowledge. However, this method only focuses on measuring the matching degree between the dialog context and sticker image, which ignores the user preference of using stickers. Hence, in this paper, we propose the   to recommend an appropriate sticker to user based on multi-turn dialog context and sticker using history of user. Specifically, PESRS first learns the representation of each utterance using a self-attention mechanism, and learns sticker representation by CNN. Second, a deep interaction network is employed to fully model the dependency between the sticker and utterances. The deep interaction network consists of a co-attention matrix that calculates the attention between each word in an utterance and each unit in a sticker representation. Third, a bi-directional attention is used to obtain utterance-aware sticker representation and sticker-aware utterance representations. Next, we retrieve the recent user sticker selections, and then propose a user preference modeling module which consists a position-aware history encoding network and a key-value based memory network to generate the user preference representation dynamically according to current dialog context. Then, a fusion network models the short-term and long-term relationship between interaction results, and a gated fusion layer is applied to fuse the current dialog interaction results and user preference representation dynamically. Finally, a fully-connected layer is applied to obtain the final sticker prediction using the output of gated fusion layer. Our model outperforms state-of-the-art methods including our previous method SRS in all metrics and the experimental results also demonstrate the effectiveness of each module in our model. In the near future, we aim to propose a personalized sticker response selection system.  
","   Stickers with vivid and engaging expressions are becoming increasingly popular in online messaging apps, and some works are dedicated to automatically select sticker response by matching the stickers image with previous utterances.   However, existing methods usually focus on measuring the matching degree between the dialog context and sticker image, which ignores the user preference of using stickers.   Hence, in this paper, we propose to recommend an appropriate sticker to user based on multi-turn dialog context and sticker using history of user.   Two main challenges are confronted in this task.   One is to model the sticker preference of user based on the previous sticker selection history.   Another challenge is to jointly fuse the user preference and the matching between dialog context and candidate sticker into final prediction making.   To tackle these challenges, we propose a   model.   Specifically, PESRS first employs a convolutional based sticker image encoder and a self-attention based multi-turn dialog encoder to obtain the representation of stickers and utterances.   Next, deep interaction network is proposed to conduct deep matching between the sticker and each utterance.   Then, we model the user preference by using the recently selected stickers as input, and use a key-value memory network to store the preference representation.   PESRS then learns the short-term and long-term dependency between all interaction results by a fusion network, and dynamically fuse the user preference representation into the final sticker selection prediction.   Extensive experiments conducted on a large-scale real-world dialog dataset show that our model achieves the state-of-the-art performance for all commonly-used metrics.   Experiments also verify the effectiveness of each component of PESRS.   %",304
"  .}  Neural machine translation  has boosted machine translation significantly in recent years . However, it is still unclear how NMT models work due to the black-box nature of neural networks. Better understandings of NMT models could guide us in improving NMT systems. Currently most of the studies towards understanding NMT models only take into account subword-based  models.  Deeper character-based  models have been shown to perform better than BPE-based models . In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism.   Previous studies have tried to interpret and understand NMT models by interpreting attention weights , using gradients , applying layer-wise relevance propagation , probing classification tasks , and more intrinsic analysis .  However, only  have probed character-based representations.  have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in . We apply more composition methods to explore how CHAR models learn linguistic knowledge and how attention extracts features directly from characters.   Probing classification tasks  have emerged as a popular method to interpret the internal representations from neural networks. Given a probing classifier, the input is usually the representation of a word and the output is the corresponding linguistic tag.  CHAR models pose new challenges for interpretability, and we investigate whether we can probe CHAR models in a way similar to word-based models. In addition, can we extract word sense and morphological information about the full word from individual hidden states, or is this information distributed across multiple states? This has implications for interpreting neural CHAR models, but can also inform novel architectures, such as sparse attention mechanisms.  Thus we first investigate the ability of CHAR models to learn word senses and morphology in Section .  We apply different methods to compose information from characters and demonstrate that the word-level information is distributed over all the characters but characters at different positions play different roles in learning linguistic knowledge.  We also explore the effect of encoder depth to answer why CHAR models outperform BPE-based models only when they have the settings with deeper encoder. The probing results show that CHAR models need more layers to learn word senses.  Then in Section , we move on to explore the attention mechanism. The distribution pattern shows that separators attract much more attention compared to other characters.  To study the effect of enforcing characters to capture the full word-level information, we investigate a sparse attention mechanism, i.e. a model that only attends to separators, which can be viewed as a word-level attention. The BLEU score drops 1.2 points when we apply the word-level sparse attention. This implies that only attending to separators by a single attention head is workable but not enough to extract all the necessary information.   The main findings are summarized as follows:  [noitemsep,topsep=0pt]         CHAR models have been shown to perform better than BPE-based models in NMT yet they pose new challenges for interpretability. In this paper, we investigate CHAR models via the WSD and six morphological probing tasks to learn how CHAR models learn word senses and morphology, in the case of translating Finnish into English. We also explore the attention distribution pattern and a sparse word-level attention to learn the working mechanism of attention.   In the probing tasks, we find that separators also have captured some linguistic knowledge.  We apply different composition methods to the characters of a word, and we demonstrate that the word sense and morphological information is distributed over all the characters rather than some specific characters. Moreover, characters at different positions play different roles in learning linguistic knowledge.  CHAR models are better at learning morphology but we need a more complicated composition method, such as a randomly initialized LSTM, to extract all the encoded information.  These results on probing tasks show that we can extract word sense information and morphological features from character-level hidden states and that these features are encoded in different ways.  In addition, we explore the effect of encoder depth and show that CHAR models require more layers to encode word senses, which explains why only deeper CHAR models outperform BPE-based models.  The attention distribution shows that separators attract a lot of attention, and we show that the sparse word-level attention only attending to separators is workable but not enough for translation.   As we have shown that characters at different positions specialize in learning word senses and morphology, it will be interesting to explore sparse attention with multiple heads in the future which could learn to extract features from different aspects.   
"," Recent work has shown that deeper character-based neural machine translation  models can outperform subword-based models. However, it is still unclear what makes deeper character-based models successful. In this paper, we conduct an investigation into pure character-based models in the case of translating Finnish into English, including exploring the ability to learn word senses and morphological inflections and the attention mechanism. We demonstrate that word-level information is distributed over the entire character sequence rather than over a single character, and characters at different positions play different roles in learning linguistic knowledge. In addition, character-based models need more layers to encode word senses which explains why only deeper models outperform subword-based models. The attention distribution pattern shows that separators attract a lot of attention and we explore a sparse word-level attention to enforce character hidden states to capture the full word-level information. Experimental results show that the word-level attention with a single head results in 1.2 BLEU points drop.",305
"  A prerequisite relation is a pedagogical relation that indicates the order in which concepts can be presented to learners. The relation can be used to guide the presentation sequence of topics and subjects during the design of academic programs, lectures, and curricula or instructional materials. %such as textbooks and study guides.   In this work, we present our systems to automatically detect prerequisite relations for Italian language in the context of the PRELEARN shared task  at EVALITA  2020 . %.  The evaluation of submissions considers:  in-domain and cross-domain scenarios defined by either the inclusion  or exclusion  of the target domain in the training set. The four domains are 'data mining' , 'geometry' , 'precalculus' , and 'physics' .  the type of resources  used to train the model -- raw text VS. structured information.  % four domains, namely 'data mining', 'geometry', 'precalculus' and 'physics'.    % PRELEARN participants had to submit their systems in a per-domain evaluation, considering in-domain and cross-domain scenarios,  % as well as discriminate between the kind of resources the models used, namely raw text such as distributional textual corpora, and structured information such as knowledge bases. % Additionally, the difference between in-domain and cross-domain lies in the inclusion or exclusion of the target domain in the training set.  The combination of these settings defined the four PRELEARN subtasks.  Formally, a prerequisite relation exists between two concepts if one has to be known beforehand in order to understand the other.  For the PRELEARN task, given a pair of concepts, the relation exists only if the latter concept is a prerequisite for the former.  Therefore, the task is a binary classification task.          We approach the problem from two perspectives: handcrafted features based on lexical complexity and pre-trained embeddings. We employed static embeddings from Wikipedia and Wikidata, and contextual embeddings from Italian-BERT model.    We tackle the task of prerequisite relation learning using a variety of systems that explore three set of features: handcrafted features based on complexity intuitions, embedding models from Wikipedia and Wikidata, and contextual embedding from Italian-BERT model. We examine the capabilities of our models in in-domain and cross-domain scenarios.    out of 4 domains, and for raw-text versus structured-information settings. Our models ranked first in all the subtask of the PRELEARN competition at EVALITA 2020. We found that although our Italian-BERT model outperformed the others, the simpler models show competitive results.    A limitation of our work is that we used all the possible domains in our experiments.  We plan to further examine the impact of using a combination of all possible domains as training set on the performance of our models. 
",   English.   We present our systems and findings for the prerequisite relation learning task  at EVALITA 2020. The task aims to classify whether a pair of concepts hold a prerequisite relation or not. We model the problem using handcrafted features and embedding representations for in-domain and cross-domain scenarios.  Our submissions ranked first place in both scenarios with average F1 score of $0.887$ and $0.690$ respectively across domains on the test sets. We made our code freely available\footnote{\url{https://github.com/ajason08/EVALITA2020_PRELEARN}\label{code}}.,306
" Task-oriented dialog systems are commonplace in automated systems that interact with end users, including digital assistants, technical support agents, and various website navigation helpers. An essential part in any task-oriented dialog system is  , which consumes data, typically fed in the form of a , and converts it into natural language output to be served to the end user. The natural language response of the NLG component should 1) contain all essential information, 2) be contextualized around the user request, and 3) be natural sounding. Such a system requires consideration for content planning, correctness, grammaticality, and naturalness.  NLG systems employed in commercial settings are typically based on template-based text generation techniques . In these, humans author a minimal set of responses templates with placeholder slot values. These slots are later filled at runtime, with the dialog input. Although template-based NLG modules are appealing due to their deterministic nature,  inherent correctness, and low latency, they have major drawbacks: First, separate templates need to be authored for different response variations; this behavior is unfavorable for scaling. Second, templates authored for a particular domain are commonly not reusable.  Lastly, no matter the complexity of the language instilled into templates, they form a strictly discrete set of responses, and therefore are bound to be limited in their response naturalness.  More recently, advances in neural-network-based  language generation prompted a new direction in NLG research . The process is typically split into two steps:  serialization of input data into a flattened meaning representation , and  using the neural generation model to generate a natural language response conditioned on the MR. The models are trained on data that includes MR, response pairs, and therefore they are able to not only generate desired responses for MRs in their training data, but they are also expected to form coherent responses for novel MRs, owing to the generalization ability of their machine learning  backbone.  However, deploying neural NLG systems in an industry setting is quite challenging. First, it is not trivial to train a model that reliably presents its input data with the high fidelity required from a user-serving dialog system. Second, the models require much high-quality human-annotated data, which is resource intensive. Consequently, data annotation is a major limiting factor for scaling model-based NLG across domains and languages.  In this work, we detail our approach to production-level neural NLG, with a focus on scalability and data efficiency. Adopting the tree-structured MR framework introduced in Balakrishnan et al.~, which allows better control over generated responses, we train sequence-to-sequence RNN models  that can produce high-fidelity responses. We then employ a multitude of techniques for reducing the amount of  required data, primarily powered by eliminating the ``hidden'' redundancy by grouping data points with similar semantics into . We train models either on the reduced data, or after increasing the size of the dataset using a novel synthetic augmentation technique. We also employ large, pre-trained attention-based language models, fine-tuning them on the same datasets, and then using novel methods to distill their knowledge into smaller sequence-to-sequence models. Further, we train models on data from multiple domains, showing gains over models trained on individual domains when the domains are semantically close together. We conclude with a compiled list of best practices for production-level NLG model development based on our analyses, and we present it as a runbook.    Several considerations are necessary for deploying model-based task-oriented dialogue systems to production. While increasing data efficiency was the primary goal of our study, we also considered and balanced data efficiency gains with several other factors such as acceptability, latency, and the required development and  maintenance resources. Focusing on four datasets for domains with varying level of complexity, we propose a sequential domain development run-book, where development of different domains can halt at different steps based on model performance evaluation. The steps are as follows:     
"," Natural language generation  is a critical component  in conversational systems, owing to its role of formulating a correct and natural text response. Traditionally, NLG components have been deployed using template-based solutions. Although neural network solutions recently developed in the research community have been shown to provide several benefits, deployment of such model-based solutions has been challenging due to high latency, correctness issues, and high data needs.  In this paper, we present approaches that have helped us deploy data-efficient neural solutions for NLG in conversational systems to production.  We describe a family of sampling and modeling techniques to attain production quality with light-weight neural network models using only a fraction of the data that would be necessary otherwise, and show a thorough comparison between each. Our results show that domain complexity dictates the appropriate approach to achieve high data efficiency. Finally, we distill the lessons from our experimental findings into a list of best practices for production-level NLG model development, and present them in a brief runbook. Importantly, the end products of all of the techniques are small sequence-to-sequence models  that we can reliably deploy in production.",307
"  Pre-training has been demonstrated as a highly effective method for boosting the performance of many natural language processing  tasks such as question answering, sentimental analysis, and so on. By training on massive unlabeled text data, pre-trained models are able to learn the contextual representations of input words, which are extremely helpful for accomplishing downstream tasks.  BERT , as one of the most widely used pre-trained models, is trained using two unsupervised tasks, namely, mask language modeling and next sentence prediction. By adding a few layers on top, BERT can be easily adapted into a task-specific model, which is then fine-tuned on the labeled data to achieve optimal performance. Such a practice has been exercised in various NLP scenarios and has achieved many state-of-the-art  results.  The study of integrating BERT into neural machine translation models, which is referred to as BERT-enhanced NMT, has received much research interest. However, exploiting BERT for NMT is not as straightforward as in other NLP tasks. The architecture of a typical NMT model consists of an encoder that transforms the source language words into a hidden representation, and a decoder that predicts the target language words based on the hidden representation. The challenge of exploiting BERT for NMT is twofold. Firstly, NMT models are mostly deep neural networks with a parameter size comparable to or even larger than that of BERT, which makes the combined model hard to optimize. Secondly, since existing NMT models are mostly trained with massive samples, the usual practice of fine-tuning BERT on the labeled corpus can lead to the problem of catastrophic forgetting .  [!t]              The recently proposed BERT-fused model  uses attention mechanisms to bridge between the NMT model and BERT. For example, they introduce an extra BERT-encoder attention module to fuse the encoder layer with the BERT representation. The outputs of the BERT-encoder attention module and the self-attention module are averaged.  Consider the case exemplified in  , it's more likely that the word  should be interpreted as  rather than other meanings in this context. However, if the training corpus doesn't contain similar expressions, the model can fail in this translation due to the ambiguity. When BERT representations are introduced, the contextual information learned by BERT can be helpful for the translation. Concretely, a BERT-encoder attention module can be used to capture the pre-trained knowledge embedded in the BERT representation that is absent in the self-attention module.   However, we find that averaging their outputs means regarding them as equally important, which can hurt performance under some circumstances. In the above example, only the BERT-encoder attention module provides useful information for interpreting the word , while the self-attention module offers faulty or noisy information. Combining their outputs directly can result in confusion during translation.  Hence we assert that it's essential to allow the model to decide which information to concentrate on. To this end, we propose to use a joint-attention module to integrate multiple representations that contain different contextual information. As shown in  , the learnable weights of the joint-attention module allow it to assign more attention to the BERT representation in this case. Compared with the BERT-fused model, our method is better at augmenting desired information and hence boosts performance.   Although existing BERT-enhanced NMT models mostly focus on leveraging BERT's last-layer representation, we find that the intermediate layers can contain semantic and contextual information that is absent in the last layer and might help improve translation performance. The dynamic fusion mechanism proposed by  allows the Transformer encoder to leverage BERT's intermediate representations. However, their method doesn't work for the decoder at the inference stage because it requires the ground truth as input. This motivates us to explore feasible techniques for generating composite BERT representations that can be used in both the encoder and the decoder.  In this paper, we introduce a BERT-enhanced NMT model called BERT-JAM, which stands for BERT-fused Joint-Attention Model. BERT-JAM is equipped with joint-attention modules that allow the encoder/decoder to selectively concentrate on the BERT representation or the encoder/decoder representation by attending to them simultaneously. Besides, we seek to improve upon the existing BERT-enhanced models by making better use of BERT's intermediate layers. Specifically, we allow each encoder/decoder layer to use a GLU module to transform BERT's intermediate representations into a composite representation used by the joint-attention module.   In order to achieve optimal performance, we train BERT-JAM following a three-phase optimization strategy which progressively unfreezes different components of the model during training.  We show that fine-tuning BERT is a crucial step to unearth the full potential of BERT-JAM, in contrast to the previous claim that fine-tuning BERT offers few gains  for NMT models. Moreover, we study how the BERT-enhanced NMT performance varies with the size of BERT by feeding different BERT models into BERT-JAM, ranging from the most compact BERT with 2 layers and embedding dimension 128 to the standard BERT-base model. This study can be beneficial because it can provide us with a guide on how to adjust the model with minimal performance loss when we have to resort to a smaller model size due to limited computation resources.    We summarize the contributions of this paper as follows:        The rest of this paper is organized as follows.  In , we introduce our approach to BERT-enhanced NMT where a detailed description of our model will be presented.  The experimental setups are described in .  In , several experiments are conducted and the results are discussed.  We give a review of related works in  and the conclusions are drawn in .      In this work, we propose a BERT-enhanced NMT model called BERT-JAM which uses joint-attention to incorporate BERT representations into the NMT model. In contrast to existing models that only utilize BERT's last-layer representation, we make full use of BERT's intermediate representations by composing them through a GLU module. Ablation studies demonstrate that feeding BERT's intermediate representations into the NMT model does improve translation qualities.  Besides, we adopt a novel three-phase optimization strategy for training the model to overcome the catastrophic forgetting problem found by previous studies in the course of fine-tuning BERT for NMT models. We show that fine-tuning BERT as the last optimization step is beneficial to further boost the performance, but it's crucial that it is under control so that the model doesn't overfit on the training data. Additionally, by studying the impact of the size of BERT on the performance of BERT-enhanced NMT models, we find that increasing the embedding dimension of BERT, rather than its layer number, is a more cost-effective way to obtain performance gains. The comprehensive evaluation shows that BERT-JAM outperforms existing models and achieves new SOTA BLEU scores on multiple translation tasks, demonstrating the effectiveness of our method.   As in many previous works, our method of incorporating BERT into an NMT model introduces extra parameters and makes the training and inference time longer than a regular NMT model.  Although previous works have explored substituting the encoder of the NMT models with BERT to keep down the model size , such an approach has been shown to achieve limited performance gains . These trade-offs between performance and speed require further studies. We leave it as our future work to address this downside and study how to leverage BERT for NMT models while considering both speed and translation qualities.  
"," BERT-enhanced neural machine translation  aims at leveraging BERT-encoded representations for translation tasks. A recently proposed approach uses attention mechanisms to fuse Transformer's encoder and decoder layers with BERT's last-layer representation and shows enhanced performance. However, their method doesn't allow for the flexible distribution of attention between the BERT representation and the encoder/decoder representation. In this work, we propose a novel BERT-enhanced NMT model called BERT-JAM which improves upon existing models from two aspects: 1) BERT-JAM uses joint-attention modules to allow the encoder/decoder layers to dynamically allocate attention between different representations, and 2) BERT-JAM allows the encoder/decoder layers to make use of BERT's intermediate representations by composing them using a gated linear unit . We train BERT-JAM with a novel three-phase optimization strategy that progressively unfreezes different components of BERT-JAM. Our experiments show that BERT-JAM achieves SOTA BLEU scores on multiple translation tasks.",308
"   %Natural language is an abstract representation of thoughts and objects which have similar meaning through a community. We use raw sensory information to represent images. However, this is not possible for words, as they have no direct physical interpretation.  There have been many studies in natural language processing  to find suitable word representations  that carry information of a language. Even if finding these word representations can be computationally demanding, this can be advantageous since it is computed only once. These learned representations can be used for various downstream tasks.  Word2Vec  finds word embeddings by predicting a word given its neighborhood  or predicting its neighborhood given the word . Words that are used together have similar word embeddings due to the training strategy. However, these embeddings do not contain word order information and contextual information. ELMo  uses bidirectional LSTM   to predict a word given its context. Since BiLSTM is used for creating embeddings, both left-to-right and right-to-left contexts are implicitly encoded. Transformer  is shown to be more appropriate for training in large datasets due to its self-attention mechanism. OpenAI GPT  has the same objective as ELMo in the forward direction, except it uses transformer architecture. BERT  also uses transformer architecture with bidirectional pre-training tasks. Training objectives affect the information encoded in embeddings. Each objective and architecture presumes a different inductive bias.  In this work, we focused on BERT as it uses multiple training objectives. These objectives can create an inhibitory effect or a regulatory effect on each other. For this reason, we applied a hierarchical multitask learning approach to BERT by modifying its original structure. Our motivation is to create embeddings that encode the information from each task in a balanced way. Our contributions are as follows:      Our experimental results show that Lower NSP has a competitive performance when compared with the original BERT structure. We also evaluate the learned embeddings on probing tasks to provide useful insights into training strategies. Results on probing task experiments show that using bigram shift task for pre-training is useful for specific tasks. The remaining part of this paper is organized as follows. In Section , we mention related works. In Section , we explain our methods in detail. In Section , we report our experiment results. Lastly, we give a conclusion in Section .      We proposed to pre-train BERT with a hierarchical multitask learning approach. Our results on restricted data  show that this approach achieves better or equal performance. We incorporate sentence-level information to solve word-level tasks. This also shows a slight increment in performance. We propose an additional pre-training task, bigram shift, which causes embeddings to contain word order information.   We believe that implementing these techniques to large-scale training will further advance the state-of-the-art. Probing tasks show that different training techniques lead embeddings to contain different linguistic properties. This is an essential point since there are various problems in the NLP domain that require different needs. Therefore selecting an appropriate pre-training strategy is an important factor.  
"," Recent works show that learning contextualized embeddings for words is beneficial for downstream tasks. BERT is one successful example of this approach. It learns embeddings by solving two tasks, which are masked language model  and the next sentence prediction . The pre-training of BERT can also be framed as a multitask learning problem. In this work, we adopt hierarchical multitask learning approaches for BERT pre-training. Pre-training tasks are solved at different layers instead of the last layer, and information from the NSP task is transferred to the masked LM task. Also, we propose a new pre-training task bigram shift to encode word order information. We choose two downstream tasks, one of which requires sentence-level embeddings , and the other requires contextualized embeddings of words . Due to computational restrictions, we use the downstream task data instead of a large dataset for the pre-training to see the performance of proposed models when given a restricted dataset. We test their performance on several probing tasks to analyze learned embeddings. Our results show that imposing a task hierarchy in pre-training improves the performance of embeddings.",309
"  As technology advances in the rapidly developing era, the exponentially increasing of text data makes analyzing and understanding textual files a tedious work . From the readers' perspective, capturing the salient information from overwhelming documents is a labor-intensive and time-consuming task. The voluminous documents are urgently required to be processed more efficiently and the abundance of text data calls for text summarization techniques. Text summarization is one of the important tasks of natural language processing that automatically convert a text or a collection of texts within the same topic into a concise summary that contains key semantic information. The length of summaries is usually significantly less than the original text . The research on automatic text summarization has been attractive in the field of natural language processing  which can be beneficial for many downstream applications such as creating news digests, search, and report generation .   According to the number of input documents, text summarization can be cast into single document summarization and multi-document summarization. Single document summarization aims to form a summary from only one document while multi-document summarization aims at generating a short and informative summary across a set of topic-related documents. From the application perspective, single document summarization may not satisfy the requirement to produce comprehensive summaries, because it does not make good use of documents that are generated around the clock . For content to be summarized, it is more comprehensive and accurate to generate a summary from multiple documents written at different times, covering different perspectives. From the technical point of view, multi-document summarization is more complicated and difficult to tackle than single document summarization . This is because in the multi-document summarization task, there is more diverse and conflicting information among documents. The volume of documents is usually longer and the relations between documents are more complicated. In such large amount of documents, documents would inevitably be complement, overlapping and conflicting to each other .  In addition, excessively long input documents often lead to model degradation . It is challenging for models to retain the most critical contents of complex input sequences, while generating the coherent, non-redundant, non-factual error and grammatically readable summaries. Therefore, multi-document summarization requires models to have stronger capabilities for analyzing the corpora, identifying and merging consistent information. Furthermore, multi-document summarization task is more computation-hungry, due to the increasing sizes of current datasets and language model parameters.     Multi-document summarization task enjoys a wide range of real world applications, including summarization on news , scientific publications , emails , product reviews , lecture feedback , Wikipedia articles generation ,  medical documents  and software project activities . Recently, multi-document summarization technology has also received a great amount of attention in the industry. An intelligent multilingual news reporter bot named Xiaomingbot  was developed for news generation. This bot is able to summarize multiple news into one article and then translate it into multiple languages. Massive application requirements and rapidly growing online data promote the development of multi-document summarization. However, the majority of existing methods still generate summaries with manually crafted features , such as sentence position features , sentence length features , proper noun features , cue-phrase features , biased word features, sentence-to-sentence cohesion, sentence-to-centroid cohesion. The existing works using traditional algorithms can be divide into the following categories: term frequency-inverse document frequency  based methods , clustering based methods , graph based methods  and latent semantic analysis based methods .  Deep learning has gained enormous attention in recent years due to its success in various domains, for instance, computer vision , natural language processing  and multi-modal . Both industry and academia have been in a race to utilize deep learning to solve complex tasks due to its capability of capturing highly nonlinear relations of data. Recently, deep learning based models are applied in multi-document summarization , which prospers the development of text summarization and enables models to achieve better performance. Comparing to the conventional approaches, deep learning based models reduce dependence on manual feature extraction drastically. This task attracts increasing attention in the natural language processing community and enjoys steady expansion ever since. The number of research publications on deep learning based multi-document summarization is increasing rapidly over the last five years -- the statistics show the number of publications has 225\% increase from 2017 to 2019. It provides strong evidence for the inevitable pervasiveness of deep learning in multi-document summarization research.  The prosperity of deep learning for summarization in both academia and industry requires a comprehensive review of current publications for researchers to better understand the process and research progress. However, most of the existing summarization review articles are based on traditional algorithms instead of deep learning based methods . Therefore, we conduct this survey to embrace the knowledge of multi-document summarization. To the best of our knowledge, this is the first comprehensive survey in the direction of deep learning for multi-document summarization. This survey has been designed in a way such that it classifies the neural based multi-document summarization techniques into diverse categories thoroughly and systematically. We also conduct a detailed discussion on the categorization and progress of these approaches to establish a clearer concept standing in the shoes of readers. We hope this survey provides a panorama for researchers, practitioners and educators to quickly understand and step into the field of deep learning based multi-document summarization. The key contributions of this survey are three-folds:             In this article, we select, summarize, discuss, and analyze 30 representative works. We used Google Scholar as the main search engine to discover related works. The high-quality papers are selected from top NLP and AI journals and conferences, include  ACL\footnote{Annual Meeting of the Association for Computational Linguistics.}, EMNLP\footnote{Empirical Methods in Natural Language Processing.}, COLING\footnote{International Conference on Computational Linguistics}, NAACL\footnote{Annual Conference of the North American Chapter of the Association for Computational Linguistics.}, AAAI\footnote{AAAI Conference on Artificial Intelligence.}, ICML\footnote{International Conference on Machine Learning.}, ICLR\footnote{International Conference on Learning Representations} and IJCAI\footnote{International Joint Conference on Artificial Intelligence.}. The major keywords we used include  multi-documentation summarization, summarization, extractive summarization, abstractive summarization, deep learning and neural networks.         In the following sections, this survey will cover various aspects of recent advanced deep learning based works in multi-document summarization. Section  gives an overview of multi-document summarization. Section  highlights network design strategies and offers a comprehensive review of deep learning based multi-document summarization techniques. This survey also summarizes objective functions in the literature , evaluation metrics , and available multi-document datasets . Finally, section  discusses the future research directions for deep learning based multi-document summarization followed by the conclusion in Section .       In this article, we present the first comprehensive review of the most notable works to date on deep learning based multi-document summarization. We propose a taxonomy scheme for organizing and clustering existing publications and devise the network design strategies based on the state-of-the-art methods. Furthermore, we also provide an overview of the existing multi-document objective functions, evaluation metrics and datasets. Additionally, some of the most pressing open problems and promising future extensions are also discussed in this survey. We hope this survey can provide readers with a comprehensive understanding of the key aspects of the multi-document summarization task, clarify the most notable advances, and shed some light on future studies.         \setlength{ 
","  Multi-document summarization  is an effective tool for information aggregation which generates an informative and concise summary from a cluster of topic-related documents. Our survey structurally overviews the recent deep learning based multi-document summarization models via a proposed taxonomy and it is the first of its kind. Particularly, we propose a novel mechanism to summarize the design strategies of neural networks and conduct a comprehensive summary of the state-of-the-art. We highlight the differences among various objective functions which are rarely discussed in the existing literature. Finally, we propose several future directions pertaining to this new and exciting development of the field.",310
"  Computer-assisted cross-lingual conversation by automatic speech-to-speech translation has been one of the most challenging problems in spoken language technologies in decades . Recent remarkable advances in speech and language processing led by deep learning techniques benefit this challenge by real-time and accurate speech translation. %\memo{}  gogole multi-task model One crucial problem in automatic speech-to-speech translation is its delay. Spoken language processing tasks are usually handled in the utterance or sentence level. Their application to the speech-to-speech translation suffers from a long delay that is proportional to the input length, because the process starts after the observation of the end of an utterance. That is similar to consecutive interpretation and is not useful for long monologues such as lecture talks. On the other hand, in such situations, simultaneous interpretation is often used for an audience not proficient in the language of a talk. Simultaneous interpretation is a challenging task to listen to the talk and speak its interpretation in a different language.  In this work, we tackle the problem of automatic simultaneous speech-to-speech translation and develop a neural system to do that from English to Japanese. Here, we call our task simultaneous , not simultaneous . We think the task of simultaneous interpretation includes some additional efforts for summarization to make the output concise for small latency and better understanding for the audience. The problem requires real-time and incremental processing for the output generated simultaneously with the input. Previous attempts to incremental neural speech translation focused on speech-to-text translation . Our work aims to speech-to-speech translation for natural information delivery by speech without a need for visual attention on text-based subtitles. Our system is based on the cascade of three processing modules: incremental speech recognition , incremental machine translation , and text-to-speech synthesis , rather than recent end-to-end approaches %\memo{} due to the difficulty of applying them to the simultaneous translation.  We follow existing studies on incremental neural speech processing. For ASR, we choose an approach using a teacher-student training framework to train an incremental student model with the help of a non-incremental teacher model . For MT, we choose an approach called , which delays the start of the decoding process simply by  steps  . For TTS, we choose approach starting the segmental speech synthesis after observing the next accent phrase . These modules exchange their input/output symbols in the forms of subwords and work in a symbol-synchronous way, so they can be cascaded even if they have different waiting strategies.  We also conduct a system-level evaluation of our system in system-level latency and module-level performance on English-to-Japanese simultaneous translation on TED Talks. The system-level latency measures are:  processing delays for waiting and computation time, and  TTS speaking latency derived from overlaps of synthesized speech outputs. The module-level performance is measured by standard metrics in ASR, MT, and TTS. This work is the first attempt of system-level evaluation for a simultaneous speech-to-speech translation system and would be beneficial to future studies.  %The remainder of this paper is organized as follows. %In section , we review the problem of simultaneous speech-to-speech translation, mainly in its difficulty. %In section , we describe the details of the incremental processing modules for ASR, MT, and TTS. %In section , we present system-wise evaluation of our system, followed by some discussions in section . %We conclude this paper in section .               The latency results revealed that our incremental system based on the cascade of three modules worked successfully with relatively small delays. However, the quality results suggested the task difficulty due to error propagation from ISR to IMT and lack of in-domain corpora in English-Japanese MT. We show two translation examples in Table. The first one is a relatively good result, and the other one is a typical error propagation example. Tight module integration would be promising, such as lattice-to-sequence , but its extension to the simultaneous translation is not trivial.  Besides, we have no common evaluation metrics for simultaneous speech-to-speech translation other than module-level ones. We used two latency metrics in this work, but the objective measurement of content delivery by speech-to-speech translation is crucial for further evaluation.       In this paper, we presented our English-to-Japanese simultaneous speech-to-speech translation system with its evaluation using English TED talks. The system works fully-incremental with speech inputs, by the cascaded modules of incremental ASR, incremental MT, and incremental TTS. The latency evaluation revealed the module-level computation could be finished with about three seconds delay at maximum. However, the system suffers from speaking latency.  Our future work includes further improvement of the modules in accuracy and efficiency, and controlling speaking duration to decrease the speaking latency.   Part of this work was supported by JSPS KAKENHI Grant Numbers JP17H06101.    References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   -------------------------------------------------------------------------    
"," This paper presents a newly developed, simultaneous neural speech-to-speech translation system and its evaluation. The system consists of three fully-incremental neural processing modules for automatic speech recognition , machine translation , and text-to-speech synthesis . We investigated its overall latency in the system's Ear-Voice Span and speaking latency along with module-level performance.",311
" The emergence of online collaboration platforms has dramatically changed the dynamics of human teamwork, creating a veritable army of virtual teams, composed of workers in different physical locations. Software engineering requires a tremendous amount of collaborative problem solving, making it an excellent domain for team cognition researchers who seek to understand the manifestation of cognition applied to team tasks.  Mining data from social coding platforms such as GitHub can yield insights about the thought processes of virtual teams.  Previous work on issue comments has focused on emotional aspects of team communication, such as sentiment and politeness.  Our aim is to map issue comments to states in team cognition such as information gathering, knowledge building and problem solving.  To do this we employ dialogue act  classification, in order to identify the intent of the speaker.  Dialogue act classification has a broad range of natural language processing applications, including machine translation, dialogue systems and speech recognition.  Semantic-based classification of human utterances is a challenging task, and the lack of a large annotated corpus that represents class variations makes the job even harder. Compared to the examples of human utterances available in standard datasets like the Switchboard  corpus and the CSI Meeting Recorder Dialogue Act  corpus, GitHub utterances are more complex.   The primary purpose of our study is the DA classification of GitHub issue comments by harnessing the strength of transfer learning, using word and sentence level embedding models fine-tuned on our dataset.  For word-level transfer learning, we have used GLoVe vectors, and Universal Sentence Encoders and BERT models were used for sentence-level transfer. This paper presents a comparison of the performance of various  architectures on GitHub dialogues in a limited resource scenario.  A second contribution is our publicly available dataset of annotated issue comments. The dataset is available at \url{https://drive.google.com/drive/folders/1kLZvzfE80VeEYA1tqua_aj6nSiT57f83?usp=sharing}. In the field of computational collective intelligence,  where people collaborate and work in teams to achieve goals, dialogue act classification can play a vital role in understanding human teamwork.     This paper demonstrates a dialogue act classification system for GitHub issue comments. Due to the lack of publicly available training sets of formal teamwork dialogues, we formulated the problem as a transfer learning task, using both sentence-level and word-level embedding models to leverage information from the SwDA dataset. A significant contribution of our work is identifying the embedding model that performs best after fine-tuning on issue comments. We used GloVe, probabilistic representation, USE, and BERT embedding to train five different models. USE showed the best performance with an accuracy of 50.71\ . The low accuracy of USE on DA classification as compared to its accuracy on other state-of-the-art NLP tasks shows the complex nature of the dialogue act classification.  We evaluated many different settings for learning rates, epochs, and batch size; even though minor accuracy improvements were achievable, the performance of the embedding models remained fairly stable.    Our aim is to map issue comments to cognitive states in the Macrocognition in Teams Model .  Drawing from research on externalized cognition, team cognition, group communication and problem solving, and collaborative learning and adaptation, MITM provides a coherent theoretically based conceptualization for understanding complex team processes and how these emerge and change over time. MITM consists of five components: Team Problem-Solving Outcomes, Externalized Team Knowledge, Internalized Knowledge, Team Knowledge Building, and Individual Knowledge Building. It captures the parallel and iterative processes engaged by teams as they synthesize these components in service of team cognitive processes such as problem solving, decision making and planning. MITM has been applied to other team problem solving scenarios in military logistics and business planning but has never been used to analyze software engineering teams. Its usage in the domain of software engineering would be a major research contribution to the field of team cognition.    Although it is possible to directly label issue comments using an MITM code book, that type of labeling would be less compatible with existing dialogue act datasets.  Instead we are constructing a mapping that relates the DAMSL tagset to these cognitive states.  For instance, the question tags in DAMSL clearly relate to information gathering processes. Also many of the DAMSL classes are less relevant to the team cognition process and could be ignored.  The most commonly occurring classes in the GitHub issue comments  are all relevant to the Macrocognition in Teams Model, and we plan to tune our dialogue act classifiers to bolster the performance on these classes. In future work, we continue to improve the size and quality of our publicly-released dataset by recruiting more annotators to help with the labeling task and also more systematically studying inter-coder reliability.   
"," Social coding platforms, such as GitHub, serve as  laboratories for studying collaborative problem solving in open source software development; a key feature is their ability to support issue reporting which is used by teams to discuss tasks and ideas.  Analyzing the dialogue between team members, as expressed in issue comments, can yield important insights about the performance of virtual teams.  This paper presents a transfer learning approach for performing dialogue act classification on issue comments.  Since no large labeled corpus of GitHub issue comments exists, employing transfer learning enables us to leverage standard dialogue act datasets in combination with our own GitHub comment dataset. We compare the performance of several word and sentence level encoding models including Global Vectors for Word Representations , Universal Sentence Encoder , and Bidirectional Encoder Representations from Transformers . Being able to map the issue comments to dialogue acts is a useful stepping stone towards understanding cognitive team processes.",312
" Large, densely-labeled datasets are a critical requirement for the creation of effective supervised learning models. The pressing need for high quantities of labeled data has led many researchers to collect data from social media platforms and online forums . Due to the presence of noise and the lack of structure that exist in these data sources, manual quality analysis  is necessary to extract structured labels, filter irrelevant examples, standardize language, and perform other preprocessing tasks before the data can be used. However, obtaining dataset annotations in this manner is a time-consuming and expensive process that is often prone to errors.   In this work, we develop automated data cleaning and verification mechanisms for extracting high-quality data from social media platforms\footnote{All code is available at \url{https://github.com/rachel-1/qa_plausibility}.}. We specifically focus on the creation of question-answer datasets, in which each data instance consists of a question about a topic and the corresponding answer. In order to filter noise and improve data quality,  we propose the task of question-answer  plausibility, which includes the following three steps:   Depending on the type of dataset being constructed, the question posed to respondents may be generated by a machine or a human. We determine the likelihood that the question is both relevant and answerable.   We predict whether the user's response contains a reasonable answer to the question.   If the response is deemed to be plausible, we identify and extract the segment of the response that directly answers the question.    Because we assume social media users generally answer questions in good faith , we can assume plausible answers are correct ones . Necessarily, if this property were not satisfied, then any adequate solutions would require the very domain knowledge of interest. Therefore, we look to apply this approach toward data with this property.  In this study, we demonstrate an application of QA plausibility in the context of visual question answering , a well-studied problem in the field of computer vision . We assemble a large VQA dataset with images collected from an image-sharing social network, machine-generated questions related to the content of the image, and responses from social media users. We then train a multitask BERT-based model and evaluate the ability of the model to perform the three subtasks associated with QA plausibility. The methods presented in this work hold potential for reducing the need for manual quality analysis of crowdsourced data as well as enabling the use of question-answer data from unstructured environments such as social media platforms.     Deep learning studies are often hindered by lack of access to large datasets with accurate labels. In this paper, we introduced the question-answer plausibility task in an effort to automate the data cleaning process for question-answer datasets collected from social media. We then presented a multi-task deep learning model based on BERT, which accurately identified the plausibility of machine-generated questions and user responses as well as extracted structured answer labels. Although we specifically focused on the visual question answering problem in this paper, we expect that our results will be useful for other question-answer scenarios, such as in settings where questions are user-generated or images are not available.      Overall, our approach can help improve the deep learning workflow by processing and cleaning the noisy and unstructured natural language text available on social media platforms. Ultimately, our work can enable the generation of large-scale, high-quality datasets for artificial intelligence models.      
"," Datasets extracted from social networks and online forums are often prone to the pitfalls of natural language, namely the presence of unstructured and noisy data. In this work, we seek to enable the collection of high-quality question-answer datasets from social media by proposing a novel task for automated quality analysis and data cleaning: question-answer  plausibility. Given a machine or user-generated question and a crowd-sourced response from a social media user, we determine if the question and response are valid; if so, we identify the answer within the free-form response.   We design BERT-based models to perform the QA plausibility task, and we evaluate the ability of our models to generate a clean, usable question-answer dataset. Our highest-performing approach consists of a single-task model which determines the plausibility of the question, followed by a multi-task model which evaluates the plausibility of the response as well as extracts answers .",313
" Underresourced languages, from a natural language processing  perspective, are those lacking the resources  needed to support state-of-the-art performance on NLP problems like machine translation, automated speech recognition, or named entity recognition. Yet the vast majority of the world's languages---representing billions of native speakers worldwide---are underresourced. And the lack of available training data in such languages usually reflects a broader paucity of electronic information resources accessible to their speakers.  %The most prominent of these languages have millions of native speakers, who have previously been deprived of access to information on the web in their native language, due to missing translation tools.   For instance, there are over six million Wikipedia articles in English but fewer than sixty thousand in Swahili and fewer than seven hundred in Bambara, the vehicular and most widely-spoken native language of Mali that is the subject of this paper. Consequently, only 53\% of the worlds population have access to ``encyclopedic knowledge'' in their primary language, according to a 2014 study by Facebook. MT technologies could help bridge this gap, and there is enormous interest in such applications, ironically enough, from speakers of the languages on which MT has thus far had the least success. There is also great potential for humanitarian response applications .  Fueled by data, advances in hardware technology, and deep neural models, machine translation  has advanced rapidly over the last ten years.  %Yet underresourced languages have yet to benefit from these advances, because they lack the large volumes of translated texts needed to drive neural machine learning.  %Although neural models are generally considered to work best in domains where large amounts of training data exist Researchers are beginning to investigate the effectiveness of   low-resource languages, as in recent WMT 2019 and WMT 2020 tasks , and in underresourced African languages. %What has been done? Which challenges have been identified, which solutions have been found?  Most prominently, the   community, a grassroots initiative, has developed open-source NMT models for over 30 African languages on the base of the JW300 corpus~, a parallel corpus of religious texts.   Since African languages cover a wide spectrum of linguistic phenomena and language families , individual development of translations and resources for selected languages or language families are vital to drive the overall progress. Just within the last year, a number of dedicated studies have significantly improved the state of African NMT:  analyzed the depth of Transformers specifically for low-resource translation of South-African languages, based on prior studies by   on the Autshumato corpus~.  developed an MT model and compiled resources for translations between Fon and French,  modeled translations between English and Hausa,  for four languages of the Edoid language family, and  investigated supervised vs. unsupervised NMT for Nigerian Pidgin.   %Superficially, it might seem like this development simply grows the pool of ``standard MT'' evaluation tasks and data sets, with some data sets being smaller than others.    In this paper, we present the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara. We discuss challenges in working with low-resource languages and propose strategies to cope with data scarcity in low-resource MT.   We discuss the socio-cultural context of Bambara translation and its implications for model and data development. Finally, we analyze our best-performing neural models with a small-scale human evaluation study and give recommendations for future development. %These contributions a deeper understanding of the shortcomings of state-of-the-art methods for high-resource languages, and  We find that the translation quality on our in-domain data set is acceptable, which gives hope for other languages that have previously fallen under the radar of MT development.      % [Can model and data be made publicly available?] We released our models and data upon publication. Our evaluation setup may serve as benchmark for an extremely challenging translation task.  %  Our study constitutes the first attempt of modeling automatic translation for the extremely low-resource language of Bambara. We identified challenges for future work, such as the development of alignment tools for small-scale datasets, and the need for a general domain evaluation set. The current limitation of processing written text as input might furthermore benefit from the integration of spoken resources through speech recognition or speech translation, since Bambara is primarily spoken and the lack of standardization in writing complicates the creation of clean reference sets and consistent evaluation.     
"," %Low-resource languages present unique challenges to machine translation.  %We discuss the case of Bambara, a Mande language where training data is scarce and requires significant amounts of pre-processing. Moreover, the socio-cultural context within which Bambara speakers live poses challenges for automated processing. We contribute the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara.  %New abstract?:  Low-resource languages present unique challenges to  machine translation. We discuss the case of Bambara, a Mande language for which training data is scarce and requires significant amounts of pre-processing. More than the linguistic situation of Bambara itself, the socio-cultural context within which Bambara speakers live poses challenges for automated processing of this language. In this paper, we present the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara. We discuss challenges in working with low-resource languages and propose strategies to cope with data scarcity in low-resource machine translation .",314
"  . } To foster research on dialog policy learning for virtual digital assistants, several task-oriented dialog corpora have been introduced in recent years, such as SimDial, MultiWoZ, Taskmaster, and Schema Guided Dialog, to name a few.  Deep learning approaches, including mixture models hierarchical encoder/decoder, reinforcement learning, and pre-trained language models, have significantly advanced dialog policy research in the past few years, setting new state-of-the-art performance limits.  %More recently, SimpleTOD and SOLOIST have shown that pre-training dialog policy using large language models, e.g., GPT-2, can lead to significantly better performance on task-oriented neural dialog policy learning by using even larger neural models.  %\ab{somewhere we want to mention that data collection is expensive -- both in time; and other resources}  However, collecting annotated data for supervised dialog policy learning is an expensive and time-consuming process. Hence, it is desirable to explore approaches to train dialog policy with limited data and transfer an existing policy with few or even no additional training data to new domains.  This practical requirement has motivated the community to research resource-constrained dialog policy learning in the past few decades. Researchers have explored approaches including employing grammar constraints for dialog policy, transfer learning , or pre-trained language models. Few-shot domain adaptation has been researched since the 2000s on both end-to-end dialog systems  as well as dialog policy learning.  % We investigate one-shot policy learning and zero-shot domain transfer using \ab{I think up to 50 examples for original training is reasonable. Emphasizing that there is only ONE training sample available may not be reasonable. The reviewer may come back and say why 1? Why not 5? Why not 10? So showing that other methods need thousands of samples to match the performance of DILP would be a more convincing argument IMHO.} \ab{We  don't believe a single dialog is going to be representative of ALL of the conversational flows that may occur in a more complicated real-life dataset . Increasing the number of training samples beyond one may help with improving the policy on MultiWoZ, not necessarily in terms of inform and success but in terms of action F1 .} \ab{Finally, zero-shot transfer is an extremely desirable property. One can argue that they don't want to re-train and want their method to work on a new domain out of the box. Having said that, your argument would be even stronger if you could also show that the baseline methods need X shot transfer to match the performance of zero-shot DILP on a new domain.} differential inductive logic programming .   % Given an encoding of the common known knowledge and a set of examples represented as a logical set of facts, an inductive logic programming  system can be hypothesised  which conforms to all of the positive and none of the negative examples. % ILP has been extensively studied in the context of symbolic AI in the past few decades, and has been also adapted to dialog management. While ILP generalizes well from a noiseless/consistent set of rules, it is known to be prone to noisy samples , and hence would not be applicable to real-world problem scenarios, and particularly noisy dialog policy data. DILP addresses the noisy/inconsistent training data via a novel probabilistic treatment of the learned rules by relaxing them to have different probabilities of being true/false and solving the relaxed problem using recent advances in gradient-based optimization.   We present   We introduce \name in Section. We apply \name to the SimDial Dataset , and MultiWoZ Dataset , showing that on the task of one-shot dialog policy learning and zero-shot domain transfer, \name outperforms several other neural baselines. Finally, Section concludes this paper.      In this paper, we introduce \mathtt{uncertain}$ with the predicted value, but it need extra labels.  The future work would naturally focus on solving these shortcomings. One way might be to jointly, in a multi-task setting, predict the templates or meta-learn them.    
","   Motivated by the needs of resource constrained dialog policy learning,   we introduce dialog policy via differentiable inductive logic . We explore the tasks of one-shot learning and zero-shot domain transfer with  representative dialog from the restaurant domain, we train \name on the SimDial dataset and obtain 99+\% in-domain test accuracy. We also show that the trained DILOG zero-shot transfers to all other domains with 99+\% accuracy, proving the suitability of \name to slot-filling dialogs. We further extend our study to the MultiWoZ dataset achieving 90+\% inform and success metrics. We also observe that these metrics are not capturing some of the shortcomings of DILOG in terms of false positives, prompting us to measure an auxiliary Action F1 score. We show that \name is 100x more data efficient than state-of-the-art neural approaches on MultiWoZ while achieving similar performance metrics. We conclude with a discussion on the strengths and weaknesses of \name.",315
"  Sequence labeling tasks are essential in web mining, such as named entity recognition , event extraction, and relation identification. For example, the NER models assign the predefined labels to tag tokens in the input sequences to indicate both the entity boundaries and types. In some web services, such as question answering, sequence labeling also plays a critical role, where it reads a passage in a Web page as the context and answers a given question by extracting a text span inside the given passage. This process is often called machine reading comprehension . MRC is also regarded as a sequence labeling task, since it predicts whether each token is the start, end, or none for the answer span.  There is a rich literature for sequence labeling. Classical methods include Hidden Markov models , maximum entropy Markov models , and conditional random field . Recently, combining neural networks as the representation layer with CRF models has further boosted the state-of-the-art performance. However, such statistical models require large amounts of training data. Consequently, they only show good performance in languages with rich training data, such as English. Sequence labeling on low-resource languages is still very challenging, mainly due to very limited training data available.  To tackle the challenge of sequence labeling in low-resource languages, some early works transfer the knowledge from rich-source languages to low-resource ones by information alignment through manually built bilingual parallel corpora,  or language-independent features. In recent years, multilingual pre-trained language models, such as Unicoder, mBERT, and XLM-Roberta , are developed for model transferring. For example, Wu et al.  fine-tune mBERT on a pseudo training set by a meta-learning method. To better leverage the unlabeled data in the target language, a teacher-student framework is proposed to distill knowledge from weighted teacher models. Inspired by back translation in neural machine translation , DualBERT is developed to learn source language and target language features simultaneously. Although these multilingual sequence labeling models can effectively locate target spans, they often fail to give the precise boundaries of the spans in the target languages.  %when predicting text spans in the target languages.  %that is, pairs of sentences with similar meanings but in different languages, %\jp{What is the conclusion we can draw from this paragraph?}  %The previous multilingual sequence labeling models can roughly identify the correct target spans, but often fail to give the precise boundaries when predicting text spans in the target languages.  We conduct an empirical study to quantitatively assess the challenge. In Figure , we categorize the mismatches between the predicted span and the ground truth span into four types:  the predicted answer is a { of the ground truth;  the predicted answer both miss some terms in the ground truth and add extra terms not in the ground truth , and   the predicted answer is adjacent to the ground truth but contains no common sub-span with it .  We further show in Table the statistics of the error cases in the cross-lingual NER task using the XLM-R model, where the boundary errors, including , , , and , contribute to a large portion of all error cases as shown in the last column. The other errors cases are mainly entity type detection errors. This observation motivates us to tackle the bottleneck of boundary detection in sequence labeling models.      [t]     $ baseline on the XGLUE-NER dataset. Detailed definitions of each error category are shown in Figure.}      \tiny     %      {l|cc|cccc|c}     \toprule      & \#Test & \#Error & \#Super & \#Sub & \#Drifted & \#Adjacent & Boundary error  \\     \midrule     en & 6,392  & 566   & 106 & 64  & 1 & 137 & 54.4 \\     es & 4,054  & 955   & 93  & 246 & 0 & 295 & 66.4 \\     de & 5,390  & 1,648 & 201 & 150 & 4 & 884 & 75.2 \\     nl & 6,884  & 949   & 148 & 96  & 0 & 42  & 30.1 \\          %         Accurately detecting answer boundaries becomes a bottleneck in sequence labeling.  To tackle the challenge, in this paper, we propose a separate model for boundary calibration based on the output of a base model. Intuitively, the base model captures the global context of the whole input sequence and roughly locates the region for answers. Then, the calibration model conducts finer search within the detected region and the neighborhood, and focuses on the local context to refine the boundary. This is analogous to the human perception and cognition process, which first locates the target, sets up the local context, and finally zooms into details.  Our design is novel for sequence labeling, and is orthogonal and complements to all existing approaches.  Using a second model to focus on detecting answer boundaries accurately is an intuitive and nice idea.  However, how to construct high-quality training data for the calibration model remains challenging. One straightforward method is to transform the original training data of sequence labeling task into a new training set for calibration model. However, the data collected in this way is still quite limited, especially for low-resource languages. To address this challenge, we strategically propose a novel phrase boundary recovery  task to pre-train the model on large-scale augmented datasets synthesized from Wikipedia documents in multiple languages. The new pre-training approach dramatically improves the capability of the calibration module to determine answer boundaries accurately.  % Besides the design of employing two models, we further equip the calibration model with a pre-training process by emphasizing on the capability of recovering meaningful phrases from noisy input.  Our approach is shown in Figure. CalibreNet consists of two modules, a base module and a calibration module. The base module can take any model of sequence labeling. The predicted answers by the base module are combined with the input sequence to form the input to the calibration module. The calibration module considers both the initial results by the base module and the whole passage to refine the span boundaries. In particular, the calibration module is pre-trained with the PBR task on large-scale multilingual synthesized data from Wikipedia-derived corpus.  We make the following technical contributions in this paper. First, we propose the CalibreNet framework for the task of cross-lingual sequence labeling to improve the accuracy of labeled answers. Second, we propose a novel phrase boundary recovery task and a weakly supervised pre-training method using Wikipedia data. This approach effectively enhances the model sensitivity to phrase boundaries. Last but not least, we conduct extensive experiments on zero-shot cross-lingual NER and improve the SOTA results. In addition, the experiments on the MRC tasks also show consistent improvement over strong baseline methods.  The rest of the paper is organized as follows. We first review the related work in Section. We then present our approach in Section. We report the extensive experimental results in Sections.  We conduct further analysis in Section, and conclude the paper in Section.     In this paper, we tackle the challenge of detecting span boundaries more precisely for sequence labeling tasks in low-resource languages. We propose the CalibreNet architecture as well as a novel Phrase Boundary Recovery task for more accurate boundary detection. Extensive experimental results verify the effectiveness of our approach and the generalization capability for multiple languages. As future works, we plan to introduce entity type prediction in the pre-training task, and also develop better methods for question generation for the MRC task.        The acknowledgments section is defined using the ""acks"" environment    . This ensures the proper    identification of the section in the article metadata, and the    consistent spelling of the heading.         The next two lines define the bibliography style to be used, and    the bibliography file.          If your work has an appendix, this is the place to put it.       
","  \footnotetext[1]{Work done during the first author's internship at Microsoft STCA.} \footnotetext[2]{Daxin Jiang and Wanli Zuo are the corresponding authors.} % \footnotetext[3]{Jian Pei's research is supported in part by the NSERC Discovery Grant program. All opinions, findings, conclusions and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.}   Lack of training data in low-resource languages presents huge challenges to sequence labeling tasks such as named entity recognition  and machine reading comprehension . One major obstacle is the errors on the boundary of predicted answers. To tackle this problem, we propose CalibreNet, which predicts answers in two steps. In the first step, any existing sequence labeling method can be adopted as a base model to generate an initial answer. In the second step, CalibreNet refines the boundary of the initial answer. To tackle the challenge of lack of training data in low-resource languages, we dedicatedly develop a novel unsupervised phrase boundary recovery pre-training task to enhance the multilingual boundary detection capability of CalibreNet. Experiments on two cross-lingual benchmark datasets show that the proposed approach achieves SOTA results on zero-shot cross-lingual NER and MRC tasks.",316
"  The Text-to-SQL task aims to translate natural language texts into SQL queries. Users who do not understand SQL grammars can benefit from this task and acquire information from databases by just inputting natural language texts. Previous works  focus on context-independent text-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task.   Recently, context-dependent text-to-SQL task has attracted more attention.  conduct experiments on ATIS dataset . Besides, two cross-domain context-dependent datasets SParC  and CoSQL  are released. Cross-domain means databases in test set differ from that in training set, which is more challenging.   EditSQL  is the previous state-of-the-art model on SParC and CoSQL datasets and it focuses on taking advantages of previous utterance texts and previously predicted query to predict the query for current turn. Table  shows the user inputs, ground truth queries and predicted queries of EditSQL for an interaction. In the second turn, EditSQL views ``Kacey"" as the name of a dog owner. However, since the context of the interaction is about dogs, ``Kacey"" should be the name of a dog. This example shows that a model using only historical information of user inputs may fail to keep context consistency and maintain thematic relations.   According to  and , to maintain thematic relations, users may change constraints, ask for different attributes for the same topic when they ask the next questions. Thus, database schema items  in current turn should have relation with items in previous turn. For example, in Table , the second question  adds a constraint of the name and asks for the age of a dog instead of the numbers of all dogs. The corresponding database schema items Dogs.age and Dogs.name in   belong to the same table as Dogs.* in previous query . Therefore, we propose to take historical information about database schema items into consideration.  %[tbp] %       In particular, we first construct a graph based on corresponding database, where graph nodes are database schema items and graph edges are primary-foreign keys and column affiliation. Short distance between graph nodes appearing in previous query and current query can reveal the context consistency since there is usually an edge between the different attributes of the same topic. We then propose a database schema interaction graph encoder to model database schema items together with historical items. Empirical results on two large cross-domain context-dependent text-to-SQL datasets - SParC and CoSQL show that our schema interaction graph encoder contributes to modeling context consistency and our proposed model with database schema interaction graph encoder substantially outperforms the state-of-the-art model.     [tbp]     ^1y^1x^2\Tilde{y}^2y^2x^3\Tilde{y}^3y^3x^4\Tilde{y}^4y^4 means that query is predicted by a model, which is EditSQL here.}        Our main contributions are summarized as follows:        .          In this paper, we focus on context-dependent cross-domain SQL generation task. We find that previous state-of-the-art model only takes historical user inputs and previously predicted query into consideration, but ignores the historical information of database schema items. Thus we propose a model named IGSQL to model database schema items in a conversational scenario. Empirical results demonstrate the efficacy of our model. We also conduct ablation experiments to reveal the significance of our database schema interaction graph encoder. For future work, we  will explore methods attempting to solve hard and extra hard questions.     This work was supported by National Natural Science Foundation of China , Beijing Academy of Artificial Intelligence  and Key Laboratory of Science, Technology and Standard in Press Industry . We appreciate the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.  
"," Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historical user inputs. In this work, in addition to using encoders to capture historical information of user inputs, we propose a database schema interaction graph encoder to utilize historicalal information of database schema items. In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens. We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.",317
" Coherence refers to the properties of a text that indicate how meaningful sentential constituents are connected to convey document-level meaning. Different theories have been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text -- by constructing an entity-grid  representation, building on Centering Theory. Subsequent work has adapted and further extended Egrid representations.  Other research has focused on syntactic patterns that co-occur in text or semantic relatedness between sentences as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units or capturing topic shifts via Hidden Markov Models~. Other work has combined approaches to study whether they are complementary.  More recently, neural networks have been used to model coherence. Some models utilize structured representations of text~ and others operate on unstructured text, taking advantage of neural models' ability to learn useful representations for the task.  Coherence has typically been assessed by a model's ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document , and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order, evaluating on realistic data and focusing on open-domain models of coherence. However, less attention has been directed to investigating and analyzing the properties of coherence that current models can capture, nor what knowledge is encoded in their representations and how it might relate to aspects of coherence.   In this work, we systematically examine what properties of discourse coherence current coherence models can capture. We devise two datasets that exhibit various kinds of incoherence and analyze model ability to capture syntactic and semantic aspects of text implicated in discourse organisation.  We furthermore investigate a set of probing tasks to better understand the information that is encoded in their representations and how it might relate to aspects of coherence.  We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further.  Finally, we release our evaluation datasets as a resource for the community to use to test discourse coherence models.       Our evaluation experiments on two coherence datasets reveal that RNN- or EGrid-based coherence models are able to detect syntactic alterations that undermine coherence, but are less effecient at detecting semantic ones even after fine-tuning on the latter.  We furthermore find that they particularly struggle with recognizing minor lexical changes even if they result in implausible meaning and resolving pronominal references.  On the other hand, these models are particularly good at detecting cases where a prefix is inserted or the subject pronoun is substituted with a lexical item, suggesting that they are capable of capturing the relevant syntactic patterns and do not solely rely on positional features.   We find that the best performing model overall is LCD which does not use an RNN sentence encoder but rather builds sentence representations by averaging BERT embeddings then utilizes a number of linear transformations over adjacent sentences to facilitate learning richer representations.    Our probing experiments reveal that models are better at encoding information regarding subject and object number  followed by verb number . These probing tasks align with Centering theory as they probe for subject and object relevant information. The task that tests for knowledge on coordination inversion is the lowest performing one overall, suggesting that there is little capacity at capturing information related to intra-sentential coherence. Excluding LCD, MTL is the best performing model; nevertheless, there is still scope for substantial improvement across all probing tasks and particularly on CoordInv and CorruptAgr.        We systematically studied how well current models of coherence can capture aspects of text implicated in discourse organisation. We devised datasets of various kinds of incoherence and examined model susceptibility to syntactic and semantic alterations. Our results demonstrate the models are robust with respect to corrupted syntactic patterns, prefix insertions and lexical substitutions. However, they fall short in capturing rhetorical and semantic corruptions, lexical perturbations and corrupt pronouns. We furthermore find that discourse embedding space encodes subject and object relevant information; however, there is scope for substantial improvement in terms of encoding linguistic properties relevant to discourse coherence. Experiments on coordination inversion further suggest that current models have little capacity at encoding information related to intra-sentential coherence.  We hope this study shall provide further insight into how to frame the task of coherence modeling and improve model performance further. Finally, we make our datasets publicly available for researchers to use to test coherence models.        
","  In this work, we systematically investigate how well current models of coherence can capture aspects of text implicated in discourse organisation. We devise two datasets of various linguistic alterations that undermine coherence and test model sensitivity to changes in syntax and semantics. We furthermore probe discourse embedding space and examine the knowledge that is encoded in representations of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we make our datasets publicly available as a resource for researchers to use to test discourse coherence models.",318
"  Early detection of dementia is important for improving clinical outcomes and management of dementia, as well as for lifestyle, financial, and future planning for patients and their caregivers . Yet, dementia is not formally diagnosed or coded in claims for over 50\% of older adults living with probable dementia . Tools that screen medical records for warning signs and present the digested information to providers may prove to be an important step for early intervention.  In this study, we aim to use NLP to detect signs of cognitive dysfunction from clinician notes in electronic health records  by applying deep learning techniques that have not been hitherto applied to this problem. We present an attention-based transformer model that allows for long text sequences to reveal signs of cognitive concerns and compare its performance to baseline models.      We applied NLP algorithms to identify patients with cognitive concerns in EHR and compared model performance. While the deep learning model's performance was the best, it was only marginally better than the term based NLP models. We posit that deeper representations will be required for more complex tasks requiring syntactical and contextual information such as classifying the stage of cognitive impairment: MCI, mild, moderate or severe dementia. Our gold standard set had a relatively smaller proportion of patients with subjective concerns or mild cognitive impairment , and the overall sample size was small. To address these issues, we plan to implement an active learning loop, starting from querying additional at-risk patients over age 65 and without a dementia related ICD code or medication, and apply the fine-tuned model to derive the probability of having cognitive concerns for these patients. For the edge cases, the notes will be manually reviewed and labeled. To improve the efficiency of this review process, we designed an annotation tool that highlights the sections with regular expression matches or higher attention weights . The new gold-standard data will serve as the basis for the next iteration of the active learning loop to further improve model performance and potentially detect patients with an earlier stage of cognitive decline.  \clearpage         \clearpage   
","   Dementia is under-recognized in the community, under-diagnosed by healthcare professionals, and under-coded in claims data. Information on cognitive dysfunction, however, is often found in unstructured clinician notes within medical records but manual review by experts is time consuming and often prone to errors. Automated mining of these notes presents a potential opportunity to label patients with cognitive concerns who could benefit from an evaluation or be referred to specialist care.  In order to identify patients with cognitive concerns in electronic medical records, we applied natural language processing  algorithms and compared model performance to a baseline model that used structured diagnosis codes and medication data only. An attention-based deep learning model outperformed the baseline model and other simpler models.",319
" A spelling corrector is an important and ubiquitous pre-processing tool in a wide range of applications, such as word processors, search engines and machine translation systems. %The popularity of mobile devices makes it increasingly crucial since typing on virtual keyboards is more error-prone. Having a surprisingly robust language processing system to denoise the scrambled spellings, humans can relatively easily solve spelling correction . %spelling correction is a relatively easy task for humans, who have a surprisingly robust language processing system  to denoise the scrambled spellings.  However, spelling correction is a challenging task for a machine, because words can be misspelled in various ways, and a machine has difficulties in fully utilizing the contextual information.   Misspellings can be categorized into , which is out-of-vocabulary, and the opposite,  misspellings . The dictionary look-up method can detect non-word misspellings, while real-word spelling errors are harder to detect, since these misspellings are in the vocabulary . In this work, we address the   spelling correction problem. It only corrects the spelling of each token without introducing new tokens or deleting tokens, so that the original information is maximally preserved for the down-stream tasks. %\textcolor{red}{[The last few sentences of this paragraph is not good, what are you trying to express?]}  We formulate the  spelling correction as a sequence labeling task and jointly detect and correct misspellings. Inspired by the human language processing system, we propose a novel solution on the following aspects:  We encode both spelling information and global context information in the neural network.  We enhance the real-word correction performance by initializing the model from a pre-trained language model .  We strengthen the model's robustness on unseen non-word misspellings by augmenting the training dataset with a synthetic character-level noise. As a result, our best model  outperforms the previous state-of-the-art result  by  absolute  score.  %As a result, we present a simple but powerful solution to the  spelling correction by simply fine-tuning a pre-trained LM to jointly detect and correct both non-word and real-word misspellings  as a sequence labeling task.  %We propose a novel solution by using transformer-encoders  to jointly perform detection and correction of misspellings. We extensively explore various training techniques. Our results show that a transformer-encoder-based architecture that encodes both local character-level and global word-level representations yields a strong performance. Specifically, both the combination of word embedding and character embedding or a subword embedding  produce strong models. We further obtain a state-of-the-art model by initializing the weight from a pre-trained language model   and training it on an augmented training dataset with a synthetic character-level noise. \textcolor{red}{[this paragraph need to rewrite. Please summarize your contribution in a coherent story. ]}  %We also explore additional training techniques such as leveraging a pre-trained language model  and adding more noise to the training corpora. Our results show that fine-tuning a pre-trained LM  with a subword embedding  yields a strong model. Furthermore, we obtain a state-of-the-art model by training it on a noisy corpus synthesized by randomly replacing correct words and characters with natural misspellings and random character. Finally, under the condition of no pre-training, we propose a strong model that outperforms the subword model by combining word and character embedding.      {[after reading introduction section, reviewers need to know roughy how you do? However, you just preset a lot what you do]}   % \fi    Formally, given a noisy input sentence , each noisy word  is drawn from a distribution of possible misspellings of the correct word $w_k{[as I said, this definition do not need on section]} \fi    We leverage novel approaches to combine spelling and context information for  spelling correction, and achieved state-of-the-art performance. Our experiments gives insights on how to build a strong  spelling corrector:  combine both spelling and context information,  leverage a pre-trained LM and  use the synthetic character-level noise.   Such a spelling corrector can serve as a pre-processing tool that maximally preserves the semantic information for various down-stream tasks, such as machine translation.           We build a state-of-the-art model by fine-tuning a pre-trained LM with a subword embedding on a noisy corpus synthesized by randomly replacing correct words and characters with natural misspellings and random character. Furthermore, we hypothesize that training a word-level and character-level combined encoder with sufficient data and appropriate pre-training tasks may produce a new state-of-the-art spelling correction model.              \clearpage    \clearpage   
"," Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings.  On the contrary, humans can easily infer the corresponding correct words %\textcolor{red}{the semantics of unknown words:the corresponding correct words of misspellings}  from their misspellings and surrounding context. Inspired by this, we address the  spelling correction problem, which  %\textcolor{red}{[do not know which refers to what, confusing, please rewrite; at the same time, can you brief introduce your novel solution here?]}  only corrects the spelling of each token without additional token insertion or deletion, by utilizing both spelling information and global context representations. We present a simple yet powerful solution that jointly detects and corrects misspellings as a sequence labeling task by fine-turning a pre-trained language model. Our solution outperform the previous state-of-the-art result by $12.8\%$ absolute $F_{0.5}$ score. %Furthermore, we obtain a state-of-the-art model by augmenting the training data with synthetic character-level noise. %We also provide three useful training techniques. Our results show that a transformer-based model that encodes both local character-level and global word-level representations yields a strong performance. Furthermore, a state-of-the-art model is obtained by leveraging pre-trained language model and augmenting the training corpus with synthetic character-level noises. %fine-tuning a pre-trained language model with a subword embedding yields a strong model. Furthermore, we obtain a state-of-the-art model by training it on a noisy corpus synthesized by randomly replacing correct words and characters with common misspellings and random characters. We also propose a strong architecture that combines character and word level encoder without pre-training.",320
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Conversational technologies offer a remarkable addition to the current approaches for providing mental healthcare. Communications with these conversational agents have been found to include discoverable psychological distress signs, such as the rate of filled pauses, speech rate, and various temporal and turn-related characteristics . In the human-human automatic analysis of patient-doctor conversations, it has also been found that different types of disfluency can indicate levels of adherence to medication . Markers of disfluency also hold predictive power for the identification of cognitive disorders .  Such devices are mainly used for processing content, which is then analyzed offline. There is much work on detecting disfluencies for offline analysis of transcripts with gold standard utterance segmentation within much of the current effort on disfluency detection on telephone conversations begun by . However, given that these models do not operate for live systems and rely on rich transcription data, including the pre-segmentation of dialogue acts, to facilitate more cost-effective study of other data, it would be easier to be able to perform directly and incrementally off the speech signal, or at least from automatic speech recognition  results as they arrive into the system. The incremental model must work with minimum latency as it receives word-by-word data and does so without modifying its initial assumptions and providing its best decisions as soon as possible in line with the principles set out in .  We combine incremental identification of disfluencies with three other essential tasks for active conversational models to provide a favorable inductive bias to disfluency detection and to study the way these tasks interact. We explore a multi-task learning  framework to enable the training of one universal model with four tasks of disfluency detection, language modelling, part-of-speech  tagging, and utterance segmentation, which in the data we use is also equivalent to dialogue act segmentation. Multi-task learning seeks to improve learning efficiency and predictive power by learning from a shared representation with multiple objectives. We investigate the entire power set of these tasks to investigate the interaction between them. We experiment with two different losses: a naive weighted sum of losses where the weights of loss are uniform and a loss function based on maximizing the Gaussian likelihood with task-dependent uncertainty.  We train and test a simple neural model for the different tasks, experiment with all the combinations of the tasks, different loss functions for each of the tasks, and experiment with different input representations .       We have presented a multi-task learning framework to enable the training of one universal incremental model with four tasks of disfluency detection, language modelling, part-of-speech tagging and utterance segmentation. We have observed that these tasks produce favorable inductive biases to each other, with utterance segmentation and disfluency detection getting the most benefits. We note that each task's optimal weighting relies heavily on the severity of the noise from the task. We showed that word timing information helps utterance segmentation and disfluency detection in an online setting, and adding new tasks with the exception of language modelling does not have a remarkable negative effect on the incremental metrics.   The results show that our framework can be suitable for online conversational systems, such as conversational agents in the mental health domain. In future work, we intend to analyze the interactions between different tasks as they occur in real time. Monitoring the interaction after each word could help highlight informative moments that contribute more to optimisation of our models. Furthermore, we intend to use raw acoustic features as the input for a strongly time-linear model.   include your own bib file like this:  
","  We present a multi-task learning framework to enable the training of one universal incremental dialogue processing model with four tasks of disfluency detection, language modelling, part-of-speech tagging, and utterance segmentation in a simple deep recurrent setting. We show that these tasks provide positive inductive biases to each other with the optimal contribution of each one relying on the severity of the noise from the task. Our live multi-task model outperforms similar individual tasks, delivers competitive performance, and is beneficial for future use in conversational agents in psychiatric treatment.",321
" We introduce \diagnnose, an open source library for analysing deep neural networks. The \diagnnose library allows researchers to gain better insights into the internal representations of such networks, providing a broad set of tools of state-of-the-art analysis techniques. The library supports a wide range of model types, with a main focus on NLP architectures based on LSTMs  and Transformers .  Open-source libraries have been quintessential in the progress and democratisation of NLP. Popular packages include HuggingFace's   -- allowing easy access to pre-trained Transformer models; % AllenNLP  -- providing useful abstractions over components in the NLP pipeline,   -- focusing on multitask and transfer learning within NLP;   -- providing a range of feature attribution methods; and   -- a platform for visualising and understanding model behaviour. We contribute to the open-source community by incorporating several \mbox{interpretability} techniques that have not been present in these packages.  Recent years have seen a considerable interest in improving the understanding of how deep neural networks operate . The high-dimensional nature of these models makes it notoriously challenging to untangle their inner dynamics. This has given rise to a novel subfield within AI that focuses on interpretability, providing us a peak inside the black box. \diagnnose aims to unify several of these techniques into one library, allowing interpretability research to be conducted in a more streamlined and accessible manner.  \diagnnose's main focus lies on techniques that aid in uncovering linguistic knowledge that is encoded within a model's representations. The library provides abstractions that allow recurrent models to be investigated in the same way as Transformer models, in a modular fashion. It contains an extensive activation extraction module that allows for the extraction of  model activations on a corpus. The analysis techniques that are currently implemented include: [leftmargin=.5cm]    , and control tasks .      that retrieve a feature's contribution to a model prediction .      Our implementation is model-agnostic, which means that any type of model architecture can be explained by it.    % <design principles> ?  In this paper we present both an overview of the library, as well as a case study on subject-verb agreement within language models. We first present a brief overview of interpretability within NLP and a background to the analysis techniques that are part of the library . We then provide an overview of \diagnnose and expand briefly on its individual modules . % Next, we provide a more extensive background on the feature attributions that are part of the library . We conclude with a case study on subject-verb agreement, demonstrating several of \diagnnose's features in an experimental setup .    \diagnnose provides essential tools for conducting interpretability research, providing cutting edge analysis techniques such as diagnostic classifiers and feature attributions. The modular design of the library allows complex hypotheses to be tested rapidly, and provides a solid basis for the development of novel interpretability techniques. The library code is open source and welcomes others to contribute: we are eagerly looking forward to collaborate on adding new features to the library.  
"," In this paper we introduce \diagnnose, an open source library for analysing the activations of deep neural networks. \diagnnose contains a wide array of interpretability techniques that provide fundamental insights into the inner workings of neural networks. We demonstrate the functionality of \diagnnose with a case study on subject-verb agreement within language models. \diagnnose is available at \url{https://github.com/i-machine-think/diagnnose}.",322
" % % % %   % % % % % \GW{Propaganda can be loosely defined as  {{www.britannica.com/topic/propaganda}}). }% % % % % % % % % % Various factors of propaganda have been studied in the humanities,  including emotionality of language, biased selection of information and deviation from facts, manipulation of cognition, and more . However, there is no consensus on the decisive factors that tell whether a given article or speech is propagandistic  or not. % % % % %  % In the modern digital world, the influence of propaganda on society has drastically increased.  % Hence, there is also a major increase in computer science, computational linguistics and computational sociology research on analyzing, characterizing and, ultimately, automatically detecting propaganda .  To a first degree, one may think of propaganda as a variation of fake news, and  some works investigate propaganda as a refined type of disinformation . % % % % % % % \GW{While false claims can be an element of propaganda, we think that fake news is merely the tip of the iceberg, and that the persuasive and manipulative nature of propagandistic contents requires deeper approaches.} Classifiers for propaganda detection need to better capture how propaganda is expressed in subtle ways by language style and rhetoric or even demagogic wording. This holds for news as well as social media posts and speeches. In all these cases, correct information may be presented in incomplete form or placed in distorted contexts, along with manipulative phrases, in order to mislead the audience.  % % % % Prior work has mostly looked into news articles  and tweets, and has typically focused on  strongly polarized topics like the 2016 US election and the related Russian Internet Research Agency  affair, the UK Brexit discussion, or political extremism. % % % % % % \LQ{All these approaches consider propaganda detection as a classification task assuming sufficient amounts of labeled in-domain training data.} \LQ{For example, in the ``Hack the News'' datathon challenge, a large number of news articles  and sentences  from such articles were annotated by distant supervision and human judgment, respectively, to train a variety of machine learning methods.} % % The resulting F1 scores on the leaderboard of this benchmark are amazingly high, around 90\%. This may give the impression that propaganda detection is a solved problem. However, most of the positively labeled samples are simple cases of ``loaded language'' with strong linguistic cues independent of the topic. Moreover, the learned classifiers % % benefit from ample training data, which is all but self-guaranteed in general.   % % % % %  % In this paper, we question these prior assumptions, hypothesizing  that propagandistic sources and speakers are sophisticated and creative and will find new forms of deception evading the trained classifiers. % % % \GW{The overall approach is still text classification; the novelty of our approach lies in {      [1]{}.~} [1]{\mbox{}} } [1]{}} [1]{{\mbox{#1}}} {\metric{P}} [1]{\mbox{\Pat@}} [1]{\mbox{\metric{R}@}} {``''} {``''} {``''} {``''}      analysis of the problem of propaganda detection in cross-domain learning settings. This encompasses several novel aspects, ranging from data collection methods, feature computation, designing different classifiers, and the corresponding analysis.  \GW{We tap into a previously unexplored content source: speeches by politicans who are known for different levels of propaganda, using them as collective and relative signals.} On the methodology side, we devise a pairwise ranking method with customized loss functions to improve the classification. The experimental results demonstrate the effectiveness of this method. Furthermore, we conduct a series of experiments to explore the most salient factors for cross-domain generalizability of propaganda detection learning. The observations and analysis reveal insightful patterns and lessons for building  more general propaganda detectors.  \GW{As our datasets are still fairly small,  our findings are of preliminary nature and our methodology is subject to ongoing research. We believe that cross-domain learning is a crucial asset for the important topic of propaganda detection, and hope that our initial results are useful for further research along these lines.}     
","  As news and social media exhibit an increasing amount of manipulative polarized content, detecting such propaganda has received attention as a new task for content analysis. Prior work has focused  % on supervised learning with training data from the same domain. However, as propaganda can be subtle and keeps evolving, manual identification and proper labeling are very demanding. As a consequence, training data is a major bottleneck.   In this paper, we tackle this bottleneck and present an approach to leverage cross-domain learning, based on labeled documents and sentences from news and tweets, as well as political speeches with a clear difference in their degrees of being propagandistic. We devise informative features and build various classifiers for propaganda labeling, using cross-domain learning.  % % % Our experiments demonstrate the usefulness of this approach, and identify difficulties and limitations in various configurations of sources and targets for the transfer step. We further analyze the influence of various features, and characterize salient indicators of propaganda. %",323
" \looseness=-1 Neural machine translation  architectures~ make it difficult for users to specify preferences that could be incorporated more easily in statistical MT models  and have been shown to be useful for interactive machine translation~ and domain adaptation~. Lexical constraints or preferences have previously been incorporated by re-training NMT models with constraints as inputs~ or with constrained beam search that drastically slows down decoding~.  \looseness=-1 In this work, we introduce a translation model that can seamlessly incorporate users' lexical choice preferences without increasing the time and computational cost at decoding time, while being trained on regular MT samples. We apply this model to MT tasks with soft lexical constraints. As illustrated in Figure, when decoding with soft lexical constraints, user preferences for lexical choice in the output language are provided as an additional input sequence of target words in any order. The goal is to let users encode terminology, domain or stylistic preferences in target word usage, without strictly enforcing hard constraints that might hamper NMT's ability to generate fluent outputs.    Our model is an Edit-Based TransfOrmer with Repositioning , which builds on recent progress on non-autoregressive sequence generation~. Specifically, the Levenshtein Transformer~ showed that iteratively refining output sequences via insertions and deletions yields a fast and flexible generation process for MT and automatic post-editing tasks. \modelname replaces the deletion operation with a novel reposition operation to disentangle lexical choice from reordering decisions. As a result, \modelname exploits lexical constraints more effectively and efficiently than the Levenshtein Transformer, as a single reposition operation can subsume a sequence of deletions and insertions. To train \modelname via imitation learning, the reposition operation is defined to preserve the ability to use the Levenshtein edit distance~ as an efficient oracle. We also introduce a dual-path roll-in policy which lets the reposition and deletion models learn to refine their respective outputs more effectively.  \looseness=-1 Experiments on Romanian-English, English-German, and English-Japanese MT show that \modelname achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer  on the standard MT tasks and exploit soft lexical constraints better: it achieves significantly better translation quality and matches more constraints with faster decoding speed than the Levenshtein Transformer. It also drastically speeds up decoding compared to lexically constrained decoding algorithms~. Furthermore, results highlight the benefits of soft constraints over hard ones \---\ \modelname with soft constraints achieves translation quality on par or better than both \modelname and Levenshtein Transformer with hard constraints~.        We introduce \modelname, a non-autoregressive transformer model that iteratively edits hypotheses using a novel reposition operation.  Reposition combined with a new dual-path imitation learning strategy helps \modelname generate output sequences that flexibly incorporate user's lexical choice preferences. Extensive experiments show that \modelname exploits soft lexical constraints more effectively than the Levenshtein Transformer~ while speeding up decoding dramatically compared to constrained beam search~. Results also confirm the benefits of using soft constraints over hard ones in terms of translation quality. \modelname also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on three standard MT tasks. These promising results open several avenues for future work, including using \modelname for other generation tasks than MT and investigating  its ability to incorporate more diverse constraint types into the decoding process.   
"," We introduce an Edit-Based TransfOrmer with Repositioning , which makes sequence generation flexible by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation~, \modelname generates new sequences by iteratively editing hypotheses. It relies on a novel reposition operation designed to disentangle lexical choice from word positioning decisions, while enabling efficient oracles for imitation learning and parallel edits at decoding time. Empirically, \modelname uses soft lexical constraints more effectively than the Levenshtein Transformer~ while speeding up decoding dramatically compared to constrained beam search~. \modelname also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on standard Romanian-English, English-German, and English-Japanese machine translation tasks.",324
"  Word embeddings like Word2Vec or Glove can learn context-sensitive vector representations of words from very large corpora. These representations have proven useful for supervised tasks like language translation, entity recognition, sentiment analysis, or question answering.  The more general problem of tracking the semantic change of words through time has initially been addressed by a number of works, either by connecting several static embeddings through mapping transformations, or by initializing the training of each slice with the results from the previous one in the Word2Vec case . More recent works can deal with all the temporal slices simultaneously, as in Bamler and Mandt, Rudolph and Blei, and Yao et. al.  These works link the slices either by a diffusion process, a random walk, or a regularization term in the cost function.  The proposed approach does not assume any sequentiality in the slices .  By combining data from different sources, these embeddings can help to understand not only semantic drift, but also cross-cultural differences  or dialect variations .  In this work we consider that each corpus is divided into a set of segments called slices. All the slices are trained simultaneously, following the Word2Vec distributional hypothesis, with the addition that each word vector representation inside a slice is obtained by adding a central representation and a slice-dependent one. Thus, the different representations of one same word across different slices are tied by a common component. This constraint can be depicted as a star-like graph. Figure shows this representation for two cases:  a newspaper corpus through covering several years, and  a multi-source corpus combining two English-language newspapers.      The rest of the paper is organized as follows.  Section introduces the proposed model, giving its formal description, vocabulary selection and implementation details. Section describes the datasets used for this work: two corpora from The New York Times and The Guardian newspapers, and a curated multi-source corpus that combines both of them.   Section provides experimental work on the three datasets, and their corresponding quantitative and qualitative analysis. The related work is detailed in Section.  Finally, our conclusions and future work are discussed in Section.       We proposed a multi-source embedding model, MW2V, aimed at dealing with general language variations.   Each slice obtained from the sources can represent time, geography, or field, among other dimensions.  To demonstrate its feasibility, we applied the MW2V to three newspaper datasets: The New York Times and The Guardian to study temporal variations, and a combination of both datasets to model cultural variations.  We performed an exhaustive evaluation of the method in text analysis tasks finding good quantitative and qualitative results compared to the state of the art, even for the temporal case, when the MW2V does not specifically model the time direction.  Future work includes the analysis of other applications, oriented to the exploitation of datasets, and also the possible implications of the use of a regularization parameter dependent on the slices and words,  instead of a constant one. Moreover, some more insight is needed to answer open questions raised by for this proposal, namely, to try a broader scope of languages and to evaluate its robustness.   
"," There is an increasing interest in the NLP community in capturing variations in the usage of language, either through time , across regions  or in different social contexts . Several successful dynamical embeddings have been proposed that can track semantic change through time.  Here we show that a model with a central word representation and a slice-dependent contribution can learn word embeddings from different corpora simultaneously. This model is based on a star-like representation of the slices. We apply it to The New York Times and The Guardian newspapers, and we show that it can capture both temporal dynamics in the yearly slices of each corpus, and language variations between US and UK English in a curated multi-source corpus. We provide an extensive evaluation of this methodology.",325
" The goal of relation extraction is to extract relationships between two entities from plain text. Supervised learning methods for relation extraction have been widely used to extract relations based on training labeled data. Distant supervision or crowdsourcing have been used to collect more examples with labels and train the model for relation extraction. However, these methods are limited by the quantity  and quality  of the training data because manually labeling the data is time-consuming and labor-intensive and data labeled by distant-supervision is noisy. To overcome the problem of insufficient high-quality data, few-shot learning have been designed to require only few labeled sentences for training. A lot of research has been done on few-shot learning for computer vision~, and some work also includes few-shot learning methods for relation extraction~. Although these works only require few instances for training, they still do not work in many scenarios in which no training instances are available.  Some work on open information extraction  discovers new relationships in open-domain corpora without labeling the data. OpenIE aims to extract relation phrases directly from text. However, this technique can not effectively select meaningful relation patterns and discard irrelevant information. In addition, this technique can not discover relations if the relation's name does not appear in the given sentence. For example, OpenIE can not identify the relation of the sentence shown in Figure.  To address the aforementioned limitations, we focus on relation extraction in the context of zero-shot learning. Zero-shot learning  is similar to the way humans learn and recognize new concepts. It is a novel learning technique that does not use any exemplars of the unseen categories during training. We propose a zero-shot learning model for relation extraction , which focuses on recognizing new relations that have no corresponding labeled data available for training. ZSLRE is modified on prototypical networks utilizing side  information.  We construct side information from labels and its synonyms, hypernyms of two name entities and keywords from training sentences. The ZSL-based model can recognize new relations based on the side information available for it instead of using a collection of labeled sentences. We incorporate side information to enable our model to extract relations that never appear in the training datasets. We also build an automatic hypernym extraction framework to help us acquire hypernyms of different entities directly from web. Details of side information construction are described in Section Side Information Extraction.     Figure shows an example of how side information can be used for extract relations. Different side information are given for different relations. The query sentence in the example has a relation of classmate\_of, but the word classmate never appears in the sentence. We first get the two name entities Nell Newman and Mayday Parker of the sentence and extract the hypernyms of the name entities person and person based on our proposed hypernym extraction module in Section Hypernyms Extraction. In this example, relationship capital\_of is eliminated because the hypernyms of capital\_of should be location and location. Then we extract the keywords course and school from the query sentence and compare the distance with the keywords in side information box.  In this way, relationship children\_of is eliminated.  To make relation extraction effective in real-world scenarios, we design our models with the ability that it can extract both relations with training instances and the relations without any training instances.  We modify the vanilla prototypical networks to deal with both scenarios and compare the distance between the query sentence and the prototype. If the exponential of the minus distance is above a threshold, we consider the query sentence has a new relation. For new relations extraction, we take the side information embedding from the query sentence and compare the distance of it with the side information embedding of new relations. We conduct different experiments on both a noisy and a clean dataset and adding different percentages of new relations to evaluate the effectiveness and robustness of our proposed model. Besides, we also evaluate our proposed model in supervised learning, few-shot learning and zero-shot learning scenarios and the results show that our proposed model outperforms other existing models in all three scenarios. The contributions of this paper can be summarized as follows:         The rest of this paper is organized as follows. Section Related Work reviews work on supervised relation extraction, open relation extraction and zero-shot learning.  Section Methodology describes the proposed ZSLRE model. Section Experiments presents the experiments and compares the performance of our model with other different models on two public datasets. Section Conclusion and Future Work includes a discussion of conclusion and promising future work.     In this paper, we propose ZSLRE, a zero-shot learning relation extraction framework based on modified prototypical networks, to detect new relations that have no corresponding labeled data available for training. ZSLRE utilizes side information constructed from labels, keywords and hypernyms of entities extracted from our proposed automatic hypernym extraction framework. In our experiments, we evaluate our model in supervised learning, few-shot learning and zero-shot learning scenarios, which demonstrates that our proposed ZSLRE outperforms other state-of-art models in all scenarios.  In addition, the results demonstrate the effectiveness and robustness of our proposed model.  In future work, we plan to explore the following directions:  Due to the surprising improvement of performance made by side information embedding, we will explore whether simply learning a good representation for each type of relation can achieve a similar or better performance with other state-of-art works using meta-learning algorithms.  We will explore ways to better embed side information and we will explore using other popular sentence encoders besides CNN for relation extraction.  We will explore zero-shot learning on cross-sentence relation extraction.  
"," Most existing supervised and few-shot learning relation extraction methods have relied on labeled training data. However, in real-world scenarios, there exist many relations for which there is no available training data. We address this issue from the perspective of zero-shot learning  which is similar to the way humans learn and recognize new concepts with no prior knowledge. We propose a zero-shot learning relation extraction  framework, which focuses on recognizing novel relations that have no corresponding labeled data available for training. Our proposed ZSLRE model aims to recognize new relations based on prototypical networks that are modified to utilize side  information. The additional use of side information allows those modified prototype networks to recognize novel relations in addition to recognized previously known relations. We construct side information from labels and their synonyms, hypernyms of name entities, and keywords. We build an automatic hypernym extraction framework to help get hypernyms of various name entities directly from web. We demonstrate using extensive experiments on two public datasets  that our proposed model significantly outperforms state-of-the-art methods on supervised learning, few-shot learning and zero-shot learning tasks. Our experimental results also demonstrate the effectiveness and robustness of our proposed model in a combination scenario. Once accepted for publication, we will publish ZSLRE's source code and datasets to enable reproducibility and encourage further research.",326
"   Unlabeled data has been leveraged in many ways in natural language processing including  back-translation~, self-training~, or language model pre-training which led to improvements in many natural language tasks~. While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest~ with controlled studies showing a clear trend of diminishing returns as the amount of training data increases~.  In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text generation tasks for decades before the arrival of neural sequence to sequence models~. Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival~, it has been an important part in the winning entries of several high resource language pairs at WMT 2019~, improving over strong ensembles that used 500M back-translated sentences.  At the low resource WAT 2019 machine translation competition, noisy channel modeling was also a key factor for the winning entry~.  Noisy channel modeling turns text generation on the head: instead of modeling an output sequence given an input, Bayes' rule is applied to model the input given the output, via a backward sequence to sequence model which is combined with the prior probability of the output, typically a language model.  This enables the effective use of strong language models trained on large amounts of unlabeled data.  The role of the backward model, or the channel model, is to validate outputs preferred by the language model with respect to the input.  A straightforward way to use language models is to pair them with standard sequence to sequence models~. However, this does not address  under which modern neural sequence models still suffer~. As a consequence, models are susceptible to producing fluent outputs that are unrelated to the input~. The noisy channel approach explicitly addresses this via the channel model.   However, a major obstacle to efficient noisy channel modeling is that generating outputs is much slower than decoding from a standard sequence to sequence model. We address this issue by introducing several simple yet highly effective approximations which increase the speed of noisy channel modeling by an order of magnitude to make it practical. This includes smaller channel models as well as scoring only a subset of the channel model vocabulary.  Experiments on WMT English-Romanian translation show that noisy channel modeling can outperform recent pre-training results. Moreover, we show that noisy channel modeling benefits much more from larger beam sizes than strong pre-training methods.       We introduced a number of approximations which greatly speed up noisy channel modeling for neural sequence to sequence models.  This includes using channel models which are a fraction of the size of commonly used sequence to sequence models, pruning most of the channel model output vocabulary, and reducing the number of beam candidates scored by the channel model.  Our approximations are simple, yet, highly effective and enable comparable inference speed to ensembles of direct models while delivering higher accuracy. Our experiments show that noisy channel modeling can outperform pre-training approaches by being able to better exploit wider beams. Moreover, this is achieved while using a smaller amount of monolingual data.       \clearpage     
"," Pre-training models on vast quantities of unlabeled data has emerged as an effective approach to improving accuracy on many NLP tasks. On the other hand, traditional machine translation has a long history of leveraging unlabeled data through noisy channel modeling.  The same idea has recently been shown to achieve strong improvements for neural machine translation. Unfortunately, na\""{i}ve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives.  We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pre-training results by achieving a new state of the art on WMT Romanian-English translation.",327
"  % Sentiment analysis is a text classification technique that analyses a given text and returns the nature of the underlying opinion. Therefore, sentiment analysis is widely used for tasks such as brand monitoring, political research analysis, product analysis, workforce analysis and many more. Sentiment analysis techniques could be fundamentally sub divided into two categories as lexicon-based approach and machine learning based approach. Recently introduced deep learning based sentiment analysis techniques have outperformed the lexicon based approaches and traditional machine learning approaches.  With the development of deep learning techniques such as Convolutional Neural Networks , Recurrent Neural Networks  and language independent features, the domain of sentiment analysis has reported impressive results. Over the years, many of these variants and combinations of deep learning techniques and feature representations have been used for high resourced languages such as English. There also exist certain advancements in sentiment analysis for languages such as Chinese, Arabic, Spanish and some Indic languages.   Sinhala, which is a morphologically rich Indo-Aryan language, has not experienced these advancements due to its insular and under-resourced nature. One of the main challenges is not having large enough annotated corpora. The data set from~ is the only publicly  available annotated data set for sentiment analysis. However it includes only 5010 comments extracted from one news source, and contains only POSITIVE and NEGATIVE samples.  %Work of~ is an example of simple solutions for Sinhala sentiment analysis. Under these approaches, rule-based techniques, lexicon based techniques, supervised and semi-supervised machine learning techniques were employed with traditional language dependent features.   The 閾夸购st experiment on using deep learning techniques for Sinhala sentiment analysis was conducted by~. Under this research, basic deep learning techniques such as Long Short-Term Memory  network and CNN were used to categorize news comments as POSITIVE and NEGATIVE. %The LSTM trained with fastText embeddings outperformed traditional machine learning techniques such as Decision Tree, SVM, and Na\""ive Bayes. ~ conducted an experiment with the same data set using Sentence-State LSTM , which is a rather advanced technique where the analysis was further improved considering the n-gram features of text with word embeddings.  In this paper, we present a more comprehensive empirical study on the use of deep learning techniques for document-level sentiment analysis for Sinhala with respect to four sentiment categories as POSITIVE, NEGATIVE, NEUTRAL and CONFLICT. The experiments were conducted with the commonly used sequence models such as RNN, LSTM, Bi-LSTM, various improvements on these vanilla models such as stacking and regularization,  as well as more recent ones such as hierarchical attention hybrid neural networks and capsule networks. % for multi-class sentiment analysis using word embeddings as language independent features. These langauge independent features were able to outperform the usage of traditional language dependent features such as part of speech tagging and lexical resources.  ~Furthermore, we present a data set of 15059 comments, annotated with these four classes to be used for sentiment analysis, based on Sinhala news comments extracted from online newspapers namely GossipLanka and Lankadeepa. This is the only publicly available multi-class, multi-source dataset for Sinhala sentiment analysis.  Our code implementation, word embedding models, and annotated data set are publicly available.       %      For experiments which were conducted to identify effect of punctuation marks and dimension of word embeddings towards the sentiment analysis task, different preprocessing techniques, word embedding models, and several neural network setups were used.    For these experiments, the data-set was splitted into train and validation sets, with a ratio of .    First, different preprocessing techniques were evaluated for a multi-level sentiment analysis task in Sinhala language with baseline models. For that, an analysis was conducted with punctuation marks, without any punctuation marks and without punctuation marks except question mark.    Next, Different dimensions for both Word2vec and fastText models were experimented with baseline LSTM model and identified that fastText with 300 dimensions could beat other word embedding models as per results in Table. Therefore, The word embedding model of fastText, and dimension size of 300 were fixed for our succeeding experiments.     The experiments which were conducted with different baseline models to identify best models for further improvements suggested that BiLSTM was the optimal architecture as the primary baseline. As per results in Table with 10-fold cross validation, BiLSTM achieved the best weighted accuracy of 63.81\  and a weighted F1 score of 57.71\ , beating Vanilla RNN, LSTM, and GRU. Therefore LSTM and BiLSTM were selected for further improvements. After that, two strategies were followed to improve the selected approaches. First strategy is combining CNN with baseline models. Even though it is expected to increase the weighted accuracy and F1 score of sentiment analysis process by following the improved model architecture based on CNN, the results do not suggest a noticeable enhancement. One reason might be not having enough data to learn trainable parameters as a complex model due to the CNN integration. Results of these models are listed in Table along with results from other improvements to the baseline models.    As the final improvement to the baseline approaches stacking was implemented. As per results in Table with 10-fold cross validation, 'Stacked BiLSTM 3' model reached a weighted accuracy of 63.13\  and weighted F1 socre of 59.42\  by outperforming all other approaches. This could be justified as the ability of the stacked BiLSTM to capture the context level information in both left and right direction while considering substantial amount of neural representation for language modeling based on stacking strategy.    The capsule-B architecture went beyond all the other experimented models producing weighted accuracy of 63.23\  and weighted F1-score of 59.11\  with 10-fold cross validation. This observation could be elaborated based on the motivation behind the capsule strategy to represent the neural architecture based on vectors which further improve language representation considering the exact order or pose of the information. Furthermore, capsule-B outperformed capsule-A due to its sophisticated architecture which is designed to capture more n-grams features compared to capsule-A. The HAHNN does not illustrate greater performance as expected. This could be due to the shorter length of comments to learn deeper neural representation with the attention mechanism.  also employed under the HAHNN.  The weighted accuracy of each experiment was bounded below the value of 65\  as per the inter-annotator agreement value. This is a direct result of the high volume of noise in the dataset. As illustrated in Table, the CONFLICT and the NEUTRAL classes seem to be considerably mis-classified as NEGATIVE comments, due to the impact of a large number of NEGATIVE comments with respect to the number of CONFLICT and NEUTRAL comments in the training set. Figure shows few comments where the model was confused while classifying. The first example illustrates a comment that is negatively classified but truly a CONFLICT comment. When considering the interpretation of the comment, the sentence includes two negative sentences with a positive sentence, which indicates some bias towards the NEGATIVE sentiment. The second and third comments include NEGATIVE and NEUTRAL comments, which are classified as POSITIVE and CONFLICT, respectively.     The observation of the second example could be justified as the effect of the positive word ``{'' , respectively. Therefore the comment is classified as CONFLICT, even though the overall sentiment of the comment is neutral.             }     ~       In this research, a comprehensive analysis was conducted with the use of state-of-the-art deep learning techniques such as RNN, LSTM, Bi-LSTM, hierarchical attention hybrid neural networks, and capsule networks for multi-class sentiment analysis of Sinhala news comments. This research could be identified as the first experiment to conduct sentiment analysis at a more granular level with four sentiment categories. Moreover, this research further established the importance of language-independent word embedding features for low-resource text classification. The obtained results are not high, owing to the noisy data set used. This was made evident by the low Kappa value as well. Despite this, the comparative results we provided, give a clear indication of the best performing deep learning architectures, input features, as well as the suitable pre-processing techniques for Sinhala text classification.  Word embedding techniques such as Word2Vec and fastText empirically analysed with respect to the dimension of the embedding and various deep learning approaches which proves the importance of word embeddings as language independent features for Sinhala sentiment analysis. Moreover, the fastText embeddings generated without punctuation marks displayed the best performance indicating the original form of Sinhala language does not include any punctuation marks while suggesting that the fastText word embeddings could capture the sub-word level information considering the inflectional nature of Sinhala language.       As a secondary contribution, a multi-class annotated data set for sentiment analysis is presented, which consists of 15059 sentiment annotated Sinhala news comments extracted from two Sinhala online news papers with four sentiment categories namely POSITIVE, NEGATIVE, NEUTRAL and CONFLICT. Further, a corpus that includes unannotated comments along with the corresponding news articles, consisting of 9.48 million tokens was used to generate Word2Vec and fastText models. Embedding models, source code for the deep learning models, and all the data are publicly available.   Finally, as further improvements, more sophisticated word embedding techniques such as BERT could be used for sentiment analysis to capture more syntactic and semantic information of the language. Language dependent features such as sentiment lexicons could also be used as auxiliary information to further optimize deep learning models. It is also important to experiment the developed models with different data set types. In the absence of customer reviews written in Sinhala, a possible data source to explore would be Sinhala Twitter data. Finally, it would be interesting to expand this research into more fine-grained sentiment analysis tasks such as emotion identification, sarcasm detection, and hate-speech detection.  Furthermore, for a comprehensive sentiment analysis procedure, a tweeter based data set could be formulated considering the vast range of fields in tweeter based data.              { - pavu ahi宄佷够aka manussay鑶. \\       - priyanta宄佺挦 kiyanna deyak 鑹抧鐗祄a nam ohoma kiyanna.otana i宀ｅ硽 madi nis鑶 api d蹇檏k鑶 issaraha p鑶﹔鑶 senaga piril鑶 innav鑶.    \\      
"," Due to the high impact of the fast-evolving fields of machine learning and deep learning, Natural Language Processing  tasks have further obtained comprehensive performances for highly resourced languages such as English and Chinese. However Sinhala, which is an under-resourced language with a rich morphology, has not experienced these advancements. For sentiment analysis, there exists only two previous research with deep learning approaches, which focused only on document-level sentiment analysis for the binary case. They experimented with only three types of deep learning models. In contrast, this paper presents a much comprehensive study on the use of standard sequence models such as RNN, LSTM, Bi-LSTM, as well as more recent state-of-the-art models such as  hierarchical attention hybrid neural networks, and capsule networks. Classification is done at document-level but with more granularity by considering POSITIVE, NEGATIVE, NEUTRAL, and CONFLICT classes. A data set of 15059 Sinhala news comments, annotated with these four classes and a corpus consists of 9.48 million tokens are publicly released. This is the largest sentiment annotated data set for Sinhala so far.   % In addition to that,  was extracted from both comments and articles of online newspapers. %Furthermore, the language-independent features such as Word2Vec and fastText were experimented for novel deep learning techniques which clearly indicate the importance of word embedding techniques for NLP tasks including sentiment analysis for Sinhala as a low resource language. % Due to the high impact of the fast-evolving field of machine learning and deep learning, the Natural Language Processing  tasks have further obtained comprehensive and prominent performances over the past few decades. Different variations and combinations of deep learning techniques have been employed for NLP tasks in general. These experiments illustrated highly improved performances with respect to the traditional rule-based and statistical machine learning techniques. These advancements were mainly impacted towards the development of popular languages such as English and Chinese. However, Sinhala which is an under-resourced language with rich morphology, have not experienced these advancements due to fewer resources for NLP tasks. For sentiment analysis, there exist only two previous research with deep learning approaches, which also conducted with less granularity while giving sub optimality with respect to recent advancements in deep learning techniques. In this paper, we present the use of state-of-the-art deep learning approaches such as RNN, LSTM, Bi-LSTM, Hierarchical Attention Hybrid Neural Networks, and capsule networks for multi-class sentiment analysis for Sinhala news comments while considering more granularity. Under this research, we present the multi-class annotated data set which consists of Sinhala news comments extracted from online newspapers. Furthermore, the language-independent features such as word2Vec and fastText were experimented for novel deep learning techniques which clearly indicates the importance of word embedding techniques for NLP tasks including sentiment analysis.",328
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. % % form to use if the first word consists of a single letter: % \IAENGPARstart{A}{demo} file is .... % % form to use if you need the single drop letter followed by % normal text : % \IAENGPARstart{A}{}demo file is .... % % Some journals put the first two words in caps: % \IAENGPARstart{T}{his demo} file is .... % % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word.  \IAENGPARstart{T}{he} Neural Machine Translation   has been used to model state-of-the-art translation systems for many high-resource languages . For many language pairs though, the amount and/or quality of parallel data is not enough to train an NMT model whose accuracy can reach an acceptable standard . This category of language pairs is known as low resource. Many works have explored how to use of the easier-to-get monolingual data to improve the quality of translation models in this category of languages -- and even high resource languages -- .  The back-translation has so far been one of the most successful methods , involving the use of the translations of the target language monolingual data to increase the amount of the training data . The additional parallel data consists of authentic sentences in the target language and their translations -- synthetic sentences in the source language -- generated using a reverse  model that is trained on the available parallel data -- see the procedure in Algorithm 1. The approach has proven to be successful at improving the quality of translations in high, middle and low resourced languages . Many studies have shown that the quality of the backward system influences the performance of the ultimate NMT model . In low resource conditions, the available parallel data may not be able to train a standard backward model and the quality of the additional data generated using this model may hurt the quality of the final model. Despite this, the aim of standard back-translation has always been to improve the performance of the target NMT model by providing sufficient training data.  Some previous works have proposed various methods to improve the performance of the backward model during training. These methods include iterative back-translation , transfer learning , self-training  and the training of a bi-directional translation model for both backward and forward translations . Others have tried to mask the deficiencies of the backward model either during inference through generating multiple translations of the same target sentence using sampling to average-out the errors in individual translations  and noising the output of beam search ; or reducing the effects of the errors in the synthetic data when training the forward model through methods such as tagged back-translation  and pre-training and fine-tuning .  We present a hybrid approach that utilizes the monolingual target data to improve both the forward and backward models in back-translation. In this approach, we used the synthetic data to enhance the backward model through self-learning and the standard back-translation for improving the forward model. The approach was preliminary investigated in  and it was shown to achieve positive results. Earlier use of stand-alone self-training in machine translation proposed extra methods of either using quality estimation  or freezing of the decoder weights  when training on the synthetic side of the training data. It was suggested that the mistakes in the synthetic data will hurt the performance of the self-trained model . Instead,  showed that self-training is capable of improving the quality of the backward model even without using either of the specialized approaches. It was shown that using all of the synthetic data generated by the backward model to help in re-training the backward model improved its performance. The work, though, did not show the benefits or otherwise of using any of the specialized approach in cleaning the data, especially in low resource languages. It also did not investigate if the model can continue to learn from its output through iterating the self-learning process.    \renewcommand{\arraystretch}{1.3} {rl} {\makecell[tl]{ALGORITHM I: STANDARD BACK-TRANSLATION }} \\ {l}{\makecell[tl]{Input: Parallel data \}, y^{})\}_{u=1}^U\) and Monolingual target \\ \quad \quad \quad \quad data \})\}_{v=1}^V\)}} \\ \);} \\ 3:& Train forward model \ on bilingual data \ and improved \ models}\\ {\textbullet}    The remainder of this paper is organized as follows: In Section , we reviewed the related works. We presented the proposed methods in Section . We reported the experiments and results in Section . We discussed the results and findings of the research work in Sections  and  respectively and, finally, the paper was concluded and directions for future work were proposed in Section .      Neural machine translation systems relies on a huge amount of parallel data to train standard, state-of-the-art translation models. For low resource languages, these models perform woefully when deployed. Back-translation is an approach that was introduced in NMT by  to enable the generation of additional data for improving translation in both low and high resource languages. Subsequent studies have shown that the approach require other special methods to reach an acceptable standard for translation quality especially in low resource set-ups . In these set-ups, the backward model is trained on a scarce data and, therefore, the quality of generated additional data may not be enough to substantially improve the target translation model. The target of back-translation is always to improve the performance of the forward model on the available monolingual data and not the intermediary backward model. But the standard of the forward model relies on the authentic data and the ability of the backward model to generate a good enough additional training data.  } & \multicolumn{4}{c|}{Sample size} \\  	& & 50 & 100 & 500 & 1000 \\    This work, therefore, presents a new variant of the back-translation that incorporates the self-learning approach, through forward translation, to use the same target-side monolingual data to improve not only the forward model, but the backward model also. The back-translation is used ultimately to improve the forward model but only after using self-training to enhance the standard of the backward model.  } & \multicolumn{4}{c|}{Sample size} \\  	& & 50 & 100 & 500 & 1000 \\    In implementing the self-learning approach, we investigated various methods namely: self-training and iterative self-training each with and without quality estimation. We implemented all the methods using the pre-training and fine-tuning strategies of  to enable each model differentiate between synthetic and authentic data during training, as this has been shown to improve the performance of models trained in such settings . All performance scores obtained through experiments have been shown to be statistically significant using the paired bootstrap resampling of  as implemented in  -- see Tables  and .  The work was evaluated on the low resource IWSLT'14 English-German neural machine translation. We observed that even though the proposed self-trained backward method  outperformed the standard back-translation's backward model without using any of quality estimation or freezing of parameters in the decoder as proposed in  and  respectively, selecting and using the best synthetic data for self-training further improves its performance. This shows that although an improved performance was achieved, the full potential of the proposed method may not be realized when using vanilla self-training because the noise in the synthetic data will degrade the decoder's performance.  We extended the positive results that were obtained using self-learning by determining the benefit or otherwise of selecting only a fraction of the synthetic data for self-training using a quality estimation system. Experimental results indicated that not only was the result not affected by the reduction in training data, but that the performance was improved significantly, achieving +2.35 BLEU. We showed that not all of the synthetic data is required -- quantity -- but that the more beneficial additional data -- quality -- is just enough to train a superior backward model. Also, when the backward model -- or any other model -- is able to differentiate between the synthetic and authentic parallel data during training, then the effects of the lack of quality in the synthetic data becomes less problematic but the more the qualitative the synthetic data is, the better the model trained.  We also implemented an iterative approach that continued to enhance the quality of the backward model on the synthetic data. Each improved backward model was used to generate a synthetic training data for training the next improved model. The approach achieved a significant +2.96 BLEU improvement over the one-time usage of the self-training on the IWSLT'14 En-De test set. We compared the iterative self-learning approach to other iterative approaches in  and  and our method was shown to be superior while also requiring less number of models -- \-less number of models in any \ iterations -- to be trained than that needed in the approach of . Also, unlike in , we showed that without data selection through quality estimation, we achieved an improved model over the baseline.  While  suggests that models trained on synthetic data only can reach a performance similar that of models trained on the authentic data only, we showed that a model trained on a sufficient number of qualitative synthetic sentences can achieve a better performance than that of a model trained on low resource authentic parallel data.  claimed that the ratio of synthetic to parallel data affects the translation model more than the quality of the backward model. This, they claimed, is because the model then tends to learn more from the synthetic data which often contain more noise. Instead, we claim that the quality of the backward model affects the performance of the approach more than the ratio because when the model is able to generate synthetic data that is close to or the same quality as human translation, the ratio of synthetic data to authentic data does not matter because the two data become more inseparable.  The iterative self-learning enhanced back-translation approach was proposed to avoid being so much reliant on the availability or reliability, thereof, of the quality estimation systems for the successful implementation of the previously proposed approaches. We determined that without such systems reliably available, retraining the backward model over some iterations is capable of achieving the same or even superior performance to the other methods.  The forward models' performances were shown to reflect the improvements in the backward models. We achieved an improved +0.48 BLEU over the performance of self-trained enhanced back-translation method. The proposed approaches achieved better performances than all the previous methods but a similar quality was observed between them. This is as expected because the performances of the backward models were not far off from each other.  In Table , we showed a sample translation from English to German. Our proposed models were able to produce exact translations to most  of the referenced translation: ''... wir 3 milliarden stunden pro woche mit online-spielen'' and the other part where the translation generated was different, the meaning was the same: ''derzeit'' 'vs' ''jetzt''. The self-trained models were able to generate exact translation to most of the referenced text but could only specify the adverb ''now'' instead of the referenced ''right now''. For the forward model, the effects of the improved backward models were observed in their performances. In Table , we also translated a given German source text to English. The performances of the last two models , and especially the last model, seemed to be more superior than the rest. The pre-training and fine-tuning approach has shown to be the better approach when applying the method we proposed in this work. As proposed in , we found that pre-training first on the synthetic data and thereafter fine-tuning the model on the authentic data is the best strategy.           This is the first work that proposed the iterative utilization of the monolingual target data using a joint backward and forward translation to improve the neural machine translation on low resource languages, to the best of our knowledge. It is also the first work that combines quality estimation and self-learning to improve low resource back-translation in NMT. This category of languages have been shown to straggle their high resource counterparts even when the same methods are applied to improve their quality. The back-translation approach that has shown tremendous potential for improving translation performance in high resource languages, has shown improved but less than desirable performance in low resource languages. This has been shown to be as a result of the lack in quality of the backward model.  In this work, we applied a joint backward and forward translation to utilize the monolingual data in the target language to train better neural machine translation systems especially in low resource languages. We proposed a variety of techniques for implementing the approach based on the availability or otherwise of another supporting system -- the quality estimation system. The self-learning was used to improve the performance of the backward model. Experimental results obtained on low resource English-German have shown that the approach is superior to the widely successful back-translation approach. The approach is not only straightforward but also easy to implement on any low resource language translation to train a better model capable of attaining a more acceptable standard of translation.  We showed that the approach is capable of enhancing the standard of the model even without using specialized the quality estimation data selection method. We also showed that when the backward model is able to differentiate between the synthetic and authentic data, its quality gets better. As shown in the training of the forward models, this is also true for all models that are trained on the synthetic and authentic data. The self-training approach was shown to perform better when quality estimation is used to extract the best translations and used to retrain the generating backward model. We also extended the self-training approach to determine whether the approach can continue to benefit the backward model over several iterations. We presented a simplified iterative approach that reduces both the number of models required and the time taken to achieve the number of iterations in previous works. We showed that it is possible to rely only on large amounts of synthetic data that gets improved iteratively especially in low resource conditions than strictly relying on the quality of fewer training data. Our work relies only on the target monolingual data as required in the traditional back-translation approach unlike the target and source monolingual data in the iterative back-translation approach. We showed that the approach works well on a low resource neural machine translation.  For future work, we aim to determine the appropriate sentences to be considered fit for self-learning for each iteration especially using data selection as an alternative to quality estimation. We also intend to apply the approach on high resource languages.     Can use something like this to put references on a page   by themselves when using endfloat and the captionsoff option.    The ""triggered"" command can be changed if desired:  \IAENGtriggercmd{}    references section    can use a bibliography generated by BibTeX as a .bbl file   BibTeX documentation can be easily obtained at:   http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/   The IAENGtran BibTeX style support page is at:   http://www.michaelshell.org/tex/IAENGtran/bibtex/     argument is your BibTeX string definitions and bibliography database       <OR> manually copy in the resultant .bbl file   set second argument of                    biography section     If you have an EPS/PDF photo  extra braces are   needed around the contents of the optional argument to biography to prevent   the LaTeX parser from getting confused when it sees the complicated         that's all folks 
"," %\boldmath Many language pairs are low resource, meaning the amount and/or quality of available parallel data is not sufficient to train a neural machine translation  model which can reach an acceptable standard of accuracy. Many works have explored using the readily available monolingual data in either or both of the languages to improve the standard of translation models in low, and even high, resource languages. One of the most successful of such works is the back-translation that utilizes the translations of the target language monolingual data to increase the amount of the training data. The quality of the backward model which is trained on the available parallel data has been shown to determine the performance of the back-translation approach. Despite this, only the forward model is improved on the monolingual target data in standard back-translation. A previous study proposed an iterative back-translation approach for improving both models over several iterations. But unlike in the traditional back-translation, it relied on both the target and source monolingual data. This work, therefore, proposes a novel approach that enables both the backward and forward models to benefit from the monolingual target data through a hybrid of self-learning and back-translation respectively. Experimental results have shown the superiority of the proposed approach over the traditional back-translation method on English-German low resource neural machine translation. We also proposed an iterative self-learning approach that outperforms the iterative back-translation while also relying only on the monolingual target data and require the training of less models.",329
"   End-to-end techniques for automatic speech recognition , most notably sequence-to-sequence models with attention  and Recurrent Neural Network Transducer  , are becoming increasingly popular. Compared to the traditional hybrid system based on Hidden Markov Model and Deep Neural Network  with individually-trained components, all parts of an end-to-end model are optimized jointly, which often leads to better performance on recognition tasks with sufficient training data and low training-testing mismatch. End-to-end systems are simpler to train; they typically do not require pronunciation lexicons, decision trees, initial bootstrapping, nor forced alignment. End-to-end models are also more suitable for on-device use cases due to the lack of external language models  or decoding graphs, whose sizes can be prohibitively large in hybrid setups because of large vocabulary support, complex LMs, and context-dependent decision trees.  End-to-end systems do have limitations, however. Their end-to-end nature leads to a lack of composability, such as that between acoustic, language, and pronunciation models in hybrid setups. This lack of composability in turn leads to challenges in personalization, which traditionally involves on-the-fly modification of external LMs  to add, boost, and penalize certain words or phrases. Previous work in end-to-end ASR addressed this issue by incorporating external LMs during beam search , with special modifications to handle the model's spiky output . A fundamental limitation of shallow fusion is that it relies on late combination, hence the model needs to have the potential to produce the correct output in the first place without access to biasing information. Another class of method  adds an attention-based  or simple  biasing module over contextual phrases to provide additional signal to the decoder component of end-to-end models. While promising, these methods were shown to have problems scaling to large and highly confusable biasing lists.  A closely related challenge of ASR personalization is entity recognition, since in many cases biasing items are entity names. Rare name recognition presents significant challenges to end-to-end models because of two main reasons. First, the output units of end-to-end models are typically graphemes or WordPieces , both of which do not work well when the spelling of a word does not correspond to how it is pronounced . Second, rare names often decompose into target sequences that are not seen enough in training, making them difficult to recognize correctly. By contrast, both problems are alleviated in hybrid systems due to the use of phonetic lexicons and/or clustered context-dependent acoustic targets. Popular solutions to this problem include upsampling entity-heavy data or generating synthetic training data with names using text-to-speech  . While this method alleviates the data sparsity issue, it does not address the underlying problems of under-trained targets and unconventional spelling of rare names.   In this work, we propose several novel techniques to address both challenges and further improve RNN-T personalization. To alleviate the problem of under-trained targets and recognition of unconventional names, we adopt on-the-fly sub-word regularization  to increase WordPiece coverage during training, perform pre-training  and multi-task learning   to strengthen the encoder, and leverage grapheme-to-grapheme   to generate alternative graphemic pronunciations for names. To address the limitation of shallow fusion relying on late combination, we introduce deep personalized LM  fusion to influence the model's predictions earlier. We show that the combination of these techniques results in 15.4\%--34.5\% relative Word Error Rate  improvement on top of a strong RNN-T baseline which leverages shallow fusion and TTS augmentation. Our final model is also competitive with a hybrid system that has significantly larger disk and memory footprint.       Deep context to have weights on bias phrases  In this paper, we showed that RNN-T personalization can be improved significantly by inducing better coverage of rare WordPieces during training, introducing extra information into the encoder, leveraging G2G to produce additional pronunciation variants in both training and decoding, and biasing earlier with deep PLM fusion. Together, these techniques help push the boundary of RNN-T personalization and close the gap with traditional hybrid systems on use cases that require contextual biasing and accurate name recognition. For future work, we plan to incorporate proper WFST and NNLM into deep PLM fusion, apply these techniques to other end-to-end models, and tackle open-domain personalization where strong context prefixes are not always available.  
"," End-to-end models in general, and Recurrent Neural Network Transducer  in particular, have gained significant traction in the automatic speech recognition community in the last few years due to their simplicity, compactness, and excellent performance on generic transcription tasks. However, these models are more challenging to personalize compared to traditional hybrid systems due to the lack of external language models and difficulties in recognizing rare long-tail words, specifically entity names. In this work, we present novel techniques to improve RNN-T's ability to model rare WordPieces, infuse extra information into the encoder, enable the use of alternative graphemic pronunciations, and perform deep fusion with personalized language models for more robust biasing. We show that these combined techniques result in 15.4\%--34.5\% relative Word Error Rate improvement compared to a strong RNN-T baseline which uses shallow fusion and text-to-speech augmentation. Our work helps push the boundary of RNN-T personalization and close the gap with hybrid systems on use cases where biasing and entity recognition are crucial.",330
"   The dominant paradigm in supervised NLP today is learning from examples, where machine learning algorithms are trained using a large set of task-specific input-output pairs. In contrast, humans learn to perform the same task by reading a description, after which they are able to perform the task in a zero-shot manner---indeed, this is how crowd-sourced NLP datasets are constructed. In this paper, we argue that learning from task descriptions in this way is a necessary attribute of a general purpose NLP system, and we propose it as a new paradigm to train and test NLP systems.    Recent work in NLP has shown significant progress in learning tasks from examples. Large pretrained language models have dramatically improved performance on standard benchmarks and have shown promising results in zero shot prediction by leveraging their language understanding capabilities.   Despite this progress, there are many serious issues that come with learning from examples.  There is an almost infinite number of tasks that a person might wish to solve with a general-purpose NLP system.  Learning to solve these tasks by reading a description instead of observing a collection of examples would solve the problem of having to create training sets for each language task.  Such a system would also be more accessible to practitioners and domain experts in other fields, who could describe their tasks and solve them, opening up new avenues of research where it is expensive or infeasible to gather training data.    Additionally, we find that current supervised learning techniques partly achieve their success due to memorizing uninteresting aspects of the training distribution.  Teaching a system to learn a task from the description alone would alleviate these biases, as new training data would not be needed to learn a novel task.  In this paper, we synthesize prior approaches to zero-shot learning in NLP and provide a formal framework for thinking about the zero-shot prediction problem.  We show that previous zero-shot approaches are limited in both scope of application and rigour of evaluation.  For example, while prior work has used zero-shot prediction for text classification, entity typing, and relation extraction, we push this to the more complex task of slot filling.  We instantiate our formalism in an English language dataset, \dataset , that is formatted similarly to reading comprehension datasets, in that we formulate task descriptions as questions and pair them with paragraphs of text.  We choose this format as it provides a natural way to crowdsource data.  This zero-shot dataset differs from typical reading comprehension datasets, however, in that each task description is paired with twenty different passages, and we evaluate a model's ability to solve the task, not just give the correct answer for a single  pair.  That is, given a question, a model produces some decision function , and it is this function which we comprehensively evaluate on many different inputs.  We also carefully select axes on which to evaluate the generalization of a model to different kinds of task descriptions, changing task descriptions in specific ways to systematically push the field towards more interesting and complex task descriptions.  We evaluate models based on recent state-of-the-art sequence to sequence architectures, which seem most suited to the task of zero shot prediction in this setting.  We find that our best model based on T5  achieves a score of only \finalscore\% on this data, leaving a significant gap to our human performance estimate of \humanestimate\%.  Zero shot learning from complex task descriptions remains a significant challenge for current NLP systems.      We introduced a framework for creating general purpose NLP systems that can solve tasks from natural language descriptions, synthesizing and extending previous work in zero-shot learning. To make progress toward this goal, we create a dataset, \dataset{}, that rigorously evaluates how well a model truly understands each task. The dataset is designed to test models' ability to systematically generalize across four different areas. State-of-the-art performance on \dataset is \finalscore\ , leaving much room for future improvement.  While we have been focused on zero shot learning from task descriptions, our framework also permits few-shot scenarios where a task description is given along with a handful of examples, making meta-learning approaches applicable.  This is an interesting avenue for future work, for which \dataset{} should also be useful. To facilitate future work, we make our models, code, and data available at .  
"," Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this framework with a new English language dataset, \dataset, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of \finalscore\% on \dataset, leaving a significant challenge for NLP researchers.\footnote{Data, evaluation code, baseline models, and leaderboard at \url{https://allenai.org/data/zest}}",331
"  Because of the fact that obtaining   supervised training labels is costly and time-intensive,   and that   unlabeled data is relatively easy to obtain,   semi-supervised learning  , which  utilizes  in-domain  unlabeled data  to improve models trained on the labeled dataset , is of growing interest.  Under the context of large-scale of language model pretraining ,  where a language model is pretrained on an extremely large, open-domain dataset  affect performances regarding  of different sizes, and  of different sizes, etc.    In this paper, we conduct comprehensive studies on the behavior of semi-supervised learning in NLP  with the presence of large-scale language model pretraining.   We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks. Our work sheds important lights on the behavior of semi-supervised learning models:  we find that   with the presence of  in-domain pretraining LM on , open-domain LM pretraining   is unnecessary, and we are able to achieve better performance with pretraining on  the in-domain dataset ;    both the in-domain pretraining strategy and the pseudo-label based strategy  lead to significant performance boosts, with the former performing better with larger , the latter performing better with smaller , and the  combination   of both performing the best;  for pseudo-label based strategies,  self-training  yields better performances when  is small, while joint training on the combination of   and  yields better performances when  is large.   Using semi-supervised learning models, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6 with the full dataset.  More importantly, our work marks an initial step toward understanding the behavior of semi-supervised learning models in the context of large-scale pretraining.    The rest of this paper is organized as follows: related work is detailed in Section 2.  Different strategies for training semi-supervised models are shown in Section 3.  Experimental results and findings are shown in Section 4, followed by a brief conclusion in Section 5.     In this paper, we conduct comprehensive  analysis on semi-supervised learning in NLP under the context of  large-scale language model pretraining.  We find that even with the presence of large-scale LM pretraining, both the in-domain pretraining strategy and the pseudo-label based strategy introduce  additional  significant performance boost,  with the former performing better with larger ,  the latter performing better with smaller , and the combination leading to the best performance.  Using semi-supervised learning models, we are able to achieve a performance of around  accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6 with the full dataset.  Our work  sheds light on   the behavior of semi-supervised learning models in the context of large-scale pretraining.      
"," The goal of semi-supervised learning is to utilize the unlabeled, in-domain dataset $U$ to improve models trained on the labeled dataset $D$.     Under the context of   large-scale language-model  pretraining,   how we  can make the best use of   $U$   is poorly understood:   Is semi-supervised learning still beneficial   with the presence of  large-scale pretraining?  Should $U$ be used for in-domain LM pretraining or pseudo-label generation? How should the pseudo-label based semi-supervised model    be actually implemented? How different semi-supervised strategies  affect performances regarding $D$ of different sizes, $U$ of different sizes, etc.   In this paper, we conduct comprehensive studies  on  semi-supervised learning in the  task of text classification   under the context of  large-scale LM pretraining. Our studies shed important  lights on the  behavior of semi-supervised learning methods.   We find that:    with the presence of  in-domain LM pretraining  on $U$, open-domain LM pretraining   is unnecessary, and we are able to achieve better performance with pretraining on  the in-domain dataset $U$;  both the in-domain pretraining strategy and the pseudo-label based strategy introduce  significant performance boosts,  with the former performing better with larger $U$,  the latter performing better with smaller $U$, and the combination leading to the largest performance gain;   vanilla self-training  yields better performances when $D$ is small, while joint training on the combination of  $D'$ and $D$ yields better performances when $D$ is large.   %We use the task of text classification as an example,  the method of which can be easily adapted to different NLP tasks.  Using semi-supervised learning strategies, we are able to achieve a performance of around $93.8\%$ accuracy with only 50 training data points on the IMDB dataset, and   a competitive performance of 96.6$\%$ with the full  IMDB dataset.  Our work marks an initial step toward understanding the behavior of semi-supervised learning models under the context of large-scale pretraining.\footnote{Code, models and datasets  can be found at https://github.com/ShannonAI/Neural-Semi-Supervised-Learning-for-Text-Classification}",332
"  \todo{Completely rewrite - emphasize that many methods have been proposed for learning embeddings  learn representations of the entities in a knowledge base  typically based on the text of each entity's Wikipedia article or the surrounding local context for mentions of each entity . %   context surrounding mentions of each entity Recent advances in neural EL have involved methods for pretraining entity embeddings using the link graph of Wikipedia to learn related entities and words . Similar to word embeddings, past work has shown that these embeddings reside in a high-dimensional pseudo-semantic space, with entities that are close in the space being semantically similar . % \glarionov{""with entities close in the space being...} However, little work has been done to understand what information different entity embeddings capture about the underlying entities and how that information affects downstream performance.  Our goal in this work is to identify semantic information in entity representations and determine how that information is linked to performance on downstream EL tasks. For this, we develop a series of probing tasks, which have previously been used to examine lexical and syntactic properties of neural model layers such as sentence encoders and decoders for neural machine translation systems .  % \glarionov{I would group these two citations at the end for readability} % . We extract structured data about entities using DBpedia and context words from Wikipedia anchor links to create probing tasks designed to evaluate the knowledge-based and distributional semantic contents of different entity embedding models.  We compare five entity embedding methods, first by them on two downstream EL tasks. We then probe the learned embeddings to evaluate what semantic information is important for the downstream tasks and how it is represented by the different models. %  We find that pretrained entity embedding methods are generally more effective at representing distributional and knowledge-based semantic information than models that generate embeddings as a byproduct of training on an EL task. These improved representations lead to better performance on the EL tasks, with the best model showing high performance on both distributional and knowledge-based semantic tasks. We further find that entity embeddings trained to predict related words and entities in a skipgram-like model are able to learn fine-grained entity type information and specific relationship types between entities without explicitly providing this information.  Our primary contributions with this work are to:        % 1) describe methods for evaluating the semantic information learned by these methods and 2) to empirically demonstrate the importance of this information in creating models of entities for use in downstream tasks. %  Our hope is that this information can provide guidance in developing architectures that better combine explicit structured information with text to improve methods for representing entities that can be used in a variety of downstream tasks, similar to existing word embeddings. Our methods can additionally be used to potentially detect deficiencies in new representation methods and biases of learned attributes through other probing tasks. % and biases of current methods by probing .     In this work, we propose a new set of probing tasks for evaluating entity embeddings which can be applied to any method that creates one embedding per entity. Using these tasks, we find that entity type information is one of the strongest signals present in all but one of the embedding models, followed by coarse information about how likely an entity is to be mentioned. We show that the embeddings are particularly able to use entity type information to bootstrap their way to improved performance on entity relationship and factual information prediction tasks and propose methods to counteract this to more accurately estimate how well they encode relationships and facts.  Overall, we find that while BERT-based entity embeddings perform well on many of these tasks, their high performance can often be attributed to strong entity type information encoding. More specialized models such as Wikipedia2Vec are better able to detect and identify relationships, while the embeddings of  better capture the lexical and distributional semantics of entities. Additionally, we provide a direct comparison of the embeddings on two downstream EL tasks, where the models that performed well on the probing tasks such as Ganea, Wiki2V, and BERT performed best on the downstream tasks. We find that the best performing embedding model depends greatly on the surrounding architecture and encourage future practitioners to directly compare newly proposed methods with prior models in a consistent architecture, rather than only compare results.  Our work provides insight into the information encoded by static entity embeddings, but entities can change over time, sometimes quite significantly. One future line of work we would like to pursue using our tests is to investigate how changes in entities over time can be reflected in the embeddings, and how those changes could be modeled as transformations in the embedding space. Context-based embeddings in particular could then be dynamically updated with new information, instead of being retrained from scratch.       
","  \todo{Complete rewrite} Pretrained entity embedding methods have shown strong results in entity linking  systems compared to methods that generate entity representations from text descriptions. Prior work has shown that these embeddings inhabit a pseudo-semantic space, but the semantic information they contain has not been thoroughly explored nor have  they been compared with other representations for differences in information.  We introduce methods for probing learned entity representations for information about their entity types, relationships, and context words using Wikipedia anchors and DBPedia structured data and use them to compare five entity embedding models. We show that improved representation of all types of semantic information is linked to improved performance on two downstream EL tasks. Our results provide potential directions for further research to better incorporate explicit semantic information into neural entity linking models.",333
" Transfer Learning  is a rapidly growing field of machine learning that aims to improve the learning of a data-deficient task by transferring knowledge from related data-sufficient tasks.  %Witness the success of deep learning, deep transfer learning has been widely studied and demonstrated remarkable performance over various applications, such as xxx.  Witnessing the great representation learning abilities of deep neural networks, neural architectures based TL methods, i.e., deep transfer learning, have gained increasing popularity and are shown to be effective for a wide variety of  applications. %%%%%  A few TL toolkits have also been developed to make it easy to apply TL algorithms. Notable projects include:        is a python based AI toolkit for training AI models and customizing them with users' own datasets. However, it mainly focuses on the computer vision field.      is an MXNet library which largely automates deep TL. It contains the ``ModelHandler'' component to extract features from pre-trained models and the ``Repurposer'' component to re-purpose models for target tasks.      is an integrated interface for 17 TL models written by python. It includes five types of models, namely ``feature-based'', ``concept-based'', ``parameter-based'', ``instance-based'' and ``deep-learning-based''.      specifically addresses model-finetuning, especially for BERT-like models. It is backended by PyTorch and Tensorflow and integrates 30+ pre-trained language models.     However, when it comes to industrial-scale real-world applications, the above mentioned toolkits might be less ideal. The reasons are threefold. i) Deep learning models are getting larger and larger, which makes it difficult to deploy those models in real-time applications. For example, pre-trained contextual representation encoders, such as BERT , RoBERTa  and GPT , have been widely adopted in a variety of Natural Language Processing  tasks . Despite their effectiveness, these models are built upon large-scale datasets and usually have parameters in the billion scale. To elaborate, the BERT-base and GPT-3 models are with M and B parameters respectively. This makes it difficult to train and deploy such models in real-time applications that have limited resources and require high inference speed. ii) There are a variety of TL algorithms proposed in literature, yet no comprehensive TL toolkit is available for users to examine different types of state-of-the-art TL algorithms. iii) A huge gap still exists between developing a fancy algorithm for a specific task and deploying the algorithm for online production. For many online applications, it is still a non-trivial task to provide a reliable service with high QPS~.   % The rest of this paper is organized as follows: Section  introduces the basic setting of deep transfer learning and background of inference attacks against deep learning models. Section  provides the general categorization of deep transfer learning and detailed privacy analysis. Section  shows the information leakage in deep transfer learning empirically. % Section briefly summarizes some % related works and Section draws the conclusion.  %% %% This is file `sample-sigconf.tex', %% generated with the docstrip utility. %% %% The original source files were: %% %% samples.dtx   %%  %% IMPORTANT NOTICE: %%  %% For the copyright see the source file. %%  %% Any modified versions of this file must be renamed %% with new filenames distinct from sample-sigconf.tex. %%  %% For distribution of the original source see the terms %% for copying and modification in the file samples.dtx. %%  %% This generated file may be distributed as long as the %% original source files, as listed above, are part of the %% same distribution.  %% %% The first command in your LaTeX source must be the \documentclass command. \documentclass[sigconf]{acmart} %%%% As of March 2017, [siggraph] is no longer used. Please use sigconf  for SIGGRAPH conferences.  %%%% Proceedings format for SIGPLAN conferences  % \documentclass[sigplan, anonymous, review]{acmart}  %%%% Proceedings format for SIGCHI conferences % \documentclass[sigchi, review]{acmart}  %%%% To use the SIGCHI extended abstract template, please visit % https://www.overleaf.com/read/zzzfqvkmrfzn  \usepackage{xcolor} \usepackage{soul} \usepackage{url} \usepackage{caption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{amsthm} \usepackage{booktabs} %\usepackage{algorithm} %\usepackage{algorithmic} \usepackage{multirow} \usepackage{array}  \usepackage{listings} \usepackage{color} \definecolor{mygreen}{rgb}{0,0.6,0} \definecolor{mygray}{rgb}{0.5,0.5,0.5} \definecolor{mymauve}{rgb}{0.58,0,0.82}  % \lstset{ % %   frame=tb,     %   backgroundcolor=,   % choose the background color %   basicstyle= %\orcid{1234-5678-9012} %\footnote{Corresponding author.} \affiliation{% 	 } %@alibaba-inc.com} %@alibaba-inc.com} %  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research. % \author{Ben Trovato} % % \authornote{Both authors contributed equally to this research.} %  % \affiliation{% %    %    %    % }  % \author{Lars Th{\o}rv{\""a}ld} % \affiliation{% %   rv{\""a}ld Group} %   rv{\""a}ld Circle} %    %    % } %   % \author{Valerie B\'eranger} % \affiliation{% %    %    %    % }  %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose.  % \renewcommand{  \renewcommand{ \renewcommand{\authors}{Qiu et al.}  %% %% The abstract is a short summary of the work to be presented in the %% article.  The literature has witnessed the success of applying deep Transfer Learning  algorithms to many NLP applications, yet it is not easy to build an easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the EasyTransfer platform is designed to make it easy to develop deep TL algorithms for NLP applications. It is built with rich API abstractions, a scalable architecture and comprehensive deep TL algorithms, to make the development of NLP applications easier. To be specific, the build-in data and model parallelism strategy shows to be 4x faster than the default distribution strategy of Tensorflow. EasyTransfer supports the mainstream pre-trained ModelZoo, including Pre-trained Language Models  and multi-modality models. It also integrates various SOTA models for mainstream NLP applications in AppZoo, and supports mainstream TL algorithms as well. The toolkit is convenient for users to quickly start model training, evaluation, offline prediction, and online deployment.  This system is currently deployed at Alibaba to support a variety of business scenarios, including item recommendation, personalized search, and conversational question answering. Extensive experiments on real-world datasets show that EasyTransfer is suitable for online production with cutting-edge performance. The source code of EasyTransfer is released at Github~\footnote{https://github.com/alibaba/EasyTransfer}.   %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %% %  % <ccs2012> % <concept> % <concept_id>10002978.10003022</concept_id> % <concept_desc>Security and privacy~Software and application security</concept_desc> % <concept_significance>500</concept_significance> % </concept> % <concept> % <concept_id>10010147.10010257.10010258.10010262.10010277</concept_id> % <concept_desc>Computing methodologies~Transfer learning</concept_desc> % <concept_significance>500</concept_significance> % </concept> % </ccs2012> %   %  %   %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.    %% This command processes the author and affiliation and title %% information and builds the first part of the formatted document.   % %       We introduced EasyTransfer, a toolkit that is designed to make it easy to develop deep transfer learning algorithms for NLP applications. It is built with a scalable architecture and comprehensive deep TL algorithms.  EasyTransfer supports the mainstream pre-trained ModelZoo, NLP applications in AppZoo, and mainstream TL algorithms as well. The toolkit is convenient for users to quickly start model training, evaluation, and online deployment.   In this study, we provide a general categorization of different deep transfer learning paradigms depending on how the domains interact with each other. Based on that, we then analyze their respective privacy leakage profiles, design different attack models for each paradigm and provide potential solutions to prevent these threats. Extensive experiments have been conducted to examine the potential privacy leakage and effectiveness of defense solutions.   under different deep transfer learning settings.   \clearpage       The next two lines define the bibliography style to be used, and    the bibliography file.     \endinput       End of file `sample-sigconf.tex'. 
"," The literature has witnessed the success of applying deep Transfer Learning  algorithms to many NLP applications, yet it is not easy to build an easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the EasyTransfer platform is designed to make it easy to develop deep TL algorithms for NLP applications. It is built with rich API abstractions, a scalable architecture and comprehensive deep TL algorithms, to make the development of NLP applications easier. To be specific, the build-in data and model parallelism strategy shows to be 4x faster than the default distribution strategy of Tensorflow. EasyTransfer supports the mainstream pre-trained ModelZoo, including Pre-trained Language Models  and multi-modality models. It also integrates various SOTA models for mainstream NLP applications in AppZoo, and supports mainstream TL algorithms as well. The toolkit is convenient for users to quickly start model training, evaluation, offline prediction, and online deployment.  This system is currently deployed at Alibaba to support a variety of business scenarios, including item recommendation, personalized search, and conversational question answering. Extensive experiments on real-world datasets show that EasyTransfer is suitable for online production with cutting-edge performance. The source code of EasyTransfer is released at Github.",334
" Sentiment polarity detection regarded as one of the significant research problems for opinion extraction in natural language processing . In recent years, the plenteous growth of the internet and the random access of e-devices facilitate the generation of voluminous reviews or opinions in textual form on social media or online platforms. Most of these reviews express the consumers feedback toward the products and services that they received. Several business companies, as well as online marketers, take advantage of these feedbacks to provide praiseworthy services to the consumers. In addition to that customer makes a perfect decision based on the previous reviews before receiving products or services.  Sentiment detection is a computational technique that attempts to uncover the viewpoint of a user towards a specific entity. It aims to identify the contextual polarity of the text contents  as the positive, neutral and negative . Sentiment analysis or detection has shown a remarkable impact in the business community, whereby taking into account the user opinions the communities can ensure the sustainability of their product or services. The restaurant is one such business, where customers opinions can be utilized to improve their quality of foods, environments, and services. Pompous lifestyle and assorted food habits led to a significant increase in the number of people in restaurants. To collect the excellence of services, customers instinct to look through the restaurant reviews before visit it. Therefore, reviewing a restaurant via the internet has become an ecumenical trend. Besides, an abundant amount of positive reviews can make a restaurant as a symbol of faith towards the customers. Also, it can assist a restaurant to reach the pinnacle of success. In contrast, without a sufficient amount of positive reviews, it becomes difficult to gain the attention of new customers by a restaurant. Sometimes, a restaurant with negative reviews loses the trustworthiness of the customers, which turned into reducing the profit.  Straightforwardly, users opinions on specific criteria such as food quality, ambience and service standards of a restaurant can have enough influence on the customers liking. However, it would not be wrong to say that customers inclination or reluctance towards a restaurant depends on the amount of positive and negative reviews. Therefore, the restaurants should appreciate the consents as well as the opinions of the customers. Nevertheless, scrutinizing every reviews one by one is a very time consuming as well as cumbersome task. Further, to govern such surveys, it requires plenty amount of investment in both money and human resources. Considering the fact of the explosive growth of the visitors as well as user preferences, it requires an automatic system that can comprehend the contextual polarity of reviewer opinions posted in different online platforms including Facebook, Twitter, company website, and blogs. Nevertheless, sentiment classification is a challenging research issue in a resource-poor language like Bengali. The inadequacy of benchmark dataset and the limited amount of e-textual contents or reviews in the Bengali language resulted in the sentiment classification task complicated. Deep learning algorithms are very effective to tackle such complications and classify the sentiments correctly . One main advantage of these algorithms are their ability to capture the semantic information in long texts. This paper proposed a deep learning-based sentiment classification technique to classify sentiment form reviews. By taking into consideration the current constraints of sentiment analysis in low resource languages, this paper contributions illustrate in the following:       In this paper, we presented a deep learning-based scheme to analyze the sentiment on Bengali restaurant reviews. Word2vec embedding technique is used to consider the semantic meaning of the Bengali reviews. BiLSTM network tuned to find out the optimal hyperparameter combination. A corpus of 8435 Bengali restaurant reviews is developed to evaluate the performance of the proposed system. The outcome of the experimentation exhibits that the proposed system outperforms the baseline ML algorithms and previous techniques on a holdout dataset. Though our approach acquires satisfactory results compared to other works, some improv-ements are still required to take this system in production level. Thus, in future, we will try to add reviews with more classes and conjoin the aspect of the reviews as well.          ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.   
"," The amount of textual data generation has increased enor-mously due to the effortless access of the Internet and the evolution of various web 2.0 applications. These textual data productions resulted because of the people express their opinion, emotion or sentiment about any product or service in the form of tweets, Facebook post or status, blog write up, and reviews. Sentiment analysis deals with the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer闁炽儲鐛 attitude toward a particular topic is positive, negative, or neutral. The impact of customer review is significant to perceive the customer attitude towards a restaurant. Thus, the automatic detection of sentiment from reviews is advantageous for the restaurant owners, or service providers and customers to make their decisions or services more satisfactory. This paper proposes, a deep learning-based technique  to classify the reviews provided by the clients of the restaurant into positive and negative polarities. A corpus consists of 8435 reviews is constructed to evaluate the proposed technique. In addition, a comparative analysis of the proposed technique with other machine learning algorithms presented. The results of the evaluation on test dataset show that BiLSTM technique produced in the highest accuracy of 91.35\%.",335
"  %  %       Storytelling is a central part of human socialization and entertainment. Many of the popular forms of storytelling throughout history \---such as novels, plays, television, and movies\--- have passive audience experiences. However, gaming is an interesting medium because interactivity is a large part of the entertainment experience, and interactivity and storytelling can often be in conflict: too much player freedom means a storyline may never be explored, while on the other hand, too many restrictions on player freedom risks reducing gaming to a passive medium. Thus, interactivity in storytelling has been an important challenge for gaming, with much design effort put into striking a balance between entertaining gameplay and compelling storytelling.  As gaming technology advances, new opportunities for interactive storytelling present themselves. Better storage technology made telling longer, more intricate stories possible, and better graphical capabilities helped foster more immersive gaming experiences. Advances in artificial intelligence have lead to more challenging opponents, more realistic NPC behavior, and other benefits. Better procedural content generation algorithms help ensure unique gameplay experiences that stay fresh for longer. Finally, recent breakthroughs in language modeling present a new opportunity: language, and thus stories, can potentially be generated on demand.   In this paper, we introduce a novel game of { \---opening sentences meant to kick-start participants' storytelling creativity\--- and the human player responds by adding a line, which we refer to from here on out as a {.  Our primary contributions are as follows:       , where humans and AI agents work together to create a story.              In this paper, we introduced the novel task of { more preferable than tuned and tuned more preferable than untuned in terms of engagingness, interestingness, and humanness metrics, as well as overall story quality preferences. Finally, we identified areas for potential future work, including evaluation of stories produced by humans and our system, integration of our system into intelligent agents such as robots and avatars, and improvement of generated story continuation quality by allowing genres or moods to be targeted.        The next two lines define the bibliography style to be used, and    the bibliography file.  \clearpage          If your work has an appendix, this is the place to put it.    \clearpage         
","   Storytelling plays a central role in human socializing and entertainment. However, much of the research on automatic storytelling generation assumes that stories will be generated by an agent without any human interaction. In this paper, we introduce the task of collaborative storytelling, where an artificial intelligence agent and a person collaborate to create a unique story by taking turns adding to it. We present a collaborative storytelling system which works with a human storyteller to create a story by generating new utterances based on the story so far. We constructed the storytelling system by tuning a publicly-available large scale language model on a dataset of writing prompts and their accompanying fictional works. We identify generating sufficiently human-like utterances to be an important technical issue and propose a sample-and-rank approach to improve utterance quality. Quantitative evaluation shows that our approach outperforms a baseline, and we present  qualitative evaluation of our system's capabilities.",336
"   The vast amounts of scientific literature can provide a significant source of information for biomedical research. Using this literature to identify relations between entities is an important task in various applications .  Existing approaches to biomedical relation extraction usually fall into one of two categories. Mention-level extraction aims to classify the relation between a pair of entities within a short span of text . In contrast, pair-level extraction aims to classify the relation between a pair of entities across an entire paragraph, document or corpus.  For both mention-level and pair-level relation extraction, recent work has been focused on representation learning. This is considered to be one of the major steps towards making progress in artificial intelligence . Representations of relations which understand their context are particularly important in biomedical research, where identifying fruitful targets is crucial due to the high costs of experimentation. Learning such representations is likely to require large amounts of unsupervised data due to the scarcity of labelled data in this domain.  Recent mention-level methods have been based on using large unsupervised models with Transformer networks  to learn representations of sentences containing pairs of entities. These representations are then used as the inputs to much smaller models, which perform supervised relation classification .  Recent pair-level methods have been based on encoding each mention of a pair of entities, and designing a mechanism to pool these encodings  into a single representation. This representation is then used to classify the relation between the entity pair .  However, representation learning methods for both mention-level and pair-level extraction typically use a point estimate for each representation. As a result, they may struggle to capture the nature of the true, potentially complex relations between each pair of entities. For example, Figure  shows sentences for two entity pairs which demonstrate that relation statements can be very different, typically depending on biological circumstances . Such nuanced relations can be difficult to capture with a single point estimate.  We hypothesise that there is a true underlying relation for each entity pair, and that this relation can be multimodal . The sentences containing each pair are textual observations of these underlying relations.  We therefore propose a probabilistic model which uses a continuous latent variable to represent the true relation between each entity pair. The distribution of a sentence containing that pair is then conditioned on this latent variable. In order to be able to model the complex relations between each entity pair, we use an infinite mixture distribution for the latent representation.  Our model provides a unified architecture for learning representations of relations between entity pairs both at mention and pair level. We show that  the posterior distribution of the latent variable can be used for mention-level relation classification. We also demonstrate that the prior distribution from the same model can be used for pair-level classification. On both tasks, we achieve results competitive with strong baselines with a model which has fewer parameters and is significantly faster to train.  The code is released at \url{ https://github.com/BenevolentAI/RELVM} %.       We have presented a model for learning representations of pairs of biomedical entities from unlabelled text corpora. We use a latent variable with an arbitrarily flexible distribution in order to be able to capture the complex relations between each pair of entities. The unified architecture can be used for both mention-level and pair-level relation extraction. On both tasks, we achieve results competitive with strong baselines. We also show significant computational gains in terms of the number of parameters and training times.  Our model presents many avenues for future work. The results in Table  show that the model's performance improves with the size of the hidden states in the networks; this suggests that there are further gains achievable simply by providing the model with more parameters. The model could be further scaled up by using a hierarchy of latent variables to increase the expressive power of the representations.   Other directions include evaluating the benefits of having a representation which explicitly captures uncertainty about the relations. For example, this can be done by assessing if the model is less confident when making predictions about entity pairs which do not occur frequently in the unlabelled corpus. Additionally, since our model can produce a representation for any pair of entities , it could be used in a link prediction setting to score unseen entity pairs.  
","     Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence  or across an entire corpus . In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.",337
"   Human communication is inherently multi-modal in nature. Our expressions and tone of voice augment verbal communication.\ This can include vocal features like speaking rate, intonation and visual features like facial expressions . Non-verbal communication is important for tasks that involve higher level cognitive expressions like emotions , persuasiveness  and mental health analysis . We focus on a multi-modal approach to emotion recognition because humans fundamentally express emotions verbally using spoken words , as well as with acoustic signals  and visual expressions .  Getting large-scale labeled datasets for emotion recognition can be challenging.\ Our primary motivation for this paper is to study effective utilization of large unlabeled datasets to improve performance of multi-modal emotion recognition systems.\ The signals we consider are speech, visual information and spoken text.\ Our motivation stems from the popular use of pre-trained models in natural language, speech and visual understanding tasks to circumvent data limitations.\ BERT is a popular model for natural language understanding  that was trained using self-supervision.\ Devlin et al. use the masked language modeling  task on the Wikipedia corpus for pre-training.\ The model was successfully fine-tuned to improve performance on several tasks like question answering and the general language understanding evaluation benchmarks . Self-supervised learning has also been successfully applied to speech based applications.\ Schneider et al.\ in  use unsupervised pre-training on speech data by distinguishing an audio sample in the future from noise samples.\ Fine-tuning this model shows state of the art results on automatic speech recognition . Liu et al.\ show in  that a BERT-like pre-training approach can be applied to speech.\ By predicting masked frames instead of masked words, the performance on tasks like speaker recognition,\ sentiment recognition and phoneme classification can be improved. For emotion recognition, Tseng et al.\ show in  that text-based self-supervised training can outperform state of the art models. The authors use a language modeling task, that involves predicting a word given its context, to pre-train the model.\ Another area of work that has leveraged unlabeled data is detection and localization of visual objects and spoken words in multi-modal input.\ Harwath et al.\ in  train an audio-visual model on an image-audio retrieval task.\ The models are trained to learn a joint audio-visual representation in a shared embedding space.\ This model can learn to recognize word categories by sounds without explicit labels.\ Motivated by the success of these approaches, we study if similar methods can be applied to multi-modal emotion recognition.\ To the best of our knowledge, a joint self-supervised training approach using text, audio and visual inputs has not been well explored for emotion recognition.   Multi-modal emotion recognition models have been well studied in literature and typically outperform uni-modal systems .\ These models need to combine inputs with varying sequence lengths.\ In video, the sequence lengths for audio and visual frames differ from the length of text tokens by orders of magnitude.\ There has been considerable prior work in fusing multi-modal features. Liang et al.\ in  studied multiple fusion techniques for multi-modal emotion recognition and sentiment analysis.\ Their methods included early and late fusion of modalities, and a dynamic fusion graph based network.\ They showed that the graph fusion model outperforms other methods.\ Early fusion and graph fusion techniques both require alignment between various modalities.\ Late fusion can be performed without alignment, but does not allow interaction of features from different modalities at the frame level.\ To overcome this limitation,\ Tsai et al.\ introduce the cross-modal transformer in .\ It scales the features using cross-modal attention.\ In the process, the modalities are projected into sequences of equal lengths, eliminating the need for any alignment.\ This architecture has been successfully applied to problems like emotion recognition, sentiment analysis  and speech recognition .\ Recently, another transformer-based method to combine multi-modal inputs was introduced by Rahman et al. in , which uses a multi-modal adaptation gate.  In this paper, we propose using the same pre-training scheme as BERT, but extend it to a model that uses audio, visual and text inputs. We discuss the relevance of this approach in Section .\ The multi-modal representations learned in pre-training are fine-tuned for emotion recognition.\ We evaluate the efficacy of the pre-training approach.\ We also perform experiments to understand the importance of each modality on the CMU-MOSEI dataset and provide case-studies to interpret the results.   This paper is organized as follows.\ In Section  we describe our model architecture and the self-supervised approach for pre-training, along with further motivation for the self-supervised learning we choose.\ In Section , we discuss the training setup and data.\ We present our results and analysis in Section  and conclude in Section .     In this paper, we present state of the art results on the emotion recognition task using the cross-modal transformer on the CMU-MOSEI dataset.\ We utilize a BERT-like pre-training scheme using audio, visual and text inputs.\ We use the VoxCeleb2 dataset to pre-train the model and fine-tune it for the emotion recognition task.\ We demonstrate up to a 3\  improvement over the baseline with the fine-tuned model. We presented our subjective analysis on the contribution of various modalities to emotion recognition.\ We also show results with missing input modalities to understand the importance of each modality for the emotion recognition task.  For our future work, we propose to initialize the text encoder with a text-only model like BERT, before multi-modal self-supervised training.\ VoxCeleb2 dataset, although large in terms of number of hours of video, is smaller when compared to the Wikipedia corpus which has billions of words. Taking advantage of a larger text-only corpus could provide improvements.\ We would also like to experiment with adapting the model on the CMU-MOSEI dataset.\ Both the VoxCeleb2 and CMU-MOSEI datasets are obtained from YouTube, but there could be domain mismatch between the two datasets. Adapting could help bridge the mismatch.\ We would also like to explore weak labels to adapt the pre-trained representations for the downstream task.\ Tseng et al.\ showed in  that weakly supervised labels can be used to effectively bias the embeddings learned by a pre-trained model. Even though we study the impact of ASR errors on emotion recognition, we do not know how these errors impact the self-supervised training. We would like to study that in the future. As noted before, our model architecture doesn't allow ablation of text. For our future work, we will focus on overcoming that limitation.  
","  Emotion recognition is a challenging task due to limited availability of in-the-wild labeled datasets.\ Self-supervised learning has shown improvements on tasks with limited labeled datasets in domains like speech and natural language.\ Models such as BERT learn to incorporate context in word embeddings, which translates to improved performance in downstream tasks like question answering.\ In this work, we extend self-supervised training to multi-modal applications.\ We learn multi-modal representations using a transformer trained on the masked language modeling task with audio, visual and text features.\ This model is fine-tuned on the downstream task of emotion recognition.\ Our results on the CMU-MOSEI dataset show that this pre-training technique can improve the emotion recognition performance by up to 3\% compared to the baseline.",338
" %  A long desired goal for AI systems is to play an important and collaborative role in our everyday lives.  Currently, the predominant approach to visual question answering  relies on encoding the image and question with a black-box transformer encoder.  These works carry out complex computation behind the scenes but only yield a single token as prediction output . Consequently, they struggle to provide an intuitive and human readable form of justification consistent with their predictions.  In addition, recent study has further demonstrated some unsettling behaviours of those models: they tend to ignore important question terms, look at wrong image regions, or undesirably adhere to superficial or even potentially misleading statistical associations.     To address this insufficiency, we reformulate VQA as a full answer generation task rather than a classification one, i.e. a single token answer. The reformulated VQA task requires the model to generate a full answer with natural language justification. We find that the state-of-the-art model answers a significant portion of the questions correctly for the wrong reasons.  To learn the correct problem solving process,  We propose \modelabbrevname{}  \underline{r}ead the question,  \underline{t}hink with multi-hop visual reasoning,      and finally  \underline{a}nswer the question.      %      Following this intuition, \modelabbrevname{} deploys four neural modules, each mimicking one problem solving step that humans would take:     %      A scene graph generation module first converts an image into a scene graph; A semantic parsing module parses each question into multiple reasoning instructions; A neural execution module  interprets reason instructions one at a time by traversing the scene graph in a recurrent manner and; A natural language generation module generates a full answer containing natural language explanations. The four modules are connected      through hidden states rather than explicit outputs.      Therefore, the whole framework can be trained end-to-end, from pixels to answers.     In addition, since \modelabbrevname{} also produces human-readable      output from individual modules during testing, we can easily     locate the error by checking the modular output.      %      %      Our experiments on GQA dataset show that      \modelabbrevname{} outperforms the state-of-the-art model by a large margin       on the full answer generation task.      Our perturbation analyses by removing relation linguistic cues from questions      confirm that      \modelabbrevname{} makes a step towards truly understanding the question rather than having a smart guess with superficial data correlations.      %      We discuss related work in Appendix A. To summarize, the main contributions of our paper are three-fold:              , an end-to-end trainable, modular VQA framework facilitating explainability and enhanced error analysis          % via plug-and-play          as compared to contemporary black-box approaches.                   % {{{https://github.com/Aishwarya-NR/LRTA\_Perturbed\_Dataset}}}                       %       We present \modelabbrevname{}, a transparent neural-symbolic reasoning framework for visual question answering, that incorporates [look, read, think and answer] steps to provide a human-readable form of justification at each step. The modular design of our methodology enables the whole framework to be trainable end-to-end. Our experiments on GQA dataset show that \modelabbrevname{} achieves high accuracy on full answer generation task, outperforming the state-of-the-art LXMERT results by a noticeable 15\  absolute margin. In addition, \modelabbrevname{} performance drops significantly more than LXMERT, when object attributes and relationships are masked, hence indicating that \modelabbrevname{} makes a step forward, towards truly understanding the question, rather than making a smart guess based on superficial data correlations. In the validation study, we have shown that when provided with an oracle scene graph, \modelabbrevname{} is able to achieve a high accuracy on both short answers  and full answers , nearing the theoretical bound 96\  on short answers. These observations indicate that better scene graph prediction methods offer a great potential in further improving \modelabbrevname{} performance on both short-answer and full-answer tasks.   
","   The predominant approach to visual question answering  relies on encoding the image and question with a ``black-box'' neural encoder and decoding a single token as the answer like ``yes'' or ``no''. Despite this approach's strong quantitative results, it struggles to come up with intuitive, human-readable forms of justification for the prediction process. To address this insufficiency, we reformulate VQA as a full answer generation task, which requires the model to justify its predictions in natural language. We propose LRTA [Look, Read, Think, Answer], a transparent neural-symbolic reasoning framework for visual question answering that solves the problem step-by-step like humans and provides human-readable form of justification at each step. Specifically, LRTA learns to first convert an image into a scene graph and parse a question into multiple reasoning instructions. It then executes the reasoning instructions one at a time by traversing the scene graph using a recurrent neural-symbolic execution module. Finally, it generates a full answer to the given question with natural language justifications. Our experiments on GQA dataset show that LRTA outperforms the state-of-the-art model by a large margin  on the full answer generation task. We also create a perturbed GQA test set by removing linguistic cues  in the questions for analyzing whether a model is having a smart guess with superficial data correlations. We show that LRTA makes a step towards truly understanding the question while the state-of-the-art model tends to learn superficial correlations from the training data.",339
"  . Question pairs from both Source and Target domains are encoded by in a common representation space, which is  the result of unsupervised adaptation of BERT on Target data.  The top k similar items from Source are then aggregated based on their label and distance to provide a final confidence.  % The  diagram roughly parallels Figure 1 from  but adapted to the DQD and cross-domain setup.  } \figlabel{knnprocess}   Duplicate question detection  is an important application in information retrieval and NLP . It allows systems to recognize when two questions share an answer. This is significant for community forums, such as StackExchange\footnote{https://stackexchange.com/}   to increase their effectiveness in avoiding redundant questions and displaying relevant answers to search questions. It is also important for FAQ retrieval question answering systems .  To learn DQD models for , question pairs are usually annotated with duplication information that is extracted from community-provided meta-data. Such annotations are sparse for most domains, e.g., a new  forum providing support for a new product.  Therefore, leveraging other training signals either from unsupervised data or supervised data from other domains is important .  Pre-trained language models  like BERT  and RoBERTA  are  great unsupervised textual representations. Several recent efforts adapt PLMs for the domains of interest  by  self-supervised fine-tuning on unsupervised domain data, which has shown  to be promising in several scenarios  .  We follow that and tune BERT on  domains to obtain richer representations for the task of DQD.   Recently,  -nearest neighbors  is applied on the PLM representations for language modeling  and dialogue . We extend this line of study and apply  for  cross-domain generalization in DQD, where the models are trained on data from a  domain, and applied on data from a  domain.  To do so, we represent pairs from source and target in a common representation space and then score target pairs using nearest neighbors in the source pairs. \figref{knnprocess} shows an illustration of this procedure.   % The specific properties of  DQD % is important to make this approach effective.  Our study on AskUbuntu as target and source datasets of , which include several domains of  and also Quora and Sprint, reveals that  is more effective compared to cross-entropy classification if  the pair representation space from PLMs is rich for the target domain, i.e., adapted on the unsupervised data  from target or similar domains; or   source and target domains have large distributional shifts.   We make the following contributions:   We present the first study of combining strengths of  and      neural representations for cross-domain generalization in a sentence matching task, i.e., DQD.   Our experimental results  on cross-domain DQD demonstrate that  on      rich question-pair representations advances the results of      cross-entropy classification, especially when shifts in source to target domains is substantial.     In this work,  we studied applying  in DQD cross-domain generalization. We compared  and a cross-entropy classifier when different  question-pair representations are available. Our results showed that domain-adaptive pre-training on target data gives rich representations, and  is more robust against distributional shifts compared to classification if question pairs are encoded by these rich representations.  We plan to extend our study to other tasks and understand better  the strengths of memorization in learning robust models where rich PLM embeddings are utilized to represent examples.  We believe concurrently that the promising results and findings of this presented study could benefit other NLP research to explore this direction more.    
","  Duplicate question detection  is important to increase efficiency of  community and automatic question answering systems.  Unfortunately, gathering supervised data in a domain is time-consuming and expensive, and our ability to leverage annotations across domains is minimal.  In this work, we leverage neural representations and study nearest neighbors for  cross-domain generalization in DQD.   We first encode question pairs of the source and target domain in a rich representation space and then using a k-nearest neighbour retrieval-based method, we aggregate the neighbors' labels and distances to rank pairs. We observe robust performance of this method in different cross-domain scenarios of StackExchange, Spring and Quora datasets, outperforming cross-entropy classification in multiple cases. We will release our codes as part of the publication. % ervised adaptation to StackExchange domains by self-supervised finetuning of contextualized embedding models like BERt. %We show the effectiveness of this adaptation in scenarios when source domain comes from different types of distributions. %Our analysis also reveals that unsupervised domain adaptation on even small amounts of data boosts the performance significantly. %Further, we show how an approach based on nearest neighbors is effective  for this problem and outperforms training the full model using cross entropy.",340
"   Learning vocabulary is a major component of foreign language learning. In the school context, initially vocabulary learning is typically organized around the words introduced by the text book. In addition to the incrementally growing vocabulary lists, some textbooks also provide thematically organized word banks. When other texts are read, the publisher or the teacher often provides annotations for new vocabulary items that appear in the text.  A wide range of digital tools have been developed to support such vocabulary learning, from digital versions of file cards to digital text editions offering annotations.  While such applications serve the needs of the formal learning setting in the initial foreign language learning phase, where the texts that are read are primarily chosen to systematically introduce the language, later the selection of texts to be read can in principle follow the individual interests of the student or adult, which boosts the motivation to engage with the book. Linking language learning to a functional goal that someone actually wants to achieve using language is in line with the idea of Task-Based Language Teaching  as a prominent strand of foreign language education .  Naturally, not all authentic texts are accessible to every learner, but linguistically-aware search engines, such as FLAIR , make it possible to identify authentic texts that are at the right reading level and are rich in the language constructions next on the curriculum. Where the unknown vocabulary that the reader encounters in such a setting goes beyond the around 2\% of unknown words in a text that can be present without substantial loss of comprehension , many digital reading environments provide the option to look up a word in a dictionary. Yet, frequently looking up words in such a context is cumbersome and distracts the reader from the world of the book they are trying to engage with. Relatedly, one of the key criteria of TBLT is that learners should rely on their own resources to complete a task . But this naturally can require pre-task activities preparing the learner to be able to successfully tackle the task . But how can a learner systematically prepare for reading a text or book they are interested in reading?  In this paper, we explore how computational linguistic methods such as distributional semantics, morphological clustering, and exercise generation can be combined with graph-based learner models to answer this question both conceptually and in practice. On the practical side, we developed an application that supports vocabulary learning as a pre-task activity for reading a self-selected book. The conceptual goal is to automatically organize the lexical semantic space of any given English book in the form of a graph that makes it possible to sequence the vocabulary learning in a way efficiently exploring the space and to visualize this graph for the users as an open learner model  showing their growing mastery of the book's lexical space.  Lexical learning is fostered and monitored through automatically generated multi-gap activities  that support learning and revision of words in the contexts in which they occur in the book.  In section we discuss how a book or other text chosen by the learner is turned in to a graph encoding the lexical space that the learner needs to engage with to read the book, and how words that are morphologically related as word families  are automatically identified and compactly represented in the graph . In section we then turn to the use of the graph representation of the lexical semantic space of the book to determine the reader's learning path and represent their growing lexical knowledge as spreading activation in the graph. In section, the conceptual ideas are realized in an application. We discuss how the new learner cold-start problem is avoided using a very quick word recognition task we implemented, before discussing the content selection and activity generation for practice and testing activities. Section then provides a conceptual evaluation of the approach and compares it with related with, before wrapping up with a conclusion in section.  % learning of rare words of English  but what is the purpose? And % the relevance of learning entire frequency bands of words is unclear  % How about combining the goal of reading a book with systematic % learning of what is needed to do so? Problem: Individuals are % interested in different books, and individual differ in language % competence and vocabulary knowledge. So how about the vocabulary of % books organizing themselves individually adaptive organization  % Goal:  %  %   % Solution: %  %       In this paper, we discussed the methodological basis and realization of a tool allowing the learner to systematically learn the lexical material needed to be able to read a book they are interested in. Automatically structuring the lexical space and sequencing the learning is achieved through distributional semantic methods, the automatic identification of word families, and concepts from network analysis. The graph-based domain model that is automatically derived from the given book serves as the foundation of a learner model supporting the selection of an efficient learning path through the lexical space to be acquired. Multi-gap activities are automatically generated from the targeted book and used for practice and testing activities.     The application is also well suited to be a dedicated vocabulary   learning application as indicated earlier. The teachers can guide the   students to master vocabulary from the books of renowned authors where   they also exposed to a the intriguing language usage.  In addition to self-guided learning for people interested in reading specific books, which may be particularly useful in the context of so-called intensive reading programs, the approach is particularly well-suited for the English for Specific Purposes context, where both the language and the particular content domain are of direct importance.  Given this kind of integration of language and content learning, a similar affinity exists to so-called Content and Language Integrated Learning .    listhe thely auisd a basis for  is aab limitation that we should mention and point to an option for overcoming it:   This application can also be upgraded to learn domain knowledge.   Since the distribution semantic space is defined by a pre-trained   vector space model.  Some of the domain specific proper nouns are   missing. This could be overcame by training a custom vector space for   the chosen text. This leverage this application to facilitate domain   knowledge learning/revising like jargon, scientific names,   geographical names etc...    Additional supporting materials could be explored to scaffold the   learning apart from the usage in the chose text, dictionary reference   and translation of the word into learner's native language which are   used currently.    Though the learn model is further pruned to improve the visualisation.   The connectivity are potentially overwhelming. There could be a   considerable improvement in reporting global and local progress in the   structured space.  Or a simplified approach of visual thesaurus could   be adopted.    This application provides a lot of scope for gamification because of   its exploratory objective of vocabulary space provided with a graph   based framework to maximise the coverage. Which could be themed around   the goal of being reaching/trained for the actual task with more   engaging activities.    
","   How can a learner systematically prepare for reading a book they are   interested in? In this paper, we explore how computational   linguistic methods such as distributional semantics, morphological   clustering, and exercise generation can be combined with graph-based   learner models to answer this question both conceptually and in   practice. Based on the highly structured learner model and concepts   from network analysis, the learner is guided to efficiently explore   the targeted lexical space. They practice using multi-gap learning   activities generated from the book focused on words that are central   to the targeted lexical space. As such the approach offers a unique   combination of computational linguistic methods with concepts from   network analysis and the tutoring system domain to support learners   in achieving their individual, reading task-based learning goals.",341
"  Recent decades have brought about an increase in the use of computer-based tools in practically every  field of human endeavor. The field of education is no exception. Such tools can be used to augment or  even completely replace traditional face-to-face teaching methods. The emergence of online learning platforms has necessitated the development of means to enable learning activities, such as  group discussions, to be performed through the use of technology. One such example of a learning  platform is the IMapBook software suite aimed at increasing the literacy and reading  comprehension skills of elementary school-aged children through the use of web-based eBooks,  embedded games related to their contents, as well as moderated group discussions. Keeping these discussions constructive and relevant can be difficult and usually requires a  discussion moderator to be present at all times. This can limit the opportunities for such discussions to take place. Leveraging the methods and insights  from the fields of artificial intelligence and machine learning, we can attempt to develop systems to automatically classify messages into  different categories and detect when the discussion has veered off course and necessitates intervention. Our research tackles this problem using a  compilation of discussions obtained during pilot studies testing the effectiveness of using the IMapBook software suite in 4th-grade classrooms.  The studies were performed in 8 different Slovene primary schools and, in total, included 342 students.  The discussions consist of 3541 messages along with annotations specifying their relevance to the  book discussion, type, category, and broad category. The ID of the book being discussed and the time  of posting are also included, as are the poster's school, cohort, user ID, and username.  Each message was also manually translated into English to aid non-Slovene-speaking researchers.  The use of the Slovene language presents unique challenges in applying standard language  processing methods, many of which are not as readily available as for other, more widely spoken languages.  Given a sequence of one or more newly observed messages, we want to estimate the relevance of  each message to the actual topic of discussion. Namely, we want to assign messages into two categories 閳 relevant to the book being discussed or not.  Additionally, we want to predict whether the message is a question, an answer, or a statement which we call the type of the message. Finally, we want to  assign a category label to each message where the possible labels can be either 'chatting', 'switching', 'discussion', 'moderating', or 'identity'.  Building a predictive model capable of performing such predictions with acceptable performance would allow us to experiment with including this new  level of automation in the IMapBook software suite as well as in any related products. The research insights are also applicable to areas such as  online user comments and content moderation.     The best results were achieved by using the Feature stacking method model built on the complete  feature subset. The results indicate the performance to be sufficient for the methods to be used  in real-world tools and platforms. A significant portion of the information needed for  correct classifications is hidden in the strong temporal interdependence of the messages which  our developed methods exploited only marginally.  
"," The increasing adoption of technology to augment or even replace traditional face-to-face learning has led to the development of a myriad of tools and platforms aimed at engaging the students and facilitating the teacher's ability to present new information. The IMapBook project aims at improving the literacy and  reading comprehension skills of elementary school-aged children by presenting them with interactive  e-books and letting them take part in moderated book discussions. This study aims to develop and  illustrate a machine learning-based approach to message classification that could be used to  automatically notify the discussion moderator of a possible need for an intervention and also to collect other useful information about the ongoing discussion. We aim to predict whether a message posted in the discussion is relevant to the discussed book, whether the message is a statement, a question, or an answer, and in which broad category it can be classified. We incrementally enrich our used feature subsets and compare them using standard classification algorithms as well as the novel Feature stacking method.  We use standard classification performance metrics as well as the Bayesian correlated t-test to show  that the use of described methods in discussion moderation is feasible. Moving forward, we seek to  attain better performance by focusing on extracting more of the significant information found in the  strong temporal interdependence of the messages.",342
" The   was proposed by  as a means to test whether a  machine has human-like intelligence. It is an alternative to the well known   and has been designed with the motivation of reducing certain problematic aspects that affect the TT. Specifically, while the TT is subjective in nature, the WSC provides a purely objective evaluation; and whereas passing the TT requires a machine to behave in a deceptive way, the WSC takes the form of a positive demonstration of intelligent capability.  The core problem of the WSC is to resolve the reference of pronouns occurring in natural language sentences.  To reduce the possibility that the task can be accomplished by procedures based on superficial or statistical characteristics, rather than `understanding' of the sentence, they specify that the test sentences used in the WSC, should be constructed in pairs, which have similar structure and differ only in some key word or phrase, and such that the correct referent of the pronoun is different in the two cases. This sentence pair, together with an indication of which pronoun is to be resolved and a pair of two possible candidates, is called a .   The following is an example of the Winograd schemas from the original WSC273 data set :     is too {.   the trophy / the suitcase, {    design Winograd schemas to require background knowledge to resolve a pronoun, which can be an evidence of thinking\/. Therefore, they exclude the sentences that can be resolved by a statistical association within a sentence.   In this paper, we introduce a keyword method to define domains in Winograd schemas. To our best knowledge, this is the first work to use keywords for defining domains in WSC and explore high-level patterns in them. To use the domain-specific high-level patterns, we also develop an advanced high-level knowledge-based reasoning method by modifying the method of . Furthermore, we suggest a simple ensemble method that combines knowledge-based reasoning and machine learning. By the experiments on the domain-specific data set, the ensemble method gives a better performance than each single method. Lastly, we also propose a `robust' accuracy  measure that is more objective by improving the switching method of .     This paper demonstrates that combining both the high-level knowledge-based reasoning method and the BERT can give a better performance in the thanking domain.  In this paper, we also introduced the keywords method to identify a domain, and this method can be applied to specify other domains. We showed that high-level patterns were found in the domain defined by the keywords. As only one domain --- the thanking domain --- was tackled, future work needs to be done with more domains in Winograd schemas. Though the number of the thanking domain is   as a pilot study, some other domains could be larger than the thanking domain. For instance, the domain that can be defined by the keywords ``love'' and ``hate'' has   and   sentences respectively. If these were genuinely separate domains and the correct resolution of each schema were based on principles in the domain corresponding to the key words it contains, this would imply that tackling around  domains could cover almost all domains in Winograd schemas.  By modifying the method of  and focusing on the domain-specific semantic roles, we were able to develop a knowledge-based reasoning method that can use domain-specific high-level patterns. Though our knowledge-based method uses background knowledge principles that are built manually, we believe that our principles are more accurate than the kinds of semantic feature that could be reliably extracted from a large corpus or by using a search engine. This is because the simple statistical method used for automatically extracting knowledge is vulnerable to data bias or special usage of words in idioms . In addition, our knowledge-based method can also be used in other natural language tasks such as Choice Of Plausible Alternaties  . But K-Parser used in our approach still needs to be improved as manual corrections were needed in some cases.  We also propose the robust accuracy by improving the method of . The decreased robust accuracies of the BERT reveal that its accuracy may not entail its real understanding.     The code for the advanced high-level knowledge-based reasoning method  can be accessed from the following repository: {\tt}  
"," The   is a common sense reasoning task that requires background knowledge. In this paper, we contribute to tackling WSC in four ways. Firstly, we suggest a keyword method to define a restricted domain where distinctive high-level semantic patterns can be found. A  was defined by keywords, and the data set in this domain is used in our experiments. Secondly, we develop a high-level knowledge-based reasoning method using semantic roles which is based on the method of . Thirdly, we propose an ensemble method to combine knowledge-based reasoning and machine learning which shows the best performance in our experiments. As a machine learning method, we used Bidirectional Encoder Representations from Transformers  . Lastly, in terms of evaluation, we suggest a `robust' accuracy measurement by modifying that of . As with their switching method, we evaluate a model by considering its performance on trivial variants of each sentence in the test set.",343
"      The use of deep learning for processing natural language is becoming a standard, with excellent results in a diverse range of tasks. Two state-of-the-art  architectures for text-related modeling are long short-term memory  networks~ and transformers~. LSTMs are recurrent neural networks that process the text sequentially, meaning that they process text one token at a time, building up its internal representation in hidden states of the network. Due to the recurrent nature of LSTM, which degrades the efficiency of parallel processing, as well as demonstrated improvements in performance, models based on the transformer architecture are gradually replacing LSTMs across many tasks. Transformers can process the text in parallel, using self-attention and positional embeddings to model the sequential nature of the text.  A common trend in using transformers is to first pre-train them on large monolingual corpora with abstract, general-purpose objective, and then fine-tune them for a specific task, such as text classification.  For example, the BERT  architecture  uses transformers and is pretrained with masked language modelling and order of sentences prediction tasks to build a general language understanding model. During the fine-tuning for a specific downstream task, additional layers are added to the BERT model, and the model is trained on specific data to capture the specific knowledge required to perform the task.   Most of the research in the natural language processing  area focuses on English, ignoring the fact that English is specific in terms of the low amount of information expressed through morphology . In our work, we focus on adapting modern deep neural networks, namely LSTMs and BERT, for several morphologically rich languages, by explicitly including the morphological information. The languages we analyze contain rich information about grammatical relations in the morphology of words instead of in particles or relative positions of words . For comparison,  we also evaluate our models on English. Although previous research has shown that the state of the art methods such as BERT already captures some information contained in the morphology~, our experiments involve several languages with rich morphology where neural networks could benefit from explicit morphological features.  Specifically, we present methods which combine BERT with separately encoded morphological properties: universal part of speech tags  and universal features . We evaluate them on three downstream tasks: named-entity recognition , dependency parsing , and comment filtering . We perform similar experiments on LSTM networks and compare the results for both architectures. Besides English, we analyze eight more languages: Croatian, Estonian, Finnish, Latvian, Lithuanian, Russian, Slovene and Swedish. The choice of these languages reflects a mix of different language groups , for which we were able to obtain sufficient resources , due to their coverage in the EU EMBEDDIA project.   Our experiments show that the addition of morphological features has mixed effects depending on the task. Across the tasks where the added morphological features improve the performance, we show that  they benefit the LSTM-based models even if the features are noisy and  they benefit the BERT-based models only when the features are of high quality , suggesting that BERT models already capture the morphology of the language; however, there is a room for improvement either in designing pre-training objectives that can capture these properties or when high-quality features are available.   The remainder of this paper is structured as follows. In Section, we present different attempts to use morphological information in machine learning, in particular neural networks, as well as an overview of recent work in the three evaluation tasks. In Section, we describe the used datasets and their properties. In Section, we present the baseline models and models with additional morphological information, whose performance we discuss in Section. Finally, we summarize our work and present directions for further research in Section.      We analysed adding explicit morphological information in the form of embeddings for POS tags and morphological features to two currently dominant neural network architectures used in NLP: LSTM networks and transformer-based BERT models. We compared models enhanced with morphological information with baselines on three tasks . To obtain general conclusions, we used subsets of eight morphologically-rich languages from different language families.   The results indicate that adding morphological information to NER prediction models is not beneficial, but it improves the performance in the NER and DP tasks. For the DP task, the improvement depends on the quality of the morphological features. The additional morphological features consistently benefited LSTM-based models for NER and DP, both when they were of high quality and predicted . For BERT-based models, the  features do not make any practical difference for the NER and DP task but improve the performance in the DP task when they are of . Testing different variants of BERT shows that language specialised variants improve the performance on the DP task and the additional morphological information is beneficial, though less and less as we shift from multilingual towards monolingual models.  The comparison of different BERT variants indicates that BERT models do not completely capture the language morphology.  Since the release of BERT, several new pre-training objectives have been proposed, such as syntactic and semantic phrase masking~ and span masking~. In further work, it makes sense to apply these models to the DP task in order to test how well they capture the morphology. Further, the effect of morphological features could be analysed on additional tasks and languages, since the explicit morphological information does not seem to benefit them equally.    This paper is supported by European Union閳ユ獨 Horizon 2020 Programme project EMBEDDIA . The research was supported by the Slovene Research Agency through research core funding no. P6-0411. The Titan X Pascal used for a part of this research was donated by the NVIDIA Corporation.   
"," Currently, deep learning approaches are superior in natural language processing due to their ability to extract informative features and patterns from languages. Two most successful neural architectures are LSTM and transformers, the latter mostly used in the form of large pretrained language models such as BERT.  While cross-lingual approaches are on the rise, a vast majority of current natural language processing techniques is designed and applied to English, and less-resourced languages are lagging behind. In morphologically rich languages, plenty of information is conveyed through changes in morphology, e.g., through different prefixes and suffixes modifying stems of words. The existing neural approaches do not explicitly use the information on word morphology. We analyze the effect of adding morphological features to LSTM and BERT models. As a testbed, we use three tasks available in many less-resourced languages: named entity recognition , dependency parsing , and comment filtering . We construct sensible baselines involving LSTM and BERT models, which we adjust by adding additional input in the form of part of speech  tags and universal features. We compare the obtained models across subsets of eight languages. Our results suggest that adding morphological features has mixed effects depending on the quality of features and the task. The features improve the performance of LSTM-based models on the NER and DP tasks, while they do not benefit the performance on the CF task. For BERT-based models, the added morphological features only improve the performance on DP when they are of high quality , while they do not show any practical improvement when they are predicted. As in NER and CF datasets manually checked features are not available, we only experiment with the predicted morphological features and find that they do not cause any practical improvement in performance.",344
"  In this paper we focus on the problem of integrating syntactic features in a neural architecture for the Frame-Semantic parsing  process. Frame-semantic parsing is the task of extracting full semantic frame structures from text, as  defined by   Frame Semantics theory .  %Semantic frames are conceptual structures describing general situations, evoked in language by target words referred to as lexical units. Each frame is enriched by a set of semantic roles called frame elements, defining specific participants in the described situation.  %An example of a sentence annotated with Frame Semantics is shown in Figure .  From a theoretical perspective, Frame-Semantic parsing can be decomposed into three sub-tasks: 1) Target Identification  -- identifying target words acting as lexical units; 2)  Frame Identification  -- disambiguating each target into a possible frame; and 3) Semantic Role Labeling  -- extracting all the possible frame elements for a given frame.  Early neural approaches have focused in this regard on the integration of features extracted from dependency trees, both for the FI and SRL tasks , with positive results. Amongst all, SRL is the task that has received more attention when investigating methods for injecting syntax into neural models, mostly due to the strict correlation between syntax and argument structures .  Several solutions have been proposed, setting new baselines over general Frame-semantic parsing and specific SRL corpora. These include the use of dependency path embeddings , the application of Graph Convolutional Networks  to learn representations of the dependency graphs , or restricting the set of candidate arguments using pruning algorithms . Multi-task learning has been also applied, either directly supervising attention to learn dependency parsing  for both TI and SRL, or to implicitly bias learned encoded representations when jointly training a simplified syntactic dependency parser , or a semantic dependency parser for both FI and SRL . %   Although effective, these approaches have focused on exploiting syntactic dependencies rather than  constituency information, partly because dependencies are more suited to be encoded as features or learned through attention mechanisms. %, as they express relationships between words.  Semantic roles are technically provided over syntactic constituents, which directly cast argument boundaries over word sequences. This is demonstrated also by earlier work on SRL, which relied on constituency derived features . It follows that using constituency information should be beneficial, especially because reconstructing argument boundaries through dependencies would require an unbounded number of hops among words, making the problem hard to model in neural architectures . Following this idea, two recent approaches have attempted to rely on such constituency information to improve SRL performance.  use linearised representations of constituency trees in different learning settings, either by extracting salient features, by multi-task learning, or by combining both approaches in an auto-encoding fashion. , instead, train a  GCN with the SRL objective to learn constituent representations, which are then infused into words through the same GCN via the message-passing operation .  In this paper, we foster the same idea of relying on constituency information for every sub-task of Frame-semantic parsing, namely TI, FI, and SRL. We train a GCN to learn specific constituency representations, which are used in turn to compute syntactic paths between constituency nodes.  Our approach is similar to that of , although it significantly differs in: i) the initialisation and topology of the underlying graph; ii) the lower number of required parameters; and iii) the way that syntactic information is infused in every word representation, i.e. computing node-to-node syntactic paths. We show that our approach improves the state-of-the-art over the main Frame-semantic parsing benchmark, i.e.\ the FrameNet corpus , on the single TI and SRL tasks, and on FI in a joint-learning setting. Moreover, we demonstrate the generality of the approach by testing the same network on the CoNLL 2005 dataset .    In this work, we investigated the integration of structural information from a constituent tree in a neural model for Frame-semantic parsing. Constituent representations are learned through a GCN,   to learn encoded representations of syntactic constituents, which is trained with the specific task objective.  and used to build constituency path features to be added to every word representation in a sequence.  Each word in a sequence is enriched with syntactic information by summing all the constituent learned encodings on the path between the word and a task-specific node in the tree, e.g. the target word of a predicate.  We tested our approach on all the Frame-semantic parsing sub-tasks, namely Target Identification, Frame Identification, and Semantic Role Labeling, showing that such features contribute mainly on the TI and the SRL tasks.   Constituency path features can be applied  Future work will cover the application of the proposed constituency path features  to other sequence labelling based tasks, e.g. Named-Entity Recognition. Moreover, other modifications of GCNs have to be tested in this same framework, e.g. to assess whether Attention-based GCN may learn more refined constituent representations. Finally, these representations may be used in a node-classification approach, inspired by seminal works , in an attempt to move away from the well-used sequence labelling model of   recent years.        \clearpage      
"," We study the problem of integrating syntactic information from constituency trees into a neural model in Frame-semantic parsing sub-tasks, namely Target Identification , Frame Identification , and Semantic Role Labeling . We use a Graph Convolutional Network to learn specific representations of constituents, such that each constituent is profiled as the production grammar rule it corresponds to. We leverage these representations to build syntactic features for each word in a sentence, computed as the sum of all the constituents on the path between a word and a task-specific node in the tree, e.g. the target predicate for SRL. Our approach improves state-of-the-art results on the TI and SRL of \texttildelow$1\%$ and \texttildelow3.5\% points, respectively , when tested on FrameNet 1.5, while yielding comparable results on the CoNLL05 dataset to other syntax-aware systems. %When testing the approach on the FrameNet 1.5 corpus, our system gives us strong insight on the role of syntax on TI, while improving the state-of-the-art on SRL of 3.5 points.  %While it yields comparable results on the CoNLL05 dataset.",345
" Sequence labeling is one of the commonly used techniques for solving natural language understanding  tasks such as named-entity recognition  and slot filling. Furthermore, for these tasks, the state-of-the-art results are typically based on deep neural networks . However, the performance of these models is highly dependent on the availability of large amounts of annotated data. Moreover, compared with classification tasks, which require only one label for a sample, the sequence learning tasks require a series of token-level labels for an entire sequence, which makes them time-consuming and a costly annotation process.  \\ This problem can be mitigated using active learning , which achieves improved performance with fewer annotations by strategically selecting the examples to annotate .  There are two major strategies for active learning, namely, diversity-based sampling and uncertainty-based sampling . % 闉愵剠璧 闉氭尗鍎婇爟 Traditionally, uncertainty-based sampling is the most common pool-based AL approach. However, previous work pointed out that focusing only on the uncertainty leads to a sampling bias . It creates a pathological scenario where selected samples are highly similar to each other, which clearly indicates inefficiency. This may cause problems, especially in the case of noisy and redundant real-world datasets. Another approach is diversity-based sampling, wherein the model selects a diverse set such that it represent the input space without adding considerable redundancy . This approach can select samples while ensuring a maximum batch diversity. However, this approach might select points that provide little new information, thereby reducing the uncertainty of the model. Certain recent studies for classification tasks implemented an algorithm named Batch Active learning by Diverse Gradient Embeddings . This algorithm first computes embedding for each unlabeled sample based on induced gradients, and then geometrically picks the instances from the space to ensure their diversity .  Although it proves to be a robust improvement when performing an image classification task, its performance in sequence labeling tasks is yet unproven. \\ In this study, we investigated some practical active learning algorithms that consider uncertainty and diversity in sequence labeling tasks over different datasets and models. Moreover, we suggested a method to expand BADGE with weighted sampling based on the sequence length to ensure cost-effective labeling. This simple modification in it has a positive implication that it tends to select cost-effective samples.  The proposed model trades off between uncertainty and diversity by selecting diverse samples in the gradient space depending on the parameters in the final layer, for which, the currently available models focus only on uncertainty. To the best of our knowledge, our study is the first to apply diverse gradient embedding to a sequence labeling task. We experimented with the CoNLL 2003 English, ATIS, and Facebook Multilingual Task Oriented Dataset . Accordingly, it was empirically demonstrated that the proposed method consistently outperformed the baseline method including Bayesian AL by disagreement , which shows state-of-the-art performance in NER task, across the datasets, tasks and model architectures.     In this study, we explored the empirical study on AL utilizing the advantages of both uncertainty and diversity by selecting weighted diverse gradient embeddings to perform a sequence labeling task. We proposed an efficient method and empirically demonstrated that it could consistently achieve a superior performance while consuming much less data. It adds robustness to the dataset and the architecture, thus proving to be a useful option for solving real-world active learning problems   
","  Recently, several studies have investigated active learning  for natural language processing tasks to alleviate data dependency. However, for query selection, most of these studies mainly rely on uncertainty-based sampling, which generally does not exploit the structural information of the unlabeled data. This leads to a sampling bias in the batch active learning setting, which selects several samples at once. In this work, we demonstrate that the amount of labeled training data can be reduced using active learning when it incorporates both uncertainty and diversity in the sequence labeling task. We examined the effects of our sequence-based approach by selecting weighted diverse in the gradient embedding approach across multiple tasks, datasets, models, and consistently outperform classic uncertainty-based sampling and diversity-based sampling.",346
"}  {I}{n} the past decade, we have seen the emergence of various Knowledge Graphs , such as YAGO and DBPedia. They have achieved great success in both academic and industrial applications, ranging from recommendation to Question Answering. However, these KGs are far from complete, which limits the benefits of transferred knowledge. Relation Extraction  is a vital step to complete KGs by extracting the relations between entities from texts. It is nontrivial since the same relation type may have various textual expressions, and meanwhile, different types of relations can also be described with the same words. Such ambiguity between relations and texts challenges the supervision of RE models.  Due to the expensive human annotation cost, distant supervision is proposed to automatically annotate the mappings between sentences and relations. It assumes that if two entities participate in a relation, a.k.a., a triple  but express another relation . As shown in Figure, given the triple , we collect two sentences that include the entity pair . Clearly, the first sentence expresses a similar meaning with the given relation type, but the second one implies another type of relation city of, which brings in noise to the training corpora\footnote{As the term relation can refer to either relation type or relation instance , in the paper, we simplify the use of term relation for relation type unless otherwise stated.}. To highlight informative sentences, many existing works introduce the attention mechanism to assign sentences with different learning weights.  In terms of quantity, on the other hand, most of the training data collected by distant supervision concentrate mainly on a few relations, leading to the issue of the lack of sufficient annotations for the remaining relations. Take the widely used dataset, New York Times , as an example, we present the number of training instances of each relation in Figure. Unsurprisingly, the annotations are long-tail concerning different relations, and the tail relations suffer from insufficient training corpora. More specifically, each relation  refers to multiple entity pairs  is smaller than that between  should be more similar with respect to RE prediction distributions because of the more common textual contexts. Therefore, how to capture relation proximity in a more precise and general way remains challenging.  Another major challenge is to distinguish between different relations, in case the knowledge transfer introduces a bias towards the same prediction for proximate relations. For example, as mentioned above, both /location/us\_state/capital and /location/fr\_region/capital indicate the capital relation, and the only difference is that between two United States entities or French entities. DPEN incorporates entity type information to learn relation-specific classifier dynamically. However, entity type information is sparse in KGs , challenging the scalability.  To address the first issue, we propose to learn relation prototypes that capture the proximity relationship among relations from involved entity pairs. Inspired by Prototypical Networks, we represent each relation prototype with the centroid of its training data, and each data point is defined as the difference between the pair of entity embeddings, namely implicit mutual relation . Given any entity pair, we compute the implicit mutual relation and its distance to each relation prototype. These proximities suggest possible relations to the classifier, which further makes correct predictions by extracting discriminative signals from supportive sentences. Relation prototypes can also be enhanced by prior information , and be applied to arbitrary sentence encoder.  To address the second issue, we enhance entity embeddings with textual information for implicit mutual relation learning. In specific, we construct an entity co-occurrence graph from unlabeled texts and modeling both the first-order and second-order structural proximity. The massive textual contexts are helpful to infer entity types for distinguishment. Besides, long-tail entity pairs can also benefit from additional textual information. We summarize our main contributions as follows:  [leftmargin=*]       A preliminary version of this work has been published in the conference of ICDE 2020. We summarize the main changes as follows:  [leftmargin=*]     %The rest of the paper is organized as follows. In Section, we formulate the problem and overview the framework, and Section introduces our proposed method in detail. We report the promising experiment results on real-world datasets in Section. Section covers the related works. Finally, we conclude the paper in Section.  [htp]              In conclusion, we have proposed a general approach to learn relation prototypes from unlabeled texts. The prototype learning method can be applied in current models for better relation extraction by transferring knowledge from relations with sufficient training data to long-tail relations. We have conducted extensive experiments to verify the effectiveness of the proposed method on two publicly available datasets and compared them with eight state-of-the-art baselines. The results present significant improvements, especially in long-tail settings. Further ablation study and case study also demonstrate the effectiveness of our proposed method and the generalization ability to current RE models from both quantitative and qualitative perspectives. In the future, we are interested in enhancing entity embeddings with KG including structure and attribute information.   investigating more advanced entity embedding models, such as Graph Attention Networks , to improve the implicit mutual relation representation as well as relation prototypes. Also, other side information  can be incorporated to enrich the entity co-occurrence graph for better modeling.  
","   Relation Extraction  is a vital step to complete Knowledge Graph  by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information.      We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements . Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes will be released later.    %Relation Extraction  is a paramount step to complete Knowledge Graph by extracting entity relations from texts. However, it usually suffers from the long-tail issue, as the training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail RE by transferring knowledge from those with sufficient data. We learn prototypes as an implicit factor between entities, to reflect the meanings of relations and their proximities. Specifically, we construct an entity co-occurrence graph from texts, and capture structural proximities for embedding learning. Furthermore, we optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to many RE framework. We have conducted extensive experiments on two publicly available datasets. Compared with eight state-of-the-art baselines, our model achieves significant improvements . Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components and the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis.",347
"  % Understanding how BERT works is important. % the presence of blackbox nlp  is an indication that the research community values the ability to understand the internals of deep neural networks. Pre-trained transformer models such as BERT  are currently ubiquitous within natural language processing  research and have demonstrated improvements in topics from sentiment analysis to semantic parsing . The widespread development and use of such models has led to an increased effort to interpret such models' decisions . % * understanding models is important in society % * BERT is used all over, so important to understand BERT As defined in , model interpretability is ``the ability [of a model] to explain or present in understandable terms to a human''.  Intuitively, a more interpretable model is easier to understand, debug and improve.  % It's hard to understand BERT because  % * it's a neural model with many, many parameters % * pre-training + fine-tuning is newer than just training from scratch  -> read literature introductions/motivations Interpreting modern pre-trained transformer models is difficult. First, modern deep learning models have hundreds of millions of parameters, and scale only continues to increase . Understanding the impact of a single parameter is nearly impossible because these models are densely connected. Combined with the sheer number of parameters, manual analysis is infeasible. Secondly, while both pre-training and fine-tuning are required for state-of-the-art performance, effort has focused on alternative pre-training methods . % Understanding the impacts of fine-tuning is still not well understood. \todo{do I need a citation here?}   % Previous work attempted to use attention Previous work uses BERT's self-attention mechanism to interpret the model's predictions . However, a body of work  shows that models' attention mechanisms cannot be interpreted on single-sequence classification tasks.  % We apply bert to a sequence classification task We apply BERT and two BERT-based models  to an existing sentence classification task proposed in . We compare BERT-based models' performances with previous baselines and then use methods presented in  and  to evaluate BERT's interpretability in single-sequence classification tasks. We find that fine-tuning can teach BERT to recognize previously unknown patterns in natural language and that BERT is more interpretable than the attention-based models analyzed in  and . To summarize, the key contributions of this paper are:        % this is nice because  % * BERT hasn't been applied to it % * professional data set, we have a baseline, all human-annotated % * it has marked spans of edits before and after % To the best of our knowledge, BERT has not been applied to the Automatic Evaluation of Scientific Writing  task.       future work and applications   might use edited versions as negative cases   seq2seq model?   compare with non-bert attention models? In this paper, we apply three BERT-based models to a sentence classification task, then quantify their interpretability through a small-scale manual study before expanding to a larger-scale automated study. We find that BERT's final attention layer is clearly interpretable by both human annotators and simple automated metrics.  Future work might expand the subset of examples that can be automatically annotated in order to further understand BERT's interpretability on different classes of edits. Additionally, more work is needed to understand the impacts of in-domain pre-training on model interpretability.     
"," Pre-trained transformer language models such as BERT are ubiquitous in NLP research, leading to work on understanding how and why these models work. Attention mechanisms have been proposed as a means of interpretability with varying conclusions. We propose applying BERT-based models to a sequence classification task and using the data set's labeling schema to measure each model's interpretability. We find that classification performance scores do not always correlate with interpretability. Despite this, BERT's attention weights are interpretable for over 70\% of examples.",348
"  .          % % final paper: en-us version           %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }    Recently, neural machine translation  has demonstrated impressive performance improvements and became the de-facto standard .    However, like other neural methods, NMT is data-hungry.   This makes it challenging when we train such a model in low-resource scenarios .   Researchers have developed promising approaches to low-resource NMT.   Among these are data augmentation , transfer learning , and pre-trained models .   But these approaches rely on external data other than bi-text.   To date, it is rare to see work on the effective use of bilingual data for low-resource NMT.    In general, the way of feeding samples plays an important role in training neural models.   A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems.   More systematic studies on this issue can be found in recent papers .   For example,  have pointed out that deep neural networks tend to prioritize learning ``easy'' samples first.   This agrees with the idea of curriculum learning  in that an easy-to-hard learning strategy can yield better convergence for training.    In NMT, curriculum learning is not new.   Several research groups have applied it to large-scale translation tasks although few of them discuss the issue in a low-resource setup .   The first question here is how to define the ``difficulty'' of a training sample.   Previous work resorts to functions that produce a difficulty score for each training sample.   This score is then used to reorder samples before training.   But the methods of this type enforce a static scoring strategy and somehow disagrees with the fact that the sample difficulty might be changing when the model is updated during training.   Another assumption behind curriculum learning is that the difficulty of a sample should fit the competence of the model we are training.   Researchers have implicitly modeled this issue by hand-crafted curriculum schedules  or simple functions , whereas there has no in-depth discussion on it yet.    In this paper, we continue the line of research on curriculum learning in low-resource NMT.   We propose a dynamic curriculum learning  method to address the problems discussed above.   The novelty of DCL is two-fold.   First, we define the difficulty of a sample to be the decline of loss .   In this way, we can measure how hard a sentence can be translated via the real objective used in training.   Apart from this, the DCL method explicitly estimates the model competence once the model is updated, so that one can select samples that the newly-updated model has enough competence to learn.      DCL is general and applicable to any NMT system.   In this work, we test it in a Transformer-based system on three low-resource MT benchmarks and different sized data selected from the WMT'16 En-De task.   Experimental results show that our system outperforms the strong baselines and several curriculum learning-based counterparts.           In this paper, we propose a dynamic curriculum learning  method to explore the effective use of bilingual data for low-resource NMT.   We define the difficulty of a training sample by the decline of loss and estimate the model competence self-adaptively based on the performance of the development set.   Different from previous work, we re-arrange the curriculum once the model is updated, so that the training data with appropriate difficulty is learned by the current model effectively.   Experimental results show that our method outperforms the strong baselines and several curriculum learning-based counterparts on several low-resource translation tasks.    DCL only modifies the training strategy without any external data, which has great practical significance for real low-resource scenarios.    With a strong baseline model, we can improve the effectiveness of other semi-supervised methods, such as generating the high quality back-translation data.    In the future, we will rethinking the existing training startegies and explore the application of DCL methods to more difficult tasks, such as unsupervised learning and model training with pseudo data.  
","      Large amounts of data has made neural machine translation  a big success in recent years.    But it is still a challenge if we train these models on small-scale corpora.   In this case, the way of using data appears to be more important.    Here, we investigate the effective use of training data for low-resource NMT.   In particular, we propose a dynamic curriculum learning  method to reorder training samples in training.   Unlike previous work, we do not use a static scoring function for reordering.   Instead, the order of training samples is dynamically determined in two ways - loss decline and model competence.   This eases training by highlighting easy samples that the current model has enough competence to learn.    We test our DCL method in a Transformer-based system.   Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT'16 En-De.",349
" The World Health Organization  estimates that typical retinal diseases such as Age-related Macular Degeneration  and Diabetic Retinopathy  are expected to affect over 500 million people worldwide shortly . Besides, generally speaking, the traditional process of retinal disease diagnosis and creating a medical report for a patient takes time in practice. The above means that ophthalmologists will become busier and busier.   As we may know, the current state of the art in Artificial Intelligence  involves deep learning research, and we claim deep learning is one of the promising ways to help ophthalmologists and improve the traditional retinal disease treatment procedure. Deep learning based models such as convolutional neural networks  or recurrent neural networks  for computer vision or natural language processing tasks, respectively, have achieved, and, in some cases, even exceeded human-level performance. There is no better time than now to propose an AI-based medical diagnosis method to aid ophthalmologists.    In this paper, we propose an AI-based method for automatic medical report generation based on an input retinal image, as illustrated in Figure . The proposed method intends to improve the traditional retinal disease diagnosis procedure, referring to Figure , and help ophthalmologists increase diagnosis efficiency and accuracy. The main idea of this method is to exploit the deep learning based models, including an effective retinal disease identifier  and an effective clinical description generator , to automate part of the traditional treatment procedure. Then, the proposed method will make the diagnosis more efficient.   To train our deep learning models and validate the effectiveness of our RDI and CDG, we introduce a new large-scale retinal disease image dataset, called DeepEyeNet . Besides, as ground truth, we provide a retinal image dataset manually labeled by ophthalmologists to qualitatively show that the proposed AI-based model is effective. The dataset helps us show the activation maps of our deep models are aligned with image features that are clinically recognized by ophthalmologists as linked with the identified disease. Our experimental results show that the proposed AI-based method is effective and successfully improves the traditional retinal disease treatment procedure. Our main contributions are summarized as follows:                       To sum up, we propose an AI-based method to automatically generate medical reports for retinal images to improve the traditional retinal diseases treatment procedure. The proposed method is composed of a DNN-based module, including RDI and CDG sub-modules, and DNN visual explanation module. To train our deep models and validate the effectiveness of our RDI and CDG, we propose a large-scale retinal disease image dataset, DEN. Also, we provide another retinal image dataset manually labeled by ophthalmologists to qualitatively evaluate the proposed method. Our experimental results show the proposed AI-based method is effective and successfully improves the conventional treatment procedure of retinal diseases.  
","     In this work, we propose an AI-based method that intends to improve the conventional retinal disease treatment procedure and help ophthalmologists increase diagnosis efficiency and accuracy. The proposed method is composed of a deep neural networks-based  module, including a retinal disease identifier and clinical description generator, and a DNN visual explanation module.     To train and validate the effectiveness of our DNN-based module, we propose a large-scale retinal disease image dataset. Also, as ground truth, we provide a retinal image dataset manually labeled by ophthalmologists to qualitatively show, the proposed AI-based method is effective. With our experimental results, we show that the proposed method is quantitatively and qualitatively effective. Our method is capable of creating meaningful retinal image descriptions and visual explanations that are clinically relevant.      {DeepOpht Github.}",350
"    One of the fundamental problems in Natural Language Processing  is learning a distributed encoding of sentences, as this is the stepping stone for many NLP tasks, such as sentence classification, sentiment analysis and natural language inference. The multitude of approaches addressing this problem can be categorised according to how a sentence is represented.  %In bag-of-words models, sentences are represented as words multisets and their encodings are generated by averaging word representations  The simpler sentence representation is bag-of-words, which depicts sentences as words multisets ignoring the word order. Despite the simple representation, it has been used to obtain meaningful sentence encodings .   Sequence representation overcomes this limitation considering the sentence as an ordered sequence of words. It allows building models which progressively constructs a sentence encoding, processing one word at the time. Recurrent Neural Network  and Long-Short Term Memory   are probably the most famous models which use this representation.% to produce sentence embeddings.  %This representation reflects how we read text, word after word. %One of the major drawbacks of bag-of-words models is that they are insensitive to word order. Sequence-based models overcome this limitation considering a sentence as a sequence of words .  A key aspect of sentences, which is missing in sequential processing, is compositionality. For example, the sentence """" is obtained by composing the two sub-phrases  """" and """" with the conjunction """". The intrinsic compositionality of sentences makes them suitable for a tree representation, where the whole sentence  is built in terms of sub-phrases  which in turn are defined in terms of smaller constituents; the base cases are words  since they are the atomic piece of information. This representation takes the name of . In Fig.\  we show the constituency tree of the sentence """": the leaves are the words while internal nodes represent syntactic categories which are the constituents of the whole sentence.  There are many models which compute a sentence encoding starting from its constituency tree. For our purposes, we restrict the discussion on bottom-up Recursive Neural Networks  . The parsing direction is constrained by the structure of constituency trees, having information  on leaf nodes. In this domain, we refer to the term  to indicate the state-transition function which computes the   of a tree node combining the representation of its  . Then, the hidden state of the root  is taken as sentence encoding.  The Matrix-Vector Recurrent Neural Network   and the Recursive Neural Tensor Network   apply the RecNN architecture to binary constituency trees using complex composition functions.  % apply the RecNN architecture to binary constituency trees. Moreover, they propose two new architectures which leverage more complex composition functions: the MV-RNN and the RNTN. In the MV-RNN , every word and sub-phrase is encoded as both a vector and a matrix. When two constituents are combined the matrix of one is multiplied with the vector of the other and vice versa, obtaining a composition function which is parameterised by constituents that participate in it. MV-RNN requires a huge number of parameters, since a composition matrix is attached to each word. RNTN  solves this limitation defining a tensor composition function. The tensor allows to obtain composition function parameters directly from the constituent that participate in it.   extends the well known Long-Short Term Memory  architecture to tree-structured data. They propose two different Tree-LSTMs : the -ary Tree-LSTM defines a composition function which considers constituent order while the child-sum Tree-LSTM ignores such an order. However, only the former model is applied to binary constituency trees. The latter is applied to dependency trees, which are another kind of tree representation for sentences, out of our scope.  In recent years, Tree-LSTM has been used as a building block to develop more sophisticated models. For example, , , ,   build new Tree-LSTM models which define dynamic composition functions depending on syntactic categories . Instead,  introduces a Bidirectional Tree-LSTM which takes advantage of both parsing directions: bottom-up and top-down. As we stated before, constituency trees are intrinsically bottom-up; to this end, the author introduces a first bottom-up pass, called , to propagate information from leaves to the root. All these models are applied only to binary constituency trees.    Thus far, we have shown that most of the models compute sentences encodings starting from binary constituency trees. This simplification solves one crucial problem of tree-structured data: the variable number of child nodes. However, the price to pay is the loss of structural information. For example, in Fig.\  and Fig.\  we report the constituency and the binary constituency tree of the sentence """". Comparing the two representation, we can observe that binary tree has one more node that breaks the ternary relation in the non-binary tree; in general, to break a node with  child nodes, we need to add  new nodes. All these new nodes create a chain which moves away the child nodes of the n-ary relation from their parent. The composition of them is obtained by considering one child at a time, as it happens in sequence representation. Hence, the binarisation removes the equality among child nodes, with the risk of weakening contribution of child nodes that are moved far away from their parent and strengthening the contribution of the ones that remain close.  As far as we know, the only work which builds a model suitable for non-binary constituency trees is the TreeNet . The idea is to consider all child nodes in a chain: the hidden state of a node depends on the hidden state of its left sibling and its rightmost child. Even if the model itself works with non-binary trees, the composition function expressed is binary since it always composes two elements. We discuss this observation in details in Sec. .  The definition of models for non-binary constituency trees requires to go beyond the standard definition of composition function. Standard RecNNs define learnable composition functions which are based on the summation of the contribution of each constituent.  proposed a generalisation of such sum-based composition functions leveraging more expressive multi-affine maps represented as tensors. The exponential number of parameters with respect to the tree out-degree  required by the full-tensorial approach can be controlled by applying tensor decomposition. The tensorial models outperform sum-based models, especially when the tree out-degree increases .  Within the scope of this paper, we unveil that non-binary constituency trees can be effectively exploited to improve predictive performance in NLP task, showing that more powerful composition functions are necessary to take advantages of such a rich representation. To this end, we introduce two new Tree-LSTM models which leverage canonical tensor decomposition: the former is suitable for binarised constituency trees, while the latter can process general non-binary constituency trees imposing weight sharing on the tensor decomposition factors. Finally, we test the quality of sentence encodings produced by our models on different NLP tasks, showing that the combination of a rich representation and a powerful composition function is able to outperform baseline models using the same number of parameters.    In this paper, we show that using non-binary constituency trees can be beneficial, especially in semantic similarity tasks. Moreover, we highlight the need of powerful composition function to exploit such a rich representation. To this end, we have introduced a new Tree-LSTM model which leverages tensor canonical decomposition and weight sharing to process non-binary trees without adding new parameters.  Such results pave the way to the definition of new tensor models which leverage suitable tensor decomposition to take advantage of non-binary constituency trees. To this end, the next step would be the application of other tensor decompositions. Among the others, the tensor train decomposition seems to be promising to define new composition functions which are sensitive to child nodes order.  Ultimately, we would like to test multiple tensor-based models on different NLP tasks, studying the relation between the bias introduced by each different tensor decomposition and the intrinsic property of the task.  
"," Processing sentence constituency trees in binarised form is a common and popular approach in literature. However, constituency trees are non-binary by nature. The binarisation procedure changes deeply the structure, furthering constituents that instead are close. In this work, we introduce a new approach to deal with non-binary constituency trees which leverages tensor-based models. In particular, we show how a powerful composition function based on the canonical tensor decomposition can exploit such a rich structure. A key point of our approach is the weight sharing constraint imposed on the factor matrices, which allows limiting the number of model parameters. Finally, we introduce a Tree-LSTM model which takes advantage of this composition function and we experimentally assess its performance on different NLP tasks.",351
" Searching for code fragments is a very common activity in software development. The advent of large code repositories like GitHub\footnote{https://github.com/} and StackOverflow\footnote{https://stackoverflow.com/} has only increased the number of developers to rely on these repositories to search and reuse existing code . Traditional Information Retrieval techniques  do not work well for code search and retrieval tasks due to limited shared vocabulary between the source code and the natural language search text . Often, developers who are new to a programming language, search for code snippets in a context-free natural language. The choice of words used to search may not overlap with the code snippets leading to failure of traditional information retrieval systems. Therefore, there is a need to gain a deeper understanding of code and text in order to find semantically relevant code snippet.  Consider an example where a developer has a functional requirement to validate if age is always lesser than  and alert otherwise. The developer is tasked to enforce this check in Java. A naive Java developer who is not familiar with the language might make a query based on the requirement as: java check condition correctness. The top 10 results\footnote{As of December 9, 2019} in StackOverflow do not discuss the assert keyword. A more programming friendly query such as java boolean check or the assert keyword itself results in code snippets demonstrating the steps as the top result in StackOverflow.  Use of deep neural network models have shown tremendous improvements in many tasks across domains including language tasks . This success can be largely attributed, in part, to their ability to learn meaningful relationships among words in documents efficiently and represent them in a way such that semantically equivalent words tend to have similar representations . One such family of models that are popular for determining text similarity are Siamese networks. First introduced by , a typical Siamese network consists of two identical sub networks that share weights. They work in tandem on different inputs and the output of both the networks are evaluated by a distance measure that also acts as a scoring function. This has been successfully applied in many similarity tasks in image domain  and recently in text domain as well . Another useful property of these models is their capability to learn from fewer data examples . Since code can be treated as a special kind of text data, one possible way to approach the problem of Semantic Code Search  is to treat it as a similarity task where the objective is to bring semantically equivalent code snippets and their natural language descriptions closer. Therefore, we study the application of Siamese networks to code and corresponding text descriptions for semantic code search.  We apply multiple variations of the base Siamese network model on two different datasets for semantic code search and study its efficacy.  We further take the state of the art baselines -  and  on these datasets and observe that Siamese networks can improve over the baseline results invariably . Finally, we present our analysis on the  performance of different Siamese network architectures explored and identify the conditions for improved performance.  The rest of the paper is organized as follows. We introduce some relevant prior art in section . Next, in section , we provide some background on Siamese networks and semantic code search and introduce terminology. In section , we describe our approach and the different architectures investigated. In section , we describe our experiments and present the results. Finally in section , we perform a detailed analysis of our observations, followed by conclusions in section .  % \tikz \draw[]  rectangle  node[pos=.2]{Answer Here:};    In this section, we analyze the results obtained above to understand the behavior of the DCS-Siamese network. We focus on this architecture since it outperforms all other architectures and baseline models considered. Specifically, we would like to analyze three observations:  A. Regularization effect of the DCS-Siamese model over the original DCS architecture  We visualize the embeddings learnt by the DCS network  and the output of the DCS extraction network, after using the Siamese network, for the text descriptions in the StaQC SQL dataset using t-SNE  in Figure . We consider the SQL dataset for visualization since the raw queries and code snippets are not available for the Java dataset.   A quick examination reveals that the embedding space has sharp, distinct clusters for the DCS-Siamese network , whereas the clusters in the original DCS network  are relatively smaller and more scattered. Further, we manually examined some of the clusters and evaluated the questions that are mapped to those clusters. Few samples are listed in Table . For the query groups DATE and JOIN, the clusters are scattered in different regions for the original DCS network. Also, the cluster corresponding to the MAX query group is adjacent to the DATE cluster. Comparatively, for the same query groups, the clusters for the DCS-Siamese network are well separated and coherent. This highlights the role of Siamese network as a regularizer when applied on top of the DCS network. The Siamese network seemingly helps in rearranging the embedding space leading to more meaningful representations, bringing similar inputs  closer in the embedding space. This effect is further reflected in the better retrieval MRR of the DCS-Siamese network.     We observe this result for both the datasets. In our experiments, the difference between the results of  and  variesd but we observe a clear trend in favor of . To understand the superior performance of , we visualize the embeddings learnt by the DCS-Siamese network at the DCS layer for two architectures with  and  respectively as shown in Figure . We consider the SQL dataset for visualization since the raw queries and code snippets are not available for the Java dataset. We use tSNE to plot the DCS embeddings for . A quick examination reveals that the embedding space has sharp, distinct clusters when , whereas the clusters when  are relatively diffused. Further, we manually examined some of the clusters and evaluated the questions that are mapped to those clusters. Some samples are listed in Table . Apart from the fact that the clusters in the right figure are smaller for the four sets of queries we examined, the ones on the left blend in with the other points in the figure, implying that the network has done a poor job at learning to distinguish between different queries at the DCS layer when . Its is unsurprising that we achieve better MRR when Code retrieval is performed at the DCS layer for .    This hints at the possibility that the narrow funnel in the network caused by having  output units at the top of the Siamese network act as a regualarizer that forces the lower layers to learn more meaningful embeddings, which in turn helps the overall task when using those embeddings for retrieval. We did observe 1 exception to this when we evaluated on the Siamese layer output with . However, the difference in performance was only marginal. This visualization, coupled with the results in Table  clearly establishes the value of the  DCS-Siamese network.     We have also observed the inverse of this regularization effect for other values of  and the model performance gradually degrades as the value of  increases.     This also explains why the clusters corresponding to a given set of similar queries are extremely well defined for the embeddings at the DCS layer than the embeddings at the top layer of the network. The restriction to compact information into 2 dimensions has led to a loss of information in the 2-d embedding space for a given set of queries, but this has led the DCS layer to learn a rich set of embeddings.    figure out how to show and pitch the scatter plot of the correct-wrong pairs of points, if at all needed    If this argument indeed holds, we would see a gradual loss of information as we look at the embeddings at the other intermediate layers of the network, upto the final layer. We visualize the embeddings of the layer between the DCS output and the final layer using tSNE for the same set of queries in table X. Indeed, we see that the cluster representing the queries in the embedding space of the intermediate layer is somewhat scattered, but not as much as that of the final layer.       B. For the DCS-Siamese network, retrieval on the output of DCS layer achieves higher MRR  We now focus on the actual embeddings learnt at the different layers of the DCS-Siamese network with .  Figure  shows the embedding plots of the final Siamese layer  and the output of the DCS layer . We focus on two specific set of queries shown in Table . We believe having  output units results in a much stronger regularization effect leading to these two sets of questions being mapped to well-defined regions in the embedding space.    As discussed earlier, the regularization effect of having  output units results in much better embedding at the DCS layer resulting in these  sets of questions being mapped to well-defined regions in the embedding space.  However, due to the low dimensionality at the final layer, there is a tremendous loss of information that deprives the layer of any meaning to its representations. The purpose of the representations is to simply reduce the loss function and guide the gradient forcing the lower layers to learn a much more meaningful embedding. The actual meaning to representations of code and text is hence obtained at a lower layer.  This effect is also consistent for embeddings of code, as observed in Figure , where we generated   a plot of the embeddings of the SQL queries corresponding to the questions in Table . In comparison, to the DCS layer embedding of the DSC-Siamese network , we observed more than one clusters for the sorting questions .      This is due to the fact that any question that involves a deletion would more-or-less be always translated to a 'DELETE FROM' clause in code . However, there could be several questions that might not explicitly ask about sorting, but still require sorting as an intermediate step in the answer. The code corresponding to such answers would have an 'ORDER BY' clause, but depending on the actual question, might involve other SQL clauses.  To summarize,  has a far stronger regularizing effect on the network as compared to larger values. However, due to this effect, there seems to be a loss of information in the final layers of the Siamese network and hence, the embeddings learned by the lower layers of the network contain richer information for the Code retrieval task.  C. The DCS extraction network greatly outperforms the other extraction networks when combined with Siamese networks  We selected the DCS setup as an extraction network because it leveraged a variety of features from code. Although we believe these features are collectively responsible for the impressive performance of the DCS-Siamese model, when considered individually with a Siamese network, they are unable to provide enough information during training, leading to an extremely poor model. This hints at the possibility that providing code as input to a deep learning network may not be straightforward and although the DCS features worked well, there are possibly other features that need to be discovered. Also, some code features might be useful for certain datasets, but not for all.     once you hav explained select and find/get/fetch are far apart, give a solution how to fix that   This result is surprising since [togther we stand paper] has reported impressive results using siamese networks with simple preprocessing and a conv-pooling-relu-FC network. However, when applied to code retrieval, siamese networks with different model architectures and embeddings sizes do not perform as well as other models such as [dcs][coacor] that rely on extracting multiple features from code. We hypothesize that this is due to different vocabularies as well as different mearnings of the same terms in code and text. For instance, the question and answer pairs of [together we stanbd paper] come from the same language , even though the distributions of these terms in questions and answers might be different.    actually, we need to think more on this and come up with more convincing experiemnts and results      Using an ablation study, we identify that API sequence tokens provide the highest performance of all the features used by the DCS model also mention this in training/model details of the simple models.  Siamese networks can achieve impressive performance on Code Retrieval tasks by learning a meaningful embedding of code and it's description text. This performance is heavily reliant on an appropriate representation of code and we have observed that the DCS architecture can achieve this reasonably well. However, while we have some understanding of the regularization provided by the Siamese network, we would like to study this effect in more detail as future work. We would also like to validate our observations on more datasets and other tasks involving code and natural language text such as Code Summarization and Code Synthesis.    File acl2020.tex      Based on the style files for ACL 2020, which were    Based on the style files for ACL 2018, NAACL 2018/19, which were    Based on the style files for ACL-2015, with some improvements     taken from the NAACL-2016 style    Based on the style files for ACL-2014, which were, in turn,    based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,    EACL-2009, IJCNLP-2008...    Based on the style files for EACL 2006 by    e.agirre@ehu.es or Sergi.Balari@uab.es    and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{acl2020} \usepackage{times} \usepackage{latexsym} \usepackage{tikz} \usepackage{multirow} \renewcommand{\UrlFont}{\ttfamily       Enter the acl Paper ID here      You can expand the titlebox if you need extra space   to show all the authors. Please do not make the titlebox   smaller than 5cm ; we will check this   in the camera-ready version and ask you to change it back.    \title{Evaluation of Siamese Networks for Semantic Code Search}     \author{}  \author{Raunak Sinha \\   IBM Research \\   rsinha05@in.ibm.com \\\And   Utkarsh Desai \\   IBM Research \\   udesai26@in.ibm.com \\\And   Srikanth Tamilselvam \\   IBM Research \\   srikanth.tamilselvam@in.ibm.com \\\And   Senthil Mani  \date{}                   
"," % Availability of large code repositories and discussion forums, has enabled code search as a common activity among developers. They tend to express their intent as a query in natural language to find examples of related code. However performance of such systems are restricted due to 1) limited shared vocabulary across code and user query and 2) lack of semantic understanding of the user query.   % In this work, we evaluate Siamese network for the task of code retrieval. Building on two sub network, our siamese model can jointly learn between code and its description and represent them based on their semantic distance. We evaluate the performance of applying siamese networks 1) as a stand-alone model directly feeding code and its description 2) as a model stacked on existing state of the art models. We experiment on 2 datasets and 3 baseline models, and conclude that applying siamese networking on top of base models yield better embedding and improves the performance of the code sesearch taks significantly.  With the increase in the number of open repositories and discussion forums, the use of natural language for semantic code search has become increasingly common. The accuracy of the results returned by such systems, however, can be low due to 1) limited shared vocabulary between code and user query and 2) inadequate semantic understanding of user query and its relation to code syntax. Siamese networks are well suited to learning such joint relations between data, but have not been explored in the context of code search. In this work, we evaluate Siamese networks for this task by exploring multiple extraction network architectures. These networks independently process code and text descriptions before passing them to a Siamese network to learn embeddings in a common space. We experiment on two different datasets and discover that Siamese networks can act as strong regularizers on networks that extract rich information from code and text, which in turn helps achieve impressive performance on code search beating previous baselines on $2$ programming languages. We also analyze the embedding space of these networks and provide directions to fully leverage the power of Siamese networks for semantic code search.",352
"  We are motivated by the problem of labelling a dataset for word sense disambiguation, where we want to use a limited budget to collect annotations for a reasonable number of examples of each sense for each word.  This task can be thought of as an active learning problem , but with two nonstandard challenges. First, for any given word we can get a set of candidate labels from a knowledge base such as WordNet . However, this label set is not necessarily representative of what occurs in the data: there may exist labels in the knowledge base that do not occur in the corpus because the sense is rare in modern English;  conversely, there may also exist true labels that do not exist in our knowledge base. For example, consider the word ``bass.'' It is frequently used as a noun or modifier, e.g., ``the  and alto are good singers'', or ``I play the  guitar''. It is also commonly used to refer to a type of fish, but because music is so widely discussed online, the fish sense of the word is orders of magnitude less common than the low-frequency sound sense in internet text. The Oxford dictionary  also notes that bass  once referred to a fibrous material used in matting or chords, but that sense is not common in modern English. We want a method that collects balanced labels for the common senses, `` frequencies'' and `` fish'', and ignores sufficiently rare senses, such as ``fibrous material''. Second, the empirical distribution of the true labels may exhibit extreme skew: word sense usage is often power-law distributed  with frequent senses occurring orders of magnitudes more often than rare senses.    When considered individually, neither of these constraints is incompatible with existing active learning approaches:  incomplete label sets do not pose a problem for any method that relies on classifier uncertainty for exploration ; and extreme skew in label distributions has been studied under the guided learning framework wherein annotators are asked to explicitly search for examples of rare classes rather than simply label examples presented by the system .  But taken together, these constraints make standard approaches impractical. Search-based ideas from guided learning are far more sample efficient with a skewed label distribution, but they require both a mechanism through which annotators can search for examples and a correct label set because it is undesirable to ask annotators to find examples that do not actually occur in a corpus.    Our approach is as follows. We introduce a frequency threshold, , below which a sense will be deemed to be ``sufficiently rare'' % to be ignored  = p_y < \thresholdp_y_y$ by using importance-weighted samples. Once we have found examples of common classes, we switch to more standard active learning methods to find additional examples to reduce classifier uncertainty.  Overall, this paper makes two key contributions. First, we present an Exemplar Guided Active Learning  algorithm that offers strong empirical performance under extremely skewed label distributions by leveraging exemplar embeddings. Second, we identify a stopping rule that makes EGAL robust to misspecified label sets and prove that this robustness only imposes a logarithmic cost over a hypothetical approach that knows the correct label set.  Beyond these key contributions, we also present a new Reddit word sense disambiguation dataset, which is designed to evaluate active learning methods for highly skewed label distributions.     We present the Exemplar Guided Active Learning algorithm that leverages the embedding spaces of large scale language models to drastically improve active learning algorithms on skewed data. We support the empirical results with theory that shows that the method is robust to mis-specified target classes and give practical guidance on its usage. Beyond word-sense disambiguation, we are now using EGAL to collect multi-word expression data, which shares the extreme skew property.  
"," We consider the problem of wisely using a limited budget to label a small subset of a large unlabeled dataset. We are motivated by the NLP problem of word sense disambiguation. For any word, we have a set of candidate labels from a knowledge base, but the label set is not necessarily representative of what occurs in the data: there may exist labels in the knowledge base that very rarely occur in the corpus because the sense is rare in modern English; and conversely there may exist true labels that do not exist in our knowledge base. Our aim is to obtain a classifier that performs as well as possible on examples of each 闁炽儲绔穙mmon class闁 that occurs with frequency above a given threshold in the unlabeled set while annotating as few examples as possible from 闁炽儲绗re classes闁 whose labels occur with less than this frequency. The challenge is that we are not informed which labels are common and which are rare, and the true label distribution may exhibit extreme skew. We describe an active learning approach that  explicitly searches for rare classes by leveraging the contextual embedding spaces provided by modern language models, and  incorporates a stopping rule that ignores classes once we prove that they occur below our target threshold with high probability. We prove that our algorithm only costs logarithmically more than a hypothetical approach that knows all true label frequencies and show experimentally that incorporating automated search can significantly reduce the number of samples needed to reach target accuracy levels.",353
" Argumentation is a paramount process in society, and debating on socially relevant topics requires high-quality and relevant arguments. In this work, we deal with the problem of , which is also known as . The goal is to develop an  which organizes arguments, previously extracted from various sources  , in an accessible form. Users then formulate a query to access relevant arguments retrieved by the .  The query can be defined as a , e.g. Energy in which case the  retrieves all possible arguments without further specification. Our work deals with a more advanced case, where a query is formulated in the form of a , and the user expects  attacking or supporting this query claim.  An example of a claim related to the topic Energy could be ``We should abandon Nuclear Energy"" and a supporting premise, e.g., ``Accidents caused by Nuclear Energy have longstanding negative impacts"". % A popular search methodology to find relevant premises is a similarity search, where the representations of the retrieved premises are similar to the representation of the  query claim. However, as noted by, the relevance of a premise does not necessarily coincide with pure text similarity.  Therefore, the authors of  advocate to utilize the similarity between the query claim and other claims in an  database and retrieve the premises assigned to the most similar claims. However, such  requires ground truth information about the premise to claim assignments and therefore has limited applicability: Either the information sources are restricted to those sources where such information is already available or can automatically be inferred, or expensive human annotations are required. To mitigate this problem and keep the original system's advantages, we propose to use a machine learning model  the relevance between premises and claims. Using this model, we can omit the  claim-claim matching step and evaluate the importance of  candidate premises directly for the query claim. Since the relevance is defined on the semantic level, we have to design an appropriate training task to enable the model to learn semantic differences between relevant and non-relevant premises. Furthermore, an essential subtask for an  is to ensure that the retrieved premises do not repeat the same ideas.  Previous approaches employ clustering to eliminate duplicates.  However, clustering approaches often group data instances by other criteria than expected by the users, as also observed in \gls{argument-mining} applications.  For our method, we propose an alternative to clustering based on the idea of , where the goal is to cover the space of relevant premises as well as possible. % This is samplepaper.tex, a sample chapter demonstrating the % LLNCS macro package for Springer Computer Science proceedings; % Version 2.20 of 2017/10/04 % \documentclass[runningheads]{llncs} % \usepackage{graphicx} \usepackage{xcolor} \usepackage{amsmath} \usepackage{amssymb} %\usepackage{ulem} \usepackage{multirow} \usepackage{booktabs} \usepackage{footnote} \makesavenoteenv{tabular} \makesavenoteenv{table} \usepackage{cite} \usepackage[ruled,vlined]{algorithm2e} \usepackage{float}  \renewcommand\UrlFont{\rmfamily}  % for equal contribution \makeatletter [1]{%   \textsuperscript{\@fnsymbol{#1}}% } \makeatother  % \title{Diversity Aware Relevance Learning for Argument Search} % %\titlerunning{Abbreviated paper title} % If the paper title is too long for the running head, you can set % an abbreviated paper title here %  \author{ Michael Fromm\thanks{equal contribution} %\orcidID{0000-0002-7244-4191} \and Max Berrendorf\printfnsymbol{1}  %\orcidID{0000-0001-9724-4009} \and Sandra Obermeier   \and Thomas Seidl  %\orcidID{0000-0002-4861-1412} \and Evgeniy Faerman  }   \authorrunning{Fromm et al.} % First names are abbreviated in the running head.  % If there are more than two authors, 'et al.' is used.   }   [1]{\textcolor{red}{#1}}  % Acronyms \usepackage[acronym]{glossaries} %\makeglossaries  % Example % {ACR}{Acronym for Clustering Representations} %  -> ACR %  -> Acronym for Clustering Representations %  -> Acronym for Clustering Representations  {AM}{Argument Mining} {ARS}{Argument Retrieval System} {BERT}{BERT} {CLAIM-SIM}{CLAIM-SIM} {relevance-model}{relevance model}  % methods {first512}{Dumani first512} {sentences}{Dumani sentences} {sliding}{Dumani sliding} {BERT Zero-Shot}{BERT Zero-Shot} {Learned Similarity}{Learned Similarity} {Biased Coreset}{Biased Coreset}  {BERT Zero-Shot + Cluster}{}   \DeclareMathOperator*{\argmax}{argmax}  %{relevance model }  % {Dumani first512 } % {Dumani sentences }  % {Dumani sliding }  % {Premise Similarity } % {Clustered Premise Similarity } % {Premise Importance }  % {Bert-Negatives } % {Simple-Negatives } % {Same-Topic-Negatives }  % disable hyperref for glossaries  \glsdisablehyper                   % typeset the header of the contribution   In this work, we focus on retrieving relevant arguments for a query claim covering diverse aspects. State-of-the-art methods rely on explicit mappings between claims and premises and thus cannot utilize extensive available collections of premises without laborious and costly manual annotation. Their diversity approach relies on removing duplicates via clustering, which does not directly ensure that the selected premises cover all aspects. This work introduces a new multi-step approach for the argument retrieval problem. Rather than relying on ground-truth assignments, our approach employs a machine learning model to capture semantic relationships between arguments. Beyond that, it aims to cover diverse facets of the query instead of explicitly identifying duplicates.  Our empirical evaluation demonstrates that our approach leads to a significant improvement in the argument retrieval task, even though it requires fewer data than prior methods. Our code is available at \url{https://github.com/fromm-m/ecir2021-am-search}.           In this work, we have presented a novel approach for the retrieval of  and  premises for the query claims. Our new approach can be applied more flexibly than previous methods since it does not require mappings between premises and claims in the database.  Thus, it can also be applied in an inductive setting, where new premises can be used without the need first to associate them with relevant claims manually. At the same time, it achieves better results than approaches that make use of this information.  
"," In this work, we focus on retrieving relevant arguments for a query claim covering diverse aspects. State-of-the-art methods rely on explicit mappings between claims and premises and thus cannot utilize extensive available collections of premises without laborious and costly manual annotation. Their diversity approach relies on removing duplicates via clustering, which does not directly ensure that the selected premises cover all aspects. This work introduces a new multi-step approach for the argument retrieval problem. Rather than relying on ground-truth assignments, our approach employs a machine learning model to capture semantic relationships between arguments. Beyond that, it aims to cover diverse facets of the query instead of explicitly identifying duplicates.  Our empirical evaluation demonstrates that our approach leads to a significant improvement in the argument retrieval task, even though it requires fewer data than prior methods. Our code is available at \url{https://github.com/fromm-m/ecir2021-am-search}.",354
"  Speaker diarization is the process of partitioning an audio stream into homogeneous segments according to speaker identities. Thus, diarization determines ``who spoke when'' in a multi-speaker environment, with a variety of applications to conversations involving multiple speakers, such as meetings, television shows, medical consultations, or call center conversations. In particular, the speaker boundaries produced by a diarization system can be used to map transcripts generated by a multi-speaker automatic speech recognition  system into speaker-attributed transcripts . Moreover, speaker embeddings inferred by diarization can help the ASR system adapt to, or focus on the speech of a targeted speaker .   Conventional speaker diarization systems are based on clustering of speaker embeddings. In this approach, several components are integrated into a single system: speech segments are determined by voice activity detection ; these speech segments are further divided into smaller chunks of fixed size; speaker embeddings are then extracted by speaker embedding extractors for each chunk; finally, those speaker embeddings are clustered to map each segment to a speaker identity . For embeddings, i-vectors , x-vectors , or d-vectors  are commonly used. Clustering methods typically used for speaker diarization are agglomerative hierarchical clustering  , k-means clustering , and spectral clustering . Recently, neural network-based clustering has been explored . Clustering-based speaker diarization achieves good performance but has several shortcomings. First, it relies on multiple modules  that are trained separately. Therefore, clustering-based systems require careful joint calibration in the building process. Second, systems are not jointly optimized to minimize diarization errors; clustering in particular is an unsupervised process. Finally, clustering does not accommodate overlapping speech naturally, even though recent work has proposed ways to handle regions with simultaneously active speakers in clustering .  End-to-end neural diarization  with self-attention  is one of the approaches that aim to model the joint speech activity of multiple speakers. It integrates voice activity and overlap detection with speaker tracking in end-to-end fashion.  Moreover, it directly minimizes diarization errors and has demonstrated excellent diarization accuracy on two-speaker telephone conversations. However, EEND as originally formulated is limited to a fixed number of speakers because the output dimension of the neural network needs to be prespecified. Several methods have been proposed recently to overcome the limitations of EEND. One approach uses a speaker-wise chain rule to decode a speaker-specific speech activity iteratively conditioned on previously estimated speech activities . Another approach proposes an encoder/decoder-based attractor calculation . The embeddings of multiple speakers are accumulated over the time course of the audio input, and then disentangled one-by-one, for speaker identity assignment by speech frame.  However, all these state-of-the-art EEND methods only work in an offline manner, which means that the complete recording must be available before diarization output is generated. This makes their application impractical for settings where potentially long multi-speaker recordings need to be processed incrementally .   In this study, we propose a novel method to perform EEND in a blockwise online fashion so that speaker identities are tracked with low latency soon after new audio arrives, without much degradation in accuracy compared to the offline system. We utilize the incremental Transformer encoder, where we attend to only its left contexts and ignore its right contexts, thus enabling blockwise online processing. Furthermore, the incremental Transformer encoder uses block-level recurrence in the hidden states to carry over information block by block, reducing computation time while attending to previous blocks. To our knowledge, ours is the first method that uses the incremental Transformer encoder with block-level recurrence to enable online speaker diarization.      We implemented two versions of a blockwise online variant of EDA-EEND, BW-EDA-EEND-UL  and BW-EDA-EEND-LL . Blockwise online processing is enabled by utilizing an incremental Transformer encoder that attends to only its left contexts and ignores its right contexts and uses block-level recurrence in the hidden states to carry over information between blocks, which makes algorithm complexity linear in time. BW-EDA-EEND-UL shows only moderate degradation of accuracy for up to two speakers using either unlimited or 10-second context, compared to offline EDA-EEND. BW-EDA-EEND-LL has accuracy comparable to an offline clustering-based system when frame-level embeddings are shuffled across blocks. Future algorithmic improvements should address the consistency of attractor direction and ordering over time, as blocks are processed incrementally.  We observe that multi-speaker training data as simulated in prior work   is not very realistic when compared to real conversational data, in terms of turn-taking behavior , and suspect that this may limit the effectiveness of model training. In future work, we plan to modify the simulation algorithm to create more realistic meeting mixtures by adopting the recently proposed method that was used to create LibriCSS test data . We also believe that test data for EEND needs to become more realistic, moving from mixtures of telephone speech channels  to far-field recordings of multiple speakers speaking and interacting in the same room.    
"," We present a novel online end-to-end neural diarization system, BW-EDA-EEND, that processes data incrementally for a variable number of speakers. The system is based on the Encoder-Decoder-Attractor  architecture of Horiguchi et al., but utilizes the incremental Transformer encoder, attending only to its left contexts and using block-level recurrence in the hidden states to carry information from block to block, making the algorithm complexity linear in time. We propose two variants: For unlimited-latency BW-EDA-EEND, which processes inputs in linear time, we show only moderate degradation for up to two speakers using a context size of 10 seconds compared to offline EDA-EEND. With more than two speakers, the accuracy gap between online and offline grows, but the algorithm still outperforms a baseline offline clustering diarization system for one to four speakers with unlimited context size, and shows comparable accuracy with context size of 10 seconds. For limited-latency BW-EDA-EEND, which produces diarization outputs block-by-block as audio arrives, we show accuracy comparable to the offline clustering-based system.",355
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. %  % form to use if the first word consists of a single letter: % {A}{demo} file is .... %  % form to use if you need the single drop letter followed by % normal text : % {A}{}demo file is .... %  % Some journals put the first two words in caps: % {T}{his demo} file is .... %  % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word.  {T}{here} are many methods for automatic speech recognition  systems, such as GMM-HMM and deep neural network  based acoustic models . Recently, end-to-end speech recognition methods  have made significantly breakthroughs. Although these ASR methods have made a lot of progresses on clean speech signals, the performance could be dramatically degraded in the noisy and reverberation environments. In realistic environments, recorded speech signals are always interfered by various background noises and reverberations. Therefore, improving the robustness of ASR is very important. This paper focuses on boosting the noise robustness of end-to-end speech recognition.  %In realistic environments, recorded speech signals are always interfered by various background noises and reverberations. However, these interferences can dramatically degrade the performance of automatic speech recognition  systems.  In order to boost the noise robustness of ASR, there are three mainstream methods. The first mainstream method is adding the speech enhancement component at the front-end of ASR. Speech enhancement methods include spectral subtraction , Wiener filtering  and deep neural network  based speech enhancement methods . However, speech enhancement optimizes their models to estimate the target speech, which is different from the speech recognition part. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, the enhanced speech by these speech enhancement methods usually generates over-smoothed speech, which is the reason of speech distortion after speech enhancement. The speech distortion can degrade the performance of ASR . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . %In addition, speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end .  The second mainstream method uses the multi-condition training  to boost the noise robustness of ASR. MCT uses different kinds of data  to train the speech recognition model. However, the complexity and computing costs of MCT are increased. In addition, it gives unimpressive performance on the unmatched conditions  and the performance is also affected by the speech distortion . In order to alleviate speech distortion problem, the enhancement front-end enhances both training and test set first, and ASR model is trained with the enhanced data. It can improve the ASR performance in some degree, but it still highly depends on the performance of the enhancement front-end. Different from the MCT method, the SpecAugment  directly applies the  data augmentation to the input features of neural networks . The SpecAugment is used only during the training, which consists of three spectrogram deformations:  time warping,  time and frequency masking. Although the SpecAugment can improve the performance of end-to-end ASR, it needs to be improved on the noisy condition.  %%MCT not only uses the clean data for training but also the noisy data. Therefore, MCT can learn different distributions from the clean and noisy data so that the speech recognition model boosts the noise robustness. However, the complexity and computing costs of MCT are increased. In addition, it gives unimpressive performance in unmatched conditions  and the performance is also affected by the speech distortion . In order to alleviate speech distortion problem, the enhancement front-end enhances both training and test set first, and ASR model is trained on the enhanced training set. It can improve the ASR performance in some degree, but it still highly dependent on the performance of the enhancement front-end. Different from the MCT method, the SpecAugment  directly applies the  data augmentation to the input features of neural networks . The SpecAugment is used only during the training, which consists of three spectrogram deformations:  time warping,  time and frequency masking. Although the SpecAugment can improve the performance of end-to-end ASR, it is still affected by the speech distortion problem.  %Then it applies the enhanced data for test. Although this method can boost the robustness of ASR in some degree, the complexity and computing costs are increased. In addition, the performance of MCT is also affected by the speech distortion .  %In order to boost the noise robustness of ASR, the mainstream method is adding the speech enhancement component at the front-end of ASR. However, speech enhancement aims to estimate the target speech , which is different from the speech recognition. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, speech enhancement usually leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . In order to alleviate this issue, the multi-condition training   method is proposed. MCT not only uses the clean data for training but also the noisy and enhanced data. Although this method can boost the robustness of ASR in some degree, the complexity and computing costs are increased. In addition, the performance of MCT is also affected by the speech distortion.  %speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end .  %In order to boost the noise robustness of ASR, the mainstream method is adding the speech enhancement component at the front-end of ASR. Speech enhancement methods include spectral subtraction , Wiener filtering  and deep neural network  based speech enhancement methods . However, speech enhancement part optimizes their models to estimate the target speech, which is different from the speech recognition part. Therefore, speech enhancement methods fail to optimize towards the final objective, which leads to a suboptimal solution . In addition, speech enhancement methods usually tend to generate over-smoothed speech because they use a particular form of loss functions such as mean squared error , which leads to speech distortion. However, the performance of ASR can be degraded by the speech distortion . Therefore, the performance of this approach is highly dependent on the performance of the enhancement front-end . %speech enhancement methods usually lead to speech distortion. %However, applying these speech enhancement methods has two shortcomings. Firstly,  %Firstly, these speech enhancement methods will increase the computation and has more complex pipelines, which limit the applications of speech recognition. Secondly,   The third mainstream method is the joint training methods . These methods apply the joint training framework to optimize the speech enhancement and recognition, simultaneously. The reason is that speech enhancement and speech recognition are not two independent tasks and they can clearly benefit from each other. In order to boost noise robustness of end-to-end ASR, in , authors propose a joint adversarial enhancement training method. They utilize the joint training framework to optimize the mask based enhancement network and attention based encoder-decoder speech recognition network. However, this method only uses the enhanced feature as the input of speech recognition, which is still affected by the speech distortion problem. In addition, in the noisy AISHELL-1  dataset, the character error rate  of this method is still more than 50\%, which needs to be improved. As for the end-to-end speech recognition, speech transformer  models have shown impressive performance and acquired state-of-the-art results. Self-attention network  is one of the key components of speech transformer and it is more powerful to model long-term dependencies than recurrent neural networks  based sequence to sequence models. Therefore, applying the joint training of enhancement and speech transformer can further improve the performance of robust end-to-end ASR. %One of the key components of speech transformer is self-attention network , which is more powerful to model long-term dependencies than Recurrent neural networks -based sequence to sequence models. Therefore, the performance of robust end-to-end ASR can be further improved by using the joint training of enhancement and speech transformer. %To address the speech distortion problem and acquire an optimal performance, the joint training method of speech enhancement and speech recognition is proposed for robust ASR . This is because that speech enhancement and speech recognition are not two independent tasks and they can clearly benefit from each other. In , a joint adversarial enhancement training method is proposed to boost noise robustness of end-to-end ASR systems. It applies the joint training of mask based-enhancement network and attention-based encoder-decoder speech recognition network. However, this method only uses the enhanced features as the input of speech recognition, which is still affected by the speech distortion problem. And the character error rate of this method is still more than 50\% in the noisy AISHELL-1  dataset. Speech transformer models have shown impressive performance in end-to-end speech recognition  and acquire state-of-the-art performances. One of the key components of speech transformer is self-attention network , which is more powerful to model long-term dependencies than Recurrent neural networks -based sequence to sequence models. Therefore, the performance of robust end-to-end ASR can be further improved by using the joint training of enhancement and speech transformer. %Liu et al propose a joint adversarial enhancement training to boost noise robustness of end-to-end ASR systems . They use the joint training of mask based-enhancement network and attention-based encoder-decoder speech recognition network   %In this paper, we propose a joint training method of enhancement and speech transformer for robust end-to-end ASR, which uses the deep attention fusion representations of noisy and enhanced features. To be our best knowledge, it is the first time to apply the speech transformer and enhancement joint training for robust end-to-end ASR. Specifically, the proposed joint training method includes two parts:   In , a one-pass robust speech recognition method is proposed. It combines the noisy and enhanced features by a gating mechanism. Although it can improve the robust of ASR, the enhancement and speech recognition are trained separately instead of the joint training algorithm. In addition, the simple gate mechanism can not make full use of the sequence information so that it can not fuse the noisy and enhanced features very well. %The speech enhancement and speech recognition are   Fig. illustrates the spectrogram example of a test speech sample. From Fig. we can find that the spectrogram of the enhanced speech by the enhancement network has significant leaks  by block boxes), which leads to the speech distortion. There are significant leaks in these black boxes. This is because that the noise is dominant in these T-F bins, which drowns the target speech. Therefore, the enhancement network deals with these T-F bins as the noise signals and removes most of the information. These leaks lose so much very important speech information, for example: formants. Although the enhancement network can remove noise signals in some degree, these leaks are unknown for the speech recognition system and lose so much speech information. These are the reasons why speech distortion damages the performance of speech recognition.   In this paper, we propose a gated recurrent fusion  with joint training framework for robust end-to-end ASR. In order to address the speech distortion problem, motivated by , the GRF is utilized to dynamically combine the noisy and enhanced features. Therefore, the GRF can offset these leaks from the noisy features. In addition, GRF can reduce the noise from the enhanced features. So the GRF aims to learn to adaptively select and fuse the relevant information from noisy and enhanced features by making full use of the gate and memory modules. The GRF can extract more appropriate and robust speech features. In addition, we apply the joint training algorithm to optimize the enhancement and speech recognition. The state-of-the-art end-to-end ASR method speech transformer with self-attention method is used as the speech recognition component. Specifically, the proposed joint training method includes three parts: speech enhancement, gated recurrent fusion and speech recognition. With the joint optimization of enhancement and recognition, the proposed model is expected to learn more robust representations suitable for the recognition task automatically.  %In this paper, we propose a joint training method of enhancement and speech transformer for robust end-to-end ASR, which uses the deep attention fusion  representations of noisy and enhanced features. We apply the state-of-the-art end-to-end ASR method speech transformer with self-attention as the speech recognition component. In addition, to further alleviate speech distortion problem, the deep attention fusion component is utilized to combine the noisy and enhanced features, which can dynamically fuse these features in a deep way so that can extract more appropriate and robust speech features. Therefore, these GRF representations can learn the raw fine structures from the noisy features to make up the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features. Specifically, the proposed joint training method includes three parts: speech enhancement, deep attention fusion and speech recognition. With the joint optimization of enhancement and recognition, the proposed model is expected to learn more robust representations suitable for the recognition task automatically.  %dynamically select appropriate speech features. As for the enhancement component, we apply the mask-based enhancement network to estimate the clean speech. As for the speech recognition component, the speech transformer with self-attention is used for ASR.   To summarize, the main contribution of this paper is two-fold. Firstly, to address the speech distortion problem, the gated recurrent fusion algorithm is utilized to dynamically fuse the noisy and enhanced features. Secondly, to the best of our knowledge, it is the first time to apply the speech transformer and single channel speech enhancement for the joint training framework. Our experiments are conducted on AISHELL-1 Mandarin dataset. Experimental results show that the proposed method achieves the relative CER reduction of 10.02\% over the conventional joint enhancement and transformer method using the enhanced features only. Especially for the low signal-to-noise ratios, our proposed method can achieve better performance.  %The rest of this paper is organized as follows. Section 2 presents the conventional joint training method for robust ASR. Our proposed method is stated in section 3. Section 4 shows detailed experiments and results. Section 5 draws conclusions. The rest of this paper is organized as follows. Section \ presents the conventional joint training method for robust ASR. Section \ introduces our proposed joint training method with gated recurrent fusion algorithm. The experimental setup is stated in section \. Section \ shows experimental results. Section \ shows the discussions. Section \ draws conclusions.     %The rest of this paper is organized as follows. Section \ presents discriminative learning for monaural speech separation using deep embedding features. Section \ introduces the proposed end-to-end post-filter speech separation method. The experimental setup is stated in section \. Section \ shows experimental results. Section \ shows the discussions. Section \ draws conclusions.       In this paper, we propose a joint enhancement and speech transformer training method with gated recurrent fusion for robust end-to-end speech recognition. The joint training compositional scheme is used to simultaneously optimize the enhancement and speech recognition. In addition, in order to address the speech distortion problem and extract more robust features for end-to-end ASR, we apply the gated recurrent fusion algorithm to combine the noisy and enhanced features. Experiments on Mandarin AISHELL-1 demonstrate that our proposed method is effective for the robust end-to-end ASR and can solve the speech distortion problem very well. In future, we will explore the time domain speech enhancement to acquire a better enhanced speech and obtain greater performance improvement for our proposed method.  In this paper, we propose a jointly traning of enhancement and speech transformer to imporove robustness of end-to-end systems. We use a jointly compositional scheme of enhancement and recognition. In addition, in order to alleviate the speech distortion problem and extract more robust features for ASR, we propose the deep attention fusion algorithm to combine the noisy and enhanced features.  Experiments on AISHELL-1 demonstrate effectiveness of our proposed method. In future, we will explore the time domain speech enhancement to acquire a better enhanced speech and obtain greater performance improvement for our proposed method.    
"," %The joint training of speech enhancement and speech recognition methods have acquired good performances for robust end-to-end automatic speech recognition . However, they only use the enhanced features as the input of speech recognition component, which is still affected by the speech distortion problem. In this paper, we propose a deep attention fusion  of noisy and enhanced features with joint enhancement and speech transformer training method for robust end-to-end ASR. We apply the state-of-the-art end-to-end ASR method speech transformer as our speech recognition component. To address the speech distortion problem and extract more robust features for ASR, we propose the deep attention fusion algorithm to combine the noisy and enhanced features deeply. Therefore, these GRF representations can learn the raw fine structures from the noisy features to alleviate the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features. Systematic experiments on AISHELL-1 show that the proposed method achieves the relative character error rate  reduction of 8.32\% over the conventional joint enhancement and transformer method using the enhanced features only. Especially for the low signal-to-noise ratios, our proposed method can achieves better performances. %The joint training framework for speech enhancement and recognition methods have obtained quite good performances for robust end-to-end automatic speech recognition . However, these methods only utilize the enhanced feature as the input of speech recognition component, which are affected by the speech distortion problem. In order to address this problem, in this paper, we propose a gated recurrent fusion  method with joint training framework for robust end-to-end ASR. The proposed method consists of speech enhancement, GRF and speech recognition. Firstly, the mask based speech enhancement network is applied to enhance the input speech. Secondly, to address the speech distortion problem and extract more robust features for end-to-end ASR, the GRF algorithm is used to dynamically combine the noisy and enhanced features. Therefore, the GRF can not only remove the noise signals from the enhanced features, but also learn the raw fine structures from the noisy features so that it can alleviate the speech distortion. Thirdly, to improve the performance of ASR, the state-of-the-art end-to-end speech recognition method speech transformer with self-attention algorithm is used as the speech recognition component. Finally, the joint training framework is utilized to optimize these three components, simultaneously. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% reduction, which suggests the potential of our proposed method. The joint training framework for speech enhancement and recognition methods have obtained quite good performances for robust end-to-end automatic speech recognition . However, these methods only utilize the enhanced feature as the input of the speech recognition component, which are affected by the speech distortion problem. In order to address this problem, this paper proposes a gated recurrent fusion  method with joint training framework for robust end-to-end ASR. The GRF algorithm is used to dynamically combine the noisy and enhanced features. Therefore, the GRF can not only remove the noise signals from the enhanced features, but also learn the raw fine structures from the noisy features so that it can alleviate the speech distortion. The proposed method consists of speech enhancement, GRF and speech recognition. Firstly, the mask based speech enhancement network is applied to enhance the input speech. Secondly, the GRF is applied to address the speech distortion problem. Thirdly, to improve the performance of ASR, the state-of-the-art speech transformer algorithm is used as the speech recognition component. Finally, the joint training framework is utilized to optimize these three components, simultaneously. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% CER reduction, which suggests the potential of our proposed method. %The joint training of speech enhancement and speech recognition methods have acquired good performances for robust end-to-end automatic speech recognition . However, they only use the enhanced features as the input of speech recognition component, which is still affected by the speech distortion problem. In this paper, we propose a gated recurrent fusion  of noisy and enhanced features with joint enhancement and speech transformer training method for robust end-to-end ASR. We apply the state-of-the-art end-to-end ASR method speech transformer with self-attention algorithm as our speech recognition component. To address the speech distortion problem and extract more robust features for end-to-end ASR, we apply the GRF algorithm to dynamically combine the noisy and enhanced features. Therefore, these GRF representations can learn the raw fine structures from the noisy features so that they can make up the speech distortion. Meanwhile, they can also remove the noise signals form the enhanced features to improve the robustness of end-to-end speech recognition. Our experiments are conducted on an open-source Mandarin speech corpus called AISHELL-1. Experimental results show that the proposed method achieves the relative character error rate  reduction of 10.04\% over the conventional joint enhancement and transformer method only using the enhanced features. Especially for the low signal-to-noise ratio , our proposed method can achieves better performances with 12.67\% reduction, which suggests the potential of our proposed method.",356
"  Machine learning systems struggle to learn predictors that are robust to distribution shift. When tested on i.i.d data drawn from the training distribution these systems can achieve nearly perfect accuracy, even when regularized to prevent over-fitting. However, performance can degrade to below-chance accuracy when the testing and training distributions are even slightly different . The field of Domain Generalization   addresses this challenge by proposing robust methods that ensure good test performance on distributions that are different from but systematically related to the training distribution . Invariant Risk Minimization   is a one of several recently successful approaches to Multi-Source Domain Generalization  which encourages models to learn predictors with invariant performance across different ``domains'', or ``environments'' . Given  different training environments, these models extract a set of predictors from the feature space such that the conditional distribution of the outcomes given the predictors is invariant  across all training environments. These predictors can consequently generalize well to all test out-of-distribution  environments which share this same invariance. Building on work in philosophy which characterizes causation as invariance , existing invariance-based DG methods have been interpreted as a weak form of causal discovery whose returned predictors are the causal factors underlying the phenomena we wish to predict.   Fairness can be often characterized by robustness to changes in ``sensitive attributes'' , especially in the context of toxicity classification.      Consider an automated moderation system used by an online news platform to determine which comments on a news article are toxic to online discourse and should be censored. The performance of a fair system should not be affected by characteristics such as whether the comment is about issues related to race, gender or other politically sensitive topics. Alternative definitions of distributive fairness differ in how the system's predictions should be invariant to changes in the sensitive attribute. For instance, statistical definitions such as Demographic Parity require that some conditional distribution of predictions given the sensitive attribute are invariant to the sensitive attribute , and causal definitions such as counterfactual fairness  require that every individual's prediction is invariant to counterfactual changes in that individual's sensitive attribute. There are a number of ethical and legal criticisms to be levied against systems that predict based on sensitive group membership . Moreover, over-reliance on sensitive information could decrease robustness when the predictive performance of this information spuriously depends on the environmental context in which it is employed. Discussion about non-caucasian racial identities, for example, may be highly predictive of comment toxicity on white supremacist internet forums whose members routinely make discriminatory remarks about ethnic minorities. However, on other internet forums that are more welcoming of diversity the association between racial identity mention and toxicity would likely be far weaker. This brittleness of sensitive information has been identified as a key challenge that Perspective API, a Google-backed internet comment toxicity classifier, faced during implementing in real-world contexts , and has also been observed to cause bias in sentiment analysis  and facial detection  tasks. Fair models, then, can perhaps be constructed by learning predictors whose performance remains invariant across a variety of different environments.   In this work, we empirically demonstrate that Domain Generalization can used to build fair machine learning systems by constructing models that are invariant to spurious correlations involving the sensitive attribute. Specifically, we assess  the performance of IRM on a fair internet comment toxicity classification task derived from the Civil Comments Dataset. In this task, the model must generalize from biased training environments exhibiting a strong but spurious correlation between mention of a particular demographic identity and toxicity to a test environment in which this correlation is reversed.    Our contributions are as follows:            In this work, we applied IRM to a toxicity classification task in order to demonstrate that Domain Generalization can serve as an important framework for building fair machine learning classifiers. Our findings show that IRM outperforms ERM with respect to both generalization accuracy and group fairness by learning invariant but likely non-causal predictors of toxicity. We hope that these results are first steps for future explorations of the relationship between robustness and fairness in machine learning.    {\small     }    
","     Robustness is of central importance in machine learning and has given rise to the fields of domain generalization and invariant learning, which are concerned with improving performance on a test distribution distinct from but related to the training distribution. In light of recent work suggesting an intimate connection between fairness and robustness, we investigate whether algorithms from robust ML can be used to improve the fairness of classifiers that are trained on biased data and tested on unbiased data. We apply Invariant Risk Minimization , a domain generalization algorithm that employs a causal discovery inspired method to find robust predictors, to the task of fairly predicting the toxicity of internet comments. We show that IRM achieves better out-of-distribution accuracy and fairness than Empirical Risk Minimization  methods, and analyze both the difficulties that arise when applying IRM in practice and the conditions under which IRM will likely be effective in this scenario. We hope that this work will inspire further studies of how robust machine learning methods relate to algorithmic fairness.",357
"   Deep Neural Networks  are the current state-of-the-art models in many speech related tasks. From a computational neuroscience perspective, DNNs can be seen as rate coding based models, in the sense that if a neuron is responsive to a given stimulus, then if we augment the stimulus intensity, the neuron output intensity will also increase. Temporal coding based models try to also take into account information carried by the temporal structure of the stimulus. In the case of Spiking Neural Networks , spike timing and delays between spikes is important in order to retrieve patterns in the spike sequences given as input to a model. %https://en.wikipedia.org/wiki/Neural_coding  There is a growing interest for SNNs applied to speech recognition tasks, from isolated word and phone recognition,to large-vocabulary automatic speech recognition  very recently. Reasons are that the audio speech signal is particularly suited to event-driven models such as SNNs, SNNs are also more biologically realistic than DNNs, hardware friendly and energy efficient models, if implemented on dedicated energy-efficient neuromorphic chips. Furthermore, it has been shown recently that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. This new approach allows to train SNNs as one would do for DNNs.  In this work, we propose to use supervised SNNs for speech command  recognition. We explore the Leaky Integrate-and-Fire  neuron model for this task, and show that convolutional SNNs can reach an accuracy very close to the one obtained with state-of-the-art DNNs, for this task. Our main contributions are the following: i) we propose to use dilated convolution spiking layers, ii) we define a new regularization term to penalize the averaged number of spikes to keep the spiking neuron activity as sparse as possible, iii) we show that the leaky variant of the neuron model outperforms the non-leaky one , used in.  In order to facilitate reproducibility, our code using PyTorch is available online\footnote{https://github.com/romainzimmer/s2net}.     In this work, we explored the LIF neuron model to define dilated convolution spiking layers for a spoken command recognition application. Contrarily to most works using SNNs applied to speech tasks, in which special mechanisms, usually non-trainable, are needed to first encode the speech input features into some type of neural encoding  as a first step to then use SNNs , our approach is unified in the sense that the first convolution layer applied to real-valued speech features is trainable and shares the same definition and implementation than the ones processing spike trains as input. Our proposed SNN, trained with back-propagation through time with surrogate gradient, achieved results competitive with standard deep convolutional neural networks.     We defined a regularization term to penalize the averaged number of spikes to keep the spiking neuron activity as sparse as possible, which is a desirable property both from a biological point of view and for a future potential implementation on low-energy dedicated chips.  Finally, we conducted ablation studies in order to estimate the impact of different components of our approach. In particular, an interesting result is that the LIF neuron model outperformed the simpler non-leaky one , used in for ASR.   Another experiment showed that learning the values for the thresholds and leak coefficients during training does not bring accuracy improvements over using defaults constant values.  In future work, we will try to confirm these results in acoustic modeling for speech recognition.   We also would like to explore the possibility to design a variant, in which a layer sends its output spikes to the next layer as soon as they are produced, in a single time loop used for the whole model. This would be more efficient in terms of computation load. It would also eventually allow to take classification decisions faster, for audio streaming applications in particular.    Below is an example of how to insert images. Delete the ``\vspace'' line,   uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''   with a suitable PostScript file name.   -------------------------------------------------------------------------        To start a new column  and help balance the last-page   column length use \vfill\pagebreak.   -------------------------------------------------------------------------  \vfill  \pagebreak    
"," Deep Neural Networks  are the current state-of-the-art models in many speech related tasks. There is a growing interest, though, for more biologically realistic, hardware friendly and energy efficient models, named Spiking Neural Networks . Recently, it has been shown that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. In this work, we report speech command  recognition experiments using supervised SNNs. We explored the Leaky-Integrate-Fire  neuron model for this task, and show that a model comprised of stacked dilated convolution spiking layers can reach an error rate very close to standard DNNs on the Google SC v1 dataset: \ER{94.5}\%, while keeping a very sparse spiking activity, below 5\%, thank to a new regularization term. We also show that modeling the leakage of the neuron membrane potential is useful, since the LIF model outperformed its non-leaky model counterpart significantly.",358
" Books have been the one of the most important mediums for recording information and imparting knowledge in human history. Books can be classified into different categories based on their physical formats, contents, languages, and so on. In this paper, we focus on the task of book classification by its genre using the information provided just by the cover. Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Figure  presents some sample book covers. The information provided by a cover includes visual and textual information . For instance, in Figure 1, the background picture contains different food items and cookware which give the readers a visual impression about the book, while the texts shown on the cover states that it is a book about the ``authentic recipes from Malaysia"". Both the visual and textual information are shown in the cover and they together indicate that its genre is ``Cookbooks, Food \& Wine"". It is worth to mention that having only the visual information often makes the task extremely hard without textual information. For instance, in Figure 1 , without reading the texts on the cover, someone may classify the book as ``Cookbooks, Food \& Wine"" as well solely based on the visual information we get from the cover that includes food items on a table in a dining room setting. Therefore, it is sometimes essential to consider both visual information and textual information extracted from the cover when we conduct book genre classification. The automatic classification of books based on only covers without human intervention would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task.     The challenges of this task are the following. First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, varies in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc . To overcome these difficulties, we present a deep learning framework involving two moralities: one for visual information and the other for textual information extracted from the covers.   Recently, deep learning approaches have reached high performances across a wide variety of problems . In particular, some deep convolutional neural networks can achieve a satisfactory level of performance on many visual recognition and categorization tasks, exceeding human performances. One of the most attractive qualities of these techniques is that they can perform well without any external hand-designed resources or task-specific feature engineering.  The theoretical foundations of deep learning are well rooted in the classical neural network  literature. It involves many hidden neurons and layers as an architectural advantage in addition to the input and output layers . A deep convolutional neural network is universal, meaning that it can be used to approximate any continuous function to an arbitrary accuracy when the depth of the neural network is large enough .  The main contributions of this paper are fourfold: []   The rest of the paper is structured as follows. Section 2 presents related works about book cover classification. Section 3 elaborates on the details of the proposed multi-modal architectures. In section 4, we discuss the experimental results. The last section concludes the paper and discusses future work.    In this paper, we proposed two multi-modal models: one with simple concatenation and the other with DCCA concatenation, for the task of book genre classification solely based on its cover. In addition, we evaluated several state-of-the-art image-based models and text-based models. By comparison, text-based models perform better in general than image-based models and the proposed multi-modal model with simple concatenation outperforms all other models. Based on the results from our experiments, the simple concatenation model has a top-1 accuracy of 56.1\
"," Book covers are usually the very first impression to its readers and they often convey important information about the content of the book. Book genre classification based on its cover would be utterly beneficial to many modern retrieval systems, considering that the complete digitization of books is an extremely expensive task. At the same time, it is also an extremely challenging task due to the following reasons: First, there exists a wide variety of book genres, many of which are not concretely defined. Second, book covers, as graphic designs, vary in many different ways such as colors, styles, textual information, etc, even for books of the same genre. Third, book cover designs may vary due to many external factors such as country, culture, target reader populations, etc. With the growing competitiveness in the book industry, the book cover designers and typographers push the cover designs to its limit in the hope of attracting sales. The cover-based book classification systems become a particularly exciting research topic in recent years. In this paper, we propose a multi-modal deep learning framework to solve this problem. The contribution of this paper is four-fold. First, our method adds an extra modality by extracting texts automatically from the book covers. Second, image-based and text-based, state-of-the-art models are evaluated thoroughly for the task of book cover classification. Third, we develop an efficient  and salable multi-modal framework based on the images and texts shown on the covers only.  Fourth, a thorough analysis of the experimental results is given and future works to improve the performance is suggested. The results show that the multi-modal framework significantly outperforms the current state-of-the-art image-based models. However, more efforts and resources are needed for this classification task in order to reach a satisfactory level.",359
"  \vskip 0.15in  Despite recent developments of activation functions for Machine Learning -based classifiers, such as the m-arcsinh~ for shallow Multi-Layer Perceptron ~, usable, repeatable and reproducible functions for both shallow and deep neural networks, e.g., the Convolutional Neural Network ~, have remained very limited and confined to three activation functions regarded as 'gold standard'. These include the Rectified Linear Unit , the sigmoid function and its modified version, hyperbolic tangent sigmoid or 'tanh'~, which extends its range from [0, +1] to [-1, +1]. The sigmoid and tanh have well-known vanishing gradient issues; thus, the ReLU function was devised to be more scalable for deep neural networks, despite its 'dying ReLU' problem, which has recently been solved by~. These have been made freely accessible in the open source Python library named 'Keras'~ for Deep Learning. The availability of these functions in the public domain has enabled not-for-profit and for-profit organisations to leverage them for several intelligence-based applications, from academic to industrial applications~~. \\  Nevertheless, considering the above-mentioned challenges in the Computer Science and ML communities, such activation functions lack robustness with classification tasks of varying degrees of complexity, e.g., slow or lack of convergence~ ~, caused by trapping at local ~. Moreover, amongst the three above-mentioned activation functions, only the ReLU is applicable from shallow to deep neural networks, with its novel quantum variations  found more scalable than its traditional version only recently~. \\  On the other side, in sciences dealing with the study of human behaviour, in the last 20 years, considerable progress has been made towards the prevention of mental health disorders~~. Specifically, professionals working in the field of counselling psychology have slightly enhanced their ability of grasping relational issues in their subjects via novel ML-based tele-monitoring technologies~. Nevertheless, these technologies have not yet changed the traditional counselling psychology practice, which is still based on a structured methodology that is adopted to help individuals to become more self-aware, more conscious of their own needs and moods~. The main goal counsellors pursue is guiding individuals to get to know themselves at a deeper level and to help them discover and resurface their own resources to better manage their emotions in their daily life. This process first requires a tailored dialogue between the counsellor and the individual and, subsequently, leveraging practical tools to aid the individual in their experience to understand their inner self more deeply~. Moreover, there are still limitations within the counselling setting. For instance, individuals, out of fear, may not reveal fundamental aspects of their  that would help counsellors guide them better in getting to know themselves. Furthermore, in many cases, subjects may express a verbal language opposite to their non-verbal one. Counsellors often hardly understand the dynamic patterns observed in the behaviours of their subjects, thus being unable to provide the required help and support to them. \\  In counselling, neural network algorithms, both shallow and deep depending on the amount of good-quality data and hardware available, have the potential to support counsellors in image and text classification tasks to understand and guide their subjects by helping them infer subtle dynamic changes in their behaviours. Via a careful and effective observation of images, micro- and macro- body movements, and facial expressions~~, it is possible to better interpret and understand the subjects' non-verbal language. Even the emotions underlying the written content from subjects may reveal inner aspects of their  that are fundamental for counsellors to help resurface to increase the subjects' self-awareness and related capability of 'self-healing'~. \\  Therefore, from both theoretical and practical standpoints, there is an increasing need for accurate and reliable open source activation functions, which reach convergence faster, avoiding trapping at local , are more stable and can also be used and scale across both shallow and deep neural network algorithms for image and text classification. Entirely written in Python and made freely available in TensorFlow~ and Keras~, the proposed hyperbolic function is demonstrated as a competitive function with respect to gold standard functions, which suits both shallow and deep neural networks, thus being accurate and reliable for pattern recognition to aid image and text classification tasks.  Thanks to its liberal license, it has been widely distributed as a part of the free software Python libraries TensorFlow~ and Keras~, and it is available for use for both academic research and commercial purposes.\\  %%%%%%%%%%%%%%% Methods section %%%%%%%%%%%%%%%%%%%%%  \vskip 0.3in     \vskip 0.15in  As demonstrated by the competitive results obtained on the 5 data sets evaluated, especially those in Tables 1 and 3 for the deep neural network CNN and Tables 4 and 5 for the shallow neural network FC-NN, the hyper-sinh is deemed a suitable activation function that scales from shallow to deep neural networks. \\ In fact, its accuracy and reliability was high across both sets of benchmark image- and text-based data sets, as quantified via appropriate metrics in sub-section 2.4, and better than some gold standard functions, e.g., considering Table 1 with the accuracy and the F1-score of the CNN using hyper-sinh being 0.70 and 0.69 respectively on the CIFAR-10 image-based data set, as opposed to that of the same CNN but using sigmoid being 0.10 and 0.02 respectively. Moreover, its accuracy and reliability were comparable to the FC-NN using ReLU , with higher reliability than the same FC-NN when leveraging the sigmoid function on the 'Reuters' text-based data set . The proposed hyper-sinh also led to increased precision on the 'IMDB' text-based data set  as opposed to sigmoid and tanh , when using the same FC-NN as that leveraged to classify the 'Reuters' data set. \\ Therefore, the hyper-sinh demonstrates that it is possible to extend the m-arcsinh to generalise across both shallow and deep neural networks for image and text classification tasks, and that the mathematical formulation of this extended function does not have to be complex at all. As an accurate and reliable activation function, the hyper-sinh is thus deemed a new gold standard activation function for both shallow and deep neural networks, freely available in TensorFlow and Keras.                   Conclusion section                          hyper-sinh was proven an accurate and robust activation function for shallow and deep neural networks for image and text classification, thus being a new gold standard that scales well for FC-NN and CNN.  Since it is made freely available, open source, on the Python, TensorFlow and Keras ecosystems, it adds to the selection of activation functions that both not-for-profit and for-profit organisations can have when tackling image and text classification tasks with data sets of various sizes.  Importantly, the proposed algorithm, being accurate and reliable, and written in a high-level programming language , can be leveraged as a part of ML-based pipelines in specific use cases, wherein high accuracy and reliability need to be achieved, such as in the healthcare sector , from small to large clinics with its suitability from shallow to deep neural networks. Future work involves further improving this function to reduce its computational cost.     Acknowledgements should go at the end, before appendices and references      Manual newpage inserted to improve layout of sample file - not   needed in general before appendices/bibliography.      
","%   <- trailing '%' for backward compatibility of .sty file This paper presents the 'hyper-sinh', a variation of the m-arcsinh activation function suitable for Deep Learning -based algorithms for supervised learning, such as Convolutional Neural Networks . hyper-sinh, developed in the open source Python libraries  and , is thus described and validated as an accurate and reliable activation function for both shallow and deep neural networks.  Improvements in accuracy and reliability in image and text classification tasks on five  benchmark data sets available from Keras are discussed.  Experimental results demonstrate the overall competitive classification performance of both shallow and deep neural networks, obtained via this novel function.  This function is evaluated with respect to gold standard activation functions, demonstrating its overall competitive accuracy and reliability for both image and text classification.",360
"  In grounded language theory, the semantics of language are given by how symbols connect to the underlying real world---the so-called ``symbol grounding problem''. For example, we want a robotic system that sees an eggplant  to ground the recognition object to a canonical symbol for `eggplant.' When a user asks ""Please grab me the eggplant,"" the robot should ground the natural language word ""eggplant"" to the same symbol that denotes the relevant visual percepts. Once both language and vision successfully ground to the same symbol, it becomes feasible for the robot to complete the task. We learn this connection by using physical sensors in conjunction with language learning: paired language and perceptual data are used to train a joint model of how linguistic constructs apply to the perceivable world.   Machine learning of grounded language often demands large-scale natural language annotations of things in the world, which can be expensive and impractical to obtain. It is not feasible to build a dataset that encompasses every object and possible linguistic description. Novel environments will require symbol grounding to occur in real time, based on inputs from a human interactor. Learning the meanings of language from unstructured communication with people is an attractive approach, but requires fast, accurate learning of new concepts, as people are unlikely to spend hours manually annotating even a few hundred samples, let alone the thousands or millions commonly required for machine learning.  % Active learning, in which a system queries for specific training data, has the potential to improve learning efficiency and reduce the number of labels required to learn a grounded language model.  In this work we study active learning, in which a system deliberately seeks information that will lead to improved understanding with less data, to minimize the number of samples/human interactions required. The field of active learning typically assumes that a pool of unlabeled samples is available, and the model can request specific example that it would like to obtain a label for. By having the model select the most informative data points for labeling, the number of samples that need to be labeled is reduced. This maps to the goal of human-robot learning with minimum training data provided by the human. Furthermore, active learning can be part of a pipeline with other few-shot learning methods.   However, active learning is not a magic bullet. When not carefully applied, it does not outperform sequential or random sampling baselines. Thoughtful selection of suitable approaches for problems is required. While active learning has been used for language grounding %, , to the best of our knowledge, we present the first broad exploration of the best methods for active learning for grounding vision-language pairs. %  In this paper, our focus is on developing guidelines by which active learning methods might be appropriately selected and applied to vision-language grounding problems. We test different active learning approaches on grounded language problems of varying linguistic and sensory complexity, and use our results to drive a discussion of how to select active learning methods for different grounded language data acquisition problems in an informed way.  We consider the grounded language task of learning novel language about previously unseen object types and characteristics. Our emphasis is on determining what methods can reduce the amount of training data needed to achieve performance consistent with human evaluation. Primarily, we address five relevant questions concerning characteristic-based grounded language learning: []  % We make conclusions with respect to these questions in . % In addition to addressing the above research questions, we verify how generalizable these learning techniques are beyond characteristic-based grounding.    We find that a right ordering of training data makes it possible to learn successfully from significantly fewer descriptions in most cases, but also that the active learning methodology chosen is specific to the nature of the learning problem. Our main contribution is a principled analysis of using active learning methods as unsupervised data sampling techniques in language grounding with a discussion of what aspects of those problems are relevant to approach selection. While our contributions are primarily analytic rather than algorithmic, we argue they address a critical need within grounded language understanding, an active research area in which questions of efficiency and data collection are widespread, and have the potential to support additional algorithmic developments.    In this work, we present a thorough exploration of different active learning approaches to grounding unconstrained natural language in real-world sensor data. We demonstrate that active learning has the potential to reduce the amount of data necessary to ground language about objects, an active area of research in both NLP and robotics as well as machine learning from sparse data generally. We additionally provide suggestions for what approach may be suitable given the perceptual and linguistic complexity of a problem. Given our analysis of the causes of performance for different algorithms and cases, we believe these results will prove to generalize beyond the relatively simple data seen here, making it possible for these guidelines to apply to more complicated language grounding tasks in future.                                  
"," % In grounded language acquisition, a physical agent uses language combined with high-frequency sensor data to learn a model of how language refers to the physical world. This approach, while powerful, often requires extensive data annotation, which can be difficult to obtain. This work  % Ordering the selection of training data using active learning can lead to improvements in learning efficiently from smaller corpora. We present an exploration of active learning approaches applied to three grounded language problems of varying complexity in order to analyze what methods are suitable for improving data efficiency in learning. We present a method for analyzing the complexity of data in this joint problem space, and report on how characteristics of the underlying task, along with design decisions such as feature selection and classification model, drive the results. We observe that representativeness, along with diversity, is crucial in selecting data samples.",361
" Deep neural networks are powerful and have been widely applied in natural language processing. However, recent studies demonstrate that these models are vulnerable to adversarial examples, which are malicious inputs intentionally crafted to fool the models. % The introduction of the adversarial example ushered in a new era to understand and improve neural the network-based models.  % Adversarial attacks and defenses against these attacks have drawn significant attention in recent years . Although generating adversarial examples for texts has proven to be a more challenging task than for images due to their discrete nature, a number of methods have been proposed to generate adversarial text examples and reveal the vulnerability of deep neural networks in natural language processing  tasks including reading comprehension , text classification , machine translation , dialogue systems , and dependency parsing . These methods attack text examples by replacing, scrambling, and erasing characters or words or other language units.  To settle the susceptible attack direction, they require a large number of queries to the target model for the predictions of given inputs. Thus the adversarial examples are typically generated for a specific model.  This motivates the main questions we aim to answer in this paper: { %are there universal adversarial examples that can transfer to any neural network-based models?  It is well known that adversarial examples exhibit black-box transferability, meaning that adversarial examples generated for one model can fool another model .  Transfer attackers launch white-box attacks on local models to find candidate adversarial examples that may transfer to the target model. % In the white-box setting, an adversary can access the model's architecture, parameters and input feature representations while not in the black-box one. % However, adversarial examples are typically overfitted to the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models.  However, which factors most affect the transferability of adversarial examples is still unclear, especially for NLP models. In this study, we quantitatively investigate how adversarial transferability is impacted by several critical factors, including the network architecture, input form, word embedding type, and model capacity.  Based on the understanding of transferability among various neural models, we study whether it is possible to craft universal, model-agnostic text adversarial examples for almost all existing models.  Universal adversarial examples have at least two advantages. First, the adversaries do not need white-box access to the target models. They launch the attacks by their own models trained on similar data, which can transfer across models .  Second, universal adversarial examples are a useful analysis tool because, unlike typical attacks, they are model-agnostic.  Thus, they highlight general input-output patterns learned by a model. We can leverage this to study the influence of dataset biases and to identify those biases that are learned by models.   [t] {1.0mm} {c|l}  {Senate Panel Gives NASA Extra Money  AP - NASA would get  \#36;16.4 billion next year under a \\ bill a Senate committee approved Tuesday, reversing a decision by House lawmakers to \textcolor{blue}{} \textcolor{red}{contract} \\ the space agency's budget below this year's levels.} \\  {Deal in Congress to \textcolor{blue}{} \textcolor{red}{preserve} tax cuts, Widening Deficit Republican and Democratic leaders agreed \\ to extend \$5 billion worth of tax cuts sought by \textcolor{blue}{} \textcolor{red}{Chairman} Bush without trying to pay for them.} \\  {Nortel Downsizes Again Aug. 23, 2004  Problem-plagued Nortel \textcolor{blue}{} \textcolor{red}{Web} Corp. \\ announced plans Thursday, Aug. 19, to eliminate an additional 3,500 jobs and fire seven more senior \\ \textcolor{blue}{} \textcolor{red}{administrators} as the company labors to reinvent.} \\  \hline \hline        In this study, we first systematically investigated a few critical factors of neural models, including network architectures , input forms , embedding types , and model capacities  and how they impact the transferability of text adversarial examples through extensive experiments on two datasets of text classification.  We vary one factor at a time while fixing all others to see which factor is more significant, and found that the input form has the greatest influence on the adversarial transferability, following by network architecture, embedding type, and model capacity. Then, we propose a genetic algorithm to find an optimal ensemble with minimum number of members on the basis of our understanding of the adversarial transferability among neural models.  The adversarial examples generated by attacking the ensemble found by our algorithm strongly transfer to other models, and for some models, they exhibit better transferability than those generated by attacking models with different random initialization. Finally, we generalize the adversarial examples constructed by the ensemble method into universal semantics-preserving word replacement rules that can induce adversaries on any text input strongly transferring to any neural network-based NLP model . Since those rules are model-agnostic, they provide an analysis of global model behavior, and help us to identify dataset biases and to diagnose heuristics learned by the models.      In this study, we investigated four critical factors of NLP neural models, including network architectures, input forms, embedding types, and model capacities and how they impact the transferability of text adversarial examples with  different models. Based on the understanding of the transferability among those models, we proposed a genetic algorithm to find an optimal ensemble of very few models that can be used to generate adversarial examples that transfer well to all the other models. We also described a algorithm to discover universal adversarial word replacement rules that can be applied to craft adversarial examples with strong transferability across various neural models without access to any of them. Finally, since those adversarial examples are model-agnostic, they provide an analysis of global model behavior and help to identify dataset biases.  {\small   }   
"," Deep neural network models are vulnerable to adversarial attacks. In many cases, malicious inputs intentionally crafted for one model can fool another model in the black-box attack setting. However, there is a lack of systematic studies on the transferability of adversarial examples and how to generate universal adversarial examples.  In this paper, we systematically study the transferability of adversarial attacks for text classification models.  In particular, we conduct extensive experiments to investigate how various factors, such as network architecture, input format, word embedding, and model capacity, affect the transferability of adversarial attacks.  Based on these studies, we then propose universal black-box attack algorithms that can induce adversarial examples to attack almost all existing models. These universal adversarial examples reflect the defects of the learning process and the bias in the training dataset.  Finally, we generalize these adversarial examples into universal word replacement rules that can be used for model diagnostics.     \if0 It has been known that adversarial examples exhibit black box transfer, i.e. malicious inputs intentionally crafted for one model can also cause another model to make mistakes. However, which factors affect the most and how they impact the transferability of adversarial examples are still unclear, especially for NLP models.  Through extensive experiments, we systematically investigate how adversarial transferability is impacted with a few critical, model-specific factors, including the network architecture, input form, pre-trained word embedding, and model capacity. Based on the understanding of the adversarial transferability among neural models, we propose a population-based algorithm to find an optimal ensemble with minimum number of models, which can be used to generate adversarial examples that strongly transfer across other neural models.  We also generalize the adversarial examples generated by the ensemble method into universal word replacement rules that can induce adversaries on any text input to fool almost all the existing models with a much higher success rate. Those rules also help us to identify dataset biases and diagnose heuristics improperly learned by the models. \fi",362
" Emotional analysis has been an active research area for a few decades, especially in recognition domains of text and speech emotions. Even if text and speech emotions are closely relevant, both kinds of emotions have different challenges. One of the challenges in text emotion recognition is ambiguous words, resulting from omitted words . On the other hand, one of the challenges in speech emotion recognition is creating an efficient model. However, this paper focuses on only the recognition of speech emotions. In this area, two types of information, linguistic and paralinguistic, were mainly considered in speech emotion recognition. The linguistic information refers to the meaning or context of speech. The paralinguistic information implies the implicit message meaning, like the emotion in speech . Speech characteristics can interpret the meaning of speech; therefore, behavioral expression was investigated in most of the speech emotion recognition works  .   In recent works, local feature learning block  , one of the efficient methods, has been used in integrating local and global speech emotion features, which provide better results in recognition. Inside LFLB, convolution neural network  was used for extracting local features, and then long short-term memory  was applied for extracting contextual dependencies from those local features to learn in a time-related relationship. However, vanishing gradient problems may occur with CNN . Therefore, residual deep learning was applied to the CNN by using skip-connection to reduce unnecessary learning and add feature details that may be lost in between layers.  Furthermore, the accuracy of speech recognition does not only rely on the efficiency of a model, but also of a speech feature selection . In terms of speech characteristics, there are many distinctive acoustic features that usually used in recognizing the speech emotion, such as continuous features, qualitative features, and spectral features . Many of them have been investigated to recognize speech emotions. Some researchers compared the pros and cons of each feature, but no one can identify which feature was the best one until now .  As previously mentioned, we proposed a method to improve the efficiency of LFLB  for deeper learning. The proposed method, deep residual local feature learning block , was inspired by the concept of human brain learning; that is, 閳ユΜepeated reading makes learning more effective,閳 as the same way that Sari  and Shanahan  were used. Responding to our inspired concept, we implemented a learning method for speech emotion recognition with three parts: Part 1 is for general learning, like human reading for the first time, Part 2 is for further learning, like additional readings, and the last part is for associating parts learned to decide types of emotions. Besides, the feature selection is compared with two types of distinctive features to find the most effective feature in our work: the normal and specific distinctive features are log-mel spectrogram , which is fully filtered sound elements, and %log-mel spectrogram,  MFCC deltas, delta-deltas, and chromagram  are more clearly identify speech characteristics extracted based on %according to  the human mood.  Our main contributions of this paper are as follows:   Deep residual local feature learning block  was proposed. DeepResLFLB was arranged its internal network as LFLB, batch normalization , activation function, normalization-activation-CNN , and deep layers.   Learning sequences of DeepResLFLB were imitated from human re-reads.   Speech emotion features, %according to  based on human mood determination factors such as LMS and LMSDDC, were applied and compared their performances.     This paper has described a DeepResLFLB model and LMSDDC feature for speech emotion recognition. The DeepResLFLB was redesigned from LFLB based on the `repeatedly reads' concept while the LMSDDC was emotional feature extracted from speech signals based on human glottal flow and human hearing. Performance of our model and emotional feature was tested on two well-known databases. The results show that the DeepResLFLB can perform better than baselines and use fewer resources in learning layers. In addition, the proposed LMSDDC can outperform conventional LMS.    Although DeepResLFLB presented in this paper have provided better performance in speech emotion recognition, many aspects still can be improved, especially activation function. In future work, we will apply different kinds of activation function in each section of neural network; this will improve the performance of DeepResLFLB.    
"," Speech Emotion Recognition  is becoming a key role in global business today to improve service efficiency, like call center services. Recent SERs were based on a deep learning approach. However, the efficiency of deep learning depends on the number of layers, i.e., the deeper layers, the higher efficiency. On the other hand, the deeper layers are causes of a vanishing gradient problem, a low learning rate, and high time-consuming. Therefore, this paper proposed a redesign of existing local feature learning block . The new design is called a deep residual local feature learning block . DeepResLFLB consists of three cascade blocks: LFLB, residual local feature learning block , and multilayer perceptron . LFLB is built for learning local correlations along with extracting hierarchical correlations; DeepResLFLB can take advantage of repeatedly learning to explain more detail in deeper layers using residual learning for solving vanishing gradient and reducing overfitting; and MLP is adopted to find the relationship of learning and discover probability for predicted speech emotions and gender types. Based on two available published datasets: EMODB鐠虹棏nd RAVDESS, the proposed DeepResLFLB can significantly improve performance when evaluated by standard metrics: accuracy, precision, recall, and F1-score.",363
" Classification is an important task of knowledge discovery in databases and data mining. It is a task of learning a discriminative function from the given data that classifies previously unseen data to the correct classes. Current research trends in natural language processing focus on developing deep neural network models such as BERT  that have been pre-trained with a large text corpus and thus show immense improvement in different text classification tasks. Despite the success of large pre-trained models, DNNs still suffer from generalizing to a balanced testing criterion in cases of data imbalance . In realistic settings, it is rarely the case where the discrete distribution of the data acquired is perfectly balanced across all classes. Realistic settings are prone to be skewed to specific classes while such classes are often the class of interest. Some situations may be binary, as in detecting spams in forums . The majority of the contents posted from users are not spams and is in accordance with the intended goal. As a result, the number of spam samples is sparse in comparison to non-spam samples. Imbalanced data may also occur in a multi-classification setting such as classifying articles into different categories .       Text classification can be used for numerous application purposes. In this paper, we address the problem of detecting sexual harassment and toxicity in comments from news articles. In the name of anonymity, online discussion platforms have become a place where people undermine, harass, humiliate, threaten, and bully others  based on their superficial characteristics such as gender, sexual orientation, and age . Each toxic comment can further be classified into classes based on their degree of toxicity . Figure  shows the overall procedure of detecting sexual harassment and performing sentimental analysis on comment data in the wild. When collecting and annotating comments, data skewness occurs naturally since users do not consider data imbalance levels when writing toxic or non-toxic comments. Classifiers trained in imbalanced settings tend to become biased toward the class with more samples in the training data. This is because standard deep learning architectures  do not take the data imbalance level into consideration. In order to develop intelligent classifiers, methods to temper the classifier from biasing towards certain classes are of great importance.   Previous methods addressing data imbalance in the text can be divided into data-level and algorithm-level methods. Data-level methods  apply manipulation on the data by undersampling majority classes or oversampling minority classes. However, most of the methods require an effective numerical representation algorithm since methods are applied directly to the representation instead of on the actual text. Algorithm-level methods modify the underlying learner or its output to reduce bias towards the majority group. However, these methods are task-sensitive and somewhat heuristic since it requires the researchers to modify the classifier considering the innate properties of the task. This property leads to the inefficiency in training the learner since heuristic approaches are often time-consuming and arbitrary. Since only traditional oversampling and undersampling methods, which simply duplicate or sample data instances, are independent of these two limitations, methods addressing data imbalance in the text without the utilization of feature spaces or task-dependent is needed.  We propose a novel training architecture, Sequential Targeting , that handles the data imbalance problem by forcing an incremental learning setting. ST divides the entire training data set into mutually exclusive partitions, target-adaptively balancing the data distribution. Target distribution is a predetermined distributional setting that enables the learner to exert maximum performance when trained with. In an imbalanced distributional setting, the target distribution is idealistic to follow a uniform distribution where all the classes hold equal importance. Optimal class distribution may differ by innate property of the data but research shows that a balanced class distribution has an overall better performance compared to other distributions~. The remaining partitions are then sorted in the magnitude of similarity with the target distribution which is measured by KL-divergence. The first partition of the split data is imbalanced while the last partition is arbitrarily modeled to be uniform across classes and all the partitions are utilized to train the learner sequentially.   We handle the issue of catastrophic forgetting~, which is an inevitable phenomenon when transfer learning, by utilizing Elastic Weight Consolidation~ to stabilize the knowledge attained from the previous tasks. This allows the discriminative model to learn from the incoming data while not forgetting the previously inferred parameters from previous tasks.   Our proposed method is both independent of the numerical representation method and the task at hand. We validate our method on simulated datasets with varying imbalance levels and apply our method to a real-world application. We annotated and construct three datasets consisting of comments made by users from different social platforms of NAVER\footnote{NAVER is the Korean No.1 web search portal where around 16 million users visit every day. www.naver.com}: two for detecting sexual harassment and one for multiple sentimental analysis. Annotations on the datasets were improved iteratively by in-lab annotations and crowdsourcing. Experimental results show that ST outperforms traditional approaches, with a notable gap, especially in extremely imbalanced cases. Lastly, ST proves to be compatible with previous approaches.  Our contribution in this paper is three-folds:      The rest of the paper is organized as follows. Section  summarizes related works. Section  provides the details of the proposed method. Section    presents dataset descriptions, experiment setups, and qualitative experimental results on various datasets. Finally, Section  concludes the paper.     It is seldom the case data in the wild has a balanced distribution. In realistic settings, there is a limitation of acquiring relatively balanced data through choices of balanced data sources. Handling data skewness is a crucial problem because learning from imbalanced data inevitably brings bias toward frequently observed classes. Data-level manipulation tries to under-sample the majority classes or over-sample the minority classes. But these methods tend to discard valuable information from observations of majority classes or overfit to a sparse representation of minority classes, especially as the imbalance level gets higher. Moreover, recent methods such as SMOTE cannot be applied directly to the text data.   We propose ST, which effectively circumvents these issues by simply decomposing the data into k splits and sequentially training a learner in the decreasing order of KL divergence with the target distribution, which in the case of data imbalance problem is the discrete uniform distribution. Through extensive experiments, we show our architecture proves to be compatible with previous methods and outperforms existing methods when validated on simulated as well as real-application tasks. Our model shows superiority in performance because it enables more focus to be put on minority instances while not forgetting about majority instances. We believe that our work makes a meaningful step towards handling data skewness in text classification and the application of incremental learning methods focused on the data imbalance problem.  For future work, ensemble methods can be used by varying the  ratio to train multiple weak learners. Moreover, since ST can be applied simultaneously with algorithm-level methods, proven methods such as focal loss  and cost-sensitive deep neural network  could be implemented together to increase optimal performance.  
"," %% Text of abstract Classification tasks require a balanced distribution of data to ensure the learner to be trained to generalize over all classes. In real-world datasets, however, the number of instances vary substantially among classes. This typically leads to a learner that promotes bias towards the majority group due to its dominating property. Therefore, methods to handle imbalanced datasets are crucial for alleviating distributional skews and fully utilizing the under-represented data, especially in text classification. While addressing the imbalance in text data, most methods utilize sampling methods on the numerical representation of the data, which limits its efficiency on how effective the representation is. We propose a novel training method, Sequential Targeting, independent of the effectiveness of the representation method, which enforces an incremental learning setting by splitting the data into mutually exclusive subsets and training the learner adaptively. To address problems that arise within incremental learning, we apply elastic weight consolidation. We demonstrate the effectiveness of our method through experiments on simulated benchmark datasets  and data collected from NAVER.",364
"  As the growth of robots interacting with humans, different levels of environment understanding is required by the robot. A robot acting in an environment has to deal with many open questions, thus needs different levels of reasoning to do a task. Usually, robots rely on their initial knowledge, perception and their cognitive abilities to be able to understand and do reasoning in their situated environment. A recently hooked topic to a better Knowledge-Based cognition is dialogic interaction between a human and a robot, where the robot captures fresh information about the environment from a user through Natural Language. Information comes from Natural Language together with visually perceived information, and a Knowledge Base  lets a cognitive agent reach different levels of understanding in the environment.   The first level of understanding can be seen as classification and detection on sensory inputs, e.g. detection of objects in visual perception, or role tagging of lexical in a sentence. The second level of understanding concerns finding relations between different sensory inputs, e.g. finding common attributes in language and vision. Some famous problems such as symbol grounding  and anchoring  concern finding correspondences in different sensory input modalities. A higher and abstract level of understanding can be thought to find relations between the entities in an environment. e.g. in a scene with a desk and a book on top, some of the relationships between these are their relative physical position and their semantics that shows how entities  are similar.   Understanding relationships between physical entities can also be extended to the attributes of entities. Indeed the same definition of the relationship between entities can be found for the attributes. For example, when a user declares freshness attribute of 'apple-1' is 'spoiled', as well as 'apple-2' and 'apple-3', but 'orange-1' and 'banana-1' are 'fresh', a relation between the values of freshness attribute exists which connects semantic of entities; In this example, is that all apples are 'spoiled', and the rest of fruits are 'fresh', with closed world assumption.   Relation and rules for attributes of entities can help a robot that is interacting with a human in many applications. For example when a user utters ""bring me a fruit"", using the rules obtained for freshness attribute, the robot notices which fruits are spoiled and which are fresh to eat. Such logical rules between attributes let the robot realize that apples are spoiled, apples should be thrown out, and added to the shopping list. Moreover, the obtained rule for attributes can be used in a robot's low-level sensory input processing. Consider an utterance where the user of our example is declaring that a physical entity is spoiled, but the robot's visual perception has doubt whether the perceived object is apple or pear. As the robot already found that all apples are spoiled and other fruits are fresh, so the perceptual detection refines the recognized object as the apple.  %Different attributes can represent characteristics of an entity, where some are computed from visual perception and some from Natural Language through interaction with a user. In this work, we deal with nine different attributes, as a category, color, label, functionality, owner, size, weight, restriction, and location of entities in a scene; where the first two are computed from visual perception and the rest are obtained from Natural Language.  %%It is worth emphasis on the importance of attributes that come from Natural Language. Such information is almost impossible to obtain from visual perception, e.g. the information that a user can give about owner of an entity, cannot be obtained from the camera. Also, an initial knowledge base only gives information about the category of an entity, and not about a particular entity , and some of the assignments might be temporary. On the other hand, information about size, weight, and location, may be used for refinement of knowledge base and camera, or just a shortcut to obtaining such information from the user.   In this work, we propose a framework for learning logical rules that represent relations between attributes in a semantic model of the robot's environment. Such logical rules help the robot to find which attributes  entail a specific attribute. A distinctive novelty of our work is to generalize rules from a semantic model built via Human-Robot Interaction , through the integration of visual and linguistic cues. Our framework goes all the way from sensory input data to abstract First-Order Logic formulas that describe abstract relationship between attributes of entities in a scene. %Our approach differs from other works as our system is able to capture more attributes from Natural Language in addition to attributes from computer vision.  %Our proposed framework compute First-Order Logic formulas, which is useful for general reasoning upon entities that have common attributes.  We focus on , which the robot can capture implicitly when a human describes objects to the robot. In other words, we do not require the user to give rules explicitly to the robot, but rather we let the robot find rules and do further reasoning based on self-computed rules for improving its interaction with the user.    This paper continues with the review of related work, and then in Section  the proposed framework is described, followed by an implementation to demonstrate the viability of the proposed framework in Section . In Section  results of a test scenario are given, followed by the discussion about the applicability of the framework. In the end, conclusions of this work are drawn.         In this work, we proposed a framework to compute First-Order Logical formulas that represent latent relations between entities in a scene semantic model. Our proposed framework creates a semantic model of the robot's  environment, in the format of entities and attributes, from visual perception and dialogic interaction with the user. This semantic model is further used for finding the latent relations between entities in the form of First-Order Logic. The obtained rules specify which attributes entail a specific property in an entity, and can be used by the robot for removing uncertainty in sensory input, or to enrich its knowledge base.   
"," Humans have a rich representation of the entities in their environment. Entities are described by their attributes, and entities that share attributes are often semantically related.  For example, if two books have ``Natural Language Processing'' as value of their `title' attribute, we can expect that their `topic' attribute will also be equal, namely, ``NLP''.  Humans tend to generalize such observations, and infer sufficient conditions under which the `topic' attribute of any entity is ``NLP''.  If robots need to interact successfully with humans, they need to represent entities, attributes, and generalizations in a similar way. This ends in a contextualized cognitive agent that can adapt its understanding, where context provides sufficient conditions for a correct understanding. In this work, we address the problem of how to obtain these representations through human-robot interaction.  We integrate visual perception and natural language input to incrementally build a semantic model of the world, and then use inductive reasoning to infer logical rules that capture generic semantic relations, true in this model.  These relations can be used to enrich the human-robot interaction, to populate a knowledge base with inferred facts, or to remove uncertainty in the robot's sensory inputs.",365
"   Mental illnesses are a common problem of our modern world. More than one in ten people was living with mental health disorders in 2017 , with women being the most affected. These disorders affect people's way of thinking, mood, emotions, behaviour and their relationships with others. Most mental illnesses remain undiagnosed because of the social stigma around them.  Depression is one of the main causes of disability globally , it affects people of all ages. Prevention is used to reduce depression and to save the lives of people at risk of suicide, but prevention is only limited to raising awareness and programs to cultivate positive thinking in case of depression and monitoring people who attempted suicide or self-harm.  With the rise in social media use, more computational efforts are made to detect mental illnesses such as depression  and PTSD , but also to detect misogyny , irony and sarcasm  from users' texts.  People tend to talk more about their emotions and mental health problems online and to seek support. The sources of mental health cues used for detection are Twitter, Facebook, Reddit and forums . Reddit is a social media site very similar to forums. It is organized in subreddits with specific topics, some dedicated to mental health problems. The use of throwaway accounts to maintain anonymity promotes disclosure, and users are more likely to share problems they have not discussed with anyone before. The use of these accounts makes it difficult for users to receive more social support because the majority of them are used only for one post .  In this work, we choose to tackle the problem of detecting early onset of depression from users' posts on social media, specifically from Reddit. As such, we explore the eRisk 2018 dataset through topic analysis by means of Latent Semantic Indexing  and learned out-of-distribution confidence scores . Due to the nature of the dataset, we repurpose the learned confidence score to make a decision on whether to label the user as depressed or non-depressed or to wait for more data, as test chunks were progressively released every week.      In this paper, we use the eRisk 2018 dataset on Early Detection of Signs of Depression for depression classification from Reddit posts. Our method uses Latent Semantic Indexing for topic modelling and to generate the embeddings used as input for our neural network, but focuses on using a learned out-of-distribution confidence score alongside the classification output to decide whether to label the user or wait for more data. Besides its initial use case in out-of-distribution detection, we repurposed the confidence score as a measure for how much the model trusts its classification output to be correct. We showed that, in general, there is a significant difference in writing topics depending on the users' mental health, to the extent that it contains enough information for use in classification.    
","   English.  Computational research on mental health disorders from written texts covers an interdisciplinary area between natural language processing and psychology. A crucial aspect of this problem is prevention and early diagnosis, as suicide resulted  from  depression being the second leading cause of death for young adults. In this work, we focus on methods for detecting the early onset of depression from social media texts, in particular from Reddit. To that end, we explore the eRisk 2018 dataset and achieve good results with regard to the state of the art by leveraging topic analysis and learned confidence scores to guide the decision process. \footnote{Copyright \copyright2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International .} %   Our analysis paves way to more in depth exploration of detection of mental illnesses from social media interactions.",366
" 	 		Neural text-to-speech  techniques have significantly improved the naturalness of speech produced by TTS systems. We refer to NTTS systems as a subset of TTS systems that use neural networks to predict mel-spectrograms from phonemes, followed by the use of neural vocoder to generate audio from mel-spectrograms. 			 		In order to improve the prosody\footnote{We use the subtractive definition of prosody from .} of speech obtained from NTTS systems, there has been considerable work in learning prosodic latent representations from ground truth speech. These methods use the target mel-spectrograms as input to an encoder which learns latent prosodic representations. These representations are used by the decoder in addition to the input phonemes, to generate mel-spectrograms. The latent representations obtained by encoding a target mel-spectrogram at the sentence level will have information that is not directly available from the phonemes, and by the subtractive definition of prosody, we may claim that these representations capture prosodic information. Several variational and non-variational methods have been proposed for learning prosodic latent representations. While these methods improve the prosody of synthesised speech, they need an input mel-spectrogram which is not available while running inference on unseen text. This gives rise to the problem of sampling from the learnt prosodic space. Sampling at random from the prior may result in the synthesised speech not having contextually appropriate prosody, as it has no relationship with the text being synthesised. In order to improve the contextual appropriateness of prosody in synthesised speech, there has been work on using textual features like contextual word embeddings and other grammatical information to directly condition NTTS systems. These methods require the NTTS model to learn an implicit correlation between the given textual features and the prosody of the sentence. One work also poses this sampling problem as a selection problem and uses both syntactic distance and BERT embeddings to select a latent prosodic representation from the ones seen at training time.  		Bringing both the aforementioned ideas of using ground truth speech to learn prosodic latent representations and using textual information, we build Kathaka, a model trained using a two-stage training process to generate speech with contextually-appropriate prosody. In Stage~\Romannum{1}, we learn the distribution of sentence-level prosodic representations from ground truth speech using a VAE. In Stage~\Romannum{2}, we learn to sample from the learnt distribution using text. In this work, we introduce the BERT+Graph sampler, a novel sampling mechanism which uses both contextual word-piece embeddings from BERT and the syntactic structure of constituency parse trees through graph attention networks. We then compare Kathaka against a strong baseline and show that it obtains a relative improvement of  in naturalness. 		 	 	  		 		 		We presented Kathaka, an NTTS model trained using a novel two-stage training approach for generating speech with contextually appropriate prosody. In the first stage of training, we learnt a distribution of sentence-level prosodic representations. We then introduced a novel sampling mechanism of using trained samplers to sample from the learnt sentence-level prosodic distribution. We introduced two samplers, 1)~the BERT sampler which uses contextual word-piece embeddings from BERT and 2)~the Graph sampler where we interpret constituency parse trees as graphs and use a Message Passing based Graph Attention Network on them. We then combine both these samplers as the BERT+Graph sampler, which is used in Kathaka. We also modify the baseline duration model to incorporate the latent prosodic information. We conducted an ablation study of the samplers and showed a statistically significant improvement over the baseline in each case. Finally, we compared Kathaka against a baseline, and showed a statistically significant relative improvement of . 	 	
"," 		In this paper, we introduce Kathaka, a model trained with a novel two-stage training process for neural speech synthesis with contextually appropriate prosody. In Stage, we learn a prosodic distribution at the sentence level from mel-spectrograms available during training. In Stage, we propose a novel method to sample from this learnt prosodic distribution using the contextual information available in text. To do this, we use BERT on text, and graph-attention networks on parse trees extracted from text. We show a statistically significant relative improvement of $13.2\%$ in naturalness over a strong baseline when compared to recordings. We also conduct an ablation study on variations of our sampling technique, and show a statistically significant improvement over the baseline in each case.",367
" Due to the growing presence of AI-powered systems in our lives, affective computing has become an important part of human-computer interaction. Emotion plays a role in our thoughts and actions and is an integral part of the way we communicate . The ability to leverage context to understand emotions communicated both verbally and non-verbally is trivial for humans but remains difficult for machines . Emotional responses depend on both our psyche and physiology and are governed by our perception of situations, people and objects. They also depend on our mental state   . The way we exhibit and perceive emotion may also differ based on our age, gender, race, culture and accent . In addition to all of this, unlike targets in other classification tasks, the emotions we experience are rarely distinct: they often coexist without clear temporal boundaries, adding considerable complexity to the task .  Despite these difficulties, automated emotion recognition has social and commercial applications that make it worth pursuing. In the medical domain, it has exciting potential: to identify and diagnose depression and stress in individuals , to monitor and help people with bipolar disorder  and to assist the general public in maintaining mental health. Commercial applications include call center customer management, advertising through neuro-marketing and social media engagement . As intelligent chatbots and virtual assistants have become more widely used, emotion detection has become a vital component in the design, development and deployment of these conversational agents .  Early research in emotion detection focused on binary classification in a single modality, whether in text, speech , or images . Text-based classifiers used the n-gram vocabulary of sentences to predict their polarity and speech models modeled the vocal dynamics that characterize these emotions. These approaches are inherently limited: a binary granularity and cues from a single modality are far removed from the actual human process they're meant to model. As a result, joint approaches which leverage all available modalities  are promising.  While existing multi-modal emotion corpora like IEMOCAP  and Crem-D  have been critical for the progress in affective computing to date, they suffer from three issues that are the focus of our work. First, these corpora tend to be small due to the high costs of annotating for emotion. This precludes the use of deep neural models with high model complexity as they require many training samples to generalize well. This also compounds the second difficulty inherent to many emotion datasets: while there are usually many neutral, happy and sad training examples, there are often very few examples of rarer emotions like disgust making them difficult to classify. This issue is not easily solved by combining different corpora due to the third issue, their lack of mutual compatibility -- they differ in the emotions identified, the types of dialogue and number of speakers represented and the naturalness of the recordings . This severely restricts the generalizability of models trained on a single corpus.  Contemporary literature has dealt with these problems by dropping labels . Hard and scarce emotions like disgust are dropped from the corpus and the models are trained and evaluated on the trimmed corpus. This allows evaluating models on different corpora by using utterances exhibiting only the most common emotions. While this is a reasonable, the resulting performance is not a complete reflection of how these models perform once deployed to production. When emotion models are used in real-world applications, we can expect them to encounter utterances corresponding to dropped labels. For such cases, these models are likely to exhibit degraded performance by predicting one of the known, but incorrect labels.  In this work, we address the problem of data sparsity by transfer learning via the pretrain-then-finetune paradigm. Deep complex models can be trained on large datasets for an auxiliary but related task to learn network parameters that reflect abstract notions related to the target task. As the expression of emotions is highly dependent on the individual, we train a multilayer TDNN  on the task of speaker identification using the VoxCeleb corpus  and then fine-tune its final few layers on the task of emotion identification using the Crema-D corpus . Using this network, we extract speech embeddings for Crema-D from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a fine-tuned BERT model  and then train an LDA - pLDA  model on the resulting dense representations.  pLDA allows our model to more easily adapt to previously unseen classes and domains, a requirement for both evaluating against a different emotion corpus with an incompatible label set and performing well in the wild.  To understand the merits of each component, we exhaustively evaluate the predictive power of every permutation: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof. Our best variant, trained on only VoxCeleb and Crema-D and evaluated on IEMOCAP, achieves an Equal Error Rate  of \%. Including a portion of IEMOCAP during training produces a 5-fold averaged EER of \%.      In this work, we present a multi-modal approach to emotion detection that first transfers learning from related tasks in speech and text to produce robust neural embeddings and then uses these embeddings to train a pLDA classifier that is able to adapt to previously unseen emotions and domains.  We show that:    In the future, we think there is promise in adapting learning from such fine-tuned emotion detection models to other emotions, domains and languages via one-shot classification with pLDA.  We are also interested in exploring the effectiveness of transferring from other auxiliary tasks like automated speech recognition.   
"," Automated emotion detection in speech is a challenging task due to the complex interdependence between words and the manner in which they are spoken. It is made more difficult by the available datasets; their small size and incompatible labeling idiosyncrasies make it hard to build generalizable emotion detection systems. To address these two challenges, we present a multi-modal approach that first transfers learning from related tasks in speech and text to produce robust neural embeddings and then uses these embeddings to train a pLDA classifier that is able to adapt to previously unseen emotions and domains. We begin by training a multilayer TDNN on the task of speaker identification with the VoxCeleb corpora and then fine-tune it on the task of emotion identification with the Crema-D corpus.  Using this network, we extract speech embeddings for Crema-D from each of its layers, generate and concatenate text embeddings for the accompanying transcripts using a fine-tuned BERT model and then train an LDA - pLDA classifier on the resulting dense representations. We exhaustively evaluate the predictive power of every component: the TDNN alone, speech embeddings from each of its layers alone, text embeddings alone and every combination thereof.  Our best variant, trained on only VoxCeleb and Crema-D and evaluated on IEMOCAP, achieves an EER of $38.05$\%. Including a portion of IEMOCAP during training produces a 5-fold averaged EER of $25.72$\% .",368
"   Vocoders were originally used for speech compression in the field of communication. Recently, vocoders have been utilized in various fields such as text-to-speech and voice conversion or speech-to-speech translation. Neural vocoders generate human-like voices using neural networks, instead of using traditional methods that contain audible artifacts .  Recently, it has been demonstrated that vocoders exhibit superior performances in generation speed and audio fidelity when trained with single speaker utterances. However, some models face difficulty when generating natural sounds in multiple domains such as speakers, language, or expressive utterances. The ability of these models can be evaluated by the sound quality when the model is trained on data of multiple speakers and the sound quality of the unseen domain . A vocoder that can generate high-fidelity audio in various domains,  regardless of whether the input has been encountered during training or has come from an out-of-domain source, is usually called a universal vocoder.  MelGAN is a vocoder based on generative adversarial networks . It is a lightweight and robust model for unseen speakers but yields lower fidelity than popularly employed models . MelGAN alleviates the metallic sound that occurs mainly in unvoiced and breathy speech segments through multi-scale discriminators that receive different scale waveforms as inputs. However, it has not been implemented efficiently for learning with multiple speakers for a universal vocoder.  In this study, we propose Universal MelGAN. The generated waveform of the original MelGAN with audible artifacts appears as an over-smoothing problem with a non-sharp spectrogram. We added multi-resolution spectrogram discriminators to the model to address this problem in the frequency domain. Our multi-scale discriminators enable fine-grained spectrogram prediction by discriminating waveforms and spectrograms. In particular, they alleviate the over-smoothing problem in the high frequency band of the large footprint model, enabling the generation of realistic multi-speaker waveforms.  To evaluate the performance of the proposed model, we compare with full-band MelGAN  as a baseline and two other vocoders: WaveGlow and WaveRNN. We designed experiments in both Korean and English for language independency. For evaluation, we prepared multiple speaker utterances that included unseen domain scenarios, such as new speakers, emotions, and languages.  The evaluation results indicate that the proposed model achieved the best mean opinion score  in most scenarios and efficiently preserved the fidelity in unseen speakers. In addition, the evaluations show that the model efficiently preserves the original speech, even in challenging domains such as expressive utterances and unseen languages. In multi-speaker text-to-speech scenarios, our model can generate high-fidelity waveforms with high MOS, and the model outperforms compared vocoders. This results without any external domain information suggest the possibility of the proposed model as a universal vocoder.      In this study, we propose Universal MelGAN, a robust neural vocoder for high-fidelity synthesis in multiple domains. We solved the over-smoothing problem that causes a metallic sound, by attaching multi-resolution spectrogram discriminators to the model. Our model is stable while generating waveforms with fine-grained spectrograms in large footprint models. The evaluation results indicate that the proposed model achieved the highest MOS in most seen and unseen domain scenarios. The result demonstrates the universality of the proposed model. For more general use of the model, we will study a lightweight model in the future and apply the multi-band strategy to reduce the complexity while preserving the sound quality.    
"," We propose Universal MelGAN, a vocoder that synthesizes high-fidelity speech in multiple domains. To preserve sound quality when the MelGAN-based structure is trained with a dataset of hundreds of speakers, we added multi-resolution spectrogram discriminators to sharpen the spectral resolution of the generated waveforms. This enables the model to generate realistic waveforms of multi-speakers, by alleviating the over-smoothing problem in the high frequency band of the large footprint model. Our structure generates signals close to ground-truth data without reducing the inference speed, by discriminating the waveform and spectrogram during training. The model achieved the best mean opinion score  in most scenarios using ground-truth mel-spectrogram as an input. Especially, it showed superior performance in unseen domains with regard of speaker, emotion, and language. Moreover, in a multi-speaker text-to-speech scenario using mel-spectrogram generated by a transformer model, it synthesized high-fidelity speech of 4.22 MOS. These results, achieved without external domain information, highlight the potential of the proposed model as a universal vocoder.",369
" %What is spoken term detection  Unsupervised speech modeling is the task of discovering and modeling speech units at various levels from audio recording without using any prior linguistic information. It is an interesting, challenging and impactful research problem as phonetic, lexical and even semantic information could be acquired without the process of transcribing and understanding the given speech data. The relevant technology is particularly important to facilitate data preparation especially in the scenarios where: 1) a large  amount of audio data are readily available online but they are untranscribed; 2) a large amount of audio recording is available for an unpopular language about which no structured linguistic knowledge or documentation can be found.  Spoken term discovery is a representative task of unsupervised speech modeling. It aims to discover repetitively occurred words and/or phrases from untranscribed audio.  The problem is commonly tackled with a two-stage approach. In the first stage, a set of subword units are automatically discovered from untranscribed speech data and these units in turn can be used to represent the speech data as a symbol sequence. In the second stage, variable-length sequence matching and clustering are performed on the subword sequence representations. One major drawback of this is that the subword decoding errors in the first stage would propagate to deteriorate the outcome of spoken term discovery in the second stage. The present study investigates the use of Siamese and Triplet networks in spoken term discovery. Siamese network has been commonly applied to pattern classification or matching problems when only weak labels are available. We propose to train a Siamese/Triplet network with a small dataset of matched and mismatched sequence pairs obtained and use the trained network to generate feature representations for unseen subword sequences. The training dataset is constructed based on hypothesized spoken term clusters from an baseline spoken term discovery system developed in our previous study. With the new feature representations learned by the Siamese/Triplet network, re-clustering of subword sequences is carried out to generate an improved set of discovered spoken terms.           The clusters can be as compatible as the baseline model.   The operation of the proposed  system is compatible to the baseline model. This shows that even in a completely unsupervised scenario, a well-performing Siamese network can still be trained with segments and soft labels generated in unsupervised manner. By maintaining a high confidence of hypothesized segment labels, the network is capable to generate segment representations  on new unseen segments    for spoken term discovery.   It is noted that referring from clustering results of other spoken term discovery system is only one of the complete unsupervised methods to obtain segment boundaries and cluster information for generating soft labels for the Siamese network training.    This method is specifically considered in this work for baseline comparison.  Other segmentation and confident data generation approaches are also feasible.    The term clusters discovered by  and  exhibit different properties and work favorably on different types of segments. Post-processing work in combining the term clusters from these two systems can be considered to improved the overall term discovery performance. Multiple clusters representation of shorter terms can also be grouped together. One way is to learn the semantic relationship between clusters by treating the segments as words for word2vec training .  , which had been shown to be possible on audio segments .    Clusters with close semantic relationship and small segment representation distances can be combined.    similar meaning reflected in the learnt representations and with similar segment features can be combined.     Depending on the goal of the clustering, alternative clustering algorithms can be considered. If we are aiming to remove noise from the segment candidates, HDBCAN might be a good choice.  But if we are aiming for full coverage of all the possible words in the recording, then other clustering algorithms such as BPGMM can be considered to assign all segment candidates into a specific term cluster.    Alternative clustering    In this work, the attempt of using Siamese and Triplet networks for spoken term discovery under a complete unsupervised scenario is made. The initial segmentation and cluster information is obtained from other spoken term discovery system. The clusters with high confidence are used to generate matched and mismatched pairs and tuples for training the Siamese and Triplet networks. The networks are used to generate representations for all the available segments, follow by HDBSCAN on the segment representations to obtain new set of spoken term clusters.  It is shown that even the exact labels of the segments are unavailable, Siamese/Triplet network can still be trained when a small set of high confidence matched and mismatched data pairs are presented.  This shows that even in a completely unsupervised scenario, a well-performing Siamese/Triplet network can still be trained with segments and soft labels generated in unsupervised manner. By maintaining a high confidence of hypothesized segment labels, the network is capable to generate segment representations for spoken term discovery.  The segment representations generated by Siamese and Triplet networks can outperform the baseline two-stage model. In the lecture recording experiment, the result is not conclusive for Triplet network. However, experiment on Zerospeech dataset shows that Triplet network is slightly better than Siamese network in learning segment representations for spoken term discovery when trained on sufficient data.   Triplet network is less favourable than Siamese network in generating segment representations for spoken term discovery.   In the problem of spoken term discovery, Triplet network is less favourable in our experiments as the cluster boundaries are less easy to determine by clustering algorithm.          
"," Spoken term discovery from untranscribed speech audio could be achieved via a two-stage process. In the first stage, the unlabelled speech is decoded into a sequence of subword units that are learned and modelled in an unsupervised manner. In the second stage, partial sequence matching and clustering are performed on the decoded subword sequences, resulting in a set of discovered words or phrases. A limitation of this approach is that the results of subword decoding could be erroneous, and the errors would impact the subsequent steps. While Siamese/Triplet network is one approach to learn segment representations that can improve the discovery process, the challenge in spoken term discovery under a complete unsupervised scenario is that training examples are unavailable. In this paper, we propose to generate training examples from initial hypothesized sequence clusters. The Siamese/Triplet network is trained on the hypothesized examples to measure the similarity between two speech segments and hereby perform re-clustering of all hypothesized subword sequences to achieve spoken term discovery.  Experimental results show that the proposed approach is effective in obtaining training examples for Siamese and Triplet networks, improving the efficacy of spoken term discovery as compared with the original two-stage method.",370
" Evidence-based medicine  is a medical practice that aims to find all the evidence to support medical decisions. This evidence nowadays is obtained from biomedical journals, usually accessible through online databases like PubMed and EMBASE, which provide free access to articles' abstracts and in some cases, to full articles. In the context of the COVID-19 pandemic, EBM is critical to making decisions at the individual level and public health since research articles address topics like treatments, adverse cases, and effects of public policies in medicine. The EBM foundation Epistemonikos has made essential contributions by curating and publishing updated guides of what treatments are working and not against COVID-19~\footnote{http://epistemonikos.org/}. Epistemonikos addresses EBM by a combination of software tools for data collection, storage, filtering , and retrieval, as well as by the vital labor of volunteer physicians who curate and label research articles based on quality , type  and PICO labels . However, this workflow has been challenged during 2020 by increasing growth and rapidly evolving evidence of COVID-19 articles published in the latest months. Moreover, to ensure the rapid collection of the latest evidence published, pre-print repositories such as medRXiv and bioRXiv have been added to the traditional online databases. % In order to support Epistemonikos' effort to filter and curate the flood of articles related to COVID-19, we present the results of an applied AI project where we implement and evaluate a text classification system to filter and categorize research articles related to COVID-19. The current model, based on Random Forests, has an acceptable performance classifying systematic reviews  but fails on classifying other document categories. In this article, we show how using BioBERT yields marginal improvements, while XLNET results in significant progress with the best performance. These results save a considerable amount of time from volunteer physicians by pre-filtering the articles worth of manual curation and labeling for EBM. In average, a physician takes two minutes in reviewing one article, while the system we present in this article can review up to  within one hour.   %With the help of volunteer physicians, they classify emergent literature for the COVID-19 virus in systematic reviews, broad syntheses, or primary studies, which is the first step for finding relevant clinical evidence. Until now, they produced a Random Forest model for classifying documents into different categories. However, in this paper, we show how the use of Transformers-based Language Models  helped this foundation save significant effort to their collaborators.   %  In this study, we have compared three methods, one of which is currently in production at the Epistemonikos foundation, the random forest. The others are BioBERT, which, although it is based on the transformer architecture, does not achieve the results shown by XLNET. Having such reliable results means a big impact in times of the COVID-19 pandemic where there is an exponential growth of available literature. In future work we will incorporate explanations obtained from transformer attention mechanisms, compare them against other explanation methods like LIME or SHAP, and conduct a user study to assess whether physicians' work is facilitated by this feature.  
","  The COVID-19 has brought about a significant challenge to the whole of humanity, but with a special burden upon the medical community. Clinicians must keep updated continuously about symptoms, diagnoses, and effectiveness of emergent treatments under a never-ending flood of scientific literature. In this context, the role of evidence-based medicine  for curating the most substantial evidence to support public health and clinical practice turns essential but is being challenged as never before due to the high volume of research articles published and pre-prints posted daily. Artificial Intelligence can have a crucial role in this situation. In this article, we report the results of an applied research project to classify scientific articles to support Epistemonikos, one of the most active foundations worldwide conducting EBM. We test several methods, and the best one, based on the XLNet neural language model, improves the current approach by 93\% on average F1-score, saving valuable time from physicians who volunteer to curate COVID-19 research articles manually.",371
"  The natural language processing community has made tremendous progress  in using pre-trained language models to improve predictive accuracy . Models have now surpassed human performance on language understanding benchmarks such as SuperGLUE . However, studies have shown that these results are partially driven by these models detecting superficial cues that correlate well with labels but which may not be useful for the intended underlying task . This brittleness leads to overestimating model performance on the artificially constructed tasks and poor performance in out-of-distribution or adversarial examples.  A well-studied example of this phenomenon is the natural language inference dataset MNLI . The generation of this dataset led to spurious surface patterns that correlate noticeably with the labels.  highlight that negation words  are often associated with the contradiction label.  show that a model trained solely on the hypothesis, completely ignoring the intended signal, reaches strong performance. We refer to these surface patterns as dataset biases since the conditional distribution of the labels given such biased features is likely to change in examples outside the training data distribution .  A major challenge in representation learning for NLP is to produce models that are robust to these dataset biases. Previous work  has targeted removing dataset biases by explicitly factoring them out of models. These works explicitly construct a biased model, for instance, a hypothesis-only model for NLI experiments, and use it to improve the robustness of the main model. The core idea is to encourage the main model to find a different explanation where the biased model is wrong. During training, products-of-experts ensembling  is used to factor out the biased model.   While these works show promising results, the assumption of knowledge of the underlying dataset bias is quite restrictive. Finding dataset biases in established datasets is a costly and time-consuming process, and may require access to private details about the annotation procedure, while actively reducing surface correlations in the collection process of new datasets is challenging given the number of potential biases .  In this work, we explore methods for learning from biased datasets which do not require such an explicit formulation of the dataset biases. We first show how a model with limited capacity, which we call a weak learner, trained with a standard cross-entropy loss learns to exploit biases in the dataset. We then investigate the biases on which this weak learner relies and show that they match several previously manually identified biases. Based on this observation, we leverage such limited capacity models in a product of experts ensemble to train a more robust model and evaluate our approach in various settings ranging from toy datasets up to large crowd-sourced benchmarks: controlled synthetic bias setup , natural language inference  and extractive question answering .  Our contributions are the following:  we show that weak learners are prone to relying on shallow heuristics and highlight how they rediscover previously human-identified dataset biases;  we demonstrate that we do not need to explicitly know or model dataset biases to train more robust models that generalize better to out-of-distribution examples;  we discuss the design choices for weak learners and show trade-offs between higher out-of-distribution performance at the expense of the in-distribution performance.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   We have presented an effective method for training models robust to dataset biases. Leveraging a weak learner with limited capacity and a modified product of experts training setup, we show that dataset biases do not need to be explicitly known or modeled to be able to train models that can generalize significantly better to out-of-distribution examples. We discuss the design choices for such weak learner and investigate how using higher-capacity learners leads to higher out-of-distribution performance and a trade-off with in-distribution performance. We believe that such approaches capable of automatically identifying and mitigating datasets bias will be essential tools for future bias-discovery and mitigation techniques.                                                                            \clearpage  
"," State-of-the-art natural language processing  models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations.  Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.",372
" Humor plays an important role in social communications. Unlike many objective classification tasks, the task of humor recognition is constrained by its subjectivity. The perception of the same joke can differ among people due to individual differences in their cognitive processes responsible for humor processing , which is as illustrated in Figure . This makes it challenging for humor recognition models to generalize to a wide range of users, as the training data may reflect the subjectivity of the annotators  or experiment participants . To achieve personalized humor recognition, it is necessary to consider the diversity of user preferences.    Previous research on automated humor recognition casts the task as a binary classification problem . These methods mainly focus on how to design humor-related linguistic features as input to a classifier to obtain high classification performance. With well-established computational humor theories , they can curate many heuristics to extract informative features. The key of heuristic rules is to design effective approaches to capture linguistic patterns  or n-gram statistics  that can distinguish humorous text from plain text. These methods are able to characterize intra-sentence and inter-sentence dependencies that are unique to humor, and thus do not rely a lot on the complexity of classifiers. Nevertheless, the feature generation process requires significant efforts and many have difficulties to cope with newly encountered terms .    Deep learning shifts the focus of AI research from feature engineering to automatic feature extraction. Convolutional Neural Networks  and Transformer-based language models  have been used for end-to-end humor recognition.  Most of previous studies are conducted on curated and explicitly balanced datasets  with the underlying assumption that people more or less agree on the distinction between humorous and non-humorous text. This assumption could limit the model's ability to generalize in practice.  Federated Learning, a technique that trains a deep neural network based on iterative averaging of decentralized local updates, has been proved to be good at handling unbalanced and non-IID data distributions . Inspired by recent progress of federated learning in diversity  and personalization , we propose to improve the ability of humor recognition models to generalize to diverse user preferences with the help of federated learning. We name the model . Specifically, we adopt the Federated Averaging  algorithm  in the fine-tuning of a pretrained Transformer-based language model on our task, and employ a diversification strategy  to handle disparate user preferences.   The main idea of our solution is to force the humor recognition model to learn from a diverse range of user preferences, thereby enhancing the adaptability to new users. For this purpose, there are two important issues to consider. First, as users are increasingly aware of privacy issues and reluctant to provide personal information , it is imperative that we preserve users' privacy and avoid direct harvesting of explicit user preference from their personal devices. To address this, we propose an approximation strategy to generate implicit user feedback  on given humorous text and we diversify the label distributions to represent diverse user preferences. Second, marginal distributions of user preferences  often lead to salient class imbalance issue which requires us to select a more suitable evaluation metric rather than widely adopted accuracy. As such, we use F1 score to evaluate and select best models.   To the best of our knowledge, FedHumor is the first federated learning-based humor recognition model. Extensive results show that our approach is able to increase the generalization bounds of the humor recognition model compared to 9 other state-of-the-art approaches. It is a promising approach to help future AI applications recommend suitable humorous texts to users under tightened data privacy protection regulations , thereby enabling innovative and emerging forms of human-AI interaction.    In this paper, we propose the FedHumor approach - a humorous text recognition model following the federated learning paradigm which can provide personalized humor recognition based on labels stored in distributed sources. It is able to account for diversity in each person's activation point for perceived funniness for the same text contents. Through extensive experiments comparing FedHumor with 9 state-of-the-art approaches, we show that it is able to achieve better personalization when recognizing humor from text contents. To the best of our knowledge, it is the first federated learning-based personalized humorous text recognition model.         
"," Understanding humor is critical to creative language modeling with many applications in human-AI interaction. However, due to differences in the cognitive systems of the audience, the perception of humor can be highly subjective. Thus, a given passage can be regarded as funny to different degrees by different readers. This makes training humorous text recognition models that can adapt to diverse humor preferences highly challenging. In this paper, we propose the FedHumor approach to recognize humorous text contents in a personalized manner through federated learning . It is a federated BERT model capable of jointly considering the overall distribution of humor scores with humor labels by individuals for given texts. Extensive experiments demonstrate significant advantages of FedHumor in recognizing humor contents accurately for people with diverse humor preferences compared to 9 state-of-the-art humor recognition approaches.",373
"  .     %      % % final paper: en-us version      %       % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. } Rhetorical Structure Theory   is one of the most influential theories of discourse analysis, under which a document is represented by a hierarchical discourse tree. As shown in Figure a, the leaf nodes of an RST tree are text spans named Elementary Discourse Units , and the EDUs are connected by rhetorical relations  to form larger text spans until the entire document is included.  The rhetorical relations are further categorized to Nucleus  and Satellite  based on their relative importance. Thus, document-level discourse parsing consists of three sub-tasks: tree construction, nuclearity determination and relation classification. Moreover, downstream natural language processing tasks can benefit from RST-based structure-aware document analysis, such as summarization  and machine comprehension .  By utilizing various linguistic characteristics , statistical approaches have obtained substantial improvement on the English RST-DT benchmark . Recently, neural networks have been making inroads into discourse analysis frameworks, such as attention-based hierarchical encoding  and integrating neural-based syntactic features into a transition-based parser . Lin et al.  and their follow-up work  successfully explored encoder-decoder neural architectures on sentence-level discourse analysis, with a top-down parsing procedure.  Although discourse parsing has received much research attention and progress, the models are mainly optimized and evaluated in English. The main challenge is the shortage of annotated data, since manual annotation under the RST framework is labor-intensive and requires specialized linguistic knowledge. For instance, the most popular benchmark English RST-DT corpus  only contains 385 samples, which is much smaller than those of other natural language processing tasks. The treebank size of other languages such as German , Dutch  and Basque  are even more limited. Such limitations make it difficult to achieve acceptable performance on these languages required to fully support downstream tasks, and also lead to poor generalization ability of the computational approaches.  Since the treebanks of different languages share the same underlying linguistic theory, data-driven approaches can benefit from joint learning on multilingual RST resources . Therefore, in this paper, we investigate two methods to build a cross-lingual neural discourse parser:  From the embedding perspective: with the cross-lingual contextualized language models, we can train a parser on the shared semantic space from multilingual sources without employing a language indicator;  From the text perspective: since each EDU is a semantically-cohesive unit, we can unify the target language space by EDU-level translation, while preserving the original EDU segmentation and the discourse tree structures . To this end, we adapted and enhanced an end-to-end neural discourse parser, and investigated the two proposed approaches on 6 different languages. While the RST data for training is still in a small scale, we achieved the state-of-the-art performance on all fronts, significantly surpassing previous models, and even approaching the upper bound of human performance. Moreover, we conducted a topic modeling analysis on the collected multilingual treebanks to evaluate the model generality across various domains.       In this paper, we investigated two approaches for cross-lingual neural discourse parsing. Experimental results show that both utilizing cross-lingual representation and adopting segment-level translation contribute to obtaining state-of-the-art performance on various treebanks. Moreover, monolingual models can also benefit from cross-lingual training by introducing data from more domains. For future work, we consider conducting domain adaption via few-shot learning to make our approach more generalizable.  
"," Text discourse parsing plays an important role in understanding information flow and argumentative structure in natural language. Previous research under the Rhetorical Structure Theory  has mostly focused on inducing and evaluating models from the English treebank. However, the parsing tasks for other languages such as German, Dutch, and Portuguese are still challenging due to the shortage of annotated data. In this work, we investigate two approaches to establish a neural, cross-lingual discourse parser via:  utilizing multilingual vector representations; and  adopting segment-level translation of the source content. Experiment results show that both methods are effective even with limited training data, and achieve state-of-the-art performance on cross-lingual, document-level discourse parsing on all sub-tasks. \newline",374
" In recent years, smart devices with built-in personal assistants like Google Assistant and Siri are becoming omnipresent. Behind these intelligent systems, a key question is how to identify the underlying intent of a user utterance, which has triggered a large amount of work on intent detection . Most existing intent detection systems are built on deep learning models trained on large-scale annotated data. However, as user demands and the functions of smart devices continue to grow, collecting supervised data for every new intent becomes time-consuming and labor-intensive.  To address this issue, some studies tackle intent detection in the zero-shot learning  manner, attempting to utilize the learned knowledge of seen classes to help detect unseen classes. The recent methods of zero-shot intent detection  can be roughly divided into two categories: The first category , referred to as the transformation-based methods, utilizes word embeddings of label names to establish a similarity matrix, which is then used to transfer the prediction space of seen intents to unseen intents. Another line of work is based on the compatibility-based methods , which aims to encode the label names and utterances into representations in the same semantic space and then calculate their similarity. In both kinds of methods, a critical problem is learning intent representations. However, most existing ZSID methods are class-inductive, which relies entirely on labeled data from seen intents in the training stage. Consequently, the representations of unseen intents cannot be learned, resulting in two limitations.  First, the ZSID methods are not good at modeling the relationship between seen and unseen intents. For the transformation-based methods, when the label names are given in the form of raw phrases or sentences, word embeddings of label names are inadequate to associate the connections between seen and unseen intents. For example, 閳ユ窂ookRestaurant閳 is similar to 閳ユ珐ateBook閳 when measured by word embeddings, as they share the word 閳ユ窂ook閳 . However, the meaning of these two intents are not that relevant. % As a result, the computed similarity matrix is inadequate in associating the connections between seen and unseen intents .  For the compatibility-based methods, they minimize the similarity between seen intent samples and seen label names in a shared semantic space, and directly transfer it to detect unseen intents. Since the unseen intent representations are not learned, they might be entangled with the representations of seen intents. This can severely hurt the accuracy of the predicted label-utterance similarity, especially when the expressions of utterances are diverse. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Second, the vanilla ZSL methods are not applicable to generalized intent detection . Compared with the ZSL setting , which assumes that the models are only presented with utterances from unseen classes at test time, GZSID requires the model to detect both seen and unseen intents. In GZSID, existing ZSL models usually suffer from the dubbed domain shift  problem, in which utterances from unseen intents are almost always mistakenly classified into seen intents.   Unlike the class-inductive methods, class-transductive ZSL uses semantic information about the unseen classes for model training . In the context of intent detection, the label name provides a proper sketch of the intent meaning. Motivated by this, we propose to utilize label names of the unseen intents to learn disentangled intent representations . Specifically, we include the unseen intents into the prediction space during training, with the label names serving as the pseudo utterances. This allows the model to learn the boundary of each seen and unseen class in the semantic space. Under this framework, we introduce an assistant task that forces the model to find the distinction between seen and unseen intents, thereby alleviating the domain-shift problem. On this basis, we refine the word embedding based similarity matrix by averaging the representations of all corresponding  utterances and  label names. As a result, we can better capture the intent meanings and the similarity matrix reflects more accurate intent connections.   In summary, our contribution is three-fold: %{\topsep}{0pt} % %{-1pt} {0pt} {0pt} %        We believe that the potential of class-transductive ZSL  in intent detection is still not fully exploited, to encourage more related studies in the future, we will release our codes and data.       In this paper, we propose a class-transductive framework to overcome the limitations of existing ZSID models. The framework learns disentangled representations for unseen intents by including them into the prediction space during training. Under the DIR framework, we present a multi-task learning objective in the training stage to encourages the model to learn the distinctions between unseen and seen intents. In the inference stage, we develop a similarity scorer, which can better associate the inter-intent connections based on the learned representations. Experiments on two benchmarks show that DIR is effective and robust, which can bring considerable improvement to ZSID systems with different zero-shot learning strategies and backbone networks.    
"," Zero-shot intent detection  aims to deal with the continuously emerging intents without annotated training data. However, existing ZSID systems suffer from two limitations: 1) They are not good at modeling the relationship between seen and unseen intents, when the label names are given in the form of raw phrases or sentences. 2) They cannot effectively recognize unseen intents under the generalized intent detection  setting. A critical factor behind these limitations is the representations of unseen intents, which cannot be learned in the training stage. To address this problem, we propose a class-transductive framework that utilizes unseen class labels to learn Disentangled Intent Representations . Specifically, we allow the model to predict unseen intents in the training stage, with the corresponding label names serving as input utterances. Under this framework, we introduce a multi-task learning objective, which encourages the model to learn the distinctions among intents, and a similarity scorer, which estimates the connections among intents more accurately based on the learned intent representations. % Moreover, we present a novel approach to calculate the inter-intent similarities, on the basis of the learned intent representations, which estimates the connections among intents more accurately.  Since the purpose of DIR is to provide better intent representations, it can be easily integrated with existing ZSID and GZSID methods. Experiments on two real-world datasets show that the proposed framework brings consistent improvement to the baseline systems, regardless of the model architectures or zero-shot learning strategies.",375
" Multi-turn open-domain dialogue modeling is an active research topic in the field of natural language processing.  However, generating a coherent and informative response for a given dialogue context remains a challenge. % However, it is still challenging for dialogue models to generate a coherent and informative response for a given dialogue context. %Research in this domain mainly addresses the following two questions: 1) How can we learn to represent the context? 2) In the presence of context representation, how can we infer the distribution of the response?  A critical challenge is the learning of rich and robust context representations of dialogue utterances~, namely the challenge of encoding a dialogue context into a vector that adequately captures the semantics . % A major challenge in this domain is to learn rich and robust context representations of dialogue utterances~, namely the challenge of encoding a dialogue context into a vector that adequately captures the semantics .  Large-scale pre-training language models using Transformer-based architectures have recently achieved remarkable successes in a variety of NLP tasks~. % Recently, large-scale pre-training language models using Transformer-based architectures have achieved remarkable successes in a variety of NLP tasks~.  As such, there are increasingly work that aims to use pre-training language models for conversation modeling~. For example, DialoGPT~ extends the GPT-2~ to generate conversation responses on large-scale dialogue corpus. Meena~ trains a sequence-to-sequence model~ with the Evolved Transformer~ on large-scale multi-turn conversations.  Blender, developed by Facebook, provides recipes for building open-domain chatbots that perform well in human evaluations~.  However, existing pre-training conversation models usually view the dialogue context as a linear sequence of tokens and learns to generate the next word through token-level self-attention.  One issue of this approach is that the high-level relationships between utterances are harder to capture using word-level semantics. % One issue of this approach is that the relationships between utterances are scattered into individual words, hindering the capturing of discourse-level coherence.  For example, the discourse-level relationship between the utterances  and   is apparent, but word-level comparisons, such as ,  and , , obscures the high-level relationship. % For example, the utterance  and  in Figure have a strong certain relationship, by contrast, pairs of individual words in these two utterances such as ,  and ,  have obscure correlations. Furthermore, this full pairwise attention is inefficient since it requires each word in the context and the decoder to interact with all other words regardless of their distances and semantic units. % Furthermore, the full pairwise attention is inefficient since it requires each word in the context and the decoder to interact with all other words regardless of their distances and semantic units.  To alleviate the issues above, we present DialogBERT, a novel conversational response generation model.  % To alleviate the aforementioned issues, we present DialogBERT, a novel conversational response generation model.  DialogBERT employs a hierarchical Transformer architecture to represent the dialogue context.  It first encodes dialogue utterances through a Transformer encoder and then encodes the resulting utterance vectors using a discourse-level Transformer to obtain a representation of the entire dialogue context.  To efficiently capture discourse-level coherence among utterances, we propose two training objectives in analogy to the original BERT training: 1) masked context regression, which masks a randomly-selected utterance and predicts the encoding vector for the masked utterance directly; and 2) distributed utterance order ranking, which %reconstructs the order of utterances that belong to the same dialog context   organizes randomly shuffled utterances of a conversation into a coherent dialogue context  through a ~ neural network.  We evaluate DialogBERT on popular multi-turn conversation datasets, namely Weibo, MultiWOZ and DailyDialog.  Results show that DialogBERT outperforms baselines in terms of perplexity, BLEU, and NIST. Human evaluation supports the superiority of our approach in capturing discourse-level semantics and generating more plausible dialogue responses.  %Our contributions can be summarized as follows: % %     %    . %     %      %    for encouraging the model to be more aware of relationships among dialog utterances and, ultimately, discourse coherence. %     %         In this paper, we proposed a neural response generation model named DialogBERT.  Instead of encoding the dialogue context as a linear sequence of tokens, DialogBERT employs a hierarchical Transformer encoder architecture.  : utterances in the dialogue context are first encoded into vectors by an utterance encoder before fed into a context encoder which learns the context sensitive encoding.  As a natural extension of the original BERT training, we proposed two training objectives: masked utterance regression and distributed utterance re-ordering. We showed that the proposed objectives enable the conversation model to capture multi-level  coherences. Additionally, we showed that DialogBERT notably outperforms baseline models on the response generation tasks.   KMY: how about future work?    
"," Recent advances in pre-trained language models have significantly improved neural response generation.  However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention.  Such token-level encoding hinders the exploration of discourse-level coherence among utterances.  This paper presents DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. DialogBERT employs a hierarchical Transformer architecture.  To efficiently capture the discourse-level coherence among utterances, we propose two training objectives, including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training.  Experiments on three multi-turn conversation datasets show that our approach remarkably outperforms the baselines, such as BART and DialoGPT, in terms of quantitative evaluation.  The human evaluation suggests that DialogBERT generates more coherent, informative, and human-like responses than the baselines with significant margins. % Pre-trained language models  have been successfully adapted to neural response generation.  % However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention. % Such token-level encoding hinders the exploration of discourse-level coherence among utterances. % In this paper, we present DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. % %In order to model the utterance-level interactions, % Instead of a flat encoding of linear tokens, DialogBERT employs a hierarchical Transformer architecture.  % %DialogBERT consists of an utterance encoder for encoding utterances and a context encoder for learning to contextualize given utterances' representations. % To efficiently capture the discourse-level coherence among utterances, we propose two new training objectives including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training.  % Experiments on three multi-turn conversation datasets show that  % our approach remarkably outperforms three baselines such as BART and DialoGPT in terms of quantitative evaluation.  % Human evaluation  % suggests  % %\jw{supports}  % that DialogBERT generates more coherent, informative and human-like responses than the baselines with significant margins.",376
" Event Detection ,  the task of which involves identifying the boundaries of event triggers and classifying them into the corresponding event types, aims to seek recognize events of specific types from given texts. As a fundamental task of information extraction, many high-level NLP tasks, such as information retrieval and question answering, need an event detector as one of their essential components.    % 娑旂喕顩﹂崝鐘茬穿閻 Recent studies show that English ED models have achieved great performance by treating the problem as a word-by-word sequence labeling task.  Different from English ED, many East Asian languages, including Chinese, are written without explicit word boundary, resulting a much tricky ED task. An intuitive solution is to apply Chinese Word Segmentation  tools first to get word boundaries, and then use a word-level sequence labeling model similar to the English ED models.  However, word boundary is ambiguous in Chinese thus word-trigger mismatch problem exists in Chinese ED, where an event trigger may not exactly match with a word, but is likely to be part of a word or cross multiple words as Figure demonstrates. Meanwhile, character-level sequence tagging is able to alleviate this problem, but Chinese character embedding can only carry limited information due to the lack of word and word-sequence information, resulting to ambiguous semantics. {UTF8}{gbsn} 	For example in Figure, the character ``閹''  in lexicon word ``閹舵洝绁'' and ``閹舵洘骞'' has entirely different meanings, triggering the event of ``Transaction:TransferMoney'' and ``Conflict:Attack'', respectively.  % Therefore, how to better integrate segmentation-related information and character-level semantics is a key feature in Chinese ED models.  Several recent works have demonstrated that considering the lexicon word information could provide more exact information to discriminate semantics of characters. ~ designed NPN, a CNN-like network to model character compositional structure of trigger words and introduced a gate mechanism to fuse information from characters and words. ~~ proposed TLNN, a trigger-aware Lattice LSTM architecture, exploiting semantics from matched lexicon words to improve Chinese ED.    Although these methods have achieved great success, they continue to have difficulty in fully exploiting the interaction between characters and lexicon words. Specifically, for each character, NPN exploits a gate mechanism to fuse its information with one corresponding word. This means that each character could only be incorporated with one matched word, but actually one character is likely to match with several words, leading to information loss. For TLNN, it constructs cut paths to link the start and end character for each matched word, but semantic information of the matched lexicon word fails to flow into all the characters it covers except the last one, due to the inherently unidirectional sequential nature of Lattice LSTM. %For characters without matched words, no extra information is provided enhance its representation.  Besides, previous ED works usually ignore semantic information maintained by the event types. We observe that event types are usually semantically related to the corresponding event triggers.  {UTF8}{gbsn} 	For example, some common event triggers of type ``Conflict:Attack'', such as ``hit'', ``strike'' and ``invade'', are specific behaviors of ``Attack''.  Such an observation shows that considering the semantic information of event labels may provide fine-grained semantic signals to guide the detection of event triggers, and accordingly benefit ED performance.  In this paper, we propose a novel neural architecture, named Label Enhanced Heterogeneous Graph Attention Networks , for Chinese ED. To promote better information interaction between words and characters, we transform each sentence into a graph.  We first connect lexicon words with all the characters it covers. And then neighboring characters are also linked with each other to provide local context information to enhance character representations, especially for those without matched lexicon word. To capture different granularity of semantic information from words and characters, we formulate words and characters as two types of nodes, thus a heterogeneous graph attention networks is utilized to enable rich information propagation over the graph. Additionally, we design a matcher module to leverage the semantic information of event labels. Specifically, we transform event labels into an event-trigger-prototype based embedding matrix by summarizing the trigger representations belonging to each event label. Based on the generated event label representation, a margin loss is further exploited to enhance the ability to discriminate confusing event labels. Comparing with previous works, our contributions are as follows:  	    In this paper, we propose a novel architecture, label enhanced heterogeneous graph attention networks model ,  for Chinese ED. To fully exploit information between characters and words, we formulate characters and words as different types of nodes, and connect them with richly functional edges. The heterogeneous graph attention networks is utilized to enable adequate information propagation. Besides, we utilize the semantic clues from event labels to guide the detection of event triggers. Experiment results show that L-HGAT consistently achieves superior performance over previous competing approaches. In the future, we would like to adapt L-HGAT for other information extraction tasks, such as named entity recognition and aspect extraction.    \def\year{2021}\relax  File: formatting-instructions-latex-2021.tex  release 2021.1 \documentclass[letterpaper]{article}   DO NOT CHANGE THIS \usepackage{aaai21}    DO NOT CHANGE THIS \usepackage{times}    DO NOT CHANGE THIS \usepackage{helvet}   DO NOT CHANGE THIS \usepackage{courier}    DO NOT CHANGE THIS \usepackage[hyphens]{url}    DO NOT CHANGE THIS \usepackage{graphicx}   DO NOT CHANGE THIS \urlstyle{rm}   DO NOT CHANGE THIS \def\UrlFont{\rm}    DO NOT CHANGE THIS \usepackage{natbib}    DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption}   DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing    DO NOT CHANGE THIS {8.5in}    DO NOT CHANGE THIS {11in}    DO NOT CHANGE THIS  \usepackage{multirow} \usepackage{amssymb}  \usepackage{booktabs} \usepackage{bm} \usepackage{CJKutf8} \usepackage[switch]{lineno} [2]{} 閺鎯ф躬鐎佃壈鈻堥崠    Leave this   /Title    Put your actual complete title  within the parentheses in mixed case   Leave the space between \Title and the beginning parenthesis alone   /Author    Put your actual complete list of authors  within the parentheses in mixed case.   Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,   remove them.    DISALLOWED PACKAGES   \usepackage{authblk} -- This package is specifically forbidden   \usepackage{balance} -- This package is specifically forbidden   \usepackage{color    \usepackage{CJK} -- This package is specifically forbidden   \usepackage{float} -- This package is specifically forbidden   \usepackage{flushend} -- This package is specifically forbidden   \usepackage{fontenc} -- This package is specifically forbidden   \usepackage{fullpage} -- This package is specifically forbidden   \usepackage{geometry} -- This package is specifically forbidden   \usepackage{grffile} -- This package is specifically forbidden   \usepackage{hyperref} -- This package is specifically forbidden   \usepackage{navigator} -- This package is specifically forbidden       -- This package is specifically forbidden   \usepackage{setspace} -- This package is specifically forbidden   \usepackage{stfloats} -- This package is specifically forbidden   \usepackage{tabu} -- This package is specifically forbidden   \usepackage{titlesec} -- This package is specifically forbidden   \usepackage{tocbibind} -- This package is specifically forbidden   \usepackage{ulem} -- This package is specifically forbidden   \usepackage{wrapfig} -- This package is specifically forbidden   DISALLOWED COMMANDS       \maketitle  
"," Event Detection  aims to recognize instances of specified types of event triggers in text. Different from English ED, Chinese ED suffers from the problem of word-trigger mismatch due to the uncertain word boundaries.  Existing approaches injecting word information into character-level models have achieved promising progress  to alleviate this problem, but they are limited by two issues. First, the interaction between characters and lexicon words is not fully exploited. Second, they ignore the semantic information provided by event labels.  We thus propose a novel architecture named Label enhanced Heterogeneous Graph Attention Networks .  Specifically, we transform each sentence into a graph, where character nodes and word nodes are connected with different types of edges, so that the interaction between words and characters is fully reserved. A heterogeneous graph attention networks is then introduced to propagate relational message and enrich information interaction. Furthermore, we convert each label into a trigger-prototype-based embedding, and design a margin loss to guide the model distinguish confusing event labels. Experiments on two benchmark datasets show that our model achieves significant improvement over a range of competitive baseline methods.",377
" % \rev{@Ileana: this is an example on how to indicate changes in the text, based on the revision.} % \todo[inline]{we need to add color bars on figures, as promised to the reviewers}    Given enough computational power, the scalability of the attention mechanism~ will allow for building ever larger Natural Language Processing  models with billions of parameters . While impressive, these advances also pose a responsibility to the NLP community to interpret the behavior of the hundreds of attention heads in a single model, and potentially to reduce the number of computations. Responding to this challenge, previous work has taken pioneering steps to discover and to explain the sparseness in the attention patters. Here, we argue that as the number of heads grows in the range of thousands, automatic measures would be needed to discover and to impose sparseness to such models.  We introduce a simple task-agnostic data-informed pruning method for attention mechanisms: Attention Pruning. We train Transformer-based models and we analyze  observed attention patterns, averaged over all input sequences in the train set, in order to identify and to remove weak connections between the input tokens. Following , we then retrain these models, enforcing sparseness through masking, and we demonstrate that attention mechanisms incorporate extraneous connections between the input tokens: we obtain comparable  % \question{or even marginally better performance} performance while using sparse attention patterns for NLP tasks such as language and sequence-to-sequence  modelling, as well as %Natural Language  Inference . \rev{prediction on GLUE tasks. Figure summarizes the impact of using our pruning method on standard NLP tasks.}     These global sparseness patterns could help improve both interpretability and inference-time computational efficiency for widely-used attention models. Our contributions are as follows:    % show theoretical computation gains     % The rest of the paper is organized as follows: In Section, we present related work. In Section, we introduce the details behind our attention pruning method. In Section, we apply AP to experiments with language modelling. In Section, we apply AP for seq2seq modelling on machine translation tasks. In Section, we extend our machine translation experiments to demonstrate that AP is compatible with -entmax regularization~, which is another promising sparseness technique. In Section, we study the effect of AP with BERT on the GLUE benchmark. % % Section. In Section we discuss theoretically how our pruned Transformers could yield speedups in terms of MACs.  % In Section, we discuss the hardware efficiency of AP and its promise for speeding up modelling for really long sequences. In Section, we conclude and we point to promising directions for future work.     We motivated Attention Pruning as a novel method for pruning attention by leveraging on data-informed sparseness. By performing controlled experiments on a broad range of tasks , we demonstrated that we can prune most computations using pre-computed attention patterns while maintaining comparable performance, and sometimes even achieving improvements. We further applied our AP method on seq2seq tasks, which allowed us to study attention patterns between self- and cross-attention, and as a result we discovered important distinctions between these two types of attention.    we conducted a controlled study to find means of incorporating positional awareness in attention mechanisms. We observed that positional awareness induces beneficial sparseness in attention matrices, and thus we devised a simple training procedure that exploits that sparseness. As a result, we demonstrated faster and more accurate Transformers.   In future work, we plan to evaluate our method for other models, other NLP tasks, and on datasets of various sizes. We also plan to implement Attention Pruning efficiently for existing hardware. We conjecture that ``co-design'' approaches for efficient sparse kernels  and their successful utilization  would be helpful for making Attention Pruning scalable. Therefore, we release our code, which currently relies on masking matrix multiplications on a GPU, to the community to encourage co-design efforts for attention pruning.  Finally, we would like to explore the usefulness of using AP as a method for guiding modelling in NLP for a larger set of NLP tasks as well as for real-world applications.      
"," The attention mechanism is a key component of the neural revolution in Natural Language Processing . As the size of attention-based models has been scaling with the available computational resources, a number of pruning techniques have been developed to detect and to exploit sparseness in such models in order to make them more efficient. The majority of such efforts have focused on looking for attention patterns and then hard-coding them to achieve sparseness, or pruning the weights of the attention mechanisms based on statistical information from the training data. In this paper, we marry these two lines of research by proposing  : a novel pruning framework that collects observations about the attention patterns in a fixed dataset and then induces a global sparseness mask for the model. Through attention pruning, we find that about 90\% of the attention computation can be reduced for language modelling and about 50\% for machine translation and %natural language inference \rev{prediction with BERT on GLUE tasks}, while maintaining the quality of  the results. Additionally, using our method, we discovered important distinctions between self- and cross-attention patterns, which could guide future NLP research in attention-based modelling. Our approach could help develop better models for existing or for new NLP applications, and generally for any model that relies on attention mechanisms. Our implementation and instructions to reproduce the experiments are available at \url{https://github.com/irugina/AP}.",378
" DEEP learning  is a modern machine learning technique based on artificial neural networks. The field of natural language processing  has significantly benefited from the use of deep learning techniques in recent years . There are three prevalent deep learning architectures concerned with  NLP tasks: long-short term memory   %networks , transformer networks  and convolutional neural networks  . LSTMs exhibit relatively slow inference speeds and are less performant than transformers and CNNs with regards to text classification accuracy . Transformers are a recent innovation and have shown significant successes in many NLP tasks . Their massive complexity with trainable parameters in the order of hundreds of millions presents critical  challenges to researchers. State-of-the-art transformers are difficult to reproduce in lab conditions as they have a high training cost in monetary terms. There are only a limited number of pre-trained transformer models available for different languages. \par CNNs have demonstrated excellent success in text classification tasks . There are two paradigms available when using CNNs for text classification tasks, namely: world-level   and character-level CNNs . \par Word-level approaches are dependant on a word-model to represent the text. The reliance on a pre-trained word-model poses the potential problem of not having one available for a particular language. Training new word models is computationally time-consuming and costly. There is also the technical challenges of dealing with misspellings and words that may not exist in the word-model. The other paradigm is char-CNNs. No pre-trained language or word models are required. They also do not require a costly pre-processing step of the text data. In general, char-CNNs are not as accurate as word-level CNNs or transformers. Adding depth has not given the benefit of improved classification accuracy, as seen in image classification tasks. There is an open question in the research literature of what is the optimal architecture for char-CNNs. Little research has been performed to address these limitations. Deep learning is an iterative process requiring the tuning of many hyper-parameters and repeated experiments to test the efficacy of any potential architecture. It is a time consuming, costly and a tedious process that requires expert skills and domain knowledge. The task of finding optimal char-CNNs is an NP-hard problem. \par Evolutionary computation   is a collection of search algorithms inspired by the principals of biological evolution, in particular the concept of . EC methods use a population of individuals  to conduct a simultaneous search during a limited time frame to improve the optimisation of a specified objective function via the exchange of information between individuals in the population. The exchange of information is one of the key motivating factors of selecting EC methods for evolving char-CNNs in this work. There is the potential that this information exchange may reveal the essential characteristics of what makes a non-performant char-CNN into a performant one. EC methods are concerned with locating near-optimal solutions to NP-hard problems. \par Evolutionary deep learning  is the technique of using EC methods to search for candidate CNN architectures combined with the backpropagation algorithm to train any potential candidate network architecture. EDL has demonstrated success when searching for performant CNN architectures on image classification tasks . EDL has not been used to search for performant char-CNN architectures. \par Motivated by the success of applying EDL techniques in the image classification domain, we propose a novel surrogate-based EDL algorithm appropriate for searching the landscape of char-CNN architectures for the text classification domain. The proposed algorithm is based on genetic programming  and an indirect encoding that is capable of representing novel char-CNN  architectures. The algorithm employs the use of surrogate models to significantly reduce the training time of the candidate char-CNNs during the evolutionary process.  In summary, the contributions of the proposed algorithm and work are:     %------------------------------------------------------------------------------   This work proposed an evolutionary deep learning approach to discover performant char-CNN architectures. This goal was achieved through the implementation of a genetic programming-based algorithm  coupled with a reduced cellular encoding scheme and the backpropogation algorithm. The SurDG-EC algorithm located, on average, higher accuracy models than those located by SurDG-Random. The fittest evolved phenotype defeated one of the state-of-the-art char-CNN models and achieved comparable results to the state-of-the-art VDCNN-29 architecture. The evolved model also generalised favourably across most unseen datasets. There is clear evidence that width may potentially add to the efficacy of char-CNNs.This does not mean that width will always result in increased accuracy, as also observed in the results. There are many other factors to consider. It is not known how much of the efficacy of the evolved phenotypes are due to increased width or some other unknown variable or combination of variables. There are, however, clear indications that the importance of width should be further researched. The SurDG-EC algorithm also revealed two interesting properties of char-CNNs. Building a rich tapestry of feature representations at the early stages of the network potentially aids in improving the accuracy of the networks as they grow deeper - in turn constructing a hierarchy of relations from this rich feature tapestry. The evolutionary crossover operation also revealed that combing the widths of two phenotypes produced a wider phenotype with greater validation accuracy. This is a further clue that there may be value in making char-CNNs with increased width.   ------------------------------------------------------------------------------ 
"," Character-level convolutional neural networks  require no knowledge of the semantic or syntactic structure of the language they classify. This property simplifies its implementation but reduces its classification accuracy. Increasing the depth of char-CNN architectures does not result in breakthrough accuracy improvements. Research has not established which char-CNN architectures are optimal for text classification tasks. Manually designing and training char-CNNs is an iterative and time-consuming process that requires expert domain knowledge. Evolutionary deep learning  techniques, including surrogate-based versions, have demonstrated success in automatically searching for performant CNN architectures for image analysis tasks. Researchers have not applied EDL techniques to search the architecture space of char-CNNs for text classification tasks. This article demonstrates the first work in evolving char-CNN architectures using a novel EDL algorithm based on genetic programming, an indirect encoding and surrogate models, to search for performant char-CNN architectures automatically. The algorithm is evaluated on eight text classification datasets and benchmarked against five manually designed CNN architectures and one long short-term memory  architecture. Experiment results indicate that the algorithm can evolve architectures that outperform the LSTM in terms of classification accuracy and five of the manually designed CNN architectures in terms of classification accuracy and parameter count.",379
" .     %     % % final paper: en-us version     %     %   % space normally used by the marker     % This work is licensed under a Creative Commons     % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } Pre-trained language models have received great interest in the natural language processing  community in the last recent years . These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence . Then, transfer learning  can be used to leverage the learned knowledge for a down-stream task, such as text-classification .   introduced the  ``Bidirectional Encoder Representations from Transformers'' , a pre-trained language model based on the Transformer architecture . BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context . The fact is, BERT has achieved state of the art results on the ``General Language Understanding Evaluation''  benchmark  by only training a single, task-specific layer at the output and fine-tuning the base model for each task. Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis , relation extraction  and word sense disambiguation , as well as its adaptability to languages other than English . However, the fine-tuning data set often contains thousands of labeled data points. This plethora of training data is often not available in real world scenarios .  In this paper, we focus on the low-resource setting with less than 1,000 training data points. Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT. That leads to the next question: How can layer freezing techniques , i.e. reducing the parameter space, impact model training convergence with fewer data points?  To answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty  for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model. To the best of our knowledge, the work presented in this paper is the first demonstration of combining modern transfer learning using pre-trained Transformer-based language model such as the BERT model with active learning to improve performance in low-resource scenarios. Furthermore, we explore the effect of trainable parameters reduction on model performance and training stability by analyzing the layer-wise change of model parameters to reason about the selection of layers excluded from training. %Furthermore, we explore whether a more sophisticated decoder architecture, i.e. convolutional neural networks  can improve the overall performance or if the added complexity hinders a fast model adaption with such little training data.  The main findings of our work are summarized as follows: a) we found that the model's classification uncertainty on unseen data can be approximated by using Bayesian approximations and therefore, used to efficiently select data for manual labeling in an active learning setting; b) by analyzing layer-wise change of model parameters, we found that the active learning strategy specifically selects data points that train the first and thus more general natural language understanding layers of the BERT model rather than the later and thus more task-specific layers.    In this paper, we evaluated the performance of a pre-trained Transformer model - BERT - in an active learning scenario for text classification in low-resource settings. We showed that using Monte-Carlo Dropout in the classification architecture is an effective way to approximate model uncertainty on unlabeled training elements. This technique enables us to select data for annotation that maximize the knowledge gain for the model fine-tuning process. Experimental results on GLUE data set show that it improves both model performance and training stability. Finally, in order to improve the efficiency of the fine-tuning process with a small amount of data, we explored the reduction of trainable model parameters by freezing layers of the BERT model up to a certain level of depth. Comparing the exclusion of layers in the front or the back of the BERT model from training, we found it to be advantageous for training stability when freezing the layers closest to the output.   We attribute this effect to the reduction of free parameters that only change very little in the short training period of the low-resource setting.  The further exploration of this aspect is subject to future work by combining the observations of layer-wise MAD with previous advances in language model fine-tuning and more sophisticated training strategies like gradual unfreezing or discriminative fine-tuning .  \ifcolingfinal 
","     Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT - a pre-trained Transformer based language model - by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings.",380
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %       % space normally used by the marker     This work is licensed under a Creative Commons      Attribution 4.0 International License.     License details:     \url{http://creativecommons.org/licenses/by/4.0/}. }  Multilingual relation extraction is an important problem in NLP, facilitating a diverse set of downstream tasks from the autopopulation of knowledge graphs  to question answering . While early efforts in relation extraction used supervised methods that rely on a fixed set of predetermined relations, research has since shifted to the identification of arbitrary unseen relations in any language. In this paper, we present a method for extracting high quality relation training examples from date-marked news articles. This technique leverages the predictable distributional structure of such articles to build a corpus that is denoised . We use this corpus to learn general purpose relation representations and evaluate their quality on few-shot and standard relation extraction benchmarks in English and Spanish with little to no task-specific fine-tuning, achieving comparable results to a significantly more data-intensive approach that is the current state-of-the-art.  The current state-of-the-art model, ``Matching the Blanks"" or MTB, is a distant supervision technique that provides large gains on many relation extraction benchmarks and builds on Harris' distributional hypothesis and its extensions. ~ assume that the informational redundancy of very large text corpora  results in sentences that contain the same pair of entities generally expressing the same relation. Thus, an encoder trained to collocate such sentences can be used to identify the relation between entities in any sentence  by finding the labeled relation example whose embedding is closest to . While~ achieve state-of-the-art on FewRel and SemEval 2010 Task 8, their approach relies on a huge amount of data, making it difficult to retrain in English or any other language with standard computational resources: they fine-tune BERT large, which has mil parameters, on mil+ relation pair statements with a batch size of  for mil steps. In contrast our method, with only  relations statements and a language-model one-third the size, achieves comparable performance when fine-tuned on little to no task-specific data.   Our main contribution is a distant supervision approach in which we assume that sections of news corpora exhibit even more informational redundancy than Wikipedia. Specifically, news in the days following an event  frequently re-summarizes the event before adding new details. As a result, news exhibits a strong form of local consistency over short rolling time windows where otherwise fluid relations between entities remain fixed. For example, the relation between Italy and France as expressed in a random piece of text is dynamic and context-dependent, spanning a wide range of possibilities that include ``enemies"", ``neighbors"" and ``allies"".  But, in the news coverage following the 2006 World Cup, it is static -- they are sporting competitors. Therefore, by considering only sentences around specific events, we extract groups of statements that express the same relation and are relatively free of noise .    Training multilingual BERT  on our denoised corpus yields relation representations that adapt well to resource-constrained downstream tasks: we evaluate their quality on FewRel and SemEval 2010 Task 8, producing near state-of-the-art results when finetuned on little to no task-specific data. In addition to the strong performance of our approach in English, it is easily generalizable to other languages, requiring only news corpora and event descriptions from Wikipedia to build a high-quality training corpus. We evaluate this in Spanish and find our method outperforms mBERT on the TAC KBP 2016 relation corpus. We share our code to allow other researchers to apply our approach to news corpora of their own.       We present an event-guided denoising approach for relation extraction corpus creation that, when used with the current state-of-the-art training procedure, achieves comparable results in English under a low-resource regime for only a fraction of the training cost. It also performs well in Spanish, demonstrating its adaptability to resource-constrained relation extraction tasks in non-English languages.   Our technique affords the broader research community the ability to approximate the current state-of-the-art in relation extraction by significantly lowering its associated training costs.  However, it requires a fairly large date-marked news corpus which may not be available in low resource languages. We leave an exploration of broader language coverage and minimal required corpus size for future work.   One promising direction for expanding language coverage is cross-lingual learning via ``codeswitched"" examples and other language modeling losses .  We hypothesize that such methods could help knowledge transfer among languages and improve results on downstream tasks.  Finally, we note that since our approach extracts relation statements from news corpora, it is likely that the resulting distribution of underlying relation types is different than the distribution found in Wikipedia. For example, Wikipedia may contain more expressions of standard ontological relations  characteristic of factoids. Despite this hypothesized difference, our approach performs well on both FewRel and SemEval 2010 Task 8, both of which include a subset of such relation types.  In the future we intend to investigate these differences and their implications more closely.    Acknowledgments: AIDA, partially supported by 
"," General purpose relation extraction has recently seen considerable gains in part due to a massively data-intensive distant supervision technique from~ that produces state-of-the-art results across many benchmarks. In this work, we present a methodology for collecting high quality training data for relation extraction from unlabeled text that achieves a near-recreation of their zero-shot and few-shot results at a fraction of the training cost. Our approach exploits the predictable distributional structure of date-marked news articles to build a denoised corpus -- the extraction process filters out low quality examples. We show that a smaller multilingual encoder trained on this corpus performs comparably to the current state-of-the-art  on few-shot and standard relation benchmarks in English and Spanish despite using many fewer examples .",381
"  Data augmentation is a widely-used technique in classification tasks. In the field of computer vision , data is augmented by flipping, cropping, tilting, and altering RGB channels of the original images~; however, similar intuitive and simple strategies do not obtain equal success in NLP tasks. Existing methods tend to produce augmentation with low readability or unsatisfying semantic consistency~.   [tp!] {!}{% {@{}ll@{}} \toprule Original                                                                        & So Cute! The baby is very lovely!                                                    \\ \midrule [c]{@{}l@{}}Naive Aug.\\ Delete + Swap              & So Cute! is The baby very!                                                           \\ \midrule [c]{@{}l@{}}Word2Vec Aug.\\ Insert + Replace & [c]{@{}l@{}}So Cute \underline{adorable}!\\ The baby is very \underline{fabulous}! \\ \midrule [c]{@{}l@{}}Back Translate Aug.\\ Eng.  Fr.  Eng. & \underline{Cute}! The baby is very \underline{cute}!                                                         \\ \midrule Data Boost                                                             & [c]{@{}l@{}}	extbf{Look at this adorable baby!}\\ 	extbf{He is so cute!} \\ % }  sentiment label.}     Table shows some output samples of popular text augmentation methods. Naive methods imitate pixel manipulation in CV, augmenting sentences by adding spelling errors~, or randomly deleting and swapping tokens~. The output of such augmentation methods are often illegible since the word order is disrupted ; even worse, crucial feature words  could be mistakenly removed through random deletion. A more advanced method is synonym insertion or replacement~, which uses Word2Vec~ to replace words with their synonyms. Such a method respects the original sentence structure but fails to consider the context. It sometimes replaces words with synonyms that are awkward in the full context of the sentence. For example, replacing lovely with fabulous to get the sentence  ``The baby is fabulous!"". Recent work leans towards translation-based methods for augmentation~. In particular,  proposed a back-translation method that first translates the text to French and then translates back to English, using the noisy output as the augmentation data. Although back-translation is intuitive and valid, its generation skews towards high frequency words , which not only causes repetition but also leads to lexical shrinkage in the augmented data. In a nutshell, existing techniques  are still far from perfect, partially due to the strong interdependency of syntactic and semantic features in text data.   In recent years, we have witnessed extensive progress in language models . Large-scale LMs such as BERT~, XLNet~, and GPT-2~, are commonly trained on large amounts of text data . One of the most interesting usages of these models is utilizing them as text generators~. In this paper, we explore whether we can leverage the generation ability of the state-of-the-art LMs, to generate augmented samples for a given target class.   Augmentation samples should exhibit features of the target class. Off-the-shelf LMs cannot be directly used to augment data; since they are not trained for specific contexts, their generation is undirected and random. Conditional LMs can generate text directed by certain condition , but they require training a LM from scratch with data covering all the conditions. , for instance, trained a 1.6 billion-parameter LM conditioned to a variety of control codes. The training is rather costly; however, collecting sufficient data for the training is also tedious, especially in low-resource tasks~.  We thus present Data Boost: a  reinforcement learning guided text data augmentation framework built on off-the-shelf LM . Data Boost requires neither collecting extra data nor training a task-specific LM from scratch. We convert GPT-2 into a conditional generator, and for a given task, we guide the generator towards specific class labels during its decoding stage through reinforcement learning. The generated samples can then serve as augmentation data which are similar to the original data in terms of semantics and readability.    The advantages of Data Boost are three-fold: First, Data Boost is powerful. We achieve significant advances in three tasks on five different classifiers compared with six related works. Second, Data Boost generates sentence-level augmentation. Unlike prior methods that do word-level or phrase-level replacement~, our augmented data is of much greater variety in terms of vocabulary and sentence structure. Human evaluations also verify the high readability and label consistency of our augmentation. Third, Data Boost is easy to deploy. It does not require external datasets or training separate systems . Instead, we take the off-the-shelf GPT-2 language model and modify its decoding stage without changing its architecture.        We have proposed a powerful and easy to deploy approach to augment text data through conditional generation. By leveraging an off-the-shelf language model , we successfully guide the generation towards a specified direction , with the help of reinforcement learning. We find that Data Boost improves the performance of classification tasks, is classifier-agnostic, and that it surpasses several prior augmentation methods in three diverse classification tasks.   In the future, we plan to implement a more sophisticated guidance for the augmentation by adding syntactic and position features to the reward function, to enable augmentation of more diverse types of text data. The code will be made available upon request.   
"," Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7\% on average when given only 10\% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations , we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.",382
" } is ``Red is the color at the end of the visible spectrum of light, next to orange and opposite violet.'' The aim is to identify the word ``color'' as the hypernym of ``red'' from all the nouns in the definition. Intuitively, this task can be solved by general resources such as WordNet dictionary  or Wikipedia. But given a word's different meanings in different contexts, these resources can not sufficiently complete this task. As an example, the term ``LDA'' in Wikipedia denotes ``Linear Discriminant Analysis'' in machine learning, ``Low dose allergens'' in medicine, and ``Landing distance available'' in aviation. The combination of general resources and context identification would also fail in some domain-specific applications where the general resources do not cover the special or technical terms in that area. Moreover, existing technical approaches also demonstrate certain limitations in the task of hypernym extraction from definitions, which we summarize as follows: [1)]   To briefly illustrate the difficulty, let us consider a definition from the Stack-Overflow with an irregular format: ``fetch-api: the fetch API is an improved replacement for XHR''. The term ``fetch-api'' is not included in any common dictionary. While the definition has the ``is an'' pattern, it does not connect to the hypernym. The definition is very short and every distinct word in this definition appears just once, which makes it difficult to accurately learn the word representation. Overall, it is challenging to find a method that would accurately identify ``API'' as the correct hypernym.   The definition of a word represents a certain type of knowledge extracted and collected from disordered data. Indeed, there are tools capable of extracting definitions from the corpora with good accuracy . Nevertheless, tools to extract hypernym from definitions remain limited.  % To cope with this issue, we propose a recurrent network method using syntactic features. Because the definition directly points to a noun, the hyponym is already given. Therefore, the hypernym extraction is to identify the correct hypernym from all words in the definition sentence. This task can be considered as a binary classification, in which the classifier judges if a candidate noun is a hypernym or not. In order to better learn the syntactic feature, we transfer the definition sentence into the part of speech  sequence after labeling the PoS of each word by a standard tool . The syntactic structure surrounding the candidate is learned by a bidirectional gated recurrent units  based model. To further fine tune the results, we use a set of features including the centrality of the word in the hypernym co-occurrence network. We use two corpora to evaluate our method. One is Wikipedia, featuring definitions with canonical syntax structure and intensively used by previous studies. The other is from Stack-Overflow, whose definition is domain-specific and usually with the irregular format. Our method is compared with several existing ones. Overall, it outperforms all others in both corpora, which demonstrates the advantage of combing both the tool of RNN and the PoS information in the task of hypernym extraction.    This paper is organized as follows. We review related works in Section  and introduce details of the method in Section . Experiments and evaluations of the proposed model are presented in Section . After that, we draw a conclusion about this research in Section .       The hyponym-hypernym relationship plays an important role in many NLP tasks. Despite intensive studies on this topic, tools that can accurately extract hypernym from a definition is limited. The definition, representing a special type of summarized knowledge, is commonly observed, not only because some corpora such as Wikipedia or GitHub directly give the definition of a term, but also because there are tools capable of extracting definitions with good accuracy. Hence, it is useful to develop a capable tool for this task. Here we construct a bidirectional GRU model for patterns learning. We use the PoS tags of words surrounding the hypernym as the feature. Our model outperforms existing methods in both the general corpus  and the domain-specific corpus . It also demonstrates a good balance between the performance and complexity, if compared with the kernels by Transformer or Bert. More importantly, by the feature and kernel ablation, we show that the PoS feature is indeed the key element that guarantees the final performance.   The application of the tool we proposed in Stack-Overflow would help us understand the evolution of technology, group users for social network study, and build the semantic network in the domain of computer science. The performance of the tool is limited by the accuracy of PoS tagging. Hence, it would be useful to try or develop other methods other than the Stanford-NLP tool. The use of PoS feature may also have potential in other text sequence labeling tasks, which may have advantages over the word embedding. All these problems will be addressed in future studies.   
"," % The abstract should briefly summarize the contents of the paper in % 150--250 words. The hyponym-hypernym relation is an essential element in the semantic network. Identifying the hypernym from a definition is an important task in natural language processing and semantic analysis. While a public dictionary such as WordNet works for common words, its application in domain-specific scenarios is limited. Existing tools for hypernym extraction either rely on specific semantic patterns or focus on the word representation, which all demonstrate certain limitations. Here we propose a method by combining both the syntactic structure in definitions given by the word闁炽儲鐛 part of speech, and the bidirectional gated recurrent unit network as the learning kernel. The output can be further tuned by including other features such as a word闁炽儲鐛 centrality in the hypernym co-occurrence network. The method is tested in the corpus from Wikipedia featuring definition with high regularity, and the corpus from Stack-Overflow whose definition is usually irregular. It shows enhanced performance compared with other tools in both corpora. Taken together, our work not only provides a useful tool for hypernym extraction but also gives an example of utilizing syntactic structures to learn semantic relationships \footnote{Source code and data available at \url{https://github.com/Res-Tan/Hypernym-Extraction}}.",383
"  Although neural machine translation  has achieved great success on sentence-level translation tasks, many studies pointed out that  translation mistakes become more noticeable at the document-level. They proved that these mistakes can be alleviated by feeding the inter-sentential contexts into context-agnostic NMT models.  Previous works have explored various methods to integrate context information into NMT models. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks  or extra context encoders . Different from representation-based approaches, ~~ and ~~ propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is usually updated when new translations are generated. Therefore, long-distance contexts would likely to be erased.  How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence  and using memory and hierarchical structures , are proposed to take global contexts into consideration. However, ~ point out that not all the words in a document are beneficial to context integration, suggesting that it is essential for each word to focus on its own relevant context.    \footnotetext{Dependency and coreference relations are from Stanford CoreNLP .}  To address this problem, we suppose to build a document graph for a document, where each word is connected to those words which have a  direct influence on its translation. Figure  shows an example of a document graph. Explicitly, a document graph %for a document  is defined as a directed graph where:  each node represents a word in the document;  each edge represents one of the following relations between words:  adjacency;  syntactic dependency;  lexical consistency; or  coreference.   We apply a Graph Convolutional Network  on the document graph to obtain a document-level contextual representation for each word,  fed to the conventional Transformer model  by additional attention and gating mechanisms. We evaluate our model on four translation benchmarks, IWSLT English--French  and Chinese--English , Opensubtitle English--Russian , and WMT English--German . Experimental results demonstrate that our approach is consistently superior to previous works  on all the language pairs.   The contributions of this work are summarized as:    % model via attention and gating mechanisms.      In this paper, we propose a graph-based approach for document-level translation, which leverages both source and target contexts. Graphs are constructed according to inter-sentential and intra-sentential relations. We employ a GCN-based graph encoder to learn the graph representations, which are then fed into the NMT model via attention and gating mechanisms.  Experiments on four translation tasks show the proposed approach consistently improves translation quality across different language pairs. Further analyses demonstrate the effectiveness of graphs and the capability of leveraging long-distance context. In the future, we would like to enrich the types of relations to cover more document phenomena.        
","     Previous works have shown that contextual information can improve the performance of neural machine translation . However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph     that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English--French, Chinese-English, WMT English--German and Opensubtitle English--Russian, demonstrate that using document graphs can significantly improve the translation quality.",384
"  Automatic summarization is a fundamental task in natural language generation and computational linguistics. It is crucial to help the user quickly read and understand daily events, and has been continuously studied for decades. . In this paper, we focus on meeting summarization, which is an extensively studied task in the field of automatic summarization. Given multiple speakers and corresponding utterances in text, the task calls for generating a shorter transcript, covering salient information of the entire meeting. An example is shown in Figure , which includes 3 speakers and their utterances , and , as well as a human-written summary.  Meeting summarization is typically regarded as a kind of abstractive summarization problem in the literature. The majority of existing studies build summarization systems based on the sequence-to-sequence model, which adopts a sequence modeling strategy for encoding utterances . Despite the effectiveness of these approaches, they typically only use sequential text information while ignoring the important influences of dialogue structure. We claim that dialogue-specific structural information is important for meeting summarization. For example, dialogue discourse is an effective structural feature. As shown in Figure , ``Contrast閳, ``Question-Answer閳 and ``Continuation閳 are three dialogue discourse relations, which can provide more precise semantic relationships between each utterance. Specifically, we can see that the existing sequence modeling method is unable to generate correct summary results ), which can be attributed to the system not knowing the  and  are opposed to the 閳ユ獨 proposal. Differently, the dialogue discourse can provide this key information via labeling the 閳ユ窅ontrast閳 relationship, as shown in Figure . Accordingly, how to effectively integrate the discourse relationship into the existing summarization model become a crucial step in meeting summarization.  In this paper, we propose Dialogue Discourse-Aware Graph Convolutional Networks  to address this problem. In detail, we first convert the entire meeting with dialogue discourse labeling into a discourse graph, which represents both utterances and discourse relationships as vertices. Afterwards, we additionally design six types of directed edges and one global vertex in the discourse graph to facilitate information flow. Finally, we employ a graph convolutional network  to encode the graph and pass the semantic representation to the RNN decoder. Besides, we further use the question-answer discourse relationship to construct a pseudo-summarization corpus for pre-training DDA-GCN. In a conversation, a question often sparks a discussion, so naturally, the question can be used as a pseudo-summary for subsequent discussions.  We conduct experiments on the widely used AMI benchmark . Our approach outperforms various baselines. Moreover, we analyze the effectiveness of dialogue discourse and pseudo-summarization corpus. In the end, we give a brief summary of our contributions:  To the best of our knowledge, we are the first to apply dialogue discourse to model the structure of a meeting for meeting summarization;  We design a discourse-aware graph model to encode the entire meeting;  Our model achieves a new SOTA on the AMI dataset.      In this paper, we apply the dialogue discourse to model the structure of a meeting for meeting summarization. We first transform the entire meeting text and corresponding dialogue discourse relations into a discourse graph. Specifically, both the utterances and discourse relations are constructed as vertices, and we design six types of edge and a global vertex to facilitate the information flow. Moreover, we develop a Dialogue Discourse-Aware Graph Convolutional Networks  which consists of an utterance encoder, a graph encoder, and a pointer decoder. In addition, we construct a pseudo-summarization corpus by utilizing the question-answer discourse relation, which can be used to pre-train our model.  Experiments on the AMI dataset show the effectiveness of our model which can achieve the SOTA performance.       
"," Sequence-to-sequence methods have achieved promising results for textual abstractive meeting summarization. Different from documents like news and scientific papers, a meeting is naturally full of dialogue-specific structural information. However, previous works model a meeting in a sequential manner, while ignoring the rich structural information. In this paper, we develop a Dialogue Discourse-Aware Graph Convolutional Networks  for meeting summarization by utilizing dialogue discourse, which is a dialogue-specific structure that can provide pre-defined semantic relationships between each utterance. We first transform the entire meeting text with dialogue discourse relations into a discourse graph and then use DDA-GCN to encode the semantic representation of the graph. Finally, we employ a Recurrent Neural Network to generate the summary. In addition, we utilize the question-answer discourse relation to construct a pseudo-summarization corpus, which can be used to pre-train our model. Experimental results on the AMI dataset show that our model outperforms various baselines and can achieve state-of-the-art performance.",385
"  Pre-trained language models such as BERT or RoBERTa learn contextualized word representations on large-scale text corpus through self-supervised learning, and obtain new state-of-the-art results on many downstream NLP tasks . Recently, researchers have observed that pre-trained language models can internalize real-word knowledge into their model parameters. For example, pre-trained language models are able to answer the questions such as ``the sky is }'' or ``Beethoven was born in }'' with moderate accuracy. To further explore their potential, researchers have proposed various approaches to guide the pre-training of the language models by injecting different forms of knowledge into them, such as structured knowledge graph or linguistic knowledge  .     [t] 	{p{5cm}XX} 		{l}{Knowledge Injection Approaches} & Pre-training data \\ 		Model & Generative tasks & Discriminative tasks\\ 		)  \\ 		%	KnowBERT & entity linking     \\ 		SenseBERT~ & supersense prediction & -  \\ 		BERT\_CS~ & - & multi-choice question answering \\ 		LIBERT~ & - & lexical relation prediction \\ 		%	MTB & relation statement comparison   \\ 		LIMIT-BERT~  & semantic/syntactic phrase masking & -  \\ 		KEPLER~ & - & knowledge representation learning  \\ 		SpanBERT~ & span masking & -  \\ 		WKLM~ &  - &  entity replacement checking \\ 		K-Adapter~ & - & relation classification, dependency relation prediction \\ 		T5+SSM~ & salient span masking & - \\ 		TEK~ & span masking on TEK-augmented text & - \\ 		CN-ADAPT~ & MLM training on synthetic knowledge corpus & - \\ 		 tasks and discriminative tasks. Generative tasks are often formulated as predicting the masked tokens given the context. By particularly masking out the words that contain certain types of knowledge  in generative pre-training, the model can be more adept in memorizing and completing such knowledge. While discriminative tasks are often formulated as a classification problem with respect to the sentence or the tokens. By training on the positive and negative examples constructed according to the external knowledge, the discriminator can be more capable of verifying the true or false knowledge in natural language. Existing research has demonstrated that generative and discriminative training have their advantages: the former has a large negative sample space so that the model can learn fine-grained knowledge, while the latter avoids the ``'' tokens in pre-training, and is therefore more consistent with fine-tuning. On the other hand, generative and discriminative capture the different aspects of data distribution and could be complementary to each other in knowledge consolidation. However, to the best of our knowledge, there is not previous work in combining the two approaches in a systematic way. Inspired by the recent success on the generative-discriminative pre-trained model named ELECTRA, we propose to learn the generator and discriminator jointly in the knowledge-guided pre-training, which we call the KgPLM model.  In this paper, we design masked span prediction as the generative knowledge completion task, and span replacement checking as the discriminative knowledge verification task. Hybrid knowledge, including link structure of Wikipedia and structured knowledge graph in Wikidata, is used to guide the both tasks. The spans covering the factual knowledge are more likely to be selected for masking or replacement, and the choices of their replacements are also related to the proximity to the original span in the knowledge space. Figure shows an example of the span masking and replacement tasks. To further explore effective ways to the joint training of the two tasks, we design two learning schemes, which we called two-tower scheme and pipeline scheme. Basically, the generator and discriminator are trained in parallel with the shared parameters in the two-tower scheme. While in the pipeline scheme, the output of generator is input to the successive discriminative training. The generator and discriminator in our KgPLM model are both pre-trained based on RoBERTa. They have some additional benefits: 1) the model can be readily extended to much larger pre-training corpus, which keeps some potential room for further improvement; 2) the model retains the same amount of parameters as RoBERTa, and does not require any modifications in fine-tuning for the downstream tasks.  We evaluate the model performance on LAMA~, which consists of several zero-shot knowledge completion tasks, and MRQA shared tasks~, which include several benchmark question answering datasets. The experiments show the proposed KgPLM, especially that trained with the pipeline scheme, achieves the state-of-the-art performance, and significantly outperform several strong baselines  on some of the tasks. The results indicate that the knowledge-guided generative and discriminative pre-training provides an effective way to incorporate external knowledge and achieve competitive performance on the knowledge intensive NLP tasks.     We have proposed a pre-training method by cooperatively modeling the generative and discriminative knowledge injecting approaches. Our model can be easily extended to larger pre-training corpus and does not introduce any modifications for downstream tasks during finetuning. Experiments show our model consistently outperforms all BASE models on a variety of question answering datasets, demonstrating that our KgPLM is a preferred choice for the knowledge intensive NLP tasks.   Our method uses two-tower and pipeline frameworks to integrate knowledge span masking with knowledge span checking for pre-training.   add TEK into pre-training and finetuning.    train from scratch   
"," Recent studies on pre-trained language models have demonstrated their ability to capture factual knowledge and applications in knowledge-aware downstream tasks. In this work, we present a language model pre-training framework guided by factual knowledge completion and verification, and use the generative and discriminative approaches cooperatively to learn the model. Particularly, we investigate two learning schemes, named two-tower scheme and pipeline scheme, in training the generator and discriminator with shared parameter. Experimental results on LAMA, a set of zero-shot cloze-style question answering tasks, show that our model contains richer factual knowledge than the conventional pre-trained language models. Furthermore, when fine-tuned and evaluated on the MRQA shared tasks which consists of several machine reading comprehension datasets, our model achieves the state-of-the-art performance, and gains large improvements on NewsQA  and TriviaQA  over RoBERTa.",386
" 	 	Knowledge graphs , such as WordNet , Freebase  and Wikidata , aggregate a large amount of human knowledge and express in a structured way. 	% are representative of existing KGs, in which knowledge is formalized as triples. 	%, such as  and . 	Unreliable relation paths are common in knowledge graphs, and   found that it is necessary to select reliable relation paths for knowledge representation learning. 	%In this work, a path-constraint resource allocation algorithm is proposed to measure the weights of inference patterns. 	They learn inference patterns between relations and paths to utilize knowledge contained in relation paths. 	%Despite its success, the modeling objects are more limited to the inference patterns between relations and paths. 	%Recently,   propose a method to model the contextual nature of triples and relation paths, and they explore the benefits of graph contextual information for link prediction tasks on two specific datasets. 	%However, simply adding graph contextual information  into the training pool is not always effective, and this operation may reduce the performance of the original model. 	Instead of relying on inference patterns, we propose PPKE, a path-based pre-training approach that integrates  graph contextual information contained in relation paths into the model parameters. 	We think this is a more general way to develop the unexploited graph contextual information. 	During the path-based pre-training procedure,  two-step relation paths are extracted from the knowledge graph and fed into the pre-training module with original triples. 	Then, the pre-trained model can be finetuned for downstream KGC tasks, such as link prediction and relation prediction. 	Our contributions are as follows: 	 		    	 	We propose a novel approach to integrate graph contextual information into a path-based pre-training model, focusing on modeling one-step and two-step relations between entities. 	 Then, the pre-trained model is finetuned for link prediction and relation prediction tasks. 	Experiments show our model outperforms previous state-of-the-art methods, 	  after incorporating a small portion of graph context information existing in knowledge graphs,  	which validates the intuition that graph contextual information is beneficial to knowledge graph completion tasks. 	 	In the follow-up work, we will try to add relation prediction objective into the pre-training procedure, and larger quantity or wider variety of graph contextual information will be explored. 	 Besides, more knowledge-driven tasks will be utilized to validate the effectiveness of our method. 	 	 relation prediction in pre-training 	 	
"," 		Entities may have complex interactions in a knowledge graph , such as multi-step relationships, which can be viewed as graph contextual information of the entities. 		Traditional knowledge representation learning  methods usually treat a single triple as a training unit, and neglect most of the graph contextual information exists in the topological structure of KGs. 		In this study, we propose a Path-based Pre-training model to learn Knowledge Embeddings, called PPKE, which aims to integrate more graph contextual information between entities into the KRL model. 		Experiments demonstrate that our model achieves state-of-the-art results on several benchmark datasets for link prediction and relation prediction tasks, indicating that our model provides a feasible way to take advantage of graph contextual information in KGs.",387
"  Machine reading comprehension  is a challenging natural language understanding task which lets the machine predict appropriate answer to the question according to a given passage or document .  According to answer styles, MRC tasks can be roughly divided into generative , extractive  and multi-choice  tasks . The multi-choice task is the focus of this work.  Recently, various datasets and tasks have been proposed, promoting a rapid improvement of MRC techniques . Early MRC datasets usually provide passages whose contents are extracted from articles . Recently, conversational reading comprehension has aroused great interests whose passages are derived from multi-turn dialogue segments , making the task be more challenging.    The popular practice to solve MRC problems is adopting pre-trained language models  as encoder module . Instead of better exploiting pre-trained LMs, this paper is motivated by human reading strategies to decouples MRC into sketchy reading by extracting the critical spans from the passage, and extensive reading by seeking external knowledge.  As a result, we propose a knowledge enhancement model based on extracted critical information called ference Knowledgeable Network)}. In detail, the proposed  refines the fine-grained critical information by a span extraction model and defines it as , then quotes relevant external knowledge in the form of quadruples by the co-occurrence information of  and answer options. An example process of  is shown in Figure .   In summary, our main contributions are follows:\\ 1) We propose a novel reference-based knowledge enhancement model , which makes the first attempt to obtain fine-grained evidence for inference and knowledge retrieving on MRC tasks.\\ 2)  uses novel knowledge quadruples to quote relevant and credible knowledge.\\ 3)  is applied to two multi-choice MRC benchmarks, RACE  and DREAM  and improves the performance of baseline models by 1.0\% and 1.1\% respectively, which both pass the significance test of MRC tasks.    To alleviate the challenge of knowledge role missing in multi-choice MRC, this work makes the first attempt to integrating  based on  into MRC modeling, presenting ference Knowledgeable Network }, which can simulate the human strategy of reading comprehension and quote external knowledge for multi-choice MRC tasks.   helps achieve significantly performance improvement on two multi-choice MRC benchmarks RACE and DREAM, which passed the significance test. In the future, we will apply  to other forms of MRC tasks.       
"," Multi-choice Machine Reading Comprehension  is a major and challenging form of MRC tasks that requires model to select the most appropriate answer from a set of candidates given passage and question. Most of the existing researches focus on the modeling of the task datasets without explicitly referring to external fine-grained commonsense sources, which is a well-known challenge in multi-choice tasks. Thus we propose a novel reference-based knowledge enhancement model based on span extraction called ference Knowledgeable Network }, which simulates human reading strategy to refine critical information from the passage and quote external knowledge in necessity. In detail,  refines fine-grained critical information and defines it as , then quotes external knowledge quadruples by the co-occurrence information of  and answer options. Our proposed method is evaluated on two multi-choice MRC benchmarks: RACE and DREAM, which shows remarkable performance improvement with observable statistical significance level over strong baselines.",388
"   Data collection is an essential part of the field of spoken dialogue systems and conversational AI. %, and requires developers to make difficult decisions and budget accordingly.   In particular, designing a dialogue system for a completely new domain is still a very challenging task.  Data collection options include running lab-based experiments, crowd-sourced tasks  or gathering data from social media platforms, such as Reddit or Twitter. Ambitious large scale data collections across multiple domains have resulted in widely used datasets, such as MultiWOZ . % and collected from various platforms .% to create representations of dialogues in the vector space.   However, starting off in a new domain from scratch still has its challenges. Difficult and costly decisions have to be made as to how and where to collect the data.    A large majority of recent dialogue corpora has been collected using crowd-sourcing either by pairing workers and letting them chat, often about a given topic , or by asking them to add the next utterance to the dialogue given a set of conditions . Other studies have recruited subjects to play the role of the system, i.e., to act as a wizard or user . Each of these approaches has its own advantages and disadvantages, depending on if the dialogue is task-oriented or not. By letting users type in an unrestricted way, the richness of the dialogue increases, which is a positive feature for chit-chat. On the other hand, too much variability could be a problem for a high stakes, task-oriented dialogues, such as in the medical domain. Letting multiple users contribute with one utterance per dialogue , speeds up the data collection, however, dialogues may lack coherence and severely diverge from real dialogues. On the other hand, hiring and training subjects to chat or perform the wizard role results in a more controlled data collection but dramatically increases the cost of the data collection and makes it less scalable.     The quality of such datasets has been often assessed according to the degree of variability  observed  or the lexical complexity of the utterances collected . %, however to the best of our knowledge, there is no work assessing the impact of the different methods directly on training dialogue models.   %This paper aims at addressing this issue by investigating the impact of two different data collection methods on the performance of the model. Furthermore, most of the above-mentioned datasets focus on increasing the size of the dataset available for dialogue research, rather than investigating the impact of the data collection strategies on the performance of the models trained. The work presented in this paper aims at highlighting the pros and cons, using a methodology to quickly leverage a robust dialogue system, minimising the cost and effort involved in the data collection process. Analyses comparing different strategies for the data collection process across various platforms have been done in the past , but we are not aware of a similar study for dialogue data.  The data used in this study was collected in the scope of an emergency response system to be used on an off-shore energy platform as part of the EPSRC ORCA Hub programme . One of the collections was done using crowd-sourcing  and the second one was done in a lab using a Wizard-of-Oz setting, where participants were interacting either with a social robot or a smart speaker. Both datasets were used to train a dialogue model using an implementation of a Hybrid Code Network  and here we compare the results achieved by models trained on data collected by either method. To validate the use of crowd-sourced data to bootstrap a dialogue system for situated interaction, we ran experiments where we train the model on the crowd-sourced data and test it on the lab data, in order to verify if it %This will result in an estimate of the number of dialogues needed to  %varied the amount of crowd-sourced dialogues during training to estimate the necessary amount of crowd-sourced data needed to  achieves comparable performances with the models trained only with the lab data.   The contributions of this paper are as follows: 1) a comparison of models trained with two datasets collected in different ways but on the same task, 2) evidence that suggests that specialised dialogue tasks, such as our emergency response task, are not well covered by current pre-trained dialogue models, and 3) a set of recommendations regarding the data collection for dialogue research.\footnote{Please find code and data in: {}.}  The paper is organised as follows. Section  will cover previous work related to this problem. Our experimental set-up will be introduced in Section , followed by the results in Section . The paper concludes with the discussion in Section  and future work and conclusions in Section .            Dialogue acts which can occur in any context such as inform\_time\_left or hold\_on\footnote{Used to keep users engaged until new information was available.} were in both scenarios difficult to learn.    With the use of the action mask, we observed the prediction of the  robot state update states has greatly improved with the use of the action mask    BoW.previous_action.nlu: MTurk, seems to be failing to get the right timing. Dialogues states seem to be predicted in adjacent turns. Problems predicting the request and action states, unlike the in lab where the unknown turns were the hardest to predict.  Results in Table  show that the model trained only with Lab data outperformed the model trained with the complete dataset on , with all the features included. When comparing the performance of models trained with the same number of dialogues , the model trained with Lab data clearly outperformed the model trained with the MTurk data. This is not surprising, since the Lab data was collected with a single wizard, who had mastered the task. The wizard behaviour is more consistent than the behaviour of crowd-workers, who had little time to familiarise themselves with the task. The perplexity scores also confirm that dialogues trained with Lab data unfold in a more predictable way when compared with crowd-sourced data.   A fine-grain analysis revealed that the outputs of models trained with MTurk data tend to rush the dialogue, minimising the number of robot status updates   while the robot is moving towards the target location  and using fewer non-task based dialogue acts, such as ``Hold on 2 seconds''. This pattern perhaps reflects the crowd-sourced worker's tendency to streamline tasks. However, even given the time constraint of the task, the above-mentioned dialogue acts  are important contributions to the dialogue, especially in terms of managing the user's confidence and stress levels. This effect could be due to the fact that face-to-face interactions require some turn-taking management, which is not reproducible in text-based dialogues.   and driving the emergency to its resolution.   Furthermore, the set-up used in the lab data collection is as close as one can get to the end application. As can be seen from Figure , the set-ups are significantly different. In the lab setting, participants are immersed in the scene in an operations-like environment, unlike in the crowd-sourced scenario where the interaction takes place through a chat window on the crowd-worker's computer.   This raises concerns about the methodology used for data collections in cases such as ours  Clearly, the lab setting is more appropriate for tasks such as the emergency response task described here, where situation awareness and full user engagement are vital to replicate the conditions of the end application . Our results suggest that with a much smaller amount of data collected in very controlled conditions the model learns faster and more accurately. While this level of control might hinder the performance of the model when dealing with outliers, this seems not to be the case for our domain, where we expect participants to be highly knowledgeable of the task and compliant with the safety protocols that should be followed to complete the task successfully.     Models tended to drive the dialogue faster than ground truth,   Although this behaviour was shown throughout the collected crowd-sourced dialogues, the models do not seem to learn the notion of time which is crucial for domains such as ours. While there has been significant progress in generating dialogue responses in the recent years, the timing aspects seem to be left behind. Our results suggest that in dialogues collected via crowd-sourcing this aspect might be lost.   and models struggle to learn the ``when'' to use a particular dialogue act.      Results from Table  show that pre-trained models perform very poorly in our task. As we have hypothesised earlier, our task is significantly different than tasks currently used in task-oriented dialogue research. Hence, system developers will have to assess whether the domain is similar enough to allow the use of pre-trained models, or if it is too specific such that it requires the model to be trained with data within domain.     collaborative task success helps understand the performance of the model when the operator behaves reasonably. In these cases, the operator would have resolved the emergency in the dialogue    Future work will involve investigating further the use of pre-trained models for specific-task based systems and the extent to which they can be used to bootstrap models for highly specific tasks such as our own. We acknowledge that the datasets in this study are small compared to datasets used to train state-of-the art neural models. This is one reason why we used the HCN method in this study as it has been shown to work well with small amounts of data . One future direction would be to duplicate such a study with a dataset of a similar size to MultiWOZ and explore further fine-grained increases in data size for training. This, however, will be very challenging and costly to collect lab data to match the size of the crowd-sourced data. With more in-lab data to retrain the models, we would plan to run a further in-depth systematic comparison of a variety of dialogue modelling approaches, for example using the methodology proposed in . Finally, to investigate the single-wizard impact, we are aiming to run a crowd-sourced data collection with a single wizard and repeat the experiments done in this paper.   In this paper, we present a study comparing how different approaches to data collection may impact a hybrid neural dialogue model performance. Results suggest that, for our domain, models trained with small sets of lab-collected data outperform models trained with larger crowd-sourced datasets and pre-trained models. Given the nature of the domain, focusing on smaller lab data collections in realistic settings will likely be the best way to rapidly improve the model. However, the challenge to improve crowd-sourced data collection, making them as close as possible to the end application, still remains.         
","  Challenges around collecting and processing quality data have hampered progress in data-driven dialogue models. %, particularly data-hungry neural and hybrid models.   Previous approaches are moving away from costly, resource-intensive lab settings, where collection is slow but where the data is deemed of high quality. The advent of crowd-sourcing platforms, such as Amazon Mechanical Turk, has provided researchers with an alternative cost-effective and rapid way to collect data.   %However, these platforms are sometimes notorious for data anomalies due to the rapid nature of which data is collected.   However, the collection of fluid, natural spoken or textual interaction can be challenging, particularly between two crowd-sourced workers. In this study, we compare the performance of dialogue models for the same interaction task but collected in two different settings: in the lab vs. crowd-sourced. We find that fewer lab dialogues are needed to reach similar accuracy, less than half the amount of lab data as crowd-sourced data.. We discuss the advantages and disadvantages of each data collection method. %, which is of interest to the community in terms of platform choice and how much data will be needed to be collected.",389
" .     %     % % final paper: en-us version     %    % space normally used by the marker  This work is licensed under a Creative Commons  Attribution 4.0 International License. \\  License details:  \url{http://creativecommons.org/licenses/by/4.0/}. } The recent surge in popularity of voice assistants, such as Google Home, Apple閳ユ獨 Siri, or Amazon閳ユ獨 Alexa resulted in interest in scaling these products to more regions and languages. This means that all the components supporting Spoken Language Understanding  in these devices, such as Automatic Speech Recognition , Natural Language Understanding , and Entity Resolution  are facing the challenges of scaling the development and maintenance processes for multiple languages and dialects.  When a voice assistant is launched in a new locale, its underlying speech processing components are often developed specifically for the targeted country, marketplace, and the main language variant of that country. Many people assume that if a device ``understands'' and ``speaks'' in a specific language, for example English, it should be able to work equally well for any English-speaking country, but this is a misunderstanding. For instance, if a speaker of UK English asks a device trained on data collected in the United States ``tell me a famous football player'', it is highly unlikely that this device will provide the user's desired answer, since football means different things in the US and UK cultures. As a result, developers need to take into account not only the language or dialectal differences, but also local culture, to provide the right information in the right language setup. An increase in the number of target marketplaces often means a linear increase in effort needed to develop and maintain such locale-specific models.  NLU models, which classify the user閳ユ獨 intent and extract any significant entities from the user閳ユ獨 utterance, face the same challenge of maintaining high accuracy while being able to accommodate multiple dialects or language content. The major tasks in NLU are intent classification and slot filling. Intent classification is a task to predict what action the user intends the voice assistant to take. Slot filling is a task to identify the specific semantic arguments for the intention. For example, if the user閳ユ獨 request is to ``play Poker Face by Lady Gaga'', the user閳ユ獨 intention will be ``play music'', while in order to fulfill this command with specified details, the system needs to capture the slots for \{song name = Poker Face\}, and \{artist name = Lady Gaga\}. These tasks are called intent classification  and named entity recognition , respectively.  One common approach is to use a max-entropy  classification model for the IC task and a conditional random fields  model for the NER task. Following the advent of deep learning techniques in related fields, such as computer vision and natural language processing, deep learning is becoming more popular in NLU as well. Some of the recent multilingual approaches to NLU include, for example, the Convolutional Neural Network  model for sentence classification , or the Long Short-Term Memory  model for NER prediction . In the deep neural network architecture, the aforementioned NLU tasks can be combined into a single multi-task classification model. An increasing number of experiments also focus on multilingual setups, especially in the field of machine translation, where the task is to translate input from one language to another .  One recent thread of multilingual research centers around learning multilingual word representation. Multilingual word embeddings in the shared cross-lingual vector space have one main property: words from different languages but with similar meaning must be geometrically close. This property allows for transfer learning from one language to another in various multilingual tasks, such as dependency parsing  or classification and NER . A number of model architectures have been proposed to pre-train multilingual word representations, such as leveraging large-scaled LSTM networks trained on monolingual corpora and adversarial setup for space alignment , or transformers trained on multilingual corpora as a single language model .  Although some of these models can be used to solve IC and NER tasks by appending corresponding decoders to generate final predictions, it is not straightforward to use them in production environments due to latency and memory constrains. A different way of benefitting from larger models could be to use them for transfer learning to smaller-size models to improve their performance by initializing some parts of the model with close-to-optimal rather than random weights. In this paper, we extend the multi-task approach studied in  to a general multilingual model for IC and NER tasks, based on deep learning techniques, such as a bidirectional Long Short-Term Memory  CRF sequence labeling model for NER along with a multilayer perceptron  for IC.  We also explore multilingual transfer learning and its benefits to our setup. Transfer learning is widely adapted for zero-shot or few-shot setups, and was explored in some multilingual NLP studies , and also has been used in multi-task IC-NER models ,  yet to the best of our knowledge, there is no study applying transfer learning for data-rich target languages in a multilingual setup. In our experiment, we apply few-shot transfer learning from data-rich languages to a language with a smaller amout of training data. In additon, we also apply  transfer learning to mimic the situation of expanding the model ability to same-level-resource language with known context from another high-resource language, such that the new multilingual model will ``inherit'' context information from its ancestors. We investigate these approaches to transfer learning and their effects on model performance. We show that transfer learning can improve NLU model performance even in data-rich conditions.    In this paper, we propose a framework for building general multilingual NLU models, which can be used across different marketplaces and languages.  To choose the model with the best performance, we use language-specific test sets to evaluate the candidate models and their corresponding baseline models  along four metrics, domain accuracy, intent accuracy, slot F1, and frame accuracy. The models which win in most of the evaluation metrics are the final picks. We find that models built from a simple multi-task biLSTM-CRF model setup are comparable to standard production models in terms of latency constraints required for on-the-fly voice assistant conversational models.  We observe performance improvements in all models with the introduction of transfer learning. Encoder transfer produced the greatest improvements whereas the transfer of the decoder did not bring much change when compared to the baseline model performance, except when tested on an English test set, when the transfer learning is performed from the model trained on English data. This is due to the fact that the target non-English language contains slots or intents which are not included in the pre-trained model, thus the decoder fails to predict correct classes simply because they are missing in the vocabulary. To mitigate this effect, a decoder with default initialization gives better performance because it now can embrace all available slots and intents in the target language realm.  Furthermore, we find that a model pre-trained in a multilingual setup performs better than the one trained on a monolingual data set. This confirms that a multilingual model built based on lexically and orthographically similar languages may provide more beneficial context information to any similar target language. Experimental result on Hindi show that such a multilingual model can work even for non-alike languages with the same or better performance improvement. This confirms that a common multilingual model can be used to support multiple language with better results than a set of monolingual models.  With a single general multilingual NLU model, bootstrapping new languages can be faster as we can use cross-lingual contextual information from all existing high-resource languages. At the same time, maintaining only one model requires much less effort in terms of regular model updates.     
"," 	With the recent explosion in popularity of voice assistant devices, there is a growing interest in making them available to user populations in additional countries and languages. However, to provide the highest accuracy and best performance for specific user populations, most existing voice assistant models are developed individually for each region or language, which requires linear investment of effort. In this paper, we propose a general multilingual model framework for Natural Language Understanding  models, which can help bootstrap new language models faster and reduce the amount of effort required to develop each language separately. We explore how different deep learning architectures affect multilingual NLU model performance. Our experimental results show that these multilingual models can reach same or better performance compared to monolingual models across language-specific test data while require less effort in creating features and model maintenance.",390
"  . 	%  	% % final paper: en-us version  	% 	%	   % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/}. }  The widespread dissemination of fake news has lead to a significant influence on personal fame, public trust, and security. For example, spreading misinformation, such as ``Asians are more vulnerable to novel coronavirus''~\footnote{https://www.thestar.com.my/news/regional/2020/03/11/myth-busters-10-common-rumours-about-covid-19} about COVID-19 has very serious repercussions, making people ignore the harmfulness of the virus and directly affecting public health. Research has shown that misinformation spreads faster, farther, deeper, and more widely than true information. Therefore, fake news detection on social media has attracted tremendous attention recently in both research and industrial fields.   Early research on fake news detection mainly focused on the design of effective features from various sources, including textual content, user profiling data, and news diffusion patterns. Linguistic features, such as writing styles and sensational headlines, lexical and syntactic analysis, have been explored to separate fake news from true news. Apart from linguistic features, some studies also proposed a series of user-based features, and temporal features about the news diffusion. However, these feature-based methods are very time-consuming, biased, and require a lot of labor to design. Besides, these features are easily manipulated by users.   To solve the above problems, many recent studies apply various neural networks to automatically learn high-level representations for fake news detection. For example, recurrent neural network , convolutional neural network , matrix factorization and graph neural network are applied to learn the representation of content and diffusion graph of news. These methods only apply more types of information for fake news detection, but paying little attention to early detection. Moreover, these models can only detect fake news in consideration of all or a fixed proportion of repost information, while in practice they cannot detect fake news in the early stage of news propagation. Some studies explore to detect fake news early by relying on a minimum number of posts. The main limitation of these methods is that they ignore the importance of publishers' and users' credibility for the early detection of fake news.   When we humans see a piece of breaking news, we firstly may use common sense to judge whether there are factual errors in it. At the same time, we will also consider the reputation of the publishers and reposted users. People tend to believe the news from a trusted and authoritative source or the news shared by lots of users with a good reputation. If the publisher is reliable, we tend to believe this news. On the other hand, if the news is reposted by many low-reputation users in a short period, it may be that some spammers tried to heat up on the news, resulting in lower credibility of the news.   Inspired by the above observation, we explicitly take the credibility of publishers and users as supervised information, and model fake news detection as a multi-task classification task. We can annotate a small part of publishers and users by their historical publishing and reposting behaviors. Although the credibility of publishers and users does not always provide correct information, they are necessary complementary supervised information for fake news detection. To make the credibility information generalized to other unannotated users, we construct a heterogeneous graph to build the connections of publishers, news, and users. Through a graph-based encoding algorithm, every node in the graph will be influenced by the credibility of publishers and users.    In this paper, we address the following challenges:  How to fully encode the heterogeneous graph structure and news content; and  How to explicitly utilize the credibility of publishers and users for facilitating early detection of fake news. To tackle the above challenges, we propose a novel structure-aware multi-head attention network for the early detection of fake news. Firstly, we design a structure-aware multi-head attention module to learn the structure of the publishing graph and produce the publisher representations for the credibility prediction of publishers. Then, we apply the structure-aware multi-head attention module to encode the diffusion graph of the news among users and generate user representations for the credibility prediction of users. Finally, we apply a convolutional neural network to map the news text from word embedding to semantic space and utilize the fusion attention module to combine the news, publisher, and user representations for early fake news detection.   The contributions of this paper can be summarized as follows:  	            and Future Work This paper proposes a novel structure-aware multi-attention network, which combines news content, the heterogeneous graphs among publishers and users, and jointly optimizes the task of false news detection and user credibility prediction for early fake news detection. Different from most existing research extracting hand-crafted features or deep learning methods, we explicitly treat the credibility of publishers and users as a kind of weakly supervised information for facilitating fake news detection. Extensive experiments conducted on three real-world datasets show that the proposed model can significantly surpass other state-of-the-art models on both fake news classification and early detection task.    
"," The\let\thefootnote\relax\footnotetext{* Corresponding author.} dissemination of fake news significantly affects personal reputation and public trust. Recently, fake news detection has attracted tremendous attention, and previous studies mainly focused on finding clues from news content or diffusion path. However, the required features of previous models are often unavailable or insufficient in early detection scenarios, resulting in poor performance. Thus, early fake news detection remains a tough challenge. Intuitively, the news from trusted and authoritative sources or shared by many users with a good reputation is more reliable than other news. Using the credibility of publishers and users as prior weakly supervised information, we can quickly locate fake news in massive news and detect them in the early stages of dissemination.  In this paper, we propose a novel Structure-aware Multi-head Attention Network , which combines the news content, publishing, and reposting relations of publishers and users, to jointly optimize the fake news detection and credibility prediction tasks. In this way, we can explicitly exploit the credibility of publishers and users for early fake news detection. We conducted experiments on three real-world datasets, and the results show that SMAN can detect fake news in 4 hours with an accuracy of over 91\%, which is much faster than the state-of-the-art models. The source code and dataset can be available at https://github.com/chunyuanY/FakeNewsDetection.",391
"  Real-world events such as sports games or elections involve competing teams, each with capabilities and tactics, aiming to win . The performance of such teams is typically not only dependent on the teams' abilities but also on the environment within which they operate. For example, a political party may have the best orators and policies but their opponents may be better at getting votes in key areas. Similarly, a top football team may be playing the worst team in a league but the fact that the latter may be facing relegation  may provide them with extra motivation to win the game. Given this, in many cases, the performance of such teams may not be easily predictable.    % In particular, in sporting events many human factors impact how a team performs in given games. There are often situations that would be very hard to represent in numbers and statistics alone. For example, sporting rivalries often affect human emotions and team performance and teams fighting to avoid relegation from a league often obtain unexpected results.   Traditional AI and machine learning techniques to predict the outcome of real-world events tend to focus on the use of statistical machine learning using historical data about the individual teams .  However, as per the examples above, historical performance may not be useful when team performance may be dependent on dynamic factors such as human performance  or environmental variables . In turn, humans can be better judges than algorithms when faced with previously unseen situations. Journalists, online communities, and experienced analysts may be better at evaluating human and environmental elements to forecast an outcome. For example, one approach of looking at more than just statistics in sports have been through sentiment analysis on social media platforms. Schumaker, Jarmoszko and  Labedz  use this approach to predict English Premier League  results and achieve an accuracy of 50\% and  show use of similar analysis being performed for American Football results in the National Football League  predicting the winner 63.8\% of the time. However, these approaches focus on opinion aggregation rather than trying to extract the potential indicators of performance for individual human teams from human experts.  Against this background, we set new baselines for results when predicting real-world sporting events involving humans based on the combination of Natural Language Processing  and statistical machine learning techniques. In more detail, we focus specifically on football games in the EPL using match previews from the media alongside statistical machine learning  techniques. The prediction of football match outcomes is a challenging computational problem due to the range of parameters that can influence match results. To date, probabilistic methods devised since the seminal work of Maher  have generated fairly limited results and appear to have reached a glass ceiling in terms of accuracy. By using media previews we can improve on the accuracy of current approaches for match outcome prediction. By so doing, we show that by incorporating human factors into our model, rather than just basic performance statistics, we can improve accuracy . Thus, the contributions of this paper are as follows:     In the next section we discuss the match outcome prediction problem for football and the new feature set we explore.  %The rest of this paper is organised as follows. Section  discusses the problem that we are aiming to solve, Section  outlines how we model human opinion and use this to predicting real-world football games. Section  provides the detail of how we test our models and set the baseline for the prediction accuracy. Finally, Section  concludes.  %   This paper has presented a novel application-focused dataset and has set new baselines of 63.19\  accuracy for predicting games of English Premier League football across a three season period using a novel dataset which we provide as part of this paper. We showed that the application of combining human opinion and machine learning to make predictions can boost the accuracy of traditional methods and those using sentiment analysis on social media. We show that we boost these methods by 6.9\  in terms of outcome accuracy and that the model accuracy increases as the season progresses and human factors/emotions begin to play a bigger part in the game.  \clearpage  
"," In this paper, we present a new application-focused benchmark dataset and results from a set of baseline Natural Language Processing and Machine Learning models for prediction of match outcomes for games of football . By doing so we give a baseline for the prediction accuracy that can be achieved exploiting both statistical match data and contextual articles from human sports journalists. Our dataset is focuses on a representative time-period over 6 seasons of the English Premier League, and includes newspaper match previews from The Guardian. The models presented in this paper achieve an accuracy of 63.18\% showing a 6.9\% boost on the traditional statistical methods.",392
"  Deep neural networks are successful at various morphological tasks as exemplified in the yearly SIGMORPHON Shared Task. However these neural networks operate with continuous representations and weights which is in stark contrast with traditional, and hugely successful, rule-based morphology. There have been attempts to add rule-based and discrete elements to these models through various inductive biases.   In this paper we tackle two morphological tasks and the copy task as a control with an interpretable model,  refers to the fact that the patterns are intended to learn abstract representations that may have multiple surface representations, which , while the abstract patterns,  throughout the paper.  An important upside of \footnote{also called encoder-decoder model} model, and add an LSTM decoder.  We initialize the decoder's hidden state with the final scores of each \sopa pattern and we also apply Luong's attention on the intermediate outputs generated by \sopa. We call this model \sopaseq.  We compare each setup to a sequence-to-sequence with a bidirectional LSTM encoder, unidirectional LSTM decoder and Luong's attention.  We show that \sopaseq is often competitive with the LSTM baseline while also interpretable by design. \sopaseq is especially good at \morphana, often surpassing the LSTM baseline, which confirm our linguistic intuition namely that subword patterns are useful for extracting morphological information.  We also compare these models using a generalized form of Jaccard-similarity and we find that some trends coincide with linguistic intuition.     We presented an application of Soft Patterns -- a finite state automaton parameterized by a neural network -- as the encoder of a sequence-to-sequence model. We show that it is competitive with the popular LSTM encoder on character-level copy and morphological tagging, while providing interpretable patterns.  We analyzed the behavior of \sopa encoders on \morphana, \lemmatization and \copytask by computing the average Jaccard similarity between the patterns extracted from the source side. We found two trends that coincide with linguistic intuition. One is that \lemmatization and morphological analysis require patterns that match less similar subwords than the other two task pairs. The other one is that \copytask and morphological analysis are more similar in languages with rich inflectional morphology.  
","  We examine the role of character patterns in three tasks: morphological analysis, lemmatization and copy. We use a modified version of the standard sequence-to-sequence model, where the encoder is a pattern matching network. Each pattern scores all possible N character long subwords  on the source side, and the highest scoring subword's score is used to initialize the decoder as well as the input to the attention mechanism.  This method allows learning which subwords of the input are important for generating the output. By training the models on the same source but different target, we can compare what subwords are important for different tasks and how they relate to each other. We define a similarity metric, a generalized form of the Jaccard similarity, and assign a similarity score to each pair of the three tasks that work on the same source but may differ in target. We examine how these three tasks are related to each other in \goodlangno languages. Our code is publicly available.\footnote{https://github.com/juditacs/deep-morphology}",393
"  Infusing emotions into conversation systems can substantially improve its usability and promote customers' satisfaction. Moreover, perceiving emotions sufficiently is the core premise of expressing emotions. In real-life scenarios, humans can instinctively perceive complex or subtle emotions from multiple aspects, including the emotion flow of dialogue history, facial expressions and personalities of speakers, and then express suitable emotions for feedback. Figure shows the organization of multi-source information in a dialogue graph and the relationship between them.        We propose a heterogeneous graph-based framework to understand dialogue content and fully perceive complex and subtle emotions from multi-source knowledge to generate coherent and emotional response. Experimental results and analysis demonstrate the effectiveness and generalizability of our model, which can be easily adapted to different number of knowledge sources. In the future, we would like to infuse knowledge from more sources and further investigate various relations between them to further improve the quality of responses.  
"," The success of emotional conversation systems depends on sufficient perception and appropriate expression of emotions. In a real-world conversation, we firstly instinctively perceive emotions from multi-source information, including the emotion flow of dialogue history, facial expressions, and personalities of speakers, and then express suitable emotions according to our personalities, but these multiple types of information are insufficiently exploited in emotional conversation fields. To address this issue, we propose a heterogeneous graph-based model for emotional conversation generation. Specifically, we design a  to represent the conversation content  with a heterogeneous graph neural network, and then predict suitable emotions for feedback. After that, we employ an  to generate a response not only relevant to the conversation context but also with appropriate emotions, by taking the encoded graph representations, the predicted emotions from the encoder and the personality of the current speaker as inputs. Experimental results show that our model can effectively perceive emotions from multi-source knowledge and generate a satisfactory response, which significantly outperforms previous state-of-the-art models.",394
" Text classification is one of the fundamental tasks in natural language processing  with wide applications such as sentiment analysis, news filtering, spam detection and intent recognition. Plenty of algorithms, especially deep learning-based methods, have been applied successfully in text classification, including recurrent neural networks ,  convolutional networks   . More recently, large pre-training language models such as ELMO , BERT , Xlnet  and so on have also shown their outstanding performance in all kinds of NLP tasks, including text classification.   Although numerous deep learning models have shown their success in text classification problems, they all share the same learning paradigm: a deep model for text representation, a simple classifier to predict the label distribution and a cross-entropy loss between the predicted probability distribution and the one-hot label vector. However, this learning paradigm have at least two problems:  In general text classification tasks, one-hot label representation is based on the assumption that all categories are independent with each other. But in real scenarios, labels are often not completely independent and instances may relate to multiple labels, especially for the confused datasets that have similar labels. As a result, simply representing the true label by a one-hot vector fails to take the relations between instances and labels into account, which further limits the learning ability of current deep learning models.  The success of deep learning models heavily relies on large annotated data, noisy data with labeling errors will severely diminish the classification performance, but it is inevitable in human-annotated datasets. Training with one-hot label representation is particularly vulnerable to mislabeled samples as full probability is assigned to a wrong category. In brief, the limitation of current learning paradigm will lead to  confusion in prediction that the model is hard to distinguish some labels, which we refer as label confusion problem . A label smoothing  method is proposed to remedy the inefficiency of one-hot vector labeling , however, it still fails to capture the realistic relation among labels, therefore not enough the solve the problem.      In this work, we propose a novel Label Confusion Model  as an enhancement component to current deep learning text classification models and make the model stronger to cope with label confusion problem. In particular, LCM learns the representations of labels and calculates their semantic similarity with input text representations to estimate their dependency, which is then transferred to a label confusion distribution . After that, the original one-hot label vector is added to the LCD  with a controlling parameter and normalized by a softmax function to generate a simulated label distribution . We use the obtained SLD to replace the one-hot label vector and supervise the training of model training. With the help of LCM, a deep model not only capture s the relations between instances and labels, but also learns the overlaps among different labels, thus, performs better in text classification tasks. We conclude our contributions as follows:      In this work, we propose Label Confusion Model  as an enhancement component to current text classification models to improve their performance. LCM can capture the relations between instances and labels as well as the dependency among labels. Experiments on five benchmark datasets proved LCM's enhancement on several popular deep learning models such as LSTM, CNN and BERT.  Our future work include the following directions:  Designing a better LCM structure for computer vision tasks and conducting more experiments on image classification.  Generalizing the LCM method to multi-label classification problems and label distribution prediction.   
"," Representing a true label as a one-hot vector is a common practice in training text classification models. However, the one-hot representation may not adequately reflect the relation between the instances and labels, as labels are often not completely independent and instances may relate to multiple labels in practice. The inadequate one-hot representations tend to train the model to be over-confident, which may result in arbitrary prediction and model overfitting, especially for confused datasets  or noisy datasets . While training models with label smoothing  can ease this problem in some degree, it still fails to capture the realistic relation among labels. In this paper, we propose a novel Label Confusion Model  as an enhancement component to current popular text classification models. LCM can learn label confusion to capture semantic overlap among labels by calculating the similarity between instances and labels during training and generate a better label distribution to replace the original one-hot label vector, thus improving the final classification performance. Extensive experiments on five text classification benchmark datasets reveal the effectiveness of LCM for several widely used deep learning classification models. Further experiments also verify that LCM is especially helpful for confused or noisy datasets and superior to the label smoothing method.",395
" Over recent years, various task-oriented conversational agents, such as Amazon Alexa, Apple閳ユ獨 Siri, Google Assistant, and Microsoft閳ユ獨 Cortana, have become more popular in people閳ユ獨 everyday life and are expected to be highly intelligent. For the NLU component, this means that we expect models to perform recognition of the actions and entities within a user閳ユ獨 request with high accuracy. When first training an NLU model on a new language , there is a strong requirement for high quality annotated data that would support the most common user requests across a range of domains. As the modeling space expands to support new features and additional languages, NLU models are regularly re-trained on updated data sets to ensure support for these new functions. The major bottleneck in both of these processes is the labor and cost associated with collecting and annotating new training utterances for every new feature or language.   Recent advances in machine learning methods, including the use of techniques such as transfer learning~ and active learning, can lead to more efficient data usage by NLU models and therefore decrease the need for annotated training data. Additionally, data augmentation models are being widely explored. The advantage of data augmentation is that once synthetic data is generated, it can be ingested into subsequent models without additional effort, allowing for faster experimentation.   NLU models in dialog systems can perform a variety of tasks. In this study, we will focus on three of them: Domain classification  -- identify the domain that the user request belongs to , Intent classification  -- extract actions requested by users , and Named Entity Recognition  -- identify and extract entities  from user requests.  For each utterance we expect our NLU model to output a domain, intent, and set of extracted entities with corresponding tags. For example, if a user requests ``play Bohemian Rhapsody by Queen'', we expect the NLU model to return \{domain: music, intent: play\_song, named\_entities: [, ]\}. We call this output annotation, and the utterance along with annotation is called an annotated utterance. Named entities with corresponding labels are called slots.  For our NLU model to perform well on real-time user requests, we need to train it on a large dataset of diverse annotated utterances. However, there could be some areas of functionality where large datasets for training are not available. To boost model performance in situations where training data is limited, we use synthetic data generated from a small set of unique utterances that cover the basic functionality of the user experience, called Golden utterances. We leverage a Sequence Generative Adversarial Networks  introduced by~ to generate new utterances from this ``seed'' set, and use these generated utterances to augment training data and evaluate the performance of the classification and recognition tasks. We also investigate how the metrics that we use to evaluate the quality of the generated synthetic data links to the performance boost in the underlying tasks.     In this paper, we evaluate the use of the SeqGAN model for synthetic annotated data generation to boost NLU model performance. We have shown that adding synthetic data to bolster our Goldens can significantly improve DNN model performance in intent classification and named entity recognition tasks. We propose a token-level reward with Monte Carlo search rollout to guide the generator model, that showed better performance when compared with a regular token-level reward implementation, sentence-level reward implementations both with and without Monte Carlo tree search, and with a pure upsampling strategy. We also show that using SeqGAN together with embeddings pre-trained on high-resource domains to generate synthetic data can significantly improve the performance of low-resource domains. Embeddings pre-trained on different tasks can carry over the information they have learned and that can be especially useful in low-resource model building scenarios.      \onecolumn   
"," Data sparsity is one of the key challenges associated with model development in Natural Language Understanding  for conversational agents. The challenge is made more complex by the demand for high quality annotated utterances commonly required for supervised learning, usually resulting in weeks of manual labor and high cost. In this paper, we present our results on boosting NLU model performance through training data augmentation using a sequential generative adversarial network . We explore data generation in the context of two tasks, the bootstrapping of a new language and the handling of low resource features. For both tasks we explore three sequential GAN architectures, one with a token-level reward function, another with our own implementation of a token-level Monte Carlo rollout reward, and a third with sentence-level reward. We evaluate the performance of these feedback models across several sampling methodologies and compare our results to upsampling the original data to the same scale. We further improve the GAN model performance through the transfer learning of the pre-trained embeddings. Our experiments reveal synthetic data generated using the sequential generative adversarial network provides significant performance boosts across multiple metrics and can be a major benefit to the NLU tasks.",396
"  	Encoder-decoder architecture~ has been extensively used in neural machine translation ~. Given a source sentence, an encoder firstly converts it into hidden representations, which are then conditioned by a decoder to generate the target sentence. Attention mechanism~ is very effective in learning the alignment between a source sentence and a target sentence. Hence, attention mechanism is usually used in the architecture to improve its capability, such as capturing long-distance dependencies.  	Similar to traditional machine learning efforts~, some recent approaches in deep learning attempt to improve encoder-decoder architecture with multiple passes of decoding~. NMT refers this to polish mechanism~. Under this scheme, more than one translations are generated for a source sentence and, except for the first translation, each of them is based on the translation from the previous decoding pass. While these methods have achieved promising results, they lack a proper termination policy to the multi-turn process.  adopt a fixed number of decoding passes that can be inflexible in deciding the optimal number of decoding passes.  use reinforcement learning ~ to automatically decide the optimal number of decoding passes. However, RL is unstable due to its high variance of gradient estimation and objective instability~. Since these methods may have premature termination or over translation, their potential can be limited.  	 	 To address this problem, we propose a novel framework, Rewriter-Evaluator, in this paper. It consists of a rewriter and an evaluator. The translation process involves multiple passes. Given a source sentence, at every pass, the rewriter generates a new target sequence aiming at improving the translation from prior passes, and the evaluator measures the translation quality to determine whether to terminate the rewriting process. We also propose a prioritized gradient descent  method that facilitates training the rewriter and the evaluator jointly. The essential idea is using a priority queue to improve sampling efficiency by collecting the translation cases that yield low scores from the evaluator for next-pass rewriting. The size of the queue is a few times larger than the batch size. Although Rewriter-Evaluator involves multiple decoding passes, training time using PGD method is comparable to that of training an encoder-decoder~ that doesn't have multiple decoding passes.  	  	 We apply Rewriter-Evaluator to improve the widely used NMT models,  RNNSearch~ and Transformer~. Extensive experiments have been conducted on two translation tasks, Chinese-English and English-German, to verify the proposed method. The results demonstrate that the proposed framework notably improves the performance of NMT models and significantly outperforms prior methods.     	In this work, we present a novel framework, Rewriter-Evaluator, that aims at achieving proper terminations for multi-pass decoding. It consists of a rewriter and an evaluator. At each translation pass, the rewriter generates a new translation to improve previous translations and the evaluator estimates the translation quality to determine whether to terminate the process. We also propose a prioritized gradient descent method that biases the training samples toward rewriting those low-quality translations. This enables training Rewriter-Evaluator that has multiple pass decodings to have comparable training time to training encoder-decoder models that only have single pass decoding. We have applied Rewriter-Evaluator to improve RNNSearch and Transformer. Extensive experiments have been conducted on two translation tasks, Chinese-English and English-German, showing that the proposed framework has notably improved the performances of NMT models and has significantly outperformed previous methods.   
"," 	 	Encoder-decoder architecture has been widely used in neural machine translation . A few methods have been proposed to improve it with multiple passes of decoding. However, their full potential is limited by a lack of appropriate termination policy. To address this issue, we present a novel framework, Rewriter-Evaluator. It consists of a rewriter and an evaluator. Translating a source sentence involves multiple passes. At every pass, the rewriter produces a new translation to improve the past translation and the evaluator estimates the translation quality to decide whether to terminate the rewriting process. We also propose a prioritized gradient descent  method that facilitates training the rewriter and the evaluator jointly. Though incurring multiple passes of decoding, Rewriter-Evaluator with the proposed PGD method can be trained with similar time to that of training encoder-decoder models. We apply the proposed framework to improve the general NMT models . We conduct extensive experiments on two translation tasks, Chinese-English and English-German, and show that the proposed framework notably improves the performances of NMT models and significantly outperforms previous baselines.",397
" The problem of predicting citation counts of papers has been a long-standing research problem. Predicting citation counts allows us to better understand the relationship between a paper %and its citation count and gives us insight into what affects a paper's impact. and its impact. However, prior research has viewed this as a static prediction problem, i.e. only predicting a single citation count at a static point in time. %With the natural development of new papers being published,  %However,  %Viewing this as a static problem  This ignores the natural development of the data as new papers are being published. Here, we propose to view the problem as a sequence prediction task, with models then having the ability to capture the evolving nature of citations.  %By extending the problem to a sequence prediction problem,  This, in turn, requires a dataset to contain the papers' citation counts over a period of time, which adds a temporal element to the data, which can then be encoded by sequential machine learning models, such as Long short-term memory models . Additionally, scholarly documents exhibit a natural graph-like structure in their citation networks. Given recent developments in modeling such data and prior research showing that modeling input as graphs can be beneficial, we hypothesize that modeling a paper's citation network is useful for predicting citation counts over time.   In this paper, we consider citation networks, a dynamic graph which evolves over time as new citations and papers are added to the network. Leveraging the structured data in the graph allows us to discover complex relationships between papers. We want to tap into that knowledge and treat the citation data as a network, such that we can further exploit topological information and not just temporal information. By doing so, we investigate the hypothesis of paper citation counts being correlated with features such as authors, venue, and topics.  We use the well-established Semantic Scholar dataset to construct our citation network. Its meta-data allows us to construct a dynamic citation network which covers a  year time-line, with an updated graph for each year. The Semantic Scholar dataset's meta-data also contains information about each paper's authors, venue, and topics, allowing us to study the correlation between these features and the citation count of a paper when considering the evolving nature of the citation network. The correlation between these features and citation counts is well known and studied by prior work. Prior studies show that citations are correlated and there is a strong correlation between features such as authors, but are limited by only predicting a single citation, and not predicting the natural evolution of a papers growth.   We propose to use the constructed dynamic citation network  to predict the trajectory of the number of citations papers will receive over time, a new sequence prediction task introduced in this work. Furthermore, we propose an encoder-decoder model to solve the proposed task, which uses graph convolutional layers to exploit the graphs' topological features and an LSTM to model the temporal component of the graphs. We compare our model against a standard GCN and standard LSTM, which individually incorporate either the topological information or the temporal information, but not both.  Our contributions are as follows: 1) A dynamic citation network based on the Semantic Scholar dataset. The dynamic citation network contains  time-steps, with an updated graph at each time-step, based on yearly information. 2) We introduce the task of sequence citation count prediction. 3) A novel encoder-decoder model based on a GCN and LSTM to extract the dynamic graph's topological and temporal components. 4) A thorough study of the correlation between citation counts and temporal components.   are as follows:        \fi    In this paper, we propose the task of citation sequence prediction. For this, we introduce a dataset of scholary documents based on a dynamic citation graph evolving of  years, starting from a single node growing to a large graph. We further study the effect of temporal and topological information, and propose a model to benefit from both information . Our results show that utilizing both the temporal and topological information is superior to only utilizing either the temporal or topological information. Using the proposed model, we study the effect of different features, to identify which information is most predictive of a paper's citation count over time. We found author information to be predictive and informative over time.  In future work, the impact of training a single GCN on the dynamic graph could be explored, since the error over time of the GCN is deteriorating fast.   
"," %Citation count prediction is the task of predicting the number of citations, which a paper has gained after a given period. Prior work view this as a static prediction task, but due to papers and their citations develops over time, predicting the sequence of citations will also capture the papers' development. We further employ the recent development in graph structured data and view the papers as a citation network, linked by papers citations. Viewing the papers as a citation network allows us to exploit the topological information of the citation network. While no prior citation network allow for predicting the development of a paper's citations over time. Therefore, we use the well known Semantic Scholar dataset to construct a dynamic citation network, i.e., a graph which evolves over time. This dynamic citation network spans over $42$. Using the constructed dynamic citation network, we introduce the task of sequence citation count prediction. To solve the introduced task, we propose a model which exploits topological and temporal information. We compare the proposed model against baseline models and analyze the performance. Furthermore, we study the importance of topological and temporal features for predicting a paper's citation count. \andreas{findings TBD} % revised Citation count prediction is the task of predicting the number of citations a paper has gained after a period of time. Prior work viewed this as a static prediction task. As papers and their citations evolve over time, considering the dynamics of the number of citations a paper will receive would seem logical. Here, we introduce the task of sequence citation prediction, where the goal is to accurately predict the trajectory of the number of citations a scholarly work receives over time. We propose to view papers as a structured network of citations, allowing us to use topological information as a learning signal. Additionally, we learn how this dynamic citation network changes over time and the impact of paper meta-data such as authors, venues and abstracts. To approach the introduced task, we derive a dynamic citation network from Semantic Scholar which spans over $42$ years. We present a model which exploits topological and temporal information using graph convolution networks paired with sequence prediction, and compare it against multiple baselines, %where we will use GCN and LSTM as standalone, to  testing the importance of topological and temporal information and analyzing model performance.  Our experiments show that leveraging both the temporal and topological information greatly increases the performance of predicting citation counts over time.",398
"   Recent advances in open domain question answering  have mostly revolved around machine reading comprehension   where the task is to read and comprehend a given text and then answer questions based on it. However, most recent work in MRC has only been in English ), which is trained on Wikipedia articles from 104 languages and equipped with a 120k shared wordpiece vocabulary, has encouraged a lot of progress on cross-lingual tasks  XNLI , NER  and QA  by performing zero-shot training: train on one language and test on unseen target languages.  In this work, we focus on multilingual QA and, in particular, on two recent large-scale datasets: MLQA and TyDiQA\footnote{All uses of TyDiQA in our paper refer to the Gold Passage task.}. Both datasets contain English QA pairs but also examples from 13 other diverse languages.  Some examples are shown in Figure . MLQA evaluates two challenging scenarios: 1) Cross-Lingual Transfer   when the question and the context are in the same language, and 2) Generalized Cross-lingual Transfer  when the question is in one language  and the context is in another language .  %In both cases, MLQA is zero-shot because it does not provide training data in any language. \avi{Maybe remove this previous sentence?} % TyDiQA consists of QA examples in English and 8 other languages.  TyDiQA is designed for XLT only. Both datasets are challenging for multilingual QA due to the large number of languages and the variety of linguistic phenomena they encompass .  Ideally, we want to build QA systems for all existing languages but it is impractical to collect manually labeled training data for all of them.  In the absence of labeled data,  suggested several research directions for pushing the boundaries in multilingual QA, including zero-shot QA, exploring data augmentation with machine translation, as well as effective transfer learning. These are avenues we explore in our work in addition to asking the following research questions:\\             \\      Prior work proposes zero-shot transfer learning from English SQuAD data  to other languages using only a pre-trained LM and competitive results are achieved  on MLQA  and TyDiQA . We venture beyond zero-shot training by first exploring data augmentation  on top of their underlying model. We achieve this by using translation methodologies  to augment the English training data.       We use machine translation to obtain additional silver labeled data allowing us to improve cross-lingual transfer at a low cost.  Our approach introduces several multilingual extensions to the SQuAD training data: translating just the questions but keeping the context in English, translating just the context but keeping the question in English, and translating the question and the context to other languages. This enables us to augment the original English human-labeled training examples with 14 times more multilingual silver-labeled QA pairs.\\          \\         %To do better MLQA, we believe it is important that the model      Our hypothesis is that we can make the cross-lingual QA transfer more effective if we can bring the embeddings in a multilingual pre-trained LM closer to each other in the same semantic space. To answer a question in French it should suffice to train the system on Hindi and not be necessary to train a system on the target language:  hence, French and Hindi should look as if they are the same language.     We propose two approaches to explore cross-lingual transfer:              In our first approach, we propose a novel strategy based on adversarial training  . We investigate how the addition of a language-adversarial task during QA finetuning for a pretrained LM can significantly improve the cross-lingual transfer performance while causing the embeddings in the LM to become less language-dependent.           In our second approach, we develop a novel Language Arbitration Framework  to consolidate the embedding representation across languages using properties of the translation.         We train additional auxiliary tasks  making sure an English question and its translation in Arabic produces the same answer when they see the same input context in Spanish. The intuition behind language arbitration is that while we are training the model on English and translated examples, the proposed multi-lingual objectives bring the language-specific embeddings closer to the English embeddings.\\               Overall, our main contributions in this paper are as follows:         \setlength          more multi-lingual          % ``pseudo'' human-labeled QA pairs than SQuAD.         silver-labeled QA pairs than SQuAD.                  In this work, we highlight open challenges in the existing multilingual approach by  and . Specifically, we show that large pre-trained multi-lingual LMs are not enough for this task. We produce  several novel strategies for multilingual QA that go beyond zero-shot training and outshine the previous baseline built on top of \mbert{}. We present a translation model that has 14 times more training data. Further, our AT and LAF strategies utilize translation as data augmentation to bring the language-specific embeddings of the LM closer to each other. These approaches help us significantly improve the cross-lingual transfer. Empirically, our models demonstrate strong results and all approaches improve over the previous ZS strategy. We hope these techniques spur further research in the field such as exploring other multilingual LMs and invoking additional networks on top of large LMs for multilingual NLP. 
"," Prior work on multilingual question answering has mostly focused on using large multilingual pre-trained language models  to perform zero-shot language-wise learning: train a QA model on English and test on other languages. In this work, we explore strategies that improve cross-lingual transfer by bringing the multilingual embeddings closer in the semantic space.  Our first strategy augments the original English training data with machine translation-generated data. This results in a corpus of multilingual silver-labeled QA pairs that is 14 times larger than the original training set. In addition, we propose two novel strategies, language adversarial training and language arbitration framework, which significantly improve the  cross-lingual transfer performance and result in LM embeddings that are less language-variant. Empirically, we show that the proposed models outperform the previous zero-shot baseline on the recently introduced multilingual MLQA and TyDiQA  datasets.",399
"   %   Researchers' ability to automate natural language processing has grown exponentially over the past few years, particularly with the advent of the Transformer architecture . Despite the fact that recent machine learning methods achieve impressive and almost human-level performance on tasks such as dialogue modeling  and natural language generation , many intelligent voice assistants still rely on rule-based architectures and cached responses in open domain dialogue . This is primarily due to the lack of controls in deep learning architectures for producing specific phrases, tones, or topics, which makes these models inherently unpredictable and therefore too risky for most entities - corporate or otherwise - who wish to deploy public-facing intelligent agents. For example, it is often desirable for a conversational agent to maintain a specific identity  throughout an exchange of dialogue and it is currently impossible to condition deep learning algorithms to maintain a coherent identity across dialogue without training them on highly specialized  data sets. Fine-tuning on these specialized data sets comes with an additional, significant cost: it can lead to catastrophic forgetting of the language model . Despite this aspect of fine-tuning, current state-of-the-art methods  require fine-tuning  of the entire network when their original data set proves unsuitable for a given task , even if the language being modeled is the same across tasks. Furthermore, models produced by current methods are almost entirely uninterpretable and therefore generally difficult to test for egregious failure cases.  %   In this paper, we address both the issue of content control as well as that of catastrophic forgetting induced by fine-tuning. We define `content control' as being able to command a network to either incorporate or eschew an exact word, phrase, topic, style, or sentiment in its output, and therefore attempt a more granular level of control than the purely topic/style-level control that has been published in recent literature . We also introduce an alternative to fine-tuning neural language models and demonstrate through experimentation that the high-cost of overwriting model weights through fine-tuning  often fails to induce the desired behavior in generalized settings.  %is inspired by the ``No Free Lunch"" theorems introduced by Wolpert \& Macready  in that we seek to avoid training a neural network to simultaneously model language and act on explicit commands.  Instead,  we recast the problem of control in natural language generation as one of combining separate models - one of the natural language itself and one of high-level command responses - to produce desired linguistic output. In doing so, we develop a framework for interpreting and subsequently controlling the hidden activations of a pretrained neural network without any adjustments being made to the pretrained model. This framework is biologically consistent with the findings of Knutson et al., who discovered that neural pathways in humans are inhibited by other neuron clusters , and has applications to other neural network architectures and questions outside the domain of controllable text generation.      The key contribution and insight of this paper is the use of a small, independently trained neural network called a Neural Programming Interface  to influence the behavior of a large pretrained model. In contrast to fine-tuning, this approach retains the linguistic breadth and versatility of the original model, allowing the possibility to control for multiple factors either in sequence or simultaneously, and to induce behavior in the language model contrary to the patterns baked into linguistic training data . We have demonstrated that this approach can be used to produce specific words within a GPT-2 model's output text, to pivot away from a specific word, and to create a linguistic aversion to offensive speech. We believe that future avenues for this research include investigations of the use for NPI models in network interpretability, regulation, and bias mitigation.  
"," %   Current solutions to the problem of controlling generative neural language models are usually formulated under a training paradigm in which the language model is trained to simultaneously model natural language and respond to high-level commands. We recast the problem of control in natural language generation as that of learning to interface with a pretrained language model to generate desired output, just as Application Programming Interfaces  control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network  learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model in real time to produce desired outputs, such that no permanent changes are made to the weights of the original language model.     It is notoriously difficult to control the behavior of artificial neural networks such as generative neural language models. We recast the problem of controlling natural language generation as that of learning to interface with a pretrained language model, just as Application Programming Interfaces  control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network  learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model to produce desired outputs. Importantly, no permanent changes are made to the weights of the original model, allowing us to re-purpose pretrained models for new tasks without overwriting any aspect of the language model. We also contribute a new data set construction algorithm and GAN-inspired loss function that allows us to train NPI models to control outputs of autoregressive transformers. In experiments against other state-of-the-art approaches, we demonstrate the efficacy of our methods using OpenAI闁炽儲鐛 GPT-2 model, successfully controlling noun selection, topic aversion, offensive speech filtering, and other aspects of language while largely maintaining the controlled model's fluency under deterministic settings. %Finally, we describe the ethical implications of this work. %   Applications for this approach include re-purposing a pretrained model  for a new task without a specialized data set in the problem domain. We present experimental results from training several NPI models to control the outputs of OpenAI's GPT-2 language model , as well as a novel data curation approach in which hidden activations of an uninterpretable pretrained model are associated with specific outputs. Finally, we describe potential methods whereby NPIs might be leveraged to interpret the inner workings of pretrained networks, as well as the related ethical implications of this work.",400
"  Emotion analysis of user-generated content  available on the web provides insights toward making meaningful decisions. Micro-blog platforms such as Twitter has gained profuse popularity for textual content holding people's opinions. The past decade has seen the active growth in emotion analysis models in many domains. Recently there has been an increasing interest in analysis of emotions of informal short texts such as tweets. In this paper, we introduce and analyze a system to accurately identify the emotions of the individual tweets with the associated intensities~\footnote{Intensity refers to the degree or amount of an emotion}.  % explain why it is important to analyze emotions  Analyzing emotions in social media such as twitter benefits society in a number of ways. Policymakers can use emotional information in social media to accurately identify concerns of people when making decisions. Monitoring social media for health issues benefits not only public health but also government decision makers. Furthermore, organizations can monitor opinion of the public on their products and services to provide better service to the society. Once emotions are recognized, emotion intensity can be used to prioritize the major concerns.  Studies in emotion analysis have often focused on emotion classification. However, emotions may exhibit varying levels of intensities. Here, emotion intensity can be defined as the degree or the intensity of particular emotion felt by the speaker. Additionally, we may observe multiple emotions simultaneously in the same tweet with varying intensities.   One purpose of this study is to develop a model to accurately identify the emotions and associated emotion intensities for a given tweet. In this paper, we propose a transfer learning approach backed by a neural network classifier and a regressor. Although the proposed neural network alone is inadequate to beat the benchmark, we show that features learned when training the above neural networks can be used to improve the overall performance when combined with other features.  Another purpose of this study is to explain how the input word level features affect the features extracted by the neural network.  % [complete the actual findings here] The findings should make an important contribution in understanding how features are used in a neural network and to effectively select features to improve the effectiveness of extracted features.   Our main contributions of this study:      \pagebreak  Major challenge in using deep learning to train emotion intensity prediction models is the lack of large labeled datasets. More recently, emoji and hashtags were used in studies to create large naturally labeled datasets. However, it is not possible to use a similar technique to obtain the intensity of emotions. Furthermore, creating a large dataset manually is time consuming and expensive.  are some existing datasets for emotional intensity prediction. Due to the limited amount of task-specific training data the previous researches have opted for transfer learning approaches~ and traditional machine learning. However, in this paper we argue that even with reasonable size dataset we can train a neural network to obtain good performance provided that there is proper regularization. Additionally, we show that features learned when training the neural network can be combined with other features to improve the overall performance of emotion intensity prediction.   % [explain methodology in brief]  []   \resizebox{0.4\textwidth}{!}{% {lllll}  & Train & Dev & Test & Total \\ anger & 1,701 & 388 & 1,002 & 3,091 \\ fear & 2,252 & 389 & 986 & 3,627 \\ joy & 1,616 & 290 & 1,105 & 3,011 \\ sadness & 1,533 & 397 & 975 & 2,905 \\ \hline % }   In \S, we outline related works on sentiment and emotion mining. Next, in \S we will discuss the datasets used in this study. After, we introduce the background and our methodology in \S and \S accordingly. Then, in \S we will discuss the evaluation results. Finally, we will conclude this paper in \S.     In this study, we propose a simple yet effective model for emotion classification and emotion intensity prediction in Tweets while suggesting a method to explain and visualize a trained DNN. We utilized a neural network with LSTM layer followed by a convolution layer with max-pooling for emotion category classification as well as emotion intensity prediction. We extend this work by transferring features from above models and two state-of-the-art models trained for different tasks to a XGBoost regressor to predict the emotion intensity in Tweets more accurately. Moreover, we suggest a technique to visualize and interpret the feature importance of trained DNNs for emotion intensity prediction. In the future, we plan on experimenting with using attentive mechanisms to improve the emotion intensity prediction further. Our models outperformed existing state-of-the-art models for emotion classification and in predicting fear and anger emotion intensities, while maintaining a competitive results in predicting other emotions.  
"," In this paper, we present an experiment on using deep learning and transfer learning techniques for emotion analysis in tweets and suggest a method to interpret our deep learning models. The proposed approach for emotion analysis combines a Long Short Term Memory  network with a Convolutional Neural Network . Then we extend this approach for emotion intensity prediction using transfer learning technique. Furthermore, we propose a technique to visualize the importance of each word in a tweet to get a better understanding of the model. Experimentally, we show in our analysis that the proposed models outperform the state-of-the-art in emotion classification while maintaining competitive results in predicting emotion intensity.",401
"  Online reviewing for businesses becomes more and more important nowadays, where customers can publish their reviews for businesses, and other potential customers or shop owners can view them. Positive feedback from customers may prosper the store businesses, while negative one could have opposite consequences. Yelp, one of the largest company founded in 2004 for publishing crowd-sourced reviews about businesses, provides one open dataset, Yelp Open Dataset , which has tremendously many data about businesses, reviews, and users. Such dataset has been proven to be a good material for personal, educational, and academic purposes.  Among multiple tasks on the Yelp Open Dataset, predicting ratings for restaurants based their reviews is one of fundamental and important tasks. This task can help Yelp classify reviews into proper groups for its recommendation system, detect anomaly reviews to protect businesses from malicious competitions, and assign rating to texts automatically.  Yelp review rating prediction can be done in multiple ways, such as sentiment analysis and 5-star rating classification. In this paper, we will focus on rating prediction for restaurants based only on their review texts. This task can be viewed as a multiclass classification problem, where the input is the textual data , and output is the predicted class . We will apply both machine learning and deep learning models. After analyzing data distribution, splitting datasets, and extracting features, we will use four machine learning methods, including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine  . Then we will focus on four transformer-based models, including BERT , DistilBERT , RoBERTa , and XLNet , where several different architectures will be tried with hyperparameter tuning. This project is done on {Google Colab}, where multi-processors and GPUs are available. The code is publicly available at GitHub .      In this paper, we predicted ratings from Yelp review texts. Yelp Open Dataset was used. The imbalanced data distribution was presented, and a balanced training dataset was built. Four machine learning models including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine were used based on numerical features from tf-idf vectorizer. Four transformer-based models including BERT, DistilBERT, RoBERTa, and XLNet were also trained and tested on the textual data. Comparisons between models and hyperparameters were done, and 64\  accuracy score for the machine learning model and 70\  accuracy score for the transformer-based one were achieved on the testing set.   Transformer-based models were summarized and experimented. Cased, large BERT models were found giving better performances than the uncased, base ones. DistilBERT has a faster computation speed with a bit lower metrics, while RoBERTa and XLNet give higher evaluation metrics with more computational resources required.  We hope our work could give some inspirations and insights for further work in Yelp review rating prediction based on machine learning and deep learning models.   ----------------------------------------------------------- {\small   } 
","    We predict restaurant ratings from Yelp reviews based on Yelp Open Dataset. Data distribution is presented, and one balanced training dataset is built. Two vectorizers are experimented for feature engineering. Four machine learning models including Naive Bayes, Logistic Regression, Random Forest, and Linear Support Vector Machine are implemented. Four transformer-based models containing BERT, DistilBERT, RoBERTa, and XLNet are also applied. Accuracy, weighted $ F_1 $ score, and confusion matrix are used for model evaluation. XLNet achieves 70\% accuracy for 5-star classification compared with Logistic Regression with 64\% accuracy.",402
"  Language processing requires tracking information over multiple timescales. To be able to predict the final word ``timescales"" in the previous sentence, one must consider both the short-range context  and the long-range context . How do humans and neural language models encode such multi-scale context information? Neuroscientists have developed methods to study how the human brain encodes information over multiple timescales during sequence processing. By parametrically varying the timescale of intact context, and measuring the resultant changes in the neural response, a series of studies  showed that higher-order regions are more sensitive to long-range context change than lower-order sensory regions. These studies indicate the existence of a ``hierarchy of processing timescales"" in the human brain. More recently,  used a time-resolved method to investigate how the brain builds a shared representation, when two groups of people processed the same narrative segment preceded by different contexts. By directly mapping the time required for individual brain regions to converge on a shared representation in response to shared input, we confirmed that higher-order regions take longer to build a shared representation. Altogether, these and other lines of investigation suggest that sequence processing in the brain is supported by a distributed and hierarchical structure: sensory regions have short processing timescales and are primarily influenced by the current input and its short-range context, while higher-order cortical regions have longer timescales and track longer-range dependencies .  How are processing timescales organized within recurrent neural networks  trained to perform natural language processing? Long short-term memory networks   have been widely investigated in terms of their ability to successfully solve sequential prediction tasks. However, long-range dependencies have usually been studied with respect to a particular linguistic function , and there has been less attention on the broader question of how sensitivity to prior context -- broadly construed --  is functionally organized within these RNNs. Therefore, drawing on prior work in the neuroscience literature, here we demonstrate a model-free approach to mapping processing timescale in RNNs. We focused on existing language models that were trained to predict upcoming tokens at the word level  and at the character level . The timescale organization of these two models both revealed that the higher layers of LSTM language models contained a small subset of units which exhibit long-range sequence dependencies; this subset includes previously reported units  as well as previously unreported units.  After mapping the timescales of individual units, we asked: does the processing timescales of each unit in the network relate to its functional role, as measured by its connectivity? The question is motivated by neuroscience studies which have shown that in the human brain, higher-degree nodes tend to exhibit slower dynamics and longer context dependence than lower-degree nodes . More generally, the primate brain exhibits a core periphery structure in which a relatively small number of ``higher order閳 and high-degree regions  maintain a large number of connections with one another, and exert a powerful influence over large-scale cortical dynamics . Inspired by the relationships between timescales and network structure in the brain, we set out to test corresponding hypotheses in RNNs:  Do units with longer-timescales tend to have higher degree in neural language models? and  Do neural language models also exhibit a ``core network"" composed of functionally influential high-degree units? Using an exploratory network-theoretic approach, we found that units with longer timescales tend to have more projections to other units. Furthermore, we identified a set of medium-to-long timescale ``controller"" units which exhibit distinct and strong projections to control the state of other units, and a set of long-timescale ``integrator units"" which showed influence on predicting words where the long context is relevant. In summary, these findings advance our understanding of the timescale distribution and functional organization of LSTM language models, and provide a method for identifying important units representing long-range contextual information in RNNs.    We demonstrated a new method for mapping the timescale organization in recurrent neural language models. Using this method, we mapped the timescale distributions of units within word-level and character-level LSTM language models, and identified a small set of units with long timescales. We then used network analyses to understand the relationship between the timescale of a unit and its connectivity profile, and we distinguished two subsets of long-timescale units with seemingly distinctive functions. Altogether, we proposed methods combining timescale and connectivity analyses for discovering timescale and functional organization in language models.  The units with longer processing timescales included some units whose role in long-range language dependencies had already been established , but almost all of the long timescale units are of unknown function. The timescale mapping procedure described here provides a model-free method for identifying nodes necessary for long-range linguistic and discursive processes . Future studies of these neural language models could focus on the specific linguistic information tracked by the long-timescale units, especially the ``controller'' units which control the information flow of other units in the network.   The current study measured unit timescales using a simple token distance, and so the method may be applied to understanding recurrent neural nets beyond language models. It will be insightful for future studies to investigate whether the processing timescales characterized via token distance are comparable to those measured using functional measures, such as syntactic distance. Relatedly, while we explored the timescale variance under several context conditions, a more thorough investigation will be needed to examine how the timescales of individual units may vary at different positions within a sentence, both in terms of token location and syntactic location.  Processing timescales may exhibit an analogous hierarchical organization in LSTMs and in the human cerebral cortex: in both cases, a subset of nodes with high degree and high inter-connectivity express unusually long timescales. More detailed testing of this apparent correspondence is required, however, because units within an LSTM layer are not spatially embedded and constrained as in biological brains, and thus the LSTM units do not express a spatially graded timescale topography.   C.J.H and H-Y.S.C gratefully acknowledge the support of the National Institutes of Mental Health    corpus used in the current study has a different linguistic structure from the Wikipedia corpus on which the WLSTM and CLSTM models were trained. Although we analyzed only the Anna Karenina sentences with low perplexity, it was important to test the robustness of our results across datasets. Thus, we mapped the timescale of each unit using the Wikipedia test set, as used by . Specifically, we sampled 500 long sentences containing ``, and"" for the Intact Context condition. As before, we generated sentences by preceding the ``shared input'' segment  with either the original prior context segment, or a randomly chosen prior context segment. Same as the original analysis, we then replaced the context segment with 30 context segments randomly sampled from other parts of the test set for generating the Random Context condition. The mapped timescales using the Wikipedia test set were highly correlated with the novel corpus, suggesting the robustness of unit timescales .      To examine how the timescales of individual units may vary across different positions in a sentence, we varied the location of the segmentation point. Instead of using the conjunction  as the segmentation point, we chose an arbitrary segmentation point: the 15th token of a long sentence, to separate context segment and shared input segment. In the Random Context condition, we replaced the context segment with the first 15 tokens from other sentences of the corpus. We found that the unit timescales were highly correlated with the condition where we used the conjunction as the segmentation point with several units shift their timescales to either directions .  This analysis was conducted using Wikipedia test set.      To examine if the timescales of individual units can flexibly reset at the beginning of a sentence, we conducted the same timescale analysis but using a ``full stop"" as the segmentation point instead of the conjunction ``, and"". Thus, if the original test string was ``The girl kicked the call, and the boy caught it"", then the full-stop version of the test string would be ``The girl kicked the ball. The boy caught it."" In this setting, the context segment and shared input segment in the Intact Context condition are two consecutive sentences. To ensure the temporal dependence between the context segment and shared input segment, we sampled 100 consecutive sentence pairs from the Anna Karenina corpus. Note that this is not possible using the Wikipedia test set from , because that set is composed of unrelated sentences. The Random Context condition was generated by replacing the first sentence with randomly sampled sentences from other parts of the novel. We found that when using ``full stop"" to segment context and shared input, most units in the network showed timescale near 0, indicating near-zero dependence on the linguistic context from the text preceding the full stop . This suggests that the units in LSTM tend to ``reset"" their context representation at the beginning of a sentence.      Inspired by the token-shuffling procedure of , we explored whether the context representations of individual units in the LSTM were shaped by individual words, rather than coherent sequences of words. For this analysis, instead of replacing the context with syntactically structured segments from other part of the corpus, we generated the ``random context"" by shuffling the order of words within the context segment. We then mapped the unit timescales as before, by examining the unit activation difference as a function of the distance from the onset of shared input. Intriguingly, we found that most of the units showed similar timescales across the context-replacement and context-shuffling procedures . This suggests that the context representations in LSTMs largely depend on the  of individual words in the context, rather than their appearance within coherent linguistic sequences. However, we did observe a subset of units  whose timescales were longer when context was replaced rather than shuffled. For this subset of units, the ability to maintain a representation of prior context over many tokens depends on that prior context being a coherent linguistic sequence. This subset of units are a promising target for future studies of syntactic representations in LSTMs.    First, for each hidden unit, we concatenated the corresponding rows in the  matrices, to generate a single ``hidden-to-gate"" projection vector for that hidden unit. Next we we z-scored the vector to get standardized projection values from that unit to all other units in the network. Using z-score 5 as criterion, we identified a total of 258 ``strong projections"" from all hidden units to the input gate and forget gate in the WLSTM. The projection strength of each unit was then calculated based on its number of ""strong projections"" . Although the criterion z-score was selected to better visualize the results in Figure , different criteria did not change the results that units with longer timescales have more strong projections. For example, using z-score 3 as threshold we obtained corr = 0.30, p0.001; z-score 4 we obtained corr = 0.35, p0.001.  Next, we identified the edges corresponding to the top 258 magnitude weight-values within the combined  matrices. Together, these edges formed a  ""strong-projection network"". Finally, we used k-core analysis to identify the main core of the strong-projection network. This main core composed our ""controller units"" .   Using the same criteria and method, we identified a total of 390 ``strong projections"" from all hidden units to the input gate and forget gate in the CLSTM. We then extracted the top 390 weight values from the weight matrices to construct a ``strong-projection network"" and again identified the main core network, composed the ``controller units"" for the CLSTM model      To examine the non-trivial roles of the controller and integrator units identified in the word-level LSTM model, we performed a preliminary group ablation analysis to look at how ablating the controller units influences model performance on predicting the next token, relative to the ablation of a random set of units. Specifically, since long-timescale integrator units should have most effect predicting tokens at the later part of the sentences , we examined the model performance on predicting tokens at two different positions:  all the tokens regardless of their positions in the sentences , and  the last tokens of sentences .   We evaluated the effects of ablation on model performance by measuring the differences of probabilities  assigned to the target words . Ablation effects for controller units  and integrator units  were compared against a baseline of ablating the same number of randomly-selected units from layer 2 of the LSTM . We used the test corpus used by  and measured the average performance of each model across 100 text-batches, randomly sampled from the Wikipedia test dataset. Each text-batch was composed of 1000 tokens that start at the beginning of a sentence.  In the ``All tokens"" condition, we calculated the P for every token in the tested text, while in the ``Final tokens"" condition, we calculated P only at the last token of every sentence . We then average the P in both conditions across text-batches to get a mean performance difference between the ablated model and the intact model.   Ablating controller units reduced the probabilities assigned to the target words, more so than ablating random units . In contrast, ablating integrator units reduced the probabilities less than ablating random units . We hypothesized that that the integrator units mostly influence the model's prediction performance for tokens where long-range information is especially relevant, such as in the later portions of clauses and sentences. Consistent with this, we found that, when we examined the ablation effects only for tokens in the final position of a sentence, ablating integrator units reduced the probabilities more than ablating random units . Interestingly, ablating controller units reduced the probability of sentence-final targets less than random units .  In summary, these ablation results indicate a non-trivial functional role for the controller and integrator units, despite the fact that each subset of units is composed of only 10 amongst 650 total hidden units. Also, the putative controller and integrator sets appear to have distinctive roles within the WLSTM, with the controllers supporting accurate predictions overall, while the integrator units appear to boost accurate predictions at the end of sentences.      To explore whether the timescale mapping methods, and our findings, may generalize to other model architectures, we trained and studied a word-level GRU language model .  As far as possible, we applied similar parameters in the GRU as were used for the LSTM by : the same Wikipedia training corpus, the same loss function , and the same hyperparameters except for a learning rate initialized to 0.1, which we found more optimal to train the GRU. The GRU model also had two layers, with 650 hidden units in each layer.  We trained the GRU model for 30 epochs, at which point the GRU converged to a validation perplexity of 118.36. Note that since we adapted similar training settings as were used for training the LSTM model by Gulordava et al. without model-specific optimization, the perplexity is higher than that of the LSTM model reported in  . We then analyzed the timescale of its hidden units using the same method as was used for analyzing the LSTMs, and using the test data derived from the training Wikipedia corpus.   Similar to the LSTM model of Gulordova et al, the majority of the units in the GRU also showed shorter timescales. More specifically, we found:  the second layer of the GRU model was more sensitive to prior context than the first layer, as in the LSTM ;  the distribution of timescales across units was similar in the GRU and LSTM, although the GRU showed a more right-skewed distribution with a larger proportion of short-timescale units .    We also performed the timescale vs. network connectivity analyses on the GRU model. Because the update of hidden states in GRU are controlled by the reset and update gate, we measured the projection patterns of hidden units by analyzing the matrix of combined hidden-to-update-gate and hidden-to-reset-gate weights. In contrast to the LSTM models, hidden units in the GRU that we trained did not show a relationship between longer timescales and stronger hidden-to-gate projections . Moreover, when using k-core analysis to identify subunits of interconnected high-degree units, the core network in the GRU contained many units with long to short timescales. Interestingly, when we visualized the position of the k-core units in the MDS space, they tended to locate at the edge of the space, similar to what we found in LSTM. This indicates that, as in the LSTM, the core units in the GRU have distinctive profiles, distant from one another and from other units in the network . However, we did not observe the pattern of ``integrator units"" in the GRU as in the LSTM.  These apparent similarities and differences between LSTM and GRU are intriguing, but we emphasize that  the perplexity of this GRU model is much higher than the LSTM, due to the sub-optimal parameter settings, and that  comparing the LSTM and GRU connection patterns is not straightforward, as the overall distribution of weights is different. Further work will be required to determine comparable thresholds for 閳ユ笩trong閳 projections and 閳ユ笁igh-degree units閳 in each case. As we noted in the manuscript and above, the connectivity results are exploratory; however, we believe that the GRU analysis demonstrates how these methods can be extended to map and compare the functional organization of language models of different architecture.   Finally, we note that when conducting the timescale analysis on an incompletely trained GRU model  the second layer of LSTM-100 showed more context sensitivity than the first layer, and  although it was difficult to quantitatively compare the unit-level timescale distribution between the LSTM-100 model and the LSTM with 650 units, they both contain a similarly small subset of long-timescale units. .  We did not observe a significant correlation between the unit timescale and number of strong projections generated by each unit in the LSTM-100 model: the long-timescale units in the LSTM-100 did not have more connections than short-timescale units. When visualizing the MDS space of connectivity similarity of LSTM-100, the ``controller units"" identified using the k-core analysis were located in the edge of the space, similar to the 650-unit LSTM model. Interestingly, we observed a subset of long-timescale units in the center of the MDS space, analogous to the ``integrator units"" found in the 650-unit LSTM model. Altogether, the pattern of ``integrator units"" might be a commonly evolved feature that is shared between LSTM model architectures, but not with GRU architectures.    \renewcommand{\thefigure}{A.\arabic{figure}} {0}       \                    
"," In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the ``processing timescales闁 of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network  with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: ``controller闁 units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while ``integrator闁 units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models.\footnote{The code and dataset to reproduce the experiment can be found at \url{https://github.com/sherrychien/LSTM_timescales}}",403
"    We summarize our contribution as follows:       In this paper, we propose a novel method named SenSeNet for keyphrase generation, which automatically  estimate whether the sentences are tended to generate the keyphrase. We use a straight-through estimator to solve the model discontinuity problem. We incorporate a weakly-supervised signal to guide the selection of significant sentences efficiently. The experiment results show that our model can successfully generate the present keyphrase and absent keyphrase. In addition, our model and the training method can be applied to most encoder-decoder architectures. Further analysis suggests that our model has an edge on semi-present keyphrase, although predicting absent keyphrase is challenging.                             
"," Keyphrase Generation  is the task of generating central topics from a given document or literary work, which captures the crucial information necessary to understand the content. Documents such as scientific literature contain rich meta-sentence information, which represents the logical-semantic structure of the documents.  However, previous approaches ignore the constraints of document logical structure, and hence they mistakenly generate keyphrases from unimportant sentences. To address this problem, we propose a new method called Sentence Selective Network  to incorporate the meta-sentence inductive bias into KG. In SenSeNet, we use a straight-through estimator for end-to-end training and incorporate weak supervision in the training of the sentence selection module. Experimental results show that SenSeNet can consistently improve the performance of major KG models based on seq2seq framework, which demonstrate the effectiveness of capturing structural information and distinguishing the significance of sentences in KG task.",404
"   % With the recent development of end-to-end text-to-speech  system, the synthesised speech has achieved high intelligibility and quality in various languages . Recently neural network based text-to-speech  systems have achieved certain success in prosody and naturalness of synthesized speech over conventional methods .  % Because Chinese is non-alphabet and its character set is very large, grapheme-to-phoneme  is essential when hiring end-to-end model in Chinese . By applying encoder-decoder framework with attention , these systems can directly predict speech parameters from graphemes or phonemes by learning acoustic and prosodic patterns via a flexible mapping from linguistic to acoustic space.  % But they still can only model part of prosody structural information from raw text  because of their limited model capacity, resulting poor expressiveness even prosody errors. % V_1021But they still can only model part of prosody structural information from raw text  resulting in poor expressiveness even prosody errors. However, the learnt prosodic patterns only contain part of prosodic structural information , resulting in poor prosody and naturalness performance even improper prosody.  % So additional prosody structure information is important to improve the naturalness of synthesized speech for text-to-speech system. % V5: So adding prosody information, such as prosody structure annotations, in encoder-decoder based models is important to improve the expressiveness of synthesized speech in TTS systems. % The G2P module converts the text input into a sequence of phonemes with tones, after which the intelligibility and naturally of synthesised Chinese speech can perform better than the conventional TTS . % However, the limited coverage of phoneme permutation in training data causes the decline of ability to predict prosody, resulting in unnatural prosody and unexpected pause.   %   % V4: There are many attempts to improve the prosody prediction ability of TTS system by introducing prosody structure information explicitly. % V5: Prosody structure annotations have been successfully applied in TTS systems to improve expressiveness. % V1020: To improve expressiveness of synthesized speech, directly adding prosodic structure annotations, such as Tones and break indices  labels  and The MATE meta-scheme % v_1021:To improve expressiveness of synthesized speech, adding prosodic structure annotations such as tones and break indices  labels  or other prosodic structure annotation  to input sequence of encoder-decoder based models has been proposed. To further improve prosody and naturalness of synthesized speech, adding prosodic structure annotations such as tones and break indices  labels  or other prosodic structure labels  to the input sequence of neural network based TTS models has been proposed. Prosodic structure annotations need to be subjectively labeled from speech, which is time-consuming.  Although these annotations can be automatically annotated by training another prosodic structure prediction model , the accuracy of predicted prosodic structure labels is still limited by using subjectively labeled annotations as the ground-truths. The high correlation between syntactic structure and prosodic information has been proved by successful syntactic-to-prosodic mapping . % V1020: The syntactic parsing models trained with a large text database with rich grammatical structure  provide text in TTS dataset with usefully syntactic structure information. A set of rule-based syntactic features such as part-of-speech  and positions of the current word in parent phrases are proposed and used in hidden Markov model  based acoustic model . % So subjective labeled prosodic structure annotations can be replaced with syntactic structure information, which obtained from text without referring to speech. % In hidden markov model  based acoustic model, a set of rules to create syntactic features including part of speech  and positions of the current word in parent phrases are hired as syntactic structure information to improve prosody and naturalness exceeds prosodic structure annotations in comprehensiveness and granularity . % This provides us with another method to implicitly improve prosody using syntactic structure information, which exceeds using prosody structure information explicitly in comprehensiveness and granularity.  % Early in the hidden markov based TTS model, rich syntactic context instead of prosody structure information is used to improved prosody of synthesized . % The word relation based features  proposed by  are prior features, which require expert knowledge to be designed. % to explore syntactic information from parse tree, to improve the generalization of synthesised speech. To utilize more syntactic structure information, phrase structure based feature  and word relation based feature  are proposed in neural network based TTS . PSF and WRF expand the set of syntactic features used in HMM model. More features such as highest-level phrase beginning with current word  and lowest common ancestor  are further introduced to model syntactic structure .  However, the expanded features are still manually designed features rather than automatically learned high-level representations. PSF only contains features from limited layers of the whole syntactic tree structure. WRF only exposes the information of partial nodes and edges from the whole syntactic parse tree.  % PSF and WRF can only model the syntactic relation among limited subtrees rather than the whole syntactic parse tree structure. % contain feature from syntactic tree structure by design. % needs  and expert knowledge to select  % V1020: which makes it harder to extract useful information and leads to instability. % and the way to select the specific layers from parse tree, which makes it harder to extract useful information and leads to instability. % V1020: And WRF focuses on the relation between two adjacent words in parsing tree structure, which can only model limited information from the whole syntactic parse tree. For example, one of WRF features is highest-level phrase beginning with current word . % WRF only models . %  which expand partial higher structure . % limited by manual selection strategy, WRF only considers the influence of former word on next word and specific layer of parent nodes, so cannot model the whole structure parse tree.  % This makes the prosody performance largely determined by the selected strategy, and at the same time very unstable. %In Fig., we show a example of how synthesised speech from phoneme sequence input  is different from reference speech  because of failing to respect syntax structure.  % Without parsing tree's limit, the third word ""cu4 jin4"" is pronounced separately .  % Besides, without parsing tree information, synthesised speech does not pause between the fifth word ""ti2 xiao4"" and the sixth word ""shi4"", which have a obvious gap in parsing tree reflected in reference speech . % simply plugging these parsing tree information during TTS does not perform well. Limited by the manual design rule, these features have some disadvantages to model syntax tree structure information. Firstly, using phrase structure feature needs to fix the number of tree layers and the way to select specific layer, while using word relation feature has to make the model select only part of parse tree structure, which cannot be proved to be the most useful part for prosody modeling. This makes the prosody performance largely determined by the selected strategy, and at the same time very unstable. Secondly, word relation feature only consider the former words' influence on next word and ignore the impact of the backward structure importance. Last but not least, manual design features require very high accuracy of syntax tree annotation, which can not be easily achieved. Otherwise, Otherwise under the influence of manual selection strategy, the destructive influence of mislabeling on prosody prediction will be magnified.  % A syntactic parse tree traversal based method is proposed to learn syntactic representation and employed in neural machine translation . To maker better use of the syntactic information, motivated by the syntactic parse tree traversal approach in neural machine translation , we propose a syntactic representation learning method to further improve the prosody and naturalness of synthesized speech in neural network based TTS.  % To make a better use of the syntactic information, in this paper, we propose a syntactic representation learning method to further improve the prosody in neural network based TTS. % which also known as phrase structure parsing, for TTS system to control prosody more effective.  Syntactic parse tree is linearized into two constituent label sequences through left-first and right-first traversal. % Word level bidirectional  Then syntactic representations are extracted from the constituent label sequences using different uni-directional GRU network for each sequence. After which, the syntactic representations are up-sampled from word level to phoneme level and concatenated with phoneme embeddings.  Tacotron 2 is employed to generate spectrogram from the concatenated syntactic representations and phoneme embeddings, with Griffin-Lim  to reconstruct the waveform. % directly Nuclear-norm maximization loss  is introduced to the constituent label embedding layer to enhance discriminability and diversity.  Compared to only hiring left-first traversal , right-first traversal is proposed to alleviate the ambiguity.  Experimental results show that our proposed model outperforms the baseline in terms of prosody and naturalness. Mean opinion score  increases from  to  compared with the baseline approach . % compared to baseline approach, with  is  from a one-way ANOVA test. ABX preference rate exceeds the baseline approach by . % One-way ANOVA test reveals a significant improvement . % We go further to explore how the enhanced controllability of prosody can benefit eliminate ambiguity. For sentences with multiple different syntactic parse trees, prosodic differences can be clearly perceived from corresponding synthesized speeches.  %We linearize a phrase parse tree into a structural label sequence and propose a rnn-based model to learn useful syntactic information by itself, and experimental shows significantly better than the method of manually extracting features. %To our best known, we first exploite syntactic information to chinese TTS system and first to apply syntactic information to lower input level than word. %We have also introduce rank loss of syntactic label embedding to enhance the ability of the syntax structure to control prosody, which expanded the specific application of parsing tree information, including different sentences in the same parsing tree structure to bring the same prosodic structure, and different trees in the same sentence to produce different prosodic readings. The latter brings solutions to the ambiguity caused by grammatical structure      In this study, we investigate a syntactic representation learning method to automatically utilize the syntactic structure information for neural network based TTS. Nuclear-norm maximization loss is introduced to enhance the discriminability and diversity of synthsized speech prosody. Experimental results demonstrate the effectiveness of our proposed approach. For sentences with multiple syntactic parse trees, prosodic difference can be clearly observed from the synthesized speeches.    To start a new column  and help balance the last-page   column length use \vfill\pagebreak.   -------------------------------------------------------------------------  \vfill  \pagebreak   \vfill\pagebreak    References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   ------------------------------------------------------------------------- 
"," Syntactic structure of a sentence text is correlated with the prosodic structure of the speech that is crucial for improving the prosody and naturalness of a text-to-speech  system.  Nowadays TTS systems usually try to incorporate syntactic structure information with manually designed features based on expert knowledge.  In this paper, we propose a syntactic representation learning method based on syntactic parse tree traversal to automatically utilize the syntactic structure information.  Two constituent label sequences are linearized through left-first and right-first traversals from constituent parse tree. Syntactic representations are then extracted at word level from each constituent label sequence by a corresponding uni-directional gated recurrent unit  network.  Meanwhile, nuclear-norm maximization loss is introduced to enhance the discriminability and diversity of the embeddings of constituent labels.  Upsampled syntactic representations and phoneme embeddings are concatenated to serve as the encoder input of Tacotron2.  Experimental results demonstrate the effectiveness of our proposed approach, with mean opinion score  increasing from $3.70$ to $3.82$ and ABX preference exceeding by $17\%$ compared with the baseline. In addition, for sentences with multiple syntactic parse trees, prosodic differences can be clearly perceived from the synthesized speeches.",405
" Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Many semantic parsing methods are based on the principle of semantic compositionality ~, of which the main idea is to put together the meanings of utterances by combining the meanings of the parts~. However, these methods suffer from heavy dependence on handcrafted grammars, lexicons, and features.  To overcome this problem, many neural semantic parsers have been proposed and achieved promising results~. %\textcolor{red}{However, compared to compositional semantic parsers, neural semantic parsers are not aware of the compositional structure of utterances, which often limits their generalization between various compound-complex utterances: However, due to the lack of capturing compositional structures in utterances, neural semantic parsers usually have poor generalization ability to handle unseen compositions of semantics~. For example, a parser trained on ``'' and ``'' may not perform well on ``''.    & \textcolor{red}{How many} \textcolor{green}{rivers run through} \textcolor{blue}{the states bordering colorado}\\ M & \\ {Q1}} & \textcolor{blue}{the states bordering colorado}\\ M1 & \\ {Q2}} & \textcolor{green}{rivers run through} \textcolor{blue}{state\}\\ M3 & \\  \\ {*}{Geo} & \\ &  \\ {*}{\tabincell{c}{Complex\\WebQuestions}} & \\ & \\ & \quad\quad\\ & \quad\quad\\ & \quad\quad}\\ {*}{\tabincell{c}{Formulas}} & \\ & \\ { % then, for each train sample , we use this preliminary base parser to check whether spans in  can be parsed to \textcolor{red}{be} a part of  \textcolor{red}{or not}. If true, we leverage these spans as pseudo supervision signals for training the utterance segmentation model, and thereby do not require any handcraft templates or additional labeled data.} %The key to implement this framework is to address the challenge of lacking labeled data for utterance segmentation. %We achieve this through cooperative training of the segmentation model and the base parser: %leverage pre-trained base parser to derive synthetic supervision signals for training the segmentation model, then leverage the segmentation model to derive synthetic supervision signals for updating the base parser.  % \textcolor{green}{Moreover, considering that there are usually no labeled data for utterance segmentation, we propose to search for reasonable segmentation points from utterances via the base parser, and use them as a distant supervision. This improves the domain adaptability of our framework.}  % While lacking the direct supervision for segmentation model, we seek to address this challenge in a distantly supervised way. % shaped like  %\textcolor{red}{ %Firstly, we train the base parser, and use it to search for and evaluate all viable ways to segment training utterances. %Then, these segmentations are leveraged as distant supervision for training the utterance segmentation model and fine-tuning the base neural semantic parser.}  In summary, our proposed framework has four advantages:  the base parser learns to parse simpler spans instead of whole complex utterances, thus alleviating the training difficulties and improving the compositional generalization ability;  our framework is flexible to incorporate various popular encoder-decoder models as the base parser;  our framework does not require any handcraft templates or additional labeled data for utterance segmentation; % our framework addresses the challenge of lacking labeled data for utterance segmentation through cooperative training.  our framework improves the interpretability of neural semantic parsing by providing explicit alignment between spans and partial meaning representations.  We conduct experiments on three datasets: Geo~, ComplexWebQuestions~, and Formulas . They use different forms of meaning representations: FunQL, SPARQL, and Spreadsheet Formula. Experimental results show that our framework consistently improves the performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gain: Geo , Formulas , ComplexWebQuestions .  [htbp]      from it and parses the span to {a partial meaning representation}}. We obtain span meanings via neural semantic parsing, then piece them to assemble whole-utterance meaning.}     In this paper, we propose a novel framework for boosting neural semantic parsers via iterative utterance segmentation. The insight is a bottom-up divide-and-conquer mechanism, which significantly improves the compositional generalization ability and interpretability of neural semantic parsers. Considering the usual absence of labeled data for utterance segmentation, we propose a cooperative training method to tackle this problem. Experimental results show that our framework consistently improves the performance of different neural semantic parsers across tasks.  In the future, we plan to improve the robustness of our framework for various complex language phenomena. We also plan to apply this framework to more semantic parsing tasks such as text-to-SQL and text-to-code.     
"," Neural semantic parsers usually fail to parse long and complex utterances into correct meaning representations, due to the lack of exploiting the principle of compositionality. To address this issue, we present a novel framework for boosting neural semantic parsers via iterative utterance segmentation. Given an input utterance, our framework iterates between two neural modules: a segmenter for segmenting a span from the utterance, and a parser for mapping the span into a partial meaning representation. Then, these intermediate parsing results are composed into the final meaning representation. One key advantage is that this framework does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the parser provides pseudo supervision for the segmenter. Experiments on Geo, ComplexWebQuestions and Formulas show that our framework can consistently improve performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gains: Geo $63.1\to 81.2$, Formulas $59.7\to 72.7$, ComplexWebQuestions $27.1\to 56.3$.",406
"     Word alignment is a task of finding the corresponding words in a sentence pair  and used to be a key component of statistical machine translation . Although word alignment is no longer explicitly modeled in neural machine translation , it is often leveraged to interpret and analyze NMT models . Word alignment is also used in many other scenarios, such as imposing lexical constraints on the decoding process , improving automatic post-editing  and providing guidance for translators in computer-aided translation .  Recently, unsupervised neural alignment methods have been studied and outperformed GIZA++  on many alignment datasets . However, these methods are trained with a translation objective, which computes the probability of each target token conditioned on source tokens and previous target tokens. This will bring noisy alignments when the prediction is ambiguous . To alleviate this problem, previous studies modify Transformer  by adding alignment modules to re-predict the target token , or computing an additional alignment loss on the full target sequence . Moreover,  propose an extraction method that induces alignment when the to-be-aligned target token is the decoder input.   Although these methods have demonstrated their effectiveness, they have two drawbacks. First, they retain the translation objective which is not tailored for word alignment. Consider the example in Figure . When predicting target token ``Tokyo'', the translation model may wrongly generate ``1968'' as it only considers the previous context, which will result in an incorrect alignment link . A better modeling is needed for obtaining more accurate alignments. Second, they need an additional guided alignment loss  to outperform GIZA++, which requires inducing alignments for entire training corpus.  In this paper, we propose a self-supervised model specifically designed for the word alignment task, namely Mask-Align. Our model masks each target token and recovers it with the source and the rest of the target tokens. For example, as shown in Figure , the target token ``Tokyo'' is masked and re-predicted. During this process, our model can identify that only the source token ``Tokio'' has not been translated yet, so the to-be-predicted target token ``Tokyo'' is aligned to ``Tokio''. Comparing with the translation model, this masked modeling method is highly related to word alignment, and based on that our model generates more accurate predictions and alignments.  % We model the target token conditioned on all other tokens in both source and target, which will disambiguate the prediction and thus lead to an accurate alignment ). As the vanilla transformer architecture requires sequential time to model this probability, we modify the attention in the decoder by separating the queries from keys and values and  % updating only the former in each layer. This allows our model to predict all target tokens in a single forward pass without information leakage. Besides, we also propose a variant of attention called leaky attention that allieviates the unexpected high attention weights on some specific tokens such as periods, which is helpful for the alignment extraction from attention matrix. Finally, we leverage the attention weights from the models in two directions by incorporating an agreement loss in the training process.  % Experiments on four public datasets show that our model significantly outperforms all existing statistical and neural methods without using guided alignment loss.  To summarize, the main contributions of our work are listed as follows:            In this paper, we propose a self-supervised neural alignment model Mask-Align. Different from the NMT-based methods, our model adopts a novel masked modeling objective that is more suitable for word alignment tasks. Moreover, Mask-Align can alleviate the problem of high attention weights on special tokens by introducing leaky attention. Experiments show that Mask-Align achieves new state-of-the-art results without guided alignment loss. We leave it for future work to extend our model in a semi-supervised setting.     
"," Neural word alignment methods have received increasing attention recently. These methods usually extract word alignment from a machine translation model. However, there is a gap between translation and alignment tasks, since the target future context is available in the latter. In this paper, we propose Mask-Align, a self-supervised model specifically designed for the word alignment task. Our model parallelly masks and predicts each target token, and extracts high quality alignments without any supervised loss. In addition, we introduce leaky attention to alleviate the problem of unexpected high attention weights on special tokens. Experiments on four language pairs show that our model significantly outperforms all existing unsupervised neural baselines and obtains new state-of-the-art results.  % However, the original translation objective ignores the future context in the target, which is available in the alignment task.",407
"   %缁楊兛绔村▓纰夌窗headline瀵板牓鍣哥憰 With the rapid growth of information spreading throughout the Internet, readers get drown in the sea of documents, and will only pay attention to those articles with attractive headlines that can catch their eyes at first sight. On one hand, generating headlines that can trigger high click-rate is especially important for different avenues and forms of media to compete for user's limited attention. On the other hand, only with the help of a good headline, can the outstanding article be discovered by readers.                     %缁楊兛绗佸▓纰夌窗閹存垳婊戦惃鍕侀崹瀣簼绠為幀搴濈疄閸 To generate better headlines, we first analyze what makes the headlines attractive. By surveying hundreds of headlines of popular websites, we found that one important feature that influences the attractiveness of a headline is its content. For example, when reporting the same event, the headline ``Happy but not knowing danger: Children in India play on the poisonous foam beach'' wins over 1000 page views, while the headline ``Chennai beach was covered with white foam for four days in India'' only has 387 readers. The popular headline highlights the fact that ``the beach is poisonous and affects children'',  which will concern more people than ``white foam''. On the other hand, the style of the headline also has a huge impact on attractiveness. For example, the headline ``Only two people scored thousand in the history of NBA Finals'' attracts fewer people than the headline ``How hard is it to get 1000 points in the NBA finals? Only two people in history!'', due to its conversational style that makes readers feel the need to see the answer to this question.      %缁楊兛绨╁▓纰夌窗challenge:婵″倷缍嶉惌銉╀壕attractive Most of the recent researches regard the headline generation task merely as a typical summarization task . This is not sufficient because a good headline should not only capture the most relevant content of an article but also be attractive to the reader. However, attractive headline generation tasks were paid less attention by researchers.  tackle this task by adversarial training, using an attractiveness score module to guide the summarization process.  introduce a parameter sharing scheme to disentangle the attractive style from the attractive text. However, previous works neglect the fact that attractiveness is not just about style, but also about content. %     the negative samples generated by the pre-trained model can be non-fluent, inconsistent, and incoherent, which makes it difficult for the scorer to learn the attractiveness standard given such huge noise.           Based on the above analysis, we propose a model named  , which learns to write attractive headlines from both style and content perspectives.  These two attractiveness attributes are learned from an attractive prototype headline, \ie the headline of the document in the training dataset that is most similar to the input document. First, DAHG separates the attractive style and content of the prototype headline into latent spaces, with two auxiliary constraints to ensure the two spaces are indeed disentangled. Second, the learned attractive content space is utilized to iteratively polish the input document, emphasizing the parts in the document that are attractive. Finally, the decoder generates an attractive headline from the polished input document representation under the guidance of the separated attractive style space. Extensive experiments on the public Kuaibao dataset show that DAHG outperforms the summarization and headline generation baselines in terms of ROUGE metrics, BLEU metrics, and human evaluations by a large margin. Specifically, DAHG triggers 22\% more clicks than the strongest baseline.           %缁楊剙娲撳▓纰夌窗閹崵绮╟ontribution The major contributions of this paper are as follows:   We devise a disentanglement mechanism to divide the attractive content and style space from the attractive prototype headline.  We propose to generate an attractive headline with the help of disentangled content space under the style guidance.  Experimental results demonstrate that our model outperforms other baselines in terms of both automatic and human evaluations.          In this paper, we propose a Disentanglement-based Attractive Headline Generator  to generate an attractive headline.    Our model is built on the fact that the attractiveness of the headline comes from both style and content aspects.   Given the prototype document-headline pair, DAHG disentangles the attractive content and style space from the prototype attractive headline.    The headline generator generates attractive headlines under the guidance of both.    Our model achieves state-of-the-art results in terms of ROUGE scores and human evaluations by a large margin.   In near future, we aim to bring the model online.                 \clearpage  
"," Eye-catching headlines function as the first device to trigger more clicks, bringing reciprocal effect between producers and viewers. Producers can obtain more traffic and profits, and readers can have access to outstanding articles. When generating attractive headlines, it is important to not only capture the attractive content but also follow an eye-catching written style.  In this paper, we propose a Disentanglement-based Attractive Headline Generator  that generates headline which captures the attractive content following the attractive style. Concretely, we first devise a disentanglement module to divide the style and content of an attractive prototype headline into latent spaces, with two auxiliary constraints to ensure the two spaces are indeed disentangled. The latent content information is then used to further polish the document representation and help capture the salient part. %The latent attractive content space further helps to distill salient and attractive knowledge from the input document. Finally, the generator takes the polished document as input to generate headline under the guidance of the attractive style.  Extensive experiments on the public Kuaibao dataset show that DAHG achieves state-of-the-art performance.  Human evaluation also demonstrates that DAHG triggers 22\% more clicks than existing models.",408
"  Task-specific finetuning of pretrained deep networks has become the dominant paradigm in contemporary NLP, achieving state-of-the-art results across a suite of natural language understanding tasks . While straightforward and empirically effective, this approach is difficult to scale to multi-task, memory-constrained settings , as it requires shipping and storing a full set of model parameters for each task. Inasmuch as these models are learning generalizable, task-agnostic language representations through self-supervised pretraining, finetuning the entire model for each task seems especially profligate.   A popular approach to parameter-efficiency with pretrained models is to learn sparse models for each task where a subset of the final model parameters  are exactly zero~. Such approaches often face a steep sparsity/performance tradeoff, and a substantial portion of nonzero parameters  are still typically required to match the performance of the dense counterparts. An alternative is to use multi-task learning or feature-based transfer for more parameter-efficient transfer learning with pretrained models~. These methods learn only a small number of additional parameters  on top of a shared model. However, multi-task learning generally requires access to all tasks during training to prevent catastrophic forgetting~, while feature-based transfer learning  is typically outperformed by full finetuning~.    Adapters~ have recently emerged as a promising approach to parameter-efficient transfer learning within the pretrain-finetune paradigm~.  Adapter layers are smaller, task-specific modules that are inserted between layers of a pretrained model, which remains fixed and is shared across tasks.  These approaches do not require access to all tasks during training, making them attractive in settings where one hopes to obtain and share performant models as new tasks arrive in stream.   find that adapter layers trained on BERT can match the performance of fully finetuned BERT on the GLUE benchmark  while only requiring 3.6\% additional parameters  per task.   In this work, we consider a similar setting as adapters but propose a new  approach with the goal of even more parameter-efficient transfer learning.  Diff pruning views finetuning as learning a task-specific \underline{diff}erence  vector%\footnote{Similar to the  command in Unix operating systems.}    \ that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks.   In order to learn this vector, we reparameterize the task-specific model parameters as , where the pretrained parameter vector  is finetuned. The diff vector is regularized with a differentiable approximation to the -norm penalty~ to encourage sparsity. This approach can become  parameter-efficient as the number of tasks increases as it only requires storing the nonzero positions and weights of the diff vector for each task. The cost of storing the shared pretrained model remains constant and is amortized across multiple tasks.  On the GLUE benchmark~, diff pruning can match the performance of the fully finetuned BERT baselines  while finetuning only  of the pretrained parameters per task, making it a potential alternative to adapters for parameter-efficient transfer learning.      We propose diff pruning as a simple approach for parameter-efficient transfer learning with pretrained models. Experiments on standard NLP benchmarks and models show that diff pruning can match the performance of fully finetuned baselines while requiring only a few additional parameters per task. We also propose a structured variant of diff pruning which provides further improvements. Avenues for future work include  applying this approach to other architectures ,  injecting parameter-efficiency objectives directly into the pretraining process , and  combining diff pruning with other techniques  to achieve even greater parameter-efficiency.  
"," While task-specific finetuning of pretrained networks has led to significant empirical advances in NLP, the large size of networks makes finetuning difficult to deploy in multi-task, memory-constrained settings. We propose  as a simple approach to enable parameter-efficient transfer learning within the pretrain-finetune framework. This approach views finetuning as learning a task-specific ``diff"" vector that is applied on top of the pretrained parameter vector, which remains fixed and is shared across different tasks. The diff vector is adaptively pruned during training with a differentiable approximation to the $L_0$-norm penalty to encourage sparsity. Diff pruning becomes parameter-efficient as the number of tasks increases, as it requires storing only the nonzero positions and weights of the diff vector for each task, while the cost of storing the shared pretrained model remains constant. It further does not require access to all tasks during training, which makes it attractive in settings where tasks arrive in stream or the set of tasks is unknown. We find that models finetuned with diff pruning can match the performance of fully finetuned baselines on the GLUE benchmark while only modifying 0.5$\%$ of the pretrained model's parameters per task. Our code is available at \url{https://github.com/dguo98/DiffPruning}}",409
"   Goal-oriented dialogue systems is a hot topic in machine learning research. The systems have widespread applications in the industry and are the foundation of many successful products, including Alexa, Siri, Google Assistant, and Cortana. One core component of a dialog system is spoken language understanding , which consists of two main problems, intent classification  and slot labeling  . In IC, we attempt to classify the goal of a user query, usually input in text or transcribed by automatic speech recognition  system from audio. SL, similar to the named-entity recognition  problem, aims to label each token in a query an entity type. The only difference is that entity types in SL are domain-specific and based upon dialog ontology. Recent advances in neural models have enabled greatly improved SLU .  However, two significant challenges hinder the broad application and expansion of the SLU models in industrial settings. First of all, neural methods require a large amount of labeled data for training . SLU is often coupled with the ontology of the underlying dialog system and thus domain-dependent. Collecting a large number of in-domain labeled data for neural models is prohibitively expensive and time-consuming. Secondly, the performance of SLU models in practice often suffers from fluctuations due to various types of noises. One common noise is adaptation data perturbation. In many industrial applications such as cloud services\footnote{Alexa ASK: https://developer.amazon.com/en-US/alexa/alexa-skills-kit; Google DialogFlow: https://dialogflow.com/}, the SLU model is built by fine-tuning  a pre-trained, shared network to the target domain with data provided by developers. The developers often have a limited background in SLU and machine learning. Thus the data provided varies in quality and is subject to different types of perturbations, such as missing or replaced data samples  and typos. Another common noise comes from the mismatch of input modalities between adaptation and inference stages. For instance, the model is adapted with human transcription yet deployed to understand ASR decoded text, or the input at adaptation and inference stages relies on the recognition of different versions of ASR models. Given that most neural methods comprise a large number of parameters and are heavily optimized for the training  data provided, the resulting model is usually sensitive to these noises. The requirement of noise-free adaptation and inference conditions also prohibits the use of neural SLU techniques because it is often infeasible to achieve such conditions.  Transfer learning and meta-learning are two conventional techniques that have been applied to address the challenge of data scarcity. Transfer learning usually refers to pre-training initial models using mismatched domains with rich human annotations and then adapting the models with limited labels in targeted domains. Previous works  have shown promising results in applying transfer learning to SLU. Note that pre-training discussed here covers methods including using a pre-trained language model like BERT  directly and further training downstream tasks on data in mismatched domains with the pre-trained model. In the following, we focus on the latter due to utilizing data from other domains better and yielding higher accuracy. In recent years, meta-learning has gained growing interest among the machine learning fields for tackling few-shot learning  scenarios. Model-Agnostic Meta-Learning   focuses on learning parameter initialization from multiple subtasks, such that the initialization can be fine-tuned with few labels and yield good performance in targeted tasks. Metric-based meta-learning, including prototypical networks   and matching networks , aim to learn embedding or metric space which can be generalized to domains unseen in the training set after adaptation with a small number of examples from the unseen domains. Recent work unveils excellent potential in applying meta-learning techniques to SLU in the few-shot learning context .  As compared to data scarcity, another challenge for SLU, the robustness against noises, is also gaining attention. Simulated ASR errors are used to augment training data for SLU models . Researchers also leverage information from confusion networks or lattices , and adversarial training techniques  for models to learn query embeddings that are robust against ASR errors. For text input, methods have also been explored on model robustness against noises from misspelling and acronym . In contrast to these noise types that have gained attention, to our best knowledge, there is no prior work investigating the impact of missing or replaced examples in adaptation data. Moreover, the intersection of data scarcity and noise robustness is unexplored. Since the scarcity of labeled data and data noisiness usually co-occur in SLU applications , the lack of studies in the intersectional areas hinders the use of neural SLU models and its expansion to broader use cases.  Given the deficiency, we establish a novel few-shot noisy SLU task by introducing two common types of natural noise, adaptation example missing/replacing and modality mismatch, to the previously defined few-shot IC/SL splits . The task is built upon three public datasets, ATIS , SNIPS , and TOP . We further propose a noise-robust few-shot SLU model based on ProtoNets for the established task. In summary, our primary contributions are 3-fold: 1) formulating the first few-shot noisy SLU task and evaluation framework, 2) proposing the first working solution for the few-shot noisy SLU with the existing ProtoNet algorithm, and 3) in the context of noisy and scarce learning examples, comparing the performance of the proposed method with conventional techniques, including MAML and fine-tuning based adaptation.      In this paper, we establish a novel SLU task, the few-shot noisy SLU, with existing public datasets. We further propose a ProtoNets based approach, Proto, to build IC and SL classifiers with few noisy examples. When there is no noise in few-shot examples, Proto yields better performance than other approaches utilizing MAML and fine-tuning frameworks. Proto also achieves the highest and most robust IC accuracy and SL F1 when two types of noise, adaptation example missing/replacing and modality mismatch, are injected in adaption and evaluation set respectively. We believe the ensemble nature of ProtoNets benefits the model robustness, and the simplicity of Proto's model architecture is also helpful in the few-shot noisy scenario. Our contribution here is a step toward the efficient and robust deployment of SLU models. While our results are promising, there is still substantial work, from the creation of few-shot SLU datasets covering more noises to studies of faster and stabler learning algorithms, in pursuit of the goal.          {\Alph{section}} 
","    Recently deep learning has dominated many machine learning areas, including spoken language understanding . However, deep learning models are notorious for being data-hungry, and the heavily optimized models are usually sensitive to the quality of the training examples provided and the consistency between training and inference conditions. To improve the performance of SLU models on tasks with noisy and low training resources, we propose a new SLU benchmarking task: few-shot robust SLU, where SLU comprises two core problems, intent classification  and slot labeling . We establish the task by defining few-shot splits on three public IC/SL datasets, ATIS, SNIPS, and TOP, and adding two types of natural noises  to the splits. We further propose a novel noise-robust few-shot SLU model based on prototypical networks. We show the model consistently outperforms the conventional fine-tuning baseline and another popular meta-learning method, Model-Agnostic Meta-Learning , in terms of achieving better IC accuracy and SL F1, and yielding smaller performance variation when noises are present.",410
"   Recurrent neural networks are the basis of the state-of-the-art models in natural language processing, including language modeling , machine translation  and named entity recognition . It is needless to say that complex learning tasks require relatively large networks with millions of parameters to be accomplished. However, large neural networks need more data and/or strong regularization techniques to be trained successfully and avoid overfitting. Without the means to collect more data, which is the case in the majority of real-world problems, data augmentation and regularization methods are standard alternative practices to overcome this barrier.  Data augmentation in natural language processing is limited, and often task-specific . On the other hand, adopting the same regularization methods that are originally proposed for feed-forward  networks needs to be done with extra care to avoid hurting the network's information flow between consecutive time-steps. To overcome such limitations, we present Sequence Mixup: a set of training methods, regularization techniques, and data augmentation procedures for RNNs. Sequence Mixup can be considered as the RNN-generalization of input mixup  and manifold mixup , which are already introduced for feed-forward neural networks. Generally speaking, the core idea behind mixup strategies is to mix training samples in the network's input or hidden layers, where by mix, we simply mean to consider random convex combinations of pairs of samples as alternatives for the actual training data points. Mixup in non-recurrent networks has led to smoother decision boundaries, more robustness to adversarial examples, and better generalization compared to many rival regularization methods . Here, we extend input mixup to RNNs and also propose two variants of manifold mixup, namely Pre-Output Mixup  and Through-Time Mixup , where mixing occurs in the hidden space of the RNN. POM and TTM differ from each other in the way information flow is passed from one time-step to the next.  In order to elucidate the effect of sequence mixup during the learning stage, consider the classification of half-moons data plotted in figure  with a simple two-timestep RNN. We have also added some levels of noise to the original data points to make the classification task more challenging. Figures  and  show the learned decision boundaries from noisy data via regular training and Pre-Output Mixup, respectively. As can be seen, mixup expands the margin between the classes and increases the decision boundary levels, which in turn renders a smoother decision boundary with less certainty about nearby cross-class samples. Intuitively speaking, this type of training creates artificial samples whose labels and hidden states are obtained from intermixing those of the original samples, in a respective manner. Based on our experiments, applying sequence mixup has improved both the test F-1 score and loss of BiLSTM-CRF model  on CoNLL-03 data  .   [t] 	 	 	 	 Original half-moons data and its learned decision boundary. Noise-corrupted half-moons and decision boundaries learned with  regular training and  Pre-Output Mixup .} 	   We have also provided a theoretical analysis on the impact of our regularization techniques in the asymptotic regime where network widths become increasingly large, and learning rates become infinitesimally small. In a nutshell, our analysis reveals that as long as the number of hidden state neurons, which we denote by  in this work, is less than the number of distinct classes in a classification problem, both POM and TTM cannot achieve a zero training error regardless of how large the training dataset is or how deep the neural networks become. Moreover, we show that as long as  is less than twice the number of classes, the hidden-state generating section of the RNN acts as a memoryless unit and produces hidden states that are almost independent of previous time-steps. On the other hand, given that  is chosen sufficiently large, both POM and TTM are able to divide the hidden representation space of the RNN into a set of orthogonal affine subspaces, where each subspace is an indicator of a unique class. We refer to this property as spectral compression of sequence mixup, which is a similar behaviour to that of manifold mixup for feed-forward networks.  The rest of the paper is organized as follows: Section  reviews a number of related works to this problem. In Section , we propose Sequence Mixup, describe its challenges and specifications in detail, and also present our theoretical analysis.  Section  is devoted to our experiments on real-world data. Finally, Section  concludes the paper. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%      We introduce Sequence Mixup, a set of regularization and data augmentation techniques for RNNs. Our work can thought as extending both input mixup  and manifold mixup , which are originally porposed for feed-forward neural nets. For the case of manifold mixup, we propose two distinct variants called Pre-Output and Throgh-Time Mixup, respectively. An asymptotic theoretical analysis reveals that Pre-Output Mixup imposes  a locally linear behavior on the network's output generating section. In a classification task, this property leads to partitioning of the hidden representation space into a set of orthogonal affine subspaces, each of which corresponds to a unique class. Experimental results showed improvement on the loss and F-1 scores of both 1) a baseline and 2) state-of-the-art model on CoNLL-2003 NER task. We have studied the correlation of mixup coefficients through consecutive time-steps, and found out that using identical coefficients achieves better loss and F-1 on the NER task. However, at the same time, we conjecture that optimal correlation values for mixup coefficients across time may vary from task to task and thus requires experimental exploration to be adjusted. Lastly, the considerable reduction in the test loss achieved by sequence mixup methods  implies that employing sequence mixup methods for language models may lead to a substantial improvement on the test perplexity.       
"," In this paper, we extend a class of celebrated regularization techniques originally proposed for feed-forward neural networks, namely Input Mixup  and Manifold Mixup , to the realm of Recurrent Neural Networks . Our proposed methods are easy to implement and have a low computational complexity, while leverage the performance of simple neural architectures in a variety of tasks. We have validated our claims through several experiments on real-world datasets, and also provide an asymptotic theoretical analysis to further investigate the properties and potential impacts of our proposed techniques. Applying sequence mixup to BiLSTM-CRF model  to Named Entity Recognition task on CoNLL-2003 data  has improved the F-1 score on the test stage and reduced the loss, considerably. @ce.sharif.edu,~motahari@sharif.edu} {https://github.com/ArminKaramzade/SequenceMixup.}}",411
" Sentiment classification is the task of analyzing a piece of text to predict the orientation of the attitude towards an event or opinion. The sentiment of a text can be either positive or negative. Sometimes, a neutral perspective is also considered for classification. SA has many different applications, such as reducing the early age suicide rate by identifying cyberbullying , discouraging unwarranted activities towards a particular community through hate-speech detection , and monitoring public response towards a proposed government bill  among many others.    The task of SA has achieved superior improvement in other languages, i.e. English - about 97.1\% accuracy for 2-class  and 91.4\% accuracy for 3-class SA . But only a few research works have been published for the SA in Bengali. This is because we lack quality datasets in Bengali for training a computation model for the sentiment classification. However, in the last few years, we have seen the rise of Internet users in the Bengali domain mostly due to the development of wireless network infrastructure throughout South East Asia. This resulted in a massive increase in the total number of online social network users as well as newspaper readers. So it became comparatively easier to collect the public comments posted online on the Bengali news websites.         BERT_{BSA}, that performs better compared to other existing models that are trained with word2vec or fastText embedding. We discuss the model and in Section .     $ and compare it to other models trained with Word2Vec and fastText embeddigns using the 2-class and 3-class Bengali SA datasets. We discuss the results in the Section .     %   % \makeatletter % \patchcmd{\@makecaption} %   { %   {} %   {} % \makeatletter % \patchcmd{\@makecaption} %   {\\} %   {.\ } %   {} %   {} % \makeatother % \def\tablename{Table}       In this paper, with solid experimental proof behind unseen co-relational depth between the task, embedding or feature extractor and end-to-end models, we achieved state-of-the-art on 2-class and 3-class sentimental tasks in a Bengali language which is yet to bloom on this very domain. Primarily, by showing limitations and drawbacks of the few available pre-processing tools, we have claimed that operating on different level embedding should be the first step to reap immediate success. Thereby through extensive analysis and relative findings, we have shown that sub-word level functional embedding, BERT, with any RNN architecture is a must in Bengali sentiment classification tasks. Moreover, we took a step closer to a real world by letting our model identify public sentiment on some newspaper topics which has never been done before on this language. However, from data-set expansion to making BERT suitable for Bengali linguistics, there is a huge room of improvement which our research team has already started working on. Furthermore, by fixing these issues, many ground-breaking applications like cyberbullying identification, hate-speech detection can be introduced to not only help make Bengali a potential language for any NLP practitioner but also to ease the life of many native Bengali speakers.      In this paper, we presented two manually tagged novel datasets for SA in Bengali. We also introduced BERT	extsubscript{BSA}, a deep learning model for SA in Bengali, which outperforms all other models. We achieved state-of-the-art performance for both the 2-class and 3-class SA tasks in Bengali. Moreover, we took a step closer to apply SA model to a real world application by analyzing public sentiment on newspaper topics. The result shows that for religious news comments people tend to possess a positive sentiment whereas for political and sports news comments, people possess negative sentiment. However, this research is a work in progress and will be regularly updated with new insights. We are continuing to increase the size of SA datasets in Bengali and we will explore the application of other deep learning models for better results. We hope that the improved performance of SA in multi-class classification tasks presented in this paper will help many ground-breaking applications like cyberbullying identification as well as hate-speech detection in Bengali.  
"," Sentiment analysis  in Bengali is challenging due to this Indo-Aryan language's highly inflected properties with more than 160 different inflected forms for verbs and 36 different forms for noun and 24 different forms for pronouns. The lack of standard labeled datasets in the Bengali domain makes the task of SA even harder. In this paper, we present manually tagged 2-class and 3-class SA datasets in Bengali. We also demonstrate that the multi-lingual BERT model with relevant extensions can be trained via the approach of transfer learning over those novel datasets to improve the state-of-the-art performance in sentiment classification tasks. This deep learning model achieves an accuracy of 71\% for 2-class sentiment classification compared to the current state-of-the-art accuracy of 68\%. We also present the very first Bengali SA classifier for the 3-class manually tagged dataset, and our proposed model achieves an accuracy of 60\%. We further use this model to analyze the sentiment of public comments in the online daily newspaper. Our analysis shows that people post negative comments for political or sports news more often, while the religious article comments represent positive sentiment. The dataset and code is publicly available \footnote{ https://github.com/KhondokerIslam/Bengali\_Sentiment}.",412
"   Natural language has provided a key cohesive ingredient for pushing the boundaries of technological advances beyond individuals to the 4th industrial revolution. In textual form, it provides a long term, stable, knowledge base, which can be used to preserve knowledge across generations. Digital evolution in the last century has greatly accelerated this preservation process and provided a means to extract hidden meaning and information from texts, largely considered illegible and indecipherable for human beings. Natural Language Processing  is a divergent field, with state-of-the-art research initiative looking towards resolving the various challenges of automatic information extraction. Foremost, amongst these challenges is the ability to identify the various concepts and their relationship, which form the epitome of the target corpus . For humans and machines, cause-effect represents an essential relation, which provides ample support for the reasoning and decision making process . Automatic causality detection has benefited greatly from numerous dedicated research efforts . However, challenges such as the dynamicity of syntax and semantics, and in particular the evolution of vocabulary have hindered the development and usage of any generic and cross-domain solution . On the other hand, applications such as information retrieval , question answering , and event reasoning and predictions  have gained valuable improvements through the identification of cause-effect relationships. \\ The commonly used approaches for causality detection, fall into two categories: pattern-based traditional rule bases, and machine learning based automatic classification and entity extraction . Pattern based approaches are based on partial or complete expert intervention for crafting and verifying the conditions based on the syntactic and semantic analysis of the corpus. This approach, requires intensive human effort and lacks cross-domain generalization. Even after utilizing a substantial amount of human time, the extracted rules cannot cover all possible linguistic patterns and are usually not usable beyond the original domain/corpus. Such an approach also suffers from the diversity in linguistic typology, leading to rules formed for a language based on the Subject-Verb-Object sentence structure  not being compatible with those based on other structures such as Subject-Object-Verb and others .\\ Automatic machine learning based approaches utilize labeled datasets for extracting causality relationships from unseen data and thereby requires less expert intervention, relatively. With this approach, most human time is spent on labeling the data and verifying the results, while providing a reusable model for cross-domain applications. However, any evolution of labels and change in text can render the model unusable. Additionally, machine learning models, are typically independent of the linguistic topology features and can be customized to work on any sentence structure albeit with some effort towards creating and optimizing language vectors, and incorporating natural heuristics derived from syntactically labelled  or a well distributed large corpus  .\\ A solution to managing change in the machine learning models and reducing the expert intervention is available as Transfer Learning, where the machine can learn a new tasks by reusing a foundational model, originally employed for a different but related task in another domain . Such a cross-domain application may not replicate the original performance benchmarks, and thereby requires some model tuning and tweaking before becoming useful. Model tuning is achieved with the help of a human expert who provides feedback to the machine learning model for improving its learning tasks, a technique more commonly known as active learning . To gain benefits of these two approaches active transfer learning is applied to various tasks in diverse domains , transferring similar models and improving its performance in a single workflow. This performance is mainly improved by enhancing the pre-trained model with few annotated dataset and expert involvement from the new domain.\\ Causality mining as an application of causality detection is typically based on two tasks, which includes identification of causal triggers, and causal pairs participating in each relationship . Also known as causal connectives; causal triggers are transitive verbs which form a bridge between causality concepts and identify the cause and its effect. Leveraging the sentence structuring in English language, typical causality relation identification methodologies, found in research literature, follow the Noun Phrase  - Verb  - NP pattern which corresponds to either Cause - Trigger - Effect or Effect - Trigger - Cause forms   . Based on this heuristic, Kaplan and Berry-Bogge  provided an early model for creating and using handcrafted linguistic template for causality detection. Kalpana Raja et al. , built upon the same idea in addition to identifying and organizing a dictionary based on causal trigger keywords, which was then used to define patterns for causality detection. R. Girju et al.  refined the process of identifying the causal verbs by utilizing the WordNet dictionary. Cole et al.  utilized a syntactic parser to convert the SVO structures into SVO triples, which were then passed through various rule based filters for causality detection. S. Zhao et al. , pointed towards the existence of diversity in the manner each causal trigger expresses causality. However, the syntactic structure of causal sentences and the way the trigger invokes the causality, can provide satisfactory categorization of the causal triggers, enabling smart application of the causality identification filters. Son Doan et al.  presented an application of causal mining by marking several verbs and nouns as causal triggers for extracting causal relations from twitter messages. Girju and Moldovan  proposed a semi-supervised approach towards causality relation identification by using the underlying linguistic patterns of the corpus.\\ Many other automatic causal pattern identification methodologies have relied on the evolution of machine learning models. In particular,  has presented a causal relation extraction model using unsupervised learning to detect the noun phrases corresponding to the subject and object of the sentence. By analysing an unannotated raw corpus and using Expected Maximization along with a Naive Bayes classifier, the authors were able to precisely identify 81.29\% of causal relations. \\ On the other hand, E. Blanco et al.  utilized a supervised learning approach by first annotating ternary instances as being a causal relation or not, and then applied Bagging with C4.5 decision trees to achieve a precision of over 95\% in causal relations ad above 86\% in non causal ones. These and many other machine learning approaches have been comprehensively classified by , which indicates a general trend towards the utilizing of the same, as the models become more mature and stable. Of particular interest are the word embedding methods, which due to their requirement of unsupervised data, scalability, and accuracy have piqued the interest of the NLP research community. \\ Several initiatives have already led to the state-of-the-art results in completing NLP tasks such as sentiment analysis, text classification, topic modeling, and relation extraction . Zeng et al.  classified relations in the SemEval Task 8 dataset using deep convolution neural networks . Nguyen et al.  introduced positional embedding to the input sentence vector in CNNs for improved relation extraction. Silva et al.  proposed a deep learning  based causality extraction methodology that can detect causality along with its direction. The author addressed the causality detection problem as a three class classification problem, where class 1 indicates the annotated pairs has causal relation with direction entity1  entity2, class 2 implies the causal relation has the direction entity2  entity1, and class 3 entities are non-causal.\\ Ning An et al.  has utilized a word embedding with cosine similarity based approach, which uses an initial causal seed list to identify the causal relationships as a multi-class  classification problem. With one-hot encoding the authors, convert the causal verbs in the seed list and the verbs identified in Noun Phrase-Verb Phrase-Noun Phrase ternary into encoding vectors. These vectors are then converted into Embedding vectors using Continuous Skip-Gram based on a Wikipedia dataset of 3.7 million articles. Finally the encoded vectors are then compared using cosine similarity and the pair with maximum similarity above a pre-defined threshold value of 0.5 are used to classify the causal relationship and evolve the seed list. This method achieved an average F-score of 78.67\%. While this methodology presents a significant improvement on previous research initiatives towards causal relationship identification, it suffers from low accuracy, due to its focus on causal verb identification based on a small initial seed list and its limited extension, and classification based, solely on these verbs meanwhile losing context of the causal phrase. \\ In this paper we present a novel causal relationship identification framework, which outperform, in the domain of causality mining in clinical text. This framework uses a multi-dimensional approach, which resolves syntactic and semantic matching problems in clinical textual data, providing causal knowledge which is useful to summarize clinical text for quick review, create patient personas for reapplication of medical procedures and predictive analysis, discovering medical knowledge from volumnous data sources, and provide evidence supporting clinical decision making.\\ This novel framework identifies causal phrases using automatic seed list generation from training data set, seed expansion using transfer learning, causal phrase generation, and BERT based phrase embedding and semantic matching. It then applies semantic enrichment on the causal phrases using Unified Medical Language System , to extend  healthcare terms with their semantic and uniquely identifiable corresponding codes. Finally, the trained model is evolved based on expert feedback, by employing active learning.\\  In the presented approach, we extracted the initial causal seed list from SemEval Task 8 dataset and expanded it by utilizing synonyms from WordNet dictionary, pre-trained Google News model , ConceptNet Numberbatch Model , and Facebook Fasttext Model . We then generated causal quads using dependency based linguistic patterns for identifying the subject, object, causal verb, and a confidence measure. Causal triples, under a threshold, were then filtered from the quads and converted into embedding vectors using BERT to create an initial model. This trained model was used to identify candidate causal triples in unseen textual data, which were then semantically enriched from UMLS and converted into a causal quad by augmenting a confidence score. The semantically enriched causal quads were then verified by an expert by increasing or decreasing the confidence value and used to evolve the trained model, iteratively.\\ This detailed methodology is presented in section , with details workflows in section  and its results following in section . Finally, section  will conclude the paper.           Active transfer learning using amalgamation of results from multiple models is a novel and, as proved above, successful methodology for identifying causal sentences. This two class problem, whereby we aimed to correctly identify the causal sentences, shows very high and maintainable recall rates. While the performance of this methodology, in terms of accuracy and precision can be improved by incorporating additional active learning iterations, the results are still significant enough to be used for practically solving any two class textual mining problem. In future, we shall look towards the application of our methodology for solving real world problems, such as generation of patient summaries from clinical text.        Copyright 2007, 2008, 2009 Elsevier Ltd       This file is part of the 'Elsarticle Bundle'.    ---------------------------------------------       It may be distributed under the conditions of the LaTeX Project Public    License, either version 1.2 of this license or  any    later version.  The latest version of this license is in       http://www.latex-project.org/lppl.txt    and version 1.2 or later is part of all distributions of LaTeX    version 1999/12/01 or later.       The list of all files belonging to the 'Elsarticle Bundle' is    given in the file `manifest.txt'.        Template article for Elsevier's document class `elsarticle'    with numbered style bibliographic references    SP 2008/03/01                     \documentclass[preprint,12pt,3p]{elsarticle}     Use the option review to obtain double line spacing  \documentclass[preprint,review,12pt]{elsarticle}     Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn    for a journal layout:    \documentclass[final,1p,times]{elsarticle}    \documentclass[final,1p,times,twocolumn]{elsarticle}    \documentclass[final,3p,times]{elsarticle}    \documentclass[final,3p,times,twocolumn]{elsarticle}    \documentclass[final,5p,times]{elsarticle}    \documentclass[final,5p,times,twocolumn]{elsarticle}  \usepackage{float}     if you use PostScript figures in your article    use the graphics package for simple commands    \usepackage{graphics}    or use the graphicx package for more complicated commands \usepackage{amsmath,amssymb,amsfonts} \usepackage{algorithmic} \usepackage{algorithm2e} \usepackage{textcomp} \usepackage{float} \usepackage{longtable} \usepackage{xcolor}    or use the epsfig package if you prefer to use the old commands    \usepackage{epsfig}     The amssymb package provides various useful mathematical symbols \usepackage{amssymb}    The amsthm package provides extended theorem environments    \usepackage{amsthm}     The lineno packages adds line numbers. Start line numbering with    . Or switch it on    for the whole article with \linenumbers after .    \usepackage{lineno}     natbib.sty is loaded by default. However, natbib options can be    provided with  command. Following options are    valid:       round  -  round parentheses are used       square -  square brackets are used   [option]      curly  -  curly braces are used      {option}      angle  -  angle brackets are used    <option>      semicolon  -  multiple citations separated by semi-colon      colon  - same as semicolon, an earlier confusion      comma  -  separated by comma      numbers-  selects numerical citations      super  -  numerical citations as superscripts      sort   -  sorts multiple citations according to order in ref. list      sort&compress   -  like sort, but also compresses numerical citations      compress - compresses without sorting              \journal{Journal of Biomedical Informatics}              Start line numbering here if you want      \linenumbers     main text    
"," Objective: Causality mining is an active research area, which requires the application of state-of-the-art natural language processing techniques. In the healthcare domain, medical experts create clinical text to overcome the limitation of well-defined and schema driven information systems. The objective of this research work is to create a framework, which can convert clinical text into causal knowledge. \\ Methods: A practical approach based on term expansion, phrase generation, BERT based phrase embedding and semantic matching, semantic enrichment, expert verification, and model evolution has been used to construct a comprehensive causality mining framework. This active transfer learning based framework along with its supplementary services, is able to extract and enrich, causal relationships and their corresponding entities from  clinical text.\\ Results: The multi-model transfer learning technique when applied over multiple iterations, gains performance improvements in terms of its accuracy and recall while keeping the precision constant. We also present a comparative analysis of the presented techniques with their common alternatives, which demonstrate the correctness of our approach and its ability to capture most causal relationships.\\ Conclusion: The presented framework has provided cutting-edge results in the healthcare domain. However, the framework can be tweaked to provide causality detection in other domains, as well. \\ Significance: The presented framework is generic enough to be utilized in any domain, healthcare services can gain massive benefits due to the voluminous and various nature of its data. This causal knowledge extraction framework can be used to summarize clinical text, create personas, discover medical knowledge, and provide evidence to clinical decision making.",413
" Content based websites such as Quora, Reddit, StackOverflow are primarily used for seeking genuine answers to questions. People from different domains put up their questions and educators or people knowledgeable in a certain field answer them. One major impediment to a plain sailing execution of information exchange is the proliferation of toxic comments. The key challenge is to weed out such toxic comments termed as Insincere Questions. An Insincere Question is designated as a comment intended to make a statement than to look for genuine answers.  An Insincere Question is characterised by:        This major class of problem pertains to Text classification which has been a benchmark problem of evaluating various research advancements in natural language processing. While traditional machine learning algorithms such as naive bayes, logistic regression and decision trees can be rightfully applied to this problem, they suffer with major impediments in their constructs. Vanilla RNNs, Gated Recurrent Unit and Long Short Term Memory Networks replaced their usage as the new state of the art. Even though LSTMs and GRUs performed well, they failed to capture the dependencies in long range sentences. Now with the advent of Transfer Learning, Language model pre-training has proven to be useful in learning universal language representations. Researchers in the field are developing new and better language models at an unprecedented speed. Applying these new state of the art models could improve current methods and replace manual labeling tasks for text classification, but also find widespread application in similar other fields, such as machine translation and question answering. In this paper, we test this by applying new transformer models from the BERT-family to improve the current method of binary text classification in the context of Insincere Questions Classification. We make use of the Quora Insincere Questions Classification dataset  for this purpose We find that all of our models achieve remarkable results in classifying the given  data , with BERT achieving the best results compared to RoBERTa, DistilBERT, and ALBERT. This indicates that the models are well equipped to take over tasks that researchers have previously solved in less optimal ways.       In this paper, we aimed to identify Insincere Questions from text state of the art NLP models. Starting off with simple methods to the most cutting edge, we illustrated how NLP models can compete with others. In order to do so, we explored how BERT and three BERT-based transformer models approach text classification. RoBERTa, DistilBERT, and ALBERT each improve the original model in a different way with regards to performance and speed. In our application, we demonstrated the easiest way to implement transformer models, how to modify the standard settings and what else to pay attention to. On the task of identifying insincere user intent BERT performed best. However, the field of NLP is fast moving - and we are excited to see what the next transformational generation of models will bring.   
","  The internet today has become an unrivalled source of information where people converse on content based websites such as Quora, Reddit, StackOverflow and Twitter asking doubts and sharing knowledge with the world. A major arising problem with such websites is the proliferation of toxic comments or instances of insincerity wherein the users instead of maintaining a sincere motive indulge in spreading toxic and divisive content. The straightforward course of action in confronting this situation is detecting such content beforehand and preventing it from subsisting online. In recent times Transfer Learning in Natural Language Processing has seen an unprecedented growth. Today with the existence of transformers and various state of the art innovations, a tremendous growth has been made in various NLP domains. The introduction of BERT has caused quite a stir in the NLP community. As mentioned, when published, BERT dominated performance benchmarks and thereby inspired many other authors to experiment with it and publish similar models. This led to the development of a whole BERT-family, each member being specialized on a different task. In this paper we solve the Insincere Questions Classification problem by fine tuning four cutting age models viz BERT, RoBERTa, DistilBERT and ALBERT",414
"     The term  measures how much energy the reader will have to expend in order to understand a writing at optimal speed and find interesting. Readability measuring formulas, such as Automated Readability Index  , Flesch Reading Ease , and Dale閳ユ弲hall formula  calculate a score that estimates the grade level or years of education of a reader based on the U.S. education system, which is illustrated in Figure . These formulas are still used in many widely known commercial readability measuring tools such as  and . This measurement plays a significant role in many places, such as education, health care, and government . Government organizations use it to ensure that the official texts meet a minimum readability requirement. For instance, the Department of Insurance at Texas has a requirement that all insurance policy documents have a Flesch Reading Ease  score of 40 or higher, which translates to the reading level of a first-year undergraduate student based on the U.S. education system. A legal document which is hard to read can lead someone to sign a contract without understanding what they are agreeing to. Another common usage area is the healthcare sector to ensure the proper readability of the care and treatment documents . Better readability will attract visitors or readers of different websites or blogs, whereas poor readability may decrease the number of readers . Readability measures are also often used to assess the financial documents such as annual reports of a company閳ユ獨 economic performance so that the information is more transparent to the reader .  is a disorder that causes difficulties with skills associated with learning, namely reading and writing, which affects up to 20\% of the general population. Readability formulas have been applied to measure the difficulty of reading texts for people with dyslexia .   The scores from readability formulas have been generally found to correlate highly with the actual readability of a text written in the English language. The adaptation of readability formulas to no-English texts is not straightforward. Measuring readability is also essential for every non-English language, but not all of the readability formulas mentioned above are language-independent. These formulas require some resources like a 3000-word list, which is easily understandable by fourth-grade American students, syllable counting dictionary, stemmer, lemmatizer etc. Resource availability for Natural Language Processing  research is an obstacle for some low-resource-languages . In this paper, we aim to develop a readability analysis tool for the Bengali Language. Bengali is the native language of Bangladesh, also used in India  and has approximately 230 million native speakers. Despite being the  most spoken language in the world, Bengali suffers from a lack of fundamental resources for NLP. For a low resource language like Bengali, the research in this area so far can be considered to be narrow and sometimes incorrect.  tried to adapt the formula-based approaches used for the English language. Unfortunately, it isn't straightforward as these formulas are developed for U.S. based education system and which predicts U.S. grade level of the reader. Since the Bangladeshi education system and grade levels are different from U.S., therefore, the mapping is faulty and led to incorrect results. There is a strong relationship between reading skills and human cognition, which varies depending on different age groups . Therefore, to eliminate this incompatibility, in this paper, we map grade level to different age groups to present age-to-age comparison. Moreover,  used traditional machine learning models to address this task on a very small scale dataset, which isn't publicly available. There are readability analysis tools available for  ,  ,  , and   language. Unfortunately, no such tool is available for Bengali language that can validate the readability of a text. On the other hand, there is no large-scale human annotated readability analysis dataset available to train supervised neural models for this extremely low-resource language. Our main contributions are summarized as follows:           In this paper, we present a readability analysis tool that would be useful for educators, content writers or editors, researchers, and readers of different ages. We adopt document-level readability formulas traditionally used for U.S. based education system to the Bengali education system with a proper age-to-age comparison. Moreover, we divide the task into sentence-level and design supervised neural models, which will serve as a baseline for the future works of this task. We present several human-annotated corpora, dictionaries, and an algorithm, which can be useful for several other tasks of this low-resource language. In the future, we wish to improve the quality of our system by increasing the size of our sentence-level dataset and will present a user-study based on our tool. Also, we will focus on the readability analysis of Bengali-English code-mixed texts.    
","  Determining the readability of a text is the first step to its simplification. In this paper, we present a readability analysis tool capable of analyzing text written in the Bengali language to provide in-depth information on its readability and complexity. Despite being the $7^{th}$ most spoken language in the world with  million native speakers, Bengali suffers from a lack of fundamental resources for natural language processing. Readability related research of the Bengali language so far can be considered to be narrow and sometimes faulty due to the lack of resources.  Therefore, we correctly adopt document-level readability formulas traditionally used for U.S. based education system to the Bengali language with a proper age-to-age comparison. Due to the unavailability of large-scale human-annotated corpora, we further divide the document-level task into sentence-level and experiment with neural architectures, which will serve as a baseline for the future works of Bengali readability prediction. During the process, we present several human-annotated corpora and dictionaries such as a document-level dataset comprising  documents with 12 different grade levels,  a large-scale sentence-level dataset comprising more than  sentences with simple and complex labels, a consonant conjunct count algorithm and a corpus of  words to validate the effectiveness of the algorithm, a list of  easy words, and an updated pronunciation dictionary with more than  words. These resources can be useful for several other tasks of this low-resource language. \footnote{We make our Code \& Dataset publicly available at \url{https://github.com/tafseer-nayeem/BengaliReadability} for reproduciblity.}",415
" Figurative language, or a figure of speech , is phrasing that goes beyond the literal meaning of words to get a message or point across. Writers and poets use figurative language to build imagery and elicit aesthetic experiences. %A handful of figurative types help make foreign concepts familiar and graspable, including but not limited to simile , metaphor , irony, etc. %.  In computational linguistics, figurative language processing  has long been an interesting research topic, including both detection  and generation tasks . [htbp] {!}{ {|c|c|l|}   \multicolumn{3}{|c|}{Writing Polishment with Simile} \\ {|c|}{Before} & \multicolumn{2}{l|}{[c]{@{}l@{}}Looking at his bloodthirsty eyes, everyone felt\\ horrible and couldn't help but step back.} \\ {|c|}{After} & \multicolumn{2}{l|}{[c]{@{}l@{}}Looking at his bloodthirsty eyes, everyone felt\\ horrible \underline{as if they were being stared at by a }\\ \underline{serpent}, and couldn't help but step back.} \\   \multicolumn{3}{|c|}{Other Figurative Language Generation} \\   Task & Status & \multicolumn{1}{c|}{Text} \\ {*}{Metaphor} & After & She devoured his novels. \\ l@{}}Non-\\ ironic & [c]{@{}l@{}}Tried to leave town and my phone died, \\ what a failure. \\   \multirow{-2}{*}{[c]{@{}c@{}}Irony} & Ironic & [c]{@{}l@{}}\underline{Nice} to leave town and my phone died, \\ \underline{definition of success.} \\ , none of existing work has ever investigated simile generation given a plain text, which is indispensable for amplifying writing with similes. % very few works have explored simile generation in the field of FLP polishing text with simile interpolation. % interpolation for text polishment.  Although sequence-to-sequence models work well for story generation , irony generation , or metaphor and personification generation , it is non-trivial for these models to generate proper and creative simile for a given text. In particular, writing polishment with similes is a unique task because it requires to together address the challenges listed below:%that together make writing polishment with similes a unique task.    %Apparently, one of the biggest challenge for most text polishing studies is data insufficiency. Either it's the lack of labelled data for continuous figurative language generation, such as metaphor  and personification , or the lack of parallel data for style transfer on text attributes such as sentiment, formality , offensivity , political slant  and irony  etc. Apart from expensive human annotation, previous works either adopted semi-supervised methods to construct new datasets, or applied complex unsupervised approaches to deal with this issue. In contrast, obtaining simile data is relatively cheap, since it can be identified with clear patterns such as the occurrence of connecting words as ``like''. Even better, there are a rich dozen of simile patterns in Chinese , which further facilitates the automatic construction of simile data.   %In the field of figurative language processing however, while the detection tasks have been thoroughly explored , very few studies actually focused on the generation task , almost all existing works are limited by the lack of annotated or parallel data to a great deal.    %Despite its simple form, simile plays a vital role for written narratives to be attractive. A creative and coherent simile that occurs at proper position of a narration will greatly improve the reading experience . However, existing work on metaphor generation are mostly non-contextual and only focus on continuous generation.  developed a web-driven approach for simile generation within a single sentence.  only focused on generating unconditional verb-oriented metaphors, which requires a pair of fit word  and target word as input. Although  studied the contextual metaphorical generation of poetry, the generation is still in continuous manner, which always generates next lines given previous lines. Hence, none of these works shed lights on polishing plain narrations with both the simile generation and positioning.     %Beyond all that, current researches on text editing or style transfer mostly focused on single sentence rephrasing towards various text attributes such as sentiment, formality , offensivity , political slant  and irony  etc. . Most existing approaches could not be directly applied in the case of narration simile polishment, since our objective is not to rephrase given sentences on the whole. Rather, the proposed task is to generate similes only at proper locations without changing anything from the original writing.   %Meanwhile, there has been great progress in neural generation approaches in recent years due to rapid growth of model architectures as well as available corpus , resulting in various creative applications, from chatbots , to livebot commenting , to streamlined video captioning , etc. Unfortunately, in the field of figurative language processing, despite well-studied detection tasks , the lack of labelled or parallel data limits the research on generation tasks to a great deal .  To this end, we propose a new task of Writing Polishment with Simile 閳ユ敄o firstly decide where to put a simile within plain input text, then figure out what content to generate as a coherent simile. To facilitate our research, we propose a new Chinese Simile  dataset, which contains roughly 5.5 million similes in fictional contexts. %from Chinese online fictions with 92\% simile extraction precision. %For model design, We also set up a benchmark model Locate\&Gen to validate the feasibility and potentials of WPS task. Locate\&Gen model is a two-stage biased generation model upon the framework of transformer encoder-decoder . At the first step, it locates a pointer position for simile interpolation, and then generates a location-specific simile using a novel insertion bias. The two-stage design allows both automatic and semi-automatic inference modes to assist writing polishment flexibly. To summarize, our contributions are three-folded:  } a large-scale Chinese Simile  dataset for public research, which contains millions of similes with contexts extracted from Chinese online fictions.   %Hence, in contrast with the romance of open machine story/metaphor generation discussed above, SP aims at practically improving narrative writings in the hands of human writers. It also poses several new challenges for AI as follows: 1) There are no existing large-scale figurative language corpus, and its annotation is fairly difficult since annotators need to be familiar with writing techniques. 2) Besides generating coherent simile that must be faithful to the original context, the model should also learn to put the generated ingredients at proper position. In this work, we address the SP problem and propose two-staged Locate\&Gen model. In order to obtain large-scale simile dataset, we adopt Chinese simile patterns\footnote[1]{Different from English, there are dozens of patterns for simile expressions in Chinese like ""婵傝棄鍎"", ""閹鎶"", ""娴犲じ缍"", ""鐎规稑顩"", ""娣囥劎鍔"", ""婵″倽瀚"", ""閻樼懓顩"", etc., all standing for the meaning of ""as if"". Also note that similes and metaphors can be exchanged easily by exchanging simile patterns with verbs such as ""Be"" or ""Become"".} to automatically extract sentences containing similes.   In this paper, we introduce a new task, Writing Polishment with Similes, and curate a large-scale Chinese simile dataset. Our experiments demonstrate the feasibility and potential of the task, which we consider as a first step towards figurative writing polishment in a real-world setting. We establish Locate\&Gen model and benchmark it on the developed dataset.  Future works include but not limited to:    Furthermore, from an AI writing assistant perspective, we surmise that assisting humans with writing polishment is more likely to develop the potentials of current AI models than just letting AIs write on the fly . Given that figurative language is an essential creative aspect of language use, we encourage the use of the CS dataset in various contexts and look forward to the emergence of intelligent writing assistant tools like magic\footnote[1]{We applied our Locate\&Gen model to generate this simile, which is ``婵″倸鎮撴鏃婀抽懜顒傛畱'' in Chinese before being translated to English.} in the future.      
"," A simile is a figure of speech that directly makes a comparison, showing similarities between two different things, e.g. ``Reading papers can be dull sometimes, like watching grass grow"". Human writers often interpolate appropriate similes into proper locations of the plain text to vivify their writings. However, none of existing work has explored neural simile interpolation, including both locating and generation. In this paper, we propose a new task of Writing Polishment with Simile  to investigate whether machines are able to polish texts with similes as we human do. Accordingly, we design a two-staged Locate\&Gen model based on transformer architecture. Our model firstly locates where the simile interpolation should happen, and then generates a location-specific simile. We also release a large-scale Chinese Simile  dataset containing 5 million similes with context. The experimental results demonstrate the feasibility of WPS task and shed light on the future research directions towards better automatic text polishment.%with model achieving 76.9\% simile positioning accuracy and decent performance on generation metrics as well as human evaluations.  %Current studies on figurative language generation are either non-contextual or focus only on continuous left-to-right generation manner, which is impractical for polishing written narratives with simile embellishments which may take place at any position of the original content. In this paper, we propose Simile Positioning \& Generation  task闁炽儲鏁刼 first decide a proper insertion position of simile then generate coherent simile content in plain narrations, to investigate whether computational methods are able to refine written narratives with similes as human novelists do. We introduce a large-scale Chinese Simile  dataset, which contains millions of similes with contexts extracted automatically from Chinese online fictions of various types. We establish baseline Insert\&Gen model performances based on SOTA transformer architecture. The experimental results demonstrate the feasibility of SPG task with model achieving 76.9\% accuracy on simile positioning and decent performance on generation metrics and human evaluations.  %Human author is capable of applying figurative language to bring their stories to life, so that readers ""devour"" his vivid narratives. Current researches mainly focused on the continuous story generation  as well as global text editing or style transfer, topics around machines learning to apply figurative techniques for writing is seldom discussed. In this paper, we propose a new task of Simile Positioning\&Generation , which aims to decorate plain narrative sentences with similes at appropriate positions, while being faithful to the original writings. We introduce a large-scale Chinese Simile  dataset, containing millions of contextual similes extracted automatically from Chinese online fictions of various types. We establish baseline Locate\&Gen model performances based on SOTA transformer architecture. The experimental results demonstrate the feasibility of SPG task with model achieving around 80\% accuracy on simile positioning and decent performance on generation metrics and human evaluations.",416
"  A contract is a legally binding agreement that recognizes and governs the rights and duties of the parties to the agreement. Correctly composing contracts is crucial to ensure its legal validity. In many real-world scenarios, a standard contract is prepared by {, which may severely impair the legal validity of the contract.  Contract review is widely used by companies to check contract inconsistencies. However, contract review is labor-intensive and costly. Big companies have to hire tens of thousands of lawyers to conduct contract review, and it is estimated that Fortune Global  and Fortune  companies spend about 35281299,62194.05\%90.90\%$.  Our contributions are summarized as follows:   We formulate the {  framework to address the CIC problem. In PBR, we propose a  that extends the Transformer encoder architecture to efficiently model meaningless blanks.   We collected and labeled  a large-scale Chinese contract corpus for CIC. The experimental results show the promising performance of our PBR method.        In this work, we formulate the {  framework to predict the consistency relation for every two blanks with high accuracy. In PBR, we extend the Transformer encoder architecture and propose BlankCoder, an off-the-shelf effective blank modeling method that could easily generalize to other tasks such as text infilling. Extensive experiments show that our model can significantly and consistently outperform existing baselines, yielding a promising balanced accuracy of  and an F1 score of . In the future, we plan to consider more complex cases  and explore more complex consistency checking scenarios that require logical reasoning.    .     
"," Contract consistency is important in ensuring the legal validity of the contract. In many scenarios, a contract is written by filling the blanks in a precompiled form. Due to carelessness, two blanks that should be filled with the same  content may be incorrectly filled with different  content. This will result in the issue of {  problem, and design an end-to-end framework, called { to address the challenge of modeling meaningless blanks. BlankCoder adopts a two-stage attention mechanism that adequately associates a meaningless blank with its relevant descriptions while avoiding the incorporation of irrelevant context words. Experiments conducted on real-world datasets show the promising performance of our method with a balanced accuracy of $94.05\%$ and an F1 score of $90.90\%$ in the CIC problem.",417
"  Building a human-like open-domain conversational agent  has been one of the milestones in artificial intelligence . Early conversational agents are primarily based on rules , e.g., Eliza , the first CA developed in 60's, simulates a Rogerian psychotherapist based on hand-crafted pattern matching rules. In recent years, with the advancement of data-driven neural networks, neural open-domain conversational models are becoming dominant .  Recent efforts in open-domain neural conversational models are primarily aiming to improve the response diversity  and endowing responses with knowledge , personality , emotion  and empathy .  All the efforts mentioned above are focusing on models that passively respond to user messages. However, in many real-world scenarios, e.g., conversational recommendation, psychotherapy and education, conversational agents are required to actively lead the conversation by smoothly changing the conversation topic to a designated one. For example, during a casual conversation, the agent may actively lead the user to a specific product or service that the agent wants to introduce and recommend.  In this paper, we follow the line of research in  and study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. As illustrated in Figure , given a target keyword ``juice"" and a random starting keyword ``comics"", the agent is required to converse with the user in multiple exchanges and lead the conversation to ``juice"". The challenge of this problem lies in how to balance the tradeoff between maximizing keyword transition smoothness and minimizing the number of turns taken to reach the target. On the one hand, passively responding to the user solely based on the conversation context would achieve high smoothness but may take many turns to reach the target, but on the other hand, directly jumping to the target word by ignoring the conversation context would minimize the number of turns but produce non-smooth keyword transitions.   proposed to break down the problem into two sub-problems: next-turn keyword selection and keyword-augmented response retrieval.  proposed a next-turn keyword predictor and a rule-based keyword selection strategy to solve the first sub-problem, allowing the agent to know what is the next keyword to talk about given the conversation history and the target keyword. In addition,  proposed a keyword-augmented response retrieval model to solve the second sub-problem, allowing the agent to produce a response that is relevant to the selected keyword.    However, there are two major limitations in existing studies . First, the training and evaluation datasets for next-turn keyword prediction are directly extracted from conversations without human annotations, thus, the majority of the ground-truth keyword transitions are noisy and have low correlations with human judgements. As illustrated in Figure , only a few keyword transitions in a conversation are considered relevant. In fact, in our human annotation studies of over 600 keyword transitions, we found that around 70\% of keyword transitions in the next-turn keyword prediction datasets are rated as not relevant, which renders the trained next-turn keyword predictor in existing studies less reliable.  Second, the rule-based keyword selection strategy primarily leverages the cosine similarity between word embeddings to select keywords that are closer to the target keyword. Word embeddings are trained based on the distributional hypothesis that words that have similar contexts have similar meanings, which may not reflect how humans relate words in conversational turn-taking.  In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs  for both next-turn keyword selection and keyword-augmented response retrieval. Humans rely on commonsense to reason, and commonsense reasoning plays an important role in the cognitive process of conversational turn-taking . Relying on a CKG for keyword transition would allow the agent to select a more target-related keyword for the next-turn.  Moreover, we leverage commonsense triplets from the CKG using Graph Neural Networks  for both next-turn keyword prediction and keyword-augmented response retrieval to achieve more accurate predictions.   In summary, our contributions are as follows:          We study the problem of imposing conversational goals/keywords on open-domain conversational agents. The keyword transition module in existing approaches suffer from noisy datasets and unreliable transition strategy. In this paper, we propose to ground keyword transitions on commonsense and propose two GNN-based models for the tasks of next-turn keyword transition and keyword-augmented response retrieval, respectively. Extensive experiments show that our proposed model obtains substantially better performance on these two tasks than competitive baselines. In addition, the model analysis suggests that CKG triplets and our proposed CKG-guided keyword selection strategy are helpful in learning utterance representation and keyword transition, respectively. Finally, both self-play simulations and human evaluations show that our model can achieve better success rate, reach the target keyword faster, and produce smoother conversations than baselines.  
"," We study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. Solving this problem enables the application of conversational agents in many real-world scenarios, e.g., recommendation and psychotherapy. The dominant paradigm for tackling this problem is to 1) train a next-turn keyword classifier, and 2) train a keyword-augmented response retrieval model. However, existing approaches in this paradigm have two limitations: 1) the training and evaluation datasets for next-turn keyword classification are directly extracted from conversations without human annotations, thus, they are noisy and have low correlation with human judgements, and 2) during keyword transition, the agents solely rely on the similarities between word embeddings to move closer to the target keyword, which may not reflect how humans converse. In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs  for both keyword transition and response retrieval. Automatic evaluations suggest that commonsense improves the performance of both next-turn keyword prediction and keyword-augmented response retrieval. In addition, both self-play and human evaluations show that our model produces responses with smoother keyword transition and reaches the target keyword faster than competitive baselines.",418
"   Despite of remarkable progress made in NMT recently , most NMT systems are still prone to translation errors caused by noisy input sequences. One common type of input noise is homophone noise, where words or characters are mis-recognized as others with same or similar pronunciation in ASR or input systems for non-phonetic languages , as illustrated by the example in Table.   Previous works suggest that incorporating phonetic embeddings into NMT and augmenting training data with adversarial examples with injected homophone noise would alleviate this issue. Intuitively, humans usually have no trouble in disambiguating sentences corrupted with moderate homophone noise via context and syllable information. We propose a human-inspired robust NMT framework tailored to homophone noise for Chinese-English translation, which is composed of a homophone noise detector  and a syllable-aware NMT  model.  []  \toprule[1pt] Clean Input~&~{UTF8}{gbsn}瀵よ桨绔撮幍鐏忓繐顒    \\ Output of NMT~&~build a primary school \\ {3pt}{3pt} Noisy Input~&~{UTF8}{gbsn}瀵ょuline{鐠佺晽閹电亸蹇擃劅  \\  Output of NMT~&~suggest a primary school \\  {3pt}{3pt} Mixed Transcript~&~{UTF8}{gbsn}瀵ょuline{yi}閹电亸蹇擃劅  \\  Output of Ours~&~build a primary school\\  {gbsn}鐠佺敍end{CJK}''  in the noisy input is a homophone corresponding to the original character ``{UTF8}{gbsn}娑''  in the clean input. The erroneous character ``{UTF8}{gbsn}鐠佺敍end{CJK}'' is replaced with its Chinese Pinyin ``yi'' in the mixed transcript which enables NMT to translate correctly.}  %   Due to the lack of data annotated with homophone noise, we propose to train our detector on monolingual data in a self-supervised manner, where Chinese characters sequences as input and their corresponding syllables sequence as label to predict the possibility that a character is homophone noise. The identified homophone errors from a source sentence are then converted into corresponding syllables to produce a new source sequence mixed with characters and syllables. Augmenting bilingual training data with instances where original source sentences are substituted with their corresponding character-syllable-mixed sequences, we train the SANMT model to translate such unconventional inputs. To examine the effectiveness of our proposed model, we conduct extensive experiments on both artificial noisy test sets and a real-world noise test set with homophone noise in speech translation  scenario. The test set will be released soon. Our experimental results on ChineseEnglish translation clearly show that the proposed method is not only significantly superior to previous approaches in alleviating the impact of homophone noise on NMT, but also achieves a substantial improvement on the clean text. %Due to the lack of data annotated with homophone noise, we propose to train our detector on monolingual data in a self-supervised manner, where Chinese characters are automatically transformed into syllables to predict homophone noise. The identified homophone errors from a source sentence are then converted into corresponding syllables to produce a new source sequence mixed with characters and syllables. Augmenting training data with instances where original source sentences are substituted with their corresponding character-syllable-mixed sequences, we train the SANMT model to translate such unconventional inputs. To examine the effectiveness of our proposed model, we conduct extensive experiments on both artificial noisy test sets and a real-world noise test set with homophone noise in speech translation  scenario. The test set will be released soon. Our experimental results on ChineseEnglish translation clearly show that the proposed method is not only significantly superior to previous approaches in alleviating the impact of homophone noise on NMT, but also achieves a substantial improvement on the clean text.        In this paper, we have presented a novel framework composed of a homophone error detector and a SANMT model to cope with homophone noise. Experimental results show that our method not only achieves substantial improvement over previous robust NMT baselines both on the test sets with artificial or real-world noise, but also outperforms the NMT baseline on the clean test sets. We consider that future studies could modeling noise detection and NMT jointly.    References should be produced using the bibtex program from suitable   BiBTeX files . The IEEEbib.bst bibliography   style file from IEEE produces unsorted bibliography list.   -------------------------------------------------------------------------  \clearpage 
"," In this paper, we propose a robust neural machine translation  framework. The framework consists of a homophone noise detector and a syllable-aware NMT model to homophone errors. The detector identifies potential homophone errors in a textual sentence and converts them into syllables to form a mixed sequence that is then fed into the syllable-aware NMT. Extensive experiments on Chinese$\rightarrow$English translation demonstrate that our proposed method not only significantly outperforms baselines on noisy test sets with homophone noise, but also achieves a substantial improvement on clean text.",419
" In recent years, there has been a dramatic surge in the adoption of voice assistants such as Amazon Alexa, Apple Siri, and Google Assistant. Customers use them for a variety of tasks such as playing music and online shopping.  These voice assistants are built on complex Spoken Language Understanding  systems that are typically too large to store on an edge device such as a mobile phone or a smart speaker. Hence, user traffic is routed through a cloud server to process requests. This has led to privacy concerns and fueled the push for tiny AI and edge processing, where the user requests are processed on the device itself.   Traditional SLU systems consist of a two-stage pipeline, an Automatic Speech Recognition  component that processes customer speech and generates a text transcription , followed by a Natural Language Understanding  component that maps the transcription to an actionable hypothesis consisting of intents and slots . An end-to-end  system that goes directly from speech to the hypothesis would help make the SLU system smaller and faster, allowing it to be stored on an edge device. It could potentially also be better optimized than a pipeline since it eliminates cascading errors.  However, E2E systems are not used in practice because they have some key issues. These systems are hard to build since they consist of large neural components such as transformers and require massive amounts of E2E training data. They also don't make use of the vastly available training data for the ASR and NLU components that could be used to enhance their performance, because the examples in these datasets may not be aligned to create an E2E training sample. Another issue is feature expansion, a scenario where a new domain, with new intents and slots, is added to the voice assistant's capabilities. Here, developers typically only have access to some synthetically generated text-hypothesis examples. Speech data isn't readily available and it is very expensive to collect. E2E models thus fail as they require lots of new audio and hypothesis data to learn this new domain.  In this work, we build an E2E model that mitigates these issues using transfer learning. We call it the Audio-Text All-Task  Model. AT-AT is an E2E transformer-based model that is jointly trained on multiple audio-to-text and text-to-text tasks. Examples of these tasks include speech recognition , hypothesis prediction from speech , masked LM prediction , and hypothesis prediction from text . Our model achieves this by converting data from all these tasks into a single audio-to-text or text-to-text format. Figure shows this joint training phase in detail. Our findings indicate that there is significant knowledge transfer taking place from multiple tasks, which in turn helps in downstream model performance. We see that the AT-AT pretrained model shows improved performance on SLU hypothesis prediction on internal data collected from Alexa traffic. We also report state-of-the-art results on two public datasets: FluentSpeech , and SNIPS Audio .   Furthermore, since our model contains a text encoder, it can consume both audio and text inputs to generate a target sequence. By jointly training on both audio-to-text and text-to-text tasks, we hypothesize that this model learns a shared representation for audio and text inputs. This allows us to simply train on new text-to-text data and get audio-to-text performance for free, giving us a way to do E2E hypothesis prediction in a zero-shot fashion during feature expansion. We test this approach on an internal dataset from Alexa traffic, and an external dataset, Facebook TOP . Since TOP consists of only text data, we collected speech data for the test split using an internal tool at Amazon. We will soon release this dataset.  In summary, our contributions are as follows.           Our evaluation clearly shows that there is a lot of knowledge transfer happening between various speech processing tasks. AT-AT when evaluated on downstream SLU tasks benefits significantly when it is pretrained with additional ASR data. This result holds when the ASR data is from the same domain  and also when the data is from a different domain . It also holds across different dataset sizes. We see that the pretraining is extremely helpful for datasets with training data size of about a 1000 such as SNIPS, and it remains helpful all the way to our limited internal music dataset  and the full music dataset . We believe this is because the decoder learns a good language model by seeing additional ASR data. We can also think of these additional pretraining tasks as good regularizers.    Our zeroshot results with AT-AT are even more interesting. We designed a way to train an end-to-end model on new data without using any corresponding audio data, real or synthetically generated, and our model's performance, while not matching an end-to-end model trained on real audio data, is still remarkable. Our approach can be adapted to make use of synthetic data if we have access to a TTS system to further improve performance. We managed to learn a shared audio-text model, not by explicitly enforcing a loss penalty to force the audio and text hidden states into the same space, but by constraining the decoder and forcing the model to learn jointly from different input sources.    On a closing note, we would like to remark that AT-AT somewhat mimics actual human learning. We typically read a lot more words than we hear. But when we hear a word for the first time, we transfer our knowledge of that word from when we read it. AT-AT similarly learns to understand and perform NLU tagging from text and then applies this knowledge when it is given speech.   We propose the Audio-Text All-Task  model that uses transfer learning to improve the performance on end-to-end SLU. AT-AT beat the performance of E2E models on our internal music data, both in the full and low-resource settings. It also achieved state-of-the-art performance on the FluentSpeech  and SNIPS audio datasets  with significant improvements over prior models. AT-AT also demonstrated its ability to perform zeroshot E2E SLU, without access to a TTS system, and by learning a shared audio-text representation without any explicit loss penalty to force the audio and text hidden states into the same space. We also showed how AT-AT can work in conjunction with a TTS system to further improve E2E performance. It achieves a zeroshot E2E EM Accuracy of 70.60 on the TOP dataset.    We set this new benchmark and release the audio data for the TOP dataset for future research.   On a closing note, we would like to remark that AT-AT somewhat mimics actual human learning. We typically read a lot more words than we hear. But when we hear a word for the first time, we transfer our knowledge of that word from when we read it. AT-AT similarly learns to understand and perform NLU tagging from text and then applies this knowledge when it is given speech.    
"," Voice Assistants such as Alexa, Siri, and Google Assistant typically use a two-stage Spoken Language Understanding pipeline; first, an Automatic Speech Recognition  component to process customer speech and generate text transcriptions, followed by a Natural Language Understanding  component to map transcriptions to an actionable hypothesis. An end-to-end  system that goes directly from speech to a hypothesis is a more attractive option. These systems were shown to be smaller, faster, and better optimized. However, they require massive amounts of end-to-end training data and in addition, don't take advantage of the already available ASR and NLU training data.  In this work, we propose an E2E system that is designed to jointly train on multiple speech-to-text tasks, such as ASR  and SLU , and text-to-text tasks, such as NLU . We call this the Audio-Text All-Task  Model and we show that it beats the performance of E2E models trained on individual tasks, especially ones trained on limited data. We show this result on an internal music dataset and two public datasets, FluentSpeech and SNIPS Audio, where we achieve state-of-the-art results. Since our model can process both speech and text input sequences and learn to predict a target sequence, it also allows us to do zero-shot E2E SLU by training on only text-hypothesis data  from a new domain. We evaluate this ability of our model on the Facebook TOP dataset and set a new benchmark for zeroshot E2E performance. We will soon release the audio data collected for the TOP dataset for future research.",420
"  Neural Machine Translation   has achieved state of the art in various MT systems, including rich and low resource language pairs . However, the quality of low-resource MT is quite unpretentious due to the lack of parallel data while it has achieved better results on systems of the available resource. Therefore, low-resource MT is one of the essential tasks investigated by many previous works .    Recently, some works present MT systems that have achieved remarkable results for low-resource language . Inspired by these works, we collect data from the TED Talks domain, then attempt to build multilingual MT systems from French, English-Vietnamese. Experiments demonstrate that both language pairs: French-Vietnamese and English-Vietnamese have achieved significant performance when joining the training. %  Although multilingual MT can reduce the sparse data in the shared space by using word segmentation, however, rare words still exist, evenly they are increased more if languages have a significant disparity in term vocabulary. Previous works suggested some strategies to reduce rare words such as using translation units at sub-word and character levels or generating a universal representation at the word and sentence levels . These help to downgrade the dissimilarity of tokens shared from various languages. However, these works require learning additional parameters in training, thus increasing the size of models.   Our paper presents two methods to augment the translation of rare words in the source space without modifying the architecture and model size of MT systems:  exploiting word similarity. This technique has been mentioned by previous works . They employ monolingual data or require supervised resources like a bilingual dictionary or WordNet, while we leverage relation from the multilingual space of MT systems.  Adding a scalar value to the rare word embedding in order to facilitate its translation in the training process.  %  Due to the fact that NMT tends to have bias in translating frequent words, so rare words  often have less opportunity to be considered. Our ideal is inspired by the works of .  and  proposed various solutions to urge for translation of rare words, including modification embedding in training. They only experimented with recurrent neural networks  while our work uses the state-of-the-art transformer architecture.  transforms the word embedding of a token into the universal space, and they learn plus parameters while our method does not.  We apply our strategies in our fine-tuning processes, and we show substantial improvements of the systems after some epochs only.    Monolingual data are widely used in NMT to augment data for low-resource NMT systems . Back-translation  is known as the most popular technique in exploiting target-side monolingual data to enhance the translation systems while the self-learning method  focuses on utilizing source-side monolingual data. Otherwise, the dual-learning strategy  also suggests using both source- and target-side monolingual data to tackle this problem. Our work investigates the self-learning method  on the low-resource multilingual NMT systems specifically related to Vietnamese. Besides, monolingual data are also leveraged in unsupervised or zero-shot translation.  % learn the lexical relative between one token on a source language and the other once from another source language without modifying the system architecture as well as the model size. We also do not use any additional resources in our systems.   The main contributions of our work are:   \setlength{     In section 2, we review the transformer architecture used for our experiments. The brief of multilingual translation is shown in section 3. Section 4 presents our methods to deal with rare words in multilingual translation scenarios. The exploitation of monolingual data for low-resource multilingual MT is discussed in section 5. Our results are described in section 6, and related work is shown in section 7. Finally, the paper ends with conclusions and future work. %   We have built multilingual MT systems for two low-resource language pairs: English-Vietnamese and French-Vietnamese, and proposed two approaches to tackle rare word translation. We show that our approaches bring significant improvements to our MT systems. We find that the pseudo bilingual can furthermore enhance a multilingual NMT system in case of French  Vietnamese translation task.  In the future, we would like to use more language pairs in our systems and to combine proposed methods in order to evaluate the effectiveness of our MT systems.   
"," % Prior works have demonstrated that a low-resource language pair can be benefited from a multilingual machine translation  system which relies on the jointly training many language pairs. In this paper, we propose two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese,  English-Vietnamese. The first strategy learns  dynamically word similarity of tokens in the shared space among source languages whilst the other one augments the translation ability of rare words through updating their embeddings during the training. In addition, we attempt to leverage monolingual data which is generated from multilingual MT to reinforce synthetic parallel in the data sparsity situation. We show that significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and release datasets for the research community.  Prior works have demonstrated that a low-resource language pair can benefit from multilingual machine translation  systems, which rely on many language pairs' joint training. This paper proposes two simple strategies to address the rare word issue in multilingual MT systems for two low-resource language pairs: French-Vietnamese and English-Vietnamese. The first strategy is about dynamical learning word similarity of tokens in the shared space among source languages while another one attempts to augment the translation ability of rare words through updating their embeddings during the training. Besides, we leverage monolingual data for multilingual MT systems to increase the amount of synthetic parallel corpora while dealing with the data sparsity problem. We have shown significant improvements of up to +1.62 and +2.54 BLEU points over the bilingual baseline systems for both language pairs and released our datasets for the research community.",421
" Although deep neural networks have recently been contributing to state-of-the-art advances in various areas , %in NLP problems ,  such black-box models may not be deemed reliable in situations where safety needs to be guaranteed, such as legal judgment prediction and medical diagnosis. Interpretable deep neural networks are a promising way to increase the reliability of neural models. To this end, extractive rationales, i.e., subsets of features of instances on which models rely for their predictions on the instances, can be used as evidence for humans to decide whether or not to trust a predicted result and, more generally, to trust a~model.   Previous works mainly use selector-predictor types of neural models to provide extractive rationales, i.e., models composed of two modules:  a selector that selects a subset of important features, and  a  predictor that makes a prediction based solely on the selected features. For example,  and  use a selector network to calculate a selection probability for each token in a sequence, then sample a set of tokens that is exclusively passed to the predictor. %The supervision is solely on the answer given by the prediction. %One then calculates a loss between the result given by the predictor and the ground-truth answer.  An additional typical desideratum in natural language pro\-cessing  tasks is that the selected tokens form a semantically fluent rationale. To achieve this,  added a non-differential regularizer that encourages any two adjacent tokens to be simultaneously selected or unselected. %The selector and predictor are jointly trained in a REINFORCE-style manner [cite Williams 92] because the sampling process and the regularizer are not differentiable.  further improved the quality of the rationales by using a Hard Kuma regularizer that also encourages any two adjacent tokens to be selected or unselected together. %, which is differentiable.    One drawback of previous works is that the learning signal for both the selector and the predictor comes mainly from comparing the prediction of the selector-predictor model with the ground-truth answer. %, while the predictor tells the selector to what extent the selected features contribute to the prediction, it does not directly tell the selector what kind of features are still missing or over-selected for a correct prediction.  Therefore, the exploration space to get to the correct rationale is large, decreasing the chances of converging to the optimal rationales and predictions.  Moreover, in NLP applications, the regularizers commonly used for achieving fluency of rationales treat all adjacent token pairs in the same way. This often leads to the selection of unnecessary tokens due to their adjacency to informative~ones. %Intuitively, if two tokens frequently occur adjacently, then they are more likely to be simultaneously selected or unselected. These important adjacent token pairs should receive more priority by the regularizer.   In this work, we first propose an alternative method to rationalize the predictions of a neural model. Our method aims to squeeze more information from the predictor in order to guide the selector in selecting the rationales. Our method trains two models: a ``guider"" model that solves the task at hand in an accurate but black-box manner, and a selector-predictor model that solves the task while also providing rationales. We use an adversarial-based method to encourage the final information vectors generated by the two models to encode the same information. We use an information bottleneck technique in two places: ~to encourage the features selected by the selector to be the least-but-enough features, and ~to encourage the final information vector of the guider model to also contain the least-but-enough information for the prediction.    Secondly, we propose using language models as regularizers for rationales in natural language understanding tasks. A language model  regularizer encourages rationales to be fluent subphrases, which means that the rationales are formed by consecutive tokens while avoiding unnecessary tokens to be selected simply due to their adjacency to informative tokens. %novel regularizer for the consecutiveness and semantic fluency of the rationale in NLP applications. This regularizer is based on a language model , which gives priority to important adjacent tokens so that they are simultaneously being selected.  The effectiveness of our LM-based regularizer is proved by both mathematical derivation and experiments. All the further details are given in the Appendix of the extended  paper.  Our contributions are briefly summarized as follows:            In this work, we proposed a novel method to extract rationales for neural predictions. Our method uses an adversarial-based technique to make a selector-predictor model learn from a guider model. In addition, we proposed a novel regularizer based on language models, which makes the extracted rationales semantically fluent.  In this way, the ``guider"" model  tells the selector-predictor model what kind of information  remains unselected or over-selected.   We conducted experiments on a task of sentiment analysis and three tasks from the legal domain.  The experimental results showed that our method improves the selection of rationales by a large margin.   This regularizer  also gives priority to important adjacent word pairs when considering whether to select or unselect them simultaneously,  which further refines the rationales.   Finally, we have conducted experiments on two datasets to prove the effectiveness of our model.  We conducted experiments on two datasets to prove the effectiveness of our model.  As future work, the main architecture of our model can be directly applied to other domains, e.g., images or tabular data. However, it remains an open question what would be a good regularizer for these domains.   For example, variational autoencoders with discrete latent space, providing rationales to different kinds of deep learning applications.     
","  Explaining the predictions of AI models is paramount in safety-critical applications, such as in legal or medical domains. One form of explanation for a prediction is an extractive rationale, i.e., a subset of features of an instance that lead the model to give its prediction on the instance.  Previous works on generating extractive rationales usually employ a two-phase model: a selector that selects the most important features  followed by a predictor that makes the prediction based exclusively on the selected features. One disadvantage of these works is that the main signal for learning to select features comes from the comparison of the answers given by the predictor and the ground-truth answers. In this work, we propose to squeeze more information from the predictor via an information calibration method. More precisely, we train two models jointly: one is a typical neural model that solves the task at hand in an accurate but black-box manner, and the other is a selector-predictor model that additionally produces a rationale for its prediction. The first model is used as a guide to the second model. We use an adversarial-based technique to calibrate the information extracted by the two models such that the difference between them is an indicator of the missed or over-selected features. In addition, for natural language tasks, we propose to use a language-model-based regularizer to encourage the extraction of fluent rationales. Experimental results on a sentiment analysis task as well as on three tasks from the legal domain show the effectiveness of our approach to rationale extraction.",422
" % background Sentence semantic matching is a fundamental Natural Language Processing~ task that tries to infer the most suitable label for a given sentence pair. For example, Natural Language Inference~ targets at classifying the input sentence pair into one of the three relations~. Paraphrase Identification~ aims at identifying whether the input sentence pair expresses the same meaning. Figure gives some examples with different semantic relations from different datasets.    % Current state As a fundamental technology, sentence semantic matching has been applied successfully into many NLP fields, e.g., information retrieval, question answering, and dialog system.  Currently, most work leverages the advancement of representation learning techniques to tackle this task.  They focus on input sentences and design different architectures to explore sentence semantics comprehensively and precisely.  Among all these methods, BERT plays an important role.  It adopts multi-layer transformers to make full use of large corpus~ for the powerful pre-trained model.  Meanwhile, two self-supervised learning tasks~ are designed to better analyze sentence semantics and capture as much information as possible.  % more citation Based on BERT, plenty of work has made a big step in sentence semantic modeling.    In fact, since relations are the predicting targets of sentence semantic matching task, most methods do not pay enough attention to the relation learning.  They just leverage annotated labels to represent relations, which are formulated as one-hot vectors.  However, these independent and meaningless one-hot vectors cannot reveal the rich semantic information and guidance of relations, which will cause an information loss.  ~ has observed that different relations among sentence pairs imply specific semantic expressions.  Taking Figure as an example, most sentence pairs with ``contradiction'' relation contain negation words~.  ``entailment'' relation often leads to exact numbers being replaced with approximates~.  ``Neutral'' relation will import some correct but irrelevant information~.  Moreover, the expressions between sentence pairs with different relations are very different.  Therefore, the comparison and contrastive learning among different relations~ can help models to learn more about the semantic information implied in the relations, which in turn helps to strengthen the sentence analysis ability of models. They should be treated as more than just meaningless one-hot vectors.   One of the solutions for better relation utilization is the embedding method inspired by Word2Vec.  Some researchers try to jointly encode the input sentences and labels in the same embedding space for better relation utilization during sentence semantic modeling.  Despite the progress they have achieved, label embedding method requires more data and parameters to achieve better utilization of relation information.  It still cannot fully explore the potential of relations due to the small number of relation categories or the lack of explicit label embedding initialization.   To this end, in this paper, we propose a novel \fullname~approach to make full use of relation information in a simple but effective way.  In concrete details, we first utilize pre-trained BERT to model semantic meanings of the input words and sentences from a global perspective.  Then, we develop a CNN-based encoder to obtain partial information~ of sentences from a local perspective.  Next, inspired by self-supervised learning methods in BERT training processing, we propose a Relation of Relation~ classification task to enhance the learning ability of \shortname~for the implicit common features corresponding to different relations.    Moreover, a triplet loss is used to constrain the model, so that the intra-class and inter-class relations are analyzed better.   Along this line, input sentence pairs with the same relations will be represented much closer and vice versa further apart.  Relation information is properly integrated into sentence pair modeling processing, which is in favor of tackling the above challenges and improving the model performance.  Extensive evaluations of two sentence semantic matching tasks  demonstrate the effectiveness of our proposed \shortname~and its advantages over state-of-the-art sentence semantic matching baselines.      In this paper, we presented a simple but effective method named \shortname~for sentence semantic matching.  This method not only uses powerful BERT and CNN to encode sentences from global and local perspectives, but also makes full use of relation information for better performance enhancement.  Specifically, we design a R classification task to help \shortname~for learning the implicit common knowledge from the pairwise relation learning processing.  Moreover, a triplet loss is employed to constrain \shortname~for better triplet based relation learning and intra-class and inter-class information analyzing.  Extensive experiments on NLI and PI tasks demonstrate the superiority of \shortname. In the future, we plan to combine the advantages of label embedding method for better sentence semantic comprehension.    
"," 	% background 	Sentence semantic matching is one of the fundamental tasks in natural language processing, which requires an agent to determine the semantic relation among input sentences.  	% current state 	Recently, deep neural networks have achieved impressive performance in this area, especially BERT.  	% problem 	Despite their effectiveness, most of these models treat output labels as meaningless one-hot vectors, underestimating the semantic information and guidance of relations that these labels reveal, especially for tasks with a small number of labels.  	% solution 	To address this problem, we propose a \fullname~for sentence semantic matching. 	 	Specifically, we first employ BERT to encode the input sentences from a global perspective. 	Then a CNN-based encoder is designed to capture keywords and phrase information from a local perspective.  	To fully leverage labels for better relation information extraction, we introduce a self-supervised relation of relation classification task for guiding \shortname~to consider more about relations.  	Meanwhile, a triplet loss is employed to distinguish the intra-class and inter-class relations in a finer granularity. 	% result 	Empirical experiments on two sentence semantic matching tasks demonstrate the superiority of our proposed model.  	As a byproduct, we have released the codes to facilitate other researches.",423
" 	Discovering novel user intents is important to improve the service quality in dialogue systems. By analyzing the discovered new intents, we may find underlying user interests, which could provide business opportunities and guide the improvement direction.  	 	 	Intent discovery has attracted much attention in recent years. Many researchers regard it as an unsupervised clustering problem, and they manage to incorporate some weak supervised signals to guide the clustering process. For example,~ propose a hierarchical semantic clustering model and collect web page clicked information as implicit supervision for intent discovery.~ utilize a semantic parsing graph as extra knowledge to mine novel intents during clustering.~ benefit from the consensus predictions of multiple clustering techniques to discover similar semantic intent-wise clusters.~ cluster questions into user intent categories under the supervision of structured outputs.~ extract intent features with an autoencoder and automatically label the intents with a hierarchical clustering method. 	  	However, all of the above methods fail to leverage the prior knowledge of known intents. These methods assume that the unlabeled samples are only composed of undiscovered new intents. A more common case is that some labeled data of known intents are accessible and the unlabeled data are mixed with both known and new intents. As illustrated in Figure, we may have a few labeled samples  of known intents in advance. The remaining known and new intent samples are all unlabeled. Our goal is to find known intents and discover new intents with the prior knowledge of limited labeled data. Our previous work CDAC+ directly tackles this problem. Nevertheless, it uses pairwise similarities as weak supervised signals, which are ambiguous to distinguish a mixture of unlabeled known and new intents. Thus, the performance drops with more new intents. 	 	To summarize, there are two main difficulties in our task. On the one hand, it is challenging to effectively transfer the prior knowledge from known intents to new intents with limited labeled data. On the other hand, it is hard to construct high-quality supervised signals to learn friendly representations for clustering both unlabeled known and new intents. 	 	To solve these problems, we propose an effective method to leverage the limited prior knowledge of known intents and provide high-quality supervised signals for feature learning.  As illustrated in Figure, we firstly use the pre-trained BERT model to extract deep intent features. Then, we pre-train the model with the limited labeled data under the supervision of the softmax loss. We retain the pre-trained parameters and use the learning information to obtain well-initialized intent representations. Next, we perform clustering on the extracted intent features and estimate the cluster number   by eliminating the low-confidence clusters. 	 	As most of the training samples are unlabeled, we propose an original alignment strategy to construct high-quality pseudo-labels as supervised signals for learning discriminative intent features. For each training epoch, we firstly perform k-means on the extracted intent features, and then use the produced cluster assignments as pseudo-labels for training the neural network. However, the inconsistent assigned labels cannot be directly used as supervised signals, so we use the cluster centroids as the targets to obtain the alignment mapping between pseudo-labels in consequent epochs. Finally, we perform k-means again for inference. Benefit from the relatively consistent aligned targets, our method can inherit the history learning information and boost the clustering performance. 	 	We summarize our contributions as follows. Firstly, we propose a simple and effective method that successfully generalizes to mass of new intents and estimate the number of novel classes with limited prior knowledge of known intents. Secondly, we propose an effective alignment strategy to obtain high-quality self-supervised signals by learning discriminative features to distinguish both known and new intents. Finally, extensive experiments on two benchmark datasets show our approach yields better and more robust results than the state-of-the-art methods.  	 	  	In this work, we have introduced an effective method for discovering new intents. Our method successfully transfers the prior knowledge of limited known intents and estimates the number of intents by eliminating low-confidence clusters. Moreover, it provides more stable and concrete supervised signals to guide the clustering process. We conduct extensive experiments on two challenging benchmark datasets to evaluate the performance. Our method achieves significant improvements over the compared methods and obtains more accurate estimated cluster numbers with limited prior knowledge. In the future, we will try different clustering methods to produce supervised signals and explore more self-supervised methods for representation learning. 	 	 	
"," 		Discovering new intents is a crucial task in dialogue systems. Most existing methods are limited in transferring the prior knowledge from known intents to new intents. They also have difficulties in providing high-quality supervised signals to learn clustering-friendly features for grouping unlabeled intents. In this work, we propose an effective method, Deep Aligned Clustering, to discover new intents with the aid of the limited known intent data. Firstly, we leverage a few labeled known intent samples as prior knowledge to pre-train the model. Then, we perform k-means to produce cluster assignments as pseudo-labels. Moreover, we propose an alignment strategy to tackle the label inconsistency problem during clustering assignments. Finally, we learn the intent representations under the supervision of the aligned pseudo-labels. With an unknown number of new intents, we predict the number of intent categories by eliminating low-confidence intent-wise clusters. Extensive experiments on two benchmark datasets show that our method is more robust and achieves substantial improvements over the state-of-the-art methods. The codes are released at \url{https://github.com/thuiar/DeepAligned-Clustering}.",424
" The U.S.~NIH's precision medicine  initiative calls for designing treatment and preventative interventions considering genetic, clinical, social, behavioral, and environmental exposure variability among patients. The initiative rests on the widely understood finding that considering individual variability is critical in tailoring healthcare interventions to achieve substantial progress in reducing disease burden worldwide. Cancer was chosen as its near term focus with the eventual aim of expanding to other conditions. As the biomedical research enterprise strives to fulfill the initiative's goals, computing needs are also on the rise in drug discovery, predictive modeling for disease onset and progression, and in building NLP tools to curate information from the evidence base being generated.   [hbt]   ll@{}}     \toprule     Facet & Input\\      \midrule     Disease & Melanoma \\     Genetic variation & BRAF  \\     Demographics & 64-year-old female \\     \midrule     Disease & Gastric cancer \\     Genetic variation & ERBB2 amplification\\     Demographics & 64-year-old male\\            In a dovetailing move, the U.S.~NIST's  TREC  has been running a PM track since 2017 with a focus on cancer. The goal of the TREC-PM task is to identify the most relevant biomedical articles and clinical trials for an input patient case. Each case is composed of    a disease name,   a gene name and genetic variation type, and  demographic information . Table shows two example cases from the 2019 track. So the search is ad hoc in the sense that we have a free text input in each facet but  the    facets themselves highlight the PM related attributes that ought to characterize the retrieved documents. We believe this style of faceted retrieval is going to be more common across medical IR tasks for many conditions as the PM initiative continues its mission.     The vocabulary mismatch problem is a prominent issue in medical IR given the large variation in the expression of medical concepts and events. For example, in the query ``What is a potential side effect for Tymlos?'' the drug is referred by its brand name. Relevant scientific literature may contain the generic name Abaloparatide more frequently. Traditional document search engines have clear limitations on resolving   mismatch issues. The IR community has extensively explored methods to address the vocabulary mismatch problem, including query expansion based on relevance feedback, query term re-weighting, or query reconstruction by optimizing the query syntax.  Several recent studies highlight exploiting neural network models for query refinement in document retrieval  settings.   address  this issue by generating a transformed query from the initial query using a neural model.  They use reinforcement  learning  to train it where an agent  learns to reformulate the initial query to maximize the expected return  through actions . In a different approach,   use RL for sentence ranking for extractive summarization.    In this paper, building on the BERT architecture, we focus on a different hybrid document scoring and reranking setup involving three components: .~a document relevance classification model, which predicts  whether a document is relevant to the given query ; .~a keyword extraction model which spots tokens in a document that are likely to be seen in PM related queries; and .~an abstractive document summarization model that generates a pseudo-query given the document context and a facet type  via the BERT encoder-decoder setup. The keywords ) and the pseudo-query ) are together compared with the original query to generate a score. The scores from all the components are combined to rerank top   documents returned with a basic Okapi BM25 retriever from a Solr index of the corpora. %This is critical because neural document-query matching and summarization are expensive operations that cannot practically scale to the full corpus.  Our main innovation is in pivoting from the focus on queries by previous methods to emphasis on transforming candidate documents into pseudo-queries via summarization. Additionally, while generating the pseudo-query, we also let the   decoder output concept codes from biomedical terminologies that capture disease and gene names. We do this by embedding both words and concepts in a common semantic space before letting the decoder generate summaries that include concepts. Our overall architecture was evaluated using the TREC-PM datasets  with the 2019 dataset used as the test set. The results show an absolute  improvement in P@10 compared to prior best approaches while obtaining a small  gain in R-Prec. Qualitative analyses also highlight how the summarization is able to focus on document segments that are highly relevant to patient cases.     In this paper, we  proposed an ensemble document reranking approach  for PM queries. It builds on pretrained BERT models to combine strategies from document relevance matching and extractive/abstractive text summarization to arrive at document rankings that are complementary in eventual evaluations. Our experiments also demonstrate that  entity embeddings   trained on an annotated domain specific corpus can help in   document retrieval settings. Both quantitative and qualitative analyses throw light on the strengths of our approach.  One scope for advances lies in improving the summarizer to generate better pseudo-queries  so that ABS starts to perform better on its own. At a high level, training data is very hard to generate in large amounts for IR tasks in biomedicine and this holds for the TREC-PM datasets too. To better train  ABS, it may be better to adapt other biomedical IR datasets. For example, the TREC clinical decision support  task that ran from 2014 to 2016 is related to the PM task. A future goal is to see if we can apply our neural transfer learning and domain adaptation efforts to repurpose the CDS datasets for the PM task.   Another straightforward idea is to reuse generated pseudo-query sentences in the eDisMax query by Solr, as a form of  pseudo relevance feedback. The $s_{   
"," Information retrieval  for precision medicine  often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patient. Other factors such as demographic attributes, comorbidities, and social determinants may also be pertinent. As such, the retrieval problem is often formulated as ad hoc search but with multiple facets  that may need to be incorporated. In this paper, we present a document reranking approach that combines neural query-document matching and text summarization toward such retrieval scenarios. Our architecture builds on the basic BERT model with three specific components for reranking: . document-query matching . keyword extraction and . facet-conditioned abstractive summarization. The outcomes of  and  are used to essentially transform a candidate document into a concise summary that can be compared with the query at hand to compute a relevance score. Component  directly generates a matching score of a candidate document for a query. The full architecture benefits from the complementary potential of document-query matching and the novel document transformation approach based on summarization along PM facets. Evaluations using NIST's TREC-PM track datasets  show that our model achieves state-of-the-art performance. To foster reproducibility, our code is made available here: \url{https://github.com/bionlproc/text-summ-for-doc-retrieval}.",425
"  . }   In real-world dialogue systems, a substantial portion of all user queries are ambiguous ones for which the system is unable to precisely identify the underlying intent.  %For example, nearly 30\% of user queries in a real-world QA system are ambiguous questions. % Can't give statistics in an academic paper without mentioning details We observed that many such queries in our question answering  system exhibited one of the following two characteristics. [partopsep=0pt,topsep=0pt]      %The ambiguous questions in our QA system can be summarized into 2 types:\\ %  \\ %        % We will illustrate the method of finding such intents set in detail in the methodology section. % %  introduced methods to ask clarification questions for information that is missing from a given linguistic context.  use generative model to generate clarification questions for solving entity ambiguities. But it has some obstacles to use these methods in real application. One reason is users in real world sometimes doesn't respond as clarification question expected like just reply ``I'm not sure"". Compared withing ask a clarification question, we directly list potential ambiguities as options.  proposed a query refinement method based on reinforcement learning, which helps to improve search results in search engine. Limited to the form of a dialogue system, it's not practical to show long list of potential results in dialogue. We aimed to interact with user by concise phrases to clarify user's question. % % A similar idea from  also suggests that, a conversational interface may be easier for users to clarify their needs given precise choices rather than expecting them to come up with particular terms.  % The complete question clarification process in our work is illustrated in Figure . Through real-world application experiments, our method has a lower rate on transferring to human agents and significant higher CTR  than other baselines. Our method also performs better than other baselines on the recall of potential FAQs on our annotated corpora.  %This paper focuses on closed-domain question clarification in dialogue, solving all kinds of ambiguous questions in one method.   {0pt} {0pt}     %    %% This part is comparison between related works.  % We investigated related works to clarify ambiguous questions in QA. The classic solution is to rank the most semantic similar questions [ranker ref] to the ambiguous questions. However, considering the limitation to display information in a dialogue based QA system, generally only the three results can be displayed, resulting in that this method cannot cover enough potential clear questions. In our experiments, we use the relevance ranker as the baseline for comparison. The results show that the human transferring rate of our method is much lower than the ranking method. The second method is to ask clarification questions . . However, the method of generative clarification question has some limitations in the real-word QA system. The biggest obstacle is that the user's answer space maybe to too open to answer, which complicates the dialogue. In addition, there is a lot of works to disambiguate questions through question refinement, but most refinement methods usually supplements information by a single key point, which not able to achieve all the key point recall we mentioned earlier.  % Question clarification is essential for a question answering system. In a real-world QA system, nearly 30\% of the user queries are ambiguous questions. Without clarification, dialogue participants risk missing information and ambiguous failing to achieve mutual understanding. The ability to ask clarification questions is one of the key desired components of conversational systems .  introduced methods to ask clarification questions for information that is missing from a given linguistic context.  use generative model to generate clarification questions for solving entity ambiguities.  % However, it is difficult to achieve a high success rate. For example, ``how to apply?"" is ambiguous, because there are too many products related to the ``apply"". By asking only one option question, such as ``Do you want to apply for a credit card?"" or two options question, such as ``Do you want to apply for a credit card or a loan ?"", which are both less efficient. Phenomena mentioned above exist in our real world customer service robot  system. CSRobot based on FAQ question answering is widely used in the real world, especially in the financial industry. When user enter a question in CSRobot system , information is retrieved by computing semantic similarity between user question and pre-manually prepared FAQ. Due to factors such as user's age, gender, geography, familiarity with our system, and urgency of user's problem, user may enter many ambiguous questions. In our CSRobot environment, the ratio is nearly 30\%. The ambiguous questions in our system can be summarized into 5 types:  Missing subject or object, e.g. ``how to apply"", ``how to change it back"",  Missing predicate, e.g. ``credit card"", ``my QR code"",  Missing of all subject predicates and objects,  e.g. ``How benefit"", ``its not right"",  Entity ambiguous,  e.g. ``My health insurance"", because health insurance contains many sub-categories,  Misspelling ambiguous. ``how to exist"" , ``exist"" may be misspelling of ``exit"". In this work, we focus on asking clarification questions using intents recommendation in FAQ-based question answering system. Previous methods either solve missing information questions or solve entity ambiguity questions, while our proposed method can handle both missing information and entity ambiguous mentioned above.   % The complete question clarification process in our work can be seen in Figure . The user enters an incomplete or ambiguous question, and agent recommends a list of candidate intents, each of which clarifies the user's question and can be clicked. Then user clicks on an intent associated with himself, and the agent finds a list of related FAQ in the FAQ knowledge base with the clarified question. Our work focuses on recommend a list of candidate intents for question clarification. A similar idea from  also suggests that, a conversational interface may be easier for users to clarify their needs given precise choices rather than expecting them to come up with particular terms.   % introduce question clarification as collection partition thought in detail   %   % One of the challenges in designing this method is how to design a cold start scenario. We use the end-to-end sequential intents recommendation method based on reinforcement learning for user question clarification. We did not use supervised method mainly because it is difficult for human annotators directly labeling intents related to user's ambiguous question . The reward is designed to recommend the closest clear question list and maximize the information gain after clicking one intent for better question clarification. We conducted offline and online experiments in a real-world CSRobot environment and collected the data of more than 100 million online real-users' interactions with our system in one month. To the best of our knowledge, we are the first to use intents recommendation for question clarification on real-world CSRobot environment, and interactions with more than 100 million of real users. The experiments proved the effectiveness and scalability of our proposed method. Contributions are summarized as follows:  %  %       %% FORMATTING  {NTCIR-13} [1]{{\mbox{#1}}} [1]{{}} {{\metricfont{Y!S1}}} {{\metricfont{GOV2}}} {\metric{RBP}} {\metric{P}} %{\metric{AP}} {\metric{MAP}} {\metric{NDCG}} {\metric{ERR}} {\metric{BPref}} {\metric{Qmeasure}}  [1]{\mbox{\Pat@}} [1]{\mbox{\RBP@}} [2]{\mbox{\RBP@}} [1]{\mbox{\NDCG@}} [1]{\mbox{\ERR@}} [1]{\mbox{\AP}} [1]{\mbox{\AP}} [1]{\mbox{\NDCG}} [1]{\mbox{\ERR}}  } {} {}  {\method{RRF}\xspace} %-- Baselines {\method{GBRT}} {\method{LSTM}} {\method{DQN}} {\method{DoDQN}} {\method{DoDDQN}} {\method{DDQN}} {\method{PER-DoDQN}} {\method{PER-DoDDQN}} {\method{PER}} {\method{MLP}}   %{\method{GBDT-BL}} %{\method{GBRT-BL}} %{\method{LambdaMART-BL}} %{\method{GBDT-Budget-BL}} %{\method{QL-BL}} % % %{\method{AdaRank-BL}} %{\method{WLM-BL}} % %%-- Experimental methods %{\method{LM-C3-Cost}} %{\method{LM-C3-CE}} %{\method{LM-C3-Rnd}} %{\method{GBDT-C3-Cost}} %{\method{GBDT-C3-CE}} %{\method{GBDT-C3-Rnd}} %{\method{GBRT-C3-Cost}} %{\method{GBRT-C3-CE}} %{\method{GBRT-C3-Rnd}} %{\method{LambdaMART-C3-Cost}} %{\method{LambdaMART-C3-CE}} %{\method{LambdaMART-C3-Rnd}} % %{\method{LM-C3-C}} %{\method{LM-C3-E}} %{\method{LM-C3-F}} %{\method{GBDT-C3-C}} %{\method{GBDT-C3-E}} %{\method{GBDT-C3-F}} %{\method{GBRT-C3-C}} %{\method{GBRT-C3-E}} %{\method{GBRT-C3-F}} %{\method{LambdaMART-C3-C}} %{\method{LambdaMART-C3-E}} %{\method{LambdaMART-C3-F}}  %-- Tools {} {}  %-- misc formatting \def\D{} \def\C{} %-- Misc control commands } }}} [1]{\makebox[15mm][l]{ \raggedright{#1.}\\[0.5ex]} [1]{\makebox[15mm][l]{ \raggedright{#1.}\\} %--- Ranking Stuff   {\var{Ans}} {\var{docweight}_{d}} } {\var{pivot}} }} {t_{\mbox{}}} {_{d,t})}}  {\rangle d,f_{d,t} \langle} {\mbox{}}\xspace} {\mbox{tfidf}\xspace} {}} {}} }\xspace} } } } }\xspace} {_d}} %--- Ops %% [1]{\mbox{	extsc{#1}}} [1]{\mbox{{#1}}}  {\opstyle{WAND}\xspace}  {\opstyle{MAXSCORE}\xspace}  {\opstyle{PST}\xspace} {\opstyle{Greedy-TAAT}\xspace} {\opstyle{TAAT}\xspace} {\opstyle{DAAT}\xspace} %--- Misc   {{}}} } {\mbox{\mbox{ {{{{{{{{} {} {} {} {\mbox{ {\opstyle{LMDS}} {\mbox{{_{\mbox{}}\xspace}} {\mbox{{\mbox{\method{NeWT}\xspace}} {\mbox{\method{NewSys}\xspace}}  {\method{Lynx\xspace}}  {\method{Terrier\xspace}} {{{{ {{  {R-Data}      {\mbox{}} {T_{\mbox{}}} {T_{\mbox{}}}   {\operatornamewithlimits{argmin}} {\operatornamewithlimits{argmax}} {\operatornamewithlimits{max}} {\operatornamewithlimits{lim}}   %-- Sizes    %-- maths }} {n_{\mbox{\tiny max}}} {{*$} {\makebox[\onedigit]{~}}    {1} [1]{{*** 	[\thetodocount]  ***\addtocounter{todocount}{1}}} % % File acl2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith   \documentclass[11pt]{article} \usepackage{coling2020} \usepackage{times} \usepackage{url} \usepackage{latexsym}  \renewcommand{\UrlFont}{\ttfamily % For formal tables \usepackage[normalem]{ulem} \usepackage{xcolor} %%xl: I need xcolour.... \usepackage{algorithm} \usepackage{algpseudocode} \usepackage{amsmath} \usepackage{mathrsfs}  \usepackage{amssymb} \usepackage{subfigure} \usepackage{makecell} \usepackage{mathtools} \usepackage[font=rm]{caption} % \usepackage{subcaption} \DeclareCaptionType{copyrightbox} \usepackage{shortvrb} \usepackage{tabularx} \usepackage{verbatim} \usepackage{xspace} \usepackage{listings} \lstset{basicstyle= \usepackage[multiple]{footmisc} \usepackage[all]{nowidow} \usepackage{balance} % This is not strictly necessary, and may be commented out, % but it will improve the layout of the manuscript, % and will typically save some space. \usepackage{microtype} \usepackage{wrapfig} % %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \usepackage[medium,compact]{titlesec} \usepackage{enumitem}     \author{       Xiang Hu\footnotemark[2]    \\ %       Ant Financial Services Group\footnotemark[2]\\    Hasso Plattner Institute, University of Potsdam\footnotemark[3]\\     \\\And    Zujie Wen\footnotemark[2] \\ %   Rutgers University\\    %       \\\And    Yafang Wang \footnotemark[2] \thanks{\ \  corresponding author, email: yafang.wyf@antfin.com}   \\ %   Ant Financial Services Group\\  %    %   \thanks{Corresponding author, Email: yafang.wyf@antfin.com}   \\\And    Xiaolong Li\footnotemark[2] \\ %   Rutgers University\\    %       \\\And   Gerard de Melo\footnotemark[3]  \\ %   Ant Financial Services Group\\  %    \\    tu   }  \date{}     %闁俺绻冮崣宥夋６閻ㄥ嫭鏌熷蹇旂窞濞撳懎褰查懗钘夌敨閺夈儲妫ゅ▔鏇㈩暕閺堢喓娈戦悽銊﹀煕閸欏秹顩敍灞芥躬閻喎鐤勭化鑽ょ埠娑擃厼绱╅崗銉ょ瑝绾喖鐣鹃幀褋淇uestion reformulation閺傝纭跺瀵伴弮鐘崇《娴兼媽顓搁幍閺堝缍旈崷銊ュ讲閼宠姤褋鍌氭礈濮濄倖鍨滄禒顑垮▏閻€劋绔寸粔宥勬唉娴滄帒绱￠惃鍕６妫版ɑ绶炲〒鍛煙濞夋洏 % \todo{explain defect of previous works} Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of human interaction, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a reinforcement model to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The model is trained using reinforcement learning with a deep policy network.  We evaluate our model based on real-world user clicks and demonstrate significant improvements across several different experiments. % The ability to ask clarification questions to solve ambiguity and missing information phenomena is essential for question answering systems. The current research mainly uses questions generation or questions ranking to ask a clarification question, which lead to low success rate and redundant information. Insufficient use of the graphic user interface  results in more interactions with users. There is usually no guarantee for replying the user after the clarification. To solve these problems, we propose a question clarification method based on intents recommendation. intents are extracted from the historical Frequently Asked Questions of our system. The recommended intents can provide more concise candidates for user to click. Once an intent is clicked, the system guaranteed to provide a clear question list relative to the real question. We use the reinforcement learning method to recommend intents, and the most challenging problem is cold start. The reward is designed to recommend the most relevant clear question list and maximize the information gain after clicking one intent for better question clarification. The method we proposed for question clarification can solve both ambiguity and missing information phenomena. Experiments on interactions with more than 100 million real-world online users shows the effectiveness of this method.            We present an end-to-end model to resolve ambiguous questions in dialogue by clarifying them using label suggestions. We cast the question clarification problem as a collection partition problem. In order to improve the quality of the interactive labels as well as reduce the semantic overlap of the labels and the user's question, we propose a novel reward based on recall of potential intents and information gain. We establish its effectiveness in a series of experiments, which suggest that this novel notion of clarification may as well be adopted for other kinds of disambiguation problems.  Our experiments shows that the way of intent interaction is more effective in solving user problems than returning relevant results. At the same time, through the comparison of online ctr, it fully proves that the intents recommend by the policy model trained via our new reward is more helpful to users.         
"," %闂侇偅淇虹换鍐矗瀹ュ锛栭柣銊ュ閺岀喎顕ｈ箛鏃傜獮婵炴挸鎳庤ぐ鏌ユ嚄閽樺鏁ㄩ柡澶堝劜濡倕鈻旈弴銏╂殨闁哄牏鍠撳▓鎴︽偨閵婏箑鐓曢柛娆忕Ч椤╊參鏁嶇仦鑺ヨ含闁活亞鍠庨悿鍕寲閼姐倗鍩犲☉鎿冨幖缁扁晠宕楅妷銈囩憹缁绢収鍠栭悾楣冨箑瑜嬫穱uestion reformulation闁哄倽顫夌涵璺侯嚗鐎典即寮悩宕囥婂ù鍏煎椤撴悂骞嶉柡鍫濐槹缂嶆棃宕烽妸銉ヨ闁煎疇濮よ閸屾碍绀堟慨婵勫栭崹婊勭椤戝灝鈻忛柣鈧妺缁斿绮斿鍕攭濞存粍甯掔槐锟犳儍閸曨垱锛栧Λ鐗埳戠欢鐐层掗崨顔界厵婵炲娲 % \todo{explain defect of previous works} Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of human interaction, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a reinforcement model to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The model is trained using reinforcement learning with a deep policy network.  We evaluate our model based on real-world user clicks and demonstrate significant improvements across several different experiments. % The ability to ask clarification questions to solve ambiguity and missing information phenomena is essential for question answering systems. The current research mainly uses questions generation or questions ranking to ask a clarification question, which lead to low success rate and redundant information. Insufficient use of the graphic user interface  results in more interactions with users. There is usually no guarantee for replying the user after the clarification. To solve these problems, we propose a question clarification method based on intents recommendation. intents are extracted from the historical Frequently Asked Questions of our system. The recommended intents can provide more concise candidates for user to click. Once an intent is clicked, the system guaranteed to provide a clear question list relative to the real question. We use the reinforcement learning method to recommend intents, and the most challenging problem is cold start. The reward is designed to recommend the most relevant clear question list and maximize the information gain after clicking one intent for better question clarification. The method we proposed for question clarification can solve both ambiguity and missing information phenomena. Experiments on interactions with more than 100 million real-world online users shows the effectiveness of this method.",426
" %Discourse Parsing is a key NLP %an important task, aiming to establish a better understanding of multi-sentential natural language. %, which is inherently ambiguous and intent-driven.  %Most research in the area thereby focuses on one of the two main discourse theories RST  or PDTB , both proposed over a decade ago. Discourse Parsing is a key Natural Language Processing  task for processing multi-sentential text. Most research in the area focuses on one of the two main discourse theories -- RST  or PDTB . The latter thereby postulates shallow discourse structures, combining adjacent sentences and mainly focuses on explicit and implicit discourse connectives. The RST discourse theory, on the other hand, proposes discourse trees over complete documents in a constituency-style manner, with tree leaves as so called Elementary Discourse Units , representing span-like sentence fragments. Internal tree-nodes encode discourse relations between sub-trees as a tuple of \{Nuclearity, Relation\}, where the nuclearity defines the sub-tree salience in the local context, and the relation further specifies the type of relationship between the binary child nodes   with automatically inferred discourse structures and nuclearity attributes from large-scale sentiment datasets already reached state-of-the-art  performance on the inter-domain discourse parsing task. Similarly,  infer latent discourse trees from the text classification task, and  employ the downstream task of summarization using a transformer model to generate discourse trees. Outside the area of discourse parsing, syntactic trees have previously been inferred according to several strategies, e.g. . %including: Discrete decisions frameworks using a Gumbel-softmax component , applying a reinforcement approach to syntactic parsing , using the reconstruction error of adjacent spans as an indicator for syntactic coherence within a sentence  or by employing a CKY approach to select syntactic trees from a soft model .  In general, the approaches mentioned above  %to automatically annotate text with discourse structures or syntactic trees  have shown to capture valuable structural information. Some models outperform baselines trained on human-annotated datasets , others have proven to enhance diverse downstream tasks . However, despite these initial successes, one critical limitation that all aforementioned models share is the task-specificity, possibly only capturing downstream-task related information. %of discourse,  This potentially compromises the generality of the resulting trees, as for instance shown for the model using text classification data  in .  %For instance, the approach by  uses document-level sentiment information to inform the discourse tree generation, with others %have been  %using summarization data  or sentence-level sentiment cues  to achieve the results.  In order to alleviate this limitation of task-specificity, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending the latent tree induction framework proposed by  with an auto-encoding objective. %.  Our system thereby extracts important knowledge from natural text by optimizing both the underlying tree structures and the distributed representations. We believe that the resulting discourse structures effectively aggregate related and commonly appearing patterns in the data by merging coherent text spans into intermediate sub-tree encodings, similar to the intuition presented in . However, in contrast to the approach by , our model makes discrete structural decisions, rather than joining possible subtrees using a soft attention mechanism. We believe that our discrete tree structures allow the model to more efficiently achieve the autoencoder objective in reconstructing the inputs, directly learning how written language can be aggregated in the wild . In general, the proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and further problems outside of NLP, like tree-planning  and decision-tree generation . Yet, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to %complement task-specific models in  generate much larger and more diverse discourse treebanks.      In this paper, we proposed a truly unsupervised and purely data-driven tree-style autoencoder to compress and reconstruct textual data. We show the potential of our T-AE approach on the task of discourse parsing, which severely suffers from training-data sparsity, due to the tedious and expensive annotation process. Our unsupervised model outperforms one of the commonly used, linguistically supervised approaches, without making any assumptions on the underlying data, except the sentence/document split. The superior performance compared to the hierarchical left branching baseline plausibly indicates that our unsupervised structures could be valuable when combined with supervised or distantly supervised models to further improve their joint performance. Furthermore, the superior performance of the large out-of-domain model trained on the Yelp'13 dataset over the small-scale within-domain model trained on the raw text of the RST-DT dataset shows the synergies between these corpora as well as strong potential for even larger datasets to enhance the performance of the approach.   In the future, we intend to extend this work in several ways: First, we want to explore the application of generative models, employing a variational autoencoder. Second, we plan to study further tasks besides predicting discourse, such as syntactic parsing, as well as additional synergistic downstream tasks . To improve our model on important downstream tasks , we want to explore a pre-training/fine-tuning approach, similar to contextualized language  models, such as BERT. Combining our novel approach with distantly-supervised and supervised models is another future direction we want to explore. Lastly, we plan to evaluate additional model adaptions, such as two independent models on sentence- and document-level, incorporating a BERT EDU encoder and an end-to-end model with soft-constraints on sentence-level.  Try generative models \\  Try syntactic parsing \\  Try more downstream tasks \\  separate models\\  soft constraint model\\  add bert\\  
"," Discourse information, as postulated by popular discourse theories, such as RST and PDTB, has been shown to improve an increasing number of downstream NLP tasks, showing positive effects and synergies of discourse with important real-world applications. While methods for incorporating discourse become more and more sophisticated, the growing need for robust and general discourse structures has not been sufficiently met by current discourse parsers, usually trained on small scale datasets in a strictly limited number of domains. This makes the prediction for arbitrary tasks noisy and unreliable. The overall resulting lack of high-quality, high-quantity discourse trees poses a severe limitation to further progress.  In order the alleviate this shortcoming, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to generate larger and more diverse discourse treebanks. In this paper we are inferring general tree structures of natural text in multiple domains, showing promising results on a diverse set of tasks.  %With this paper, we intend to initiate a new line of research on inferring discourse structures in an unbiased manner. %With a growing need for robust and general discourse structures in many downstream tasks and real-world applications, the current lack of high-quality, high-quantity discourse trees poses a severe shortcoming. %In order the alleviate this limitation, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop such method to complement task-specific models in generating much larger and more diverse discourse treebanks.",427
"  Retrieval technique or response selection is a very popular and elegant approach  to framing a chatbot i.e. open-domain dialog system. Given the conversation context, a retrieval-based chatbot aims to select the most appropriate utterance as a response from a pre-constructed database. %that saves a large number of human written utterances. In order to balance the effectiveness and efficiency, mosts of the retrieval-based chatbots  employ coarse-grained selection module to recall a set of candidate  that are semantic coherent with the conversation context to speed up processing.  % 鐠囧瓨妲戦敍姘妧娑斿孩鏅ラ悳鍥ф嫲閺佸牊鐏夐獮鏈电瑝閼宠棄鍙忛柈銊ュ絿瀵 % 閸滃elated work闁插矂娼伴柌宥咁槻娴滃棴绱濋惄瀛樺复閸掔姴骞撻敍 To the best of our knowledge, there are two kinds of approaches  to build a coarse-grained selection module in retrieval-based chatbots:  sparse representation:  TF-IDF or BM25  is a widely used method. It matches keywords with an inverted index and can be seen as representing utterances in highdimensional sparse vectors ; %This method runs very quickly, but lacks rich semantic information.  dense representation:  Large scale pre-trained langauge models , e.g. BERT  are commonly used to obtain the semantic representation of utterances, which could be used to recall semantic coherent candidates by using cosine similarity . %Due to the high computational burden of similarity calculating, %this method runs slowly, but could consider rich semantic information %.  % 鐠囧瓨妲戦惄顔煎楠炶埖妫ょ化鑽ょ埠娑擃亜顕В鏃撶礉鐠囧瓨妲戠紒鍡氬Ν閸欘垯浜掗崷銊ョ杽妤犲奔鑵戦幍鎯у煂閿涘矂妲撻弰搴＄杽妤犲瞼绮ㄩ弸婊冩嫲dense vectors閻ㄥ墜eakness閿涘瞼鍔ч崥搴＄穿閸戠儤鍨滄禒顒傛畱閸欙缚绔存稉鐚祌oposed method % Luan2020SparseDA鏉╂瑤閲滅拋鐑樻瀮娑旂喕顕╅弰搴濈啊BM25閺堝妞傞崐娆愭櫏閺嬫粍娲挎總 % Dense 閺鐟版倳 BERT So far, there is no systematic comparison between these two kinds of approaches in retrieval-based chatbots, and which kind of method is most appropriate in real scenarios is  still an open question that confuses researchers in dialog system community. Thus, in this paper, we first conduct extensive experiment to compare these two approaches from four important aspects:   effectiveness;  search time cost;  index storage occupation;  human evaluation. Extensive experiment results on four popular response selection datasets  demonstrate that the dense representation  significantly outperforms the sparse representation at the expense of  the lower speed and bigger storage than sparse representation, which is unsufferable in real scenarios. Then, in order to overcome the fatal weaknesses of dense representation methods, we propose an ultra-fast, low-storage and highly effective  Deep Semantic Hashing Coarse-grained selection module  %based on a given dense representation method, which effectively balances the effectiveness and efficiency. Specifically,  we first stack a novel hashing optimizing module that consists of two autoencoders on a given  dense representation method. Then, three well designed loss functions are used to optimize  these two autoencoders in hashing optimizing module:  preserved loss;  hash loss;  quantization loss. After training, the autoencoders could effectively preserve rich semantic and similarity information of the dense vectors into the hash codes, which are very computational and storage efficient .      The rest of this paper is organized as follows: we introduce the important concepts and background covered in our paper in Section 2. The experiment settings is presented in Section 3. In Section 4, we systematically compare the current two kinds of methods in coarse-grained selection module:   sparse representation;  dense representation. In Section 5, we introduce our proposed DSHC model, and detailed experiment results are elaborated. In Section 6, we conduct the case study. Finally, we conclude our work in Section 7. Due to the page limitation, more details and extra analysis can be found in Appendix.     閹存垳婊戦惃鍕煙濞夋洑绱扮 REALM 缁涘顣╃拋顓犵矊閹绘劒绶电敮顔煎И In this paper, we first systematically compare the dense and sparse representation method in retrieval-based chatbot from four important aspects:  effectiveness;  search time cost;  index stoarge;  human evaluation. Extensive experiment results demonstrate that dense representation method could achieve better performance  at the expense of more time cost and higher storage occupation, In order to overcome these fatal weaknesses, we propose a deep semantic hashing based corase-grained  selection method. Extensive experiment results prove the effectiveness and the efficiency of DSHC model.       
","   We study the coarse-grained selection module in retrieval-based chatbot.   Coarse-grained selection is a basic module in a retrieval-based chatbot,   which constructs a rough candidate set from the whole database to speed up the interaction with customers.   So far, there are two kinds of approaches for coarse-grained selection module:     sparse representation;  dense representation.   To the best of our knowledge, there is no systematic comparison between these two approaches in retrieval-based chatbots,   and which kind of method is better in real scenarios is still an open question.   In this paper, we first systematically compare these two methods from four aspects:     effectiveness;  index stoarge;  search time cost;  human evaluation.   Extensive experiment results demonstrate that dense representation method    significantly outperforms the sparse representation,    but costs more time and storage occupation.   In order to overcome these fatal weaknesses of dense representation method,    we propose an ultra-fast, low-storage, and highly effective    Deep Semantic Hashing Coarse-grained selection method, called DSHC model.   Specifically, in our proposed DSHC model,   a hashing optimizing module that consists of two autoencoder models is    stacked on a trained dense representation model,   and three loss functions are designed to optimize it.   The hash codes provided by hashing optimizing module effectively    preserve the rich semantic and similarity information in dense vectors.   Extensive experiment results prove that,   our proposed DSHC model can achieve much faster speed and lower storage than sparse representation,   with limited performance loss compared with dense representation.   Besides, our source codes have been publicly released for future research\footnote{\url{https://github.com/gmftbyGMFTBY/HashRetrieval}}.",428
"  With huge quantities of natural language documents, search engines have been essential for the time saved on information retrieval tasks. Usually, deployed search engines achieve the task of ranking documents by relevance according to a query. \\ Recently, research has focused on the task of extracting the span of text that exactly matches the user's query through Machine  Reading Comprehension and Question Answering. \\ Question Answering deals with the extraction of the span of text in a short paragraph that exactly answers a natural language question. Recent deep learning models based on heavy pretrained language models like BERT achieved better than human performances on this tasks .  \\ One could try to apply QA models for the Open-Domain Question Answering paradigm which aims to answer questions taking a big amount of documents as knowledge source. Two main issues emerge from this : first, applying 100M parameters language models to potentially millions of documents requires unreasonable GPU-resources. Then, QA models allow to compare spans of text coming exclusively from a single paragraph while in the open-domain QA paradigm, one needs to compare spans of text coming from a wide range of documents. \\ Our system, as done in previous work, deals with the resources issue thanks to a Retriever module, based on the BM25 algorithm, that allows to reduce the search space from millions of articles to a hundred of paragraphs. The second issue is tackled by adding a deep learning based Scorer module that re-ranks with more precision the paragraphs returned by the Retriever. Eventually, the Extractor module uses a QA deep learning model to extract the best span of text in the first paragraph returned by the Scorer. To avoid a heavy and hardly scalable pipeline consisting of two huge deep learning models, we parallelize the re-ranking and span extraction tasks thanks to multitask learning : while maintaining high performances, it allows to significantly reduce both memory requirements and inference time. Our system achieve state-of-the-art results on the open-squad benchmark.     We introduced MIX, a multi-task learning approach to solve  open-domain question answering, relying  on the  BM25  algorithm as a Retriever to reduce the search space and the powerful RoBERTa language model finetuned to achieve both paragraph re-ranking  and spans of text extraction . Our system achieves state-of-the-art results on the squad-open benchmark. Thus, we showed that a simple pipeline consisting of a retriever, a re-ranker and a reader is sufficient to overcome the issue of comparing 2 possible answers from different paragraphs. The novelty of our paper comes from the usage of multi-task learning to allow performing simultaneous re-ranking and reading comprehension. Our system is particularly adapted to a search engine setting where several results ranked by relevance are provided to the user. Our evaluation shows that the results are more limited by the performance of the Scorer than by the Retriever that reduces the search space of million of paragraphs to a hundred so future research might be directed to the improvement of the Scorer performances. Furthermore, we confirm the finding of , according to which shorter paragraphs allow better performances on the open-domain QA task. Eventually, further improvements might probably be achieved by using heavier language model such as RoBERTa-large.   
","   In this paper, we introduce MIX : a   multi-task deep learning approach to solve Open-Domain Question  Answering. First, we design our system as a multi-stage pipeline made of 3 building blocks : a BM25-based Retriever, to reduce the search space; RoBERTa based Scorer and Extractor, to rank retrieved paragraphs and extract relevant spans of text respectively. Eventually, we further improve computational efficiency of our system to deal with the scalability challenge : thanks to multi-task learning,   we parallelize the close tasks solved by the Scorer and the Extractor. Our system is on par with state-of-the-art performances on the squad-open benchmark while being simpler conceptually.",429
"  Named Entity Recognition  is the task of identifying the span and the class of a Named Entity  in unstructured text. NEs typically include but are not limited to persons, companies, dates, and geographical locations .   Legal NER is a central task in language processing of legal documents, especially for extracting key information such as the name of the parties in a case, the court name or the case number, or references to laws or judgements, to name a few. The extracted NEs could be integrated in legal research workflows for functionalities such as search, document anonymization or case summarization  thereby enabling and expediting insights for legal professionals .  NER is commonly formalized as a sequence labeling task: each token of the document is assigned a single label that indicates whether the token belongs to an entity from a predefined set of categories . To create a training dataset in such a format the annotator is required to manually label each token in a sentence with the respective category. In this format, both the NE and the location of the NE in the source text are known. This format of training data is what we refer to hereafter as 閳ユ笀old standard閳 data. Obtaining the required voluminous gold standard data to train such models is, therefore, a laborious and costly task.    In this paper, we perform NER in filed lawsuits in US courts. Specifically, we aim to identify the party names in each case, i.e. the names of the plaintiffs and the defendants, in a large collection of publicly available cases from more than 200 courts in different US jurisdictions. The party names have been identified by legal annotators but their exact location in the text is unknown. In this respect, we do not have access to 閳ユ笀old standard閳 training data even though the target NEs are available. This feature of our dataset introduces a key difference of our task to most NER tasks.  One solution to this problem is to generate the 閳ユ笀old standard閳 training data by searching for the locations of the known NEs in the source text . By performing this additional transformation to our data, we would be able to train sequence labeling NER models. For the following reasons, this solution is nontrivial. First, as our source text is also extracted from scanned PDF files , it contains Optical Character Recognition  mistakes and/or typos which may not be present in the target NEs. Second, besides the potential OCR errors at the character level, the closely spaced, two-column page layouts that can be often found as headers in the filed cases, represent an additional challenge for the OCR, which tends to concatenate the text across columns . In such cases, the tokens that make up the NEs in the source text may be intertwined with other words and/or sentences. Third, variations of the names may be also present in the source text and in our human-generated labels, such as presence of first and/or middle names whole or as initials and, to a lesser extent, typos.     [h]     To address some of the challenges imposed by the format of our training data and inspired by the work in the field of abstractive summarization, we propose to reformulate the NER task, not as a sequence labeling problem, but as a text-to-text sequence generation problem with the use of a pointer generator network . With this reformulation, in contrast to sequence labeling, we do not require knowledge of the NE閳ユ獨 locations in the text as training labels. A recent study by  proposed a different formulation of the NER task as a question answering task and achieved state-of-the-art performance in a number of published NER datasets . In this study, we adopt a hybrid extractive-abstractive architecture, based on recurrent neural networks coupled with global  attention and copying  attention  mechanisms . The proposed architecture can be successfully used for abstractive summarization since it can copy words from the source text via pointing and can deal effectively with out-of-vocabulary  words 閳 words that have not been seen during training. Our approach is conceptually simple but empirically powerful and we show that the pointer generator outperforms the typical NER architectures in the case of noisy and lengthy inputs where the NE's location in the text is not known.   In addition, we examine how our approach can be used for the related NER task of case number extraction. The case number is a unique combination of letters, numbers and special characters as a single token and are, therefore, particularly challenging for NER models as they are often dealt with as OOV words by the model. As in the party names task discussed above, in the case number task we do not have 閳ユ笀old standard閳 labels of the case number閳ユ獨 location in the text. We show that a character level sequence generation network can dramatically increase our ability to extract case numbers from the source text, compared to a word level sequence generation network.  The rest of the paper is organized as follows. In Section 2, we discuss related work in the field of NER in the legal domain. In Section 3, we describe our proposal of NER as a text-to-text sequence generation task in the absence of gold standard data and formulate the task in two ways:  as a combination of automatically labeling the NE's location and then using the conventional sequence labeling method for NER, and  as a text-to-text sequence generation task where the NEs are directly generated as text. Section 4 presents our experimental design, results and analysis. Section 5 presents the case number case study. Finally, we conclude and discuss directions for future work.     This work presents a simple yet powerful reformulation of the NER task as a text-to-text sequence generation task by applying a pointer generator network, a model architecture that have been predominantly used in the NLP field of summarization. There are several key advantages in the proposed formalization:   there is no need to acquire 閳ユ笀old閳 data for the NER task when only the target NEs are known but not their indices in the source text,  the Pointer Generator network outperforms popular sequence-labeling architectures at the NER task in the case of longer text inputs, and  the Pointer Generator is able to accurately generate NEs that are corrupted due to OCR errors in extracting the two-column formatted text. In the future, we would like to explore the capacity of the Pointer Generator to extract additional types of NEs.          
"," Named Entity Recognition  is the task of identifying and classifying named entities in unstructured text. In the legal domain, named entities of interest may include the case parties, judges, names of courts, case numbers, references to laws etc. We study the problem of legal NER with noisy text extracted from PDF files of filed court cases from US courts. The 闁炽儲绗old standard闁 training data for NER systems provide annotation for each token of the text with the corresponding entity or non-entity label. We work with only partially complete training data, which differ from the gold standard NER data in that the exact location of the entities in the text is unknown and the entities may contain typos and/or OCR mistakes. To overcome the challenges of our noisy training data, e.g. text extraction errors and/or typos and unknown label indices, we formulate the NER task as a text-to-text sequence generation task and train a pointer generator network to generate the entities in the document rather than label them. We show that the pointer generator can be effective for NER in the absence of gold standard data and outperforms the common NER neural network architectures in long legal documents.",430
" % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  %.     %      % % final paper: en-us version      % %       % space normally used by the marker %     This work is licensed under a Creative Commons  %     Attribution 4.0 International License. %     License details: %     \url{http://creativecommons.org/licenses/by/4.0/}. %}  Relation extraction aims to extract relations between entities in text, where distant supervision proposed by automatically establishes training datasets by assigning relation labels to instances that mention entities within knowledge bases. However, the wrong labeling problem can occur and various multi-instance learning methods have been proposed to address it. Despite the wrong labeling problem, each instance in distant supervision is crawled from web pages, which is informal with many noisy words and can express multiple similar relations. This problem is not well-handled by previous approaches and severely hampers the performance of conventional neural relation extractors. To handle this problem, we have to address two challenges:  Identifying and gathering spotted relation information from low-quality instances;  Distinguishing multiple overlapped relation features from each instance.    First, a few significant relation words are distributed dispersedly in the sentence, as shown in Figure, where words marked in red brackets represent entities, and italic words are key to expressing the relations. For instance, the clause ``evan\_bayh son of birch\_bayh"" in S1 is sufficient to express the relation /people/person/children of evan\_bayh and birch\_bayh. Salient relation words are few in number and dispersedly in S1, while others excluded from the clause can be regarded as noise. Traditional neural models have difficulty gathering spotted relation features at different positions along the sequence because they use Convolutional Neural Network  or Recurrent Neural Network  as basic relation encoders, which model each sequence word by word and lose rich non-local information for modeling the dependencies of semantic salience. Thus, a well-behaved relation extractor is needed to extract scattered relation features from informal instances.  Second, each instance can express multiple similar relations of two entities. As shown in Figure, Changsha and Hunan possess the relations /location/location/contains and /location/province/capital in S2, which have similar semantics, introducing great challenges for neural extractors in discriminating them clearly. Conventional neural methods are not effective at extracting overlapped relation features, because they mix different relation semantics into a single vector by max-pooling or self-attention. Although  first propose an attentive capsule network for multi-labeled relation extraction, it treats the CNN/RNN as low-level capsules without the diversity encouragement, which poses the difficulty of distinguishing different and overlapped relation features from a single type of semantic capsule. Therefore, a well-behaved relation extractor is needed to discriminate diverse overlapped relation features from different semantic spaces.  To address the above problem, we propose a novel Regularized Attentive Capsule Network  to identify highly overlapped relations in the low-quality distant supervision corpus. First, we propose to embed multi-head attention into the capsule network, where attention vectors from each head are encapsulated as a low-level capsule, discovering relation features in an unique semantic space. Then, to improve multi-head attention in extracting spotted relation features, we devise relation query multi-head attention, which selects salient relation words regardless of their positions. This mechanism assigns proper attention scores to salient relation words by calculating the logit similarity of each relation representation and word representation. Furthermore, we apply disagreement regularization to multi-head attention and low-level capsules, which encourages each head or capsule to discriminate different relation features from different semantic spaces. Finally, the dynamic routing algorithm and sliding-margin loss are employed to gather diverse relation features and predict multiple specific relations. We evaluate RA-CapNet using two benchmarks. The experimental results show that our model achieves satisfactory performance over the baselines. Our contributions are summarized as follows:       {0pt}     {0pt}          In this paper, we propose a novel regularized attentive capsule network for overlapped relation extraction. RA-CapNet embeds relation query multi-head attention into the capsule network and uses a novel disagreement regularization term to encourage the diversity among heads and capsules, making it capable of gathering salient information from diverse semantic spaces. Our model is resistant to the noise of distant supervision and achieves significant improvements on both standard and complex datasets.  In the future, we will experiment with different forms of regularization terms and their application to other components of our model.  
","   Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network  to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our model achieves significant improvements in relation extraction.",431
" 	Identifying the user's open intent plays a significant role in dialogue systems. As shown in Figure, we have two known intents for specific purposes, such as book flight and restaurant reservation. However, there are also utterances with irrelevant or unsupported intents that our system cannot handle. It is necessary to distinguish these utterances from the known intents as much as possible. On the one hand, effectively identifying the open intent can improve customer satisfaction by reducing false-positive error. On the other hand, we can use the open intent to discover potential user needs. 	 	We regard open intent classification as an -class classification task as suggested in, and group open classes into the  class . Our goal is to classify the n-class known intents into their corresponding classes correctly while identifying the  class open intent. To solve this problem,~ propose the concept of open space risk as the measure of open classification.~ reduce the open space risk by learning the closed boundary of each positive class in the similarity space. However, they fail to capture high-level semantic concepts with SVM.  	~ manage to reduce the open space risk through deep neural networks , but need to sample open classes for selecting the core hyperparameters.~ use the softmax probability as the confidence score, but also need to select the confidence threshold with negative samples.~ replace softmax with the sigmoid activation function, and calculate the confidence thresholds of each class based on statistics. However, the statistics-based thresholds can not learn the essential differences between known classes and the open class.~ propose to learn the deep intent features with the margin loss and detect unknown intents with local outlier factor. However, it has no specific decision boundaries for distinguishing the open intent, and needs model architecture modification.  	 	Most of the existing methods need to design specific classifiers for identifying the open class and perform poorly with the common classifier. Moreover, the performance of open classification largely depends on the  decision conditions. Most of these methods need negative samples for determining the suitable decision conditions. It is also a complicated and time-consuming process to manually select the optimal decision condition, which is not applicable in real scenarios.  	 	To solve these problems, we use known intents as prior knowledge, and propose a novel post-processing method to learn the adaptive decision boundary  for open intent classification. As illustrated in Figure, we first extract intent representations from the BERT model. Then, we pre-train the model under the supervision of the softmax loss. We define centroids for each known class and suppose known intent features are constrained in the closed ball areas. Next, we aim to learn the radius of each ball area to obtain the decision boundaries. Specifically, we initialize the boundary parameters with standard normal distribution and use a learnable activation function as a projection to get the radius of each decision boundary.  	 	The suitable decision boundaries should satisfy two conditions. On the one hand, they should be broad enough to surround in-domain samples as much as possible. On the other hand, they need to be tight enough to prevent out-of-domain samples from being identified as in-domain samples. To address these issues, we propose a new loss function, which optimizes the boundary parameters by balancing both the open space risk and the empirical risk. The decision boundaries can automatically learn to adapt to the intent feature space until balance with the boundary loss. We find that our post-processing method can still learn discriminative decision boundaries to detect the open intent even without modifying the original model architecture. 	 	We summarize our contribution as follows. Firstly, we propose a novel post-processing method for open classification, with no need for prior knowledge of the open class. Secondly,  we propose a new loss function to automatically learn tight decision boundaries adaptive to the feature space. To the best of our knowledge, this is the first attempt to adopt deep neural networks to learn the adaptive decision boundary for open classification. Thirdly, extensive experiments conducted on three challenging datasets show that our approach obtains consistently better and more robust results compared with the state-of-the-art methods.  	[t!]} 			\toprule 			Dataset & Classes & \#Training & \#Validation & \#Test & Vocabulary Size & Length  \\ 			\midrule 			BANKING & 77 & 9,003 & 1,000 & 3,080 & 5,028 & 79 / 11.91\\ 			OOS & 150 & 15,000 & 3,000 & 5,700 & 8,376 & 28 / 8.31 \\ 			StackOverflow & 20 & 12,000 & 2,000 & 6,000 & 17,182 & 41 / 9.18 \\ 			 		 	 	  	In this paper, we propose a novel post-processing method for open intent classification. After pre-training the model with labeled samples, our model can learn specific and tight decision boundaries adaptive to the known intent feature space. Our method has no require for open intent or model architecture modification. Extensive experiments on three benchmark datasets show that our method yields significant improvements over the compared baselines and is more robust with less labeled data and fewer known intents. 	 	
"," 		Open intent classification is a challenging task in dialogue systems. On the one hand, we should ensure the classification quality of known intents. On the other hand, we need to identify the open  intent during testing. Current models are limited in finding the appropriate decision boundary to balance the performances of both known and open intents. In this paper, we propose a post-processing method to learn the adaptive decision boundary  for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we use the well-trained features to automatically learn the adaptive spherical decision boundaries for each known intent. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open samples and is free from modifying the model architecture. We find our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods.\footnote{Code: https://github.com/thuiar/Adaptive-Decision-Boundary}",432
"    Neural Machine Translation   yields  state-of-the-art translation performance when a large number of parallel sentences are available. However, only a few parallel corpora are available for the majority of language pairs and domains. It has been known that NMT does not perform well in the specific domains where the domain-specific corpora are limited, such as medical domain. As such, high-quality domain-specific machine translation  systems are in high demand whereas general purpose MT has limited applications.  There are many studies of domain adaptation for NMT, which can be mainly divided into two categories: data-centric and model fine-tuning.  Data-centric methods focus on  selecting or generating target domain data from general domain corpora, which is effective and well explored.  In this paper, we focus on the second approach. Fine-tuning is  very common in domain adaptation, which first trains a base model on the general domain data and then fine-tunes it on each target domain . However, unconstrained or full fine-tuning  requires very careful hyper-parameter tuning,  and is prone to over-fitting on the target domain as well as forgetting on the general domain. To tackle these problems, researchers have proposed several constructive approaches, with the view to limiting the size or plasticity of parameters in the fine-tuning stage, which can be roughly divided into two categories: regularization and partial-tuning strategy. Regularization methods often integrate extra training objectives to prevent parameters from large deviations, such as model output regularization , elastic weight consolidation  . Regularization methods, which impose arbitrary global constraints on parameter updates, may further restrict the adaptive process of the network, especially when domain-specific corpora are scarce. Partial-tuning methods either freeze several sub-layers of the network and fine-tune the others, or integrate domain-specific adapters into the network. By only fine-tuning the domain-specific part of the model, they can alleviate the over-fitting and forgetting problem in fine-tuning. However, the structure designed to adapting is usually hand-crafted, which relies on experienced experts and the adapter brings additional parameters. Therefore,  a more adaptive, scalable, and parameter-efficient approach for domain adaptation is very valuable and worth well studying.  [!ht]                          In this paper, we propose \method, a novel domain adaptation method via adaptive structure pruning. Our motivation is inspired from Continual Learning  and  that a randomly-initialized, dense neural network contains a sub-network which  can match the test accuracy of the original network after training for at most the same number of iterations.   We therefore suppose that multiple  machine translation models for different domains can share different sparse subnetworks within a single neural network.   Specifically, we first apply a standard pruning technique to automatically uncover the subnetwork from a well-trained NMT model in the general domain.  The  subnetwork is capable of  reducing the parameter without compromising accuracy. Therefore, it has the potential to keep as much general information as possible.   Then we freeze this informative sparse network and leave the unnecessary  parameters unfixed for the target  domain, which enables our approach to be parameter efficient, and eases the scalability of the approach to more domains.  The capacity of these non-fixed parameters can be tuned to match the requirements of the target domain, while keeping the parameters of the general domain. Our method successfully circumvents catastrophic forgetting problem and retains the quality on the general domain.  As the benefits of the flexible design, \method can be easily extended to other transfer learning problems, such as multilingual machine translation.      We summarize our main contribution as follows:       problems in domain adaptation.     , \method outperforms several strong competitors including Fine-tuning, EWC, Model Distillation, Layer Freeze and Adapter in target domain test set without the loss of general domain performance.      and ZhEn, which shows the possibilities of training a single model to serve different domains without performance degradation.              % --------------------Background--------------------   In this work, we propose \method, an effective way for adapting neural machine translation models which first generates an informative subnetwork for the general domain via gradual pruning and then fine-tunes the unnecessary parameters for the target domain. By doing so, \method is able to retain as much general information as possible and alleviate the catastrophic forgetting problems. Experiments show that the proposed \method outperforms fine-tuning and several strong baselines and it is shown to be much more robust compared to fine-tuning due to the complete retainment of the general information. Beyond that, \method can be extended to adapting multiple domains by iteratively pruning and tuning, which is naturally suitable for multi-lingual scenario. We leave the multi-lingual problem as our future work.    --------------------Acknowledgements-------------------- 
"," Fine-tuning is a major approach for domain adaptation in Neural Machine Translation .  However, unconstrained fine-tuning requires very careful hyper-parameter tuning otherwise it is easy to fall into over-fitting on the target domain and degradation on the general domain.  To mitigate it, we propose \method, a novel domain adaptation method via gradual pruning.  It learns tiny domain-specific subnetworks for tuning. During adaptation to a new domain, we only tune its corresponding subnetwork.  \method alleviates the over-fitting and the degradation problem without model modification. Additionally, with no overlapping between domain-specific subnetworks, \method is also capable of sequential multi-domain learning.    Empirical experiment results show that \method outperforms several strong competitors in the target domain test set without the quality degradation of the general domain in both single and multiple domain settings. \footnote{The source code and data are available at \url{https://github.com/ohlionel/Prune-Tune}}",433
" As an important task in a dialogue system, response selection aims to find the best matched response from a set of candidates given the context of a conversation. The retrieved responses usually have natural, fluent and diverse expressions with rich information owing to the abundant resources. Therefore, response selection has been widely used in industry and has attracted great attention in academia.  Most existing studies on this task pay more attention to the matching problem between utterances and responses, but with insufficient concern for the reasoning issue in multi-turn response selection. Just recently, MuTual, the first human-labeled reasoning-based dataset for multi-turn dialogue, has been released to promote this line of research. Reasoning is quite different from matching in the conversations. Specifically, matching focuses on capturing the relevance features between utterances and responses, while reasoning not only needs to identify key features , but also needs to conduct inference based on these clue words. The challenges of this new task include:  how to identify the clue words in utterances, which is fundamental for inference;  how to conduct inference according to the clue words in utterances. Figure illustrates a motivating example. To infer the current time, we must first identify the clue words `10:45' in  and `15 minutes' in . Then we must conduct a logical inference based on these clue words in  and .    To tackle these challenges, first, we need better contextual representation for identifying the clue words in conversations. This is because clue word identification inevitably relies on the context of a conversation. Although previous literature publications have achieved promising results in context modeling, there are still several limitations of these approaches. More concretely, the existing studies either concatenate the utterances to form context or process each utterance independently, leading to the loss of dependency relationships among utterances or important contextual information. It has been validated that the chronological dependency between utterances, as well as the semantical dependency between utterances, are crucial for multi-turn response selection. Thus, how to model the dependencies in utterances remains a challenging problem for context representation.  Second, we need to devise a new strategy to collect the clue words scattered in multiple utterances and need to reason according to these clue words. In recent years, we have witnessed great success in KBQA  and MRC  tasks. However, new obstacles emerge for transferring current reasoning approaches in KBQA and MRC to conversational reasoning.  A clear reasoning path based on entities in a well-structured knowledge base exists in KBQA, but there is no similar reasoning path in utterances.  Current approaches on MRC conduct inference based on graph while taking shared entities as nodes, while it is difficult to construct such graphs based on entities in short utterances, which usually suffer from greater coreference resolution, poor content and serious semantic omission problems in comparison with document text.  In this paper, we propose a new model named GRN  which can tackle both challenges in an end-to-end way. We first introduce two pre-training tasks called NUP  and UOP  which are specially designed for response selection. NUP endows GRN with context-aware ability for semantical dependency, and UOP facilitates GRN with the ability to capture the chronological dependency. These customized pre-training methods are beneficial for modeling dependencies contained in utterances to achieve better context representation. We perform task-adaptive pre-training with the combined NUP and UOP tasks based on the ALBERT model. To conduct reasoning based on clue words, we devise a graph neural network called UDG , which not only models the dependencies between utterances with each utterance as a node but also collects the clue words from different utterances. Reasoning is achieved by propagating the messages of clue words between nodes along various utterance paths on UDG, and this graph reasoning structure realizes the inference based on an utterance-level context vector with local perspective. On the other hand, we also implement a reasoning network by the output of the trained model and self-attention mechanism. This sequence reasoning structure realizes the inference based on the highly summarized context vector with global perspective. To summarize, we make the following contributions:       ^{plus}$ datasets.     In this paper, we propose a new architecture for multi-turn response reasoning. Concretely, we first propose NUP and UOP pre-training tasks for response selection. We design the UDG of utterance for reasoning. We introduce sequence and graph reasoning structure jointly, where the sequence reasoning module can capture the key information from the global perspective and the graph reasoning module is responsible for capturing the clue words information from the local perspective. The experiment results on MuTual and  achieve a new heights. There is still expansive room for improvement in performance on . In future work, we will further investigate how to balance safe response and meaningful candidate response. 
"," We investigate response selection for multi-turn conversation in retrieval-based chatbots. Existing studies pay more attention to the matching between utterances and responses by calculating the matching score based on learned features, leading to insufficient model reasoning ability. In this paper, we propose a graph reasoning network  to address the problem. GRN first conducts pre-training based on ALBERT using next utterance prediction and utterance order prediction tasks specifically devised for response selection. These two customized pre-training tasks can endow our model with the ability of capturing semantical and chronological dependency between utterances. We then fine-tune the model on an integrated network with sequence reasoning and graph reasoning structures. The sequence reasoning module conducts inference based on the highly summarized context vector of utterance-response pairs from the global perspective. The graph reasoning module conducts the reasoning on the utterance-level graph neural network from the local perspective. Experiments on two conversational reasoning datasets show that our model can dramatically outperform the strong baseline methods and can achieve performance which is close to human-level.",434
" As a fundamental task in natural language processing , coherence analysis can benefit various downstream tasks, such as sentiment analysis  and document summarization . Rhetorical Structure Theory   is one of the most influential theories of text coherence, under which a document is represented by a hierarchical discourse tree, which consists of a set of semantic units organized in the form of a dependency structure, labeled with their rhetorical relations.$Equal contribution.} As shown in Figure , the leaf nodes of an RST discourse tree are basic text spans called Elementary Discourse Units , and the EDUs are iteratively connected by rhetorical relations  to form larger text spans until the entire document is included.  The rhetorical relations are further categorized to Nucleus and Satellite based on their relative importance, in which Nucleus corresponds to the core part while Satellite corresponds to the subordinate part. While manual coherence analysis under the RST theory is labor-intensive and requires specialized linguistic knowledge, a discourse parser serves to automatically transform a document into a discourse tree. Document-level discourse parsing consists of three sub-tasks: hierarchical span splitting, rhetorical nuclearity determination, and rhetorical relation classification.    Models for RST-style discourse parsing have made much progress in the past decade. While statistical methods utilize hand-crafted lexical and syntactic features , data-driven neural approaches reduce feature-engineering labor by effective representation learning, and are capable of characterizing implicit semantic information. Neural networks are first used as feature extractors along with traditional shift-reduce approaches  or dynamic programming approaches . Then,  bridges the gap between neural and traditional methods by an end-to-end transition-based neural parser via an encoder-decoder architecture. Recently, pointer networks are introduced to achieve linear-time complexity, and models with top-down parsing procedures achieve favorable results on sentence-level discourse analysis tasks .  However, there is still much space for improvement in document-level discourse parsing. First, compared to sentence-level parsing, document-level parsing is more challenging due to the deeper tree structures and longer dependencies among EDUs: in the benchmark dataset RST Discourse Tree Bank  , the average EDU number at the document level is 56, which is 20 times larger than that of sentence-level parsing. Thus modeling context information across a long span is essential, especially if considering a top-down parsing procedure where poor accuracy at the top of the tree will propagate toward the leaf nodes. Second, the three sub-tasks of discourse parsing strongly rely on nuanced semantic judgments, which require comprehensive contextual representation with various types of linguistic information. Take discourse relation classification for example, explicit relations are overtly signaled by a connective word such as ``although'' and ``because'', which can be determined by lexical and syntactic features. However, this approach can not be readily adapted to implicit discourse relations determination, as it requires high-order features with semantic information. Moreover, to compensate for the lack of large-scale corpora, prior work in neural modeling has leveraged inductive biases through syntactic features such as part-of-speech tagging to improve performance. However, such models still suffer from insufficient linguistics information from the lack of data, thus they are incapable of acquiring deeper and richer contextual representations useful for discourse processing.  In this paper, to tackle the aforementioned challenges, we propose a document-level neural discourse parser with robust representation modeling at both the EDU and document level, based on a top-down parsing procedure. To take advantage of widely-adopted vector representations that encode rich semantic information, we first exploit a large-scale pre-trained language model as a contextual representation backbone.  Then we incorporate boundary information with implicit semantic and syntactic features to the EDU representations, and introduce a hierarchical encoding architecture to more comprehensively characterize global information for long dependency modeling. To improve inference accuracy and alleviate the aforesaid error propagation problem, we present breadth-first span splitting to propose a layer-wise beam search algorithm.  We train and evaluate our proposed model on the benchmark corpus RST-DT\footnote{https://catalog.ldc.upenn.edu/LDC2002T07} , and achieve the state-of-the-art performance on all fronts, significantly surpassing previous models while approaching the upper bound of human performance. We also conduct extensive experiments to analyze the effectiveness of our proposed method.    We proposed to exploit robust representations of multiple levels of granularity at the syntactic and semantic levels and in turn incorporated such representations in an end-to-end encoder-decoder neural architecture for resourceful discourse processing. Our document-level discourse parser compares favorably with the current state-of-the-art. Experimental results show that our document-based neural discourse parser benefits the most from incorporating boundary information at the EDU level and from modeling global information.   
"," Document-level discourse parsing, in accordance with the Rhetorical Structure Theory , remains notoriously challenging. Challenges include the deep structure of document-level discourse trees, the requirement of subtle semantic judgments, and the lack of large-scale training corpora. To address such challenges, we propose to exploit robust representations derived from multiple levels of granularity across syntax and semantics, and in turn incorporate such representations in an end-to-end encoder-decoder neural architecture for more resourceful discourse processing. In particular, we first use a pre-trained contextual language model that embodies high-order and long-range dependency to enable finer-grain semantic, syntactic, and organizational representations. We further encode such representations with boundary and hierarchical information to obtain more refined modeling for document-level discourse processing. Experimental results show that our parser achieves the state-of-the-art performance, approaching human-level performance on the benchmarked RST dataset.",435
" Due to the substantial growth and effortless access to the Internet in recent years, an enormous amount of unstructured textual contents have generated. It is a crucial task to organize or structure such a voluminous unstructured text in manually. Thus, automatic classification can be useful to manipulate a huge amount of texts, and extract meaningful insights which save a lot of time and money. Text categorization is a classical NLP problem which aims to categorize texts into organized groups. It has a wide range of applications like machine translation, question answering, summarization, and sentiment analysis. There are several approaches available to classify texts according to their labels. However, deep learning method outperforms the rule-based and machine learning-based models because of their ability to capture sequential and semantic information from texts . We propose a classifier using CNN , and BiLSTM  to classify technical texts in the computer science domain. Furthermore, by sequentially adding these networks, remarkable accuracy in several shared classification tasks can be obtained. The rest of the paper is organized as follows: related work given in section 2. Section 3 describes the dataset. The framework described in section 4. The findings presented in section 5.   %%%%%%%%%%%% Related Work %%%%%%%%%   This paper presents a detail description of the proposed system and its evaluation for the technical texts classification in different languages. As the baseline method, we used CNN and BiLSTM, and compare these methods with the proposed model . Each model is trained, tuned and evaluated separately for subtasks 1 and 2. The proposed method showed better performance in terms of accuracy for subtasks  of task 1 and task 2a on development set. However, in the case of test set, the system performed better for the subtasks 1a, 1b, 1c, 1g and 2a. More dataset can be included for improved performance. In future, the attention mechanism may be explored to observe its effects on text classification tasks.   
"," This paper illustrates the details description of technical text classification system and its results that developed as a part of participation in the shared task TechDofication 2020. The shared task consists of two sub-tasks:  first task identify the coarse-grained technical domain of given text in a specified language and  the second task classify a text of computer science domain into fine-grained sub-domains. A classification system  is developed to perform the classification task using three techniques: convolution neural network , bidirectional long short term memory  network, and combined CNN with BiLSTM. Results show that CNN with BiLSTM model outperforms the other techniques concerning task-1 of sub-tasks  and task-2a. This combined model obtained $f_1$ scores of 82.63 , 81.95 , 82.39 , 84.37 , and 67.44  on the development dataset. Moreover, in the case of test set, the combined CNN with BiLSTM approach achieved that higher accuracy for the subtasks 1a , 1b , 1c , 1g  and 2a .",436
" The traditional task-oriented dialogue systems, which focuses on providing information and performing actions by the given databases or APIs, often meet the limitation that the DB/API can not cover enough necessary cases. A good enhance can be achieved with lots of relevant domain knowledge in the form of descriptions, FAQs and customer reviews, which we call unstructured knowledge. Track 1 of the 9th Dialogue System Technology Challenges , Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access, aims at generating a response based on dialogue history and unstructured knowledge access. The whole task can be divided into three subtasks, knowledge-seeking turn detection, knowledge selection and knowledge-grounded response. Test set of this track includes seen and unseen parts. The unseen test set are collected on different domains, entities, and locales, aiming to evaluate models' generalization ability.   Knowledge-seeking turn detection, as the first subtask, needs to determine whether the related knowledge is contained in the unstructured knowledge base. In other words, this subtask can be modeled as a binary classification problem. If the model predicts that there exists related knowledge, then subtask 2  will search for the most relevant knowledge snippets and then pass them to the generation process . If the model predicts that there is no related knowledge for the specific question, the remaining two subtasks will not be performed. In this paper, we first conduct an entity matching for each question and then add the domain label from matching results to the end of dialogue history as model input.  Knowledge selection is to retrieve the most relevant knowledge snippets from the database according to the dialogue history and provide information for the subsequent response generation. The dialogue history is a conversation between the human speaker and the machine. Close to the end of the conversation, the human speaker brings up a question about a certain place  or service . The given knowledge database consists of question-answer pairs involving diverse facts and is organized by different domains and entities. % Note that the knowledge-seeking turn detection model determines whether our dialog system needs to access the knowledge database before generating the response.  % We perform knowledge selection for the samples  that requires relevant knowledge in the database. The retrieved knowledge snippets provide information for the subsequent response generation.  % Information retrieval  techniques are widely applied to search for related candidates in retrieval-based knowledge-grounded system. Some researchers  compute the traditional tf-idf score to search the most relevant document to the user's query, while others leverage the power of neural networks to learn the ranking score directly through an end-to-end learning process. Recently, due to the significant improvements on numerous natural language processing tasks, large scale pre-trained language models have also been applied to better model the semantic relevance in knowledge selection.  In this paper, we first apply retrieval techniques to narrow down the searching space and then use a neural network initialized by a pre-trained model to formulate the ranking function. % We propose two base models for the knowledge selection, and the final ensemble model combines the predictions of different base models to improve the selection performance.  % The Retrieve \& Rank model first gathers the knowledge snippets of potentially relevant entities from the knowledge base, then a ranking model is trained to select the most plausible knowledge snippets from the retrieved candidates. % Different from the Retrieve \& Rank model, Three-step model divides the ranking model into three cascade parts to rank domain, entity and documents respectively in order to force the model to take the knowledge hierarchy into account. % We also ensemble these two models together and experiments show the ensemble model has a better performance than two base model separately.   % briefly introduce the three-step pipeline model.   Knowledge-grounded response generation requests to give a response automatically from the model using dialogue history and unstructured knowledge as input. There are two different types of dialogue systems, retrieval-based system, and generation-based system. Retrieval-based dialogue system, giving responses from a list of candidate sentences, only has fixed answer forms in candidate sets. To deal with our problem, which needs more flexible and natural responses, the generation-based model is a better choice. Dialogue generation requires an encoder to represent the input and a decoder to generate the response. The network often needs to minimize the cross-entropy loss between the output and the ground truth. In this paper, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism.  % Pre-trained language models make a great progress on dialogue generation. Note that bi-directional model is not designed for dialogue generation task, and thus PLATO and  PLATO-2 use uni- and bi-directional processing for pre-training. Moreover, large-scale Reddit and Twitter conversations are utilized to further pre-train the generation model to reduce data distribution gaps. Furthermore, a latent variable  is used to capture one-to-many relations of post-response pairs.  As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics. In the following sections, we will explain the details of our proposed model. Experiment results will be shown next with some analysis and conclusions.    This paper describes our overall system that is evaluated in Track 1 of DSTC 9.  Pre-trained language models, ELECTRA and RoBERTa, are used as our base encoder, and task-specific components are applied to improve performance. In the released evaluation results, we rank second under objective metrics and rank fourth under human metrics. Considering the gap between validation and test set, it is worthwhile for us to further study how to generalize our model in a better way, that is, transferring our in-domain system to the out-of-domain scenario. 
"," Task-oriented conversational modeling with unstructured knowledge access, as track 1 of the 9th Dialogue System Technology Challenges , requests to build a system to generate response given dialogue history and knowledge access. This challenge can be separated into three subtasks,  knowledge-seeking turn detection,  knowledge selection, and  knowledge-grounded response generation. We use pre-trained language models, ELECTRA and RoBERTa, as our base encoder for different subtasks. For subtask 1 and 2, the coarse-grained information like domain and entity are used to enhance knowledge usage. For subtask 3, we use a latent variable to encode dialog history and selected knowledge better and generate responses combined with copy mechanism. Meanwhile, some useful post-processing strategies are performed on the model's final output to make further knowledge usage in the generation task.  As shown in released evaluation results, our proposed system ranks second under objective metrics and ranks fourth under human metrics.",437
"  %    Recent years have witnessed the rapid advancement of online recruitment platforms. With the increasing amount of online recruitment data, more and more interview related studies have emerged such as person-job  fit and automatic analysis of asynchronous video interviews , which aim to enable automated job recommendation and candidate assessment. Among these studies, person-job fit is to casting the task as a supervised text match problem. Given a set of labeled data , it aims to predict the matching label between the candidate resumes and job description. More recently, deep learning has enhanced person-job fit methods by training more effective text match or text representations models. AVI is to determine whether the candidate is hirable by evaluating the answers of interview questions. In AVIs, an interview is usually considered as a sequence of questions and answers containing salient socials signals. To evaluate the candidates more comprehensively, AVI models will extract the features of video , text, and voice in the process of answering questions. In this work, we focus on the scoring of multiple QA pairs,  we only extract the features of text modality and define this task as the scoring competency of candidates rather than the score of whether or not to be employed. Based on the anatomy of the human interviewers' evaluation process, the solutions consist of two stages:  analyzing and evaluating individual QA pair one by one, then acquiring the evaluation status, and  grading the competency of the candidate based on the evaluation status of multiple QA pairs.        For the first stage, existing methods tend to employ text matching or attentional text matching algorithms to evaluate QA pairs, which feeds the concatenated representation of the question and the answer to the subsequent classifier. As we all know, questions in an asynchronous video interview are not limited to specific domains. That is to say, candidates can answer questions according to their work or study experience. In this way, the candidates' answers will be varied and it is difficult to evaluate the answer accurately only by text matching. Intuitively, it is more reasonable to evaluate QA pairs through the semantic interaction between questions and answers. A critical challenge along this line is how to reveal the latent relationships between each question and answer.  %Intuitively, experienced interviewers could discover the semantic-level correlation between interview questions and candidates' answers, then obtain a preliminary judgement on the answer to the current question, and finally give an assessment based on the judgements of several problems. Therefore,  %In this work, we propose a sentence-level reasoning GNN to assess the single QA pair at the semantic interaction level. Graph neural networks  can learn effective representation of nodes by encoding local graph structures and node attributes. Due to the compactness of model and the capability of inductive learning, GNNs are widely used in modeling relational data and logical reasoning. Recently, ~ proposed a GNN variant, Named ExpressGNN, to strike a nice balance between the representation power and the simplicity of the model in probabilistic logic reasoning.~ constructed the DialogeGCN to address context propagation issues present in the RNN-based methods. Specifically, they leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Inspired by, we present a sentence-level relational GCN to represent the internal temporal and QA interaction dependency in the process of answering questions. %Recently, graph neural network or graph emebedding has attracted wide attention. Graph neural networks have been effective at tasks thought to have rich relational structure and can preserve global structure information of a graph in graph emebedding.   %In this work, we aim to address the task of automatically scoring the textual answer of candidates at the semantic interaction level.  %The automatic short answer scoring  is a task of estimating a score of a short text answer written as response to a given prompt on the basis of whether the answer satisfies the rubrics prepared by a human in advance. ASAS systems have mainly been constructed to markedly reduce the scoring cost of human rater.   %鐟欏嫬鍨鍫ユ閸掕泛鐣鹃敍灞芥礈濮濄倕顕梻顕顣介崪灞芥礀缁涙棃妫跨拠顓濈疅娴溿倓绨伴惃鍕閹烘ɑ妯夊妤佹纯閸旂娀鍣哥憰 %閸ョ偓膩閸ㄥ婀梻顕顣介幒銊ф倞娴犺濮熸稉濂僥ep learning has proven to be effective in long text NLP tasks. Due to the lack of information in the short sentence of the ASAS corpus, it seems not good enough in the ASAS task.  For the second stage of grading the candidate, based on the representation of QA pairs, exists methods prefer to encoder question-answer pairs as a sequence directly. However, this kind of approaches lead to insufficient interaction between the semantic information of question and answer pairs. Therefore, it is difficult to ensure the rationality and explainability of the evaluation. To mitigate this issue, in the first stage, we present a semantic-level graph attention network  to model the interaction states of each QA session.    %Automatic scoring of answer transcriptions in job interview aims to evaluate multiple question-answer pairs.  %To alleviate this limitation of previous approaches, To this end, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic scoring of answer transcriptions  in job interviews. Specifically, the proposed sentence-level relational graph convolutional neural network  is used to capture the contextual dependency, and the semantic-level Reasoning graph attention network  is applied to acquire the latent interaction states. And the contribution of our work can be summarized as follows:      In this paper, we propose a hierarchical reasoning graph neural network  for the automatic scoring of answer transcriptions  in the video job interview. The ASAT task is to score the competency of candidates based on several textual question-answer pairs. Unlike other matching based methods or frameworks, HRGNN can utilize the relational dependency of sentences in the questions and answers, and aggregate them in the semantic level with reasoning flow between different graph layers. Particularly, the proposed relational graph convolutional network  module constructs internal temporal dependency and question-answer interaction dependency to represent the relations between sentences in the question and the answer. And in the graph-based reasoning part, we propose a graph attention network to further aggregate semantic interactions between the question and the answer. Finally, we apply a GRU-based classifier to discriminate the candidate is competent or not. Empirical results with 10 random seeds show that our model achieves state-of-the-art on a Chinese real-world dataset .   We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the video job interview. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and then give the evaluation results combined with multiple interaction states. Recent studies tend to use text matching approaches to evaluate each QA pair roughly, which fails to take advantage of the semantic association between questions and answers. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level reasoning graph neural network to capture the latent semantic interaction of sentences in the question or the answer. Based on these graphs, we employ a semantic-level graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit with a global fusion mechanism to aggregates evidence of temporal QA pairs for the final score. Empirical results conducted on CHNAT  clearly validate that our proposed model significantly outperforms text-matching based benchmark models.    
"," %Automatic scoring of answer transcripts in job interview aims to evaluate multiple question-answer pairs. The key challenge is how to conduct deep interaction on the semantic level for each question-answer pair, and give the evaluation results combined with multiple interaction states. Recent studies either use text matching approaches to evaluate each question-answer pair roughly, or employ the sequential model to deal with disordered question-answer pairs which fail to take advantages of the semantic association between questions and answers, and the logical connection between question-answer pairs. In this work, we propose a hierarchical reasoning Graph Neural Network  for the automatic assessment of multi-question answering. Specifically, we construct a sentence-level reasoning GNN to assess the single question-answer pair. Based on these graphs, we propose a document-level reasoning GNN to model the interaction states of question-answer pairs. The first module utilizes each sentence in the question and answer to establish the connection between them. The second module adopts a graph convolutional network to encoder interaction states of each pair and aggregates evidence with graph attention mechanism for predicting the final score. Empirical results on Chinese and English interview datasets show that our proposed model outperforms both sequence-based and pre-training based  benchmark models.  %We address the task of automatically scoring the answer competency of candidates based on textual features from the automatic speech recognition transcriptions. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and give the evaluation results combined with multiple interaction states. Recent studies either use text matching approaches to evaluate each QA pair roughly, or employ the sequential model to deal with disordered QA pairs which fail to take advantages of the semantic association between questions and answers, and the logical connection between QA pairs. In this work, we propose a hierarchical reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level reasoning GNN to assess the single QA pair. Based on these graphs, we propose a document-level reasoning GNN to model the interaction states of QA pairs. The first module utilizes each sentence in the question and answer to establish the connection between them. The second module adopts a graph convolutional network to encoder interaction states of each pair and aggregates evidence with graph attention mechanism for predicting the final score. Empirical results conducted on CHNAT and ENGIAT  clearly validate that our proposed model outperforms both text matching based benchmark models.  %We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the video job interview. The key challenge is how to conduct deep interaction on the semantic level for each question-answer  pair, and then give the evaluation results combined with multiple interaction states. Recent studies tend to use text matching approaches to evaluate each QA pair roughly, which fails to take advantage of the semantic association between questions and answers. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the latent semantic interaction of sentences in the question or the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit with a global fusion mechanism to aggregates evidence of temporal QA pairs for the final score. Empirical results conducted on CHNAT  clearly validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.    We address the task of automatically scoring the competency of candidates based on textual features, from the automatic speech recognition  transcriptions in the asynchronous video job interview . The key challenge is how to construct the dependency relation between questions and answers, and conduct the semantic level interaction for each question-answer  pair. However, most of the recent studies in AVI focus on how to represent questions and answers better, but ignore the dependency information and interaction between them, which is critical for QA evaluation. In this work, we propose a Hierarchical Reasoning Graph Neural Network  for the automatic assessment of question-answer pairs. Specifically, we construct a sentence-level relational graph neural network to capture the dependency information of sentences in or between the question and the answer. Based on these graphs, we employ a semantic-level reasoning graph attention network to model the interaction states of the current QA session. Finally, we propose a gated recurrent unit encoder to represent the temporal question-answer pairs for the final prediction. Empirical results conducted on CHNAT  validate that our proposed model significantly outperforms text-matching based benchmark models. Ablation studies and experimental results with 10 random seeds also show the effectiveness and stability of our models.",438
"  Social media is a unique source of information. On the one hand, their low cost, easy access and distribution speed make it possible to quickly share the news. On the other hand, the quality and reliability of social media news is difficult to verify . This is the source of a lot of false information that has a negative impact on society.   Over the past year, the world has been watching the situation developing around the novel coronavirus pandemic. The COVID-19 pandemic has become a significant newsworthy event of 2020. Therefore, news related to COVID-19 are actively discussed on social media and this topic generates a lot of misinformation. Fake news related to the pandemic have large-scale negative social consequences, they provoke huge public rumor spreading and misunderstanding about the COVID-19 and aggravate effects of the pandemic. Moreover, recent studies  show an increase in symptoms such as anxiety and depression in connection with the pandemic. This is closely related to the spread of misinformation, because fake news can be more successful when the population is experiencing a stressful psychological situation . The popularity of fake news on social media can rapidly increase, because the rebuttal is always published too late. In this regard, there is evidence that the development of tools for automatic COVID-19 fake news detection plays a crucial role in the regulation of information flows.  In this paper, we present our approach for the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in English  that attracted 433 participants on CodaLab. This approach achieved the weighted F1-score of 98.69  on the test set among 166 submitted teams in total.  The rest of the paper is organized as follows. A brief review of related work is given in Section 2. The definition of the task has been summarized in Section 3, followed by a brief description of the data used in Section 4. The proposed methods and experimental settings have been elaborated in Section 5. Section 6 contains the results and error analysis respectively. Section 7 is a conclusion.     In this work, we propose a simple but effective approach to COVID-19 fake news detection based on CT-BERT and ensembling learning. Our experiments confirmed that BERT-based models specialized in the subject area successfully cope with such tasks and perform high-quality binary classification.  The experimental results showed that our solution achieved 98.69\  of the weighted F1-score on test data and ranked in the first place in the Constraint@-AAAI2021 shared task. For future work, we can experiment with different training and data augmentation techniques. We can also apply and evaluate hybrid models combining BERT-based architectures with other methods of natural language processing .      ---- Bibliography ----     BibTeX users should specify bibliography style 'splncs04'.   References will then be sorted and formatted in the correct style.     
"," The COVID-19 pandemic has had a huge impact on various areas of human life. Hence, the coronavirus pandemic and its consequences are being actively discussed on social media. However, not all social media posts are truthful. Many of them spread fake news that cause panic among readers, misinform people and thus exacerbate the effect of the pandemic. In this paper, we present our results at the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in English. In particular, we propose our approach using the transformer-based ensemble of COVID-Twitter-BERT  models. We describe the models used, the ways of text preprocessing and adding extra data. As a result, our best model achieved the weighted F1-score of 98.69 on the test set  of this shared task that attracted 166 submitted teams in total.",439
" Identifying emotion in dialogues is one of the most challenging tasks in the area of natural language processing, which is very important for building the dialogue systems . Though sentiment analysis is being studied in natural language community for a long time , understanding multiple emotions expressed in chats and conversations comes up with relatively harder challenges  for many reasons.   Various individuals may respond differently towards the same comment. Absence of voice modulations and facial expressions in informal chatting makes it difficult to capture emotion in the conversation. This type of problem is earlier addressed in EmoContext  and EmotionX-2018 . People during chatting often take help of emojis, figures and message contractions not only to reduce effort and shorten comments but also to express their emotions better. In this case, GIFs play an essential role . Often social media users reply with GIFs only, without any text response. Therefore, to understand the different emotions associated with a GIF in the reply, it is necessary to consider the comment and its relation with its associated reply. This type of problem is earlier addressed in EmotionX-2019 challenge  where the task was to predict emotions in spoken dialogues and chats.   The enhanced and better expressiveness of GIFs in comparison to other popular graphics-based media, such as emojis and emoticons have made their utilization amazingly mainstream on social media and a significant expansion to online human communication which has motivated the introduction of EmotionGIF 2020  shared task.     Given an unlabelled tweet and its reply , the challenge is to recommend the possible categories its GIF response may belong to. All tweets in the training set have their GIF responses with some tweets having their text responses as well. The task requires to return a non-empty subset of 1-6 categories from among 43 possible GIF categories for a given unlabelled tweet.      & text & reply & categories & mp4 \\ \hline 47 & what is sleep & - & [""shrug"", ""idk""] & 4f69c84cb....mp4 \\ 42 & Are your ready for America & - & [""slow\_clap"", ""applause"", & \\ & to Open Up Again? & &  ""yes""] &  9acf47aab3....mp4 \\ 95 & we as a collective have been & & & \\ & traumatized by men with J & & & \\ & names. & Woahhhhh lol & [""yes""] & 12d85340f....mp4 \\ \hline     In this paper, we develop a deep learning framework for predicting the categories of a GIF response for an unlabelled tweet. We build multiple deep neural-based systems, such as CNN , Bidirectional Gated Recurrent Unit  , and Bidirectional Long Short Term Memory Networks  . We support our models with an attention mechanism  that emphasizes on the important parts of a given input tweet. We combine multiple basic models to result in a couple of stacked architectures , and finally, we report the final predictions from a majority voting-based ensemble  method that combines all the developed models. Our proposed frameworks are less complex than the standard transformer models, but can provide reasonably good results and can be trained using local GPU support conveniently.  The rest of the paper is organized as follows: Section 2 gives a brief description of the dataset and the various preprocessing measures applied to them. The details of the proposed methodologies are discussed in section 3. In Section 4, we discuss the various experimental details and their results. Finally, we conclude the paper in section 5.    In this work, we build an ensemble deep learning framework on top of several attention-based deep neural networks to achieve the task objective of predicting categories for a GIF response. We effectively incorporate both the tweets and their text responses in building our automated systems. Our participation in EmotionGIF 2020 has been a wonderful learning experience for our team as we have achieved 5\textsuperscript{th} rank in both the rounds with attained MR@6 scores of 0.5292 and 0.5380, respectively. We look forward to learn more from the best-performing systems. Results indicate that our models can serve as strong baselines as an alternative framework to transformer-based approaches.   In future, we will try to enrich the learning of our developed end-to-end systems by effectively incorporating multimodal features extracted from the GIFs of training data and map them to unlabelled test data .    
"," In this paper, we describe the systems submitted by our IITP-AINLPML team in the shared task of SocialNLP 2020, EmotionGIF 2020, on predicting the category of a GIF response for a given unlabelled tweet.  For the round 1 phase of the task, we propose an attention-based Bi-directional GRU network trained on both the tweet  and their replies  and the given category for its GIF response. In the round 2 phase, we build several deep neural-based classifiers for the task and report the final predictions through a majority voting based ensemble technique. Our proposed models attains the best Mean Recall  scores of 52.92\% and 53.80\% in round 1 and round 2, respectively.",440
"   Machine translation has been shown to exhibit gender bias , and several solutions have already been proposed to mitigate it . The general gender bias in Natural Language Processing  has been mainly attributed to data . Several studies show the pervasiveness of stereotypes in book collections , or Bollywood films , among many others. As a consequence, our systems trained on this data exhibit biases. Among other strategies, several studies have proposed to work in data augmentation to balance data  or forcing gender-balanced datasets . In parallel, other initiatives focus on documenting our datasets  to prioritize transparency.  However, data is not the only reason for biases, and recent studies show that %algorithms and training strategies matter.  our models can be trained in a robust way to reduce the effects of data correlations . In , the authors explored available mitigations and by increasing dropout, which resulted in improving how the models reasoned about different stereotypes in WinoGender examples .   The purpose of the current paper is to explore if the Multilingual Neural Machine Translation  architecture can impact the amount of gender bias. To answer this question, we compare MNMT architectures trained with the same data and quantify their amount of gender bias with the standard WinoMT evaluation benchmark . Results show that the Language-Specific encoders-decoders  exhibit less bias than the Shared encoder-decoder . Then, we analyze and visualize why the MNMT architecture impacts mitigating or amplifying this bias by studying its internal workings. We study the amount of gender information that the source embeddings encode, and we see that Language-Specific surpasses Shared in these terms, allowing for a better prediction of gender.  Additionally, and taking advantage that both Shared and Language-Specific are based on the Transformer , we study the coefficient of variation in the attention , which shows that the attention span is narrower for the Shared system than for the Language-Specific one. Therefore, the context taken into account is smaller for the Shared system, which causes a higher gender bias.   %We observe that this is caused by using a Shared encoder-decoder with several languages since pairwise Bilingual systems have a wider attention span. Given the similarities of not sharing modules and parameters across languages in both Bilingual and Language-Specific, this characteristic  of the Bilingual systems prevails in the language-specific architecture.   Finally, we also do a manual analysis to investigate which biases have a linguistic explanation. %implications in gender bias has the target language from the linguistic and social point of view.       This paper shows that the MNMT architecture by itself has an impact on gender accuracy. Language-Specific outperforms Shared in two different language sets: English, German, Spanish, French and English, German, Spanish, French, and Russian. We observe that the difference in gender accuracy is higher in the language set including Russian.   Further interpretability analysis of the results shows that source embeddings in the Language-Specific architecture retain higher information on gender. Moreover, this architecture also keeps enough diversion in the attention, especially when including Russian. Both elements help in better inferring the correct gender.   Finally, a manual analysis shows that most of the errors are made assuming a masculine occupation instead of a feminine one. In contrast, the inverse error tends to come when there is a feminine version of that word with another meaning.    
"," Multilingual Neural Machine Translation architectures mainly differ in the amount of sharing modules and parameters among languages. In this paper, and from an algorithmic perspective, we explore if the chosen architecture, when trained with the same data, influences the gender bias accuracy. Experiments in four language pairs show that Language-Specific encoders-decoders exhibit less bias than the Shared encoder-decoder architecture. Further interpretability analysis of source embeddings and the attention shows that, in the Language-Specific case, the embeddings encode more gender information, and its attention is more diverted. Both behaviors help in mitigating gender bias.",441
" Commonsense question answering  is recently an attractive field in that it requires systems to understand the common sense information beyond words, which are normal to human beings but nontrivial for machines. There are plenty of datasets that are proposed for this purpose, for instance, CommonsenseQA , CosmosQA , WIQA . Different from traditional machine reading comprehension  tasks such as SQuAD  or NewsQA  that the key information for answering the questions is directly given by the context paragraph, solving commonsense questions requires a more comprehensive understanding of both the context and the relevant common knowledge, and further reasoning out the hidden logic between them. There are varieties of knowledge bases that meet the need, including text corpora like Wikipedia, and large-scale knowledge graphs .  Recent popular solution resorts to external supporting facts from such knowledge bases as evidence, to enhance the question with commonsense knowledge or the logic of reasoning . However, the quality of the supporting facts is not guaranteed, as some of them are weak in interpretability so that do not help the question answering. Specifically, current methods are mainly two-fold. The first group of methods  pre-train language models on those external supporting facts  so that the models could remember some of the common knowledge, which is empirically proven by Tandon et al.  and Trinh and Le . The second group of methods  incorporates the question with knowledge subgraphs or paths that carry information such as relation among concepts or show multi-hop reasoning process. The structured information is typically encoded via graph models such as GCN , and after which merged with the question features. Generally, current methods all handle evidence by brute force, without further selection or refinement according to the interpretability of the supporting facts. But as the example shown in Figure, some of the supporting facts do not interpret the question, regardless that they are semantically related. Thus, there is need for models that will further our processing of the evidence.  In this paper, we introduce a new recursive erasure memory network  that further refines the candidate supporting fact set. The REM-Net consists of three main components: a query encoder, an evidence generator, and a novel recursive erasure memory  module. Specifically, the query encoder is a pre-trained encoder that encodes the question. The evidence generator is a pre-trained generative model that produces candidate supporting facts based on the question. Compared with those retrieved supporting facts, the generated facts provides new question-specific information beyond the existing knowledge bases. The REM module refines the candidate supporting fact set by recursively matching the supporting facts and the question in feature space to estimate each fact's quality. This estimation helps both updating the question feature and the supporting fact set. The question feature is updated by a residual term, whereas the supporting fact set is updated by removing the low-quality facts. Compared with the standard attention mechanisms  that allocate weights to the supporting facts once, the multi-hop operation in REM module widens the gap of how much each supporting fact contributes to the question answering by the number of recursive steps their features are incorporated for the feature update. Therefore this procedure leads to a refined use of given supporting facts.  We conduct experiments on two commonsense QA benchmarks, WIQA  and CosmosQA . The experimental results demonstrate that REM-Net outperforms current methods, and the refined supporting facts are more qualified for the questions. Our contributions are mainly three-fold:        In this paper, we propose a recursive erasure memory network  that refines evidence for commonsense question. It recursively estimates quality of each supporting fact based on the question, and refines the supporting fact set accordingly. The recursive procedure leads to repeated use of high-quality supporting facts, so that the question answering is conducted by useful information. Experimental results demonstrates that REM-Net is effective for the commonsense QA tasks, and the evidence refinement is interpretable. Besides, we evaluate the quality of generated evidence compared to retrieved evidence, learning that using generated evidence gives better performance.   
"," When answering a question, people often draw upon their rich world knowledge in addition to the particular context. While recent works retrieve supporting facts/evidence from commonsense knowledge bases to supply additional information to each question, there is still ample opportunity to advance it on the quality of the evidence. It is crucial since the quality of the evidence is the key to answering commonsense questions, and even determines the upper bound on the QA systems' performance. In this paper, we propose a recursive erasure memory network  to cope with the quality improvement of evidence. To address this, REM-Net is equipped with a module to refine the evidence by recursively erasing the low-quality evidence that does not explain the question answering. Besides, instead of retrieving evidence from existing knowledge bases, REM-Net leverages a pre-trained generative model to generate candidate evidence customized for the question. We conduct experiments on two commonsense question answering datasets, WIQA and CosmosQA. The results demonstrate the performance of REM-Net and show that the refined evidence is explainable.",442
"  We typically train neural machine translation  systems on human-translated parallel texts, then ask them to decode previously-unseen source sentences.  Trained parameter values induce a distribution  over all pairs of source/target strings .  Given a new source string , the NMT decoder searches for the best target string :        = \argmax_y P   This optimization is unsolvable for general recurrent neural networks , while  present an exact optimization search algorithm for consistent NMT models.       {|c}{Decoder beam = 512} \\ pair & Length ratio & Empty ratio \\   Our training data does not contain any source strings translated to empty strings, so why does NMT learn to assign high probability to empty translations? Our findings are:         We investigate why NMT systems assign high probability to empty translations. We find that label smoothing mainly decreases the probability of normal-length target sentences, and that the single-EoS smoothing effect increases the probability of the empty translation.    
","  We investigate why neural machine translation  systems assign high probability to empty translations. We find two explanations. First, label smoothing makes correct-length translations less confident, making it easier for the empty translation to outscore them. Second, NMT systems use the same, high-frequency EoS word type to end all target sentences, regardless of length. This creates an implicit smoothing that increases the relative probability zero-length translations.  Using different EoS types in target sentences of different lengths exposes this implicit smoothing.",443
"   Understanding emotion in human social conversations or chitchat has gained popularity in the natural language processing community due to its usefulness in developing human-like conversational agents. Emotions revealed in social chitchat are rather complex. It has many categories of emotions to distinguish due to subtle variations present in human emotion. For example, Sadness and Disappointment are pursued and dealt differently in human conversations. Also, the listeners' reaction to emotions is not always a straightforward mirroring effect of the speakers' emotions. Rather it can be more neutral and convey a specific intent, as is evident from the dialogue example in Table .   [ht!] {|r X|}  \\ Listener:&oh no! That閳ユ獨 scary! What do you think it is? \\ Speaker:&I don閳ユ獩 know, that閳ユ獨 what閳ユ獨 making me anxious. \\ Listener:&I閳ユ獡 sorry to hear that. \\ ; Agreeing; Acknowledging; Sympathizing; Encouraging; Consoling: Suggesting; Wishing; and Neutral . They have automatically annotated the EmpatheticDialogues dataset  with 32 fine-grained emotions and the 9 empathetic response intents and discovered frequent emotion-intent exchange patterns in human social conversations. They observe that this type of dataset tagged with fine-grained emotions and response intents could train neural chatbots to generate empathetically appropriate responses conditioned on a selected emotion or intent. However, for this purpose, a large-scale emotion and intent labeled dataset is even more desirable. Curating such a dataset is technically challenging because 1) annotating such a large-scale dataset require human labor that is costly, and 2) given the fine-granularity of the emotion and intent labels, the human labeling task is more difficult compared to more generic Angry-Happy-Sad. As a result, existing manually labeled emotional dialogue datasets such as IEMOCAP , MELD , and DailyDialogue  are smaller in scale and contain only a limited set of emotions , with simpler dialogue responding strategies, or both. Also, existing datasets often contain a label Neutral or Other for responses that do not convey emotion, which introduces vagueness and limits the ability of automatic agents that use such datasets in learning useful response strategies.   % , EmotionLines , and EmoContext   To fill the above gap, we curate a novel large-scale dialogue dataset, OSED , containing 1M emotional dialogues from movie subtitles, in which each dialogue turn is automatically annotated with 32 fine-grained emotions and 9 empathetic response intents. Movie subtitles well approximate human social conversations and how emotion is handled in them. It is one of the major sources to learn emotional variations and corresponding response strategies. To reduce the cost of human labeling and the complexity of labeling dialogues with fine-grained emotions and intents, we devise a semi-automated human computation task to collect fine-grained emotion and intent labels for a small set of movie dialogues . We then follow a semi-supervised approach to expand the labeled data and train a dialogue emotion classifier to automatically annotate 1M emotional dialogues.   The process of curating the dataset consists of several stages. First, we apply automatic turn and dialogue segmentation methods on movie subtitles in the OpenSubtitles  corpus  and obtain close to 9M dialogues. After data cleaning and removing duplicates, we reduce its size to 4M. Then, we apply a weak labeler, EmoBERT  trained on the EmpatheticDialogues dataset , to label utterances in OS dialogues and filter 1M emotional dialogues . Thirdly, with semi-supervised learning methods, we refine EmoBert and obtain EmoBert+, a more advanced dialogue emotion classifier trained on OS dialogues. To evaluate EmoBert+, we compare it with FastText. The former is more accurate than FastText. Finally, we use EmoBert+ to label dialogues in OSED initial to obtain the final 1M OSED dataset. We evaluate the quality of the resultant dataset by visually inspecting the emotion-intent flow patterns that occur in the dataset and checking if they conform to the patterns of human social conversations discovered in existing work . Figure  summarizes the process of creating OSED. The data curation pipeline we follow substantially reduces the cost of human labor, while ensuring quality annotations.   Our contributions in this paper are three-fold. 1) We curate a dialogue dataset, OSED, containing 1M emotional dialogues labeled with 32 fine-grained emotions and 9 empathetic response intents. Compared to existing dialogue datasets tagged with emotions, OSED is more general-purpose, significantly larger, and contains more fine-grained emotions and empathetic response strategies. 2) We outline the complex pipeline used to derive this dataset and evaluate the annotation quality using visualization methods. 3) We release our fine-grained emotion classifier used to annotate the OSED dataset, which can be used as a general-purpose classifier capable of recognizing fine-grained emotions and empathetic response intents in social chitchat.        In this work, we curated a large-scale dialogue dataset, OSED, comprising of 1M emotional dialogues from movie subtitles. This dataset is more general-purpose, larger in size, and contains more fine-grained emotion categories and empathetic response intents than the existing emotional dialogue datasets. To facilitate annotation, we developed a dialogue emotion classifier capable of recognizing 32 fine-grained emotions and 9 empathetic response intents with significant accuracy. It was trained on movie dialogues initially annotated using human computation and extended using self-labeling, and sentence similarity approaches. As future work, we intend to extend the taxonomy of empathetic response intents using new labels discovered during this process and utilize the OSED dataset to develop a controllable neural chatbot capable of generating empathetic responses during social chitchat.       
"," We propose a novel large-scale emotional dialogue dataset, consisting of 1M dialogues retrieved from the OpenSubtitles corpus and annotated with 32 emotions and 9 empathetic response intents using a BERT-based fine-grained dialogue emotion classifier. This work explains the complex pipeline used to preprocess movie subtitles and select good movie dialogues to annotate. We also describe the semi-supervised learning process followed to train a fine-grained emotion classifier to annotate these dialogues. Despite the large set of labels, our dialogue emotion classifier achieved an accuracy of $65\%$ and was used to annotate 1M emotional movie dialogues from OpenSubtitles. This scale of emotional dialogue classification has never been attempted before, both in terms of dataset size and fine-grained emotion and intent categories. Visualization techniques used to analyze the quality of the resultant dataset suggest that it conforms to the patterns of human social interaction.",444
"  Neural machine translation  has advanced significantly in recent years . In particular, the Transformer model has become popular for its well-designed architecture and the ability to capture the dependency among positions over the entire sequence . Early systems of this kind stack 4-8 layers on both the encoder and decoder sides , and the improvement often comes from the use of wider networks . More recently, researchers try to explore deeper models for Transformer. Encouraging results appeared in architecture improvements by creating direct pass from the low-level encoder layers to the decoder , and proper initialization strategies .  Despite promising improvements, problems still remain in deep NMT. Deep Transformer stacked by dozens of encoder layers always have a large number of parameters, which are computationally expensive and memory intensive. For example, a 48-layer Transformer is  larger than a 6-layer system and  slower for inference. It is difficult to deploy such models on resource-restricted devices, such as mobile phones. Therefore, it is crucial to compress such heavy systems into light-weight ones while keeping their performance.  Knowledge distillation is a promising method to address the issue. Although several studies  have attempted to compress the 12-layer BERT model through knowledge distillation, effectively compressing extremely deep Transformer NMT systems is still an open question in the MT community. In addition, these methods leverage sophisticated layer-wise distillation loss functions to minimize the distance between the teacher and the student models, which requires huge memory consumption and enormous training cost.  In this paper, we investigate simple and efficient compression strategies for deep Transformer. We propose a novel Transformer compression approach ) to transfer the knowledge from an extremely deep teacher model into a shallower student model. We disturb the computation order among each layer group during the teacher training phase, which is easy to implement and memory friendly. Moreover, to further enhance the performance of the teacher network, we introduce a vertical ``dropout''  into training by randomly omitting sub-layers to prevent co-adaptations of the over-parameterized teacher network. Although similar technique has been discussed in 's work, we believe that the finding here is complementary to theirs. Both Gpkd and regularization training methods can be well incorporated into the teacher training process, which is essential for obtaining a strong but light-weight student model.  \pgfdeclarepatternformonly{soft horizontal lines}{\pgfpointorigin}{\pgfqpoint{100pt}{1pt}}{\pgfqpoint{100pt}{3pt}}% {   \pgfsetstrokeopacity{0.3}   \pgfsetlinewidth{0.1pt}   \pgfpathmoveto{\pgfqpoint{0pt}{0.5pt}}   \pgfpathlineto{\pgfqpoint{100pt}{0.5pt}}   \pgfusepath{stroke} }  \pgfdeclarepatternformonly{soft crosshatch}{\pgfqpoint{-1pt}{-1pt}}{\pgfqpoint{6pt}{6pt}}{\pgfqpoint{5pt}{5pt}}% {   \pgfsetstrokeopacity{0.3}   \pgfsetlinewidth{0.4pt}   \pgfpathmoveto{\pgfqpoint{4.5pt}{0pt}}   \pgfpathlineto{\pgfqpoint{0pt}{4.5pt}}   \pgfpathmoveto{\pgfqpoint{0pt}{0pt}}   \pgfpathlineto{\pgfqpoint{4.5pt}{4.5pt}}   \pgfusepath{stroke} }  \definecolor{ugreen}{rgb}{0,0.5,0}      = [line width=0.5pt,minimum height=2.4em,minimum width = 1em,inner sep=3pt,rounded corners=1pt,font=\tiny]   \tikzstyle{legend_node} = [minimum height=1em,minimum width = 1em,draw,thick,inner sep=3pt,rounded corners=1pt,font=;   	_{1}};   	_{1}};   	_{1}};   	_{n}\mathrm{Group}_{1}\mathrm{Group}_{2}L_{1}L_{2}L_{3}2.4630.630.503.2$ times with almost no loss in BLEU.        Our contributions in this work are two folds.  We propose a  method to compress the deep model into a shallower one with minor performance sacrifice, which outperforms the  method by a large margin.  The proposed Skipping Sub-Layer method reduces the overfitting problem when training extremely deep encoder systems by randomly omitting sub-layers during training phase. The experimental results on three widely-used benchmarks validate the effectiveness of the proposed methods. After the incorporating of two methods, the strong but light-weight student models show competitive performance which is application friendly.  
","     Recently, deep models have shown tremendous improvements in neural machine translation . However, systems of this kind are computationally expensive and memory intensive. In this paper, we take a natural step towards learning strong but light-weight NMT systems. We proposed a novel group-permutation based knowledge distillation approach to compressing the deep Transformer model into a shallow model. The experimental results on several benchmarks validate the effectiveness of our method. Our compressed model is $8\times$ shallower than the deep model, with almost no loss in BLEU. To further enhance the teacher model, we present a Skipping Sub-Layer method to randomly omit sub-layers to introduce perturbation into training, which achieves a BLEU score of 30.63 on English-German newstest2014. The code is publicly available at https://github.com/libeineu/GPKD.",445
"  {S}{emantic} role labeling , also known as shallow semantic parsing, conveys the meaning of a sentence by forming a predicate-argument structure for each predicate in a sentence, which is generally described as the answer to the question ""Who did what to whom, where and when?"". The relation between a specific predicate and its argument provides an extra layer of abstraction beyond syntactic dependencies  , such that the labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Given a sentence in Figure , SRL pipeline framework consists of 4 subtasks, including predicate identification , predicate disambiguation , arguments identification  and arguments classification . SRL is a core task of natural language processing  having wide range of applications such as neural machine translation , information extraction , question answering , emotion recognition from text , document summarization  etc.   Semantic role labeling can be categorized into two categories, span and dependency. Both types of SRL are useful for formal semantic representations but dependency based SRL is better for the convenience and effectiveness of semantic machine learning. Johansson and Nugues  concluded that the best dependency based SRL system outperforms the best span based SRL system through gold syntactic structure transformation. The same conclusion was also verified by Li et al.  through a solid empirical verification. Furthermore, since 2008, dependency based SRL has been more studied as compared to span based SRL. With this motivation, we focus on dependency based SRL, which is mainly popularized by CoNLL-2008 and CoNLL-2009 shared tasks .   The traditional approaches to SRL focus on feature engineering which struggles in apprehending discriminative information  while neural networks are proficient enough to extract features automatically . Specifically, since large scale empirical verification of Punyakanok et al. , syntactic information has been proven to be extremely beneficial for SRL task. Later works  achieve satisfactory performance for SRL with syntax-agnostic models which creates conflict with the long-held belief that syntax is essential for high-performance SRL .  The study of Li et al.  shows that the empirical results from neural models on the less importance of syntax indicate a potential challenge and despite the satisfactory performance of syntax-agnostic SRL systems, the reasons behind the absence of syntax in these models are three-fold. First, the effective incorporation of syntax in neural SRL models is quite challenging as compared to traditional approaches. Second, neural SRL models may cover partial syntactic clues more or less. Third, syntax has always been a complicated formalism in linguistics and its not easy to encode syntax for later usage. %Despite the satisfactory performance of syntax-agnostic SRL models, the reasons behind the absence of syntax in these models are two-fold. First, the effective incorporation of syntax information in neural SRL models is quite challenging. Second, the unreliability of syntactic parsers on account of the risk of erroneous syntactic input may lead to error proliferation. This has been proven by Li et al.  through a strong empirical verification. They show that the effective method of syntax incorporation and the high quality of syntax can promote SRL performance.%     This paper presents a neural framework for semantic role labeling, effectively incorporating a filter generation network to extract important syntactic features encoded by BiLSTM and Tree-LSTM by generating filters conditioned on inputs. The adaptive convolution endows flexibility to existing convolution operations. With the extraction of important syntax information, we are able to enlarge the gap between syntax aware and syntax agnostic SRL systems. We further study a hashing technique which drastically decreases the size of the filter generation network. Lastly, we explore the effects of syntax quality on SRL systems and conclude that the high quality syntax can improve SRL performance. Experiments on CoNLL-2009 dataset validate that our proposed model outperforms most previous SRL systems for both English and Chinese languages.  
"," Semantic role labeling  aims at elaborating the meaning of a sentence by forming a predicate-argument structure. Recent researches depicted that the effective use of syntax can improve SRL performance. However, syntax is a complicated linguistic clue and is hard to be effectively applied in a downstream task like SRL. This work effectively encodes syntax using adaptive convolution which endows strong flexibility to existing convolutional networks. The existing CNNs may help in encoding a complicated structure like syntax for SRL, but it still has shortcomings. Contrary to traditional convolutional networks that use same filters for different inputs, adaptive convolution uses adaptively generated filters conditioned on syntactically-informed inputs. We achieve this with the integration of a filter generation network which generates the input specific filters. This helps the model to focus on important syntactic features present inside the input, thus enlarging the gap between syntax-aware and syntax-agnostic SRL systems. We further study a hashing technique to compress the size of the filter generation network for SRL in terms of trainable parameters. Experiments on CoNLL-2009 dataset confirm that the proposed model substantially outperforms most previous SRL systems for both English and Chinese languages.",446
"     {}             The randomly sampling-based user simulator neglects the fact that human learning supervision is often accompanied by a curriculum . For instance, when a human-teacher teaches students, the order of presented examples is not random but meaningful, from which students can benefit . Therefore, this randomly sampling-based user simulators bring two issues:   issue:  since the ability of the dialogue agent does not match the difficulty of the sampled user goal, it takes a long time for the dialogue agent to learn the optimal strategy . For example, in the early learning phase, it is possible that the random sampling method arranges the dialogue agent to learn more complex user goals first, and then learn simpler user goals.   issue:  using random user goals to collect experience online is not stable enough, making the learned dialogue policy unstable and difficult to reproduce. Since RL is highly sensitive to the dynamics of the training process, dialogue agents trained with stable experience can guide themselves more effectively and stably than dialogue agents trained with instability.     Most previous studies of dialogue policy have focused on the efficiency issue, such as reward shaping , companion learning , incorporate planning , etc. However, stability is a pre-requisite for the method to work well in real-world scenarios. It is because, no matter how effective an algorithm is, an unstable online leaned policy may be ineffective when applied in the real dialogue environment. This can lead to bad user experience and thus fail to attract sufficient real users to continuously improve the policy. As far as we know, little work has been reported about the stability of dialogue policy. Therefore, it is essential to address the stability issue.    In this paper,  we propose a novel policy learning framework that combines curriculum learning and deep reinforcement learning,  namely Automatic Curriculum Learning-based Deep Q-Network . As shown in Figure, this framework replaces the traditional random sampling method in the user simulator with a teacher policy model that arranges a meaningful ordered curriculum and dynamically adjusts it to help dialogue agent  for automatic curriculum learning. As a scheduling controller for student agents, the teacher policy model arranges students to learn different user goals in different learning stages without any requirement of prior knowledge. Sampling the user goals that match the ability of student agents regarding different difficulty of each user goal, can not only increases the feedback of the environment to the student agent but also makes the learning of the student agent more stable.  There are two criteria for evaluating the sampling order of each user goal: the learning progress of the student agent and the over-repetition penalty. The learning progress of the student agent emphasizes the efficiency of each user goal, encouraging the teacher policy model to choose the user goals that match the ability of the student agent to maximize the learning efficiency of the student agent. The over-repetition penalty emphasizes the sampled diversity, preventing the teacher policy model from cheating\footnote[1]{The teacher policy model repeatedly selects user goals that the student agent has mastered to obtain positive rewards.}. The incorporation of the learning progress of the student agent and the over-repetition penalty reflects both sampled efficiency and sampled diversity to improve efficiency as well as stability of ACL-DQN.   Additionally, the proposed ACL-DQN framework can equip with different curriculum schedules. Hence, in order to verify the generalization of the proposed framework, we propose three curriculum schedule standards for the framework for experimentation: i) Curriculum schedule A: there is no standard, only a single teacher model; ii) Curriculum schedule B: user goals are sampled from easiness to hardness in proportion; iii) Curriculum schedule C: ensure that the student agents have mastered simpler goals before learning more complex goals.   Experiments have demonstrated that the ACL-DQN significantly improves the dialogue policy through automatic curriculum learning and achieves better and more stable performance than DQN. Moreover, the ACL-DQN equipped with the curriculum schedules can be further improved. Among the three curriculum schedules we provided, the ACL-DQN under curriculum schedule C with the strength of supervision and controllability, can better follow up on the learning progress of students and performs best. In summary, our contributions are as follows:         In this paper, we propose a novel framework, Automatic Curriculum Learning-based Deep Q-Network , to innovatively integrate curriculum learning and deep reinforcement learning in dialogue policy learning. We design a teacher model that automatically arranges and adjusts the sampling order of user goals without any requirement of prior knowledge to replace the traditional random sampling method in user simulators. Sampling the user goals that match the ability of student agents regarding the difficulty of each user goal, maximizes and stabilizes student agents learning progress. The learning progress of the student agent and the over-repetition penalty as the criteria of the sampling order of each user goal, guarantee both of the sampled efficiency and diversity. The experimental results demonstrate the efficiency and stability of the proposed ACL-DQN. Besides, the proposed method has strong generalizability, because it can be further improved by equipping with curriculum schedules. In the future, we plan to explore the factors in the curriculum schedules that have a pivotal impact on dialogue policy learning, and evaluate the efficiency and stability of our approach by adopting different types of curriculum schedules.     
"," Dialogue policy learning based on reinforcement learning is difficult to be applied to real users to train dialogue agents from scratch because of the high cost. User simulators, which choose random user goals for the dialogue agent to train on, have been considered as an affordable substitute for real users. However, this random sampling method ignores the law of human learning, making the learned dialogue policy inefficient and unstable. We propose a novel framework, Automatic Curriculum Learning-based Deep Q-Network , which replaces the traditional random sampling method with a teacher policy model to realize the dialogue policy for automatic curriculum learning. The teacher model arranges a meaningful ordered curriculum and automatically adjusts it by monitoring the learning progress of the dialogue agent and the over-repetition penalty without any requirement of prior knowledge. The learning progress of the dialogue agent reflects the relationship between the dialogue agent's ability and the sampled goals' difficulty for sample efficiency. The over-repetition penalty guarantees the sampled diversity. Experiments show that the ACL-DQN significantly improves the effectiveness and stability of dialogue tasks with a statistically significant margin. Furthermore, the framework can be further improved by equipping with different curriculum schedules, which demonstrates that the framework has strong generalizability.",447
"  Sentence embeddings map sentences into a vector space. The vectors capture rich semantic information that can be used to measure semantic textual similarity~ between sentences or train classifiers for a broad range of downstream tasks~. State-of-the-art models are usually trained on supervised tasks such as natural language inference~, or with semi-structured data like question-answer pairs~ and translation pairs~. However, labeled and semi-structured data are difficult and expensive to obtain, making it hard to cover many domains and languages. Conversely, recent efforts to improve language models include the development of masked language model  pre-training from large scale unlabeled corpora .  While internal MLM model representations are helpful when fine-tuning on downstream tasks, they do not directly produce good sentence representations, without further supervised  or semi-structured  fine-tuning.  In this paper, we explore an unsupervised approach, called Conditional Masked Language Modeling , to effectively learn sentence representations from large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on sentence level representations produced by adjacent sentences. The model therefore needs to learn effective sentence representations in order to perform good MLM. Since CMLM is fully unsupervised, it can be easily extended to new languages. We explore CMLM for both English and multilingual sentence embeddings for 100+ languages.  Our English CMLM model achieves state-of-the-art performance on SentEval~, even outperforming models learned using supervised signals. Moreover, models training on the English Amazon review data using our multilingual vectors exhibit strong multilingual transfer performance on translations of the Amazon review evaluation data to French, German and Japanese, outperforming existing multilingual sentence embedding models by  for non-English languages and by  on the original English data.   We further extend the multilingual CMLM to co-train with parallel text  retrieval task, and finetune with cross-lingual natural language inference  data, inspired by the success of prior work on multitask sentence representation learning~ and NLI learning~. We achieve performance  better than the previous state-of-the-art multilingual sentence representation model . Language agnostic representations require semantically similar cross-lingual pairs to be closer in representation space than unrelated same-language pairs~. While we find our original sentence embeddings do have a bias for same language sentences, we discover that removing the first few principal components of the embeddings eliminates the self language bias.  The rest of the paper is organized as follows. \Cref{sec:cmlm} describes the architecture for CMLM unsupervised learning. In \Cref{sec:en_cmlm} we present CMLM trained on English data and evaluation results on SentEval. In \Cref{sec:en_cmlm} we apply CMLM to learn sentence multilingual sentence representations. Multitask training strategies on how to effectively combining CMLM, bitext retrieval and cross lingual NLI finetuning are explored. In \Cref{sec:analysis}, we investigate self language bias in multilingual representations and how to eliminate it.  The contributions of this paper can be summarized as follows:  A novel pre-training technique CMLM for unsupervised sentence representation learning on unlabeled corpora .  An effective multitask training framework, which combines unsupervised learning task CMLM with supervised learning Bitext Retrieval and cross-lingual NLI finetuning.  An evaluation benchmark for multilingual sentence representations.  A simple and effective algebraic method to remove same language bias in multilingual representations. The pre-trained models are released at \url{https://tfhub.dev/s?q=universal-sentence-encoder-cmlm}.     We present a novel sentence representation learning method Conditional Masked Language Modeling  for training on large scale unlabeled corpus. CMLM outperforms the previous state-of-the-art English sentence embeddings models, including those trained with supervised signals. For multilingual representations learning, we discover that co-training CMLM with bitext retrieval and cross-lingual NLI finetuning achieves state-of-the-art performance. We also discover that multilingual representations have the same language bias and principal component removal  can eliminate the bias by separating language identity information from semantics.  
"," This paper presents a novel training method, Conditional Masked Language Modeling , to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval~, even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval~ and natural language inference~ tasks outperforms the previous state-of-the-art multilingual models by a large margin. We explore the same language bias of the learned representations, and propose a principle component based approach to remove the language identifying information from the representation while still retaining sentence semantics.",448
" Many seemingly convincing rumors such as ``Most humans only use 10 percent of their brain'' are widely spread, but ordinary people are not able to rigorously verify them by searching for scientific literature. In fact, it is not a trivial task to verify a scientific claim by providing supporting or refuting evidence rationales, even for domain experts.  %Such The situation worsens as misinformation is proliferated  %by the  on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes more and more crucial for combating  %against  the spread of misinformation.  %There are many existing datasets and %the corresponding  %systems for fact-verification tasks %, emphasizing on  %in various domains, such as Wikipedia , social media , and politics . These tasks  %are  The existing fact-verification tasks usually consist of three sub-tasks: document retrieval, rationale sentence extraction, and fact-verification. However, due to the nature of scientific literature that requires domain knowledge, it is challenging to collect a large scale scientific fact-verification dataset, and further, to perform fact-verification under a low-resource setting with limited training data.  collected a scientific claim-verification dataset, SciFact, and proposed a scientific claim-verification task: given a scientific claim, find evidence sentences that support or refute  %such the claim  %from  in a corpus of scientific paper abstracts.  also proposed a simple, pipeline-based, sentence-level model, VeriSci, as a baseline solution based on .  %Despite the simplicity of VeriSci ,  VeriSci is a pipeline model that runs modules for abstract retrieval, rationale sentence selection, and stance prediction sequentially, and thus the error generated from  %the an upstream module may propagate to the downstream modules. To overcome this drawback, we hypothesize that a module jointly optimized on multiple sub-tasks may mitigate the error-propagation problem to improve the overall performance.  %On the other hand,  In addition, we observe that a complete set of rationale sentences usually contains multiple inter-related sentences from the same paragraph. Therefore, we propose a novel, paragraph-level, multi-task learning model for the SciFact task.  In this work, we employ compact paragraph encoding, a novel strategy of computing sentence representations using BERT-family models. We directly feed an entire paragraph as a single sequence to BERT, so that the encoded sentence representations are already contextualized on the neighbor sentences by taking advantage of the attention mechanisms in BERT. In addition, we jointly train the modules for rationale selection and stance prediction as multi-task learning  by leveraging the confidence score of rationale selection as the attention weight of the stance prediction module. Furthermore, we compare two methods of transfer learning that mitigate the low-resource issue: pre-training and domain adaptation . Our experiments show that: % the compact paragraph encoding method is beneficial over separately computing sentence embeddings, and  with negative sampling, the joint training of rationale selection and stance prediction is beneficial over the pipeline solution. %\todo{you may want to create a list of contribution. -Violet}  [leftmargin=*]      method is beneficial over separately computing sentence embeddings.         In this work, we propose a novel paragraph-level multi-task learning model for  task. Experiments show that  The compact paragraph encoding method is beneficial over separately computing sentence embeddings.  With negative sampling, the joint training of  and  is beneficial over the pipeline solution.  
"," Even for domain experts, it is a non-trivial task to verify a scientific claim by providing supporting or refuting evidence rationales. The situation worsens as misinformation is proliferated on social media or news websites, manually or programmatically, at every moment. As a result, an automatic fact-verification tool becomes crucial for combating the spread of misinformation. % collected a scientific claim-verification dataset, SciFact, to facilitate research on scientific claim-verification.  In this work, we propose a novel, paragraph-level, multi-task learning model for the SciFact task by directly computing a sequence of contextualized sentence embeddings from a BERT model and jointly training the model on rationale selection and stance prediction.",449
" Self attention networks  have been widely studied on many natural language processing  tasks, such as machine translation , language modeling  and natural language inference . It is well accepted that SANs can leverage both the local and long-term dependencies through the attention mechanism, and are highly parallelizable thanks to their position-independent modeling method.  However, such position-independent models are incapable of explicitly capturing the boundaries between sequences of words, thus overlook the structure information that has been proven to be robust inductive biases for modeling texts . Unlike RNNs that model sequential structure information of words by using memory cells, or CNNs that focus on learning local structure dependency of words via convolution kernels, SANs learn flexible structural information in an indirect way almost from scratch. One way to integrate structural information into SAN models is via pre-training, such as BERT , which learns to represent sentences by using unsupervised learning tasks on the large-scale corpus. Recent studies  have shown the ability of pre-training models on capturing structure information of sentences.  Another method to deal with structural information is introducing structure priors into SANs by mask strategies.   proposed the directional self-attention mechanism, which employs two SANs with the forward and backward masks respectively to encode temporal order information.   introduced the Gaussian prior to the transformers for capturing local compositionality of words. Admittedly, structure priors can strengthen the model's capability of modeling sentences and meanwhile assist in capturing proper dependencies. With the help of these learned structure priors, SANs can model sentences accurately even in resource-constrained conditions. [!t] 	 	    Though these models get success on many NLP tasks, these studies commonly focus on integrating one single type of structure priors into SANs, thus fail at making full use of multi-head attentions. One straightforward advantage of using the multi-head attentions lies in the fact that different heads convey different views of texts . In other words, multi-head attentions enable the model to capture the information of texts at multiple aspects, which in return brings thorough views when modeling the texts.  Besides, it is well accepted that one type of structural prior can only reveal part of the structural information from one single perspective. A variety of types of structural priors are needed in order to gain complete structural information of texts. This can be achieved by introducing different structural priors into different parts of attention heads, where different structural priors can complement each other, guiding the SAN models to learn proper dependencies between words. Therefore, to gain a better representation of the texts, a desirable solution should make full use of the multi-head attention mechanism and utilize multiple types of structural priors.  To better alleviate the aforementioned problems, in this paper, we propose a lightweight self attention network, i.e., the Multiple Structural Priors Guided Self Attention Network . The novel idea behind our model lies in the usage of the multi-mask based multi-head attention , which helps our model to better capture different types of dependencies between texts. Thanks to the MM-MH Attention mechanism, our model can capture multiple structural priors, which in return brings benefits in modeling sentences.  Especially, the structural priors we employed come from two categories: the sequential order and the relative position of words. Since the standard SANs are incapable of distinguishing the order between words, we apply the direction mask  directly to each attention head. Motivated by the Bidirectional RNNs , we split the attention heads into two parts. For a given word, we apply the forward mask to the first half of attention heads, which allows it to attend on only the previous words when modeling the reference word. Accordingly, the backward mask is applied to the rest of the attention heads.  Since the direction masks take no consideration of the difference between long-distance words and nearby words, we employ the second category of structural prior as a complement, which could be measured by the distance between pair of words. We integrate two types of distance masks into different attention heads. The first one we utilized is the word distance mask, which describes the physical distance between each pair of words. Besides, for the purpose of capturing the latent hierarchical structure of sentences, we integrate another kind of distance information, i.e., dependency distance that is defined as the distance between each pair of words on a dependency syntax tree. The word distance mask helps our model to focus on the local words and the dependency distance mask enables our model to capture the hierarchical relationships between words. Consequently, they provide our model the ability of capturing the local and non-local dependency of words properly.  To illustrate the effectiveness of our model, we conduct experiments on two NLP tasks: natural language inference and sentiment classification. Experimental results show that MS-SAN outperforms other baselines and achieves a competitive performance comparing with the state-of-the-art models.   Our contributions are listed as follows:      We propose the Multiple Structural Priors guided Self Attention Network , which utilize multiple types of structural priors to model texts. By applying the multi-mask strategy, we can take full use of the multi-head attention to capture the information of texts at multiple aspects. We introduce two categories of structural priors into the MS-SAN, containing the sequential order and the relative position of words. Thanks to these priors, MS-SAN gains better understanding of the texts with a stronger ability of modeling latent sentence structure. Experiments on the NLI and SC tasks also demonstrate the effectiveness of our model. In the future, we will try to combine relational position embeddings with the structural priors together for modeling sentence structures better. Besides, we will try to introduce structural priors into pre-trained models for model reduction.      The file named.bst is a bibliography style file for BibTeX 0.99c   
"," Self attention networks  have been widely utilized in recent NLP studies. Unlike CNNs or RNNs, standard SANs are usually position-independent, and thus are incapable of capturing the structural priors between sequences of words. Existing studies commonly apply one single mask strategy on SANs for incorporating structural priors while failing at modeling more abundant structural information of texts. In this paper, we aim at introducing multiple types of structural priors into SAN models, proposing the Multiple Structural Priors Guided Self Attention Network  that transforms different structural priors into different attention heads by using a novel multi-mask based multi-head attention mechanism. In particular, we integrate two categories of structural priors, including the sequential order and the relative position of words. For the purpose of capturing the latent hierarchical structure of the texts, we extract these information not only from the word contexts but also from the dependency syntax trees. Experimental results on two tasks show that MS-SAN achieves significant improvements against other strong baselines.",450
"  Building intelligent conversation systems is a long-standing goal of artificial intelligence and has attracted much attention in recent years . A central challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a pool of candidate responses .  [tb]  \resizebox{0.95{ {|l|}  Dialogue Context Between Two Speakers A and B \\  A: Would you please recommend me a good TV series \\ to watch during my spare time?  \\ B: Absolutely! Which kind of TV series are you most\\ interested in?  \\ A: My favorite type is fantasy drama. \\ B: I think both Game of Thrones and The Vampire \\ Diaries are good choices.  \\ }  \\   \\ P1: Awesome, I believe both of them are great TV \\ series! I will first watch Game of Thrones. \; \\  %Difficult  \\ %P2: Agree! I am a huge fan of Jon Snow. \\ P2: Cool! I think I find the perfect things to kill my\\ \; weekends. \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \; \;\\ }  \\  \\ N1: This restaurant is very expensive. \; \; \; \; \; \; \\  %Difficult  \\ %R2: Thanks very much for your advice! & \\ % R2: Modern Family is an American TV sitcom. & \\ N2: Iain Glen played Ser Jorah Mormont in the HBO \\ \; fantasy series Game of Thrones. \; \; \; \; \;\\   }        To tackle the response selection problem, different matching models are developed to measure the matching degree between a conversation context and a response candidate . Despite their differences, most prior works train the matching models with training data constructed by a simple heuristic. For each dialogue context, the human-written response is considered as positive  and the responses from other dialogue contexts are considered as negative . In practice, the negative responses are often randomly sampled and the training objective is to ensure that the positive responses score higher than the negative ones.  Recently, some researchers  has raised the concern that randomly sampled negative responses are often too trivial . Models trained with such negative data lacks the ability to handle strong distractors during testing. In general, the problem stems from the ignorance of the diversity in context-response matching; all random responses are treated as equally negative regardless of their distracting strength. For example, in Table , two negative responses  are presented. For N1, one can easily dispel its legality %as there is no lexical overlap and off-topic semantic meaning.  as it does not follow the topic discussed in the dialogue context. %as its semantic meaning  is obviously off the topic ,  On the other hand, judging a strong distractor like N2 can be difficult as its content overlaps significantly with the context . Only with close observation, %we find that N2 does not properly reply the context . %the semantic incoherence between the context and N2 can be spotted.  we find that N2 does not strongly maintain the coherence of the discussion, i.e., it starts a parallel discussion about an actor in Game of Thrones rather than elaborating on the enjoyable properties of the TV series. %Similarly, we can observe the same phenomena on the positive side.  Similarly, the positive side has the same phenomena. For the positive response P1, one can easily confirm its legality as it naturally replies the context. As for P2, while it expatiates on the enjoyable properties of the TV series, it doesn't exhibit any obvious matching clues, such as lexical overlap with the context. %share any lexical overlap with the context.  Thus, to correctly identify P2, the relationship between P2 and the context has to be carefully reasoned by the model.  %Game of Thrones, the character name Jon Snow  has not appeared in the context. Thus, to correctly identify it, the relationship between Jon Snow and Game of Thrones has to be carefully reasoned by the model.  To conclude, the above observations suggest that, to accurately recognize different positive and negative responses, the model is required to possess different levels of discriminative capability.  %require different levels of model capability to accurately recognize. %\textcolor{red}{brandenwang: since the difficulty of random sampled negative responses is diverse. Besides, we observe that such diversity also applies to the positive responses: in some cases, the relationship between context and response is explicit and easy to identify, while in others, it is difficult to find the implicit relationship between the context and response. These two kinds of diversity may result in an unstable training process and poor accuracy in real-world applications.}  %Motivated by the intuition that one should first learn to deal with easy cases before handling harder ones, we propose to employ the idea of curriculum learning   to tackle the task of response selection.   Inspired by the aforementioned observations, we propose to employ the idea of curriculum learning   for a better learning of response selection models.  %to better learn matching models for response selection.  CL is reminiscent of the cognitive process of human being, the core idea is first learning easier concepts and then gradually transitioning to learning more complex concepts based on some pre-defined learning schemes. %.  In various NLP tasks ), CL has demonstrated its benefit in improving the model performance as well as the learning convergence.%, such as , leading to improved model performance as well as faster learning convergence.%better generalization.  %which has been successfully applied to many machine learning tasks . The core idea of CL is first learning easier concepts and then gradually transitioning to learning more complex concepts.  %.  The key to applying CL is to specify an appropriate learning scheme under which all training examples are gradually learned %. . In this work, we tailor-design a hierarchical curriculum learning  framework according to the characteristics of the concerned response selection task. Our HCL framework consists of two complementary curriculum strategies, namely corpus-level curriculum  and instance-level curriculum , covering the two distinct aspects of response selection. Specifically, in CC, the model gradually increases its ability in finding matching clues between the context and the positive response. As for IC, it progressively strengthens the model's ability in identifying the mismatch information between the context and negative responses. To order all positive and negative examples, we need to assess millions of possible context-response combinations in the training data. To overcome this computational challenge, we propose to use a fast neural ranking model to assign learning priorities to all training examples based on their pairwise context-response similarity score.   Notably, our proposed learning framework is independent to the choice of matching models. % and it can be conveniently implemented without any additional modelling effort . Therefore, for a comprehensive evaluation, we test our approach with three representative matching models, including the latest advance brought by pre-trained language models. Results on two benchmark datasets demonstrate that the proposed learning framework leads to remarkable performance improvement across all evaluation metrics.   In summary, our contributions are:  We propose a new hierarchical curriculum learning framework to tackle the task of response selection; % We design a decomposable neural model which works coherently with the proposed learning framework; and  and  Experimental results on two benchmark datasets demonstrate that our approach can significantly improve the performance of strong matching models, including the state-of-the-art one.  [t]  	{3pt}  	 denotes the positive response). For each training instance, we show three associated negative responses  whose difficulty level increase from the bottom to the top. In the negative examples, the words that also appear in the dialogue context are marked as italic.} %	           In this work, we propose a novel hierarchical curriculum learning framework for training response selection models for multi-turn conversations. During training, the proposed framework simultaneously employs the corpus-level and instance-level curriculum to dynamically select suitable training data based on the state of learning process. Extensive experiments and analysis on two benchmark datasets show that our approach can significantly improve the performance of various strong matching models.   To test our approach, we conduct extensive experiments and analysis using three representative matching models. The results on two benchmark datasets demonstrate the effectiveness of the proposed approach.   Experimental results on two benchmark datasets using three representative matching models verify the effectiveness of the proposed approach.   
"," We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that random negatives are often too trivial to train a reliable model, we propose a hierarchical curriculum learning  framework that consists of two complementary curricula: % Motivated by the idea of curriculum learning, we propose a new hierarchical curriculum learning framework which consists of two curriculum strategies:  corpus-level curriculum ; and  instance-level curriculum . In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response. On the other hand, IC progressively strengthens the model's ability in identifying the mismatched information between the dialogue context and a response. Empirical studies on two benchmark datasets with three state-of-the-art matching models demonstrate that the proposed HCL significantly improves the model performance across various evaluation metrics\footnote{All data, code and models are made publicly available at https://github.com/yxuansu/HCL/.}.",451
" Sequence-to-Sequence  learning~ has advanced the state of the art in various natural language processing  tasks, such as machine translation~, text summarization~, and grammatical error correction~. Seq2Seq models are generally implemented with an encoder-decoder framework, in which a multi-layer encoder summarizes a source sequence into a sequence of representation and another multi-layer decoder produces the target sequence conditioned on the encoded representation.   Recent studies reveal that fusing the intermediate encoder layers  is beneficial for Seq2Seq models, such as layer attention~, layer aggregation~, and layer-wise coordination~. Despite its effectiveness, not much is known about how fusing encoder layer representations work. The intuitive explanation is that fusing encoder layers exploits surface and syntactic information embedded in the lower encoder layers~.  However, other studies show that attending to lower encoder layers  does not improve model performance~, which is conflicted with existing conclusions. It is still unclear why and when fusing encoder layers should work in Seq2Seq models.  This paper tries to shed light upon behavior of Seq2Seq models augmented with EncoderFusion method. To this end, we propose a novel {.  Based on this observation, we simplify the EncoderFusion approaches by only connecting the encoder embedding layer to softmax layer . The SurfaceFusion approach shortens the path distance between source and target embeddings, which can help to learn better bilingual embeddings with direct interactions. Experimental results on several Seq2Seq NLP tasks show that our method consistently outperforms both the vanilla Seq2Seq model and the layer attention model.  Extensive analyses reveal that our approach produces more aligned bilingual word embeddings by shortening the path distance between them, which confirm our claim.  Our main contributions are as follows:  %       In this paper, we investigate how encoder layer fusion works on solving the source representation bottleneck. Based on a series of experiments on different Seq2Seq tasks, we find that the encoder embedding layer is important to the success of EncoderFusion by exploiting the useful surface information. Based on this observation, we propose a novel SurfaceFusion to directly connect the encoder embedding layer and softmax layer.   Experiments show that SurfaceFusion consistently outperforms the conventional EncoderFusion in several datasets. Extensive analyses reveal that SurfaceFusion enhances the learning of expressive bilingual word embeddings for Seq2Seq models, which confirm our claim.  Future directions include validating our findings on more Seq2Seq tasks  and model architectures . It is also worthwhile to explore more alternatives to EncoderFusion from the perspective of exploiting the embedding layer.   
"," Encoder layer fusion  is a technique to fuse all the encoder layers  for sequence-to-sequence  models, which has proven effective on various NLP tasks. However, it is still not entirely clear why and when EncoderFusion should work. In this paper, our main contribution is to take a step further in understanding EncoderFusion. Many of previous studies believe that the success of EncoderFusion comes from exploiting surface and syntactic information embedded in lower encoder layers. Unlike them, we find that the encoder embedding layer is more important than other intermediate encoder layers.  In addition, the uppermost decoder layer consistently pays more attention to the encoder embedding layer across NLP tasks. Based on this observation, we propose a simple fusion method, SurfaceFusion, by fusing only the encoder embedding layer for the softmax layer. Experimental results show that SurfaceFusion outperforms EncoderFusion on several NLP benchmarks, including machine translation, text summarization, and grammatical error correction.   It obtains the state-of-the-art performance on WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive analyses reveal that SurfaceFusion learns more expressive bilingual word embeddings by building a closer relationship between relevant source and target embeddings. Source code is freely available at \url{https://github.com/SunbowLiu/SurfaceFusion}.  . Recent studies have found that widening the bottleneck by fusing the surface features from lower level representations can boost the performance of Seq2Seq, but none of them explain the intrinsic mechanism of this benefit. In this paper, we take the first step to probe into the essence of the bottleneck on three typical Seq2Seq tasks, i.e.~machine translation, text summarization, and grammatical error correction. We observe that the representation learning of higher decoder layer suffers from the bottleneck, and thus propose a simple yet effective surface fusion method to mitigate the issue. The results over a variety of benchmarks confirm the effectiveness of the proposed method. Source code will be released. \fi",452
"  Word segmentation is a fundamental and challenging task in text classification and other NLP applications. Word segmenter determines the boundaries of words in the shape of beginning and ending. It has been largely investigated in many space-delimited languages including English, Arabic, Urdu and non-space delimited languages including Chinese, Japanese, and Burmese . However, the word segmentation in low-resource Sindhi language has not been studied well, mainly due to the lack of language resources.   Sindhi word segmentation exhibits the space omission and space insertion  problems. Although, the white spaces between words are a good sign for predicting word boundaries,  the space omission and space insertion between words bring ambiguity in the segmentation process. Therefore, the SWS task is a challenging problem because of resource scarcity, lack of standard segmentation benchmark corpus, and rich morphological features in Sindhi language. Previously, little work has been proposed to address the SWS problem by employing dictionary-based and rule-based approaches. Thus, the existing approaches lack the applicability towards open-source implementation due to following reasons,  inability to deal with out-of-vocabulary words,  less robust on the large datasets, and  lower segmentation accuracy. Our proposed novel deep SGNWS model has the capability of dealing with such issues for SWS with the Subword Representation Learning  approach.   Recently, deep neural architectures have largely gained popularity in NLP community by greatly simplifying the learning and decoding in a number of NLP applications including word segmentation with neural word embedding and powerful recurrent neural architectures. More recently, self-attention has also become a  popular approach to boost the performance of neural models. Therefore, we tackle the SWS problem by taking advantage of BiLSTM, self-attention, SRL, and CRF without relying on external feature engineering.  In this paper, we propose a language-independent neural word segmentation model for Sindhi. The proposed model efficiently captures the character-level information with subword representation learning. We convert segmentation into a sequence tagging problem using B, I, E, S, X tagging scheme. Where B denotes [Beginning], I [Inside], E [Ending] of a word in the given corpus, S [Single] is used for the tagging of a single or special character in the unlabeled text, and X tag is used for [hard-space] between words. We train task-oriented Sindhi word representations with character-level subword approach. To the best of our knowledge, this is the first attempt to tackle SWS as a sequence labeling task. We provide the open-source implementation for further investigation\footnote{https://github.com/AliWazir/Neural-Sindhi-word-segmenter}. Our novel contributions are listed as follows:        % The remaining parts of the paper are organized in the following sequence;  Section  presents the related work on SWS and its morphology, the evolution and usage of Recurrent Neural Networks  and its variants of LSTM, BiLSTM and GRU in the text segmentation in various languages. Section  presents an overview of Sindhi writing system and segmentation challenges, followed by the proposed methodology in Section  where RNN variants are employed for our task. Section  presents the experiments and results analysis. Lastly, Section  concludes this paper.    The word segmentation is an essential and non-trivial task in Sindhi language. The white spaces between words are a good sign for predicting word boundaries, but the existence of space-omission and space-insertion bring ambiguity in the segmentation process. We proposed the SGNWS model, keeping in view the challenges related to SWS, respectively. The proposed model has the ability to learn and extract subword features automatically by eliminating the constraints such as hand-craft features for segmentation or any other type of prior domain-specific knowledge.    in this paper, we propose a deep BiLSTM-CRF based framework with subword representation learning. The novel character-level  For that task, we construct five benchmark datasets and empirically analyze proposed SGNWS and the chosen baselines approaches. The proposed model also surpases existing Sindhi word segmenters by achieving high F-Score of 98.13\ , 97.62\  on developed benchmark datasets Awami-Awaz, 96.26\  on Wiki-dumps, 97.37\  on twitter, 97.93\  on books corpus, and best F-Score of 98.51\  on the SGSEG dataset. The performance of Wiki-dumps is comparatively lesser due to the existence of noise in the text.    In this paper, we empirically demonstrate that our proposed model yields the best performance in SWS because of its high efficiency and robustness for sequential modeling tasks with great ability to capture the word information at the morphemic level for the prediction of word boundaries. The SGNWS model is an effective and elegant neural solution for SWS, which can also be applied to other sequence tagging problems.    
"," Deep neural networks employ multiple processing layers for learning text representations to alleviate the burden of manual feature engineering in Natural Language Processing . Such text representations are widely used to extract features from unlabeled data. The word segmentation is a fundamental and inevitable prerequisite for many languages. Sindhi is an under-resourced language, whose segmentation is challenging as it exhibits space omission, space insertion issues, and lacks the labeled corpus for segmentation.  In this paper, we investigate supervised Sindhi Word Segmentation  using unlabeled data with a Subword Guided Neural Word Segmenter  for Sindhi. In order to learn text representations, we incorporate subword representations to recurrent neural architecture to capture word information at morphemic-level, which takes advantage of Bidirectional Long-Short Term Memory , self-attention mechanism, and Conditional Random Field .  Our proposed SGNWS model achieves an F1 value of  98.51\% without relying on feature engineering. The empirical results demonstrate the benefits of the proposed model over the existing Sindhi word segmenters.   % , such as dictionaries, morphological analyzers, or rules, for the Sindhi word segmentation. The conducted extensive empirical study demonstrates the benefits of the proposed model over the existing Sindhi word segmenters and state-of-the-art deep learning approaches.",453
"   Indonesian colloquialism is everyday and everywhere, e.g. in social media posts and conversational transcripts. Yet, existing research on Indonesian NLP models including NMTs often disregards qualitative analysis when the models are given strictly colloquial inputs. This is mainly due to the fact that the data readily available for training and testing the models are in formal Indonesian. %This follow naturally due to the fact that the models are style-agnostic, that is,   Colloquial Indonesian has several different word choices from formal language due to the diversity of regional languages and dialects. We define the spoken colloquial as a clean colloquial. In addition, in written media,  colloquial Indonesian is often abbreviated, disemvoweled, or written with voice alteration, which we define as the noisy colloquial .    [ht!]    }           & Example \\          {Ayo} \textcolor{red}{bertemu} \textcolor{orange}{dengan} \textcolor{purple}{pak} Ridho \\          ~Google Translate & Let's meet Pak Ridho \\           ~Gold-standard & Let's meet Mr. Ridho \\ \rule{0pt}{3.5ex}          Clean colloquial & \textcolor{blue}{kuy} \textcolor{red}{ketemu} \textcolor{orange}{sama} \textcolor{purple}{pak} Ridho \\          ~Google Translate & kuy met Pak Ridho \\ \rule{0pt}{3.5ex}          Noisy colloquial & \textcolor{blue}{kuy} \textcolor{red}{ktemu} \textcolor{orange}{sm} \textcolor{purple}{pk} ridho \\          ~Google Translate & I met you at happy time \\           from Malang, \textcolor{red}{bertemu-ketemu}, \textcolor{orange}{dengan-sama} from Betawi). Social-media text introduces additional typographical noise, such as diemvowelling .}         To better evaluate English-Indonesian MT systems against colloquial text, we first create 2 new test-sets of Indonesian-English colloquial pairs. The first test is a clean colloquial taken from a YouTube transcript. The second test-set is a noisy colloquial from Twitter annotated by our team of annotators. We found that NMT systems trained on formal dataset did not perform very well on these test-sets.  Next, we develop synthetic colloquial text data by performing word-level translation of several words in the formal text into a colloquial form based on a word-to-word dictionary. By combining the formal dataset and the synthesized colloquial dataset, we increase the NMT performance on the colloquial test-set by 2.5 BLEU points.      We investigated data augmentation methods to improve NMT's translation capabilities by converting formal Indonesian data to colloquial and adding he experimental results it to our dataset. With several test schemes show an increase in the translation ability of the models that have been made. Determining how many words to covert into colloquial Indonesian is important. To the best of our knowledge, we are the first to perform a data augmentation method for Colloquial Indonesian at NMT.  
","  Neural machine translation  is typically domain-dependent and style-dependent, and it requires lots of training data. State-of-the-art NMT models often fall short in handling colloquial variations of its source language and the lack of parallel data in this regard is a challenging hurdle in systematically improving the existing models. In this work, we develop a novel colloquial Indonesian-English test-set collected from YouTube transcript and Twitter. We perform synthetic style augmentation to the source formal Indonesian language and show that it improves the baseline Id-En models  over the new test data. %Our experimental data and code are available on github.com.",454
"  Large-scale language models have greatly advanced NLP research in various sub-areas, such as question answering, text summarization, story generation and so on . However, these generation models still suffer from at least three major problems when applied to the dialogue system building, 1) generic and repeated responses ,   2) inconsistent statements with the dialogue context , and 3) uncontrollable task-oblivious replies  .  Many previous studies have attempted to address these problems . For instance,  penalized repetitive and inconsistent behaviors with unlikelihood loss in open-domain chats.  detected and rewrote the contradicting responses to achieve a more consistent personality.  However, these methods optimize the language model by minimizing the loss in supervised learning, which may lead to exposure bias and uninterpretable behaviors, and consequently,  makes it harder for humans to regulate the model.   To alleviate these problems, previous work has explored RL-based methods in dialogue system building . %For instance,  integrated the goal of coherent into the reward design  and made the first step towards .designed for better generation.   However, such methods not only rely on hand-crafted user simulators that are inherently hard to build , but also require meaningful rewards that are difficult to design. To address these issues, we propose to teach the model to extract a policy directly from the data and learn from its own mistakes without the use of simulators. Leveraging decoding methods such as Nucleus Sampling , the language model finetuned on a persuasion task is able to generate lexically diverse response candidates given the same context. %One example is shown in Figure.  Some candidates are appropriate, while others are repetitive or inconsistent with the context. These good and bad examples are used as positive and negative feedback to the model through meaningful rewards in RL, and help refine the language model. During testing, to fully utilize the refined language model, we use it to generate multiple candidates again,  and filter out the repetition and inconsistency afterwards. Beyond being nonrepetitive and consistent, a good response also needs to accomplish the dialogue task, in our case, to persuade people. Therefore, we ask humans to demonstrate the persuasion process, and build a response imitator to imitate these human demonstrations and select the most persuasive response.  The above issues in language models are especially salient in complex strategic dialogue tasks such as persuasion and negotiation. These dialogues involve both a specific task goal and social contents to build rapport for better task completion, and therefore, have richer and more complicated language structures . Furthermore, due to their inherent similarity to task-oriented and open-domain dialogues, improvements made on these systems would also help in both dialogue settings. Therefore, we choose a strategic donation persuasion task  to perform our study, and conduct both automatic and human evaluations to evaluate our models.     This work  makes multiple contributions. First, we propose DialGAIL, an RL-based generative algorithm to refine MLE-based language models for dialogue  generation without the use of user simulators.  Second, we design an effective and practicable framework for strategic dialogue systems that achieves state-of-the-art performance on a complex persuasion task, with only small amount of human demonstration efforts.  %Such system achieves more diverse, consistent and fluent conversations with better persuasion outcomes on a complex persuasion task compared to the MLE-based baselines.   %a framework to automatically detect repetitive and inconsistent responses, and imitate human demonstration to select persuasive responses.  %Furthermore, experiments show that our model produces more diverse, consistent and fluent conversations with better persuasion outcomes on a complex persuasion task compared to the MLE-based baselines.  Previous dialogue research has mostly focused on pure task-oriented dialogues and pure social conversations; but looking forward, it becomes more and more important to pay attention to strategic dialogues that involves both task and social components. We sincerely hope this work could inspire more research and discussions on strategic dialogues in the community.   % how to refine the dialogue generation with limited amount of data? MLE fine-tuning woldn't work with the limited data % social content + a specific end-goal --> persuasionforgood. advance research in this area % how to easily get a usable lm without computational resources? % explore the possibility to apply GAIL in dialogue generation in a simple way  % the first to explore GAIL % raise more attention in persuasion in the community % with small amount of human demo % task-independent in repetition detection strengthen       The problems with repetition and inconsistency still persist in dialogue response generation.  Large-scale language models still suffer from repetition and inconsistency problems when applied to dialogue response generations.   Current large-scale language models still suffer from repetition and inconsistency when applied to dialogue response generation.  Current dialogue systems suffer from repetition and inconsistency.    The repetition and inconsistency problems still persist in dialogue response generation with large-scale language models.  Large-scale language models still suffer from repetition and inconsistency when applied to dialogue generation. To address the exposure bias issue in MLE, we propose DialGAIL to  refine the MLE-based language model and extract a policy directly from the data without user simulators by learning from its own mistakes.   by penalizing its own mistakes.  With the same context, the model  generates multiple response candidates, some of which are repetitive and inconsistent. These negative examples send feedback to the model via a reward function to reduce repetition and inconsistency.  Furthermore, we provide human demonstration for the model to imitate human persuasion activity and select the most persuasive candidate. Experiments show that our model achieves state-of-the-art performance in a complex persuasion task, and produces more diverse, consistent, and persuasive conversations with small amount of human efforts. Looking into the future, strategic dialogues with both task and social contents will become more and more important, and it is our sincere hope that this work could inspire more research and discussion in strategic dialogue tasks in the community.  besides being nonrepetitive and inconsistent, a good response also contributes to task success. To achieve this, we provide human demonstration for the model to imitate human persuasion activities. Our experiments show that our model performs better than the baselines on both automatic metrics and human evaluations, and produces more diverse and persuasive conversations.                                                                                    \clearpage   
"," Despite the recent success of large-scale language models on various downstream NLP tasks, the repetition and inconsistency problems still persist in dialogue response generation. Previous approaches have attempted to avoid repetition by penalizing the language model's undesirable behaviors in the loss function. However, these methods focus on token-level information and can lead to incoherent responses and uninterpretable behaviors. To alleviate these issues, we propose to apply reinforcement learning to refine an MLE-based language model without user simulators, and distill sentence-level information about repetition, inconsistency and task relevance through rewards. In addition, to better accomplish the dialogue task, the model learns from human demonstration to imitate intellectual activities such as persuasion, and selects the most persuasive responses. Experiments show that our model outperforms previous state-of-the-art dialogue models on both automatic metrics and human evaluation results on a donation persuasion task, and generates more diverse, consistent and persuasive conversations according to the user feedback.% We will release the code and data upon acceptance.",455
"   Large-scale pre-training has draw much attention in both the community of Compute Vision  and Natural Language Processing  due to its strong capability of generalization and efficient usage of large-scale data. Firstly in CV, a series of models were designed and pre-trained on the large-scale dataset ImageNet, such as AlexNet , VGG  and ResNet , which effectively improved the capability of image recognition for numerous tasks. Recent years have witnessed the burst of pre-training in NLP, such as BERT , RoBERTa , XLNet  and BART , which greatly improve the capability of language understanding and generation. However, the above researches towards the single-modal learning and can only be used in single-modal  scenarios. %which greatly restricts their ability to process multi-modal  information. In order to adapt to multi-modal scenarios, a series of multi-modal pre-training methods were proposed and pre-trained on the corpus of image-text pairs, such as ViLBERT , VisualBERT  and UNITER , which greatly improve the ability to process multi-modal information. However, these models can only utilize the limited corpus of image-text pairs and cannot be effectively adapted to single-modal scenarios . %Moreover, the size of the corpus of image-text pairs is very limited, and large scale of single-modal data can't be effectively utilized.     A smarter AI system should be able to process different modalities of information effectively. There are large scale of data in different modalities on the Web, mainly textual and visual information. The textual knowledge and the visual knowledge usually can enhance and complement with each other. As the example shown in Figure , it's difficult to answer the question correctly only with the visual information in the image.  However, if we connect the visual information to the textual information which describes the background of a baseball game, it's very easy to determine the correct answer. Also, the visual information can make it easier to understand the scene described by the text. The research in neuroscience by  reveals that the parts of the human brain responsible for vision can learn to process other kinds of information, including touch and sound. Inspired by the research, we propose to design a unified-modal architecture UNIMO which can process multi-scene and multi-modal data input, including textual, visual and vision-and-language data, as shown in Figure .  The greatest challenge to unify different modalities is to align and unify them into the same semantic space which are generalizable to different modalities of data. Existed cross-modal pre-training methods try to learn cross-modal representations based on only limited image-text pairs by simple image-text matching and masked language modeling . They can only learn specific representations for image-text pairs, which are not generalizable for single-modal scenarios. So their performance will drop dramatically when applied to language tasks . In this work, UNIMO learns visual representations and textual representations in similar ways, and unify them into the same semantic space via cross-modal contrastive learning  based on a large-scale corpus of image collections, text corpus and image-text pairs.  %Our unified-modal architecture can utilize large scale of image collections and text corpus, and align the visual and textual information into the same semantic space via cross-modal contrastive learning on image-text pairs. %Effectively utilizing large-scale of images and text corpus can improve the capability of vision and textual understanding respectively. UNIMO effectively utilizes the large-scale of text corpus and image collections to learn general textual and visual representations.  The CMCL aligns the visual representation and textual representation, and unifies them into the same semantic space based on image-text pairs. To facilitate different levels of semantic alignment between vision and language, we propose to utilize a series of text rewriting techniques to improve the diversity of cross-modal information. As shown in Figure , we utilize back-translation to generate several positive examples for an image-text pair. Also, to enhance the detail semantic alignment between text and image, we further parse the caption to scene graph  and randomly replace either the objects, attributes or relations in the caption to generate various negative samples. Sentence-level retrieval and replacement is also utilized to enhance the sentence-level alignment. In this way, our model can effectively unify different levels of visual and textual representations into the same semantic space.  The unified-modal architecture mainly has the following advantages compared with previous methods:         In this work, we propose a unified-modal pre-training architecture UNIMO, which can leverage large-scale of non-paired text corpus and image collections for cross-modal learning. Our model can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Based on the unified-modal architecture, the textual knowledge and visual knowledge can enhance each other in the unified semantic space. Our UNIMO model outperforms previous methods on both the multi-modal and single-modal downstream tasks. In the future, we will utilize larger scale of image collections and text corpus for unified-modal learning, and extend UNIMO to other modalities of data such as video, audio and so on.    
","  Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data  or limited multi-modal data . In this work, we propose a unified-modal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections can be utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning  is leveraged to align the textual and visual information into a unified semantic space over a corpus of image-text pairs. As the non-paired single-modal data is very rich, our model can utilize much larger scale of data to learn more generalizable representations. Moreover, the textual knowledge and visual knowledge can enhance each other in the unified semantic space. The experimental results show that UNIMO significantly improves the performance of several single-modal and multi-modal downstream tasks.",456
" %What is ToD Task-oriented dialogue systems  are the core technology of the current state-of-the-art smart assistant . These systems are either modularized, Natural Language Understanding , Dialogue State Tracking , Dialogue Policy  and Natural Language Generation , or end-to-end, where a single model implicitly learn how to issue APIs  and system responses .   % what is the current problem we are trying to solve These systems are often updated with new features based on the user needs, e.g., adding new slots and intents, or even completely new domains. However, existing dialogue models are trained with the assumption of having a fixed dataset at the beginning of the training, and they are not designed to add new domains and functionalities through time, without incurring the high cost of a whole system retraining. Therefore, the ability to acquire new knowledge continuously, a.k.a. Continual Learning , is crucial in the design of a dialogue system. Figure shows an high-level intuition of CL in ToDs.     In this setting the main challenge is catastrophic forgetting. This phenomena happens since there is a distributional shift between the tasks in the curriculum which leads to catastrophic forgetting the previously acquired knowledge. To overcome this challenge three kind of methods are usually deployed such as loss regularization, for avoiding to interfere with the previously learned task, rehearsal, which uses an episodic memory to recall previously learned tasks, and architectural, which adds task-specific parameters for each learned task. However, architectural methods are usually not considered as a baseline, especially in sequence-to-sequence generation tasks, because they usually require a further step during testing for selecting which parameter to use for the given task.    To the best our knowledge, Continual Learning  in task-oriented dialogue systems is mostly unexplored or it has been studied in specific settings  using only few tasks learned continuously. Given the importance of the task in the dialogue setting, we believe that a more comprehensive investigation is required, especially by comparing multiple settings and baselines. Therefore in this paper: [noitemsep]         In Section we introduce the basic concepts and notation used throughout the paper, for both task-oriented dialogue modelling and continual learning, in Section we introduce the proposed architectural CL method, in Section we describe datasets, baselines, evaluation metrics and experimental settings, and Section we describe the main findings of the paper.    % Based on our experimental results, we discovered that two technique are particularly effective, but they both have a linear cost with respect to the number of learned tasks. To elaborate, in rehearsal-based methods the number of samples stored in the episodic-memory grows linearly with the learned task, while instead in architectural methods the number of parameters grows linearly. Hence concluding that there is not an absolute best  when comparing different methods based on the resources needed, both in term of additional parameters or sample stored in memory.      In this paper, we proposed a benchmark for Continual Learning in task-oriented dialogue systems, with 37 tasks to be learned continuously on four settings such as Intent recognition, Dialogue State Tracking, Natural Language Generation, and end-to-end. Then, we implemented three different Continual Learning methodologies such as regularization, rehearsal and architectural. In the latter, we propose a simple yet effective methods based on residual adapters and uses an entropy-based classifier to select which adapter to use at testing time. Finally, we analyse the trade-off between performance, number-of-parameters, and episodic memories size of the evaluated baselines, unveiling a no-free lunch among this methods.         
"," Continual learning in task-oriented dialogue systems can allow us to add new domains and functionalities through time without incurring the high cost of a whole system retraining. In this paper, we propose a continual learning benchmark for task-oriented dialogue systems with 37 domains to be learned continuously in four settings, such as intent recognition, state tracking, natural language generation, and end-to-end. Moreover, we implement and compare multiple existing continual learning baselines, and we propose a simple yet effective architectural method based on residual adapters. Our experiments demonstrate that the proposed architectural method and a simple replay-based strategy perform comparably well but they both achieve inferior performance to the multi-task learning baseline, in where all the data are shown at once, showing that continual learning in task-oriented dialogue systems is a challenging task. Furthermore, we reveal several trade-off between different continual learning methods in term of parameter usage and memory size, which are important in the design of a task-oriented dialogue system. The proposed benchmark is released together with several baselines to promote more research in this direction.",457
" % Background: % What is MT, history of MT, and current state of MT % What is NMT, current state of NMT % Reason: % Sufficient and necessity condition for writing this article % Organization of this article   [!t]           Machine Translation  is an important task that aims to translate natural language sentences using computers. The early approach to machine translation relies heavily on hand-crafted translation rules and linguistic knowledge. As natural languages are inherently complex, it is difficult to cover all language irregularities with manual translation rules. With the availability of large-scale parallel corpora, data-driven approaches that learn linguistic information from data have gained increasing attention. Unlike rule-based machine translation, Statistical Machine Translation  learns latent structures such as word alignments or phrases directly from parallel corpora. Incapable of modeling long-distance dependencies between words, the translation quality of SMT is far from satisfactory. With the breakthrough of deep learning, Neural Machine Translation  has emerged as a new paradigm and quickly replaced SMT as the mainstream approach to MT.  Neural machine translation is a radical departure from previous machine translation approaches. On the one hand, NMT employs continuous representations instead of discrete symbolic representations in SMT. On the other hand, NMT uses a single large neural network to model the entire translation process, freeing the need for excessive feature engineering.  The training of NMT is end-to-end as opposed to separately tuned components in SMT. Besides its simplicity, NMT has achieved state-of-the-art performance on various language pairs. In practice, NMT also becomes the key technology behind many commercial MT systems.  As neural machine translation attracts much research interest and grows into an area with many research directions, we believe it is necessary to conduct a comprehensive review of NMT. In this work, we will give an overview of the key ideas and innovations behind NMT. We also summarize the resources and tools that are useful and easily accessible. We hope that by tracing the origins and evolution of NMT, we can stand on the shoulder of past studies, and gain insights into the future of NMT.  The remainder of this article is organized as follows: Section will review the methods of NMT. We first introduce the basics of NMT, and then we selectively describe the recent progress of NMT. We focus on methods related to architectures, decoding, and data augmentation. Section will summarize the resources such as parallel or monolingual corpora that are publicly available to researchers. Section will describe tools that are useful for training and evaluating NMT models. Finally, we conclude and discuss future directions in Section.   ~ Neural machine translation has become the dominant approach to machine translation in both research and practice. This article reviewed the widely used methods in NMT, including modeling, decoding, data augmentation, interpretation, as well as evaluation. We then summarize the resources and tools that are useful for NMT research.  Despite the great success achieved by NMT, there are still many problems to be explored. We list some important and challenging problems for NMT as follows:   
"," Machine translation  is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation  has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions. %Machine translation  is an important sub-field of natural language processing which aims to translate natural language sentences between different languages using computers. Recent years has witnessed the great success of end-to-end neural machine translation  models, which has dominated the mainstream approach in commercial machine translation systems. In this work, we first provide a broad review of the methods and challenges in NMT. We introduce three basic components in NMT methods, namely modeling, inference, and learning. The modeling part starts with the encoder-decoder framework and the celebrated attention mechanism, which is followed by Recurrent Neural Networks , Convolutional Neural Networks , and Self-Attention Networks  as potential instances in an NMT architecture. The inference part focuses on the generation of translation sentences from NMT models, which consists of autoregressive,  non-autoregressive, and bidirectional decoding methods. The learning part concentrates on the methods that enhances the expressive capacity of NMT models to learn from data. We highlight the design of training objectives and the use of monolingual data in this part. In addition to the three basic parts, we highlight some of the most significant challenges in NMT, including open vocabulary, prior knowledge integration, as well as the interpretability and robustness issues. Then we summarize useful resources and tools for MT research and maintainance. Finally, we conclude with a discussion of promising future research directions.",458
" NMT is the task of transforming a source sequence into a new form in a particular target language using deep neural networks. Such networks commonly have an encoder-decoder architecture , in which an encoder maps a given input sequence to an intermediate representation and a decoder then uses the same representation to generate candidate translations. Both encoder and decoder are neural networks that are trained jointly. Due to the sequential nature of the NMT task, early models usually relied on recurrent architectures , or benefited from the sliding feature of convolutional kernels to encode/decode variable-length sequences .   Recently, Transformers  have shown promising results for NMT and become the new standard in the field. They follow the same concept of encoding and decoding but in a relatively different fashion. A Transformer is fundamentally a feed-forward model with its unique neural components  that alter the traditional translation pipeline accordingly. Therefore, it is expected if such a model behaves differently than its recurrent or convolutional counterparts. Our goal in this research is to study this aspect in the presence of noise.     NMT engines trained on clean samples provide high-quality results when tested on similarly clean texts, but they break easily if noise appears in the input . They are not designed to handle noise by default and Transformers are no exception. Many previous works have focused on this issue and studied different architectures . In this work, we particularly focus on Transformers\footnote{We assume that the reader is already familiar with the Transformer architecture.} as they are relatively new and to some extent understudied.   A common approach to make NMT models immune to noise is fine-tuning , where a noisy version of input tokens is intentionally introduced during training and the decoder is forced to generate correct translations despite deformed inputs. FT is quite useful for almost all situations but it needs to be run with an optimal setting to be effective. In our experiments, we propose a slightly different learning-rate scheduler to improve FT. We also define a new extension that not only modifies input words but also adds complementary tokens to the target side. We refer to this extension as Target Augmented Fine-Tuning , which is the first contribution of this paper.   In our study, we realized that data augmentation techniques  might not be sufficient enough for some cases and we need a compatible training process and neural architecture to deal with noise. Therefore, we propose Controlled Denoising  whereby noise is added to source sequences during training and the encoder is supposed to fix noisy words before feeding the decoder. This approach is implemented via an auxiliary loss function and is similar to adversarial training. CD is our second contribution.   CD only takes care of noise on the encoder side, so we propose a Dual-Channel Decoding  strategy to study what happens if the decoder is also informed about the input noise. DCD supports multi-tasking through a -channel decoder that samples target tokens and corrects noisy input words simultaneously. This form of fusing translation knowledge with noise-related information has led to interesting results in our experiments. DCD is the third and last contribution of this work.   The remainder of the paper is organised as follows: First, we review previously reported solutions for the problem of noise in NMT in Section , then we present details of our methods and the intuition behind them in Section . To validate our methods, we report experimental results in Section . Finally, we conclude the paper and discuss possible future directions in Section .    In this paper, we studied the problem of noise in the context of NMT and particularly focused on Transformers. We proposed three novel techniques to augment data and change the training procedure as well as the neural architecture. Experimental results show that our techniques can protect NMT engines from noise. Our models only affect the training phase and do not add any overhead in terms of space and/or time complexities at inference time. Findings of our research can be summarized as follows:    In this research, we ran an extensive number of experiments in order to find the best configuration of each model and optimize hyper-parameters, but there still exist some unexplored topics/areas. In our future work, we are planning to experiment with other language pairs with different morphological and grammatical structures. , e.g. it would be interesting to see how our models deal with a language such as Mandarin that mainly relies on characters.  We are also interested in studying other noise classes. We could only afford to work with one class and we selected natural noise as we find it more realistic among others, but this work can be extended to other noise classes. Finally, our models are not unique to Transformer and NMT. We aim to evaluate them in other language processing/understanding tasks.  
"," Transformers  have brought a remarkable improvement in the performance of neural machine translation  systems, but they could be surprisingly vulnerable to noise. Accordingly, we tried to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behaviour of conventional models for the problem of noise but it seems Transformers are understudied in this context.  Therefore, we introduce a novel data-driven technique to incorporate noise during training. This idea is comparable to the well-known fine-tuning strategy. Moreover, we propose two new extensions to the original Transformer, that modify the neural architecture as well as the training process to handle noise. We evaluated our techniques to translate the English--German pair in both directions. Experimental results show that our models have a higher tolerance to noise. More specifically, they perform with no deterioration where up to $10$\% of entire test words are infected by noise.",459
" Robust and accurate detection of hate speech is important for minimizing the risk of harm to online users. However, this task has proven remarkably difficult and concerns have been raised about the performance, generalizability and fairness of existing systems. A key challenge in the research community is the lack of high quality datasets that can be freely shared among researchers, have finegrained annotations, contain challenging edge-case content and are not unduly biased by the over-representation of certain demographic groups.  We address these problems in online hate classification by utilizing a system for dynamic data collection, model training and evaluation. Specifically, we use a human-and-model-in-the-loop approach, whereby an initial model is trained and annotators are then tasked with entering content that would fool it into making an incorrect classification. Models are then retrained on all collected data and the process is repeated. New rounds of data are collected, with annotators still trying to trick the improved models by entering the most difficult and unusual forms of content. In this way models `learn from the worst' as the more challenging content they are shown, the faster they will hopefully improve.  The dataset formation was organized into four $} Third, as part of the dataset we present over 14,000 challenging contrastive examples, which were created during the dynamic data generation process.  Dynamic dataset generation through a human-and-model-in-the-loop approach offers several advantages over static datasets. First, problems can be addressed as work is conducted -- rather than creating the dataset and then discovering any inadvertent design flaws, as would be the case with static benchmarks. Second, the model-in-the-loop means that annotators' work is guided by the model; they receive real-time feedback from the model about how effectively different strategies are beating it; this lets them target their efforts to exploit key weaknesses, creating a dataset with many hard-to-classify entries. Third, the dataset can be constructed to better meet the requirements of machine learning; our dataset is balanced, comprising 54\% hate. It includes hate targeted against a large number of targets, providing variety for the model to learn from, and many entries were directed to counter established problems in hate detection model training, such as overfitting on keywords.      To create systems that can detect and classify online hate accurately, robustly and in a way that is generalisable across target, type and domain requires having datasets which are large, varied, expertly annotated and contain challenging edge case content. Dynamic dataset generation offers a powerful and scalable way of creating such datasets. More than that, it helps to guide their creation so they contain the most hard-to-classify content. By learning from the worst, i.e. the most complex and difficult entries, more robust and high performing models can be created. A further advantage of training detection systems on dynamically generated datasets is that they can quickly adapt to new forms of hate.  However, dynamic dataset collection also poses challenges. First, it requires substantial infrastructure and this project would not have been possible without the Dynabench interface. Second, it requires substantial domain expertise for a complex task like online hate, including knowledge of where to find content to inspire synthetic entries. This requires cross-disciplinary teams, combining social science with machine learning expertise. Third, introducing perturbations into the process requires additional annotator training and linguistic expertise; it requires knowing how to adjust text in a realistic and varied way to preserve the key contrastive features .  This project identifies several important lessons for future use of dynamically generated datasets in online hate classification. First, smaller rounds may work better as they enable the design of the task and guidelines to be updated more responsively.  Second, content is not just `real' or `synthetic' but is better understood as part of a spectrum, ranging from entirely synthetic and ultra-unrealistic content through to directly harvested entries; most data sits in between, and the most effective synthetic content is closely inspired by real world examples. Third, robustness is as important as performance. Models that maximize performance on a small static benchmark may overfit and lack robustness -- but this may not be apparent unless they are adversarially tested. Dynamic dataset collection and model evaluation allows for robustness to be optimized in the training process as non-robust models will easily be exploited. However, this can be hard to evaluate in most settings.  In future work we aim to expand the diversity of the annotator pool and will evaluate different in-the-loop models beyond the RoBERTa baseline.  We will also explore the use of online learning. The M3 model is now available online and data collection for training future models is ongoing.\footnote{}        The authors thank the team of annotators for their hard work and commitment to this project.      Statements which express hate through a convoluted sentence structure, e.g. `I閳ユ獟l make sure that X don閳ユ獩 exist anymore because that's what I want to do and I dont care if nobody knows it"".   Statements which initially appear to be neutral  about an identity, but then undermine it with a caveat, e.g. `some people say it閳ユ獨 not fair to call X dumb閳 but 9 times out of 10 it閳ユ獨 true!'.   Statements which are long, often involving multiple clauses and sentences. In such cases the hate may use more subtle forms of language and/or rely more heavily on tone, e.g. `Italy is a beautiful country and I love it so much, from its food to its fashion, from its history to its arts, but i am very worried about the current trends which remind me of the 1930s, So let's import X to change up the food, fashion, history and arts?'   Statements which ask a rhetorical question in order to make a hateful point. Their goal is to express hate rather than to elicit an actual answer, e.g. `Aren閳ユ獩 all X penny pinching, after all?'   Words which are intentionally misspelt so that, although the meaning is clear to a human, the model misclassifiers it because the hate is 閳ユΖbfuscated閳.\footnote{Note we use * for the first vowel in each slur to minimize use of harmful language.} e.g. n*gga: `N!gg@', `n!gga', `n1gg4閳', f*g: `f@g', `f4g', `f@gz', k*ke: `k1ke', `k1k3'.   Words which are misspelt so that they are far longer, with many repeated letters, e.g. `I think letting gaaaaaaaay people have rights is just soooooooo awful'.   Words which are misspelt so that there are spaces between letters. Misspelling of terms includes: 閳ユゾ R A N N Y S閳, 閳ユク e w s閳, 閳ユゴ i g g a閳.   Statements which contain rarely used terms,, e.g. rather than saying 閳ユ are scum閳 they use the phrase, 閳ユ are all sour misanthropes閳. Equally, such statements might just direct unusual / less common forms of abuse at the targeted identity. Infrequently used terms to insult a group include: 閳ユサbnoxious閳, 閳ユガeinous閳, 閳ユアbhorrent閳, 閳ユタnlawful閳, 閳ユェishonorable閳.   Statements which contain positive phrases and terms but still express hate. In particular, phrases which use the term 'love' are often identified as not hateful, e.g. `I just love it when all of the brown people leave my office so I dont have to look at them'.   Statements which are 'random' and do not content any features which are likely to trick the model.      Statements which challenge, counter or dismiss hate, which are expressed by someone who does not identify with the group being attacked. This often involves referring to or quoting hateful content -- which models may mistake for genuine hate.   Statements which challenge, counter or dismiss hate, which are expressed by someone who is from the group being attacked.   Statements which are neutral and state that a group does not have a  attribute. This is presented primarily as a contrast set to Negated non-hate , e.g. `Jews are not money lovers'.   Statements that mention a referent for an identity . For instance, using `black' as a colour, e.g. `the black cat down the road really ruins the neighbourhood'.   Statements which contain a profanity in a not hateful way. They are often used as an adjective or adverb to describe an emotion or to place emphasis on what is being said, e.g. `fucking hell today was a lot of bullshit'.   Statements which attack, criticise or express negativity against inanimate objects, such as sofa or a car, e.g. `this cup is totally rubbish'.   Statements which are aggressive, insulting or abusive against an individual using a direct personal pronoun , e.g. `you are a complete joke and no-one respects you, loser'.   Statements which are aggressive, insulting or abusive against an individual who is not part of the conversation and as such is referred to with an indirect personal pronoun , e.g. `he is such a waste of space. I hope he dies'.   Statements which attack, criticise or express negativity against concepts and ideologies, such as political ideologies, economic ideas and philosophical ideals, e.g. `I've never trusted capitalism. It's bullshit and it fucks society over'.   Statements which attack, criticise or express negativity against animals, e.g. `dogs are just beasts, kick them if they annoy you'.   Statements which attack, criticise or express negativity against institutions; such as large organisations, governments and bodies, e.g. `the NHS is a badly run and pointless organisation which is the source of so much harm'.   Statements which attack, criticise or express negativity against something that is NOT an identity -- and the targets are not identified elsewhere in this typology, e.g. `the air round here is toxic, it smells like terrible'.            Table  shows dev set performance numbers.      Following  we provide a `data statement', which documents the process and provenance of the final dataset.   In order to study the potential of dynamically generated datasets for improving online hate detection, we used the DynaBench interface to generate a large-scale synthetic dataset of 41,000 entries, collected over 4 rounds, with a `model-in-the-loop' design. Data was not sampled. Instead a team of trained annotators created synthetic content to enter into the interface.    All of the content entered into Dynabench was in English. We opted for English language due to the available annotation team, and resources and the project leaders' expertise. The system that we developed could, in principle, be applied to other languages.   Due to the synthetic nature of the dataset, the speakers are the same as the annotators.    Annotator demographics are reported in the paper, and are reproduced here for fullness. Annotation guidelines were created at the start of the project and then updated after each round. Annotations guidelines will be publicly released with the dataset.  We followed the guidance for protecting and monitoring annotator well-being provided by . 20 annotators were recruited. Ten were recruited to work for 12 weeks and ten were recruited for the final four weeks. Annotators completed different amounts of work depending on their availability, which is recorded in the dataset.  All annotators attended a project onboarding session, half day training session, at least one one-to-one session and a daily 'standup' meeting when working. They were given a test assignment and guidelines to review before starting work and received feedback after each round. Finally, annotators could ask the experts questions in real-time over a messaging platform, which also facilitated group discussions.  Of the original ten annotators, seven had annotated hate in previous projects.  Of the 20 annotators, 35\  were male and 65\   were female. 65\  were 18-29 and 35\  were 30-39. 10\  were educated to high school level, 20\  to undergraduate, 45\  to taught masters and 25\  to research degree~. 70\  were native English speakers and 30\  were non-native but fluent. Respondents had a range of nationalities, including British , as well as Polish, Spanish and Iraqi. Most annotators identified as ethnically white , followed by Middle Eastern  and two or more ethnicities . Participants all used social media regularly, including 75\  who used it more than once per day. All participants had seen other people targeted by online abuse before, and 55\  had been targeted personally.    All data was created from 21st September until 14th December 2020. As part of their training annotators were encouraged to visit a range of online platforms, with adequate care and supervision from the project leaders, to better understand online hate as it appears online.   The composition of the dataset, including the distribution of the Primary label  and the type  is described in the paper.    
"," We present a first-of-its-kind large synthetic training dataset for online hate classification, created from scratch with trained annotators over multiple rounds of dynamic data collection. We provide a 40,623 example dataset with annotations for fine-grained labels, including a large number of challenging contrastive perturbation examples. Unusually for an abusive content dataset, it comprises 54\% hateful and 46\% not hateful entries. We show that model performance and robustness can be greatly improved using the dynamic data collection paradigm. The model error rate decreased across rounds, from 72.1\% in the first round to 35.8\% in the last round, showing that models became increasingly harder to trick -- even though content become progressively more adversarial as annotators became more experienced. Hate speech detection is an important and subtle problem that is still very challenging for existing AI methods. We hope that the models, dataset and dynamic system that we present here will help improve current approaches, having a positive social impact.",460
"   Speech separation, also known as cocktail party problem, aims to separate target speech from interference background . It is often used as the front end of speech recognition for improving the accuracy of human-machine interaction. Conventional speech separation technologies include computational auditory scene analysis , non-negative matrix factorization , HMM-GMM , and minimum mean square error . Recently, deep learning based speech separation becomes a new trend , which is the focus of this paper. According to whether speakers閳 information is known as a prior, deep-learning-based speech separation techniques can be divided into three categories, which are speaker-dependent , target-dependent, and speaker-independent speech separation.    Speaker-dependent speech separation needs to known the prior information of all speakers, which limits its practical applications. Nowadays, the research on speech separation is mostly speaker-independent and target-dependent.      Speaker-independent speech separation based on deep learning faces the speaker permutation ambiguity problem. In order to solve this problem, two techniques have been proposed. The first one is deep clustering %     .     It projects each time-frequency unit to a higher-dimensional embedding vector by a deep network, and conducts clustering on the embedding vectors for speech separation.     The second technique is permutation invariant training %     . For each training mixture, it picks the permutation of the speakers that has the minimum training error among all possible permutations to train the network.    % Besides, there are some other effective algorithm based on deep learning, such as deep ensemble learning and deep attractor network.  Target-dependent speech separation based on deep learning aims to extract target speech from a mixture given some prior knowledge on the target speaker. The earliest speech separation method takes the target speaker as the training target . It has to train a model for each target speaker, which limits its practical use. To prevent training a model for each target speaker, speaker extraction further takes speaker codes extracted from a speaker recognition system as part of the network input . Some representative speaker extraction methods are as follows.  applies a context adaptive deep neural network to extract the target speaker through a speaker adaptation layer. It takes the estimated mask and ideal binary mask as the training objective.  proposed a temporal spectrum approximation loss to estimate a phase sensitive mask for the target speaker.  generalized the end-to-end speaker-independent speech separation  to the end-to-end speaker extraction. [t]         % It is more practical when only registered speakers need to be responded, such as speaker diarization and speech recognition .  The aforementioned methods are all single-channel methods. Although they work well in clean scenarios, their performance degrades significantly in reverberant scenarios. To improve the performance of speech separation in reverberant scenarios, many multichannel methods were proposed, which has the following two major forms. The first form combines spatial features that are extracted from microphone arrays, such as interaural time difference and interaural level difference, with spectral features as the input of single-channel speech separation networks . The second form uses a deep network to predict a mask for each speaker at each channel, and then conducts beamforming for each speaker . For brevity, we call this method deep beamforming. Some methods combined the above two forms for boosting their advantages together in reverberant scenarios, e.g. .  The aforementioned multichannel methods are only studied with traditional fixed arrays, such as linear arrays or spherical arrays. However, for  far-field speech separation problems with high reverberation, they suffer significant performance degradation. How to maintain the estimated speech at the same high quality throughout an interested physical space is of broad interests.  Ad-hoc microphone array, which is a group of randomly distributed microphones collaborating with each other, is a solution to the problem. Figure  gives a comparison example where a target speaker extraction problem with a fixed array is on the left and that with an ad-hoc microphone array on the right. From the figure, we see that, compared with the fixed array that is far from the target speaker, the ad-hoc microphone array has several apparent advantages. First, an ad-hoc microphone array may put a number of microphones around the target speaker, which significantly reduced the probability of far-field speech processing. By channel selection, it might be able to form a local microphone array around the target speaker. At last, it may be able to incorporate application devices of various physical sizes.   [t]                In literature, ad-hoc microphone arrays have consistently been an important research topic . However, they face many practical problems due to the lack of important priors. Recently,  addresses the difficulties of ad-hoc microphone arrays, such as lack of priors and insufficient estimation of variables, by deep learning for the first time. The proposed method, named deep ad-hoc beamforming , was originally designed for speech enhancement only, which predicts segment-level signal-to-noise-ratio  by deep neural networks for supervised channel selection. Later on, some speech separation methods based on ad-hoc microphone arrays were proposed.  proposed a transform-average-concatenate strategy for a filter-and-sum network  to realize the channel reweighting/selection ability for ad-hoc microphone arrays. Because ad-hoc microphone arrays lack the prior of the number and spatial distribution of microphones,  proposed a network architecture by interleaving inter-channel processing layers and temporal processing layers to leverage information across time and space alternately. %{ ASR in ad-hoc microphone array...}  % The filter-and-sum network   first conducts pre-separation on a selected reference microphone by estimating its beamforming filters, and then estimates the beamforming filters of all remaining microphones based on pair-wise cross-channel features. They further improved the channel reweighting/selection ability of FaSNet by a transform-average-concatenate strategy  for ad-hoc microphone arrays.   However, existing deep learning based speech separation with ad-hoc microphone arrays are all speaker-independent. To our knowledge, target-dependent speech separation with ad-hoc microphone arrays are far from explored yet. In many applications, extracting and tracking target speech is of more interests than separating a mixture into its components. This is particularly the case for ad-hoc microphone arrays, where several speakers may locate far apart and talk independently.   In this paper, we propose a target-dependent speech separation algorithm with ad-hoc microphone arrays, named DAB based on speaker extraction . Our algorithm consists of three components: first, we propose a supervised channel selection based on speaker extraction, which applies bi-directional long short-term memory  networks to estimate the utterance-level SNR of the target speaker. Then, we employ the heuristic channel selection algorithms in  to pick the channels with high SNRs. We further apply a single-channel speaker extraction algorithm to the selected channels for the mask estimation problem of the target speech. At last, we use the estimated masks to derive a beamformer for the target speaker, such as minimum variance distortionless response  . Experimental results on a WSJ0-adhoc corpus show that the proposed DABse performs well in reverberant environments.   The rest of the paper is organized as follows. We introduce the signal model of the speaker extraction problem with ad-hoc microphone arrays in Section . Then, we present the deep ad-hoc beamforming system based on speaker extraction in Section . In Section , we present the experimental results. Finally, we conclude this study in Section .      In this paper, we have proposed deep ad-hoc beamforming based on speaker extraction, which is the first work of the target-dependent speech separation based on ad-hoc microphone arrays and deep learning. DABse uses the channel-weight estimation network based on speaker extraction to estimate the SNR of the target speaker, and then takes the SNR as the channel weight for the selection of high-quality channels, and finally takes the selected channels for the deep learning based MVDR. The deep learning based MVDR first takes the single-channel target-dependent speaker extraction network to estimate the clean spectrum of the target speech at each selected channel, and then uses the estimated spectrum to derive an MVDR filter for the final speech separation. Because the two deep models in DABse are trained in a single-channel fashion, it is able to handle any number of microphones in the test stage. Because MVDR is a linear filter, DABse does not suffer from nonlinear distortions. We have conducted extensive experiments in the scenarios where the speech sources are located randomly in large rooms. We compared DABse with the baselines of 'single-channel' and 'all-channels'. Experimental results demonstrate that the proposed DABse outperforms the baselines significantly, which illustrates the effectiveness of DABse in the adverse environments.    In addition to the advantages above, our system still has some areas to be improved. First, although we assume that all microphones of the ad-hoc microphone array are synchronized when simulating the experimental environment, the entire system still has a time delay due to air propagation. We can even assume that the microphones are asynchronous during experiments in the future , since it exists in the real world. Therefore, how to synchronize the speech of mixed speaker leaves us to find out. Second, we occur a problem during the test. {{Because the position of the microphonea in the ad-hoc microphone array may vary greatly, we can't get a unified result of evaluation metrics when we test those microphones far away from each other.}} Therefore, we think the common-used evaluation metrics nowadays have improving room for ad-hoc microphone arrays.           Text for this sub-sub-heading\ldots  \paragraph*{Sub-sub-sub heading for section}  Text for this sub-sub-sub-heading\ldots   In this section we examine the growth rate of the mean of ,  and . In  addition, we examine a common modeling assumption and note the  importance of considering the tails of the extinction time  in  studies of escape dynamics.  We will first consider the expected resistant population at  for  some ,     \[  E T_x\approx-\frac{1}{\lambda_0}\log xv>0$ .    
"," % abstract %\parttitle{First part title} %if any %Text for this section. %\parttitle{Second part title} %if any %Text for this section. Recently, the research on ad-hoc microphone arrays with deep learning has drawn much attention, especially in speech enhancement and separation. Because an ad-hoc microphone array may cover such a large area that multiple speakers may locate far apart and talk independently, target-dependent speech separation, which aims to extract a target speaker from a mixed speech, is important for extracting and tracing a specific speaker in the ad-hoc array. However, this technique has not been explored yet. In this paper, we propose deep ad-hoc beamforming based on speaker extraction, which is to our knowledge the first work for target-dependent speech separation based on ad-hoc microphone arrays and deep learning. The algorithm contains three components. First, we propose a supervised channel selection framework based on speaker extraction, where the estimated utterance-level SNRs of the target speech are used as the basis for the channel selection. Second, we apply the selected channels to a deep learning based MVDR algorithm, where a single-channel speaker extraction algorithm is applied to each selected channel for estimating the mask of the target speech. We conducted an extensive experiment on a WSJ0-adhoc corpus. Experimental results demonstrate the effectiveness of the proposed method.",461
" In this paper, we tackle the problem of screening a finite pool of documents, where the aim is to retrieve relevant documents satisfying a given set of predicates that can be verified by human or machines . In this context, if a document does not satisfy at least one predicate, it is treated to be irrelevant. A predicate represents a property, a unit of meaning, given in natural language . By this means a predicate might be interpreted in a variety of ways in text, so making keywords-based search hard to reach high recall while keeping a decent level of precision . We interpret the screening problem as high recall problem, i.e., the aim is to retrieve all relevant documents maximizing precision. %we assume predicates and candidate documents are given  % Since predicates can be interpreted in a variety of ways, it makes the problem of document screening very challenging especially when there is a little training data.  The screening finds application in many domains, such as i) in systematic literature reviews ; % -SLRs-  AND papers studying older adults }; ii) database querying - where items filtered  in/out based on predicates ; iii) hotel search - where the hotels retrieve  based upon filters of interest . Consequently,  the document screening is an instance of finite pool binary classification problems  , where we need to classify a finite set of objects minimizing cost. % As an instance of the problem, we choose the screening phase of SLRs what makes the problem rather challenging since each review is different and has a unique set of predicates . Typically, authors of an SLR retrieve a candidate pool of documents executing a keywords-based query on a database such as Scopus. To avoid missing papers, the query tends to be inclusive, which means that it returns hundreds or thousands of results  that are later manually screened by researchers based on predefined predicates. For example, researchers might look for papers that describe all of the following predicates at the same time: 1) ""include papers that study older adults 85+ years"", 2) ""include papers conducted randomized controlled trial"", 3) "" include papers about behavioral intervention"". Therefore, here we have the conjunctive query of three inclusive predicates. A bottleneck of the screening process is the predicate evaluation, i.e.,  identifying which of the given predicates are satisfied in a current document. For example, in literature reviews, authors validate predicates, however, this is time-consuming, exhaustive, and very expensive .   An effective technique to solve screening problems is crowdsourcing where the crowd can solve even complex screening tasks with high accuracy and lower cost compared to expert screening . However, achieving a good performance in crowd-based screening requires a deep understanding of how to design tasks and model their complexity , how to test and filter workers , how to aggregate results into a classification decision, or how to improve worker engagement .   Machine learning  algorithms have also made a very impressive progress in solving complex screening tasks. However, obtaining a sufficiently large set of training data is still a key bottleneck for accurate ML classifiers. Active learning   accelerates this process by minimizing the size of training data that is required to train better classifiers via selecting the most informative instances for annotation. The effectiveness of AL have been proven in many domains , but most of the work considers single-label cases while multi-label AL problems have been far less investigated. The challenge in applying AL to a multi-label classification problem is that the algorithm should measure the unified informativeness of each unlabeled item across all labels. The state of the art multi-label AL strategies follow an object-wise  labeling, where the AL algorithm first finds the relevance scores  of  pairs, and then aggregates these scores to find the informativeness of items . However, it may ignore the interaction between labels .     We investigate how to efficiently combine crowdsourcing and ML for item screening. It is a challenging task since the budget is limited and there are countless number of ways to spend it on the problem. We propose a multi-label AL screening specific sampling technique for querying unlabelled items for annotating. Our algorithm takes a decision how to choose unlabeled data to annotate by crowd workers in order to maximize the performance of a screening task. Unlike existing multi-label AL approaches that rely on global labeling, we choose the local labeling method, where for each label  we determine the relevancy to each item.    In this paper, we proposed and evaluated the objective-aware active learning strategy designed for screening classification and selecting efficiently item, predicate for annotating based on the overall classification objective. We demonstrated that objective-aware sampling outperforms uncertainty and random AL techniques under different conditions. We further aim to examine more screening datasets, extend this study to other classes of screening problems and hybrid crowd-machine algorithms.  
"," In this paper, we explore how to efficiently combine crowdsourcing and machine intelligence for the problem of document screening, where we need to screen documents with a set of machine-learning filters. Specifically, we focus on building a set of machine learning classifiers that evaluate documents, and then screen them efficiently. It is a challenging task since the budget is limited and there are countless number of ways to spend the given budget on the problem. We propose a multi-label active learning screening specific sampling technique -objective-aware sampling- for querying unlabelled documents for annotating. Our algorithm takes a decision on which machine filter need more training data and how to choose unlabeled items to annotate in order to minimize the risk of overall classification errors rather than minimizing a single filter error.  We demonstrate that objective-aware sampling significantly outperforms the state of the art active learning sampling strategies. % on multi-filter classification problems.",462
"   One of the hallmarks of human intelligence is the ability to generalize seamlessly across heterogeneous sensory inputs and different cognitive tasks. We see objects, hear sounds, feel textures, smell odors, and taste flavors to learn underlying concepts present in our world. Much of AI's existing progress in multimodal learning, however, focuses primarily on a fixed set of predefined modalities and tasks that are consistent between training and testing. As a result, it is unclear how to transfer knowledge from models trained for one modality  to another  at test time. This scenario is particularly important for low-resource target modalities where unlabeled data is scarce and labeled data is even harder to obtain . In the unimodal case, this is regarded as meta-learning or few-shot learning. In contrast, we formally define the cross-modal generalization setting as a learning paradigm to train a model that can  quickly perform new tasks in a target modality  and  doing so while being trained on a different source modality. In this paper, we study the data and algorithmic challenges for cross-modal generalization to succeed. %Such a learning paradigm is particularly useful in leveraging high-resource source modalities to help low-resource target modalities, where unlabeled data is scarce and labeled data is even harder to obtain, such as audio from low-resource languages, real-world environments, and medical images.      As a motivating example, Figure illustrates a scenario where large-scale image classification benchmarks can help audio classification, which is a less studied problem with fewer large-scale benchmarks. In this ambitious problem statement, a key research question becomes: how can we obtain generalization across modalities despite using separate encoders for different source  and target  modalities? The technical challenge involves aligning shared knowledge learned from source image tasks with target audio tasks. Our problem statement differs from conventional meta-learning and domain adaptation where one can take advantage of the same source and target modality with shared encoders which helps generalization by having the same representation space. In our case, the discrepancies in modalities requires one to learn new output concepts expressed in new input modalities. As a result, cross-modal generalization requires new ideas to synchronize  multimodal sources and targets. What is the minimal extra supervision required to perform this alignment?  In this paper, we formalize the conditions required for successful generalization and show that another level of supervision is necessary under partial observability across modalities and tasks. Supervision comes in the form of cross-modal meta-alignment  to capture a space where representations of similar concepts in different modalities are close together while ensuring quick generalization to new tasks . We introduce a novel algorithm called  expressed in new input modalities. %We show that existing domain adaptation, meta-learning, and transfer learning approaches are unable to bridge the gap between such heterogeneous paradigms where both input modalities and output tasks are different.  % emphasize cant share encoders, need explicit alignment % emphasize different label space, generalize meta-learning % formulate crossmodal ml and therefore we propose meta alignment % first para ok. like to learn but different modalities. % second para. compared to ml and da, 1 critical issue when trying to do crossmodal - have hetero data between source and target. cant use shortcut such as same encoder for images of different domains. need different encoders 1 for each. how do we solve this? need another level of supervision to help - where meta alignment comes in. what we propose - a technique to address the core technical challenge of crossmodal ml which is how to learn different encoders. meta alignment is a way to do that, a contrastive learning approach.  %To account for this technical challenge, we formalize the conditions required for successful generalization and show that another level of supervision is necessary under partial observability across modalities and tasks. This form of supervision comes in the form of cross-modal alignment to capture a space where representations of similar concepts in different modalities are close together while ensuring quick generalization to new low-resource tasks . Our analysis leads to a novel algorithm based on contrastive learning called  expressed in new input modalities. We show that existing domain adaptation, meta-learning, and transfer learning approaches are unable to bridge the gap between such heterogeneous paradigms where both input modalities and output tasks are different.  % how do we handle limited resource modalities and task, we explore cross-modal approach % note: define modality, concept, task % note: a better way of saying cross-modal cross-task  %, which allows us to learn a classifier for transfer from source to target tasks. %This makes it particularly suitable for generalization across modalities and tasks due to the presence of unseen concepts and annotations in the target modality. %We show that this space:  groups similar concepts expressed across different modalities,  is well-clustered across concepts, and  generalizes well to new concepts, making it particularly suitable for generalization across modalities and tasks. %While our first attempt at meta-alignment uses strong pairings across source and target modalities , we further provide an extension to use only weak pairs between modalities. Weak pairs represent coarse groupings of semantic correspondence which better capture the many-to-many relations between real-world multimodal data  and allow us to use large banks of weakly paired multimodal data available on the internet and prepared for machine learning studies such as video data  and image captioning data . %Finally, we quantify the trade-offs between labeling more data in the target modality versus obtaining better source-target alignment.  %provide theoretical justification to quantify the benefits of our approach: { ZIYIN TODO} \zing[ziyin: should mention and focus on the difficulty of definition and formalization] %instead of a classical generalization error in the target modality that scales wrt the sample complexity of the target modality, our approach is bounded by the sample complexity in the source modality. As a result, the error is therefore reduced with ample samples in the source modality and a well-aligned space.  We present experiments on three cross-modal tasks: generalizing from  text to image,  image to audio, and  text to speech. In all cases, the goal is to classify data from a new target modality given only a few  labeled samples. %We find that \names\ accurately performs few-shot alignment of concepts from different modalities, thereby allowing generalization from concepts in the source modality to new concepts in the target modality. We perform extensive experiments to compare with related approaches including target modality meta-learning that would be expected to perform well since they have seen thousands of labeled examples from the target modality during meta-training. Surprisingly, \names\ is competitive with these baselines and significantly outperforms other cross-modal approaches. In addition, we study settings where the target modality suffers from noisy or limited data, a scenario particularly prevalent in low-resource modalities. %While this setting makes it difficult to directly train in the target modality, our approach efficiently leverages cross-modal information to perform well.       In this work, we proposed cross-modal generalization: a learning paradigm where abundant source modalities are used to help low-resource target modalities. We showed that meta-alignment using cross-modal data can allow quick generalization to new concepts across different modalities. Our experiments demonstrate strong performance on classifying data from an entirely new target modality under limited samples and noisy labels, which is particularly useful for generalization to low-resource images, speech, and languages.  \iffalse  
"," The natural world is abundant with concepts expressed via visual, acoustic, tactile, and linguistic modalities. Much of the existing progress in multimodal learning, however, focuses primarily on problems where the same set of modalities are present at train and test time, which makes learning in low-resource modalities particularly difficult. In this work, we propose algorithms for cross-modal generalization: a learning paradigm to train a model that can  quickly perform new tasks in a target modality  and  doing so while being trained on a different source modality. We study a key research question: how can we ensure generalization across modalities despite using separate encoders for different source and target modalities? Our solution is based on meta-alignment, a novel method to align representation spaces using strongly and weakly paired cross-modal data while ensuring quick generalization to new tasks across different modalities. We study this problem on 3 classification tasks: text to image, image to audio, and text to speech. Our results demonstrate strong performance even when the new target modality has only a few  labeled samples and in the presence of noisy labels, a scenario particularly prevalent in low-resource modalities. %Despite vast differences in these raw modalities, humans seamlessly perceive multimodal data, learn new concepts, and show extraordinary capabilities in generalizing across input modalities. %In addition, our method works particularly well when the target modality suffers from noisy or limited labels, a scenario particularly prevalent in low-resource modalities. %, sometimes outperforming within modality few-shot baselines that have seen thousands of labeled examples from that target modality during meta-training. %\zing[Ziyin: heterogeneous -> multimodal? since we are assuming there is an underlying shared space, so maybe not heterogeneous] %\zing[Ziyin: since this is the first sentence in the intro, maybe remove this?] %Similarly, truly general artificial intelligence  systems must learn to generalize across multiple input modalities and output tasks. %In this work, we define and propose algorithms for a new notion of generalization:  %, languages, and concepts. %We believe that our proposed methods could open new doors towards better generalization in multimodal AI systems.",463
"  Program source code contains rich structure information, like the syntax structure and control or data flow. Learning from these structures has been a hot topic in the area of deep learning on source code. In recent years, instead of applying basic sequential neural models, researchers have used more complex neural networks to capture the explicit structure of source code. Most researches use abstract syntax trees  as they are easy-to-acquire for most programming languages and semantically equivalent to source code.  A problem of ASTs is that they do not explicitly reflect structural information beyond syntax dependencies, like control and data flow. A viable solution is adding different types of control and data flow edges on ASTs to generate program graphs, and apply graph neural networks  on programs to learn their representations . However, these approaches do not consider that apart from control or data flow edges, the nodes and edges of the original ASTs are also differently typed. For example, in ASTs, some nodes refer to identifiers, and some nodes define upper-level structures as control flows. For parent-child links, the relation between a function definition node to its function body or one of its arguments is apparently different. We believe if we explicitly add node and edge types to programs graphs, it will help neural models to understand programs better.  Our idea of adding types to nodes and edges in AST coincides with the concept of heterogeneous graphs. Heterogeneous graphs, or heterogeneous information networks , refer to a group of graphs with multiple types of nodes and edges. A typical example of heterogeneous graphs is knowledge graphs, in which the nodes are different types of entities, and the edges represent different relations. In this paper, we propose an approach for building heterogeneous program graphs from ASTs. To obtain the type of AST nodes and edges, we use the abstract syntax description language   grammar.  After we acquire heterogeneous graphs for code snippets, we need to find a GNN model to effectively represent these graphs. Although some existing GNN-for-code works  have pointed out that there exist different types for AST nodes, they only consider node type in the initial node embedding and neglect their differences in the message passing  step. So we turn our sight to the field of heterogeneous graph embeddings. Recently, heterogeneous graph neural networks have become widely used in heterogeneous graph embedding. Unlike traditional graph neural networks, heterogeneous graph neural networks are capable of integrating node and edge type information in the message passing stage and map different types of nodes to different feature space. We use heterogeneous graph transformer   on our heterogeneous program graphs to calculate the representation of programs.  We evaluate our approach on two tasks: comment generation and method naming, with two Python datasets from different domains. These two tasks can be seen as two different forms of code summarization, so both of them require understanding the semantics of the input code snippets. The results show that our approach outperforms existing GNN models and other state-of-the-art approaches, indicating the extra benefit of bringing heterogeneous graph information to source code.  To summarize, our contributions are:  To our knowledge, we are the first to put forward the idea of representing programs as heterogeneous graphs and apply heterogeneous GNN on source code snippets.  We propose an approach of using ASDL grammars to build heterogeneous program graphs from program ASTs.  We evaluate our approach on two different tasks involving graph-level prediction on source code snippets. Our approach outperforms other GNN models on both comment generation and method naming tasks.    In this paper, we put forward the idea of heterogeneity in program ASTs, and presented a framework of representing source code as heterogeneous program graphs  using ASDL grammars. By applying heterogeneous graph transformer on our HPG, our approach significantly outperforms previous GNN models on two graph-level prediction tasks for source code: comment generation and method naming.  In the future, we plan to evaluate our approach on more tasks, especially node or link prediction tasks. We would also extend our approach to other programming languages and propose new models more suited for heterogeneous program graphs.  
"," Program source code contains complex structure information, which can be represented in structured data forms like trees or graphs. To acquire the structural information in source code, most existing researches use abstract syntax trees . A group of works add additional edges to ASTs to convert source code into graphs and use graph neural networks to learn representations for program graphs. Although these works provide additional control or data flow information to ASTs for downstream tasks, they neglect an important aspect of structure information in AST itself: the different types of nodes and edges. In ASTs, different nodes contain different kinds of information like variables or control flow, and the relation between a node and all its children can also be different.  To address the information of node and edge types, we bring the idea of heterogeneous graphs to learning on source code and present a new formula of building heterogeneous program graphs from ASTs with additional type information for nodes and edges. We use the ASDL grammar of programming language to define the node and edge types of program graphs. Then we use heterogeneous graph neural networks to learn on these graphs. We evaluate our approach on two tasks: code comment generation and method naming. Both tasks require reasoning on the semantics of complete code snippets. Experiment results show that our approach outperforms baseline models, including homogeneous graph-based models, showing that leveraging the type information of nodes and edges in program graphs can help in learning program semantics.",464
" [htb] % trim is left bottom right top , clip, width=\textwidth]{topic_discovery_diagram}      % Every day pharmaceutical companies receive numerous medical inquiries related to their products from patients, healthcare professionals, research institutes, or public authorities from a variety of sources .  % These medical inquiries may relate to drug-drug-interactions, availability of products, side effects of pharmaceuticals, clinical trial information, product quality issues, comparison with competitor products, storage conditions, dosing regimen, and the like.  % On the one hand, a single medical inquiry is simply a question of a given person searching for a specific information related to a medicinal product. On the other hand, a plurality of medical inquiries from different persons may provide useful insight into matters related to medicinal products and associated medical treatments. % Examples of these insights could be early detection of product quality or supply chain issues, anticipation of treatment trends and market events, improvement of educational material and standard answers/frequently asked question coverage, potential changes in treatment pattern, or even suggestions on new possible indications to investigate. % From a strategic perspective, this information could enable organizations to make better decisions, drive organization results, and more broadly create benefits for the healthcare community.   % transition paragraph - machine learning can help However, obtaining high-level general insights is a complicated task since pharmaceutical companies receive  copius amounts of medical inquiries every year. Machine learning and natural language processing represent a promising route to automatically extract insights from these large amounts of unstructured  medical text. % % % text mining in general and in the biomedical domain Natural language processing and text mining techniques have been widely used in the medical domain, with particular emphasis on electronic health records.  In particular, deep learning has been successfully applied to medical text, with the overwhelming majority of works in supervised learning, or representation learning  to learn specialized word vector representations . % %There is little work however on unsupervised learning from unstructured medical text.  Conversely, the literature on unsupervised learning for medical text is scarce despite the bulk of real-world medical text being unstructured, without any labels or annotations. % Unsupervised learning from unstructured medical text is mainly limited to the development of topic models based on latent Dirichlet allocation . Examples of applications in the medical domain are clinical event identification in brain cancer patients from clinical reports, modeling diseases and predicting clinical order patterns from electronic health records, or detecting cases of noncompliance to drug treatment from patient forums. % Only recently, word embeddings and unsupervised learning techniques have been combined to analyze unstructured medical text to study the concept of diseases, medical product reviews, or to extract informative sentences for text summarization.  % real-world corpus of medical inquiries and its challenges In this work, we combine biomedical word embeddings and unsupervised learning to discover topics from real-world medical inquiries received by Bayer\texttrademark. % A real-world corpus of medical inquiries presents numerous challenges. From an inquirer  perspective, often the goal is to convey the information requested in as few words as possible to save time. This leads to an extensive use of acronyms, sentences with atypical syntactic structure, occasionally missing verb or subject, or inquiries comprising exclusively a single noun phrase. % Moreover, since medical inquiries come from different sources, it is common to find additional  information related to the text source; examples are references to internal computer systems, form frames  alongside with the actual form content, lot numbers, email headers and signatures, city names.  % % mixture of layman and medical language The corpus contains a mixture of layman and medical language depending  on the inquirer being either a patient or a healthcare professional. Style and content of medical inquiries vary quite substantially according to which therapeutic areas  a given medicinal product belongs to.  % add sentence to refer to the text representation %as one can see from Fig., As already mentioned, medical inquiries are short. More specifically, they comprise less than fifteen words in the vast majority of cases.  % Standard techniques for topic modelling based on LDA do not apply, since the main assumption - each document/text is a distribution over topics - clearly does not hold given that the text is short.  % Approaches based on pseudo-documents or using auxiliary information are also not suitable since no meaningful pseudo-document nor auxiliary information are available for medical inquiries. % Moreoever, these models aim to learn semantics  directly from the corpus of interest. However, the recent success of pretrained embeddings shows that it is beneficial to include semantics learned on a general  corpus, thus providing semantic information difficult to obtain from smaller corpora. This is particularly important for limited data and short text settings. To this end, there has been recently some work aimed at incorporating word embeddings into probabilistic models similar to LDA  and that - contrary to LDA - satisfies the single topic assumption .  Even though these models include  semantic information in the topic model, it is not evident how to choose the required hyper-parameters, for example determining an appropriate threshold when filtering semantically related word pairs. Concurrently to our work, document-level embeddings and hierarchical clustering have been combined to obtain topic vectors from news articles and a question-answer corpus.  % summary Here, we propose an approach based on specialized biomedical word embeddings and unsupervised learning to discover topics from short, unstructured, real-world medical inquiries. This approach - schematically depicted in Fig. - is then used to discovery topics in medical inquiries received by Bayer\texttrademark\ Medical Information regarding the oncology medicinal product Stivarga\texttrademark.       advantages This study introduces an unsupervised machine learning approach to automatically discover topics from medical inquiries.  After the initial  effort for preprocessing  and hyper-parameters determination, the algorithm runs without requiring any human intervention, discovering key topics as medical inquiries are received.    Topics can be discovered even if only a small number of inquiries is present, and are generally specific, thus enabling targeted, informed decisions by medical experts.  Being completely unsupervised, the algorithm can discover topics that were neither known nor expected in advance, topics which often are the most valuable.   This is in stark contrast with ontology or supervised based approaches, where topics need to be defined  , and incoming text can be associated only to these predefined lists of topics, thus hindering the discovery of  unknown topics.   The machine learning approach introduced here does not use ontologies , and instead it incorporates domain knowledge via specialized biomedical word embeddings.   This allows to readily apply the topic discovery algorithm to different medicinal products, without the burden of having to develop specialized ontologies for each product or therapeutic area. Indeed, the algorithm is periodically analyzing medical inquiries for a total of sixteen Bayer\texttrademark\ medicinal products, encompassing cardiology, oncology, gynecology, hematology, and ophthalmology.    disadvantages Our approach has several limitations. First, it can happen that a small fraction of inquiries associated to a given topic are actually extraneous to it, especially for semantically broad topics.  This is because - due to the noise present in this real-world dataset - the soft clustering HDBSCAN algorithm must be applied with a low probability threshold for cluster assignment to avoid the majority of inquiries being considered as outliers .    Second, even though the topic names are generally quite informative, a medical expert needs to read the actual inquiries to fully grasp the topic meaning, especially if a decision will be made on the grounds of the discovered topics. This is however not burdensome because inspection is limited to the inquiries associated to a given topic .   Last, some discovered topics are judged by medical experts - based on their expert knowledge - so similar that they could have been merged in a single topic, but are considered distinct by the algorithm. In these cases, manual topic grouping might be required to determine the top topics by inquiry volumes. Still, these similar topics very often appear close to each other in the topic map.    value despite the limitations Despite these limitations, this study demonstrates that medical inquiries contain useful information, and that machine learning can extract this information in an automatic way, discovering topics that are judged by medical information specialists as meaningful and valuable. The hope is that this will stimulate mining of medical inquiries, and more generally the use of natural language processing and unsupervised learning in the medical industry.   Interesting future directions are the inclusion of   expert knowledge  while at the same time maintaining the ability to discover new and previously unknown topics, and grouping topics in meta-topics though a clustering hierarchy.     Since our dataset comprises real-word medical inquiries, preprocessing is a crucial step to limit the amount of noise in the corpus.    acronyms The corpus contains numerous acronyms: a first step is thus acronym resolution,  substitute a given acronym with its extended form. A dictionary for the most recurring acronyms  scispaCy embedding vector is retrieved; the sentence representation is then obtained simply by calculating the arithmetic average of the vectors representing each token over all tokens belonging to a given sentence.    Even though the overwhelming majority of out-of-vocabulary  words are not of interest for medical topic discovery, a very small  subset of important oov words would be missed if one were to simply use the word2vec model. We thus devise a strategy to overcome this, as described below.   For each product, the most recurring oov words are automatically detected; these words need to be included in the word2vec model so that they can be represented by a vector which accurately captures their meaning. Training a new embedding to include these new terms is not a good approach given the sparseness problem described above.  To overcome this, we combine a definition mapping and embedding strategy.   definition mapping of out-of-vocabulary words Specifically, first each of the relevant oov terms is manually mapped to a short definition; for example, the oov  is mapped to  since ReDOS refers to a dose-optimisation phase 2 study on regorafenib .    definition embedding Then, using the text from these definitions, a meaningful vector representation for the oov words is obtained with the embedding strategy described above . This procedure has two main benefits. First, it does not require any training data nor any training effort. Second, it ensures by construction that the added word vectors are compatible with the word representation model in use.   Pharmaceutical product trade names are oov words of particular interest for medical topic discovery.   Indeed, being able to take into consideration drug trade names is of importance since there is a substantial amount of questions which mention for instance drug interactions.  However, they are are generally not included in the scispaCy model. Thus, a slightly different procedure is used to ensure that all trade names appearing in medical inquiries are added to the model, regardless of them belonging to the most recurring oov words or not.    Luckily, international non-proprietary names  of drugs are included. For instance, the oncology product trade name \texttrademark\ is not present, while its corresponding INN  is.  Thus, to automatically detect drug trade names we utilize the scispaCy named entity recognizer  and the scispaCy UmlsEntityLinker as follows.   First, the NER is used to extract entities from the text; then, for each entity, the UmlsEntityLinker performs a linking with the Unified Medical Language System  by searching within a knowledge base of approximately 2.7 million concepts via string overlap as described in Ref. \onlinecite{neumann-2019}. To limit the number of false positive matches we increase the UmlsEntityLinker threshold to 0.85 from the default of 0.7. For each entities that has been successfully linked to UMLS, several information regarding the identified concepts are returned by the UmlsEntityLinker: concept unique identifier , concept preferred name, concept definition, concept aliases, and concept type unique identifier . In particular, the latter defines to which semantic group the linked concept belongs to ; an up-to-date list of semantic type mappings can be found at . A TUI value of T121 indicates that the concept found is a . Extracting the entities with TUI equal to T121 allows to automatically identify drug trade names.  Each drug trade name is then mapped to the concept preferred name; if that is not present, the concept definition is used; if that is also not present, drug trade name is replaced by to the phrase .  Once this mapping is performed, the same embedding strategy used for the other oov words is followed in order to obtain semantically meaningful word vector representations.    The HDBSCAN algorithm starts by defining a mutual reachability distance based on a density estimation; the data is then represented as a weighted graph where vertices are data points and edges have weight equal to the mutual reachability distance between points.    The minimum spanning tree is built, and converted to a hierarchy of connected components via a union-find data structure: starting from an initial cluster containing all points, the data is subsequently split at each level of the hierarchy according to the distance, ultimately returning as many clusters as data points when the threshold distance approaches zero. This cluster hierarchy is commonly depicted as dendogram.   To obtain a meaningful set of clusters, this hierarchy needs to be condensed. The crucial point is to discern - at any given split - if two new meaningful clusters are formed by splitting their parent cluster, or instead the parent cluster is simply loosing points . In HDBSCAN, this decision is governed by the minimum cluster size hyper-parameter : a cluster split is accepted only if both newly formed clusters have at least  points. The final clusters are then chosen from this set of condensed clusters by means of a measure of stability as defined by Ref. \onlinecite{campello-2013}.       how we define the hyperparameters The main factor in defining  is the number of inquiries for a given product: we want to obtain  100 clusters so that results can be easily analyzed by medical experts.  It is important to point out that  does not strictly specify the number of clusters that will be formed, but rather provides to the algorithm an indication regarding the desired granularity, as outlined above. In our case,  ranges only between 5 and 10 depending on the number of inquiries. This small range of variation substantially facilitate the hyper-parameter search. Moreover, we noticed that - for approximately the same amount of inquiries and same  - the number of returned clusters increases with data variety, where data variety is qualitatively evaluated by manual inspection: for products with more diverse inquiries HDBSCAN tends to return a higher number of clusters, .   ceteris paribus means all things being equal We utilize the leaf cluster selection method instead of the excess of mass algorithm because the former is known to return more homogeneous clusters .    we use the soft clustering Due to the noise in the dataset, using the standard  HDBSCAN clustering results in a large portion of the dataset  considered as outliers consistently across all products.  To overcome this, we use the soft HDBSCAN clustering, which returns - instead of a  cluster assignment - the probability that an inquiry belongs to a given cluster. We then define a probability threshold under which a point is considered to be an outlier; for all other points above this threshold, we associate them to the cluster with the highest probability through an argmax operation. This probability threshold ranges between  and   and it is chosen such that approximately 10\  of the inquiries are classified as outliers.   As mentioned in the main text, for computational reasons, we project via UMAP to a lower dimensional space before clustering is performed. Specifically, we project to 100 dimensions for products with less than 15,000 inquiries, and to 20 dimensions for products with more than 15,000 inquiries.   Moreover, inquiries longer than 800 characters are also considered as outliers: this is because the text representation  degrades for long sentences. These inquiries are gathered in the outlier cluster and made available to medical experts for manual inspection.   Given a topic, the vector representation for each word in the topic name is calculated; the topic name vector is then obtained by averaging the word vectors of the words present in the topic name. Topics are merged if their similarity - evaluated as cosine similarity between their topic name vectors - is larger than a threshold. Threshold values range between 0.8 and 0.95 depending on the medicinal product considered.    The most popular topic evaluation metrics for topic modelling on long text are UCI  and UMass . However, both UCI and UMass metrics are not good indicators for quality of topics in short text topic modelling due to the sparseness problem. In Ref. \onlinecite{quan-2015}, a purity measure is introduced to evaluate short text topic modelling; however, it requires pairs of short and long documents , and thus it is not applicable here because there is no long document associated to a given medical inquiry. Indeed, evaluation of short text topic modelling is an open research problem .    An additional challenge is the absence of labels. Performing annotations would require substantial manual effort by specialized medical professionals, and would be of limited use because one of the main goals is to discover previously unknown topics as new inquiries are received. The absence of labels precludes the use of the metrics based on purity and normalized mutual information proposed in Ref. \onlinecite{rosenberg-2007}, \onlinecite{huang-2013}, \onlinecite{yin-2014}.    distributional semantic Ref. \onlinecite{aletras-2013} bring forward the valuable idea of using distributional semantic to evaluate topic coherence, exploiting the semantic similarity learned by word2vec models. Topic coherence is assessed by calculating the similarity among the top n-words of a given topic: semantically similar top n-words lead to higher topic coherence. If this might be in general desirable, in the case of discovering medical topics it is actually detrimental: interesting  topics are often characterized by top n-words which are not semantically similar.  For example, a medical topic having as top 2-words   and  is clearly relevant from a medical topic discovery standpoint. However,  and  are not semantically similar, and thus the metric proposed in Ref. \onlinecite{aletras-2013} would consider this as a low coherence  topic, in stark contrast with human expert judgment.   Analogous considerations apply to the indirect confirmation measures in Ref. \onlinecite{roeder-2015}: words emerging in novel topics would have rarely appeared before in a shared context. For this reason, we introduce a new measure of topic compactness which takes into account the semantics of the inquiries, and does not require any labeled data. Specifically, we compute the similarity of all inquiries belonging to a given topic with each other , sum the elements of the resulting similarity matrix, and divide by the total number of elements in this matrix. The topic semantic compactness  of topic  reads  where  is the cardinality of topic  ,   is the word vector representing inquiry  , and  is a function quantifying the semantic similarity between inquiry  and , taking values between 0 and 1 . Given the chosen normalization factor ,  and thus  can be directly used as  topic quality score.  The topic compactness maximum  is attained if and only if every sentence  contains exactly the same words. It is important to point out that  automatically takes semantics into account: different but semantically similar medical inquiries would still have high similarity score, and thus would lead  to a high topic semantic compactness, despite these inquiries using different words to express similar content.     add here example of glutine Contrary to Ref. \onlinecite{aletras-2013}, the topic semantic compactness  introduced in Eq. does not artificially penalize novel topics just because they associate semantically different words appearing in the same inquiry. To come back to the previous example, if numerous inquiries in a discovered topic contain the words  and , the topic semantic compactness would be high , regardless from the fact that the top 2-words are not semantically similar since the similarity is evaluated at the inquiry level .  It is also beneficial to evaluate how representative the topic name is for the topic it represents. To this end, we calculate the name saliency  for medical topic  by calculating the similarity of the word vector representing the topic name with the word vectors representing the inquiries in the topic, sum these similarity values, and divide by the total number of inquiries in the topic. This reads  where  is the cardinality of topic  ,  is the word vector representing the name of topic , and  is the vector representing inquiry .  This returns a score  which quantifies how representative  the name is for the topic it represents. As in the case of the topic semantic compactness, the name saliency  takes natively semantics  into account via  in Eq. .   In both Eq.  and Eq. , the cosine similarity is used as similarity measure.   Financial support for the research was provided by Bayer AG. The authors reports a  patent application on  submitted on April 21st, 2020 .      {}   A.Z. led and thereby ideated and implemented the topic discovery algorithm, and is the main author the manuscript. M.S., C.B., D.R. provided valuable suggestions on the topic discovery algorithm. C.B., O.T., and T.W. designed and implemented the software architecture and data engineering pipeline for the algorithm deployment. T.W., J.V., J.L., S.K., X.M., A.M., D.R., and M.S. provided the in-house resources for the study, supervised the overall project, and provided domain knowledge expertise. All authors revised and commented on the manuscript.   The data used in the study are the proprietary of Bayer AG, and not publicly available.   A.Z. thanks Robin Williams and Nikki Hayward from Bayer\texttrademark\ Medical Information for providing expert insightful and in-depth feedback on the results of topic discovery.     include your own bib file like this:       
"," %141 words % the motivation Millions of unsolicited medical inquiries are received by pharmaceutical companies every year.  It has been hypothesized that these inquiries represent a treasure trove of information, potentially giving insight into matters regarding medicinal products and the associated medical treatments.  % the challenge However, due to the large volume and specialized nature of the inquiries, it is difficult to perform timely, recurrent, and comprehensive analyses. % the solution Here, we propose a machine learning approach based on natural language processing and unsupervised learning to automatically discover key topics in real-world medical inquiries from customers. This approach does not require ontologies nor annotations.  % the results The discovered topics are meaningful and medically relevant, as judged by medical information specialists, thus demonstrating that unsolicited medical inquiries are a source of valuable customer insights. % the implications and outlook Our work paves the way for the machine-learning-driven analysis of medical inquiries in the pharmaceutical industry, which ultimately aims at improving patient care.",465
" Dynamic models of text aim at characterizing temporal changes in patterns of document generation. Most successful dynamic language models are Bayesian in nature, and lag behind state-of-the-art deep language models in terms of expressibility. A natural space to study some of the temporal aspects of language is that of the large review datasets found in e-commerce sites.  The availability of millions of reviewed items, such as business or services, books or movies, whose reviews have been recorded in time scales of years, opens up the possibility to develop deep scalable models that can predict the change in taste and preference of users as time evolves. Originally, the interaction of users in these e-commerce sites were studied in the context of collaborative filtering, where the goal was to predict user ratings, based on user interaction metrics. Here we aim to look directly at the content of reviews as time evolves.  %More KDD probably, to much focus on the ratings and recommendations  %-------- %The shear size of e-commerce and review web sites naturally lend itself to the development of data mining tools which are able to provide users with a way to sort out relevant information. This is the task assigned to recommender systems.  Originally kick started by the Netflix competition, matrix factorization  methods through collaborative filtering, aim at predicting user ratings based on user interaction metrics. This rating based methods are lacking as they are unable to clarify the nature of the user preferences, in particular how those preferences change on time. In order to address this issue, methodologies that exploit costumers reviews are gaining attention.  %--------- Costumer reviews provide a rich and natural source of unstructured data which can be leverage to improve recommender system performance . Indeed, reviews are effectively a form of recommendation. % Recently, a variety of deep learning solutions for recommendation have profit from their ability to extract latent representations from review data, encoding rich information related to both users and items. % %Review  content naturally encodes  % This type of data  % Review content is of contextual nature, as the text arises from the interaction of user preferences and items at hand.  % Time represents yet another dimension of context, as user preference and item availability change with time % -- and indeed, % causal and temporal relations have been known to improve the performance of recommender systems  .  % Despite this fact, % recent natural language processing  methodologies for rating and reviews  lag behind at incorporating temporal structure in their language representations. In the present work we exploit recurrent neural network  models for point processes, and feed them neural representations of text, to characterize costumer reviews. Our goal is to capture the changes in user taste and item importance during time, and to exploit those changes to better predict when are new reviews arriving, and what do they actually say. We summarize our contributions as follows:      {}  % [ht!] %       %      %      %     We present the related work in Section  and introduce our model in Section . The baseline models used for comparison in this paper are presented in Section . The experimental setup and results are presented in Section . Finally, in Section  we conclude and discuss future work.     In this work we introduced neural dynamic language models of text for review data. We are able to leverage dynamic representations of point process models in language modelling tasks, and augment the point processes with text representations.     . We provide two dynamical models, as well as their extension through two different language models: recurrent and temporal convolution networks.  We showed that our approach improves performance on both content and arrival times prediction, as well as opens the door for dynamic generative language models. Future work includes the implementation of attention mechanisms, as well as the inclusion of neural factorization machines aimed at predicting ratings values.     
"," Deep neural network models represent the state-of-the-art methodologies for natural language processing.  % Here we build on top of these methodologies to incorporate temporal information and model how review data changes with time. % Specifically, we use the dynamic representations of recurrent point process models, % % which encode the nonlinear relations between content and timing of the reviews received by e.g. businesses or services,  % which encode the history of how business or service reviews are received in time,  % to generate instantaneous language models with improved prediction capabilities.  % Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations.  % % as that encoded in recurrent point process models, and improve the predictive power of these model by incorporating the text representations.  % % Our methodologies resemble that of a hierarchical model, whereupon the temporal information is used as a  representation for the language model.  % We provide recurrent network and temporal convolution solutions for modeling the review content. % We deploy our methodologies in the context of recommender systems,  % as to enhance the expressibility of current models, % effectively characterizing the change in preference and taste of users as time evolves. Source code is available at .",466
"  Most authentication methods commonly used today rely on users setting custom passwords to access their accounts and devices. Password-based authentications are popular due to their ease of use, ease of implementation and the established familiarity of users and developers with the method.   However studies show that users tend to set their individual passwords predictably, favoring short strings, names, birth dates and reusing passwords across sites.  Since chosen passwords exhibit certain patterns and structure, it begs the question whether it is possible to simulate these patterns and generate passwords that a human user realistically might have chosen.  Password guessing is an active field of study, until recently dominated by statistical analysis of password leaks and construction of corresponding generation algorithms . These methods rely on expert knowledge and analysis of various password leaks from multiple sources to generate rules and algorithms for efficient exploitation of learned patterns.  On the other hand, in recent years major advances in machine-driven text generation have been made, notably by novel deep-learning based architectures and efficient training strategies for large amounts of training text data. These methods are purely data driven, meaning they learn only from the structure of the input training text, without any external knowledge on the domain or structure of the data. % Deep learning models have recently shown remarkable performance concerning text classification and text generation.  Major advancements in the field have been fueled by the development in several central directions such as:   	 mechanisms}. Considering a token  within a textual environment, the idea is to develop a flexible notion of context that connects the given token with other pieces of the textual environment. Intuitively, this allows the learning model to better grasp the textual structure , thus leading to eventual improvements in terms of text classification/generation/interpretability. Among others, well-known attention-based examples are given by BERT, ELMO, GPT and various further types of Transformers. 	 	. Remarkable  \\progress has been made in designing more flexible deep learning structures . The success of these deep neural networks can to a large extent be attributed to their representation capability, i.e. they create an appropriate transformation of the data  that renders the data easier to handle and solve a given problem. In this regard a central class of deep learning models is given by the so-called 	extbf{autoencoders} whose goal is not only to create a meaningful and useful data representation/transformation  but also to be able to go back and reconstruct the initial data from the representation . An upshot is that one could generate new data by sampling points in the representation space and then decoding back. 	 	. The above tools would not be as efficient, had not it been for the corresponding methods to select  the parameters and weights of the neural networks. Among others these include appropriate momentum and annealing-driven stochastic gradient descents, Wasserstein regularization and variational approaches.   In this paper we will continue the exploration of data driven deep-learning text generation methods for the task of password-guessing. While some applications to password guessing already show promising results, most frameworks still can not reach or surpass state-of-the-art password generation algorithms. % On the other hand, considering password guessing problems, some popular frameworks  as well as a large body of state-of-art research suggest that advanced deep learning methodologies are still to be further explored.  Ideally, one would attempt to design more efficient password-guessing models aided by neural networks and cutting-edge practices.  Our findings and contributions can be summarized as follows:                      The present work illustrates various deep learning password generation techniques. Conducting a thorough unified analysis we discuss password-matching capabilities, variability and quality of sampling and robustness in training. On one hand, we bridge and extend previous methods based on attention schemes, GANs and Wasserstein autoencoding; on the other hand, we provide a promising novel approach based on Variational Autoencoders that allows for efficient latent space modeling and further sampling mechanisms. Lastly, we hope our work will facilitate and provide benchmark lines for further deep learning and ML practitioners interested in the field of password guessing.  In terms of further investigation, the application of deep learning techniques to password generation poses further intriguing questions on the interplay between classical probabilistic methods and neural networks, where one would ultimately hope to construct more efficient and reliable domain-inspired password representation schemes - e.g. based on carefully crafted fragmentations.   
","     Password guessing approaches via deep learning have recently been investigated with significant breakthroughs in      their ability to generate novel, realistic password candidates.     In the present work we study a broad collection of deep learning and probabilistic based models in the light of password guessing:      attention-based deep neural networks, autoencoding mechanisms and generative adversarial networks.      We provide novel generative deep-learning models in terms of variational autoencoders exhibiting state-of-art sampling performance,     yielding additional latent-space features such as interpolations and targeted sampling.     Lastly, we perform a thorough empirical analysis in a unified controlled framework over well-known datasets .      Our results not only identify the most promising schemes driven by deep neural networks, but also illustrate the strengths of each approach in terms of generation variability and sample uniqueness.",467
" % 1 page  % Definition and importance of the causality knowledge.  % causality knowledge, as an important knowledge for artificial intelligence  systems, has been proven helpful in many downstream tasks, especially in the NLP domain. % % In this work, we follow ConceptNet and COPA to focus on the causal relations between daily events. % However, due to the lack of a high-quality and large-scale causality knowledge resource, the application of causality knowledge in downstream tasks is still limited.  Humans possess a basic knowledge about facts and understandings for commonsense of causality in our everyday life.  For example, if we leave five minutes late, we will be late for the bus; if the sun is out, it's not likely to rain; and if we are hungry, we need to eat. %Causality is an important commonsense reasoning that humans use all the time,  Such causality knowledge has been shown to be helpful for many NLP tasks. Thus, it is valuable to teach machines to understand causality.   Causal relations in the commonsense domain are typically contributory and contextual.  %   By contributory\footnote{The other two levels are absolute causality  and conditional causality , which commonly appear in the scientific domain rather than our daily life.}, we mean that the cause is neither necessary nor sufficient for the effect, but it strongly contributes to the effect.  By contextual, we mean that some causal relations only make sense in a certain context. The contextual property of causal relations is important for both the acquisition and application of causal knowledge. For example, if some people tell the AI assistant  ``they are hungry'' in a meeting, a basic assistant may suggest them to order food because it has the knowledge that `being hungry' causes `eat food'. A better assistant may suggest ordering food after the meeting because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the meeting context.  % \ye{I made small adaptation to this paragraph } % For example, if a person is in the middle of a meeting, he/she may tell the AI assistant  that he/she is hungry, a good AI assistant may suggest him/her to eat some food because it has the knowledge that `being hungry' cause `eat food', but an extraordinary AI assistant may suggest that ``I can help order some food for you to eat after the meeting'' because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the context of a meeting. Without understanding the contextual property of causal knowledge, achieving such a level of intelligence would be challenging.  To help machines better understand the causality commonsense, many efforts have been devoted into developing the causality knowledge bases.  For example, ConceptNet and ATOMIC leverage  human-annotation to acquire small-scale but high-quality causality knowledge. After that, people try to leverage linguistic patterns  to acquire causality knowledge from textual corpus. However, causality knowledge, especially those trivial knowledge for humans, are rarely formally expressed in documents, a pure text-based approach might struggle at covering all causality knowledge. Besides that, none of them take the aforementioned contextual property of causal knowledge into consideration, which may restrict their usage in downstream tasks.     % Causal relations in the commonsense domain are typically contributory and contextual.  % By contributory\footnote{The other two levels of causality are absolute causality  and conditional causality , which commonly appear in the scientific domain rather than our daily life.}, we mean that the cause is neither necessary nor sufficient for the effect, but it strongly contributes to the effect.  % By contextual, we mean that some causal relations only make sense in a certain context. % The contextual property of causal relations is important for both the acquisition and application of causality knowledge. % For example, if some people tell the AI assistant  ``they are hungry'' in a meeting, a basic assistant may suggest them to order food because it has the knowledge that `being hungry' causes `eat food'. A better assistant may suggest ordering food after the meeting because it knows that the causal relation between `being hungry' and `eat food' may not be plausible in the meeting context.  % Without understanding the contextual property of causality knowledge, achieving such a level of intelligence would be challenging.  % [t] %      % % limitation of existing acquisition methods % Conventional approaches  \ye{i think this should be more elaborated. maybe give an example?} However, two drawbacks of these approaches significantly limit their usage in downstream tasks: % [leftmargin=*] %     %  %      %     In this paper, we propose to ground causality knowledge into the real world and explore the possibility of acquiring causality knowledge from visual signals .  By doing so, we have three major advantages:  Videos can be easily acquired and can cover rich commonsense knowledge that may not be mentioned in the textual corpus;  Events contained in videos are naturally ordered by time. As discussed by, there exists a strong correlation between temporal and causal relations, and thus such time-consecutive images can become a dense causality knowledge resource;  Objects from the visual signals can act as the context for detected causality knowledge, which can remedy the aforementioned lack of contextual property issue of existing approaches.   To be more specific, we first define the task of mining causality knowledge from time-consecutive images and propose a high-quality dataset .  To study the contextual property of causal relations, for each pair of events, we provide two kinds of causality annotations: one is the causality given certain context and the other one is the causality without context.  Distribution analysis and case studies are conducted to analyze the contextual property of causality. An example from Vis-Causal is shown in Figure, where the causal relation between ``dog is running'' and ``blowing leaves'' only makes sense when the context is provided because the dog is running on the leaves, so its high speed and quickly-moved pow cause the leaves blow around. Without the context  ``leaves on the ground'', this causal relation is implausible. After that, we propose a Vision-Contextual Causal  model, which can effectively leverage both the pre-trained textual representation and visual context to acquire causality knowledge and can be used as a baseline method for future works. Experimental results demonstrate that even though the task is still challenging, by jointly leveraging the visual and contextual representation, the proposed model can better identify meaningful causal relations from time-consecutive images. To summarize, the contributions of this paper are three-fold:  We formally define the task of mining contextual causality from the visual signal;  We present a high-quality dataset Vis-Causal;  We propose a Vision-Contextual Causal  model to demonstrate the possibility of mining contextual causality from the vision signal. % Experimental results prove that considering context is crucial for understanding causality and representing the visual context with textual representation is helpful. % Further analysis shows that the proposed task is still challenging for current models, and we may need to consider injecting external knowledge to better understand the videos and acquire causality knowledge. % \ye{there's no real reference to the text part in the into, NLP people might think it's not suitable for ACL? maybe add that the models use some description and objects which are represented in a textual form}   %   %   In this paper, we explore the possibility of learning causality knowledge from time-consecutive images. To do so, we first formally define the task and then create a high-quality dataset Vis-Causal , which contains 4,000 image pairs, 23,558 event pairs, and causal relation annotations under two settings. On top of the collected dataset, we propose a Vision-Contextual Causal  model to demonstrate that with the help of strong pre-trained textual and visual representations and careful training, it is possible to directly acquire contextual causality from visual signals. Further analysis shows that even though VCC can outperform all baseline methods, it is still not perfect. As the visual signal could serve as an important causality knowledge resource, we will keep exploring how to better acquire causal knowledge from the visual signal  in the future.    effectively leverage both the pre-trained textual representation and visual context to learn causality from visual signals, which can also preserve the contextual property of extracted causality knowledge.   Experiments and analysis demonstrate the importance of both the pre-trained textual representation and visual context.   Experiment results show that the task is challenging for all current models.   Further analysis also proves our observation that context is crucial for understanding causal relations.   Further analysis also suggests the importance of leveraging external knowledge for better causal relation extraction. Both the dataset and code will be released to encourage research on the causality acquisition.  
","  Causality knowledge is crucial for many artificial intelligence systems. Conventional textual-based causality knowledge acquisition methods typically require laborious and expensive human annotations. As a result, their scale is often limited. Moreover, as no context is provided during the annotation, the resulting causality knowledge records  typically do not take the context into consideration. To explore a more scalable way of acquiring causality knowledge, in this paper, we jump out of the textual domain and investigate the possibility of learning contextual causality from the visual signal. Compared with pure text-based approaches, learning causality from the visual signal has the following advantages:  Causality knowledge belongs to the commonsense knowledge, which is rarely expressed in the text but rich in videos;  Most events in the video are naturally time-ordered, which provides a rich resource for us to mine causality knowledge from;  All the objects in the video can be used as context to study the contextual property of causal relations. In detail, we first propose a high-quality dataset Vis-Causal and then conduct experiments to demonstrate that with good language and visual representation models as well as enough training signals, it is possible to automatically discover meaningful causal knowledge from the videos. Further analysis also shows that the contextual property of causal relations indeed exists, taking which into consideration might be crucial if we want to use the causality knowledge in real applications, and the visual signal could serve as a good resource for learning such contextual causality. Vis-Causal and all used codes are available at: \url{https://github.com/HKUST-KnowComp/Vis_Causal}. % In detail, we first identify events from the videos, which are represented with natural sentences, and then leverage the visual signal to predict the contextual causal relations among these events.     % In this work, we mimic how human beings learn causality and explore the possibility of acquiring causality knowledge with visual signal. % To do so, we first define the task of mining contextual causality knowledge from visual signals, which aims at evaluating models' abilities to identify causal relation given certain visual context, and then employ the crowd-sourcing to annotate a high-quality dataset Vis-Causal. % On top of that, we propose a Vision-Contextual Causal  model that can utilize the images as context to better acquire causality knowledge. % Different from existing \revisehm{causality knowledge acquisition works}, \revisehm{to the best of our knowledge, }the proposed solution \revisehm{is the first one that }has the potential to preserve contextual property  of causal relations.",468
"  % The advent of deep learning techniques has dramatically improved accuracy of speech recognition models . Deep learning techniques first saw success by replacing the Gaussian Mixture Model  of the Acoustic Model  part of the conventional speech recognition systems  with the Feed-Forward Deep Neural Networks  , further with Recurrent Neural Network  such as the  Long Short-Term Memory  networks  or Convonlutional Neural Networks . In addition to this, there have been improvements in noise robustness by using models motivated by auditory processing , data augmentation techniques , and beam-forming . Thanks to these advances, voice assistant devices such as Google Home  and Amazon Alexa have been widely used at home environments.  Nevertheless, it was not easy to run such high-performance speech recognition systems  on devices largely because of the size of the Weighted Finite State Transducer   handling the lexicon and the language model.  Fortunately, all-neural end-to-end  speech recognition systems were introduced which do not need a large WFST or an n-gram Language Model   . These complete end-to-end systems have started surpassing the performance of the conventional WFST-based decoders with a very large training  dataset  and a better choice of target unit  such as Byte Pair Encoded  subword units.  In this paper, we provide a comprehensive review of the various components and algorithms of an end-to-end speech recognition system. In Sec., we give a brief overview of the various neural building blocks of an E2E Automatic Speech Recognition  model. The most popular E2E ASR architectures are reviewed in Sec.. Additional techniques used to improve the performance of E2E ASR models are discussed in Sec.. Techniques used for compression and quantization of the all-neural E2E ASR models are covered in Sec.. Sec. gives a summary of the paper. % % %# Data augmentation and overfitting    In this paper, we reviewed various end-to-end all neural automatic speech recognition systems and their optimization techniques for on-device applications. On-device speech recognition has huge advantages compared to the server-side ones in terms of user privacy, operation without internet, server-cost, and latency.  To operate speech recognition systems on embedded processors, we need to  consider several factors such as recognition accuracy, computational cost, latency,  and the model size. We compared pros and cons of different neural network components such as Long Short-Term Memory , Convolutional Neural Network ,   and attention mechanism. We explained and compared different end-to-end neural speech recognition architectures such as  a stack of LSTM layers with the Connectionist Temporal Classification  loss ,  Recurrent Neural Network-Transformer , attention-based models, and models based on Monotonic Chunk-wise Attention  . Further improvement is achieved by combining a streaming model with a low-latency non-streaming model,  by applying shallow-fusion with a Language Model , and by applying spell correction using a list of named entities . We also discussed several model compression techniques including quantization, singular value decomposition, pruning, and knowledge distillation.  These recent advances in all neural end-to-end speech recognition made it possible to commercialize all neural on-device end-to-end speech recognition systems  .                           
","   In this paper, we review various end-to-end automatic speech recognition   algorithms and their optimization techniques for on-device applications.   Conventional speech recognition systems comprise a large number of discrete   components such as an acoustic model, a language model, a pronunciation model,    a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted Finite State   Transducer , and so on. To obtain sufficiently high speech recognition   accuracy  with such conventional speech recognition systems, a very large   language model  is usually needed. Hence, the corresponding   WFST size becomes enormous, which prohibits their on-device implementation. Recently, fully neural network end-to-end speech recognition algorithms have been   proposed. Examples include speech recognition systems based on  Connectionist Temporal Classification , Recurrent Neural Network Transducer , Attention-based Encoder-Decoder models , Monotonic   Chunk-wise Attention ,    transformer-based speech recognition systems, and so on. These fully neural   network-based systems require much smaller memory footprints compared to   conventional algorithms, therefore their on-device implementation has become   feasible. In this paper, we review such end-to-end speech recognition models.   We extensively discuss their structures, performance, and advantages compared   to conventional algorithms.",469
" %  %     God, grant me the serenity to accept the things I cannot change, % courage to change the things I can, and wisdom to know the difference.\\ %  -Serenity Prayer %     Solving math word problems  poses unique challenges for understanding natural-language problems and performing arithmetic reasoning over quantities with commonsense knowledge. As shown in \autoref{fig:example}, a typical MWP consists of a short narrative describing a situation in the world and asking a question about an unknown quantity. To solve the MWP in \autoref{fig:example}, a machine needs to extract key quantities from the text, such as ""100 kilometers"" and ""2 hours"", and understand the relationships between them. General mathematical knowledge like ""distance = velocity  time"" is then used to calculate the solution.   %The task of automatically solving Math Word Problems  requires mapping the human-readable natural language into machine-understandable logic forms, e.g., expressions, followed by execution process that calculates the numeric answer. \autoref{fig:example} an example of a math word problem, the ground truth answer and the expression that derives the answer when executed. Recently, researchers have focused on solving MWPs using neural models. The advantage of these neural models is that they do not rely on hand-crafted features.  Researchers have recently focused on solving MWPs using neural-symbolic models. These models usually consist of a neural perception module  \times 100 + 100/2 \times 3.5 if we want to calculate the speed first and then multiply it by total hours. Or, we can solve it by , if we first compute the length of the second part of the journey given the ratio of time spans, and add it to the first part. However, only the first expression is given as the ground-truth expression in the dataset, and thus neural models tend to ``punish'' the second expression. In this way, fully-supervised learning fails to generate more diverse and correct expressions. The second problem is ``train-test discrepancy''. It means that MLE uses a surrogate objective of maximizing equation likelihood during training, while the evaluation metric of the task is solution accuracy, which is non-differentiable.  proposes to solve this via reinforcement learning but still use pre-trained MLE model. Last but not least, there's the problem of lack of fully annotated data online. Recruiting crowd-workers to provide the correct equations is time consuming. However, thousands of MWPs have been posted in online forums, where the final answers can be easily mined. These data can be useful if we can train our model without the supervision of expressions.   %To address these issues, we propose to solve the MWPs with weak supervision, where only the problem texts and the final answers are required for learning. % We adopt the goal-driven tree-structured  model proposed by~ as the base model. %Since the execution process of arithmetic expressions in previous deep learning models is non-differentiable, it is infeasible to use back-propagation to compute gradients. A straightforward approach is to employ policy gradient methods like REINFORCE. In weakly-supervised MWP, policy gradient methods explore the solution space and update the policy based on generated solutions that happen to hit the right answers, while incorrect solutions are totally abandoned. Since the solution space is quite large, policy gradients methods usually converge slowly or sometimes even fail to converge.  To improve the efficiency of weakly-supervised learning, we propose a novel fixing mechanism to learn from incorrect predictions, which is inspired by the human ability to learn from failures via abductive reasoning. The fixing mechanism propagates the error from the root node to the leaf nodes in the solution tree and finds the most probable fix that can generate the desired answer. The fixed solution tree is further used as a pseudo label to train the neural model. \autoref{fig:framework} shows how the fixing mechanism corrects the wrong solution tree by tracing the error in a top-down manner.  Furthermore, we design two practical techniques to traverse the solution space and discover possible solutions efficiently. First, we observe a positive correlation between the number of quantities in the text and the size of the solution tree , and propose a tree regularization technique based on this observation to limit the range of possible tree sizes and shrink the solution space. Second, we adopt a memory buffer to track and save the discovered fixes for each problem with the fixing mechanism. All memory buffer solutions are used as pseudo labels to train the model, encouraging the model to generate more diverse solutions for a single problem.   In summary, by combining the fixing mechanism and the above two techniques, the proposed learning-by-fixing  method contains an exploring stage and a learning stage in each iteration, as shown in \autoref{fig:framework}. We utilize the fixing mechanism and tree regularization to correct wrong answers in the exploring stage and generate fixed expressions as pseudo labels. In the learning stage, we train the neural model using these pseudo labels.  We conduct comprehensive experiments on the Math23K dataset. The proposed LBF method significantly outperforms the reinforcement learning baselines in weakly-supervised learning and achieves comparable performance with several fully-supervised methods. Furthermore, our proposed method achieves significantly better answer accuracies of all the top-3/5 answers than fully-supervised methods, illustrating its advantage in generating diverse solutions. The ablative experiments also demonstrate the efficacy of the designed algorithms, including the fixing mechanism, tree regularization, and memory buffer.  % This paper makes three major contribution: % [leftmargin=*,noitemsep,nolistsep] %    %Policy gradient methods like REINFORCE are frequently used in weakly-supervised tasks . Such methods suffer from sparse reward, cold start and inefficient exploration of solution space. This is because neural network make wrong perceptions and generate negative samples which are ``abandoned'' by REINFORCE. Human beings, like neural networks, tend to make inaccurate perceptions. However, they embody the skills to correct their misperceptions when reasoning about the wrong forms and guessing about correct patterns. What's more, they are able to approach the solutions in diverse ways. For example, in \autoref{fig:framework}, a child might provide a wrong expression  given the problem. Then he started to reason about where he did wrong and found out he could actually fix the expression by replacing the first """" with """". The fixed expression looks nothing like the ground truth expression *$) provided by the dataset.    % Therefore, policy gradients methods converge slowly or even fail to converge without MLE pre-training on fully-supervised data.  %Inspired by this, we propose a novel fixing mechanism which resembles human閳ユ獨 ability to diagnose and fix the expressions that cannot generate desired answers. The ground truth answer  is propagated through the expression tree in a top-down manner. In the meantime, we try depth-first-search for a possible fix. Similar to Memory-Augmented Policy Optimization which utilizes a memory buffer to save previous successful trajectories given by REINFORCE, we adopt a memory buffer to store all successful fixes. Different expressions in the buffer are all used to train the model, thus allowing us to generate more diverse answers.  % Our contributions are summarized as following: % [leftmargin=*,noitemsep,nolistsep] %  %      &  \makecell[l]{A truck travels 100 kilometers in 2 hours. At this\\ speed, if it travels for another 3.5 hours, how many\\ kilometers will it complete for the entire journey?}\\ %     & 275\\ %     \\ of Math23K} & *\\ %     \\ Our Model}&\makecell[l]{100/2*3.5+100, 100+100*3.5/2,\\ 100+100/, 100/), *}\\ %     \relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21}  % DO NOT CHANGE THIS \usepackage{times}  % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier}  % DO NOT CHANGE THIS \usepackage[hyphens]{url}  % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm}  % DO NOT CHANGE THIS \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing  % DO NOT CHANGE THIS {8.5in}  % DO NOT CHANGE THIS {11in}  % DO NOT CHANGE THIS % additional packages \usepackage{latexsym} \usepackage{makecell} \usepackage{amsmath,amssymb,mathtools,bm,etoolbox} \usepackage{algorithm} \usepackage[noend]{algorithmic} \usepackage{enumitem} \usepackage{xcolor} \usepackage{pifont} \usepackage{multirow} \usepackage{diagbox} \usepackage[switch]{lineno} \usepackage[autostyle=false, style=english]{csquotes} \MakeOuterQuote{""} \usepackage[draft]{hyperref} %  Disable links - according to AAAI format guide  \DeclarePairedDelimiter{\rceil} \DeclarePairedDelimiter\floor{\lfloor}{\rfloor}  {Algorithm}     \renewcommand{  In this work, we propose a weakly-supervised paradigm for learning MWPs and a novel learning-by-fixing framework to boost the learning. Our method endows the MWP learner with the capability of learning from wrong solutions, thus significantly improving the answer accuracy and learning efficiency.   Specifically, the fixing mechanism endows the MWP learner with the capability of learning from wrong solutions, thus significantly improving the answer accuracy and learning efficiency. The tree regularization efficiently shrinks and explores the solution space by limiting the tree size within a empirical range. The memory buffer encourages the model to learn diverse solutions for each problem.  One future direction of the proposed model is to prevent generating equivalent or spurious solutions during training, possibly by making the generated solution trees more interpretable with semantic constraints.  
"," % Most previous solvers of math word problems  are learned with full supervision and fail to generate diverse solutions for each problem. In this paper, we address this issue by introducing a weakly-supervised paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel learning-by-fixing  framework that mimics the human ability to learn from incorrect predictions. Specifically, the fixing mechanism propagates the error from the root node to the leaf nodes of a solution tree and infers the most probable fix that can be executed to the desired answer. To generate more diverse solutions, tree regularization is applied to guide the efficient shrinkage and exploration of the solution space, and a memory buffer is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions. Previous neural solvers of math word problems  are learned with full supervision and fail to generate diverse solutions. In this paper, we address this issue by introducing a weakly-supervised paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel learning-by-fixing  framework, which corrects the misperceptions of the neural network via symbolic reasoning. Specifically, for an incorrect solution tree generated by the neural network, the fixing mechanism propagates the error from the root node to the leaf nodes and infers the most probable fix that can be executed to get the desired answer. To generate more diverse solutions, tree regularization is applied to guide the efficient shrinkage and exploration of the solution space, and a memory buffer is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions.",470
"  % Hate speech, its extensiveness, effect     % --> humans can't inspect every sample  % Memes, their use & hateful memes % HM data set % challenge       % --> not all dataset is 'truly' requires multi-modality     % --> HM dataset proposes 'benign confounders' to make sure multimodality is required    Memes have gained huge popularity over the past years, resulting in over 180m posts on different social media platforms until 2018. Although memes are oftentimes harmless and generated especially for humorous purposes, they have also been used to produce and disseminate hate speech in toxic communities. Hate Speech  is a direct attack on people based on race, ethnicity, national origin, religious affiliation, sexual orientation, sex, gender, and serious disease or disability -- a growing problem in modern society. Giant tech companies, such as Facebook, own platforms where millions of users log in daily and they are obliged to remove a tremendous amount of HS to protect their users. According to Mike Schroepfer, Facebook CTO, they took an action on  pieces of content for violating their HS policies in the first quarter of 2020. This amount of malicious content cannot be tackled by having humans inspect every sample. Consequently, machine learning and in particular deep learning techniques are required to alleviate the extensiveness of online hate speech. Detecting hate speech in memes is challenging due to the multimodal nature of memes . Therefore, these techniques have to process the content the way humans do: holistically. When viewing a meme, a human would not think about the words and the picture independently; but understand the combined meaning. Moreover, while the visual and linguistic information of a meme is typically neutral or funny individually, their combination may result in a hateful meme.      A recent study shows that state-of-the-art methods for hate speech detection in multimodal memes perform poorly compared to humans:  vs.  accuracy. To catalyze sophisticated research in this area, Facebook AI launched the Hateful Memes Challenge and published a dataset containing more than 10,000 newly created multimodal memes. Multimodal tasks reflect many real-world problems, including how humans perceive and understand the world around them.       There has been a surge of interest in multimodal problems since 2015 in visual question answering, image captioning, speech recognition and beyond. But it is not always clear to what extent genuinely multimodal reasoning and understanding are needed to solve current challenges. For instance, for some datasets language can unintentionally impose strong priors, which might result in a remarkable performance, without any understanding of the visual content. The Hateful Memes challenge design and dataset are created to encourage and measure truly multimodal understanding and reasoning of the models. A key point to achieve this are the so-called ``benign confounders''  which addresses the risk of exploiting unimodal priors by models: for every hateful meme, there are alternative images or text that flip the label to not-hateful. Such image and text confounders require multimodal reasoning to classify the original meme and its confounders correctly. Thus, making the dataset challenging and appropriate for testing the true multimodality of a model.    In the following, we analyze the challenge dataset and describe our prize-winning solution that placed third among 3,173 participants in the Hateful Memes Challenge in detail. Our solution achieves  AUROC with an accuracy of  on the challenge test set, which improves all the benchmark models, including the state-of-the-art models at that time, such as ViLBERT  and VisualBERT . Nevertheless, the accuracy is still behind humans with a mentionable gap, highlighting the need for progress in multimodal research.        We proposed an approach detecting hate speech in internet memes multimodally, i.e. considering visual and textual information holistically. We took part in the Hateful Memes Challenge and placed third out of 3,173 participants. Our approach utilizes a pre-trained VisualBERT , fine-tuned on an expanded train dataset, finally applying Majority Voting over the 27 best models. Our approach achieves 0.811 AUROC with an accuracy of 0.765 on the challenge test set, which is a considerable result but also shows that we are still far from the accuracy of human judgement.     \small 
","   Memes on the Internet are often harmless and sometimes amusing. However, by using certain types of images, text, or combinations of both, the seemingly harmless meme becomes a multimodal type of hate speech -- a hateful meme. The Hateful Memes Challenge\footnote{\url{https://www.drivendata.org/competitions/70/hateful-memes-phase-2/}} is a first-of-its-kind competition which focuses on detecting hate speech in multimodal memes and it proposes a new data set containing 10,000+ new examples of multimodal content. We utilize VisualBERT -- which meant to be the ``BERT of vision and language'' -- that was trained multimodally on images and captions and apply Ensemble Learning. Our approach achieves 0.811 AUROC with an accuracy of 0.765 on the challenge test set and placed third out of 3,173 participants in the Hateful Memes Challenge\footnote{HateDetectron at \url{https://www.drivendata.org/competitions/70/hateful-memes-phase-2/leaderboard/}}. The code is available at  %        \url{https://github.com/rizavelioglu/hateful_memes-hate_detectron}     %",471
"  The internet is having a huge impact on all of our lives and our virtual presence reflects both our personalities and beliefs but also our biases and prejudices. Billions of people are interacting with various online content every day and while some of it is highly useful and enriches our knowledge and understanding of the world, an increasing portion of this content is also harmful. This includes hate speech, misinformation and other forms of online abuse. An increasing amount of effort is required to quickly detect this content, scale up the review work and make automatic decisions to take down the harmful media fast in order to minimize the inflicted harm to the readers.  Many of our interactions happen on social media platforms, which we use to share messages and pictures with our private community or general public audiences.  Facebook AI has launched a competition  to flag hateful memes consisting of both images and text. For this purpose they provide a unique labeled dataset of 10,000+ high quality new multimodal memes. The goal of the challenge is to create an algorithm that identifies multimodal hate speech in internet memes, while also being robust to their benign flip. A meme might be mean or hateful either because of the meme image itself, or the text or their combination. Benign flipping is an augmentation technique used by the competition organizers to flip a meme from hateful to non-hateful and viceversa. This requires changing either the meme text or the image to flip its label. Figure shows how this process works.  Since the problem is formulated as a binary classification task, the primary evaluation metric used to rank the results is the area under the receiver operating characteristic curve . This represents the area under the ROC curve, which in turn plots the True Positive Rate  vs. False Positive Rate  at different classification thresholds T. The goal is to maximize the AUROC.   Accuracy is the secondary tracked metric and it calculates the percentage of instances where the predicted class \^{y} matches the actual class, y in the test set.   Ideally, the model maximizes both these metrics.  In summary, our contribution is threefold:                In this work, we propose a simple yet effective set of techniques to help detect hate speech in a unique labeled dataset of high quality multimodal memes from Facebook AI. The goal is to identify hate speech using a multimodal model, while also being robust to the ""benign confounders"" that cause the binary label indicating whether a meme is hateful to flip.  We experiment with a number of large pre-trained Transformer based architectures and fine-tune both single stream state-of-the-art models such as VL-BERT, VLP and UNITER and dual stream models such as LXMERT. We compare their performance to the baselines provided by  and show all of the single-stream models significantly outperform these. We justify our choice for these transformers architectures by the possible advantages coming from the fact they were pre-trained on a wide spectrum of datasets from different domains. We also propose and adapt a novel bidirectional cross-attention mechanism to couple inferred caption information with the meme text obtained through optical character recognition. This addition achieves higher classification accuracy in labeling memes as hateful. Furthermore we show deep ensembles, a simple yet very powerful trick can improve on single model predictions by a significant margin. As expected, we find training these large architectures from scratch performs poorly on a small set of examples such as the Hateful Memes dataset. However, we also find the choice of pre-training datasets also matters in terms of domain similarity to the fine-tuning dataset.   We conclude that although the multimodal models are becoming increasingly sophisticated, there is still a large gap when comparing to human performance. This leaves considerable room for developing new algorithms to deal with multimodal understanding.  \medskip    \small     \clearpage 
","   While significant progress has been made using machine learning algorithms to detect hate speech, important technical challenges still remain to be solved in order to bring their performance closer to human accuracy. We investigate several of the most recent visual-linguistic Transformer architectures and propose improvements to increase their performance for this task. The proposed model outperforms the baselines by a large margin and ranks 5\textsuperscript{th} on the leaderboard out of 3,100+ participants.    \footnote{Code is available at \url{https://github.com/vladsandulescu/hatefulmemes}.}",472
" In traditional ad-hoc retrieval, queries and documents are represented by variants of bag-of-words representations. This leads to the so called vocabulary mismatch problem: when a query contains words that do not exactly match words in a relevant document, the search engine may fail to retrieve this document. Query expansion and document expansion, the methods of adding additional terms to the original query or document, are two popular solution to alleviate the vocabulary mismatch problem.   Document expansion has been shown to be particularly effective for short text retrieval and language-model based retrieval . Most of the existing works in document expansion are unsupervised: using information from the corpus to augment document representation, e.g., retrieval based  and clustering based , or using external information to augment document representation .  Recently,  proposed a new approach to document expansion, which is based on a popular generative sequence-to-sequence model  in NLP, transformers . It leverages supervision to train the model to predict expansion terms conditional on each document. The paper has shown significant improvement on passage  datasets, when trained in-domain. In this paper, we follow this line of supervised neural document expansion approach and explore its performance on standard IR benchmarking dataset. Our main contributions are: 1. Adapting the method to unlabeled datasets by exploring transfer learning and weak-supervision approaches. 2. Adapting the method to traditional IR datasets, where a large number of long documents are present.      We showed that a document expansion model trained on passage-level datasets of  pairs can be directly applied to out-of-domain IR datasets to improve retrieval performance. 
","     Recently,  proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data.     In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.",473
"    Cognitive studies show that human infants develop object individuation skill from diverse sources of information: spatial-temporal information, object property information, and language~. Specifically, young infants develop object-based attention that disentangles the motion and location of objects from their visual appearance features. Later on, they can leverage the knowledge acquired through word learning to solve the problem of object individuation: words provide clues about object identity and type. The general picture from cognitive science is that object perception and language co-develop in support of one another .     Our long-term goal is to endow machines with similar abilities. In this paper, we focus on how language may support object segmentation. Recent work has studied the problem of unsupervised object representation learning, though without language. As an example, factorized, object-centric scene representations have been used in various kinds of prediction~, reasoning~, and planning tasks~, but they have not considered the role of language and how it may help object representation learning.  As a concrete example, consider the input images shown in \fig{fig:teaser} and the paired questions. From language, we can learn to associate concepts, such as {, with the referred object's visual appearance. Further, language provides cues about how an input scene should be segmented into individual objects: a wrong parsing of the input scene will lead to an incorrect answer to the question. We can learn from such failure that the handle belongs to the frying pan  and the chair has four legs .  Motivated by these observations, we propose a computational learning paradigm, \modelfull , associating learned object-centric representations to their visual appearance  in images, and to concepts---words for object properties such as color, shape, and material---as provided in language. Here the language input can be either descriptive sentences or question-answer pairs. \model requires no annotations on object masks, categories, or properties during the learning process.  In \model, four modules are jointly trained. The first is an image encoder, learning to encode an image into factorized, object-centric representations. The second is an image decoder, learning to reconstruct masks for individual objects from the learned representations by reconstructing the input. These two modules share the same formulation as recent unsupervised object segmentation research: learning to decompose the image into a series of { if it's a descriptive sentence.  The correctness of the executor's output and the quality of reconstructed images  are the two supervisory signals we use to jointly train Modules 1, 2, and 4.  %  % % %  We integrate the proposed \model with state-of-the-art unsupervised segmentation methods, MONet~ and Slot Attention~. The evaluation is based on two datasets: ShopVRB~ contains images of daily objects and question-answer pairs; PartNet~ contains images of furniture with hierarchical structure, supplemented by descriptive sentences we collected ourselves. We show that \model consistently improves existing methods on unsupervised object segmentation, % much more likely to group different parts of a single object into a single mask.  We further analyze the object-centric representations learned by \model. In \model, conceptually similar objects  appear to be clustered in the embedding space. Moreover, experiments demonstrate that the learned concepts can be used in new tasks, such as visual grounding of referring expressions, without any additional fine-tuning. %  %  %  % % % % % % % % %  %   {We have proposed \modelfull, a paradigm for learning object-centric representations from vision and language.} Experiments on Shop-VRB-Simple and PartNet-Chairs show that language significantly contributes to learning better representations. This behavior is consistent across two unsupervised image segmentation models.    {Through systematic studies, we have also shown how \model helps models to learn object representations that encode conceptual information, and are useful for downstream tasks such as retrieval, visual reasoning, and referring expression comprehension.}         \clearpage      \clearpage   
","     We present \modelfull , a paradigm for learning disentangled, object-centric scene representations from vision and language. \model builds upon recent advances in unsupervised object segmentation, notably MONet and Slot Attention. While these algorithms learn an object-centric representation just by reconstructing the input image, \model enables them to further learn to associate the learned representations to concepts, \ie, words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. \model can be integrated with various unsupervised segmentation algorithms that are language-agnostic. Experiments show that the integration of \model consistently improves the object segmentation performance of MONet and Slot Attention on two datasets via the help of language. We also show that concepts learned by \model, in conjunction with segmentation algorithms such as MONet, aid downstream tasks such as referring expression comprehension.",474
"   % Automatic speech recognition  systems are typically trained on vast quantity of paired audio and text data to attain competitive performance. Obtaining these paired data requires substantial human annotation efforts and is often time-consuming, expensive and error-prone. With the emerging popularity of end-to-end ASR models, the need for large amounts of training data is more demanding than the conventional hybrid-based ASR systems. For this purpose, semi-supervised learning  is often investigated for speech recognition, where a model is trained using a finite amount of labeled data and a much larger amount of unlabeled data.   In the long history of semi-supervised learning  in speech recognition, self-training approach  and knowledge distillation , or known as teacher-student model training  are the two commonly used SSL methods. Recent success of representation learning enables a new approach towards leveraging unlabeled data. In natural language processing community,  BERT, ELMo, XLNet , GPT   and its follow-ups are classical examples of representation learning. The key philosophy of representation learning is based on using self-supervised learning, where we obtain `free' labels from unlabeled data and train them in a supervised manner via some proxy tasks. In the context of BERT, two proxy tasks are defined including masked language model task and two-sequence prediction task. These proxy tasks are designed to force the learning of a robust, meaningful representation.  After the representation has been learned, a downstream task model is then trained using labeled data with the learned representation. Optionally, the representation learning block and downstream task block can be fine-tuned together.   Learning efficient speech representation can be traced back to restricted Boltzmann machine , which allows pre-training on large amounts of unlabeled data before training the deep neural network speech models.  More recently, speech representation learning has drawn increasing attention in speech processing community and has shown promising results in semi-supervised speech recognition .  The design of proxy tasks in learning speech representation can be categorized into two types. The first type is based on contrastive loss and has been applied to speech representation such as wav2vec and its variants . The model is trained to learn representations containing information that most discriminates the future or masked frame from a set of negative samples via contrastive loss.  The second type is based on reconstructive loss. The proxy task for these representation learning methods is to reconstruct temporal slices of acoustic features based on contextual information. These reconstruction tasks can be defined as autoregressive reconstruction, or masked-based reconstruction. APC  and its follow-up  are examples to use autoregressive reconstruction loss.  In many state-of-the-art pretrained language model task, masked-based prediction is adopted in the proxy tasks such as BERT  and XLNet .  In speech, instead of prediction, we randomly mask temporal slices of acoustic features and attempt to reconstruct them .  Orthogonal to the contrastive-/reconstructive-loss based speech representation learning, vector-quantized speech representations have been proposed. One motivation to apply vector quantization  is that enforcing quantization can lead to better linguistic unit discovery  due to the discrete nature of phonetic units. In VQ-APC , the authors use VQ as a way to limit model capacity and control information needed in encoding representation. In VQ-wav2vec  and wav2vec 2.0 , the author use VQ to facilitate direct application of BERT and other NLP algorithms.  In this paper, we introduce DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We take inspirations from many recent advances in speech representation learning, and propose multiple improvements over vanilla DeCoAR. We summarize the contributions of this paper as follows: [leftmargin=*,itemsep=0pt, topsep=1pt]       % The rest of the paper is organized as follows. Section gives a brief overview of our previous DeCoAR method and related work in vector quantized speech representation learning. Section describes the proposed DeCoAR 2.0 approach. Experimental results on semi-supervised speech recognition are presented in Section followed by conclusion in Section.   % Learning robust speech representation has been exploited in recent years. Among these approaches, wav2vec 2.0  uses 10 minutes of labeled data with 53k hours of unlabeled data to achieve a word error rate  of 5.2/8.6 on LibriSpeech benchmark. The model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the contrastive loss formulation can result in several locally optimal codebooks, for exmaples, acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations.   %Furthermore, the codes at each time step the model select right after their feature encoder hardly contained meaningful phonetic information. So their contrastive approach might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.   % A simple workaround could be using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information in the codes, helping mitigatate the codebook learning problems in contrastive loss as discussed above. And compared to simple reconstruction where we utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. By utilizing the VQ layer, the model is able to keep the representation from those unwanted information flowing.     % Automatic speech recognition  systems are typically trained on vast quantity of paired audio and text data to attain competitive performance. Obtaining these paired data requires substantial human annotation efforts and is often time-consuming, expensive and error-prone. With the emerging popularity of end-to-end ASR models, the need for large amounts of training data is more demanding than the conventional hybrid-based ASR systems. For this purpose, semi-supervised learning  is often investigated for speech recognition, where a model is trained using a finite amount of labeled data and a much larger amount of unlabeled data.   % In the long history of SSL in speech recognition, self-training approach  is the most commonly used approach. In self-training methods, a `seed' ASR model is trained using paired audio/text data. The resulting model is then applied to transcribe the unlabeled audio data. The resulting hypotheses, combined with different data selection criteria, are treated as `pseudo-labels' and added to the original labeled dataset to retrain a new model. Simple in concept, self-training works well in practice with one major caveat - the pseudo-label injects systematic bias introduced by the seed model. To alleviate this, careful confidence calibration with system combinations are often used . Another family of SSL is based on knowledge distillation , or teacher-student model training , and is mostly applied to acoustic model training in hybrid-based ASR. In these setups, a teacher model  generates frame-wise soft label instead of hard label, and a student model is trained on the soft labels via KL divergence loss instead of a standard cross-entropy loss based on forced alignment. The knowledge distillation based SSL partially mitigates the systematic bias but is rarely being investigated towards sequence-level loss  or end-to-end ASR systems.    % Recent success of efficient representation learning, in particular in natural language processing , enables a new approach towards leveraging unlabeled data. Classical examples of representation learning for NLP include BERT, ELMo, XLNet , GPT and its follow-ups , to name but a few.  The key philosophy of representation learning is based on self-supervised learning, where we obtain `free' labels from unlabeled data and train them in a supervised manner via some proxy tasks. In the context of the well-known BERT, two proxy tasks are defined including masked language model task and two-sequence prediction task. These proxy tasks are defined in a way to force the learning of a robust, meaningful representation.  A downstream task is then trained on the labeled data with the learned representation. Optionally, the representation learning block and downstream task can be fine-tuned together.     % This paper presents DeCoAR 2.0, a follow-up on DeCoAR . We take inspiration from many recent advances in speech representation learning, and propose multiple improvements over vanilla DeCoAR. We summarize the contributions of this paper as follows: % [leftmargin=*,itemsep=0pt, topsep=1pt] %       % The rest of the paper is organized as follows. Section gives an overview on related work in speech representation learning, and a brief recap of our previous DeCoAR method. Section describes the proposed vector quantized DeCoAR approach. Experimental results on semi-supervised speech recognition are presented in Section followed by conclusion in Section.   % In this work, we propose an improved speech representation learning paradigms towards semi-supervised speech recognition based on our previous work .   % Current state-of-the-art models for speech recognition require vast amounts of transcribed audio data to attain good performance. In particular, end-to-end ASR models are more demanding in the amount of training data required when compared to traditional hybrid models. While obtaining a large amount of labeled data requires substantial effort and resources, it is much less costly to obtain abundant unlabeled data.   % For this reason, semi-supervised learning  is often used when training ASR systems. Recently, self-supervised learning閳ユ攣 paradigm that treats the input itself or modifications of the input as learning targets閳 has obtained promising results. Those self-supervised speech representation can be fall into main categories: Contrastive Predictive Coding  incorporates contrastive objective to learn representations containing information that  most discriminates the future or masked frame from a set of negative samples. Another approach is Autoregressive Predictive Coding  , which tries to directly predict or reconstruct the frame based on context.  % More recently, vector-quantized representations of audio data has drawn increasing attention in speech processing . The motivation is that enforcing the quantization leads to a better representation for acoustic unit discovery due to the discrete nature of phonetic units. VQ-APC  also try to exactly quantified of information , to control the capacity of the models. And the use of vector quantization limited capacity are forced to retain information to achieve maximal prediction.   % Despite the success of the wav2vec 2.0 model , the model relies on a diverse codebook learnt to correlates the underlying speech units to speech representations via the contrastive loss. However, the codes at each time step the model select right after feature encoder hardly contained meaningful phonetic information. More importantly, contrastive loss formulation can result in several locally optimal codebooks. A few highly probable optima observed were acoustic condition-sensitive codebooks: where the model can easily optimized by assign acoustic condition  to the the codebooks, and temporally invariant codebooks: where the model assigns specific codes to fixed temporal locations to enable a good contrastive loss. Hence, the codebook learning methodology using contrastive loss might not generalize well to all datasets, espically the real world data consisted a lot of nausence factor like noise, different recording environment.    % A simple solution could be to enforce the codes to explicitly carry information about the input features in the process. Using frame reconstruction as objective, the network allows a flow of information from the input feature back to the the latent space to preserve meaningful information, helping mitigatate the codebook learning problems in contrastive loss as discussed above. Thus, we propose an novel self-supervised model that learns vector quantized deep transformer acoustic representations based on frames reconstruction. Since simple reconstruction utilize all the information available  to achieved maximal prediction while those information are less relevant to ASR. And by utilizing the VQ layer to limit those unwanted information flow into final representation, Vector Quantized Deep Contextualized Acoustic Representations  are able to achieve much better representation that's better suited for semi-supervised ASR tasks. By using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using deep transformer and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification  loss for the ASR task. We only train the small ASR model instead of fine-tuning for computing-efficiency. Our approach showed that supervision with 10 hours of labeled data on DeCoAR 2.0 achieves performance on par with training on all 960 hours directly.    In this paper, we present vector quantized Deep Contextualized Acoustic Representation , an improved speech representation learning approach based on DeCoAR and vector quantization.  DeCoAR 2.0 has multiple modification over the its predecessor, with a deep Transformer as encoding block, and the addition of a vector quantization module before reconstruction module. In extreme data-limited semi-supervised conditions, we observe that using 10 hours of labeled data with DeCoAR 2.0 achieved performance on par with the system trained on 960 hours of conventional filterbank features. DeCoAR 2.0 also performed comparably to wav2vec 2.0 in all different semi-supervised scenarios. Future work includes exploring the efficacy of representation learning in real world data including noisy and adverse conditions, and extension to neural transducers  and other end-to-end ASR systems as downstream tasks.   
","  Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR  and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.   % \yuzong{rewrite this} % We propose a novel approach for vector quantized deep contextualized acoustic representations. Following the same schema in DeCoAR, we first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from context frames. The new resulting deep contextualized acoustic vector quantized representations  are then used to train a small CTC-based ASR system using a small amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR 2.0 consistently outperform ones trained on other acoustic representations, giving the state-of-art and comparable results with wav2vec 2.0  on semi-supervised experiments on Librispeech. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 10 hours of labeled data achieves performance on par with training on all 960 hours directly.",475
" % % {A}{utomatic}  speech recognition , one of the core components in speech technology, has achieved significant advancements during the past decade . A key driving force behind these advancements is the rapid development of deep learning techniques .  % State-of-the-art  ASR systems    are usually trained with thousands of hours of transcribed speech data and a massive amount of text data. % % State-of-the-art  ASR systems    usually requires thousands of hours of transcribed speech data and a massive amount of text data to train a hybrid deep neural network-hidden Markov model  based acoustic model     and a recurrent neural network  language model . % Moreover, a hand-crafted pronunciation lexicon and a phoneme inventory based on linguistic expertise are often needed. Recently, end-to-end  ASR architectures, in which AM and LM training is integrated as a single pipeline, have gradually become the mainstream in ASR academic research , compared to   hybrid deep neural network-hidden Markov model  architectures . E2E architectures have the advantage of removing the need of a pronunciation lexicon and a phoneme inventory during system development. However, training an E2E ASR system tends to require even more transcribed speech data than for a hybrid DNN-HMM ASR system .  There are around  spoken languages in the world .  For most of them, the amount of transcribed speech data resources is very limited, or even non-existent . Many of these low-resource languages, such as ethnic minority languages in China and languages in Africa, may have never been formally studied. In addition to the lack of enough transcribed speech data, linguistic knowledge about such languages is incomplete, or may even be entirely lacking. Conventional supervised acoustic modeling  can therefore not be applied directly. This leads to the current situation that high-performance ASR systems are only available for a small number of major languages, e.g., English, Mandarin, French. To facilitate ASR technology for low-resource languages, investigation of unsupervised acoustic modeling  methods is necessary, which aims to find and model a set of basic speech units that represents all the sounds in the language of interest, i.e., the low-resource, target language.   Recently, there has been a growing research interest in UAM .  A strict assumption of UAM is that for the target language only raw speech data is available, while the transcriptions, phoneme inventory  and pronunciation lexicon are unknown. This is known as the zero-resource assumption .   %It is a challenging task, yet with significant research impact in a broad area of speech and language science and technology, e.g., query-by-example spoken term detection , text-to-speech without text , understanding the mechanisms underlying infant language acquisition , and the documentation  of endangered languages .  There are two main research strands in UAM. The first strand formulates the problem as discovering a finite set of phoneme-like speech units . This is often referred to as acoustic unit/model discovery  . The second strand formulates the problem as learning acoustic feature representations that can distinguish subword  units of the target language, and is robust to linguistically-irrelevant factors, such as speaker  . This is often referred to as unsupervised subword modeling . In essence, the second strand is focused on learning an intermediate representation towards the ultimate goal of UAM, while the first strand aims directly at the ultimate goal. These two strands are closely connected and can benefit from each other; for instance, a good subword-discriminative feature representation  % good feature representation that is discriminative to subword units and is robust to speaker variation  has been shown beneficial to AUD , while conversely,  discovered speech units with good consistency with true phonemes are helpful to % could provide phoneme-like pseudo transcriptions to assist the  learning   subword-discriminative acoustic feature representations .   This study addresses unsupervised subword modeling in UAM. Learning subword-discriminative feature representations in the zero-resource scenario has been shown to be a non-trivial task . The major difficulty is the separation of linguistic information   from non-linguistic information .   For instance, a speech sound such as [\ae]\footnote{International Phonetic Alphabet  symbol.} produced by different speakers  might be mistakenly modeled as different speech units .    There are many interesting attempts to unsupervised subword modeling . One typical research direction is to leverage purely unsupervised learning techniques. One method is the clustering of speech sounds that have acoustically similar patterns and that potentially correspond to the same subword units  , which results in phoneme-like pseudo transcriptions that can be used to facilitate subword-discriminative feature learning . % , e.g. cluster posteriorgrams  or DNN bottleneck features  .  Unsupervised and self-supervised representation learning algorithms are applied to learn, without using external supervision, speech features that retain the linguistic content in the original data while ignoring linguistically-irrelevant information, particularly speaker variation  .    A second research direction to unsupervised subword modeling is to exploit cross-lingual knowledge . Speech and text resources from out-of-domain  resource-rich languages have been shown beneficial to modeling subword units of in-domain low-resource languages. For instance,  used an OOD AM to extract cross-lingual bottleneck features , while  used an OOD ASR to generate cross-lingual phone labels. % by past studies .   % One idea is to utilize a pre-trained DNN AM from an OOD language to generate phoneme-discriminative representations of target speech, such as bottleneck features  . % The second idea would be to leverage an OOD ASR system to decode speech utterances in the target language and obtain cross-lingual phone labels as supervision for subsequent subword modeling  .  % These two ideas realize cross-lingual knowledge transfer at the AM level and phone label level respectively.  % Cross-lingual knowledge transfer can be done at AM level, i.e., an OOD pretrained AM used to generate  for speech of the  target language.  % It can also be done at  phone label level, i.e., an OOD ASR system decoding target speech utterances to generate phone labels as cross-lingual supervision .  %  This study adopts a two-stage learning framework which combines both research directions within the area of unsupervised subword modeling.  % The  high-level overview  of  the  proposed  framework  is  shown  in Fig. .  %, and  At the first stage, the front-end, a self-supervised representation learning model named autoregressive predictive coding      is trained. APC preserves phonetic  and speaker information from the original speech signal, but makes the two information types more separable . %This makes APC a suitable method for unsupervised subword modeling.   At the second stage, the back-end, a cross-lingual, OOD DNN model with a bottleneck layer  is trained using the APC pretrained features as the input features to create the missing  frame labels. % , as seen in Fig. .  %Frame labels required for DNN-BNF model training are not directly available due to the zero-resource assumption. In our framework, the labels are obtained using an OOD ASR system.  %By doing so, cross-lingual phonetic knowledge is exploited.  This system framework was proposed in our recent study ,  and showed state-of-the-art performances on the subword discriminability task on two databases in UAM: ZeroSpeech 2017  and Libri-light .   In this work, we expand and extend the work in . Specifically, we  compare the proposed approach to a supervised topline system that is trained on transcribed data of the target language;  compare the proposed approach with another cross-lingual knowledge transfer method  ; % investigate which of the AM-level or phone label-level knowledge transfer methods is more effective;   %  investigate the effects of the recently proposed APC model architectures in front-end pretraining in detail;   investigate the potential of our approach in relation to the amount of unlabeled training material by varying the data between  hours   and  hours, and compare the models' performance to the topline model. Throughout our experiments, English is chosen as the target low-resource language. Its phoneme inventory and transcriptions are assumed unavailable during system development. Dutch and Mandarin are chosen as the two OOD languages for which phoneme inventories and transcriptions are available.  Unsupervised subword modeling is typically evaluated using overall performance measures, such as ABX  , purity , normalized mutual information  . These metrics, however, do not provide insights on the approaches閳 ability of modeling individual phonemes or phoneme categories. As the ultimate goal beyond unsupervised subword modeling is to discover basic speech units that have a good consistency with the true phonemes of the target language, we, to the best of our knowledge for the first time in the literature, additionally present detailed analyses that explore the question of the effectiveness of the proposed approach to capturing phoneme and articulatory feature  information of the target language. % To answer this question The analyses are based on the standard ABX error rate evaluation , which we adapted for this work , and consist of two parts, i.e., an analysis at the phoneme level and at the AF level. The analyses are aimed at investigating what phoneme and AF information is  captured by the learned subword-discriminative feature representation, which can be used to guide future research to improve unsupervised subword modeling as well as AUD. Moreover, we correlate the phoneme-level ABX error rates and the quality of the cross-lingual phone labels which are used to train our back-end DNN-BNF model in order to study why the proposed approach performs differently in capturing different target phonemes' information, and how the performance is affected by the quality of cross-lingual phone labels.    %The analysis at the AF level is carried out as we are interested in the  extent to which the AF information in the target language can be learned by our subword-discriminative feature representation.  % AFs describe the target of the articulators in the vocal tract when pronouncing a specific phone . The use of AFs has been shown beneficial to low-resource ASR    and acoustic unit discovery .  % {do we need a introduction to AF?} % The AFs describe the movement of the tongue, lips and other organs to produce speech sounds. % {state why do we do this}  % The AF is a  compact and universal representation of speech, and is   more language-independent than the phoneme inventory representation.  % We are interested in to which extent is the AF information in the target language  learned in our subword-discriminative feature representation. %In the AF-level analysis, a new evaluation metric is proposed to measure the efficacy of our approach in capturing AF information. This metric replaces the phoneme inventory in the ABX discriminability task with the AF category.  % Specifically, the task is to predict whether a test speech segment  belongs to the same AF attribute as  or  as , where  and  contain speech sounds belonging to different AF attributes.  %Several AFs are investigated in this study, including place of articulation  and manner of articulation  for consonants, tongue height and tongue backness for monophthong vowels. %The AF-level analysis could potentially provide guidance on future research to improve unsupervised subword modeling as well as AUD. To our knowledge there are very few previous studies on AF-level analysis to unsupervised subword modeling and AUD.  % For instance, two systems achieving the same overall subword modeling performance might vary greatly in linguistic implications.   % overall performance metrics, such as ABX subword discriminability  , purity , normalized mutual information  .    % , or used as the input to perform further subword-discriminative learning .      % ,  i.e., the unsupervised feature representation learning problem.  % { high-level review representative approaches.  purely unsupervised learning approaches 1-1. clustering; 1-2 unsupervised feature learning.  leveraging OOD resources.} % {Text to be colored} % to train the deep neural network -based acoustic model and massive amount of text data to train the  %   The remainder of this paper is organized as follows. Section  provides a review of related works on the unsupervised subword modeling task. In Section , we provide a detailed description of the proposed approach to unsupervised subword modeling, and introduce comparative approaches to compare against our approach. Section  describes the methodology  used for the phoneme-level and AF-level analyses. Section   introduces the experimental design of this study, while Section  reports the results. Section  describes the setup for conducting the phoneme- and AF-level analyses, and discusses the results of the analyses. Finally, Section  draws the conclusions.     The present study addresses unsupervised subword modeling. A two-stage learning framework that consists of an APC front-end and a cross-lingual DNN-BNF back-end was proposed to tackle this problem. To evaluate the proposed approach, in addition to the widely adopted ABX subword discriminability metric, a comprehensive and systematic analysis was carried out at the phoneme-level and the articulatory feature -level to investigate the type of information that is  captured by the newly created feature representations. In order to do so, new metrics that focus on phoneme-level ABX subword discriminability and attribute-level ABX AF discriminability have been proposed.    A second focus of this study is to explore the effectiveness of the proposed approach    in modeling  each particular phoneme and AF of a target language detailedly. To this end, a comprehensive and systematic analysis is carried out at both   phoneme level analysis and AF level.   Experiments were conducted using two databases: Libri-light and ZeroSpeech 2017. Using the overall ABX subword discriminability metric,  the experimental results show that our approach is competitive or even superior to the state-of-the-art . Front-end APC pretraining brings performance improvement to the entire learning framework compared to a system with only the DNN-BNF back-end. Performance further increased when the amount of training material was increased from  hours to  hours. The proposed system閳ユ獨 best performance, achieved by using  hours of untranscribed training data without any linguistic knowledge of the target language, is very close to that of a supervised system trained on -hour   transcribed data of the target language. Moreover,  the proposed back-end performs better than a cross-lingual AM based BNF method in exploiting cross-lingual knowledge transfer.     Front-end APC pretraining brings performance improvement to the entire learning framework comparing to the system  with only the DNN-BNF back-end, and  brings greater improvement when the amount of training material increases from  hours to  hours. Our best performance achieved by using -hour  untranscribed training data without any linguistic knowledge of the target language is very close to a supervised system trained with -hour   transcribed data of that language.    This is an encouraging finding, as it indicates with sufficiently large amount of untranscribed data available for an unknown language, the proposed approach, without requiring any  linguistic knowledge of that language,  could   perform on par with a  supervisedly-trained system for that language.    Results also show  that towards cross-lingual knowledge transfer,  the back-end of our approach is more effective than a cross-lingual AM based method.    trained with in-domain transcribed data.     The second part of  Subsequent in-depth analyses investigated what information was captured by the newly created feature representations and this was compared to the information captured by baseline MFCC features and front-end APC features. The phoneme-level analysis showed that compared to MFCC, our two-stage approach achieves is better able to capture diphthong information than monophthong vowel information, and this is true in both the front-end and the back-end of our approach. For consonants, the improvement in capturing phoneme information from MFCC to our approach varies greatly to different consonants. Our results showed a positive correlation between the effectiveness of the back-end in capturing a target phoneme's information and the quality of cross-lingual phone labels assigned to that target phoneme.  The  AF-level analyses showed that the proposed approach is better than MFCC and front-end APC features in capturing manner and place of articulation information and vowel height and backness information. In the analysis of MoA, stop and fricative information are less well captured than affricate, approximant, and nasal information. The analysis of PoA showed that palatal is the best captured attribute, which is partially explained by the palatal AF attribute only consisting of a single phoneme /Y/, while most other PoA attributes consist of multiple phonemes with multiple manners of articulation. The analyses indicate MoA is better captured by the proposed approach than PoA which is in line with previous research , and both MoA and PoA information are better captured than vowel height and backness information. Comparing the outcomes of the analyses at the AF and phoneme level suggests that AF information is less language-dependent than phoneme information, which is in line with the linguistic principles underlying articulatory features and phonemes.  In conclusion, both the front-end and back-end of the proposed approach are effective in capturing information that distinguishes individual phonemes. It demonstrates the importance of both the front-end and the back-end of our approach in the task of unsupervised subword modeling.  Regarding AF information, the front-end is effective in capturing MoA and PoA information, but is less well able to capture vowel height and backness information. In contrast, the back-end is effective in capturing all the MoA, PoA, vowel height and backness information. The phoneme-level and the AF-level analyses both indicate monophthong vowel information is much more difficult to capture than consonant information. This suggests that a possible direction to improve unsupervised subword modeling is investigating methods that improve the effectiveness of capturing monophthong vowel information.     
"," % This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding  as the front-end and a cross-lingual deep neural network  as the back-end.  This study addresses unsupervised subword modeling, i.e., learning acoustic feature representations that can distinguish between subword units of a language. We propose a two-stage learning framework that combines self-supervised learning and cross-lingual knowledge transfer. The framework consists of autoregressive predictive coding  as the front-end and a cross-lingual deep neural network  as the back-end.  % Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases show our approach is competitive or superior to state-of-the-art studies. APC pretraining brings improvement to the entire framework, and brings larger improvement with increased amount of training data. Our best performance achieved by using unlabeled training data without linguistic knowledge of the target language is very close to that of a supervised system trained with labeled data of that language. The back-end of our approach is found more effective than a cross-lingual AM based BNF in cross-lingual knowledge transfer. Experiments on the ABX subword discriminability task conducted with the Libri-light and ZeroSpeech 2017 databases showed that our approach is competitive or superior to state-of-the-art studies.  % A comprehensive and systematic analysis at the phoneme- and articulatory feature - level is carried out to investigate the type of information that is captured by our learned feature representation. New metrics are proposed for the phoneme-level ABX subword discriminability task and attribute-level ABX AF task. The phoneme-level analysis showed that compared to MFCC, our approach achieves larger improvement in capturing diphthong information than monophthong vowel information, and the improvement varies greatly to different consonants. Results found there is a positive correlation between the effectiveness of the back-end in capturing a phoneme's information and the quality of cross-lingual phone labels assigned to that phoneme. The AF-level analysis showed that the proposed approach is better than MFCC and APC features in capturing manner of articulation , place of articulation , vowel height and backness information. Results indicate MoA is better captured by the proposed approach than PoA, and both MoA and PoA are better captured than vowel height and backness. Results implies AF information is less language-dependent than phoneme information.   Comprehensive and systematic analyses at the phoneme- and articulatory feature -level showed that our approach was better at capturing diphthong than monophthong vowel information, while also differences in the amount of information captured for different types of consonants were observed. Moreover, a positive correlation was found between the effectiveness of the back-end in capturing a phoneme's information and the quality of the cross-lingual phone labels assigned to the phoneme. The AF-level analysis together with t-SNE visualization results showed that the proposed approach is better than MFCC and APC features in capturing manner and place of articulation information, vowel height, and backness information.  % Taking all the analyses together, the two stages in our approach are both effective in capturing phoneme information. Monophthong vowel information is much more difficult to be captured than consonant information, which suggests a future research direction to improve the effectiveness of capturing monophthong vowel information.  Taken together, the analyses showed that the two stages in our approach are both effective in capturing phoneme and AF information. Nevertheless, monophthong vowel information is less well captured than consonant information, which suggests that future research should focus on improving capturing monophthong vowel information.",476
