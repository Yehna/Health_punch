document,summary,id
" % about unsupervised topic modeling and transfer learning in topic modeling  Unsupervised topic models, such as LDA , RSM , DocNADE , NVDM , etc.  have been popularly used to discover topics from large document collections.   However in sparse data settings, the application of topic modeling is challenging due to limited context in a small document collection or short documents   and the topic models produce incoherent topics.  To deal with this problem, there have been several attempts   that introduce prior knowledge such as pre-trained word embeddings  to guide meaningful learning.    
"," Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of  documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly:  sharing generative homologies  over lifetime to transfer prior knowledge, and  minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling  framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task. Code: \url{https://github.com/pgcool/Lifelong-Neural-Topic-Modeling}",0
"  Neural networks benefit from large quantities of labeled training data.  However, in many settings labeled data is much harder to come by than unlabeled data:  current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide~.  Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them - a process that requires learning good representations of speech.  In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to fine-tune the model on labeled data.  This has been particularly successful for natural language processing~ and is an active research area for computer vision~.  In this paper, we present a framework for self-supervised learning of representations from raw audio data.  Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations~, similar to masked language modeling~. The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors~ ~.  As part of training, we learn discrete speech units~ via a gumbel softmax~ to represent the latent representations in the contrastive task  which we find to be more effective than non-quantized targets. After pre-training on unlabeled speech, the model is fine-tuned on labeled data with a Connectionist Temporal Classification  loss~ to be used for downstream speech recognition tasks     Previous work learned a quantization of the data followed by a contextualized representations with a self-attention model~, whereas our approach solves both problems end-to-end. Masking parts of the input with Transformer networks for speech has been explored ~, but prior work relies either on a two-step pipeline or their model is trained by reconstructing the filter bank input features. Other related work includes learning representations from auto-encoding the input data~ or directly predicting future timesteps~.   Our results show that jointly learning discrete speech units with contextualized representations achieves substantially better results than fixed units learned in a prior step. We also demonstrate the feasibility of ultra-low resource speech recognition:  when using only 10 minutes of labeled data, our approach achieves word error rate  4.8/8.2 on the clean/other test sets of \libri{}. We set a new state of the art on TIMIT phoneme recognition as well as the 100 hour clean subset of \libri{}.  Moreover, when we lower the amount of labeled data to just one hour, we still outperform the previous state of the art self-training method of~ while using 100 times less labeled data and the same amount of unlabeled data. When we use all 960 hours of labeled data from \libri{}, then our model achieves 1.8/3.3 WER .   
","   We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.   wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned.   Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets.   When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data.   Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER.   This demonstrates the feasibility of speech recognition with limited amounts of labeled data.\footnote{Code and models are available at \url{https://github.com/pytorch/fairseq}}",1
"  The work aims at resolving the long-standing plight of unfamiliarity with command line interface in UNIX based systems. This will not only improve the efficiency of the user but also improve the learning curve for the beginners. The concerned research work treats the problem of UNIX Command Line Prediction as a sequence prediction problem instead of the traditionally adapted provision of recommendation systems. RNN  is able to 閳ユ涪heoretically閳 use information from the past in predicting the future. However, plain RNNs suffer from vanishing and exploding gradients problem making them hard to use practically. For this problem, we used LSTM which uses gates to flow gradients back in time solving the vanishing and exploding gradient problem. Thus, with the advent of a command line prediction system involving accurate prediction, a GUI prototype of the UNIX shell can be brought in place subsequently realising the serious necessity of a user-friendly environment for amateur end users. Our model delves into the user閳ユ獨 bash history and learns from it while providing a thorough path to him/her from the past usage patterns of the professionals and scientists. We have also been able to establish a novel method and outperform the 50\% threshold of accuracy set by previous works. The former maximum accuracy was accomplished by  in which  was extended to employ consideration of error output with the history of commands and dynamic file name replacement, attaining 47.9\% accuracy.  
"," 		Despite being an open-source operating system pioneered in the early 闁90s, UNIX based platforms have not been able to garner an overwhelming reception from amateur end users. One of the rationales for under popularity of UNIX based systems is the steep learning curve corresponding to them due to extensive use of command line interface instead of usual interactive graphical user interface. In past years, the majority of insights used to explore the concern are eminently centered around the notion of utilizing chronic log history of the user to make the prediction of successive command. The approaches directed at anatomization of this notion are predominantly in accordance with Probabilistic inference models. The techniques employed in past, however, have not been competent enough to address the predicament as legitimately as anticipated. Instead of deploying usual mechanism of recommendation systems, we have employed a simple yet novel approach of Seq2seq model by leveraging continuous representations of self-curated exhaustive Knowledge Base  to enhance the embedding employed in the model. This work describes an assistive, adaptive and dynamic way of enhancing UNIX command line prediction systems. Experimental methods state that our model has achieved accuracy surpassing mixture of other techniques and adaptive command line interface mechanism as acclaimed in the past.",2
" Recent work in neural machine translation  has led to dramatic improvements in both research and commercial systems. However, a key weakness of contemporary systems is that performance can drop dramatically when they are exposed to input perturbations , even when these perturbations are not strong enough to alter the meaning of the input sentence.  Consider a Chinese sentence, %``{UTF8}{gkai}{鏉╂瑦鐏︽鐐存簚濞屸剝婀侀幘鐐扮瑐娴ｅ繐顔峔textcolor{red}{閹存澊閸栧娅岄敍灞界杽閸︺劍妲告總鍥姉閵嗗'' ``zhejia feiji meiyou zhuangshang zhujia huo yiyuan, shizai shi qiji''. If we change the word ``huo~ continuous noise which is modeled as a real-valued vector applied to word embeddings, and  discrete noise which adds, deletes, and/or replaces characters or words in the observed sentences. In both cases, the challenge is to ensure that the noisy examples are still semantically valid translation pairs. In the case of continuous noise, it only ensures that the noise vector lies within an -norm ball but does not guarantee to maintain semantics. %an  constraint on the noise vector does not guarantee that the semantics are preserved. %\jacob{can we say any more about why continuous noise doesn't work well?} While constructing semantics-preserving continuous noise in a high-dimensional space proves to be non-trivial, state-of-the-art NMT models are currently based on adversarial examples of discrete noise. For instance,  generate adversarial sentences using discrete word replacements in both the source and target, guided by the NMT loss. This approach achieves significant improvements over the Transformer on several standard NMT benchmarks. Despite this promising result, we find that the generated adversarial sentences are unnatural, and, as we will show, suboptimal for learning robust NMT models.  %But despite the empirical success of this approach, the generated adversarial sentences are unnatural \jacob{disfluent?}, and, as we will show, suboptimal for learning robust NMT models.  In this paper, we propose { adversarial sentences from the vicinity distribution according to their interpolated embeddings. Our intuition is that the introduced vicinity distribution may increase the sample diversity for adversarial sentences. Our idea is partially inspired by \mixup, a technique for data augmentation in computer vision, and we also use a similar vicinity distribution as in \mixup~ to augment the authentic training data. Our { %\jacob{to Lu: yes I'm fine with that, as long as it's clear what the new distribution is. The understanding that I tried to convey in this paragraph -- which may be wrong! -- is that the Cheng et al 2019 paper introduced the method of building adversarial examples from discrete noise, and that the contribution of the present submission is the combination of   and  .} %In both cases, the central difficulty is how to specify a ``vicinity'' distribution around individual examples that adds diversity while preserving semantics.  %In this paper we consider a new approach, which we call ancyname, based on interpolating between  of training examples.  %This basic idea is applied in two ways.  %First, given a pair of authentic training instances, we construct novel instances by interpolating between their embeddings, similar to the \mixup approach in computer vision, but with the critical extension to sequence-to-sequence learning. Second, we apply this same interpolation approach to pairs of adversarial examples, which are generated from discrete noise \jacob{I see that this helps, but why?}.  %This provides a novel characterization of the vicinity distribution around the training set, increasing diversity while preserving semantics.  %\jacob{i'm struggling with this paragraph and have tried a fresh start above} %% J: Summarizing this paragraph to make sure I understand it %% 1. we introduce a vicinity distribution around each training example  %% 2. Unlike Cheng et al 2019, we generate adversarial examples in the discrete space and then virtual adversarial examples in the continuous space. %% 3. We think this increases diversity %% 4. Mixup: another approach for sampling in the continuous space, using pairs of authentic examples %% 5. We sample from both vicinity distributions  %In this paper, we propose { adversarial sentences from the vicinity distribution according to their interpolated embeddings. %Our intuition is that the introduced vicinity distribution increases the sample diversity for adversarial sentences. Our approach is partially inspired by mixup, a technique for data augmentation in computer vision; a contribution of this paper is to extend this work to the sequence-to-sequence task. % Our ancyname approach finally trains on the embeddings sampled from both vicinity distributions. As a result, we augment the training using virtual sentences in the feature space as opposed to the data space. The novelty of our paper is the new vicinity distribution for adversarial examples and the augmentation algorithm for sequence-to-sequence learning.  Extensive experimental results on three translation benchmarks  show that our approach achieves significant improvements of up to  BLEU points over the Transformer, outperforming the former state-of-the-art in adversarial learning  by up to  BLEU points. When compared with widely-used data augmentation methods, we find that our approach yields better performance even without using extra corpora. We conduct ablation studies to gain further insights into which parts of our approach matter most. In summary, our contributions are as follows:         %------------------------------------------------------------------------------------------------------------ 
"," In this paper, we propose a new adversarial augmentation method for Neural Machine Translation . The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, of which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, { achieves significant improvements over the Transformer , and substantially outperforms other data augmentation techniques  without using extra corpora.",3
" Text generation, as a basic natural language processing task, has many applications, such as dialogue robots , machine translation , paraphrasing  and so on. With the rise of deep learning, different neural networks are introduced to generate text.  For example, researchers use the recurrent neural network   to train the language model because of its capability to process sequential data. However, the RNN suffers from the gradient vanishing problem  when the sequence becomes longer. To address this problem, Long Short-Term Memory  is further adopted as a sequential neural network model to generate sentences.  Lately, the Generative Adversarial Networks  framework  has been introduced into the NLP community. GAN has two different models for completing the data-generating task. One of them is Generator G, which is responsible for generating data, and another one is discriminator D, which determines whether the input data is the real data or not. The generator G continuously optimizes generated data based on the judgment of discriminator D. After several epochs, the generated data will become more realistic.  However, GAN was originally designed to process continuous data, and using discrete data as input would make it impossible to update the gradients of the GAN framework. To process discrete data, several variants of the GAN model for generating text have been proposed. These GAN variants could achieve good performances in text generation task, such as MaskGAN , RankGAN , and TextGAN .   In order to make these models fit the distribution of real text data better, the number of parameters of text generation models based on neural network are increased, which means that training these neural network models often takes a lot of time even using GPU. Conventionally, topic-related text generation models incorporate an arbitrary topic as an input by adopting mechanisms like attention . Therefore, each time when the user wants to generate new sentences with another topic or sentimental tendency, the text generation models have to be retrained with all parameters to satisfy new requirements. In some scenarios, e.g., news generation, spending lots of time retraining model is not practical and the user wants new responding quickly.   To tackle this problem, a novel text generation model based on GAN is proposed, which is called User-Defined Generative Adversarial Networks . The key idea is to separate the sentence syntax model as the basic model and the topic-related model as a higher-level model, and these two could be trained independently from each other. So, when the topic or other user-defined information is modified, e.g., sentimental tendency, only one of both models needs to be retrained. In this way, once the basic syntax model is established, the following training will become much faster, since only the higher-level model has to be retrained.  In our proposed method, the discriminator is constructed based on this idea. One of the discriminators called discriminator-general, which learns to determine the proper context information and whether the input sentence is a valid syntactic structure. Another discriminator is called the discriminator-special, which ensures the output is user-defined. Inspired by SeqGAN , we use the evaluation results of the generated text from discriminators as a reward to guide the generator to select future actions, which is to generate an updated word.   For training the discriminator-special, it will take feature vectors as input, instead of sentences. The feature vector is defined based on the sentiment detection and topic relevance of generated sentence. The cosine similarity based on TF-IDF and length penalty are jointly adopted to represent topic relevance.   Note that the UD-GAN is designed to be more practical to generate short paragraphs, which means sentences generated by it should be context-aware and behave like a paragraph together with surrounding sentences. To achieve this idea, discriminator-general is designed with hierarchical multiple LSTM layers. The LSTM at the top of the network processes paragraph-level information while the bottom LSTMs process sentence-level information.  The organization of the paper is as follows: First, we discussed the related works of our method in the section 2. The proposed method is described in the Section 3, including the feature extraction and model definition and training. In the Section 4, the experiment settings and evaluation results of the comparing methods are depicted. Finally, the concluding remarks and future works are described in the Section 5.  [ht]        [1] \STATE {Initialize ,  and  with random weights,  and }  \STATE {Pre-train  using MLE on real text data set} \STATE {Generate negative samples using  to train  and } \STATE {Generate synthetic positive samples to train } \STATE {Minimizing the cross entropy to pre-train } \STATE {Minimizing the cross entropy to pre-train} \FOR{ to } \FOR{ to } \STATE {Generate a sequence G\thetak\leftarrow 1PG\thetaD\phil\leftarrow 1TG\thetaD\gammaG\thetaD\gamma\thetaD\gammaD\phi \STATE {Do 8G\thetaD\gamma$ with negative and synthetic feature vectors via Eq.} \ENDFOR \ENDFOR   
","   This study focused on efficient text generation using generative adversarial networks . Assuming that the goal is to generate a paragraph of a user-defined topic and sentimental tendency, conventionally the whole network has to be re-trained to obtain new results each time when a user changes the topic. This would be time-consuming and impractical. Therefore, we propose a User-Defined GAN  with two-level discriminators to solve this problem. The first discriminator aims to guide the generator to learn paragraph-level information and sentence syntactic structure, which is constructed by multiple-LSTMs. The second one copes with higher level information, such as the user-defined sentiment and topic for text generation. The cosine similarity based on TF-IDF and length penalty are adopted to determine the relevance of the topic. Then, the second discriminator is re-trained with generator if the topic or sentiment for text generation is modified. The system evaluations are conducted to compare the performance of the proposed method with other GAN-based ones. The objective results showed that the proposed method is capable of generating texts with less time than others and the generated text are related to the user-defined topic and sentiment. We will further investigate the possibility of incorporating more detailed paragraph information such as semantics into text generation to enhance the result.",4
"  Text prediction is a challenging problem in machine learning and natural language processing, while at the same time there is a growing need for novel techniques for efficient and accurate text prediction in several application domains, such as in dictation and typing systems for people with disabilities or clinical text prediction for healthcare practitioners . More concretely, with text prediction we refer to the task of predicting the next block of text in an online fashion, where block can refer to different text granularity levels, e.g., sentences, words, syllables, or characters  .  The main focus of this paper is medical text with the concrete task of predicting the next word given an incomplete text. We also refer to this problem as  for medical text. When applied in the clinical setting , physicians can vastly benefit from a fast and accurate predictive keyboard system, since it can assist them with  a speedy compilation of the intended text,  means for prevention of potential text errors due to work overload,  means for speedier patient discharge.  Initial efforts towards solving the predictive keyboard problem for radiology reports are described by Eng and Eisner 2004 , where a 3-Gram language model achieves an average keystroke reduction of a factor of 3.3. Following this line of research, we employed N-Gram-based statistical language modeling, which refer to as N-GLM, to predict the next word of a clinical text. We vary N from 1 to 10 and show that 4-Gram models achieve 38\% accuracy when predicting the next word in a clinical text, outperforming other N-GLMs. Observe that accuracy in this case measures the fraction of times when the next word was predicted correctly, hence inducing an equivalent typing speedup at the word level. We additionally investigated two neural language models that employ  a Recurrent Neural Network  language model based on Long-Short Term Memory, which we refer to as LSTMLM  and  a Gated Recurrent Unit  based language model, which we refer to as GRULM. This model achieves higher levels of accuracy compared to 4-GLM, since our experimental evaluation demonstrates that accuracy can reach up to 51.3\% . An example of the output of this task is depicted in Table , where we can observe the next word predictions made by LSTMLM and 4-GLM, with the correctly predicted words indicated in '[]'.  Next, we outline the related work in the area of clinical text prediction, followed by a summary of our contributions.  [!t]     |}\hline          LSTMLM & ""the lungs are clear without [evidence] [of] focal infiltrate [or] [effusion] [there] [is] [no] [pneumothorax] [the] [visualized] [bony] [structures] [reveal] [no] [acute] [abnormalities]""   \\\hline         4-GLM  & ""the lungs are [clear] without evidence [of] [focal] infiltrate or effusion [there] [is] [no] pneumothorax [the] visualized bony [structures] reveal [no] [acute] [abnormalities]""\\\hline               The study of the benefits of computer-assisted text generation dates back to more than two decades ago . When applied to clinical notes, such as radiology reports, a statistical 3-Gram language model  achieved substantial keystroke reductions . Recently, an even simpler 3-Gram language model  outperformed the earlier  while also decreasing the typing time for the clinician by one third . These results demonstrate that N-Gram models can provide promising solutions to our problem, and hence in this paper we provide a more extensive evaluation of these models on medical text. Besides computer-assisted typing, language models have also been used for spelling correction in clinical notes . This work does not focus on spelling correction, but what these works verify is that the words suggested by the language model during typing, are also checked for their correctness , hence the generated text will be of equal or even higher quality.   With the recent advance of deep learning, deep neural networks, such as Long-short Term Memory   models, have improved the performance in natural language processing  tasks of the biomedical field, such as Name Entity Recognition  ; medical codes prediction ; relation classification ; predicting hospital readmission . And language modeling is also part of this advent, since it is often employed as a pre-training step . For the task of next word prediction in a medical setting, however, neural language modeling is heavily under explored. To our knowledge, the only application of a neural language model was that of a baseline LSTM network , which was improved when structured information from electronic health data  was integrated . The authors reported 8\% Accuracy  for the baseline LSTM, which ranks it much lower than competing statistical language models . However, neural networks have been reported to outperform statistical language modeling in non-medical domains .  In this work, we compare statistical and neural language modeling, a comparison which has not been studied before, and we show that the neural approach outperforms the statistical approach in next word prediction by a large margin.    The main contributions of this paper can be summarized as follows:  We highlight the importance of the problem of keyword prediction for clinical text, and demonstrate how language models can be employed for providing scalable solutions to this problem;  We provide an extensive benchmark on clinical text obtained from two real-world medical datasets by comparing the performance of the N-GLM model for different values of N in terms of accuracy and keystroke reduction;  We additionally compare an RNN language model based on LSTM and GRU on the same datasets and demonstrate their superiority against N-GLMs as they can achieve an accuracy of up to 51.3\%, indicating a speedup  of the same degree, and a keyword reduction of up to 41.12\%, indicating a speedup  of the same degree.  
"," A language model can be used to predict the next word during authoring, to correct spelling or to accelerate writing . Language models, however, have only been applied in a very small scale to assist physicians during authoring . But along with the assistance to the physician, computer-based systems which expedite the patient's exit also assist in decreasing the hospital infections. We employed statistical and neural language modeling to predict the next word of a clinical text and assess all the models in terms of accuracy and keystroke discount in two datasets with radiology reports. We show that a neural language model can achieve as high as 51.3\% accuracy in radiology reports . We also show that even when the models are employed only for frequent words, the physician can save valuable time.",5
"  Despite suggetsions that the stock market is not predictable , many investors and researchers seek methods that can provide market fluctuation predictions to aid investment strategy. Advances in Machine Learning  and Natural Language Processing  have led to a shift in focus from technical to fundamental analysis. This new approach uses data such as news articles and historical stock prices, and is based upon the Efficient Market Hypothesis which states that an asset price reflects all available information . Advances in predictive models have also led to more complex trading strategies. Most research regarding trading strategies and ML is focused on technical analysis , however, some works consider news and other fundamental data as part of their strategy . The development of trading strategies based solely on fundamental data are rare throughout the relevant literature.  Early research shows no relation between headlines and stock volatility , however the development of more advanced predictive models and availability of larger datasets has led to more accurate market trend predictions based on headlines. Although complete news articles  or social media content  are used in some works, the use of headlines has become most common in this area of research due to the belief that they contain less noise than other sources of textual data . Headlines are commonly sourced from major financial news outlets such as the Wall Street Journal . A wide range of prediction targets are considered throughout the relevant literature, including major indices such as the S\&P 500  and collections of individual companies . The time-span of market fluctuations analysed is also varied. For example Mittermayer  focuses on intra-day predictions whereas long-term trends are briefly considered in the work of Ding . Methods such as Support Vector Machines  and complex Decision Trees  remain popular for predictive tasks of this nature. These commonly use a Bag of Words  feature representation approach, where words are represented independently without consideration of word-order or context. Variations of this method include -gram BoW, where phrases of length  are extracted as features as opposed to single words, and Term Frequncy-Inverse Document Frequency , which introduces consideration of a word's frequency within a sentence and across the entire collection. However these representations typically lead to sparsity issues when applied to a large corpus . Probabalistic approaches such as the Naive Bayes method can also be applied to tasks of this nature .   The development of Artificial Neural Networks  has provided new classification and feature representation methods for text-based tasks. An ANN is a collection of nodes known as neurons that are interconnected in layers. Originally proposed by Rosenblatt , the architecture is based on the transmission of signals and firing of biological neurons in a nervous system. Variations on the basic ANN architecture have been made to produce types of neural network with additional mathematical features suited to different tasks. Convolutional Neural Networks  have gained popularity in text-based tasks. Commonly used for image recognition, CNNs  utilise a convolutional layer to detect patterns in input data that can be used for accurate classification or prediction. For example, in image detection these patterns may represent edges and shapes of a specific object depicted by its pixel values. CNNs have demonstrated state-of-the-art performance in multiple NLP tasks, including sentence classification  and sentence modelling . Some applications of CNNs to market prediction exist in the literature, both for major indices  and discrete price prediction .  This work presents a CNN for predicting next-day stock price fluctuations of three major technology companies using headlines relating to each company. Next-day returns are used due to the inability to access the large amounts of historical intra-day stock price data required for intra-day fluctuation prediction. However the effect of news headlines has been found to resonate during the next-day period . Experiments are conducted to identify an optimal model configuration for trend classification in terms of the word-embedding  and convolutional layer states. Using class predictions from these experiments, trading simulations are presented based on day-averaged predictions for each asset. Finally, modifications to both the baseline trading strategy and labelling of headlines are made with the intention of reducing risk present in simulated trading.   
"," This work presents a Convolutional Neural Network  for the prediction of next-day stock fluctuations using company-specific news headlines. Experiments to evaluate model performance using various configurations of word-embeddings and convolutional filter widths are reported. The total number of convolutional filters used is far fewer than is common, reducing the dimensionality of the task without loss of accuracy. Furthermore, multiple hidden layers with decreasing dimensionality are employed.  A classification accuracy of 61.7\% is achieved using pre-learned embeddings, that are fine-tuned during training to represent the specific context of this task. Multiple filter widths are also implemented to detect different length phrases that are key for classification. Trading simulations are conducted using the presented classification results. Initial investments are more than tripled over a 838 day testing period using the optimal classification configuration and a simple trading strategy. Two novel methods are presented to reduce the risk of the trading simulations. Adjustment of the sigmoid class threshold and re-labelling headlines using multiple classes form the basis of these methods. A combination of these approaches is found to more than double the Average Trade Profit  achieved during baseline simulations.",6
"    The fields of Programming Languages  and Natural Language Processing  have long relied on separate communities, approaches and techniques. Researchers in the Software Engineering community have proposed the Software Naturalness  hypothesis which argues that programming languages can be understood and manipulated with the same approaches as natural languages.   The idea of transferring representations, models and techniques from natural languages to programming languages has inspired interesting research. However, it raises  the question of whether language model approaches based purely on source code can compensate for the lack of structure and semantics available to graph-based approaches which incorporate compiler-produced features. The application of language models to represent the source code has shown convincing results on code completion and bug detection.  The use of structured information, such as the Abstract Syntax Tree , the Control Flow Graph  and the Data Flow Graph , has proven to be beneficial for vulnerability identification in source code. However, the extraction of structured information can be very costly, and requires the complete project. In the case of the C and C++ languages, this can only be done through complete pre-processing and compilation that includes all the libraries and source files. Because of these requirements, approaches based on structural information are not only computationally expensive but also inapplicable to incomplete code, for instance a pull request.  This work explores the software naturalness hypothesis, employing the pre-training/fine-tuning paradigm widely used with transformer-based language models   to address tasks involving the analysis of both syntax and semantics of the source code in the C language. To investigate syntax, we first introduce a novel sequence labeling task that directly probes the language model's understanding of AST, as produced by Clang .  Furthermore, we investigate the capabilities of LMs in handling complex semantics in the source code through the task of vulnerability identification . All of our experiments involved LMs pre-trained from scratch on the C language source code of 100 open source repositories. The use of language models with source code rather than natural language leads to multiple challenges.  Data sparsity is a major problem when building language models, leading to out-of-vocabulary  terms and poor representations for rare words.  These issues are particularly severe for PL because variable and function names can be of almost arbitrary length and complexity.  There is a tradeoff between the granularity of tokenization and availability of long-range context for the LM.  Fine-grained tokenizations break identifiers into many small common tokens, alleviating issues with rare tokens, but at the risk of spreading important context across too many tokens. We address the OOV and rare words problems by investigating three choices for tokenization, which span the context/vocabulary size tradeoff  from the extreme of character-based tokenization to subword tokenization styles familiar to the NLP community. We indicate that the choice of the pre-training objective is closely connected to the choice of the vocabulary. In particular, with character based tokenization the pre-training task seems too easy. We introduce a more difficult, whole word masking  pre-training objective.  In our experiments, we show that our language model is able to effectively learn how to extract AST features from source code. Moreover, we obtain compelling results compared to graph-based methods in the vulnerability identification task. While current approaches to code analysis depend heavily on features derived from the AST, our approach works using raw source code without leveraging any kind of external features. This a major advantage, since it avoids a full compilation phase to avoid the extraction of structural information. Indeed, our model can identify vulnerabilities during the software development stage or in artifacts with incomplete code, which is a valuable feature to increase productivity. We show the merits of simple approaches to tokenization, obtaining the best results using the character based tokenization with WWM pre-training.  The contributions of this work are summarized as follows:     1) we investigate the application of transformer-based language models for complex source code analysis tasks;     2) we demonstrate that character-based tokenization and pre-training with WWM eliminate the OOV and rare words;      3) this work is the first to investigate whether such language models can discover AST features automatically;     4) our language model outperforms graph-based methods that use the compiler to generate features. \documentclass{article}  % if you need to pass options to natbib, use, e.g.: %     \PassOptionsToPackage{numbers, compress}{natbib} % before loading neurips_2020  % ready for submission % \usepackage{neurips_2020}  % to compile a preprint version, e.g., for submission to arXiv, add add the % [preprint] option: %     \usepackage[preprint]{neurips_2020}  % to compile a camera-readyi version, add the [final] option, e.g.: %     \usepackage[final]{neurips_2020}  % to avoid loading the natbib package, add option nonatbib: \usepackage[preprint, nonatbib]{neurips_2020} %\usepackage[nonatbib]{neurips_2020}  \usepackage[utf8]{inputenc} % allow utf-8 input \usepackage[T1]{fontenc}    % use 8-bit T1 fonts \usepackage{hyperref}       % hyperlinks \usepackage{url}            % simple URL typesetting \usepackage{booktabs}       % professional-quality tables \usepackage{amsfonts}       % blackboard math symbols \usepackage{nicefrac}       % compact symbols for 1/2, etc. \usepackage{microtype}      % microtypography \usepackage[colorinlistoftodos]{todonotes}  \usepackage{adjustbox} \usepackage{graphicx} \usepackage{tabularx} \usepackage{xspace}  \def\tokenkind{{{FFmpeg\xspace} {QEMU\xspace} {LM\xspace} {VI\xspace}  \makeatletter [1]{%   \textsuperscript{\@fnsymbol{#1}}% } \makeatother    \title{Exploring Software Naturalness through\\Neural Language Models}  \author{Luca Buratti\thanks{Equal Contribution. In no particular order.} \\ IBM Research \\  \And Saurabh Pujar\printfnsymbol{1} \\ IBM Research \\  \And Mihaela Bornea\printfnsymbol{1} \\ IBM Research \\  \And Scott McCarley\printfnsymbol{1} \\ IBM Research \\  \And Yunhui Zheng \\ IBM Research \\  \And Gaetano Rossiello \\ IBM Research \\  \And Alessandro Morari \\ IBM Research \\  \And Jim Laredo \\ IBM Research \\  \And Veronika Thost \\ IBM Research \\  \And Yufan Zhuang \\ IBM Research \\  \And Giacomo Domeniconi \\ IBM Research \\  \\ }     The Software Naturalness hypothesis argues that programming languages can be understood through the same techniques used in natural language processing. We explore this hypothesis through the use of a pre-trained transformer-based language model to perform code analysis tasks. Present approaches to code analysis depend heavily on features derived from the Abstract Syntax Tree  while our transformer-based language models work on raw source code. This work is the first to investigate whether such language models can discover AST features automatically. To achieve this, we introduce a sequence labeling task that directly probes the language model's understanding of AST. Our results show that transformer based language models achieve high accuracy in the AST tagging task. Furthermore, we evaluate our model on a software vulnerability identification task. Importantly, we show that our  approach obtains vulnerability identification results comparable to graph based approaches that rely heavily on compilers for feature extraction.                       Our model architecture is a multi-layer bidirectional transformer  based on BERT , which we refer to as  = [x_{1}=CLS, x_2,\ldots,x_{T-1},x_{T}=SEP]\mathbf{H} = [h_{1}=h_{CLS},h_2,  \ldots,h_{T}=h_{SEP}] ^{T\times 768}h_t ^{768}\mathbf{W_{LM}} ^{768\times |V|}V\mathbf{W_A} ^{768\times |V_{AST}|}V_{AST}h_{CLS}\mathbf{w} ^{768}$. We fine-tune using the cross entropy between the true labels and   evaluated across all context windows. 
"," The Software Naturalness hypothesis argues that programming languages can be understood through the same techniques used in natural language processing. We explore this hypothesis through the use of a pre-trained transformer-based language model to perform code analysis tasks. Present approaches to code analysis depend heavily on features derived from the Abstract Syntax Tree  while our transformer-based language models work on raw source code. This work is the first to investigate whether such language models can discover AST features automatically. To achieve this, we introduce a sequence labeling task that directly probes the language model's understanding of AST. Our results show that transformer based language models achieve high accuracy in the AST tagging task. Furthermore, we evaluate our model on a software vulnerability identification task. Importantly, we show that our  approach obtains vulnerability identification results comparable to graph based approaches that rely heavily on compilers for feature extraction.",7
" % Authors wishing to code their contribution with \LaTeX{}, as well as those who have already coded with \LaTeX{}, will be provided with a document class that will give the text the desired layout. Authors are requested to adhere strictly to these instructions; {, then the LLNCS class should not give you any major difficulties. It will change the layout to the required LLNCS style (it will for instance define the layout of \verb|
", The abstract should summarize the contents of the paper using at least 70 and at most 150 words. It will be set in 9-point font size and be inset 1.0 cm from the right and left margins. There will be two blank lines before and after the Abstract. \dots,8
"   Cross-lingual learning aims to build models which leverage data from other languages to improve performance. This has been a long standing interest in the speech community~ which includes systems able to transcribe multiple languages~.  However, the vast majority of work in speech processing has focused on supervised cross-lingual training which requires labeled data in multiple languages. Transcribed speech is often much scarcer than unlabeled speech and requires non-trivial human annotation.  % Supervised pretraining for seq2seq~   Unsupervised representation learning, or pretraining, does not require labeled data and has received a lot of recent attention in computer vision~ after much success in natural language processing~.  For the latter, cross-lingual pretraining has been shown to be very effective, particularly, for low resource languages~. In speech processing, most work in this area has focused on monolingual unsupervised representation learning~.   In this paper, we focus on the cross-lingual setting by learning representations on unlabeled data that generalize across languages. We build on the pretraining approach of~ which jointly learns contextualized speech representations as well as a discrete vocabulary of latent speech representations. The latter serves to effectively train the model with a contrastive loss~ and the discrete speech representations are shared across languages~. Different to recent work on unsupervised cross-lingual pretraining, we fine-tune the Transformer part of the model instead of freezing all pretrained representations~ or feeding them to a separate downstream model~. We extend the work of~ by pretraining on multiple languages instead of just English and we experiment on top of a stronger baseline.   We evaluate \xlsr{} on 14 languages of the BABEL benchmark~ which is conversational telephone data and ten languages of CommonVoice~, a corpus of read speech . Multilingual pretraining outperforms monolingual pretraining in most cases, except for resource rich languages and we show that increased model capacity significantly closes the gap. % Moreover, pretraining on multiple languages still outperforms pretraining on just English when controlling for the amount of data. We also demonstrate that \xlsr{} representations can be fine-tuned simultaneously on multiple languages to obtain a multilingual speech recognition system whose performance is competitive to fine-tuning a separate model on each language~\autoref{sec:results}). % On CommonVoice, we report a relative phoneme error rate  reduction of 72\% compared to the previous best known results~.  % On BABEL we outperform prior supervised multilingual work on a comparable data setup by a relative 38\% in terms of character error rate~, and by a relative 16\% when measuring word error rate~;~\autoref{sec:results}).   
"," This paper presents \xlsr{} which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72\% compared to the best known results. On BABEL, our approach improves word error rate by 16\% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.\footnote{}",9
" In 2019, Americans spent \2781nn89.783$\%.   One of the key criticisms of deep learning  methods is  that the decision process of the DL model is intractable thereby making it difficult to understand the  reasoning behind its decisions. For applications such as AD detection, it is imperative that some form of reasoning  be provided, because of the human angle involved.  Hence in this work, .  The main contributions of our work are:   The rest of the paper is organized as follows. Section, describes related work. Section, discusses the proposed explainable deep learning models. Section, details the datasets, the experimental set up, data bias compensation mechanism, example explanations and discussions of the results. Section discuses the conclusions we draw from this work. 
"," \label{sec:abs} % In this work we propose three explainable deep learning architectures to automatically detect patients with Alzheimer闁炽儲鐛 disease based on their language abilities. The architectures use:  only the part-of-speech features;  only language embedding features and  both of these feature classes via a unified architecture.  We use self-attention mechanisms and interpretable 1-dimensional Convolutional Neural Network  to generate two types of explanations of the model's action: intra-class explanation and inter-class explanation. The inter-class explanation captures the relative importance of each of the different features in that class, while the inter-class explanation captures the relative importance between the classes. Note that although we have considered two classes of features in this paper, the architecture is easily expandable to more classes because of its modularity. Extensive experimentations and comparison with several recent models show that our method outperforms these methods with an accuracy of $92.2$\% and F1 score of $0.952$ on the DementiaBank dataset while being able to generate explanations. We show by examples, how to generate these explanations using attention values.",10
"  Alexa's ASR platform relies on a large hand-curated lexicon which is comprised of word-pronunciation pairs. This lexicon can never provide complete coverage over the vocabulary as it is often not worth the time or cost required to create these phonetic sequence mappings compared to how infrequently certain words occur during any given Alexa interaction. % too long, can also mention pretty high G2P accuracy  Grapheme to Phoneme systems, on the other hand, can learn these mappings automatically with high accuracy and are responsible for transcribing any out-of-vocabulary  tokens into phonemic representations.  These phonemic representations are an important component that lies between the language model and acoustic model of an ASR system. These OOV tokens often include rare and foreign words, the amount of which may vary depending on the language.  The challenge in designing the G2P system is to create a many-to-many mapping system that will learn not only the mapping between one grapheme and one phoneme, but also where one phoneme is represented by multiple graphemes . And for certain languages like English, these mappings can be inconsistent and ambiguous, especially in the case of names and foreign words.  Sequence to sequence  neural network models are one such way of learning the mappings between graphemes and phonemes where the input and output sequence can vary in length. Originally designed for machine translation, they have been applied on wide variety of problems, such as generative language models.  More recent Seq2Seq models heavily incorporate attention mechanism  and residual learning, while other models use encoder-decoder architectures that are recurrent  or self-attention  based.   Recurrent seq2seq models pose a distinct advantage due to their ability to take in the input history when determining the output state, often outperforming n-gram models on classification tasks due to an n-gram閳ユ獨 heavy dependence on the previous n graphemes . This means that Recurrent Neural Networks  are better suited for sequence problems where longer term context and 閳ユ笩oft閳 input embeddings are important. Long-Short-Term Memory  networks  are in turn better at handling longer sequences and can have more layer depth as they are less prone to diminishing and exploding gradients. Bi-directional LSTMs that consider both past and future contexts have become increasingly popular over RNNs or uni-directional LSTMs that only consider past contexts. % remove LSTM part   Seq2Seq models can be trained on multiple languages at once and used in multi-task and multi-modal learning scenarios. In context of G2P conversion, they allow joint learning of the alignment and translation of graphemes and phonemes in an end-to-end fashion. Therefore, they are a natural fit for our multilingual G2P task, especially since we can train it on large pronunciation lexicons with relatively short sequences lengths.  Seq2Seq models have been found to perform better on these short sequences than compared to very long sequences  . However, until now neural G2P models have not shown superior results on their own , compared to traditional joint-sequence based n-gram models for G2P.  In this paper, we examine whether Seq2Seq LSTM models perform better compared to traditional joint-sequence based n-gram models, given the latest advancements in Seq2Seq modeling for single language pairs. In addition, we investigate whether we can build a single multilingual G2P model which may outperform individual models trained on single language lexicons by utilizing transfer learning thereby improving performance on under-resourced languages and foreign words from different locales. The goal of such a system is to have a single multilingual model that matches or improves over the results of monolingual models without the degradation in accuracy introduced by using multilingual dataset. In particular, the model must be able to distinguish between languages where the same grapheme is paired to different phones, such that the model does not learn a single pairing and apply this pairing to all instances of the grapheme, regardless of input language.  Thus, we wish to avoid situations where a larger lexicon overwhelms a smaller lexicon and erroneously labels a particular grapheme. By using Seq2Seq LSTM models we achieve better PER and WER than low-resource monolingual models, while reducing the potential influence larger-resource lexicons might have in a multilingual model. %In addition, we note that the model which accepts the language label  as input performs better than models which also take as input a language distribution.   
","  Grapheme-to-phoneme  models are a key component in Automatic Speech Recognition  systems, such as the ASR system in Alexa, as they are used to generate pronunciations for out-of-vocabulary words that do not exist in the pronunciation lexicons .   Most G2P systems are monolingual and based on traditional joint-sequence based n-gram models . As an alternative, we present a single end-to-end trained neural G2P model that shares same encoder and decoder across multiple languages. This allows the model to utilize a combination of universal symbol inventories of Latin-like alphabets and cross-linguistically shared feature representations. Such model is especially useful in the scenarios of low resource languages and code switching/foreign words, where the pronunciations in one language need to be adapted to other locales or accents. We further experiment with word language distribution vector as an additional training target in order to improve system performance by helping the model decouple pronunciations across a variety of languages in the parameter space. We show 7.2\% average improvement in phoneme error rate over low resource languages and no degradation over high resource ones compared to monolingual baselines.",11
" ASK is an increasingly important part of Alexa user experience. In ASK work flow, the skill developer provides a set of  , and a list of , which can be mapped to actions, and a set of example phrases defining the grammar of an intent. .\\    From these examples NLU and language modeling  models are built for the skill. Note that it is up to the developer to anticipate all ways their users will interact with the skill. Interactions not covered by the provided examples often have much lower ASR recognition and NLU classification accuracy. Coming up with an exhaustive list of examples can be a hard task for the developer and incomplete coverage can be a frustrating experience to the user. In this work, we propose to use paraphrasing to expand the coverage of developer-provided examples, and thus reduce burden on skill developers and make skill interactions more natural to Alexa customers. Instead of relying on the developer to come up with an exhaustive list of examples for a given intent, in the proposed work flow, we will only require a few examples and then use paraphrasing model to generate other ways a customer might phrase the same command, and then use that data to build better NLU and LM models. Figure  gives an example of the desired paraphrases for a customer utterance.    Paraphrasing is used in various Natural Language Processing applications, such as natural language generation, summarization, information extraction, sentence compression and question answering. Traditional paraphrase generation methods exploit hand-crafted rules  or automatically learned complex paraphrase patterns , use thesaurus-based  or semantic analysis driven natural language generation approaches , or leverage statistical machine translation ; .  In this paper, we propose to use neural machine translation  as a simple and flexible approach to MT to address the paraphrase generation problem. We observe that in translation, there is not a single correct translation target, but rather several variants of the sentence, carrying the same meaning, or paraphrases. From this perspective, translation can be seen as paraphrasing the source sentence in a different language. Therefore, NMT is quite natural approach to paraphrasing. It has been shown to have comparable performance to the phrase-based translation systems , and it is very flexible and modular, allowing to reuse pre-trained components, such as word embeddings or other networks trained on different datasets.  The remainder of the paper is organized as follows:  Section  presents a brief overview of the sequence to sequence models and techniques used in this work, Section  describes the available data, Section  explains the experimental setup, Section  presents the evaluation results, Section  analyzes the results and discusses future work, Section  is conclusion. %, followed by .   
"," AAAI creates proceedings, working notes, and technical reports directly from electronic source furnished by the authors. To ensure that all papers in the publication have a uniform appearance, authors must adhere to the following instructions.",12
"  Due to emergence as global language, English as a second language has been established as primary medium of instruction in higher education in several developing countries including India. Reading in a second language  differs from reading in a first language  in distinct ways. L1 learners have well-developed oral proficiency, vocabulary knowledge, and tacit grammar knowledge at the time they start learning to read, which leads to fluent processing of text information. L2 learners have limited oral proficiency and learners, and underdeveloped grammar knowledge. Therefore, compared to L1 learners, L2 learners are invariably slower and less accurate in processing text. One highly recommended procedure for improving L2 learners' oral fluency is learners readings .   Researchers of different domains such as prosody, acoustics, lexicon, syntactics proposed several measures to evaluate L2 learners' oral proficiency and narrative production. These measures can be applied for evaluating the impact of learners' repeated readings on their proficiency improvement. Also, availability of computational tools belonging to above mentioned domains make possible to develop a CALL system for L2 learners. A Computer Assisted Language Learning  system enables convenient and low-cost language learning, which focuses on developing the speaking, listening, and writing skills, and some of them are put to practice .   RR requires learners to reread a passage several times to achieve a pre-established level of fluency. The goals for RR are to increase learners' reading speed, transfer learning to new passages, and improve comprehension. There are two forms of repeated reading: unassisted and assisted. With unassisted repeated reading, learners are given reading passages that contain recognizable words at their independent reading levels. Each learner silently or orally reads his/her passage several times until he/she reaches the predetermined level of fluency. Assisted repeated reading, on the other hand, involves repeated reading whilst or after listening to either a teacher reading the same text or a recorded version . RR has been found to increase fluency and comprehension for first-language  \& second-language  learners not only with treatment texts but with new, unpracticed learners . Thus, RR leads to improvements in speech prosody, a component of reading fluency indicative of learners' comprehension of texts .   Prosody describes variation in intonation, duration, rhythm, and intensity, is a critical component of perceived fluency in spoken language, as prosodic variation signals not only syntactic and semantic structure of sentences but also emotion. For example, Kuhn et al.  stated `in addition to the role of rate and accuracy, prosodic fluency requires appropriate expression or intonation coupled with phrasing that allows for the maintenance of meaning' .   Several researchers have assessed the relationship between prosody and acoustics, about how prosodically fluent learners cue syntactic structure and semantic structure. For example, speakers often cue syntactic phrase boundaries through the employment of intonational phrase boundaries, the presence of silence between words and a pitch excursion, which can be rising in interrogatives sentences or falling in declaratives. Imoto et al.  addressed sentence-level stress detection of English for Computer-Assisted Language Learning by Japanese learners .  %Researchers deal with this complexity by relying on trained human annotators' perception of prosodic features, as in the Tones and Break Indices  systems .  Trofimovich et al.  used acoustic features  to determine how accurately five prosody features  were produced by L2 English speakers .    Horst et al.  found that repeated readings of L2 text help learners to identify the meaning and form of words without access to a dictionary or other learning support . With phonological support, it provides a supportive environment for both incidental and intentional novel vocabulary acquisition. Many L2 development studies have reported that a variety of lexical richness measures, along with measures of accuracy, fluency, and grammatical complexity, can be used as reliable and valid indices of the learner's developmental level or overall proficiency in an L2 .    Ortega  stated `Syntactic complexity  refers to the range of forms that surface in language production and the degree of sophistication of such forms' . The measures used to examine syntactic complexity in L2 English writing development include length of production unit , amount of embedding, subordination and coordination, range of structural types, and structural sophistication. Several studies have examined relations between the syntactic complexity of speech and the speakers' holistic speaking proficiency levels. Iwashita's  study on Japanese L2 speakers found that length-based complexity features  are good predictors for oral proficiency . To realize a voice-interactive CALL system, Anzai et al. proposed n-gram model based methods for improving recognition accuracy of speech with grammatical mistakes .  
"," Repeated reading  helps learners, who have little to no experience with reading fluently to gain confidence, speed and process words automatically. The benefits of repeated readings include helping all learners with fact recall, aiding identification of learners' main ideas and vocabulary, increasing comprehension, leading to faster reading as well as increasing word recognition accuracy, and assisting struggling learners as they transition from word-by-word reading to more meaningful phrasing. Thus, RR ultimately helps in improvements of learners' oral fluency and narrative production. However, there is no open audio datasets available on oral responses of learners based on their RR practices. Therefore, in this paper, we present our dataset, discuss its properties, and propose a method to assess oral fluency and narrative production for learners of English using acoustic, prosodic, lexical and syntactical characteristics. The results show that a CALL system can be developed for assessing the improvements in learners' oral fluency and narrative production.",13
"  NMT is a new approach to machine translation that has achieved great success in the last a few years . Compared to plain SMT , a neural language model decoder  is better at long-distance re-ordering, and attention mechanisms  have been proven effective in modeling long-distance dependencies, while these two issues were both challenging for SMT.  The Transformer , which has outperformed previous RNN/CNN based translation models , is based on multi-layer multi-head attention networks and can be trained in parallel very efficiently. Though attentional networks can connect distant words via shorter network paths than RNNs, empirical results show that its ability in capturing long-range dependencies does not significantly outperform RNNs, and it is still a problem for the Transformer to fully model long-distance dependencies .  Using phrases instead of words enables conventional SMT to condition on a wider range of context, and results in better performance in re-ordering and modeling long-distance dependencies. It is intuitive to let the NMT model additionally condition on phrase level representations to capture long-distance dependencies better, but there are two main issues which prevent NMT from directly using phrases:    Instead of using phrases directly in NMT, in this work, we address the issues above with the following contributions:     Our approach empirically brings about significant and consistent improvements over the strong Transformer model . We conducted experiments on the WMT 14 English-German and English-French news translation task, and obtained  and  BLEU improvements respectively on top of the strong Transformer Base baseline, which demonstrates the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. It also shows effectiveness with the Transformer Big setting. We also conducted length analysis with our approach, and the results show how our approach improves long-distance dependency capturing, which supports our conjecture that phrase representation sequences can help the model capture long-distance relations better.  
"," The Transformer translation model  based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation . Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies . Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation  approach through the use of larger translation blocks  and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.",14
" Dialog systems research has primarily been focused around two main types of applications -- task-oriented dialog systems that learn to use clarification to aid in understanding a user's goal~, and open-ended dialog systems that are expected to carry out unconstrained ``chit chat'' conversations~.  Much of this research, assumes access to training dialogs of the type the system is expected to perform, and aims to build a dialog system that can then engage in the same type of interactions.   This is also the case with most machine learning research, which is focused on a learning problem in the context of a fixed domain and task, that do not change between training and test time. However, when these systems are used in real-world scenarios, the domain is often wider than that of the original training set, and the requirements of the task may change over time.  Lifelong learning research aims to develop machine learning systems that can be robust to this kind of change, making use of knowledge from previous tasks to improve the performance and sample efficiency of future tasks. Lifelong learning can reduce the dependence of learned systems on narrow well-defined tasks and large annotated datasets. %Lifelong learning was first introduced in the context of control problems in robotics by  as a method to generalize the knowledge from previously learned control tasks to new control tasks over time.  %Another early work is by  where an agent must build a theory of a domain and and choose which of multiple learnable tasks to learn next.  In this position paper, we present the problem of designing dialog systems that enable lifelong learning as an important challenge problem, in particular for applications involving physically situated robots.    Lifelong learning is particularly relevant for robotics applications that involve an agent physically interacting with its environment because it is difficult and expensive to obtain labelled data during training that adequately covers all scenarios that the agent is likely to encounter during operation.  Recent work has tried to address this issue using simulation techniques to increase the robustness of differences in the task and domain from training and operation time~. However, a complementary direction would be to leverage interactions with humans that such a system is likely to have during operation to obtain additional labelled data to adapt to changes in domains or tasks that occur during operation.   Dialog systems by their very nature place the system in a position where it is interacting with a human user. Thus the system is in a position where it can query the user for additional information which may be useful for future interactions. With dialog systems, as with most machine learning systems, it is reasonable to assume that if the trained dialog system interacts with users in the future, those dialogs can be used to further improve the system. In particular, since open-ended dialog systems do not in principle make assumptions about the domain of discussion or the types of dialog acts that the user and system engage in, they can be considered to be performing some form of lifelong learning.  However, there are many types of information that dialog systems can explicitly query users for during operation. Some examples include entries to be added to a knowledge base , new words that refer to concepts for which the system has a learned model , or labels that can be used to train supervised models .   We propose a new focus area for dialog systems research that includes identifying such information-gathering dialog acts that are relevant to different types of dialog systems, learning dialog systems that make use of such dialog acts, and user studies and other supportive research necessary for making such systems more usable in real-world scenarios. We call this area ``dialog for supporting lifelong learning,'' and present it as an interesting challenge problem for dialog researchers, and review some initial directions on work in this area.  We believe this is especially relevant for dialog systems on embodied robots, as these systems face more difficulties due to the shortage of available training data, and can hence benefit more by using learning techniques that are better designed to adapt to novel test data. We believe this area presents a number of interesting challenges regarding dataset and task design, speech processing, sample efficiency, and dialog user analyses.  The rest of this position paper discusses existing work in the area and challenges for the future.  
"," Dialog systems research has primarily been focused around two main types of applications -- task-oriented dialog systems that learn to use clarification to aid in understanding a goal, and open-ended dialog systems that are expected to carry out unconstrained ``chit chat'' conversations.  However, dialog interactions can also be used to obtain various types of knowledge that can be used to improve an underlying language understanding system, or other machine learning systems that the dialog acts over.  In this position paper, we present the problem of designing dialog systems that enable lifelong learning as an important challenge problem, in particular for applications involving physically situated robots.   We include examples of prior work in this direction, and discuss challenges that remain to be addressed.",15
"  Sequence-to-sequence  learners~ demonstrated remarkable performance in machine translation, story generation, and open-domain dialog~. These capabilities, however, come at the expense of being hardly interpretable black-boxes, which spurred an interest in better understanding their inner working~.  In this work, we focus on studying inductive biases of seq2seq models. We start from an observation that, generally, multiple explanations can be consistent with a limited training set, each leading to different predictions on unseen data. A learner might prefer one type of explanations over another in a systematic way, as a result of its inductive biases~. %The study of models generalization from a finite training set has long been studied from a theoretical point of view~  To illustrate the setup we work in, consider a quiz-like question: if  maps to  , what does  map to? The ``training'' example is consistent with the following answers: 6 \{3,5,8\}\mathcal{M}yx^*$ \\ Facebook AI \And Rahma Chaabouni\thanks{Equal contribution} \\ Facebook AI / ENS Ulm }       Sequence-to-sequence  learners are widely used, but we still have only limited knowledge about what inductive biases shape the way they generalize. We  address that by investigating how popular seq2seq learners generalize in tasks that have high ambiguity in the training data. We use SCAN and three new tasks  to study learners' preferences for memorization, arithmetic, hierarchical, and compositional reasoning.  Further, we connect to Solomonoff's theory of induction and propose to use description length as a principled and sensitive measure of inductive biases.  In our experimental study, we find that LSTM-based learners can learn to perform counting, addition, and multiplication by a constant from a . Furthermore, Transformer and LSTM-based learners show a bias toward the hierarchical induction over the linear one, while CNN-based learners prefer the opposite.  On the SCAN dataset, we find that CNN-based, and, to a lesser degree,  Transformer-  and LSTM-based learners have a preference for compositional generalization over memorization. Finally, across all our experiments, description length proved to be a sensitive measure of inductive biases.             
"," Sequence-to-sequence  learners are widely used, but we still have only limited knowledge about what inductive biases shape the way they generalize. We  address that by investigating how popular seq2seq learners generalize in tasks that have high ambiguity in the training data. We use SCAN and three new tasks  to study learners' preferences for memorization, arithmetic, hierarchical, and compositional reasoning.  Further, we connect to Solomonoff's theory of induction and propose to use description length as a principled and sensitive measure of inductive biases.  In our experimental study, we find that LSTM-based learners can learn to perform counting, addition, and multiplication by a constant from a . Furthermore, Transformer and LSTM-based learners show a bias toward the hierarchical induction over the linear one, while CNN-based learners prefer the opposite.  On the SCAN dataset, we find that CNN-based, and, to a lesser degree,  Transformer-  and LSTM-based learners have a preference for compositional generalization over memorization. Finally, across all our experiments, description length proved to be a sensitive measure of inductive biases.",16
" Cross-lingual text summarization  is the task of compressing a long article in one language into a summary in a different language.  Due to the dearth of training corpora, standard sequence-to-sequence approaches to summarization cannot be applied to this task.  Traditional approaches to XLS thus follow a pipeline, for example, summarizing the article in the source language followed by translating the summary into the target language or vice-versa. Both of these approaches require separately trained summarization and translation models, and suffer from error propagation.        Prior studies have attempted to train XLS models in an end-to-end fashion, through knowledge distillation from pre-trained machine translation  or monolingual summarization models~, but these approaches have been only shown to work for short outputs.  Alternatively,  proposed to automatically translate source-language summaries in the training set thereby generating pseudo-reference summaries in the target language. With this parallel dataset of source documents and target summaries , an end-to-end model is trained to simultaneously summarize and translate using a multi-task objective.  %They consider this dataset as the parallel data, and train models to simultaneously summarize and translate using a multi-task objective.  %  construct a large-scale cross-lingual summarization dataset  using a machine translation  model. They train a model to do end-to-end CLS with a multi-task learning objective of being able to do MS, MT and CLS simultaneously.  Although the XLS model is trained end-to-end, it is trained on MT-generated reference translations and is still prone to compounding of translation and summarization errors.   In this work, we propose to train an end-to-end XLS model to directly generate target language summaries given the source articles by matching the semantics of the predictions  with the semantics of the source language summaries. To achieve this, we use reinforcement learning  with a bilingual semantic similarity metric as a reward. This metric is computed between the machine-generated summary in the target language and the gold summary in the source language.  Additionally, to better initialize our XLS model for RL, we propose a new multi-task pre-training objective based on machine translation and monolingual summarization to encode common information available from the two tasks. To enable the  model to still differentiate between the two tasks, we add task specific tags to the input.  We evaluate our proposed method on English--Chinese and English--German XLS test sets. These test corpora are constructed by first using an MT-system to translate source summaries to the target language, and then being post-edited by human annotators.  Experimental results %  demonstrate that just using our proposed pre-training method without fine-tuning with RL improves the best-performing baseline by up to 0.8 ROUGE-L points. Applying reinforcement learning yields further improvements in performance by up to 0.5 ROUGE-L points. Through extensive analyses and human evaluation, %,  we show that when the bilingual semantic similarity reward is used, our model generates summaries that are more accurate, longer, more fluent, and more relevant than summaries generated by baselines.   
"," Cross-lingual text summarization aims at generating a document summary in one language given input in another language. It is a practically important but under-explored task, primarily due to the dearth of available data. Existing methods resort to machine translation to synthesize training data, but such pipeline approaches suffer from error propagation. In this work, we propose an end-to-end cross-lingual text summarization model. The model uses reinforcement learning to directly optimize a bilingual semantic similarity metric between the summaries generated in a target language and gold summaries in a source language. We also introduce techniques to pre-train the model leveraging monolingual summarization and machine translation objectives. Experimental results in both English--Chinese and English--German cross-lingual summarization settings demonstrate the effectiveness of our methods. In addition, we find that reinforcement learning models with bilingual semantic similarity as rewards generate more fluent sentences than strong baselines.\footnote{\url{https://github.com/zdou0830/crosslingual_summarization_semantic}.}",17
"  %\let\thefootnote\relax\footnotetext{* indicates the corresponding author.}   Text classification is one of the fundamental tasks in Natural Language Processing . The goal is to classify texts according to certain criteria. It has many practical applications such as sentiment analysis  and topic categorization .  %Traditional approaches, on the basis of the text representation, extract features for a general classifier. %For instance, in the model of bag-of-words, statistics on unigrams, bigrams, and -grams are used as features . %These traditional methods ether totally ignore the word order or constrain their focus on  small word tuples, %resulting in inevitable information loss. Moreover, they suffer from the problems of data sparsity and high dimensionality.  Traditional approaches, for example, bag-of-words, extract features such as statistics on unigrams and bigrams for a general classifier. In recent years, the development of pre-trained word embeddings and deep neural networks has brought new inspiration to various NLP tasks. Word embeddings are used to map words to an implicit space where semantic relationships are preserved in their reciprocal closeness commonly measured by the Euclidean norm. This type of representation can alleviate the data sparsity problem . Moreover, researchers demonstrate that pre-trained word embeddings are able to capture meaningful syntactic and semantic regularities . With the help of word embeddings, deep learning models such as convolutional neural network   and recurrent neural network   are proposed to further process the semantic representation within texts. This methodology has made impressive progress in text classification .   CNN has been proven to be a powerful semantic composition model for modeling texts . CNN treats texts as a 2D matrix by concatenating embedding of words together. It utilizes a 1D convolution operator to perform the feature mapping, and then conducts a 1D pooling operation over the time domain for obtaining a fixed-length output feature vector. Based on the convolution operation, it is able to capture both local and position-invariant features in texts.  Alternative popular neural network model, RNN , treats texts as sequential data and analyzes texts word by word.  Fixed-length hidden units in RNN stores the contextual information up to the present word. Long Short-term Memory  units  and gated recurrent units   are two popular prototypes that aim to solve gradient vanishing and gradient explosion problems.   Some approaches attempt to combine CNN and RNN to incorporate the advantages of both models . However, most of them integrate CNN and RNN only by streamlining the two networks , which might decrease the performance of them.   %In this paper, we propose a novel method to combine these two models. The intuition underlying our model is that %different words contribute differently to the meaning of the whole text and the key parts can be well extracted by CNN. % As an example in Table  , both sentences are labeled as Science and Technology. % Obviously, one can verify that the words in bold are the most informative. This type of information, due to its locality, % as mentioned above, can be captured by a CNN due to its capability of extracting local and position-invariant features.  In addition, prior works neglect the fusion of contextual information when learning the word's contextual representation.  %Another point that is neglected by prior work is on the fuse of bidirectional word representations.  Most methods incorporating bi-directional RNN to model each word's contextual representation usually choose concatenating the forward and backward hidden states at each time step .  As a result, the resulting vector does not have interaction information of the forward and backward hidden states.  %However, hidden state in one direction becomes ``myopic'', and may be against to the meaning collected by another hidden state.  Meanwhile, the hidden state in one direction may be ``myopic'' and against the meaning collected by another hidden state.  Intuitively, a word閳ユ獨 contextual representation is more accurate when holistic semantics are  collected and fused from two directions.  Failure in doing so may lose true meaning for the focus word,  especially for polysemic words whose meanings are context-sensitive.  %We believe hidden state in one direction becomes ``myopic'', and may be against to the meaning collected by another hidden state.   %It is a good choice to use a representation that fuse hidden states of forward and backward RNN more directly.  %This problem can be solved by improving the interaction between forward and backward hidden states. %In other words, it cannot capture the compositional meaning of context. %It is a good choice to use a representation that fuse hidden states of forward and backward RNN more directly. %   %Different words contribute differently to the representation of the text meaning. Take XXX as example, both sentence are labeled as Science and Technology. The words in bold are most informative of each sentence.  %%閺鐟板煂鏉╂瑩鍣锋禍%  %Different words contribute differently to the representation of the text meaning. %We hypothesize importance of each word in text can be inferred from local context. %% For example, it can give practitioners hints on %%  how semantically conflict words jointly determining the polarity in sentimental %%  analysis. After revisiting this problem, our paper we assume that some key %%words in text may be enough in determining the semantic meaning and polarity. %Take  as an example, both sentences should be classified as . The key words which are important to determine the category is %in bold. % % %%For example, in Table . The key words that determine the category %%can be well extracted just based on local context. %% %%we assume that some of words have stronger correlations to the %%meaning of text than others. %% %%However, none of them could provides insight into which words in text contribute to the classification decision which can be of value in applications and analysis . %%However, none of these methods could capture different aspects of text and gives for classification and give importance of each words in text. % % % %    %\fbox{ %{0.8\linewidth} %    %        % %} % %   In this paper, we propose a neural network model that incorporates CNN and RNN in a novel way. The intuition underlying our model is that different words contribute differently to the meaning of the whole text and the key parts can be well extracted by CNN. As an example in Figure , the words in bold are the most informative for the sentences labeled as Science and Technology. Due to the capacity of capturing local and position-invariant features, CNN is utilized to extract local features and learn a 2D matrix that shows the importance of each word from different aspects. Meanwhile, the bi-directional RNN is applied to learn  word contextual representations. A neural tensor layer is introduced on the top of the bi-directional RNN to obtain the fusion of the bi-directional contextual information surrounding the focus word. We call this novel neural network as convolutional recurrent neural network  and apply it to the task of text classification. By combining the convolution and recurrent structure, our method preserves the advantage of both CNN and RNN.  Our contributions in this paper are listed as follows:  %- We propose a self-attention mechanism which based on CNN to extract different aspect of text.        %In this paper, we assume that some of words have stronger correlations to the %meaning of text than others and these correlations could be partly captured %based on the local information in text. Therefore, in order to achieve %the relative importance among words. we propose a CNN-based self-attention %neural network for text classification. The relative importance %in the context of neural network, helps a model to dedicate the capacity to %those having strong correlations with desired output. our method use CNN on word sequence %to get attention information, requiring no extra inputs.  \iffalse 
","  Convolutional neural network  and recurrent neural network  are two popular architectures used in text classification. Traditional methods to combine the strengths of the two networks rely on streamlining them or concatenating features extracted from them. In this paper, we propose a novel method to keep the strengths of the two networks to a great extent. In the proposed model, a convolutional neural network is applied to learn a 2D weight matrix where each row reflects the importance of each word from different aspects. Meanwhile, we use a bi-directional RNN to process each word and employ a neural tensor layer that fuses forward and backward hidden states to get word representations. In the end, the weight matrix and word representations are combined to obtain the representation in a 2D matrix form for the text. We carry out experiments on a number of datasets for text classification. The experimental results confirm the effectiveness of the proposed method.",18
"  Human learners can acquire any of the world's languages from finite data.  The acquisition of a particular language involves two factors:  data from that language,  and the learner's inductive biases, which are the factors that determine how the learner will generalize beyond the particular utterances in the data . Many inductive biases are shared by all humans ( often refers to cognitive biases, we use it to encompass all pressures that shape the language that a learner learns; see Figure and the Background section.}    , a technique in which a learner is exposed to a variety of tasks, each of which comes with a limited amount of data. This process instills in the learner a set of inductive biases which allow it to learn tasks similar to those it has seen before from limited data. In our setting, each ``task"" is a different language, and the inductive biases that result from meta-learning are encoded in a neural network's initial state. This initial state is found in a data-driven manner; by controlling the data, we can influence which inductive biases will be encoded in the initial state, and the initial state can then be analyzed to verify that it encodes the universal inductive biases that it is intended to encode.    As a first case study, we show the effectiveness of this approach on the acquisition of a language's syllable structure, a paradigmatic example of universal linguistic inductive biases. We define a set of inductive biases relating to syllable structure that we intend to give our model, and we then translate this set of inductive biases into a space of possible languages from which we have a model meta-learn. Through analysis of the meta-learned initial state, we verify that meta-learning has successfully imparted the inductive biases that it was intended to impart; for example, the model has meta-learned that the presence of certain input-output mappings in a language implies the presence of other input-output mappings.\footnote{Our code is at \url{https://github.com/tommccoy1/meta-learning-linguistic-biases}; there is also a demo at \url{http://rtmccoy.com/meta-learning-linguistic-biases.html}.}      
"," How do learners acquire languages from the limited data available to them?  This process must involve some inductive biases---factors that affect how a learner generalizes---but it is unclear which inductive biases can explain observed patterns in language acquisition. To facilitate computational modeling aimed at addressing this question, we introduce a framework for giving particular linguistic inductive biases to a neural network model; such a model can then be used to empirically explore the effects of those inductive biases. This framework disentangles universal inductive biases, which are encoded in the initial values of a neural network's parameters, from non-universal factors, which the neural network must learn from data in a given language. The initial state that encodes the inductive biases is found with meta-learning, a technique through which a model discovers how to acquire new languages more easily via exposure to many possible languages. By controlling the properties of the languages that are used during meta-learning, we can control the inductive biases that meta-learning imparts. We demonstrate this framework with a case study based on syllable structure. First, we  specify the inductive biases that we intend to give our model, and then we translate those inductive biases into a space of languages from which a model can meta-learn. Finally, using existing analysis techniques, we verify that our approach has imparted the linguistic inductive biases that it was intended to impart.   Keywords:  meta-learning, inductive bias, language universals,  syllable structure typology, neural networks",19
" Language models  predict a probability distribution over text, and are a fundamental technology widely studied in the natural language processing  community . Modern LMs are almost exclusively based on  recurrent  or self-attentional  neural networks. These models are of interest scientifically as one of the purest tests of our ability to capture the intricacies of human language mathematically . They also have broad downstream applications in generating text in systems such as machine translation , summarization , or dialog generation , as well as in the unsupervised representation learners that now power many applications in NLP . % Text generation is a key element of many NLP tasks, such as machine translation, summarization, and dialogue generation. % These systems often adopt an encoder-decoder architecture where the decoder is based on neural language models \gn{Maybe cite a few, just for completeness?}. % In unsupervised setting, such neural language models usually generate sentences word-by-word from scratch.  % which may lack the inductive bias to faithfully represent the full diversity of complex utterances and does not expose an interpretable way of the generation process.  However, there has been a recent move towards  neural LMs  that generate sentences by first selecting examples from an external datastore. % For instance,  model the token-level probability at test time by interpolating the language model with a kNN distribution from the nearest context-token pairs in the datastore, while~ store external memories on sentence level and feature a prototype-then-edit process of  selecting a  sentence from a the prototype datastore, and  editing this prototype to the final desired output. % In this paper, we focus on the prototype-then-edit model family which is a lot lighter relatively in terms of memory and time cost at test time. % though being still absolutely very expensive that we will discuss later.  Intuitively, these non-parametric LMs are attractive because they help remove some of the pressure on the parametric model to memorize the entirety of the language it must model. % These intuitive advantages are also borne out in superior performance on language modeling tasks , as well as down-stream applications such as dialogue response generation~, machine translation~, and code generation~. % In addition, the prototypes and continuous representations of the edits in prototype-based models lend an element of interpretability to the modeling process. % On the down side, however, previous prototype-driven generation methods usually need to store and index a large prototype candidate pool , leading to significant issues with memory and speed efficiency at test time.   In this paper, we hypothesize that, in fact, a  set of prototypes is sufficient to achieve the great majority of the gains afforded by such non-parametric models. Intuitively, in a large corpus many sentences look very similar and may be represented by minor transformations of a single prototype sentence. For example, the sentence ``I ordered a burger with fries'' can serve as the prototype for data samples with the form ``I ordered [NOUN PHRASE] with [NOUN PHRASE]''. This is evidenced by~'s observation that 70 of the test set in the Yelp restaurant review corpus~ is within word-token Jaccard distance 0.5 of one training sentence.   To take advantage of this intuition, we propose a novel generative model that samples prototypes from a  prototype distribution, which itself is sampled from a symmetric Dirichlet prior, as shown in Figure . The Dirichlet prior with appropriate hyperparameters is able to encourage a  prototype selection distribution, allowing us to reduce the prototype support set at test time to greatly improve efficiency. Moreover, we utilize amortized variational inference~ to train our model, which introduces a learnable prototype retriever to identify prototypes useful for generating each sentence . This is different from~ where prototypes for each sentence are fixed before training through edit distance heuristics.  % \gn{This is a slight bit repetitive with the abstract. Not sure if it's bad enough to modify.} We evaluate our approach on the MSCOCO~ and Yelp restaurant review~ corpora. Our method is able to improve perplexity over the neural language model baseline by up to 14 points and previous neural editor model by 6 points while achieving over 1000x memory savings and a 1000x speedup at test time . Interestingly, we find that the learned prototypes are able to represent different features when varying sparsity levels -- a strong sparsity prior forces the model to share prototypes and the induced prototypes turn out to represent more generic features . On the text generation side, our model is able to generate sentences that resemble the given prototype while allowing for smooth interpolation on the edit space as well .     % mention soft prototype work % wang2019neural   
"," % We propose a novel generative model that learns to identify a sparse prototype support set to generate sentences. We assume that a small set of sentences serve as prototypes  to be edited to generate a large number of diverse examples. Different from previous prototype-driven models which usually require to store and index a large retrieval database, our model is trained to identify only a small number of sentences as prototypes automatically. This is achieved by imposing a Dirichlet prior on the prototype selection distribution to encourage sparsity. In experiments, our model outperforms previous neural edit models on language modeling while bringing 100x memory saving and 50x speed-up at test time. More interestingly, we show that the learned prototypes are able to capture semantics or syntax at different granularity as we vary the sparsity of prototype selections, and certain sentence attributes can be controlled by specifying the prototype for generation.\footnote{Code will be released after the review period.}  Prototype-driven text generation uses non-parametric models that first choose from a library of sentence ``prototypes'' and then modify the prototype to generate the output text. While effective, these methods are inefficient at test time as a result of needing to store and index the entire training corpus. Further, existing methods often require heuristics to identify which prototypes to reference at training time. In this paper, we propose a novel generative model that automatically learns a  prototype support set that, nonetheless, achieves strong language modeling performance. This is achieved by  imposing a sparsity-inducing prior on the prototype selection distribution, and  utilizing amortized variational inference to learn a prototype retrieval function. In experiments, our model outperforms previous prototype-driven language models while achieving up to a 1000x memory reduction, as well as a 1000x speed-up at test time. More interestingly, we show that the learned prototypes are able to capture semantics and syntax at different granularity as we vary the sparsity of prototype selection, and that certain sentence attributes can be controlled by specifying the prototype for generation.\footnote{Code is available at \url{https://github.com/jxhe/sparse-text-prototype}.}",20
" %AMINE: Common sense or commonsense??  %Common sense reasoning is one of the long standing challenging problems - previous work on capturing common sense knowledge of a model deals with indirect measure. Humans are good at reasoning based on the knowledge they possess, but for machine learning models,  it has always been strenuous. Even the simple tasks like reference resolution for example : The trophy would not fit in the brown suitcase because it was too i. What was too big?, for humans, it's implicit that it's the trophy that's too big to fit in a suitcase. Still, for Natural Language Understanding  models, it's challenging to understand what it refers too. The earlier work by Devlin~ and Rahman~ has shown a impressive results on reference resolution task. Common sense reasoning is one of the long-standing problems in natural language understanding.  Previous work on modeling common sense knowledge deals mainly with indirect  tasks %measures such as co-reference resolution , % ,  or selecting the plausible situation based on the given subject or scenario . %. %%% Comment by CH % Humans are good at reasoning based on the knowledge they have, but for machine learning models, this has always been challenging, even for simple tasks like co-reference resolution. For instance, consider this reference resolution example: ``The trophy would not fit in the brown suitcase because 	extbf{it was too big閳ユ絹. The question is ``What was too big?閳ユ絹. For humans, it is implicit that it is the trophy that is too big to fit in a suitcase. Still, for Natural Language Understanding  models, it is challenging to understand what 閳ユ泛textit{it閳 refers too. The earlier works by Devlin et al. ~ and Rahman and Ng  have shown impressive results on the co-reference resolution task. % %Although, those systems does a great job in reference resolution it does not mean the system process common sense knowledge. If we give a false statement  to the system  for example I bought an elephant and kept it in the fridge. What it refers too? the system is capable of classifying it refers to elephant but the statement itself does not make sense and system cannot classify or generate an explanation to it. %%% Comment by CH % While those systems do a great job in reference resolution, they do not process common sense knowledge. For example, if  the system is fed with a false statement  like ``I bought an elephant and kept it in the fridge閳ユ絹, and is  asked 	extit{``What does ``it閳 refer to?閳ユ絹,  it is able to classify 	extit{``it閳ユ絹 as referring to 	extit{``elephant閳ユ絹. % but it is not capable of classifying the statement as non-sensical or of generating a coherent explanation on why it does not make sense. %Nevertheless, the system is not capable of distinguishing the statement between sensical and non-sensical. %%% Comment by CH % Nevertheless, it is not capable of distinguishing a sensical statement from a non-sensical one.  % %Even the existing the language model by X,Y are capable of  generating response based on the passage. But all these systems are incapable for generating response as a standalone system to generate response or explanations when a false statement is the input. %In order for a NLU system to start being more lifelike, it needs to have common sense. As common sense vary from person to person to person depending upon various factors. We define 	extbf{common sense for a NLU system as a basic ability for the system to make a practical judgement based on knowledge and experience, which have already been learned through other sources. %%% Comment by CH % Existing language models by , ,   are also capable of generating a response based on a textual passage, but when a false statement is introduced as input, all these models, as standalone systems, fail to generate a response that includes explanations. A more lifelike NLU system needs to encompass a common sense component. For an NLU system, we define common sense    % In this paper we tackle the task of generating    In this paper, we present our system that we devised to tackle Task C, Explanation , of the SemEval 2020 Task 4 - Commonsense Validation and Explanation . %the task 4 of common sense reasoning.  %For a  Given a false or non-sensical statement, the task consists of generating the reason why the given statement does not make sense. We propose a mUlti-task learNIng for cOmmonsense reasoNing . %  language model. %, which is a . %The UNION is a multi-head transformer architecture, we train the language model using ComVE , Open-book , Common sense Explanation   and Open Mind Common Sense   dataset. We train all the dataset parallel in a multi-task learning technique using average weight over the loss function for all the dataset. % The backbone of UNION is a multi-head transformer architecture.  It combines datasets including ComVE , OpenBook , Common sense Explanation   and Open Mind Common Sense   in a multi-task framework.  % parallel training is performed, using average weight over the loss function for all the datasets. % The backbone of UNION is a large pre-trained language model -- GPT2. % Rather than training the language model from scratch, we leverage a pre-trained language model as it saves a significant amount of time, computation power and data to train.    % In contrast, we propose a mUlti-task learNIng for cOmmonsense reasoNing %  language model. Rather, than training the language model from the scratch, we leverage the pretrained language model as it save a significant amount of time, computation power and data to train. The UNION is a multi-head transformer architecture, we train the language model using ComVE , Open-book , Common sense Explanation   and Open Mind Common Sense   dataset. We train all the dataset parallel in a multi-task learning technique using average weight over the loss function for all the dataset.  We compare the proposed system to different baselines % and degenerate versions of it,  and report a significant improvement in BLEU score and other evaluation metrics.  % %Our initial submission was on training using ComVE, CoS-E, Open-Book data and we achieved a BLEU score of 15.7 while pretraining the model with OMCS dataset further improved the BLEU score by 0.7.  %Our proposed model also achieves a human evaluation score of 2.10, and it is the highest human evaluation score obtained in the ComVE dataset in the Sem Eval task. Our proposed model achieves a human evaluation score of 2.10, which ranked first on the final leader board for Task C of SemEval 2020 Task 4. In our initial submission, we used ComVE, CoS-E, and OpenBook datasets for training. In that case, a BLEU score of 15.7 is achieved. Pretraining the model with OMCS dataset further improved the BLEU score by 0.7. In addition, we show some of our generations in the appendix.  
"," In this paper, we describe our mUlti-task learNIng for cOmmonsense reasoNing  system submitted for Task C of the SemEval2020 Task 4, which  % - Commonsense Validation and Explanation , which  %to solve the problem of  % The task is to generate a reason explaining why a given false statement  %does not make sense. is non-sensical.  %The human evaluation for the generated explanation by our model is carried out by the Sem-Eval organizers.  % The SemEval organizers carried out a human evaluation for the generated explanations. % by our system.  %UNION.  However, we found in the early experiments that simple adaptations such as fine-tuning GPT2 often yield dull and non-informative generations .  In order to generate more meaningful explanations, we propose UNION, a unified end-to-end framework, to utilize several existing commonsense datasets so that it allows a model to learn more dynamics under the scope of commonsense reasoning.  In order to perform model selection efficiently, accurately and promptly,  we also propose a couple of auxiliary automatic evaluation metrics  %to evaluate  so that we can extensively compare the models from different perspectives. Our submitted system not only results in a good performance in the proposed metrics but also outperforms its competitors with the highest achieved score of 2.10 for human evaluation while remaining a BLEU score of 15.7.  Our code is made publicly available at GitHub \footnote{\url{https://github.com/anandhperumal/ANA-at-SemEval-2020-Task-4-UNION}}. %which also  % Results show  %consolidate the % good performances achieved by our system. %in evaluation.",21
"   % %%  Neural networks are providing ground-breaking results in many complex tasks such as object detection, speech recognition or sentiment analysis. This success is usually attributed to the ability of deep neural networks to generalize well when trained with high quantities of data.  % %%  Among the various neural network types, Convolution Neural Networks  have become particularly popular because of their ability to mimic the functionality of the human brain visual cortex. As a results, CNNs are applied in image-related tasks such as object detection, fingerprint recognition, computer vision etc. The basic structure of a CNN was first applied by LeCun  in  for recognizing images of handwritten digits. A decade of hibernation  passed and they showed back in the late 2000s rebranded as . At this time they also became essential part of various proposed architectures such as  in ,  in  and more.   %  %%  Many natural language processing researchers explored use of CNNs or Recurrent Neural Networks  for text mining tasks such as sentiment analysis, reporting excellent results with little computation load. However, neural network models are usually data hungry and require bigger datasets of training samples. The other problem is the difficulty in finding the optimal hyperparameter setup or design choices when using various types of networks. Optimal network configuration depends on characteristics of available data which should be taken into account.  % \par  %%  We present in this paper the work we conducted for constructing two relatively big datasets of emotionally labeled songs and the results of many experiments with text datasets of different size and document lengths for simplifying neural network construction. For emotional labeling of songs, we utilized social tags crawled from  music portal. We also adapted a model of music emotions that is highly compatible with the popular model of Russell, together with an annotation scheme based on emotion tags each song has received. Furthermore, the works in  and  are extended both quantitatively and qualitatively. The first introduces three variants of a neural network architecture that uses convolution and max-pooling layers for text feature extraction and selection as well as a regularized feed-forward layer for classification. In the second paper, various relations between data properties and neural network parameters with respect to optimal performance are explored. In this work we report accuracy scores of a higher number of experiments with more datasets . Our results can help researchers to simplify hyper-parameter optimization of neural networks that are used for sentiment analysis experiments.  % \par  %% summary of results  % A fact that we noticed is that bigger datasets are better interpreted by repeating several stacks of parallel convolutions followed by max-pooling layers. An interesting regularity is the one that relates length of documents with pooling region size. The later is the parameter that dictates the size of produced feature maps. According to our results, top scores are achieved when pooling region size is set to produce feature maps that are 6 to 18 units long. Also, convolutions with filter lengths one, two and three are usually enough. Utilizing convolutions of longer filters did not improve results. Regarding the three neural network design we proposed, the basic version with max-pooling layers directly following each convolution layer resulted the best one. The flexibility it offers and its low training time make it a good option as a prototyping basis for practitioners.  % \par  %%  The rest of the paper is structured as follows: Section  presents an overview of various neural network models recently used in text mining tasks. Section describes the steps that were followed for the construction of the two music emotion datasets. Section presentes preprocessing steps, utilized datasets, and obtained network parameter optimization results. In Section, we describe the three network architectures we propose. Section  presents the high-level architectural parameters and decisions, together with the literature baselines we compare against. Section discusses obtained results and finally, Secton concludes.  % 
"," % The fabulous results of convolution neural networks in image-related tasks, attracted attention of text mining, sentiment analysis and other text analysis researchers. It is however difficult to find enough data for feeding such networks, optimize their parameters, and make the right design choices when constructing network architectures. In this paper we present the creation steps of two big datasets of song emotions. We also explore usage of convolution and max-pooling neural layers on song lyrics, product and movie review text datasets. Three variants of a simple and flexible neural network architecture are also compared. Our intention was to spot any important patterns that can serve as guidelines for parameter optimization of similar models. We also wanted to identify architecture design choices which lead to high performing sentiment analysis models. To this end, we conducted a series of experiments with neural architectures of various configurations. Our results indicate that parallel convolutions of filter lengths up to three are usually enough for capturing relevant text features. Also, max-pooling region size should be adapted to the length of text documents for producing the best feature maps. Top results we got are obtained with feature maps of lengths 6 to 18. An improvement on future neural network models for sentiment analysis, could be generating sentiment polarity prediction of documents using aggregation of predictions on smaller excerpt of the entire text.  %",22
" 	Recently, task agnostic pre-training with large-scale transformer models  and general text corpora has achieved great success in natural language understanding  as well as natural language generation, especially open-domain dialogue generation. For instance, based on the general language model GPT-2 , DialoGPT  is further trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena  scales up the network parameters to 2.6B and employs more social media conversations in the training process, leading to significant improvement on response quality. To mitigate undesirable toxic or bias traits of large corpora, Blender  further fine-tunes the pre-trained model with human annotated datasets and emphasizes desirable conversational skills of engagingness, knowledge, empathy and personality.  	 	Besides the above attempts from model scale and data selection, PLATO  aims to tackle the inherent one-to-many mapping problem to improve response quality. The one-to-many mapping refers to that one dialogue context might correspond to multiple appropriate responses. It is widely recognized that the capability of modeling one-to-many relationship is crucial for response generation . PLATO explicitly models this one-to-many relationship via discrete latent variables, aiming to boost the quality of dialogue generation. 	 	In this work, we will try to scale up PLATO to PLATO-2 and discuss its effective training schema via curriculum learning . There are two stages involved in the whole learning process, as sketched in Figure . In the first stage, under the simplified one-to-one mapping modeling, a coarse-grained generation model is trained for appropriate response generation  	under different conversation contexts. The second stage continues to refine the generation with a fine-grained generation model and an evaluation model. The fine-grained generation model explicitly models the one-to-many mapping relationship for diverse response generation. To select the most appropriate responses generated by the fine-grained generation model, the evaluation model is trained to estimate the coherence of the responses. 	  	 	As for response selection, previous studies have employed variant scoring functions, including forward response generation probability , backward context recover probability  and bi-directional coherence probability . However, the forward score favors safe and generic responses due to the property of maximum likelihood, while the backward score tends to select the response with a high overlap with the context, resulting in repetitive conversations. In order to ameliorate the above problems, we adopt the bi-directional coherence estimation in the evaluation model of PLATO-2, whose effectiveness is also verified in the experiments. 	 	We trained PLATO-2 models with different sizes: 1.6 Billion parameters and 310 Million parameters. In addition to the English models, we also trained Chinese models with massive social media conversations. Comprehensive experiments on both English and Chinese datasets demonstrate that PLATO-2 outperforms the state-of-the-art models. We have released our English models and source codes at GitHub, hoping to facilitate the research in open-domain dialogue generation. 	 	
"," 		To build a high-quality open-domain chatbot, we introduce the effective training process of PLATO-2 via curriculum learning. There are two stages involved in the learning process. In the first stage, a coarse-grained generation model is trained to learn response generation under the simplified framework of one-to-one mapping. In the second stage, a fine-grained generation model and an evaluation model are further trained to learn diverse response generation and response coherence estimation, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-of-the-art results.",23
" %  .  While deep reinforcement learning  have emerged as a promising solution for complex and high-dimensional decision-making problems, the determination of an effective reward function remains a challenge, especially in multi-domain task-oriented dialog systems. Many recent works have struggled on sparse-reward environments and employed a handcrafted reward function as a breakthrough . However, such approaches are often unable to guide the dialog policy through user goals. For instance, as illustrated in Figure , the user can't reach the goal because the system  that exploits the handcrafted rewards completes the dialog session too early. Moreover, the user goal usually varies as the dialog proceeds.   %   addresses the issue of resolving the ambiguities in selecting a distribution that does not exhibit additional preferences for any decisions.  Inverse Reinforcement Learning   and MaxEnt-IRL  tackles the problem of recovering reward function and using this reward function to generate optimal behavior. Although Generative adversarial imitation learning  , which exploits the GANs framework , has proven that the discriminator   % However, these IRL methods are extremely demanding and have difficulty producing optimal policy when environmental dynamics vary. [16]{r}{0.36\textwidth} {     % [14]{r}{0.38\textwidth} % { % } Do 	extcolor{purple{the block}} in front of 	extcolor{yellow{the tiny yellow cylinder}} and 	extcolor{red{the tiny thing}} that is to the right of 	extcolor{green{the large green shiny object}} have the same color? {} No} % } %  %   %  %  % As standards of living rise and the world閳ユ獨 population grows, the demands for freshwater have been increasing. % Dialog policy       % we propose VRB which uses variational information bottleneck to constrain uninformative gradients in reward estimator and thereby optimizing dialog policy generator. % discerning reward engineering defining. has proven successful on remarkable As handling elaborate sophisticate goals across multi-domain, reward sparsity,  % This is a significant barrier for multi-domain task-oriented dialog system. can be defined as a reward function, GAIL fails to generalize and recover the reward function. Adversarial inverse reinforcement learning   enables GAIL to take advantage of disentangled rewards. Guided dialog policy learning   uses AIRL framework to construct the reward estimator for multi-domain task-oriented dialogs. However, these methods often encounter difficulties in balancing the performance of the policy generator and reward estimator, and produce excessively uninformative gradients.     %  The VRB focuses on capturing discriminative features, by exploiting in-formation bottleneck on mutual information. % The VRB exploits information bottleneck on mutual information between encoded dialog state-dialog action pairs and human dialogs, thereby ensuring a highly informative representation. % enforces upper bound on %    to  VRB exploits a stochastic encoder and enforces an upper bound on mutual information between the encoding and inputs.  In this paper, we propose the Variational Reward Estimator Bottleneck , an effective regularization algorithm. The VRB uses information bottleneck  to constrain unproductive information flows between dialog state-action pairs and internal representations of the reward estimator, thereby ensuring highly informative gradients and robustness. The experiments demonstrate that the VRB achieves the state-of-the-art performances on a multi-domain task-oriented dataset. %  encourages the reward estimator to learn approximation between distributions of human dialog sessions and the policy generator which mimics human behaviors.  %  Note that VRB can infuse meaningful reward information into the policy generator and thereby can guide dialog policy through user goal more successfully. %  where a generator and discriminator are learned adversarially and simultaneously, ameliorating each other. Though GAIL  \clearpage 
"," % This is a significant barrier for approximation between the distributions of human dialogs and policy generator which imitates human behaviors.    Despite its notable success in adversarial learning approaches to multi-domain task-oriented dialog system, training the dialog policy via adversarial inverse reinforcement learning often fails to balance the performance of the policy generator and reward estimator. During optimization, the reward estimator often overwhelms the policy generator and produces excessively uninformative gradients. We proposes the Variational Reward estimator Bottleneck , which is an effective regularization method that aims to constrain unproductive information flows between inputs and the reward estimator. The VRB focuses on capturing discriminative features, by exploiting information bottleneck on mutual information. Empirical results on a multi-domain task-oriented dialog dataset demonstrate that the VRB significantly outperforms previous methods. %     approximation between distributions of human dialog sessions and the policy generator which imitates human behaviors. % VRB uses constraint on mutual information to capture discriminative features. % which make this task more complicated.",24
"    Robots must execute commands that are extended in time while being responsive to changes in their environments. % A popular representation for such commands is linear temporal logic, LTL. % Commands expressed in LTL encode both spatial and temporal constraints that should be true while executing the command. % Executing such commands is particularly difficult in robotics because integration is required between the complex symbolic reasoning that finds satisfying sequences of moves for an LTL command and data-driven perceptual capabilities required to sense the environment. % While individual formulas can be learned by deep networks with extensive experience, we demonstrate how to compose together tasks and skills to learn a general principle of how to encode all LTL formulas and follow them without per-formula experience. % We demonstrate how to integrate the learning abilities of neural networks with the symbolic structure of LTL commands to achieve a new capability: learning to perform end-to-end zero-shot execution of LTL commands.  Given a command represented as an LTL formula, our approach turns that formula into a specific recurrent deep network which encodes the meaning of that command; see~. % The resulting network takes as input the current map state, extracts the environment around the robot, processes it with a co-trained feature extraction network, and predicts which actions will satisfy the formula. % This compositional approach ties together neural networks and symbolic reasoning allowing any LTL formula to be encoded and followed, even if it has never been seen before at training time.  In our experiments, we generate random LTL formulas and train an RL agent to follow those formulas. % We develop a mechanism for generating hard and diverse LTL formulas, as random instances tend to be homogeneous and trivially solved. % This is generally useful for other large-scale experiments on following commands that can be encoded as LTL formulas. % In two different domains,  and , we show that this approach can learn to execute never-before-seen formulas. % The  domain is more akin to boolean satisfiability, where an accepting string must be generated for an LTL formula. % The  domain is a simplified Minecraft introduced by Andreas \etal 2017  to test the integration with robotics; see  for an example of the network in  executing a command in the  domain. % In all cases, we compare against baselines to demonstrate that each part of our model plays a key role in encoding temporal structures. % All components of our networks are learned end-to-end, in a process that automatically isolates the meaning of each sub-network allowing us to compose sub-networks together in novel ways.    This work makes four contributions: % [1.]  % This work can be seen as a novel approach to composing the policies of multi-task reinforcement learning agents in a principled manner according to a particular logic. % While we only discuss LTL here, this approach suggests how other logics might similarly be encoded to create new powerful zero-shot deep approaches to reinforcement learning. %  The best aspects of symbolic reasoning in robotics are compatible with deep networks when both are correctly formulated. % Perhaps in the future, such approaches could be used for model checking with LTL formulas.  % This general approach has the potential to significantly increase the space of % commands that are executable by trained robotic agents. % % % It also  % % Taking as input an LTL formula and finding satisfying assignments for model % checking and robotics is well-explored. %  % LTL on finite trace,   % benchmarking LTL satisfiability checking   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
","   We demonstrate a reinforcement learning agent which uses a compositional   recurrent neural network that takes as input an LTL formula and determines   satisfying actions. The input LTL formulas have never been seen before, yet   the network performs zero-shot generalization to satisfy them. This is a novel   form of multi-task learning for RL agents where agents learn from one diverse   set of tasks and generalize to a new set of diverse tasks. The formulation of   the network enables this capacity to generalize. We demonstrate this ability   in two domains. In a symbolic domain, the agent finds a sequence of letters   that is accepted. In a Minecraft-like environment, the agent finds a sequence   of actions that conform to the formula. While prior work could learn to   execute one formula reliably given examples of that formula, we demonstrate   how to encode all formulas reliably. This could form the basis of new   multi-task agents that discover sub-tasks and execute them without any   additional training, as well as the agents which follow more complex   linguistic commands. The structures required for this generalization are   specific to LTL formulas, which opens up an interesting theoretical question:   what structures are required in neural networks for zero-shot generalization   to different logics?",25
"    The ability to understand and communicate in natural language can improve the accessibility of systems such as robots, home devices and computers to non-expert users.  %Such a system minimally requires a dialog agent that can understand high level natural language commands, and detect and indicate when it has failed to understand what a user requires. %Voice assistant applications that understand high level instructions in natural language are increasingly becoming a part of a variety of devices. Since language is often be ambiguous, it is desirable for such systems to engage in a dialog with the user to clarify their intentions and obtain missing information.  We use clarification to refer to any dialog act that enables the system to better understand an ongoing user request. Common clarification questions obtain or clarify the value of a slot or argument that is part of a goal the user is trying to communicate. %Sometimes, information might be missing in the original command, requiring the system to get more information to fully identify the action desired by the user.  A particular application may also contain domain-specific vocabulary or concepts that were not encountered during training. For example, a system in a shopping domain may need to be updated with the introduction of new clothing styles. Hence, it is desirable for a system to adapt to the operating environment using information from user interactions. We use the term active learning to refer to dialog acts used to obtain such knowledge with the primary purpose of improving the underlying language understanding model and thereby improving performance on future interactions.  Prior work on dialog and user interaction typically focuses either exclusively on clarification, or active learning.  The primary contributions of this work are introducing a dialog task that combines both clarification and active learning, and learning a corresponding dialog policy for this setting that outperforms a static baseline policy. Specifically, we train a hierarchical dialog policy to jointly learn to choose clarification and active learning queries in interactive image retrieval for a fashion domain.    A sample interaction is shown in Figure . We consider an application where a dialog system is combined with a retrieval system to help a customer find an article of clothing. Instead of just showing a large number of retrieved results, the dialog system attempts to use clarifications to refine the search query, and active learning questions to obtain labelled examples for novel concepts unseen during training.  %Clarification questions have been used in dialog systems for robotics applications but these typically use very constrained questions, such as selecting an item from a list .  Task-oriented dialog often requires the system to identify one or more user goals using a slot-filling model.  These systems learn to choose between a set of clarification questions that confirm or acquire the value of various slots.  However, for tasks such as natural language image retrieval, it is difficult to extend the slot-filling paradigm for clarification, as there is no standard set of slots into which descriptions of images can be divided. Also, learned models are needed to identify aspects such as objects or attributes, which are difficult to pre-enumerate. %Note that in both these cases, the response itself need not be a word from a list, but it is assumed that a language understanding component can map the response to a single slot value.  %Such a description is in fact likely to correspond to a single slot value in a larger task - say a task where a robot can store, fetch and manipulate objects.   Some tasks such as GuessWhat?! or discriminative question generation allow the system to ask unconstrained natural language clarification questions.  However they require specially designed models to ensure that learned questions actually decrease the search space.  Such open ended questions are also difficult to answer in simulation, which is often necessary for learning good dialog policies. Hence, in these tasks, the system often learns to ask ``easy'' questions that can be reliably answered by a learned answering module.  In this work, we explore a middle-ground approach with a form of attribute-based clarification.  We use the term ``attribute'' to refer to a mix of concepts including categories such as ``shirt'' or ``dress'', more conventional attributes such as colors, and domain specific attributes such as ``sleeveless'' and ``V-neck''. Although we work with a dataset that contains a fixed set of attributes annotated for each image, we simulate the setting where novel visual attributes are encountered at test time.   Dialog interaction can also be used to improve an underlying model using Opportunistic Active Learning   .  Active learning allows a system to identify unlabeled examples which, if labeled, are most likely to improve the underlying model.  OAL  incorporates such queries into an interactive task in which an agent may ask users questions that are irrelevant to the current dialog interaction to improve performance in future dialog interactions.  Opportunistic queries are more expensive than traditional active learning queries as they may distract from the task at hand, but they can allow the system to perform more effective lifelong learning.  Such queries have been shown to improve performance in interactive object retrieval.  However, this, and other works in reinforcement learning  of policies for active learning do not account for the presence of other interactive actions such as clarification.  We present a dialog task that combines natural language image retrieval with { attribute-based clarification. We then learn a hierarchical dialog policy that jointly learns to choose both appropriate clarification and active learning questions in a setting containing both uncertain visual classifiers and novel concepts not seen during training.  We observe that in our challenging setup, it is necessary to jointly learn dialog policies for choosing clarification and active learning questions to improve performance over employing one-shot retrieval with no interaction.  
"," Intelligent systems need to be able to recover from mistakes, resolve uncertainty, and adapt to novel concepts not seen during training.  Dialog interaction can enable this by the use of clarifications for correction and resolving uncertainty, and active learning queries to learn new concepts encountered during operation. Prior work on dialog systems has either focused on exclusively learning how to perform clarification/ information seeking, or to perform active learning.  In this work, we train a hierarchical dialog policy to jointly perform both clarification and active learning in the context of an interactive language-based image retrieval task motivated by an online shopping application, and demonstrate that jointly learning dialog policies for clarification and active learning is more effective than the use of static dialog policies for one or both of these functions.",26
"  Deep learning methods have achieved state-of-the-art performance in many applications when learning to solve a single problem. In this domain, current efforts are partly focused on engineering new neural architectures or exploring novel methods to improve the accuracy of these single-task models. There is another strain of research that explores learning models that share a certain degree of knowledge among different related tasks. A common approach is to use multitask learning .  In this setting, two or more related tasks are jointly trained such that it is expected that the model achieves better generalization for all the tasks. The main assumption is that related tasks might contain complementary information, which should act as an inductive bias to guide the model towards a better optimum as compared to training one model for each task in isolation .  Multitask learning has been applied to many problems over time, particularly in NLP.  proposed a unified architecture to mutually learn different sequence labeling tasks, a language modeling task and a semantically related words problem.  used an MTL-based model to solve 10 different tasks in natural language  by casting them as question answering.  proposed an MTL-based Named Entity Recognition  in biomedical text mining. In Biomedicine, there are a variety of named entities datasets for the different sub-domains . Some of these datasets are very small, which harms the performance of a model trained to classify named entities. However, obtaining labeled datasets is usually done manually by experts and are expensive to develop. Therefore,  cast domain-specific NER datasets as the different tasks and proposed a model to mutually learn them to improve generalization. Their results disclose yet another benefit of MTL models, by learning multiple tasks the model capitalizes from more data, such that tasks with small training sets can generalize better.  Even though great average improvements were achieved, the results are often mixed when evaluating each task in isolation and comparing them with the state-of-the-art performance of single-task models . The challenge of selecting a set of tasks, which guarantees better generalization for all tasks in multitask settings, remains partially unsolved. This is, to a certain extent, due to the few theoretical investigations that clearly expose the conditions under which multitask learning leads to better performance.  The few works that do try to theoretically understand generalization in MTL, however, rely mostly on strong assumptions about how tasks are related or are only applied to simple scenarios, such as settings where all tasks contribute with datasets of the same size.  proposed generalization bounds for MTL using VC-dimension and covering numbers, however, he assumes that task relatedness is given a priori.  used the Rademacher complexity to analyze linear multitask methods. In a later work,  have expanded their evaluation to nonlinear methods and assumed that similar tasks shared a common feature representation. They proposed a bound based on the Gaussian average while evaluating their theoretical results for noiseless binary classification tasks.  The mixed results obtained in MTL motivate to study models that are more theoretically grounded while avoiding being trapped into prior assumptions and constrained problems. In this work, we skip strictly deriving bounds for a well-defined domain and rather follow the scientific methodology that inspired Johannes Kepler to be the first to correctly explain the elliptical orbit of planets amongst other riddles involving planetary motion. In his work, Kepler applied the scientific methodology of using observational data to discover functions relating variables of interest. The laws empirically obtained by Kepler were later mathematically proven by the theoretical works of Isaac Newton.  Following an approach similar to Kepler's, we will try to learn formulas from data by using a method called Symbolic Regression. The empirically obtained expressions are able to explain the generalization performance of multitask models with respect to given parameter . With that, we also expect to find a more elementary way to group tasks in multitask models and elucidate the benefits of MTL.      
"," We study and quantify the generalization patterns of multitask learning  models for sequence labeling tasks. MTL models are trained to optimize a set of related tasks jointly. Although multitask learning has achieved improved performance in some problems, there are also tasks that lose performance when trained together. These mixed results motivate us to study the factors that impact the performance of MTL models. We note that theoretical bounds and convergence rates for MTL models exist, but they rely on strong assumptions such as task relatedness and the use of balanced datasets. To remedy these limitations, we propose the creation of a task simulator and the use of Symbolic Regression to learn expressions relating model performance to possible factors of influence. For MTL, we study the model performance against the number of tasks , the number of samples per task  and the task relatedness measured by the adjusted mutual information . In our experiments, we could empirically find formulas relating model performance with factors of $$, $$,  which are equivalent to sound mathematical proofs in , and we went beyond by discovering that performance relates to a factor of $$.",27
" %1. background Inspired by the success of BERT on natural language understanding, there has been a surging research interest in developing multimodal pre-training methods for vision-and-language representation learning . When finetuned on downstream tasks, these pre-trained models have achieved state-of-the-art performance across diverse V+L tasks, such as Visual Question Answering , Visual Commonsense Reasoning , and Referring Expression Comprehension.  %2. motivation However, due to the immense capacity of large-scale pre-trained models yet limited amount of labeled data in downstream tasks, aggressive finetuning often falls into the overfitting trap. , a method to combat adversarial attacks in order to create robust neural networks, has recently shown great potential in improving the generalization ability of pre-trained language models and image classifiers. A natural question that came to our mind: can we apply similar adversarial training techniques to V+L problems to improve model performance?  % 3. our proposed method We propose Villa %, %\jj{How about Avalan? The modeling part seems a bit stretchy.}  , which advocates the use of adversarial training for V+L representation learning. As illustrated in Figure, Villa consists of two training stages:   adversarial pre-training ; followed by   adversarial fine-tuning . Intuitively, if well-designed, multimodal pre-training tasks such as image-conditioned masked language modeling and image-text matching can resonate well with many downstream tasks that require visual grounding and reasoning abilities. This leads to our hypothesis that the improved generalization ability of pre-trained models learned during APT stage can be readily transferred to the AFT stage for diverse tasks. In other words, APT is able to uniformly lift model performance for all downstream tasks in a task-agnostic way, while AFT can further enhance the finetuned models by leveraging task-specific supervision signals.  To bring in more flexibility in generating adversarial examples for robust training, we propose to perform adversarial training on the embedding level for multi-modalities, instead of operating on image pixel and sub-word token level in conventional practice. For text modality, we add adversarial perturbations to word embeddings. For image modality, most previous work observes that robustness is at odds with generalization, , trained models are able to resist adversarial attacks on clean images at the expense of performance. Distinctive from these studies, we directly add adversarial perturbations to extracted image-region features, as our end goal is the final V+L model performance rather than crafting adversarial image examples. Experiments show that this strategy leads to large performance gain on clean inputs.   Adversarial training procedure is time-consuming and computationally expensive. To power efficient large-scale training, we adopt the recently proposed ``free'' adversarial training strategy, which obtains the gradients of parameters with almost no extra cost when computing the gradients of inputs. In addition to requiring adversarial perturbations to be label-preserving, we also introduce KL-divergence-based regularization to enforce the confidence level of the prediction to be close, characterized by the ``dark'' knowledge hidden in the probability vectors. This promotes higher smoothness of the training objective and has empirically proven as important regularization effective for further performance boost.   For evaluation, we mostly focus on UNITER, the current best-performing V+L model with state-of-the-art performance across many popular V+L benchmarks, and enhance UNITER with Villa through comprehensive experiments on six V+L tasks: VQA, VCR, NLVR, Visual Entailment, Referring Expression Comprehension, and Image-Text Retrieval. Villa is a generic framework that can be applied to any multimodal pre-training method. To demonstrate its versatility, we further apply it to LXMERT on VQA, GQA, and NLVR tasks for generalizability test.  [t!]       framework for vision-and-language representation learning.}               The main contributions are summarized as follows.  We present Villa, the first known effort on adversarial pre-training and adversarial finetuning for V+L representation learning.  Instead of operating on pixel and word token level, we propose to add adversarial perturbations in the embedding space of multi-modalities, and introduce a smoothness-inducing adversarial regularization term on top of the ``free'' adversarial training strategy.  Villa achieves new state of the art across six popular V+L tasks. In particular, by relying on standard bottom-up image features only, Villa improves the single-model performance of UNITER-large from 74.02 to 74.87 on VQA, and from 62.8 to 65.7 on VCR. With ensemble, VQA performance is further boosted to 75.85.  %To the authors' best knowledge, this is the first work that studies adversarial training for V+L models.     
","   We present Villa, the first known effort on large-scale adversarial training for vision-and-language  representation learning. Villa consists of two training stages:  task-agnostic adversarial pre-training; followed by  task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the ``free'' adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply Villa to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR$^2$.\footnote{Code is available at \url{https://github.com/zhegan27/VILLA}.}",28
"  Mathematicians solve problems by relying on rules, correct derivations and proven methods of computation that are guaranteed to lead to a correct solution. Over time, they have developed a rich set of computational techniques that can be applied to many problems, and were said to be  ``unreasonably effective'' . Most of those computational techniques are not intuitive, they have to be derived from theory and applied by trained scientists or built into software libraries. They are the building blocks of every advanced computation.   Many recent studies showed that deep learning models can learn complex rules from large datasets, only from examples. In natural language processing, models learn to output grammatically correct sentences without prior knowledge of grammar and syntax , or to automatically map one language into another . In mathematics, deep learning models have been trained to perform logical inference , SAT solving , basic arithmetic  and symbolic integration .  In this paper, we investigate whether deep learning models can be trained to perform complex computations and to deduce the qualitative behavior of mathematical objects, without built-in mathematical knowledge. We consider three questions of higher mathematics: the local stability and controllability of differential systems, and the existence and behavior at infinity of solutions of partial differential equations. All three problems have been widely researched and have many applications outside of pure mathematics. They have known solutions that rely on advanced symbolic and computational techniques, from formal differentiation, Fourier transform, geometrical full-rank conditions, to function evaluation, matrix inversion, and computation of complex eigenvalues. Surprisingly, we find that neural networks can solve these problems with a very high accuracy, by simply looking at instances of problems and their solutions, while being totally unaware of the underlying theory. These results are unintuitive given the advanced numerical techniques required by the theory and the difficulty of neural networks to perform simple arithmetic tasks, suggesting that the model might be using a different approach than the known theory to correctly predict the output. %   After reviewing prior applications of deep learning to differential equations and symbolic computation, we introduce the three problems we consider, describe how we generate datasets, and detail how we train our models. Finally, we present our experiments and discuss their results.  
"," Can advanced mathematical computations be learned from examples? Using transformers over large generated datasets, we train models to learn properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect estimates of qualitative characteristics of the systems, and good approximations of numerical quantities, demonstrating that neural networks can learn advanced theorems and complex computations without built-in mathematical knowledge.",29
"  The non-local block, which models long-range dependency between pixels, has been widely used for numerous visual recognition tasks, such as object detection, semantic segmentation, and video action recognition. Towards better understanding the non-local block's efficacy, we observe that it can be viewed as a self-attention mechanism for pixel-to-pixel modeling. This self-attention is modeled as the dot-product between the features of two pixels in the embedding space. At first glance, this dot-product formulation represents  relationships. After further consideration, we find that it may encode  information as well, in the sense that a pixel may have its own independent impact on all other pixels. Based on this perspective, we split the dot-product based attention into two terms: a whitened pairwise term that accounts for the impact of one pixel { over all the pixels.  [t]             We investigate the visual properties of each term without interference from the other. Specifically, we train two individual networks, with either the whitened pairwise term or the unary term removed in the standard attention formula of the non-local block. It is found that the non-local variant using the whitened pairwise term alone generally learns within-region relationships , while the variant using the unary term alone tends to model salient boundaries . However, the two terms do not learn such clear visual clues when they are both present within a non-local block, as illustrated in the top row of Fig.. This observation is verified via statistical analysis on the whole validation set. Also, the standard non-local block combining both terms performs even worse than the variant that includes only the unary term . This indicates that coupling the two terms together may be detrimental to the learning of these visual clues, and consequently affects the learning of discriminative features.  To address this problem, we present the disentangled non-local  block, where the whitened pairwise and unary terms are cleanly decoupled by using independent Softmax functions and embedding matrices. With this disentangled design, the difficulty in joint learning of the whitened pairwise and unary terms is greatly diminished. As shown in second row of Fig., the whitened pairwise term learns clear within-region clues while the unary term learns salient boundaries, even more clearly than what is learned when each term is trained alone.  The disentangled non-local block is validated through various vision tasks. On semantic segmentation benchmarks, by replacing the standard non-local block with the proposed DNL block with all other settings unchanged, significantly greater accuracy is achieved, with a 2.0\% mIoU gain on the Cityscapes validation set, 1.3\% mIoU gain on ADE20k, and 3.4\% on PASCAL-Context using a ResNet-101 backbone. With few bells and whistles, our DNL obtains state-of-the-art performance on the challenging ADE20K dataset. Also, with a task-specific DNL block, noticeable accuracy improvements are observed on both COCO object detection and Kinetics action recognition.  
","  The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics. Code is available at \\\url{https://github.com/yinmh17/DNL-Semantic-Segmentation}, \\ \url{https://github.com/Howal/DNL-Object-Detection}.",30
" Online recruitment platforms, e.g., LinkedIn, %%ww: add Ping'an system here make it easy for companies to post jobs and for job seekers to submit resumes. In recent years, the number of both job posts and resumes submitted to online recruitment platforms is growing rapidly. For example, in U.S, there are over 3 million jobs posted on LinkedIn in every month. %% https://technode.com/2019/07/10/online-job-site-boss-zhipin-is-profitable-and-ready-for-ipo-ceo-says/ % Fig. shows the monthly job posts and resumes on our company's internal recruitment platform.  Traditionally, resumes submitted for each job are reviewed manually by the recruiter to decide whether to offer the candidates the job interview. However, manual reviewing is slow and expensive to handle the overwhelming new job posts and resumes on online platforms. It is essential to design effective algorithms to do job-resume matching automatically. This problem is called person-job fit.   Multiple approaches have been proposed for person-job fit. Earlier solutions consider person-job fit as a recommendation problem and apply collaborative filtering  algorithms. However, CF algorithms ignore the content of the job post and the resume, e.g., the working experience of the candidate and the job requirement. In contrast, when we do manual reviewing, we read the resume to understand the candidate ; we read the job post to understand the requirements; then we make a decision, i.e., whether the candidate should be offered an interview. We can see that the content of the resume and job post plays a key role in person-job fit. It is thus vital to extract effective representation of the content.   Recently, deep learning models have largely improved the performance of natural language processing tasks, including semantic matching and question answering. Deep-learning-based methods are consequently introduced for person-job fit, focusing on learning effective representations of the free text of the job post and resume. The learned representations are then compared to generate a matching score. However, they only process the text paragraphs including the working experience and job requirements, and fail to comprehend other  structured fields like the education, skills, etc. This is partly because  %these fields are short in terms of number of words, whereas  deep learning models are typically applied for natural language sentences instead of  structured fields. As a result, valuable information from these fields are left unexploited.        Moreover, most of existing deep-learning-based solutions ignore the historical applications of the candidate and the job post. It is common for a candidate to apply multiple jobs and for a job to receive multiple resumes, as shown in Figure. The numbers are derived from our experiment dataset. In specific, about 36\% have applied more than one jobs and about 88\% jobs have received more than one resumes.  The history data could provide extra information of the candidate and job. Specifically, sometimes the job description is not written carefully or comprehensively, e.g., missing some requirements or unclear preference between two skills ; sometimes the recruiter's requirement or expectation may change dynamically, e.g., increasing if the received resumes so far are of very high quality. For such cases, the history data including accepted resumes and rejected resumes of a job could help to infer the recruiter's implicit intentions not elaborated in the job description.     In addition, deep learning models are typically difficult to interpret due to complex internal transformations, although a lot of attention has been paid to this issue. As a result, deep-learning-based person-job fit solutions face the interpretation problem. In real deployment, yet, it is necessary to explain why a candidate is accepted or rejected for a given job post.   Towards these issues, in this paper, we propose a feature fusion solution. First, we propose a semantic entity extraction step to extract semantic entities, including the university and working years, from the whole resume and job post. All extracted entities are then concatenated into a vector, which captures the high-level semantics of the content and is easy to interpret. The semantic vector is further transformed through an adapted DeepFM model to learn the correlations among the entities. We also apply a convolutional neural network  over the text fields in the resume  following existing works. The outputs from DeepFM and CNN are fused via concatenation to produce a feature vector representing the explicit intention of a resume .  Second, to exploit the history information, we propose a new encoding scheme for the job-resume pair from an application. All the historical applications, including both the accepted and rejected cases, of a candidate  are processed by a LSTM model to learn the implicit intention.  Last, we do a late fusion of the representations for the explicit and implicit intentions to represent the job and candidate comprehensively.   Extensive experimental evaluations over real data show that our solution outperforms existing methods. We also conduct ablation studies to verify the effectiveness of each component of our solution. In addition, case studies are presented to demonstrate the contribution of semantic entities to model interpretation. Our solution has been deployed partially for one year. Some experience on improving the efficiency and reducing the cost will be shared at the end of this paper.  Our contributions include          
"," Person-job fit is to match candidates and job posts on online recruitment platforms using machine learning algorithms.  The effectiveness of matching algorithms heavily depends on the learned representations for the candidates and job posts.  In this paper, we propose to learn comprehensive and effective representations of the candidates and job posts via feature fusion. First, in addition to applying deep learning models for processing the free text in resumes and job posts, which is adopted by existing methods, we extract semantic entities from the whole resume  and then learn features for them. By fusing the features from the free text and the entities, we get a comprehensive representation for the information explicitly stated in the resume and job post. Second, however, some information of a candidate or a job may not be explicitly captured in the resume or job post. Nonetheless, the historical applications including accepted and rejected cases can reveal some implicit intentions of the candidates or recruiters. Therefore, we propose to learn the representations of implicit intentions by processing the historical applications using LSTM. Last, by fusing the representations for the explicit and implicit intentions, we get a more comprehensive and effective representation for person-job fit. Experiments over 10 months real data show that our solution outperforms existing methods with a large margin. Ablation studies confirm the contribution of each component of the fused representation. The extracted semantic entities help interpret the matching results during the case study.",31
"   There is a growing line of work  demonstrating that neural networks can leak information about the underlying training data in unexpected ways. In particular, many of these works show that generative sequence models, which include commonly-used language models, are prone to  rarely-occurring phrases in the data. Large-scale learning often involves training over sensitive data, and such memorization can result in blatant leaks of privacy . Thus, for any novel learning framework of interest, it is crucial to test the resilience of models trained in the framework against such memorization. Techniques to mitigate memorization must be identified to ensure the privacy of training data.  The framework of Federated Learning  has emerged as a popular approach for training neural networks on a large corpus of decentralized on-device data . FL operates in an iterative fashion: in each round, sampled client devices receive the current global model from a central server to compute an update on their locally-stored data, and the server aggregates these updates using the Federated Averaging algorithm to build a new global model. A hallmark of FL is that each participating device only sends model weights to the central server; raw data never leaves the device, remaining locally-cached. Although this, by itself, is not sufficient to provide formal privacy guarantees  for the training data, it is important to note that the canonical setting of FL does differ in many aspects from the well-studied  setting where all the data is stored at a central server. In this work, we initiate a formal study to understand the effect of the different components of FL, compared to the central learning setting, on unintended memorization in trained models.  We also investigate the effect of using a training procedure, with a formalized privacy guarantee, on such memorization. To this end, we use Differential Privacy  , which has become the standard for performing learning tasks over sensitive data. DP has been adopted by companies like Google, Apple, Microsoft, and LinkedIn, as well as the US Census Bureau. Intuitively, DP prevents an adversary from confidently making any conclusions about whether any particular user's data was to train a model, even while having access to the model and arbitrary external side information.      We build on the ``secret sharer"" framework from  that was designed to measure the unintended memorization in generative models.  At a high-level,  examples  are inserted into a training corpus, and a model trained on this corpus is then evaluated using various techniques to measure the extent to which the model has  the canaries. Since datasets in FL are inherently partitioned according to users, we adapt this framework to the FL regime by  introducing two parameters to control the presence of a  canary in such settings.  An illustration of our federated secret sharer framework is shown in Figure.  Given a canary with parameters  and , we let  be the  probability with which each user in a dataset is selected to be a secret sharer of the canary , whereas  denotes the probability with which each example in such a secret sharer's data is replaced by the canary .    Our empirical evaluations  for this paper demonstrate the following key contributions. First, we show clustering the training data according to users,  has a significant effect in reducing unintended memorization. Note that such a clustering of the data happens by design in FL settings. Next, given data clustered according to users, we show that replacing the learning optimizer from SGD to Federated Averaging provides a further reduction in such memorization. Lastly, we demonstrate that training in FL with a strong user-level DP guarantee results in models that exhibit the least amount of unintended memorization.  \mypar{Organization of the paper} We provide a formal definition of differential privacy in Section. In Section, we identify the main components separating central learning from the canonical setting of FL. Section contains the results of our empirical evaluation. We state the conclusions of this work in Section.      There is a wide variety of work demonstrating unexpeted information leakage from datasets in unexpected ways:  design a general  whereas there are other works  that design . Apart from  , other works have also studied memorization in generative text models.  The FL paradigm, which is a major focus of this work, has been used to train multiple production scale models. We refer the reader to  which provides an excellent overview of the state-of-the-art in the field, along with a suite of interesting open problems.  This work also studies the effectiveness of a user-level DP guarantee in reducing unintended memorization.  While many works on DP focus on  DP guarantees , recent works  have designed techniques tailored to user-level DP guarantees.       \usepackage{amsthm,amsmath} \usepackage{bbm} \usepackage{times} \usepackage{array,float} \usepackage{url} \usepackage{amstext,amssymb} \usepackage{hyphenat} \usepackage{verbatim} \usepackage{dsfont} \usepackage{bm} \usepackage{wrapfig} \usepackage[noend]{algorithmic} \usepackage{algorithm} \usepackage{graphicx,color} \usepackage{xparse,etoolbox} \usepackage{threeparttable} \usepackage{dsfont} \usepackage{xspace} \usepackage{subcaption} \usepackage[utf8]{inputenc} \usepackage{outlines} \usepackage{comment} \usepackage{footnote} \usepackage{multirow} \usepackage{ulem} \usepackage{cleveref} \usepackage{authblk}  {}} {}} {Theorem} {Lemma}[section] [theorem]{Remark} [1]{\|#1\|_[lemma]{Informal Theorem} [lemma]{Corollary} [lemma]{Problem} [lemma]{Definition} [lemma]{Fact} [lemma]{Assumption} [lemma]{Claim} {Input:} \renewcommand{\algorithmicensure}{Output:} [theorem]{Definition} {Corollary}[section] [1]{ #1} {} {\mathcal{M}}  {\Delta} {\mathcal{N}} {}   {} {[2]{{#2}}  [1]{\left\|#1\right\|_{{} } [1]{\left\|#1\right\|_{F}} [1]{\|#1\|_{\C}} [1]{\|#1\|_{\Q}} [1]{\|#1\|_{\Q^*}} [1]{\|#1\|_{\Q,q}} [1]{\|#1\|_{}} [1]{\|#1\|_{\C^*}} {\theta} {\text{polylog}\,} {[1]{\left\|#1\right\|_F} {{{\theta^{priv}}  \renewcommand{\paragraph}[1]{}  } } }} }  [1]{{#1}\,} {Informal Theorem}[section]    {\tilde{h}}  {\tilde{x}} {\tilde{y}} {\tilde{z}}   {\mathbb{I}}   {\mathsf{True}} {\mathsf{False}} {\mathsf{Flag}}  {\mathsf{PrvLearn}} }  }}  {\mathsf{Priv}}   {\mathsf{AUX}}     }  }  {\mathsf{left}} {\mathsf{right}}  {\gamma_{\mathsf{f}}}     \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min}   {{[2]{\underset{#1}{\mathbb{P}}\left[ #2 \right]} }\left[ #2 \right]} [2]{\underset{#1}{\mathbf{Var}}\left[ #2 \right]}  {\operatorname{\rm Bin}} {\operatorname{\rm Pois}} {} {} \def\polylog{\operatorname{polylog}} {} } } } } {}}  [1]{\|#1\|} [1]{\|#1\|_2} [1]{\|#1\|_1} [1]{\|#1\|_{{{\tilde{{\mathcal{A}}  {\tilde{\mathcal{I}}} }} }} }}    } {a_{     }       {\tilde{{\tilde{j}}  }      }  } {\mathcal{Z}_{+}} {\mathcal{Z}_{-}} {},~\frac{1}{}\}^m}   {{{{} {\mathbb{E}}    }      {\theta^\perp} {w^\dagger} {\tilde{w}} {\tilde{F}} {\tilde{f}} {\mathbb{R}} {\mathcal{R}} {\mathbb{B}}  {\operatorname{\rm Enc}} {\operatorname{\rm Dec}}  \renewcommand{\S}{\mathbb{S}} {\tilde{  {\tilde{\mathcal{U}}} {\tilde{D}} {[1]{:}}} }  {LDP} {{{\tilde J}^{\text{priv}}}} {{{J}^\#}}  \renewcommand{ [1]{{TODO: #1}}}    {\mathcal{A}}     }   {\mathsf{poly}}  {\mathsf{Median}}  {\mathsf{GenProj}}  {\mathsf{GenHist}}  {\mathsf{RndGen}}  {\mathsf{Error}}   {\mbox{on-average}}  {\mbox{FO}} {{{{{{{{   {\mathsf{Prefixes}} {\mathsf{NewPrefixes}} {\mathsf{SuccHist}} {\mathsf{FreqList}} {\mathsf{Final}} {\mathsf{FreqEst}} {{{{{\Gamma} {{{\widehat{\ds}} {{} \def\pfwe{\A_{} {{{{[2]{{{#1}^{}}} \def}  {{\widehat v}} {{\widehat u}} {joint differential privacy\,} {Joint Differential Privacy\,} {Joint differential privacy\,} {{{Y}  {{\widehat{Y}}} {\Theta^{[1]{\left\|#1\right\|_{{{\widehat{\mat{Y}}}} {1}\tag{\theequation}} {{\widehat{V}}} {{\widehat{\Pi}}} {{\Pi^{ \renewcommand\Authands{\qquad}   \makeatletter } \makeatother   \renewcommand{\paragraph}[1]{\medskip }  {List} {\mathsf{P_{error}}} {\mathsf{P_{min-error}}}  [1]{{\left}}   \title{Understanding Unintended Memorization \\ in Federated Learning} \author{Om Thakkar} \author{Swaroop Ramaswamy} \author{Rajiv Mathews} \author{Fran鑾給ise Beaufays} \affil{Google LLC,\\ Mountain View, CA, U.S.A. \\    @google.com}}                 \newpage \appendix    
"," Recent works have shown that generative sequence models  have a tendency to memorize rare or unique sequences in the training data. Since useful models are often trained on sensitive data, to ensure the privacy of the training data it is critical to identify and mitigate such  memorization. Federated Learning  has emerged as a novel framework for large-scale distributed learning tasks. However, it differs in many aspects from the well-studied  setting where all the data is stored at the central server. In this paper, we initiate a formal study to understand the effect of different components of canonical FL on unintended memorization in trained models, comparing with the central learning setting. Our results show that several differing components of FL play an important role in reducing unintended memorization. Specifically, we observe that the clustering of data according to users---which happens by design in FL---has a significant effect in reducing such memorization, and using the method of Federated Averaging for training causes a further reduction. We also show that training with a strong user-level differential privacy guarantee results in models that exhibit the least amount of unintended memorization.",32
"  Conversational search has recently attracted much attention as an emerging information retrieval  field. The ultimate goal of conversational search systems is to address user information needs through multi-turn natural language conversations. This goal is partially addressed in previous work with several simplifying assumptions. For example, the TREC Conversational Assistance Track  in 2019 has focused on multi-turn conversational search, in which users submit multiple related search queries. Similarly, conversational question answering based on a set of related questions about a given passage has been explored in the natural language processing  literature. However, the existing settings are still far from the ideal  scenario, in which both user and system can take any permitted action at any time to perform a natural conversation. In other words, most existing work in conversational search assumes that users always ask a query and the system only responds with an answer or a ranked list of documents.   Recent conversational information seeking platforms, such as Macaw, provide support for multi-turn, multi-modal, and mixed-initiative interactions. There have been recent efforts to go beyond the ``user asks, system responds'' paradigm by asking clarifying questions from the users, including offline evaluation of search clarification, clarifying question generation for open-domain search queries, and preference elicitation in conversational recommender systems. Past research in the area of search clarification has shown significant promise in asking clarifying questions. However, utilizing user responses to clarifying questions to improve the search performance has been relatively unstudied. In this paper, we propose a model that learns an accurate representation for a given user-system conversation. We focus on the conversations in which the user submits a query, and due to uncertainty about the query intent or the search quality, the system asks one or more clarifying questions to reveal the actual information need of the user. This is one of the many necessary steps that should be taken to achieve an ideal mixed-initiative conversational search system.  Motivated by previous research on improving query representation by employing other information sources, such as the top retrieved documents in pseudo-relevance feedback, we propose a neural network architecture that uses multiple information sources for learning accurate representations of user-system conversations. We extend the Transformer architecture by proposing a novel attention mechanism. In fact, the sequence transformation in Transformer networks are guided by multiple external information sources in order to learn more accurate representations. Therefore, we call our network architecture  or \model. %It learns both self-attention and cross-attention from multiple external information sources.  We train an end to end network based on the proposed architecture for two downstream target tasks: document retrieval and next clarifying question selection. In the first target task, the model takes a user-system conversation and scores documents based on their relevance to the user information need. On the other hand, the second task focuses on selecting the next clarifying question that would lead to higher search quality. For each target task, we also introduce an auxiliary task and train the model using a multi-task loss function. The auxiliary task is identifying the actual query intent description for a given user-system conversation. For text representation, our model takes advantage of BERT, a state-of-the-art text representation model based on the Transformer architecture, modified by adding a ``task embedding'' vector to the BERT input to adjust the model for the multi-task setting.   In our experiments, we use two sets of information sources, the top retrieval documents  and the pool of different clarifying questions for the submitted search query. The rational is that these sources may contain some information that helps the system better represent the user information needs. We evaluate our models using the public Qulac dataset and follow the offline evaluation methodology recently proposed by . Our experiments demonstrate that the proposed model achieves over  relative improvement in terms of MRR compared to competitive baselines, including state-of-the-art pseudo-relevance feedback models and BERT, for the document retrieval task. We similarly observe statistically significant improvements in the next clarifying question selection task compared to strong baselines, including learning to rank models that incorporate both hand-crafted and neural features, including BERT scores.   In summary, the major contributions of this work include: [leftmargin=*]       % However, the existing settings are still far away from the ideal mixed initiative scenario, where each entity of system can take any permitted action at any time.  % Information seeking conversational search is an emerging field in information retrieval. The definition of the task in the general sense is acquiring users' information needs from a search engine through a natural form of a conversation. This goal is partially addressed in the previous works with some simplification assumption. However, the existing settings are still far away from the ideal mixed initiative scenario, where each entity of system can take any permitted action at any time.  % Significant number of existing works on conversational search are in ``user asks, system responds'' setting, that is, users issue one or series of queries, and system responds the user's information need with a result list. In contrast, here we try to get one step closer to conversational search in a mixed initiative scenario, where user and system communicate in a more interactive way. We study conversational search systems in the setting where in cases that the initial query is ambiguous or faceted, the system can get back to users with a clarifying or follow up questions, rather than a merely result list, to get better understanding of what user intend is, and serves the ultimate goal, which is addressing user information need, in a more satisfying way.     % In this paper we introduce \model, which is an extension to the transformer architecture with the ability of utilizing external information. We applied our model for task of representation learning in conversation search, where the clarifying questions form the system side is also allowed , and the system is responsible to retrieve a list of passage or a document level answers from a corpus . To address this problem, we hypothesis that the conversation context itself is not enough for a sufficient history representation learning. Accordingly, we suggest, to understand a more universal representation of the conversation context, the model, also, utilize an external source information into the learning process. Generally speaking, the external source could be any source which contains related information to the initial query. In this paper, we conducted our experiments on two different external source: 1- Pool of the clarifying questions for each query, which enable the system to select the most appropriate clarifying question among the pool, from the user when it is required.  % Our model integrated the external information into the learning process with enjoying the novel \model attention mechanism. For the contextualized representation of the text, we use the state of the art model for language modeling, Bert , while we modify each transformer layer of the Bert, and embed a \model layer in it. We also add ``task embedding'' to original Bert architecture which, helps us to reuse one model for multiple task, and use the recourse in a more efficient way.   % We evaluated our model in two down-stream task: 1- re-ranking 2- Selecting the next clarifying question for the conversation. The idea is if a representation is good, it should improves the end goals. In re-ranking task, we show how a more comprehensive representation for the conversation context leads to a better result list. In the second task, selecting the next clarifying question, we show how the learned representation helps the system to take wiser choices in each turn of the conversation, and ask better clarifying questions from the user.      % All in all, our contribution in this paper can be summarized as:    % The following part of this paper is as follows: section  is about the importance of the task, and existing shortcomings, section  is about the previous works related to this task, section  describe our proposed model, section  elaborate on experimental setup, and section  conclude all of our findings.         % 
"," Asking clarifying questions in response to ambiguous or faceted queries has been recognized as a useful technique for various information retrieval systems, especially conversational search systems with limited bandwidth interfaces. Analyzing and generating clarifying questions have been studied recently but the accurate utilization of user responses to clarifying questions has been relatively less explored. In this paper, we enrich the representations learned by Transformer networks using a novel attention mechanism from external information sources that weights each term in the conversation. We evaluate this Guided Transformer model in a conversational search scenario that includes clarifying questions. In our experiments, we use two separate external sources, including the top retrieved documents and a set of different possible clarifying questions for the query. We implement the proposed representation learning model for two downstream tasks in conversational search; document retrieval and next clarifying question selection. Our experiments use a public dataset for search clarification and demonstrate significant improvements compared to competitive baselines.  % Hamed: I just edited the abstract. It's not perfect and doesn't contain some information about multi-tasking, etc. But it should be sufficient for abstract submission.  % Conversational search in information retrieval is an emerging task which recently attracted much attention. Here, we want to focus on conversational search systems in a setting that system can get back to user with a clarifying question, for faceted or ambiguous queries. The task that we want to study is learning a neural representation for an interactive conversation history between a user and a system. We hypothesis that, the conversation context, itself is not enough for learning a good history representation. Accordingly, we introduce two external resources for boosting the representation learning. To be able to utilize information of the external resource into modeling, we extended the architecture of transformer, such that, in addition to self-attention mechanism, it could apply some external attention to the input as well. We train our model in multi-task learning regime, and evaluate on two down-stream task: 1-re-ranking, and 2- selecting next clarifying question. Experiments on both tasks show significant improvement over both supervised, and non-supervised baselines.",33
"   \dropcap{O}ne of the key components of human intelligence is our ability to reason about abstract relations between stimuli. Many of the most unremarkable human activities -- scheduling a meeting, following traffic signs, assembling furniture -- require a fluency with abstraction and relational reasoning that is unmatched in nonhuman animals. An influential perspective on human uniqueness holds that relational concepts are critical to higher-order cognition . By far the most common case study of abstract relations has been equality.\footnote{We use the term ``equality'' here, though different literatures have also used ``identity.''} Equality is a valuable case study because it is simple and ubiquitous, but also completely abstract in the sense that it can be evaluated regardless of the identity of the stimuli being judged.  Equality reasoning has been studied extensively across a host of systems and tasks, with wildly variant conclusions. In some studies, equality is very challenging to learn: only great apes with either extensive language experience or specialized training succeed in matching tasks in which a  pair, AA, must be matched to a novel same pair, BB . Preschool children struggle to learn these regularities in a seemingly similar task . In contrast, other studies suggest that equality is simple: bees are able to learn abstract identity relationships from only a small set of training trials , and human infants can generalize identity patterns  and succeed in relational matching tasks . We take the central challenge of this literature to be characterizing the conditions that lead to success or failure in learning an abstract relation in a way that can be productively generalized to new stimuli.  The learning task in all of these cases can be described using the predicate  , which operates over two inputs and returns { is easy and under what conditions it is hard or unlearnable -- and how learning proceeds in these hard cases. We take the development of such an account to be our goal here.  We are inspired by an emergent perspective in the animal learning literature that suggests that the representations underlying non-human animals' and human infants' successes in equality reasoning tasks are graded . This view acknowledges the increasing evidence that other species like pigeons , crows , and baboons  can make true, out-of-sample generalizations of  and  relations, but it also recognizes that the observed patterns of behavior do not show the hallmarks of all-or-none symbolic representations. Instead, performance is graded. Out-of-sample generalization is possible but the level of performance depends critically on the diversity of the training stimuli . Success typically requires hundreds, thousands, or even tens of thousands of training trials. And the eventual outcome of learning is noisy and imperfect.  These learning signatures appear to be a close match to the kind of learning exhibited by neural network models, a flexible framework for arbitrary function learning that has enjoyed a huge resurgence of interest in recent years . Contra this proposal, however,  argued that a broad class of recurrent neural networks were unable to learn equality relations. These claims were subsequently challenged by the presentation of evidence that neural networks are able to learn  the tasks that  posed . The subsequent debate  revealed a striking lack of consensus on some of the ground rules regarding what sort of generalization would be required to show that the learned function was suitably abstract. In addition, only a narrow range of network architectures and representations was explored.     We revisit this debate here, adopting stringent criteria for generalization and considering a broader range of representations across three models. We model three cases of identity-based reasoning that have featured prominently in discussions of the role of symbols in relational reasoning :  learning to discriminate pairs of objects that exemplify the relation  or ,  learning sequences with repeated  elements , and  learning to distinguish hierarchical  and  relations in a context with pairs of pairs exemplifying these relations . This last problem is more challenging and requires vastly more data. We show how using pretraining -- an active area of development in recent artificial intelligence research  -- can help achieve far faster learning.  Across these three models, we find strong support for their ability to learn equality relations. These results should serve to revise the conclusions of the earlier debate. Marcus and colleagues  showed experimentally that neural networks using feature representations cannot generalize to binary features unseen in training. We agree with this claim . However, they concluded from this result that neural networks will need to have primitive symbolic operators to solve hard relational reasoning tasks. On this point, we disagree. Our experiments show that networks without these primitives can solve a range of these tasks, as long as they use non-featural representations. Overall, these findings suggest that essential aspects of symbolic reasoning can emerge from entirely data-driven, non-symbolic learning processes.  
","   Humans have a remarkable capacity to reason about abstract relational structures, an ability that may support some of the most impressive, human-unique cognitive feats. Because equality  is a simple and ubiquitous relational operator, equality reasoning has been a key case study for the broader question of abstract relational reasoning. This paper revisits the question of whether equality can be learned by neural networks that do not encode explicit symbolic structure. Earlier work arrived at a negative answer to this question, but that result holds only for a particular class of hand-crafted feature representations. In our experiments, we assess out-of-sample generalization of equality using both arbitrary representations and representations that have been pretrained on separate tasks to imbue them with abstract structure. In this setting, even simple neural networks are able to learn basic equality with relatively little training data. In a second case study, we show that sequential equality problems  can be solved with only positive training instances. Finally, we consider a more complex, hierarchical equality problem, but this requires vastly more data. However, using a pretrained equality network as a modular component of this larger task leads to good performance with no task-specific training. Overall, these findings indicate that neural models are able to solve equality-based reasoning tasks, suggesting that essential aspects of symbolic reasoning can emerge from data-driven, non-symbolic learning processes.",34
"  Humans can learn from captioned images because of their ability to associate words to image regions. For instance, humans perform such word-region associations while acquiring facts from news photos, making a diagnosis from MRI scans and radiologist reports, or enjoying a movie with subtitles. %Likewise, computers could better perform diagnosis, retrieval, and other tasks if they could associate words with images in medical reports, new photos, and other multimedia documents. %Humans often acquire new visual information by associating novel textual information with parts of an image. We perform such an association between text and image regions while understanding images in medical reports, news photos, movies with subtitles etc.  This word-region association problem is called word or phrase grounding and is a crucial capability needed for downstream applications like visual question answering, image captioning, and text-image retrieval.%, and is typically approached as a problem of explicit assignment or alignment between individual words and regions.  %% Captioned images, found for example in medical reports, news photos, and safety reports, can be used to learn diagnosis, retrieval, and other tasks if the words can be associated with specific image regions.  This word-region association problem is called {   %Phrase grounding\textemdash the problem of mapping noun phrases in a caption to the corresponding image regions\textemdash has vast applications in visual question answering, image captioning, image retrieval, and referential expression generation. This challenging problem not only requires a categorical understanding of objects and their attributes in the scene, but also the ability to relate phrases to detected object regions in the image.     Existing object detectors can detect and represent object regions in an image, and language models can provide contextualized representations for noun phrases in the caption. However, learning a mapping between these continuous, independently trained visual and textual representations is challenging in the absence of explicit region-word annotations. We focus on learning this mapping from weak supervision in the form of paired image-caption data without requiring laborious grounding annotations.%The current state-of-the-art approaches either require manual region-phrase correspondence in training or rely on weakly supervised discovery of such correspondence. While the former is costly and time consuming, the latter often requires heuristics and inductive bias to distinguish the corresponding image region for a caption among many other correlated candidate regions.  %without direct grounding supervision %that gracefully handle synonyms , hypernyms , polysemes , attributes , and interactions . However, learning a mapping between these continuous, independently trained visual and textual representations without direct grounding supervision continues to be a challenging open problem.   Current state-of-the-art approaches formulate weakly supervised phrase grounding as a multiple instance learning  problem. The image can be viewed as a bag of regions. For a given phrase, all images with captions containing the phrase are treated as positive bags while remaining images are treated as negatives. Models aggregate per region features or phrase scores to construct image-level predictions that can be supervised with image-level labels in the form of phrases or captions. Common aggregation approaches include max or mean pooling, noisy-OR, and attention. Popular training objectives include binary classification loss  or caption reconstruction loss  or ranking objectives .   Fig. provides an overview of our proposed contrastive training. We propose a novel formulation of the weakly supervised phrase grounding problem as that of maximizing a lower bound on mutual information between set of region features extracted from an image and contextualized %\JK{After reading Sec 2, I'm confused about ""contextualized"". See comment in Sec 2.5.}  word representations. We use pretrained region and word representations from an object detector and a language model and perform optimization over parameters of word-region attention instead of optimizing the region and word representations themselves. Intuitively, to compute mutual information with a word's representation, attention must discard nuisance regions in the word-conditional attended visual representation, thereby selecting regions that match the word. For any given word, the learned attention thus functions as a soft selection or grounding mechanism over regions.   %In this work, we propose a principled weakly supervised framework that discovers phrase grounding by maximizing the mutual information  between image and caption pairs. As computing MI is intractable, we formulate the problem as maximizing the InfoNCE lower bound on the mutual information which recently has shown promising results in representation learning for images, videos, image-and-text, and video-and-text .   Since computing MI is intractable, we maximize the recently introduced InfoNCE lower bound on mutual information. The InfoNCE bound requires a compatibility score between each caption word and the image to contrast positive image and caption word pairs with negative pairs in a minibatch. We use two objectives. % and . The first objective  contrasts a positive pair with negative pairs with the same caption word but different image regions. The second objective  contrasts a positive pair with negative pairs with the same image but different captions. We show empirically that sampling negative captions randomly from the training data to optimize  does not yield any gains over optimizing  only. Instead of random sampling, we propose to use a language model to construct context-preserving negative captions by substituting a single noun word in the caption.     We design the compatibility function using a  attention mechanism. The  and , computed from words and regions respectively, are used to compute a word-specific attention over each region which acts as a soft alignment or grounding  between words and regions. The compatibility score between regions and word is computed by comparing attended visual representation and the word representation.     %We design the compatibility function using a  attention mechanism.  vectors, generated for each word in the caption, are compared against , generated for each region, to get word-to-region attention scores. The attention scores are used as weights to linearly aggregate region  vectors. The compatibility score between the image and the word is obtained by comparing the aggregated region  vector to the word  vector.  %This imposes several challenges in our problem; Since we are interested in phrase grounding, an image is represented as a set of candidate regions generated by a detector. In this case, it is not trivial how to define a flexible and expressive compatibility function between a phrase and a set of regions. Moreover, in contrast to the previous work which uses mutual information for representation learning, we are interested in image region and phrase alignments. Thus, the compatibility function should be designed such that MI maximization naturally yields grounding. Finally, InfoNCE loss trains the compatibility function by contrasting positive pairs against negative instances. Hence, negative instance sampling plays a key role in the quality of training.  %Our model is shown in Fig.. We show that by formulating the compatibility function as an attention-based image-word matching model, we can learn to ground phrases using the InfoNCE loss. The compatibility scores between an image and a caption word are computed by comparing the contextualized word representation with a word-specific attended image representation. Intuitively, the bound on mutual information can thus be maximized when the attention model selects only the regions corresponding to the words, while discarding nuisance regions.   %To train the parameters of our attention-based model, we propose two objective functions, based on the InfoNCE loss. The first objective trains the model by contrasting positive region-phrase pairs against negative instances whose image regions are replaced . This term forces the model to discover the corresponding image region for each word. We complement this objective by defining a loss function that contrasts positive pairs against those with negative captions . However, for the latter loss function instead of blindly sampling uniformly from negative words, we propose a novel approach that extracts plausible negative words related to the original caption.  %For the second issue, we show that pre-trained language models are able to extract hard-negative words that are related to an image, but are less likely to be present in it. This technique enables us to consider more negative words in the InfoNCE loss without actually increasing the batch size blindly. For the third issue, we provide empirical evidence that maximizing the InfoNCE bound is positively correlated with the accuracy of phrase grounding, but higher InfoNCE bound does not necessarily translates to a better phrase grounding.   %region-word attention to maximize the InfoNCE lower bound on mutual information between image and caption pairs leads to phrase grounding. In our formulation, InfoNCE requires computing a compatibility score between each caption word and the image. A lower InfoNCE loss corresponds to higher scores for true image-caption pairs and lower scores for false pairs. The scores between an image and a caption word are computed by comparing the contextualized word representation with a word-specific attended image representation. The lower bound on mutual information can thus be maximized by using attention to select only the regions corresponding to the words while discarding nuisance regions.    Our key contributions are:  a novel MI based contrastive training framework for weakly supervised phrase grounding;  an InfoNCE compatibility function between a set of regions and a caption word designed for phrase grounding; %using a  attention mechanism and strong pretrained region and word representations  and  a procedure for constructing context-preserving negative captions that provides  absolute gain in grounding performance. % - A mutual information based pseudo-objective for weakly supervised phrase grounding   % - Novel captions   Our work is closely related to three active areas of research. We now provide an overview of prior arts in each. %We highlight the differences between our work and the related work in each area.  \myparagraph{Weakly Supervised Phrase Grounding.} Weakly supervised phrase localization is typically posed as a multiple instance learning  problem where each image is considered as a bag of region proposals. Images whose captions mention a word or a phrase are treated as positive bags while rest of the images are treated as negatives for that word or phrase. Features or scores for a phrase or the entire caption are aggregated across all regions to make a prediction for the image. Common methods of aggregation are max or average pooling, noisy-OR, or attention. With the ability to produce image-level scores for pairs of images and phrases or captions, the problem becomes an image-level fully-supervised phrase classification problem or an image-caption retrieval problem. An alternatives to the MIL formulations is the approach of Ye~ Recently MI-based approaches have shown promising results on a variety representation learning problems. Computing the MI between two representations is challenging as we often have access to samples but not the underlying joint distribution that generated the samples. Thus, recent efforts rely on variational estimation of MI. An overview of such estimators is discussed in  while the statistical limitations are reviewed in .  In practice, MI-based representation learning models are often trained by maximizing an estimation of MI across different transformations of data. For example, deep InfoMax maximizes MI between local and global representation using MINE. Contrastive predictive coding inspired by noise contrastive estimation assumes an order in the features extracted from an image and uses summary features to predict future features. Contrastive multiview coding maximizes MI between different color channels or data modalities while augmented multiscale Deep InfoMax and SimCLR extract views using different augmentations of data points. Since the infoNCE loss is limited by the batch size, several previous work rely on memory banks to increase the set of negative instances.  \myparagraph{Joint Image-Text Representation Learning.} With the advances in both visual analysis and natural language understanding, there has been a recent shift towards learning representation jointly from both visual and textual domains. ViLBERT and LXMERT learn representation from both modalities using two-stream transformers, applied to image and text independently. In contrast, UNITER, VisualBERT, Unicoder-VL, VL-BERT and B2T2 propose a unified single architecture that learns representation jointly from both domains. Our method is similar to the first group, but differs in its fundamental goal. Instead of focusing on learning a task-agnostic representation for a range of downstream tasks, we are interested in the quality of region-phrase grounding emerged by maximizing mutual information. Moreover, we rely on the language modality as a weak training signal for grounding, and we perform phrase-grounding without any further finetuning.    %Generic Image and text %Video and language. %VQA and captioning    
"," Phrase grounding, the problem of associating image regions to caption words, is a crucial component of vision-language tasks. We show that phrase grounding can be learned by optimizing word-region attention to maximize a lower bound on mutual information between images and caption words. Given pairs of images and captions, we maximize compatibility of the attention-weighted regions and the words in the corresponding caption, compared to non-corresponding pairs of images and captions. A key idea is to construct effective negative captions for learning through language model guided word substitutions. Training with our negatives yields a $.   %A key idea is to construct effective word substitutions using a language model, leading to much more effective learning than randomly sampling negative captions. By training either on COCO-Captions or the much smaller Flickr30K Entities train set , we achieve state-of-the-art results for weakly supervised phrase grounding on Flickr30K Entities test set.",35
" % models are big and grow Large neural networks tend to perform better than smaller ones. This observation motivated researchers to train ever larger networks, resulting in models containing billions of parameters becoming commonplace ) only defines the layer operation and dimensions. The necessary weights are disentangled from specific layers and obtained through one or more  that can use parameters as they deem fit.  Thus, SSNs can implement  network with an arbitrary number of parameters.  There are three main questions to consider when implementing SSNs: 1) How can we effectively reuse parameters? 2) Where is it most constructive to reuse parameters? 3) What do we do when a layer asks for more parameters than is available in a parameter store?  The ``how"" of reusing parameters is the problem most addressed in prior work, most notably Savarese~\etal, and we generalize and expand upon their parameter sharing methodology.  However, to the best of our knowledge, we are the first to try to investigate methods for automatically determining ``where"" parameter sharing can be performed, even when layers are different sizes/types. Instead, prior work hand selected where sharing would occur.  Similarly, to the best of our knowledge, we are the first to investigate ``what"" can be done to get more weights to implement a layer without adding more parameters (.  Each Parameter Group has its own Parameter Allocator,    \setlength         
"," Fitting a model into GPU memory during training is an increasing concern as models continue to grow.  To address this issue, we present Shapeshifter Networks , a flexible neural network framework that decouples layers from model weights, enabling us to implement any neural network with an arbitrary number of parameters.  In SSNs each layer obtains weights from a parameter store that decides where and how to allocate parameters to layers.  This can result in sharing parameters across layers even when they have different sizes or perform different operations. SSNs do not require any modifications to a model's loss function or architecture, making them easy to use.  Our approach can create parameter efficient networks by using a relatively small number of weights, or can improve a model's performance by adding additional model capacity during training without affecting the computational resources required at test time.  We evaluate SSNs using seven network architectures across diverse tasks that include image classification, bidirectional image-sentence retrieval, and phrase grounding, creating high performing models even when using as little as 1\% of the parameters.   %Our approach is based on the observation that many neural networks are severely overparameterized, resulting in significant waste in computational resources as well as being susceptible to overfitting. SSNs address this by learning where and how to share parameters between layers in a neural network while avoiding degenerate solutions that result in underfitting. Specifically, we automatically construct parameter groups that identify where parameter sharing is most beneficial.  Then, we map each group's weights to construct layers with learned combinations of candidates from a shared parameter pool.  SSNs can share parameters across layers even when they have different sizes, perform different operations, and/or operate on features from different modalities.  We evaluate our approach on a diverse set of tasks, including image classification, bidirectional image-sentence retrieval, and phrase grounding, creating high performing models even when using as little as 1\% of the parameters.  We also apply SSNs to knowledge distillation, where we obtain state-of-the-art results when combined with traditional distillation methods.  %We present Shapeshifter Networks, a flexible neural network framework that improves performance and reduces memory requirements on a diverse set of scenarios over standard neural networks.  Our approach is based on the observation that many neural networks are severely overparameterized, resulting in significant waste in computational resources as well as being susceptible to overfitting.  Our Shapeshifter Networks address this by learning where and how to share parameters between layers in a neural network while avoiding degenerate solutions that result in underfitting. Specifically, we automatically construct parameter groups that identify where parameter sharing is most beneficial.  Then, we map each group's weights to construct layers with learned combinations of candidates from a shared parameter pool.  Our Shapeshifter Networks can share parameters across layers even when they have different sizes, perform different operations, and/or operate on features from different modalities.  We show that our approach improves performance and efficiency on a diverse set of tasks, including image classification, bidirectional image-sentence retrieval, phrase grounding, and knowledge distillation, especially when few parameters are available.",36
"  When using language, humans have a remarkable ability to recombine known parts to understand novel sentences they have never encountered before . For example, once humans have learned the meanings of ``walk'', ``jump'' and ``walk twice'', it is effortless for them to understand the meaning of ``jump twice''. This kind of ability relies on the compositionality that characterizes languages. The principle of compositionality refers to the idea that the meaning of a complex expression  is determined by the meanings of its constituents  together with the way these constituents are combined  . Understanding language compositionality is a basic and essential capacity for human beings, which is argued to be one of the key skills towards human-like machine intelligence .  Recently,  made a step towards exploring and benchmarking compositional generalization of neural networks. They argued that leveraging compositional generalization was an essential ability for neural networks to understand out-of-domain sentences. The test suite, their proposed Simplified version of the CommAI Navigation  dataset, contains compositional navigation commands, such as ``walk twice'', and  corresponding action sequences, like . Such a task lies in the category of machine translation, and thus is expected to be well solved by current state-of-the-art translation models . However, experiments on SCAN demonstrated that modern translation models dramatically fail to obtain a satisfactory performance on compositional generalization. For example, although the meanings of ``walk'', ``walk twice'' and  ``jump'' have been seen, current models fail to generalize to understand ``jump twice''. Subsequent works verified that it was not an isolated case, since convolutional encoder-decoder model  and Transformer  met the same problem. There have been several attempts towards SCAN, but so far no neural based model can successfully solve all the compositional challenges on SCAN without extra resources .  In this paper, we propose a memory-augmented neural model to achieve compositional generalization by Learning Analytical Expressions . Motivated by work in cognition which argues compositionality can be captured by variable slots with symbolic functions , our memory-augmented architecture is devised to contain two cooperative neural modules accordingly: Composer and Solver. Composer aims to find structured analytical expressions from unstructured sentences, while Solver focuses on understanding these expressions with accessing Memory . These two modules are trained to learn analytical expressions together in an end-to-end manner via a hierarchical reinforcement learning algorithm . Experiments on a well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, reaching  accuracies in all tasks . As far as we know, our model is the first neural model to pass all compositional challenges addressed by previous works on SCAN without extra resources. We open-source our code at \url{https://github.com/microsoft/ContextualSP}.  
","  Compositional generalization is a basic and essential intellective capability of human beings, which allows us to recombine known parts readily. However, existing neural network based models have been proven to be extremely deficient in such a capability. Inspired by work in cognition which argues compositionality can be captured by variable slots with symbolic functions, we present a refreshing view that connects a memory-augmented neural model with analytical expressions, to achieve compositional generalization. Our model consists of two cooperative neural modules, Composer and Solver, fitting well with the cognitive argument while being able to be trained in an end-to-end manner via a hierarchical reinforcement learning algorithm. Experiments on the well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, solving all challenges addressed by previous works with $100$\% accuracies.",37
" Although infants are not supposed to acquire the symbolic representational system at the sensorimotor stage, based on Piaget's definition of infant development, the preparation of language development, such as a pre-symbolic representation for conceptualization, has been set at the time when the infant starts babbling . %in the form of conceptualizing objects and sensorimotor primitives .  Experiments have shown that infants have established the concept of animate and inanimate objects, even if they have not yet seen the objects before . Similar phenomena also include the conceptualization of object affordances such as the conceptualization of containment . This conceptualization mechanism is developed at the sensorimotor stage to represent sensorimotor primitives and other object-affordance related properties.   During an infants' development at the sensorimotor stage, one way to learn affordances is to interact with objects using tactile perception, observe the object from visual perception and thus learn the causality relation between the visual features, affordance and movements as well as to conceptualize them. This learning starts with the basic ability to move an arm towards the visual-fixated objects in new-born infants , continues through object-directed reaching at the age of 4 months , and can also be found during the object exploration of older infants . From these interactions leading to visual and tactile percepts, infants gain experience through the instantiated `bottom-up' knowledge about object affordances and sensorimotor primitives. Building on this, infants at the age of around 8-12 months gradually expand the concept of object features, affordances and the possible causal movements in the sensorimotor context . %In this period, infant gradually starts to notice the surrounding world and perceives objects' visual features . For instance, they realize that it is possible to pull a string that is tied to a toy car to fetch it instead of crawling towards it. {An associative rule has also been built that connects conceptualized visual feature inputs, object affordance and the corresponding frequent auditory inputs of words, across various contexts . At this stage, categories of object features are particularly learned in different contexts due to their affordance-invariance . }  Therefore the integrated learning process of the object's features, movements according to the affordances, and other knowledge is a globally conceptualized  process through visual and tactile perception.  This conceptualized learning is a precursor of a pre-symbolic representation of language development. {This learning is the process to form an abstract and simplified representation for information exchange and sharing\footnote{For comparison of conceptualization between engineering and language perspectives, see .}.  To conceptualize from visual perception, it usually includes a planning process: first the speaker receives and segments visual knowledge in the perceptual flow into a number of states on the basis of different criteria, then the speaker selects essential elements, such as the units to be verbalized, and last the speaker constructs certain temporal perspectives when the events have to be anchored and linked .  Assuming this planning process is distributed between ventral and dorsal streams, the conceptualization process should also emerge from the visual information that is perceived in each stream, } {associating the distributed information in both streams.  As a result, the candidate concepts of visual information are statistically associated with the input stimuli. For instance, they may represent a particular visual feature with a particular class of label  . Furthermore, the establishment of such links also strengthens the high-order associations that generate predictions and generalize to novel visual stimuli . Once the infants have learned a sufficient number of words, they begin to detect a particular conceptualized cue with a specific kind of wording. At this stage, infants begin to use their own conceptualized visual `database' of known words to identify a novel meaning class and possibly to extend their wording vocabulary .   Thus, this associative learning process enables the acquisition and the extension of the concepts of domain-specific information  with the visual stimuli. }  This conceptualization will further result in a pre-symbolic way for infants to communicate when they encounter a conceptualized object and intend to execute a correspondingly conceptualized well-practised sensorimotor action towards that object. For example, behavioral studies showed that when 8-to-11-month-old infants are unable to reach and pick up an empty cup, they may point it out to the parents and execute an arm movement intending to bring it to their lips. The conceptualized shape of a cup reminds infants of its affordance and thus they can communicate in a pre-symbolic way. Thus, the emergence from the conceptualized visual stimuli to the pre-symbolic communication also gives  further rise to the different periods of learning nouns and verbs in infancy development .  This evidence supports that the production of verbs and nouns are not correlated to the same modality in sensory perception: experiments performed by  suggest that nouns are more related to the movement orientation caused by the intrinsic properties of an object, while verbs are more related to the trajectories of an object. Thus we argue that such differences of acquisitions in lexical classes also relate to the conceptualized visual ventral and dorsal streams. The finding is consistent with's hypothesis that verb generation is modulated by the perception of conceptualization of movement and its spatio-temporal relationship.   For this reason, we propose that the conceptualized visual information, which is a prerequisite for the pre-symbolic communication, is also modulated by perception in two visual streams.  {Although there  have been studies of modeling the functional modularity in the development of ventral and dorsal streams , the bilinear models of visual routing , in which a set of control neurons dynamically modifies the weights of the `what' pathway on a short time scale, or transform-invariance models  by encouraging the neurons to fire invariantly while transformations are performed in their input stimuli. However, a model that explains the development of conceptualization from both streams and results in an explicit representation of conceptualization of both streams while the visual stimuli is presented is still missing in the literature. } This conceptualization should be able to encode the same category for information flows in both ventral and dorsal streams  like `object files' in the visual understanding  so that they could be discriminated in different contexts during language development.  On the other hand, this conceptualized representation that is distributed in two visual streams is also able to predict the tendency of appearance of an action-oriented object in the visual field, which causes some sensorimotor phenomena such as object permanence  showing the infants' attention usually is driven by the object's features and movements. For instance, when infants are observing the movement of the object, recording showed an increase of the looking times when the visual information after occlusion is violated in either surface features or location . Also the words and sounds play a top-down role in the early infants' visual attention . This could hint at the different development stages of the ventral and dorsal streams and their effect on the conceptualized prediction mechanism in the infant's consciousness.{ {Accordingly, the model we propose about the conceptualized visual information should also be able to explain the emergence of a predictive function in the sensorimotor system, e.g. the ventral stream attempts to track the object and the dorsal stream processes and predicts the object's spatial location, when the sensorimotor system is involved in an object interaction. } We have been aware of that this build-in predictive function in a forward sensorimotor system is essential: neuroimaging research has revealed the existence of internal forward models in the parietal lobe and the cerebellum that predict sensory consequences from efference copies of motor commands  and supports fast motor reactions .  Since the probable position and the movement pattern of the action should be predicted on a short time scale, sensory feedback produced by a forward model with negligible delay is necessary in this sensorimotor loop.  %Based on this finding, some model architectures have been proposed based on different combinations of forward models and other models .   %Among them, based on the theory of sensorimotor contingency  which assumes that a motor action of an agent is generated from the sensory awareness in the percept by a pre-learned mapping, some sensorimotor integration models simply aim at finding out how to emerge the pair-up of the sensorimotor integration. For instance, MLP   and SOM    are applicable methods to learn this relation. However, in terms of inclusion of forward models, all of these models assume a constant delay-time in the sensorimotor system. Thus the predictive mechanism , if necessary, is implicitly encoded into the weights of these SMC pair-up models.   %We also argue that, although the explicit representation of the sensory prediction may not increase the action performance or speed up the training, the inclusion of such  predictive information is akin to the biological separation of the forward model and the motor action model. An example is the song syllable learning which consists of an associational function with prediction of the feedback signal from audition. This forward model is learned and generated in the song system nucleus HVc . Similar predictive mechanisms also exist in the visual system to extrapolate from motor action control within the sensorimotor loop , which may also be supported by the V1 complex cells of the dorsal stream .    %Though the development sequence of two visual streams is still under debate, from the assertion of the joint development of two visual streams , the conceptualized encoding of the visual-for-action information in the dorsal stream and that of action-oriented-object information in the ventral stream may also assist in the integrated prediction consisting both visual object movement and features in the sensorimotor context. .  %This behaviour could account for the ability in this stage of sharing conceptualized experiences in the same category towards a specific property of the percept, such as colors, shapes, in a pre-symbolic way.   % The similar object-and-infant interaction also happens without object and its affordance, but only sensorimotor interaction with mother and infant  as well. To conclude these cases,  when the dynamic interactive process between object and the other person are being observed or executed, the emergence of the pre-symbolic representation happens simultaneously in the sensorimotor learning progress, a product of the conceptualization in linguistics.    %The concepturlization may also assists to develop new primitive knowledge with the initially sensed from percept or framed by trial-and-error actions  .   %The infants' conceptualization that results in the continuum of language development of pre-symbolic intentional communication  to the beginning of symbolic communication should be partly devoted by the infants' visual percept.   %One explanation of this anticipation is that it stem from conceptualization of different visual feature extraction, movement pattern extraction, etc .    %The fully development of both streams happens when the infant acquires motor ability in the sensorimotor stage. This is not a co-incident, but causal: the infant firstly obtain the ability to reach the object at the age of 5-month when the ventral stream has developed and she is able to recognize an object with its features. At the age of 6-8 month, more mature sensorimotor abilities, such as exploring the objects and adjusting the hand movement for grasping,  become possible, which is also accompanied with the incremental development of dorsal streams and its provision of vision-for-action function . Thus, these actions towards an object oriented task may indicate the mature development of dorsal stream and its improvement in the oriented actions of geometric properties of objects. %One may connect the conceptualization in the sensorimotor stage with the information of object identity and object movement in order to accomplish the infants' behavior such as object permanence. This pre-symbol representation of a certain object of interest further assists the infant interaction with sensorimotor behaviors.   %After this stage, the emergence of sensorimotor primitives and the infants can even anticipate the upcoming sensorimotor primitives with the emergence of dorsal stream in the later sensorimotor stage.    %The emergence of conceptualization on the perception of object affordance, the visual features and their possible movement in response to some motor actions  plays an essential cue for the object predictions in sensorimotor integration, such as object permanence.         % Regarding the predictive capabilities of the sensorimotor learning, a few computational models have been proposed.  %In terms of the internal models of sensorimotor learning, for instance, they attempt to figure out how the sensorimotor coupling or integration happens between motor commands and the set of sensory inputs.    % %However, disputation still exists in terms of which role of sensory  feedback plays in the sensorimotor integration.    % These loops rely on a forward model that integrates the sensory inflow and motor outflow to evaluate the consequence of the motor commands sent to a limb, such as the arm. In such a model, the probable position and velocity of an effector can be estimated with negligible delays and even predicted in advance, thus making feedback strategies possible for fast reaching movements. The parietal lobe and cerebellum appear to play a crucial role in this process. T  %Rather, reaching towards a target requires an integrative control scheme in which feedforward specification of the motor command, forward modeling of the dynamics of the arm and online updating of the initial pattern of muscle activation are synthesized in reliable feedback loops, which are thought to involve the cerebellum and PPC.   %Based on this concept of visual prediction, a predictive sensorimotor embodied architecture  was proposed. A RNN sensory prediction and reward-driven motor accomplish the tasks of visual cue prediction and a reward-driven motor action learning. However, since only one simple recurrent network is applied for visual extrapolation, a single set of spatio-temporal sequences in the sensory data space  can be learnt.  % Particularly, the predictive sensorimotor model we propose is  suitable to work as one of the building modules that takes into account the predictive object movement in a forward sensorimotor system to deal with object interaction from visual stimuli input as Fig. shows. This system is similar to 's sensorimotor integration, but it includes an additional sensory estimator  which takes into account the visual stimuli from the object so that it is able to predict the dynamics of both the end-effector  and the sensory input of the object. This object-predictive module is essential in a sensorimotor system to generate sensorimotor actions like tracking and avoiding when dealing with fast-moving objects, e.g. in ball sports. {We also assert that the additional inclusion of forward models in the visual perception of the objects can explain some predictive developmental sensorimotor phenomena, such as object permanence. % {  Therefore, the model we propose basically focuses on a predictive perception function based on contextual visual stimuli of the object by unfolding the higher-level abstract perception files, i.e. the PB units.  We assert that the additional inclusion of forward models in the visual perception for the visual objects can explain some predictive developmental sensorimotor phenomena, such as object permanence.}       %As shown in Fig., the desired position of the end-effector serves as reference sensory input. This is then applied as an input to the inverse model  to generate the necessary motor command. The efferent copy of this motor command is fed as the input to the end-effector forward model  which predicts the future position of the end-effector .  %At the same time, using the visual input, the object forward model  observes the object in the receptive field and predicts the possible movement from its innate model, which can be formulated as an HMM model. In this way, the comparison of the three values  results in the desired position of the end-effector for the next time-step.  In this paper, we will concentrate on the modeling for the object forward model , with emphasis on the built-in function of conceptualization of the visual ventral and dorsal streams.   In summary, we propose a model that establishes links between the development of ventral/dorsal visual streams and the emergence of the conceptualization in visual streams, which further leads to the predictive function of a sensorimotor system. To validate this proof-of-concept model, we also conducted experiments in a simplified robotics scenario. Two NAO robots were employed in the experiments: one of them was used as a `presenter' and moved its arm along pre-programmed trajectories as motion primitives. A ball was attached at the end of the arm so that another robot could obtain the movement by tracking the ball. Our neural network was trained and run on the other NAO, which was called the `observer'.  In this way, the observer robot perceived the object movement from its vision passively, so that its network took the object's visual features and the movements into account.  Though we could also use one robot and a human presenter to run the same tasks, we used two identical robots, due to the following reasons: 1. the object movement trajectories can be done by a pre-programmed machinery so that the types and parameters of it can be adjusted; 2. the use of two identical robots allows to interchange the roles of the presenter and observer in an easier manner.  {As other humanoid robots, a sensorimotor cycle that is composed of cameras and motors also exists in NAO robots.  Although its physical configurations and parameters of sensory and motor systems are different from those in human beings' or other biological systems,  our model only handles the pre-processed information extracted from visual stimuli. Therefore it is sufficient to serve as a neural model that is running in a robot CPU to explain the language development in the cortical areas. }       % 
"," The acquisition of symbolic and linguistic representations of sensorimotor behavior is a cognitive process performed by an agent when it is executing and/or observing own and others' actions. According to Piaget's theory of cognitive development, these representations develop during the sensorimotor stage and the pre-operational stage.  We propose a model that relates the conceptualization of the higher-level information from visual stimuli to the development of ventral/dorsal visual streams. This model employs neural network architecture incorporating a predictive sensory module based on an RNNPB  and a horizontal product model. We exemplify this model through a robot passively observing an object to learn its features and movements. During the learning process of observing sensorimotor primitives, i.e. observing a set of trajectories of arm movements  and its oriented object features, the pre-symbolic representation is self-organized in the parametric units.  These representational units act as bifurcation parameters, guiding the robot to recognize and predict various learned sensorimotor primitives. The pre-symbolic representation also accounts for the learning of sensorimotor primitives in a latent learning context. %[original submission version]The acquisition of symbolic and linguistic representation of sensorimotor behaviour is a cognitive process obtained by an agent when it is executing and/or observing own and others' actions. According to Piaget's theory of cognitive development, these representations develop during the sensorimotor stage and the pre-operational stage. We model this process through a robot action and gesture imitation learning experiment. This pre-symbolic language acquisition capability is based on a neural network architecture incorporating a predictive sensory module based on an RNNPB  and horizontal product model. During the imitation learning process of sensormotor primitives, i.e. a set of arm movement trajectories and features of their orientated object, the pre-symbolic representation is self-organized in the parametric units. These representational units act as bifurcation parameters, guiding the robot to recognize and follow various learnt sensorimotor primitives. The pre-symbolic representation also accounts for the learning of sensorimotor primitives in a latent learning context.   \tiny    Pre-symbolic Communication, Sensorimotor Integration, Recurrent Neural Networks, Parametric Biases, Horizontal Product   %All article types: you may provide up to 8 keywords; at least 5 are mandatory.",38
" LinkedIn serves as a job marketplace that matches millions of jobs to more than 675 million members. To create economic opportunity for every member of the global workforce, LinkedIn needs to understand the job marketplace precisely. However, understanding job postings is non-trivial due to its lengthy and noisy nature. Job postings usually cover a wide range of topics ranging from company description, job qualifications, benefits to disclaimers. It is challenging to model job postings directly in tasks such as job recommendation and applicant evaluation. To address this challenge, we develop job understanding models that take noisy job postings as input and output structured data for easy interpretation. To be specific,  that represent the characteristics of a job, for example, .  Standardizing job information has a tremendous impact on the LinkedIn ecosystem. Firstly, it helps recruiters to do better candidate targeting. Using the extracted key skill entities, recruiters can target the candidates that have the right skill set. Secondly, it helps members to find jobs easily. It is convenient for members to search jobs based on the professional entities standardized from the job postings such as occupation and requirements. Lastly, it improves the overall hire efficiency. By extracting assessment questions and key skill entities from jobs, LinkedIn can automatically evaluate job-applicant fit by comparing them with member-side entities.  However, developing a good job understanding model is a challenging task in many aspects.  First, we need to define an extensive professional entity taxonomy that covers a wide range of industries and occupations. Without a comprehensive taxonomy, it is hard to represent jobs using entities. Second, we need domain-specific natural language understanding models to understand jobs. Compared to ordinary articles, job posting text is often long, noisy, and has job-specific writing styles, general-purpose Natural Language Processing  models are less suitable for this task.  Third, job understanding models need to be market-aware. To be specific, it needs to go beyond simply identifying mentioned entities and understand market importance of entities via modeling each different job market and the hiring experts in the market.  Unfortunately, existing methods haven閳ユ獩 addressed all the above challenges. Models such as SPTM and TATF perform job-skill analysis on IT skills only. DuerQuiz focuses on job content only and ignores the market dynamics. Lastly, none of these models explicitly model market variance via establishing a feedback loop between models and hiring experts.  In this work, we present LinkedIn's deep job understanding and demonstrate LinkedIn's job posting flow powered by our work.  We combined machine learning techniques and linguist experts to curate the world's largest professional entity taxonomy.  To develop domain-specific content understanding model, we used deep transfer learning to adapt open-domain NLP models and professional entity embeddings trained on LinkedIn member profiles to our domain.  We engineered market-specific features and established a feedback loop with job posters. We showed the model outputs and allowed them to override the suggestions to collect the feedback. By developing the feedback loop, we were able to collect domain-specific and market-aware training data to continuously improve our model. Moreover, such a feedback loop also empowered the job posters while giving options to them to delegate decisions to AI models. .  
"," As the world闁炽儲鐛 largest professional network, LinkedIn wants to create economic opportunity for everyone in the global workforce. One of its most critical missions is matching jobs with processionals. Improving job targeting accuracy and hire efficiency align with LinkedIn's Member First Motto. To achieve those goals, we need to understand unstructured job postings with noisy information. We applied deep transfer learning to create domain-specific job understanding models. After this, , including titles, skills, companies, and assessment questions. To continuously improve LinkedIn闁炽儲鐛 job understanding ability, we designed an expert feedback loop where we integrated job understanding models into LinkedIn闁炽儲鐛 products to collect job posters闁 feedback. In this demonstration, we present LinkedIn闁炽儲鐛 job posting flow and demonstrate how the integrated deep job understanding work improves job posters闁 satisfaction and provides significant metric lifts in LinkedIn's job recommendation system.",39
" Recent advances in  Reinforcement Learning  have been driven by the development of novel simulation environments, such as the Arcade Learning Environment ~, StarCraft~, BabyAI~, Obstacle Tower~, Minecraft~, and Procgen Benchmark~.  These environments introduced new challenges for state-of-the-art methods and demonstrated failure modes of existing RL approaches. For example,  highlighted that methods performing well on other ALE tasks were not able to successfully learn in this sparse-reward environment. This sparked a long line of research on novel methods for exploration~ and learning from demonstrations~. However, this progress has limits: the current best approach on this environment, Go-Explore~, overfits to specific properties of ALE and Montezuma's Revenge. While Go-Explore is an impressive solution for Montezuma's Revenge, it exploits the determinism of environment transitions, allowing it to memorize sequences of actions that lead to previously visited states from which the agent can continue to explore.  We are interested in surpassing the limits of deterministic or repetitive settings and seek a simulation environment that is complex and modular enough to test various open research challenges such as exploration, planning, skill acquisition, memory, and transfer. However, since state-of-the-art RL approaches still require millions or even billions of samples, simulation environments need to be fast to allow RL agents to perform many interactions per second. Among attempts to surpass the limits of deterministic or repetitive settings,  are a promising path towards testing systematic generalization of RL methods~. Here, the game state is generated programmatically in every episode, making it extremely unlikely for an agent to visit the exact state more than once during its lifetime.  Existing procedurally generated RL environments are either costly to run~ or are, as we argue, of limited complexity~.  To address these issues, we present the  , a procedurally generated environment that strikes a balance between complexity and speed. It is a fully-featured  environment~ around the popular open-source terminal-based single-player turn-based ``dungeon-crawler'' game, ~.  Aside from procedurally generated content,  is an attractive research platform as it contains hundreds of enemy and object types, it has complex and stochastic environment dynamics, and there is a clearly defined goal .  Furthermore,  is difficult to master for human players, who often rely on external knowledge to learn about strategies and NetHack's complex dynamics and secrets.\footnote{``NetHack is largely based on discovering secrets   and tricks during gameplay. It can take years for one to become   well-versed in them, and even experienced players routinely discover   new ones.'' } Thus, in addition to a guide book released with  itself, many extensive community-created documents exist, outlining various strategies for the game~.  In summary, we make the following core contributions: [label=] , a fast but complex and feature-rich Gym   environment for RL research built around the popular terminal-based   game, , , and 's symbolic observation space by presenting in-depth qualitative analyses of   trained agents.   
"," Progress in Reinforcement Learning  algorithms goes hand-in-hand with the development of challenging environments that test the limits of current methods. While existing RL environments are either sufficiently complex or based on fast simulation, they are rarely both. Here, we present the~ , a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research based on the popular single-player terminal-based roguelike game, . We argue that NetHack is sufficiently complex to drive long-term research on problems such as exploration, planning, skill acquisition, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience. We compare \NLE{} and its task suite to existing alternatives, and discuss why it is an ideal medium for testing the robustness and systematic generalization of RL agents. We demonstrate empirical success for early stages of the game using a distributed Deep RL baseline and Random Network Distillation exploration, alongside qualitative analysis of various agents trained in the environment. \NLE{} is open source and available at \url{https://github.com/facebookresearch/nle}.",40
"      proposed a dense co-attention transformer model for attention using the regional features of . This work reproduced the co-attention with transformer for better effectiveness, leveraging the ability to select the features that are much more relevant.  Also, transformer model creates attention through combining all/any features, introducing lots of irrelevancy, inconsistency, and contamination.  The irrelevancy is extracted either from the influence of image features or the natural language. What our novel transformer model introduced an extra query based on the content of the features and tries to identify them. It helps in composing another set of features that can identify the activities and their importance in an image. If we consider the feature space to be , then it is important to generate attentions to extract different combinations  that can capture useful information from the attentions. However, models are biased to capture similar features  because of the trained weights. Present research considers encoding approaches  expecting that the important features will segregate. But it is a bad strategy  to encode through recurrent network function  as  where the features overlap to generate the final features instead of intelligent combination.  is the encoded attention at time . Encoding fails to scale and generalize well. Our approach creates an extra set of functionality that can help identify the attentions or features that are more helpful.           The fundamental problem, that we are trying to solve, is to counter the over-dependency of the models to combine multiple information. There are challenges that need to be solved to counter the lack of effectiveness when dealing with multiple frames/features. In VQA, there is the lack of identification of the feature that can be helpful. Furthermore, several layers of non-linear approximation reduces the visibility of many attributes in the feature space of the objects. This creates misjudgment and biases in the network and is difficult to remove. In this work, we have proposed a self-segregation strategy that can help in better effectiveness and can reduce the model contents to a great extent. There are two kinds of strategies to identify what is useful. One is self-understanding and another is coordinated-understanding of the feature space. In visual question answering, we can identify these useful features, when attended them with more sincerity. This sincerity comes with self-segregation and we have identified this problem as very intriguing. This could reduce the feature space before the generation of the representation. This reduction in feature space can also help in better performance and reduce the burden of the weights to capture different attributes and intricacies of the image. Figure  provide a comparative overview of different segregation techniques.  Since dual attention  has been very successful in many applications for enhancing the performance, we have used both segregation strategies in such architecture to show its influence on inference.  Our architectures  have successfully outperformed many of the previous works or as good as them.  Later, we combined the segregated attention to generate the relevant tensors for the applications.     The rest of the document is arranged with  %revisit of the literature in Section ,  architecture and details of implementation in Section ,  % in Section ,  analysis of the experiments in Section  and concluding remarks in Section .   The main contributions of this work are the followings. 1) strategies of segregation of the relevant information through self-segregation and coordinated-segregation in a deep dense co-attention Transformer network is introduced 2) solved the bottleneck of biases of multimodal interaction and any/all combinations of features selected for attention  3)  our segregation model helped in multi-modular co-attention model to perform better than many of the previous works 4) it is a simultaneous dense feature selector cum attention generation approach and has been introduced for the first time in the literature.   %
", Attention mechanism has gained huge popularity due to its effectiveness in achieving high accuracy in different domains. But attention is opportunistic and is not justified by the content or usability of the content. Transformer like structure creates all/any possible attention. We define segregating strategies that can prioritize the contents for the applications for enhancement of performance. We defined two strategies: Self-Segregating Transformer  and Coordinated-Segregating Transformer  and used it to solve visual question answering application.  Self-segregation strategy for attention contributes in better understanding and filtering the information that can be most helpful for answering the question and create diversity of visual-reasoning for attention. This work can easily be used in many other applications that involve repetition and multiple frames of features and would reduce the commonality of the attentions to a great extent. Visual Question Answering  requires understanding and coordination of both images and textual interpretations. Experiments demonstrate that segregation strategies for cascaded multi-head transformer attention outperforms many previous works and achieved considerable improvement for VQA-v2 dataset benchmark.,41
"  Code-switching is the use of more than one language in a single conversation or utterance and is prevalent in multilingual communities all over the world. Automatic Speech Recognition of code-switched speech is challenging due to the lack of transcribed code-switched speech data, a larger set of words to recognize, confusions between similar-sounding words in both languages and the lack of code-switched text data for language modeling, among other reasons. However, significant progress has been made in increasing the accuracy of ASR for code-switched languages with advances in acoustic modeling, language modeling, decoding strategies and data augmentation .  Code-switched speech co-occurs with monolingual speech, and it is imperative that ASR systems that perform well on code-switched speech also perform well on monolingual speech of one or both languages that are being mixed. Research on code-switched ASR and code-switched speech and language processing in general has focused on improving the accuracy of models on code-switched test sets. There has been very little focus so far on the effect of these techniques on monolingual speech recognition.  In this paper, we investigate the effect of standard techniques such as data pooling and fine-tuning on code-switched data on both code-switched and monolingual test sets and show that while these techniques show improvements on code-switched speech, there is a loss in performance on monolingual test sets. We hypothesize that this happens due to the phenomenon of ``catastrophic forgetting"", in which the model forgets the distribution of monolingual speech in favor of code-switched speech.   We experiment with three languages - Telugu, Tamil and Gujarati and their code-switched counterparts with English. We address two situations - the first being when we have access to a monolingual model, but not monolingual training data and want to adapt the model to recognize code-switched speech without sacrificing monolingual accuracy. For this scenario, we propose using the Learning Without Forgetting  technique  to ensure that the model does not forget the distribution of monolingual speech. The second scenario we address is when we have access to monolingual data in addition to the monolingual model. We propose various fine-tuning and regularization strategies that improve the performance on both code-switched and monolingual test sets. We suggest that future papers in code-switched speech and language processing should also report results on monolingual test sets to ensure that models can generalize across both code-switched and monolingual data.  The organization of this paper is as follows: Section 2 relates our work to prior work. Section 3 describes our training and test datasets and the experimental setup followed by a discussion of the results. Section 4 concludes. % What CS is, why it is challenging for ASR systems  % How current CS ASR approaches focus on higher accuracies on a CS test set - how real-world scenarios will always have monolingual speech in addition to CS - important to do well on both. Perform better on CS while not regressing  on monolingual.  % Contributions:  % Experiments to show that pooling/fine-tuning with CS does hurt mono performance - baseline experiment numbers on 3 CS datasets  % Approaches to fine-tuning without hurting mono performance   
","  Recently, there has been significant progress made in Automatic Speech Recognition  of code-switched speech, leading to gains in accuracy on code-switched datasets in many language pairs. Code-switched speech co-occurs with monolingual speech in one or both languages being mixed. In this work, we show that fine-tuning ASR models on code-switched speech harms performance on monolingual speech. We point out the need to optimize models for code-switching while also ensuring that monolingual performance is not sacrificed. Monolingual models may be trained on thousands of hours of speech which may not be available for re-training a new model. We propose using the Learning Without Forgetting  framework for code-switched ASR when we only have access to a monolingual model and do not have the data it was trained on. We show that it is possible to train models using this framework that perform well on both code-switched and monolingual test sets. In cases where we have access to monolingual training data as well, we propose regularization strategies for fine-tuning models for code-switching without sacrificing monolingual accuracy. We report improvements in Word Error Rate  in monolingual and code-switched test sets compared to baselines that use pooled data and simple fine-tuning.",42
"  Speaker diarization is the process of partitioning audio according to the speaker identity, which is an essential step for multi-speaker audio applications such as generating written minutes of meetings . Related techniques have been evaluated in telephone conversations , meetings , and various {.  
","   Speaker diarization is an essential step for processing multi-speaker audio.   Although an end-to-end neural diarization  method achieved state-of-the-art performance, it is limited to a fixed number of speakers.   In this paper, we solve this fixed number of speaker issue by a novel speaker-wise conditional inference method based on the probabilistic chain rule. In the proposed method, each speaker's speech activity is regarded as a single random variable, and is estimated sequentially conditioned on previously estimated other speakers' speech activities.   Similar to other sequence-to-sequence models, the proposed method produces a variable number of speakers with a stop sequence condition.   We evaluated the proposed method on multi-speaker audio recordings of a variable number of speakers.   Experimental results show that the proposed method can correctly produce diarization results with a variable number of speakers and outperforms the state-of-the-art end-to-end speaker diarization methods in terms of diarization error rate.",43
"  %   %   One of the long-standing goals of speech and cognitive scientists is to develop a computational model of language acquisition . Early on in their lives, human infants learn to recognize phonemic contrasts, frequent words and other linguistic phenomena underlying the language . The computational modeling framework of generative models is well-suited for the problem of spoken language acquisition, as it relates to the classic analysis-by-synthesis theories of speech recognition . Although, generative models are theoretically elegant and informed by theories of cognition, most recent success in speech representation learning has come from self-supervised learning algorithms such as Wav2Vec , Problem Agnostic Speech Encoding  , Autoregressive Predictive Coding  , MockingJay   and Deep Audio Visual Embedding Network  . Generative models present many advantages with respect to their discriminative counterparts. They have been used for disentangled representation learning in speech . Due to the probabilistic nature of these models, they can be used for generating new data and hence, used for data augmentation  for Automatic Speech Recognition , and anomaly detection .  In this paper, we focus solely on designing a generative model for low-level linguistic representation learning from speech. We propose Convolutional Deep Markov Model , a Gaussian state-space model with non-linear emission and transition functions parametrized by deep neural networks and a Deep Convolutional inference network. The model is trained using amortized black box variational inference  . Our model is directly based on the Deep Markov Model proposed by Krishnan et. al , and draws from their general mathematical formulation for BBVI in non-linear Gaussian state-space models. When trained on a large speech dataset, ConvDMM produces features that outperform multiple self-supervised learning algorithms on downstream phone classification and recognition tasks, thus providing a viable latent variable model for extracting linguistic information from speech.  We make the following contributions: [1)]       
"," Probabilistic Latent Variable Models  provide an alternative to self-supervised learning approaches for linguistic representation learning from speech. LVMs admit an intuitive probabilistic interpretation where the latent structure shapes the information extracted from the signal. Even though LVMs have recently seen a renewed interest due to the introduction of Variational Autoencoders , their use for speech representation learning remains largely unexplored. In this work, we propose  Convolutional Deep Markov Model , a Gaussian state-space model with non-linear emission and transition functions modelled by deep neural networks. This unsupervised model is trained using black box variational inference. A deep convolutional neural network is used as an inference network for structured variational approximation. When trained on a large scale speech dataset , ConvDMM produces features that significantly outperform multiple self-supervised feature extracting methods on linear phone classification and recognition on the Wall Street Journal dataset. Furthermore, we found that ConvDMM complements self-supervised methods like Wav2Vec and PASE, improving on the results achieved with any of the methods alone. Lastly, we find that ConvDMM features enable learning better phone recognizers than any other features in an extreme low-resource regime with few labelled training examples.",44
"  UNESCO閳ユ獨 ``Atlas Of The World閳ユ獨 Languages In Danger閳 marks 43\% of the languages in the world  as endangered. It has been  argued that at the current rate of extinction, more than 90\% of the world's languages will disappear in the next hundred years. Loss of a language leads to loss of cultural identity, the loss of linguistic diversity and in general, loss of knowledge. There are many reasons for a language to become endangered as mentioned in . The steps taken for documenting endangered languages is quite painstaking. It includes codifying the rules governing the language by trained linguists at different levels such as phonetics, phonology, morphology, syntax and so on. In order to facilitate language documentation the data collected by field linguists consists of speech data, its orthographic transcription, if available, and spoken/written translation in a high resource language. For many languages no orthographic form exists. Nevertheless, it is relatively easy to provide written or spoken translations for audio sources, as speakers of a minority language are often bilingual and literate in a high-resource language . Hence, oftentimes the only textual information that is available for endangered languages is in the form of translations. This paired speech-translation data source can be exploited by machine learning algorithms to build systems of linguistic structure discovery for speech in endangered language, as shown by the excellent work presented in . In this work, we present the Contrastive Speech Translation Network , a deep learning based multi-modal framework, that learns low-level linguistic representations for speech by exploiting the paired speech-translation data source.  Our CSTNet is inspired by the multimodal Deep Audio-Visual Embedding Network  and subsequent ResDAVENet of Harwath et al., along with their more recent research on sub-word unit learning within the DAVENet and ResDAVENet models. They propose neural models of Audio-Visual grounding, where they construct neural network models that learn by associating spoken audio captions with their corresponding image. Their framework consists of an audio encoder and an image encoder, both parametrized using Deep Neural Networks. Association between the spoken audio captions and their corresponding image are learned by using a constrastive learning framework which is a triplet loss between the embeddings outputted by the audio and the image encoders. They show that by performing the speech-image retrieval task, linguistic representations emerge in the internal representation of the audio encoder. In this work, we reach similar conclusion by performing the task of speech-translation retrieval using the aforementioned contrastive learning framework. We give details about our model in Section.  As a proof of concept, we train the CSTNet on speech-translation pairs where the speech side is always English and the text translation side is either French, German, Spanish, Portugese or Italian. We obtain this paired dataset from the Multilingual Speech Translation Corpus  . In this work, we make the following contributions:   %we show that we can learn linguistic representations inside the audio data, and also that the acoustic features resulting from our model allow to get better performances than regular fBanks in basic ASR tasks.  % in the conclusion maybe : The model could also be used to significantly accelerate the language documentation process or to be able to use NLU techniques on unwritten languages.  
"," More than half of the 7,000 languages in the world are in imminent danger of going extinct. Traditional methods of documenting language proceed by collecting audio data followed by manual annotation by trained linguists at different levels of granularity. This time consuming and painstaking process could benefit from machine learning. Many endangered languages do not have any orthographic form but usually have speakers that are bi-lingual and trained in a high resource language. It is relatively easy to obtain textual translations corresponding to speech. In this work, we provide a multimodal machine learning framework for speech representation learning by exploiting the correlations between the two modalities namely speech and its corresponding text translation. Here, we construct a convolutional neural network audio encoder capable of extracting linguistic representations from speech. The audio encoder is trained to perform a speech-translation retrieval task in a contrastive learning framework. By evaluating the learned representations on a phone recognition task, we demonstrate that linguistic representations emerge in the audio encoder's internal representations as a by-product of learning to perform the retrieval task.",45
"  Deep learning has penetrated machine learning in the past years, including speech technology and language modeling in particular. Despite the success of this architectural paradigm shift, application of Neural Network Language Models  in a single decoding pass is still challenging due to their structure and computational complexity. NNLMs can still be used in ASR, when passing to the 2-pass decoding scheme: in the first pass, a small footprint generic Language Model  is used, and the output of this step is a simplified recognition network with reduced search space. On this reduced lattice, a second decoding pass is applied with the NNLM for rescoring the hypotheses obtained in the first pass. Although by splitting the decoding into two parts we can leverage knowledge of the NNLMs and demonstrate significant Word Error Rate Reduction , it also introduces considerable processing delay.  Therefore, techniques exploiting the capabilities of NNLMs in a single-pass decoding approach have received particular attention recently. A possible technique is to augment the in-domain training data with a large text corpus generated by an NNLM. Of course, there is a compromise: the augmented model is no more suitable for capturing long contexts, and lose capability to support continuous space features. So far there has been no throughout evaluation of what NNLM capabilities can be transferred by neural text based data augmentation and how these compare to traditional Back-off N-gram Language Models , especially for the morphologically rich languages. The only exception is our earlier study for Hungarian showing that by combining subword lexical modeling with text based approximation of NNLM  we can greatly improve the performance of an online ASR system.  In this paper we significantly extend our previous work:  we quantify the amount of knowledge that can be transferred from the NNLM to single pass decoding with a BNLM augmented with data generated by the NNLM;  we show that the performance of offline decoding can also be significantly improved if we apply the augmented model in the first-pass for generating the lattice;  we evaluate the impact of training corpus size on the effectiveness of the data augmentation method. Rich morphology, per se, results in extremely large vocabularies, which constitutes a challenge for language modeling. Since data sparsity problems can be often handled by estimating language models on statically derived subword units , we will also evaluate morph-based models in our experiments.  In a related work, Suzuki et al. use a domain balanced mixture of the training corpora to train a shallow RNNLM for text generation and improve speech recognition results for Japanese, Korean and English tasks. For Korean subword-based language models are also utilized, but only for text generation, since in the language model of the ASR system subwords are merged. Using subword units for language models and ASR has been mostly considered for Finnish and Estonian, which are morphologically very rich languages. In, the authors managed to outperform word-based baseline model on Finnish and Estonian conversations by training subword RNNLMs and utilizing them in the second pass to rescore ASR lattices. N-gram based approximation of RNNLM was also investigated in a recent paper, where subword and character-based models were trained for Finnish and Arabic OOV keyword search tasks. Although the interpolation of approximated RNNLM and BNLM models improved OOV retrieval the proposed system was not evaluated on in-vocabulary tokens and no Word Error Rate  was presented either.   
","  Advanced neural network models have penetrated Automatic Speech Recognition  in recent years, however, in language modeling many systems still rely on traditional Back-off N-gram Language Models  partly or entirely. The reason for this are the high cost and complexity of training and using neural language models, mostly possible by adding a second decoding pass . In our recent work we have significantly improved the online performance of a conversational speech transcription system by transferring knowledge from a Recurrent Neural Network Language Model  to the single pass BNLM with text generation based data augmentation. In the present paper we analyze the amount of transferable knowledge and demonstrate that the neural augmented LM  can help to capture almost 50\% of the knowledge of the RNNLM yet by dropping the second decoding pass and making the system real-time capable. We also systematically compare word and subword LMs and show that subword-based neural text augmentation can be especially beneficial in under-resourced conditions. In addition, we show that using the RNN-BNLM in the first pass followed by a neural second pass, offline ASR results can be even significantly improved.",46
"  Recognizing code-switched speech is challenging for Automatic Speech Recognition  systems due to the lack of large amounts of labeled code-switched speech and text data for training Acoustic and Language Models. Recently, we showed that even if there is sufficient code-switched speech data to train models, there is a loss in performance on monolingual test sets when monolingual models are trained or fine-tuned with code-switched data . Since code-switched and monolingual speech co-occur, it is imperative that models perform well on code-switched speech while not deteriorating on monolingual speech.  With this goal in mind, in  we proposed strategies for learning how to recognize code-switched speech while not forgetting monolingual speech recognition in the following scenarios:\\ Case 1: If monolingual and code-switched data are both available and a model can be retrained from scratch, regularization strategies and fine-tuning a pooled model that uses all data leads to best results across data sets.\\ Case 2: If only a monolingual model is available and a new model cannot be retrained from scratch, the Learning Without Forgetting  framework can be used to improve performance on all test sets compared to a monolingual model fine-tuned on code-switched data.   In this work, we build upon our findings for Case 1, in which we have access to both monolingual and code-switched data and can retrain a model from scratch. When we train a joint model to learn both monolingual and code-switched speech recognition tasks with task specific and shared layer parameters, the model tends to drift towards one particular task. This drift is because shared layers try to learn task specific features which is not ideal for a joint model that needs to perform well on both tasks. Hence, we need to learn task invariant or agnostic shared layer parameters which lead to task agnostic features at shared layers and discriminant features at task specific layers.  In this work, we learn task agnostic shared layer parameters by adversarial discriminative learning. We show that it is possible to improve performance by using adversarial learning over our previously proposed techniques of fine-tuning and regularization on monolingual and code-switched test sets that span three language pairs - Tamil-English, Telugu-English and Gujarati-English. In this work, we assume that there exists a classifier that will classify code-switched and monolingual utterances prior to recognition by our model, however, our technique can also be used if this assumption does not hold.  The rest of the paper is organized as follows. Section 2 relates our work to prior work. Section 3 describes our experimental setup and results. Section 4 concludes.  
","  Recognizing code-switched speech is challenging for Automatic Speech Recognition  for a variety of reasons, including the lack of code-switched training data. Recently, we showed that monolingual ASR systems fine-tuned on code-switched data deteriorate in performance on monolingual speech recognition, which is not desirable as ASR systems deployed in multilingual scenarios should recognize both monolingual and code-switched speech with high accuracy. Our experiments indicated that this loss in performance could be mitigated by using certain strategies for fine-tuning and regularization, leading to improvements in both monolingual and code-switched ASR. In this work, we present further improvements over our previous work by using domain adversarial learning to train task agnostic models. We evaluate the classification accuracy of an adversarial discriminator and show that it can learn shared layer parameters that are task agnostic. We train end-to-end ASR systems starting with a pooled model that uses monolingual and code-switched data along with the adversarial discriminator. Our proposed technique leads to reductions in Word Error Rates  in monolingual and code-switched test sets across three language pairs.",47
"  The attention-based encoder-decoder model paradigm has recently witnessed rapidly increased applications in end-to-end automatic speech recognition . It provides a generic framework for speech-to-text generation tasks, and achieves state-of-the-art performance on ASR as an alternative to CTC  models. The recent surge of end-to-end speech-to-text translation  studies is also due to the application of attention-based encoder-decoder models. And very recent works have demonstrated the possibility of combining the two related tasks, ASR and ST, under the same encoder-decoder architecture to achieve better performance. When targeting at ST only, transfer learning from ASR is helpful to warm-starting acoustic modeling  and enabling ST model training to focus more on learning language modeling and alignment .  In this paper, we study how to utilize ST to improve cross-lingual transfer learning for ASR. Transfer learning from high-resource languages is known to be an efficient way to improve end-to-end ASR for low-resource languages. Pre-trained or jointly trained encoder-decoder models, however, do not share the language modeling  for the same language, which is likely to be inefficient for distant target languages. We introduce ST as an auxiliary task to incorporate additional knowledge of the target language and enable transferring from that target language. Unlike previous ideas for leveraging translation data, our approach does not require any modification to the ASR model architecture. It leverages ST data instead of text-to-text translation data for ST training, which avoids speech-to-text modality adaption in the encoder. Moreover, we train ST with machine translation  pseudo-labels on high-resource ASR transcripts, which overcomes the shortage of real ST data and consistently brings gains to the transfer learning. MT pseudo-labeling also simplifies ST model training  and allows beam-searching diverse labels to alleviate overfitting. \documentclass[a4paper]{article}  \usepackage{INTERSPEECH2020} \usepackage{multirow} \usepackage{caption} \usepackage{subcaption} \usepackage{tikz} \usepackage{pgfplots} \usepgfplotslibrary{groupplots}  \title{Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation}  \address{Facebook AI, USA} @fb.com}     %        
"," Transfer learning from high-resource languages is known to be an efficient way to improve end-to-end automatic speech recognition  for low-resource languages. Pre-trained or jointly trained encoder-decoder models, however, do not share the language modeling  for the same language, which is likely to be inefficient for distant target languages. We introduce speech-to-text translation  as an auxiliary task to incorporate additional knowledge of the target language and enable transferring from that target language. Specifically, we first translate high-resource ASR transcripts into a target low-resource language, with which a ST model is trained. Both ST and target ASR share the same attention-based encoder-decoder architecture and vocabulary. The former task then provides a fully pre-trained model for the latter, bringing up to 24.6\% word error rate  reduction to the baseline . We show that training ST with human translations is not necessary. ST trained with machine translation  pseudo-labels brings consistent gains. It can even outperform those using human labels when transferred to target ASR by leveraging only 500K MT examples. Even with pseudo-labels from low-resource MT , ST-enhanced transfer brings up to 8.9\% WER reduction to direct transfer.",48
"      Medication names are extremely hard to pronounce for patients without a proper medical background. Thus, when interacting with Alexa on medication names, patients without this background may have many different ways to refer to a medication , Bumex , high blood pressure pill ). On the other hand, patients with medical knowledge may use abbreviations or specialized ways to refer to medication names. For example, patients may use 閳ユ笅mmune meds閳 to refer to 閳ユ笗ycophenolate mofetil hydrochloride閳 in their prescription list.   In this paper, we describe a new problem about finding the generic medication name  based on a patient's description  from a list of medications the patient is consuming. According to our internal user research, in the United States, patients with chronic diseases usually take around four to five medications daily. This problem is different from medical concept normalization  which tries to map a health-related entity mention in a free-form text to a concept in a controlled vocabulary  which is a generic concept list rather than a patient specific prescription list and is generally much longer.   We structure this  as a ranking problem. Here we rank all medications a patient is consuming based on the relationship with the patient's description and the one ranked highest will be the inference result.  We present a hard attention based entity boosted CNN architecture achieving 4\% over earlier ranking methods.   Furthermore, the mapping between SMN and DMP contains the patient's understanding of the medications, especially from the usage perspective of the medications. Using latent output from our model, we build a medication clustering system which groups together medications with similar effects and disease treatments. The output is designed to aid physicians to consider other medications as a substitution for decreasing cost as well as helping patients distinguish medications that are similar in their impression but should, in reality, be used in different conditions. Moreover, with clustering patients will have an intuitive understanding of the relationship of the medications they are consuming.  Our contributions are as follows:           
"," Recent advancements in medical entity linking have been applied in the area of scientific literature and social media data. However, with the adoption of telemedicine and conversational agents such as Alexa in healthcare settings, medical name inference has become an important task.  Medication name inference is the task of mapping user friendly medication names from a free-form text to a concept in a normalized medication list. This is challenging due to the differences in the use of medical terminology from health care professionals and user conversations coming from the lay public.  We begin with mapping descriptive medication phrases  to standard medication names . Given the prescriptions of each patient, we want to provide them with the flexibility of referring to the medication in their preferred ways. We approach this as a ranking problem which maps SMN to DMP by ordering the list of medications in the patient闁炽儲鐛 prescription list obtained from pharmacies. Furthermore, we leveraged the output of intermediate layers and performed medication clustering. We present the Medication Inference Model  achieving state-of-the-art results. By incorporating medical entities based attention, we have obtained further improvement for ranking models.",49
"} Over the past several years, large pretrained language models   have shifted the NLP modeling paradigm from approaches based on pipelines of task-specific architectures to those based on pretraining followed by fine-tuning, where a large language model discovers useful linguistic properties of syntax and semantics through massive self-supervised training, and then small amounts of task specific training data are used to fine-tune that model .  More recently, similar approaches have been explored for knowledge representation and reasoning  with researchers asking questions like `Language Models as Knowledge Bases?' . Results suggest that  the answer is a resounding `sort of' : while language models can be coerced to answer factual queries, they still lack many of the properties that knowledge bases typically have. In particular, when evaluating LM-as-KRR models there are three explanations for why a model outputs a correct answer; 1) The model has successfully performed some reasoning or generalization required to make a novel inference, 2) the dataset contains some statistical biases that the model is exploiting, or 3) the model has memorized the exact answer, potentially from pre-training data that overlaps with the test cases. % william's comment % )}}  } % haitian's comment [1]{\textcolor{red}{Pat: #1}} % pat's comment [1]{\textcolor{orange}{William: #1}} % william's comment [1]{\textcolor{olive}{Livio: #1}} % livio's comment [1]{\textcolor{gray}{}} %#1}} % gray out the text  %   \addtocounter{footnote}{-1}%   [1][1]{  \author{Pat Verga*, Haitian Sun*, Livio Baldini Soares, William W. Cohen \\     Google Research \\    @google.com} \\ %   \And %   William Cohen \\ %   Affiliation / Address line 1 \\ %   Affiliation / Address line 2 \\ %   Affiliation / Address line 3 \\ %    \\   }  \date{}      Massive language models are the core of modern NLP modeling and have been shown to encode impressive amounts of commonsense and factual information.  However, that knowledge exists only within the latent parameters of the model, inaccessible to inspection and interpretation, and even worse, factual information memorized from the training corpora is likely to become stale as the world changes.  Knowledge stored as parameters will also inevitably exhibit all of the biases inherent in the source materials. To address these problems, we develop a neural language model that includes an explicit interface between symbolically interpretable factual information and subsymbolic neural knowledge.  We show that this model dramatically improves performance on two knowledge-intensive question-answering tasks.  More interestingly, the model can be updated without re-training by manipulating its symbolic representations. In particular this model allows us to add new facts and overwrite existing ones in ways that are not possible for earlier models.        
"," Massive language models are the core of modern NLP modeling and have been shown to encode impressive amounts of commonsense and factual information.  However, that knowledge exists only within the latent parameters of the model, inaccessible to inspection and interpretation, and even worse, factual information memorized from the training corpora is likely to become stale as the world changes.  Knowledge stored as parameters will also inevitably exhibit all of the biases inherent in the source materials. To address these problems, we develop a neural language model that includes an explicit interface between symbolically interpretable factual information and subsymbolic neural knowledge.  We show that this model dramatically improves performance on two knowledge-intensive question-answering tasks.  More interestingly, the model can be updated without re-training by manipulating its symbolic representations. In particular this model allows us to add new facts and overwrite existing ones in ways that are not possible for earlier models.",50
"  This short example shows a contrived example on how to format the authors' information for IJCAI--PRICAI--20 Proceedings.  
", This short example shows a contrived example on how to format the authors' information for {.,51
" The task of spoken language understanding  aims at extracting useful information from spoken utterances. Typically, SLU can be decomposed with a two-stage method: 1) an accurate automatic speech recognition  system transcribes the input speech into texts, and then 2) language understanding techniques are applied to the transcribed texts. These two modules can be developed separately, so most prior work developed the backend language understanding systems based on manual transcripts.  Despite the simplicity of the two-stage method, prior work showed that a tighter integration between two components can lead to better performance. Researchers have extended the ASR 1-best results to n-best lists or word confusion networks in order to preserve the ambiguity of the transcripts. . Another line of research focused on using lattices produced by ASR systems. Lattices are directed acyclic graphs  that represent multiple recognition hypotheses. An example of ASR lattice is shown in Figure.  % Due to the fact that multiple hypotheses may share substructure in a lattice, lattices can encode hypotheses more efficiently compared to a n-best list.  introduced LatticeRNN, a variant of recurrent neural networks  that generalize RNNs to lattice-structured inputs in order to improve SLU.  proposed a similar idea for Chinese name entity recognition. % With the rich information from lattices,  proposed extensions to enable the transformer model to consume lattice inputs for machine translation.  proposed to adapt the transformer model originally pre-trained on written texts to consume lattices in order to improve SLU performance.  also found that utilizing lattices that represent multiple granularities of sentences can improve language modeling.    
"," Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language.  Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency. Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs.\footnote{The scource code is available at: \url{https://github.com/MiuLab/Lattice-ELMo}.}",52
"  Motivated by a monolingual speaker acquiring translation ability by referring to a bilingual dictionary, we propose a novel MT task that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize . This task is also distinct to supervised/semi-supervised MT task that mainly depends on parallel sentences .   The bilingual dictionary is often utilized as a seed in bilingual lexicon induction  that aims to induce more word pairs within the language pair . Another utilization of the bilingual dictionary is for translating low-frequency words in supervised NMT . We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI for initializing an unsupervised MT system , we use the ground-truth bilingual dictionary and apply it throughout the training process.  We propose Anchored Training  to tackle this task. Since word representations are learned over monolingual corpora without any parallel sentence supervision, the representation distances between source language and target language are often quite large, leading to significant translation difficulty. As one solution, AT selects words covered by the bilingual dictionary as anchoring points to drive the distance between the source language space and the target language space closer so that translation between the two languages becomes easier. Furthermore, we propose Bi-view AT that places anchors based on either source language view or target language view, and combines both views to enhance the translation quality.   Experiments on various language pairs show that AT performs significantly better than various baselines, including word-by-word translation through looking up the dictionary, unsupervised MT, and dictionary-supervised cross-lingual word embedding transformation to make distances between both languages closer. Bi-view AT further improves AT performance due to mutual strengthening of both views of the monolingual data. When combined with cross-lingual pretraining , Bi-view AT achieves performances comparable to traditional SMT systems trained on more than 4M parallel sentences. The main contributions of this paper are as follows:      
"," In this paper, we propose a new task of machine translation , which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training  to tackle the task. AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language. Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT. On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences{\footnote {Code is available at \url{https://github.com/mttravel/Dictionary-based-MT} } }.",53
" There are several methods in which humans and computers can converse, like speaking  and writing . At present, research in the field of NLP has advanced a lot to attain a good understanding of textual data but there are still some ways to go to properly contemplate the audio/speech data.   Word embeddings are extensively used in NLP applications since they have proven to be an extremely informative representation of the textual data. Language models like GloVe  and Word2Vec  successfully transform textual words from its raw form to semantically and syntactically correct, fixed dimensional vectors. These type of word representations for the spoken words can be widely used to process speech/audio data for tasks like Automatic Summarization {}, Machine Translation {}, Named Entity Recognition {}, Sentiment Analysis {}, Information Retrieval {}, Speech Recognition {}, Question-Answering {} etc.  % The Word2Vec model {} successfully transforms textual words from its raw form to semantically and syntactically correct, fixed dimensional vectors. Compared to text, not much research has been done in the field of audio-based modeling primarily due to the lack of large, reliable, clean, and publicly available datasets on which the spoken word language models can be trained. Also, spoken words unlike textual words have a different meaning when they are spoken in a different tone, expression, accent, etc, and incorporating them exponentially increases the difficulty of building such language models. Such models also face difficulties such as different people can have different pronunciations, tones, and pauses for the exact same words.  The proposed model, aims at generating syntactically and semantically adequate contextualized vector representation of the variable length audio files , where each file corresponds to a single spoken word in a speech and further validates the vector representations by evaluating it on three benchmark word similarity datasets . To further increase the interpretability, this paper also provides illustrations of the vector space generated by the proposed model.  
"," % A lot of work has been done recently to build text-based language models, but not much has been done to model speech/audio. In the case of text, words are represented by unique fixed-length embeddings that hold semantical and syntactic relationships between words. Such embeddings for speech/audio type data can not only lead to great advances in the speech/audio related natural language processing tasks but can also reduce the loss of information like tone, expression, accent, etc while converting speech to text in order to perform these tasks. This paper proposes a novel model architecture that produces syntactically and semantically adequate contextualized representation of varying length spoken words. The performance of the spoken word embeddings generated by the proposed model was validated by  inspecting the vector space generated, and  evaluating its performance on three benchmark datasets for measuring word similarities.  A lot of work has been done to build text-based language models for performing different NLP tasks, but not much research has been done in the case of audio-based language models. This paper proposes a Convolutional Autoencoder based neural architecture to model syntactically and semantically adequate contextualized representations of varying length spoken words. The use of such representations can not only lead to great advances in the audio-based NLP tasks but can also curtail the loss of information like tone, expression, accent, etc while converting speech to text to perform these tasks. The performance of the proposed model is validated by  examining the generated vector space, and  evaluating its performance on three benchmark datasets for measuring word similarities, against existing widely used text-based language models that are trained on the transcriptions. The proposed model was able to demonstrate its robustness when compared to the other two language-based models.",54
" The rapid emergence of the novel coronavirus without much known history has engrossed the international scientific community, resulting in an overwhelming amount of publications and data released on a daily basis. The rate of publications has far exceeded the time-consuming peer-review process, leaving many important information with little to no attention. In an attempt to absorb and utilize the unprecedented amount of COVID-19 scientific literature, prominent journals have opened publications to the public and several platforms have prompted the data science community to aid in the process. One of the notable platform has been COVID-19 Open Research Dataset  containing thousands of papers published on PubMed and multiple tasks to understand the papers. \par Recent progress on language processing has made possible the exploration of massive text corpus otherwise infeasible by manual work. Attention-based mechanisms  and pre-trianed language representations such as BERT , Open GPT-2 , XLNet , and ELMo  have achieved a great success in many language fields, including sentence prediction and text summarization. Many language models are adopting a common practice of pre-training on a huge corpus mined from the web, followed by a fine-tuning process targeted for specific tasks. Following this trend, we focus on utilizing the popular BERT architecture for text summarization task, more specifically extractive summarization, where important sentences are picked from the text verbatim. This task fits the need of the scientific community to rapidly process and extract important information from the inundating number of COVID-19 publications while adhering to their original text. \par However, as COVID-19 papers are published on a daily basis, many of them with time-sensitive or unseen content, the model also needs to train in an online fashion without experiencing catastrophic forgetting. To this end, we propose , a novel BERT architecture built on existing techniques to learn and extract summaries from a continual stream of new tasks while retaining previously learned information. Heavily inspired by , our architecture utilizes two separate BERT models with layer-wise connections and deploys an alternating training process to minimize catastrophic forgetting. It also stacks a small Transformer encoder on top for extracting summary sentences from text.  
"," The scientific community continues to publish an overwhelming amount of new research related to COVID-19 on a daily basis, leading to much literature without little to no attention. To aid the community in understanding the rapidly flowing array of COVID-19 literature, we propose a novel BERT architecture that provides a brief yet original summarization of lengthy papers. The model continually learns on new data in online fashion while minimizing catastrophic forgetting, thus fitting to the need of the community. Benchmark and manual examination of its performance show that the model provide a sound summary of new scientific literature\footnote{Our code is available at \url{https://git.io/JJJfO}}.",55
" The sustained increase in the volume of scientific publications in the past decades has made reference selection substantially more challenging, especially for inexperienced researchers or investigators who are approaching a new field. Automated citation recommendation can help ease this challenge by suggesting the most appropriate citations for a query document, e.g., a paper draft to be submitted to ICPR 2020. Most existing citation recommendation systems rank the document candidates based on their relevance to a given query, and recommend the top entries. In alternative to simple ranking, other approaches have proposed using submodular scoring functions to select the best candidates based on a trade-off between their relevance, coverage and diversity  or their information flow in a citation network . In all cases, query-based citation recommendation systems heavily rely on the effectiveness of the underlying textual representation and the scoring functions used to assess the similarity between the query and the candidates or the candidates themselves.    Textual representation have been heavily studied as inputs to natural language understanding tasks, but they also play an important role in content-based information retrieval. Tasks in both these fields rely on textual representations that can express the semantic similarity  between textual elements, viewed as sequences of words, word subunits or characters. Recently, pre-trained language models such as ELMo , GPT-2  and BERT  have proved effective as textual representations in a broad variety of tasks. These models compute contextualized embeddings for each token which can be used as inputs for task-specific neural architectures. In this paper, we show that such models can also contribute significantly to improve the accuracy of citation recommendation.  The main contributions of our work can be summarized as:            and a state-of-the-art citation recommendation approach, Citeomatic\footnote{https://github.com/allenai/citeomatic/}, on the ACL Anthology Network corpus\footnote{http://clair.eecs.umich.edu/aan/index.php}. The proposed approach has outperformed all other approaches in a range of metrics.    The rest of this paper is organized as follows: Section II presents the related work. Section III reviews feature-based document scoring and submodular selection. Section IV presents the proposed approach for deep textual representation, including the proposed fine-tuning strategies. Section V presents the experimental results, and Section VI presents the conclusion.  %%%%%%%%%%%%%%%%% 
"," With the rapid growth of the scientific literature, manually selecting appropriate citations for a paper is becoming increasingly challenging and time-consuming. While several approaches for automated citation recommendation have been proposed in the recent years, effective document representations for citation recommendation are still elusive to a large extent. For this reason, in this paper we propose a novel approach to citation recommendation which leverages a deep sequential representation of the documents  cascaded with Siamese and triplet networks in a submodular scoring function. To the best of our knowledge, this is the first approach to combine deep representations and submodular selection for a task of citation recommendation. Experiments have been carried out using a popular benchmark dataset -- the ACL Anthology Network corpus -- and evaluated against baselines and a state-of-the-art approach using metrics such as the MRR and F1@k score. The results show that the proposed approach has been able to outperform all the compared approaches in every measured metric.",56
" Humans have been communicating for thousands of years using natural languages. It is estimated that there are currently around 6,500 spoken languages around the world . As the main method for communication, automating language understanding is a fundamental concept that has been studied for many years in the literature. As a result, many tasks, shown in Table 1, have been introduced to verify and validate these studies.  Deep learning is employed to serve and craft models for these tasks as their complexity is, by an order of magnitude, out of the scope of the traditional machine learning algorithms. Training a deep learning model is not always affordable due to the huge computing resources and large datasets requirements for these models. This motivates to start exploring other directions to transfer knowledge from one deep learning model to another.  The general idea of transfer learning, formally defined in section , is to transfer parameters or knowledge from one trained model to another. Based on the availability of labeled dataset, transfer learning can be divided into transductive and inductive transfer learning . These approaches are general and can be applied to many tasks in machine learning. For instance,  surveyed the literature for the transfer learning methods followed in recommendation systems with auxiliary data. Moreover,   discussed multi tasking and transfer learning in the domain of bioinformatics using machine learning and data mining techniques. Recently transfer learning has been applied heavily in natural language processing.  discussed the evolution of transfer learning in natural language processing. They mainly focus on the most dominant approach for transfer learning which is sequential fine tuning. However, we believe that transfer learning in NLP should be studied more thoroughly with highlights to all transfer learning approaches.   In the past few years, language models have evolved and they achieved much better results compared to traditional language models. These trained language models were used to transfer knowledge to solve many natural language processing tasks.  In this survey, we highlight the latest advances, summarize them and categorize each one in the corresponding category of transfer learning. We follow a similar taxonomy developed by  and  in categorizing and analyzing the literature. Since there is a huge number of papers related to natural language processing we focused only on the recent papers with a reasonable number of citations.   \def\arraystretch{2} [htp!]  {|p{5cm}|p{10.5cm}|}                   & Description                                                                                                               \\ \hline Summarization             & The process of extracting the more important points in a set of documents and providing a shorter, more compact version      \\ \hline Question Answering and Classification       & Given a question and a certain context, we want to index the answer to the question in the given context. This is usually called abstractive question answering. On the other hand, generative question answering considers the problem as a generative task. Another related task is question classification where we classify questions semantically to different categories.                    \\ \hline Text Entailment           & Given a pair of sentences, we want to predict whether the truth of the first sentence implies the truth of the second sentence.               \\ \hline Semantic Role Labeling    & A labeling task where each word is given its semantic role in the current context.                                           \\ \hline Co-reference Resolution  & The process of collecting expressions that refer to the same object.                                                            \\ \hline Named Entity Extraction and Recognition   & The task of extracting entities  along with their labels .                                                                     \\ \hline Sentiment Analysis        & To classify sentences or paragraphs according to the sentiment, e.g., positive, negative or neutral.        \\ \hline Reading Comprehension     & A similar problem to question answering where for a given context we want to comprehend it by answering questions correctly. \\ \hline Translation               & The process of pairing each given sentence in a language  to another sentence in language  that has the same meaning.      \\ \hline Sentence Pair Classification & Classify if a given pair of sentences are semantically equivalent.\\ \hline Natural Language Understanding & Considers how effective models are on different tasks including  question answering, sentiment analysis, and textual entailment, etc. .  \\ \hline User Intent Classification & Association of text to a specific purpose or goal.\\ \hline Natural Language Inference & Determine if given a premise whether the hypothesis is entailment, contradiction, or neutral. \\ \hline Part of Speech Tagging & Label each word to its corresponding part of speech based on its meaning and context. \\ \hline  Document Grounded Dialog Response & Given web document and conversation history what is the proper response ?\\ \hline    
"," Deep learning models usually require a huge amount of data. However, these large datasets are not always attainable. This is common in many challenging NLP tasks. Consider Neural Machine Translation, for instance, where curating such large datasets may not be possible specially for low resource languages. Another limitation of deep learning models is the demand for huge computing resources. These obstacles motivate research to question the possibility of knowledge transfer using large trained models. The demand for transfer learning is increasing as many large models are emerging. In this survey, we feature the recent transfer learning advances in the field of NLP. We also provide a taxonomy for categorizing different transfer learning approaches from the literature.",57
"  Never-ending information generation and sharing on the Web provides us with abundant data, most of which constitute the unstructured text sources. To better make sense of and draw associations among those data, we, human beings, use relational facts among the subjects  in the text. For a more comprehensive understanding of specific domains such as bioinformatics, finance, social networking etc., we need computers to process those information.   It is essential to represent the information delivered by the text in machine-readable format. One way to do that is to represent entities and their relations in so called triples, which indicate unambiguous facts about entities. A triple  implies that entity  has relation  with another entity .  Knowledge graphs  such as FreeBase  and DBpedia  are examples of such representations. They are directed and labeled graph structured data which aim to express such explicit semantics and relations of entities in triple form.  Relation extraction is a sub-task of natural language processing  which aims to discover relations  between entity pairs  and  given unstructured text data. Earlier work on relation extraction from text heavily relies on kernel based and feature based methods .However, recent research studies make use of data-driven deep learning methods eliminating conventional NLP approaches for relation extraction.  explained how the conventional deep learning methods are integrated into relation extraction.  reviewed relation extraction literature focusing on distant supervision. As the number of research studies on relation extraction increases, the need of a survey on current state-of-the-art of neural relation extraction methods arises.  This work provides a comprehensive and comparative review on the research field, focusing on the challenges together with improvement ideas.  Section  explains various approaches for relation extraction. In section  neural relation extraction methods are classified in terms of data supervision and explained. Section  describes existing challenges in this field of research.  In section , commonly used datasets in model assessment are evaluated. We discuss possible future research directions and improvement ideas in section  and we conclude our survey in section .  
"," Neural relation extraction discovers semantic relations between entities from unstructured text using deep learning methods. In this study, we present a comprehensive review of methods on neural network based relation extraction. We discuss advantageous and incompetent sides of existing studies and investigate additional research directions and improvement ideas in this field.",58
" Deep neural networks  have shown promise in various tasks of natural language processing , but a DNN is usually considered as a black-box model. In recent years, explaining features encoded inside a DNN has become an emerging direction. Based on the inherent hierarchical structure of natural language, many methods use latent tree structures of language to guide the DNN to learn interpretable feature representations. However, the interpretability usually conflicts with the discrimination power. There is a considerable gap between pursuing the interpretability of features and pursuing superior performance.  Therefore, in this study, we aim to explain a trained black-box DNN in a post-hoc manner, so that the explanation of the DNN does not affect its performance. This is essentially different from previous studies of designing new network architectures or losses to learn interpretable features, ~physically embedding tree structures into a DNN.  Given a trained DNN, in this paper, we propose to analyze interactions among input words, which are used by the DNN to make a prediction. Our method generates a tree structure to objectively reflect interactions among words. Mathematically, the interaction of several words is quantified as the difference of the contribution between the case when these words  contribute jointly to the prediction and the case when each individual word contributes independently to the prediction. The interaction between words may bring either positive or negative effects on the prediction. For example, the word  and the word  in the sentence  have a strong and positive interaction to the prediction of the person's identity, because the words  and  indicate a ``novice"" jointly, rather than work individually to represent a hand with a green color.  The core challenge in this study is to guarantee the objectiveness of the explanation.  the tree needs to reflect true interactions among words without significant bias. The Shapley value is widely considered as a unique unbiased estimation of the word contribution, which satisfies four desirable properties, . Thus, we define the interaction benefit among words based on the Shapley value. Let us consider a constituent with  words.  denote numerical contributions of each word to the prediction of a DNN, respectively.  represents the numerical contribution of the entire constituent to the prediction. Hence,  measures the interaction benefit of this constituent. If , interactions among these  words have positive effects on the prediction; otherwise, negative effects. Here,  can be computed as Shapley values.    Given a trained DNN and an input sentence with  words, Figure shows the tree structure that reflects word interactions encoded inside the DNN. In the tree,  leaf nodes represent  input words. Each non-leaf node corresponds to a constituent of the input sentence. A parent node connects two child nodes with significant interaction benefits. We use the parent node to encode interactions among its child sub-constituents. More specifically, there are two types of interactions among words, ~ interactions within a constituent and  interactions between constituents.  { In the aforementioned sentence, interactions between the constituent  and its adjacent constituent  are composed of all potential interactions among all combinations of words from the two constituents, including interactions between  ,  ,  ,  ,  ,  ,  ,  ,  .   The tree selects and encodes the most salient interactions among words, in order to reveal the signal processing in a DNN. We further propose additional metrics to diagnose interactions among words, ~the quantification of interactions within a constituent, the quantification of interactions between two adjacent constituents, and ratios of interactions that are modeled and unmodeled by the tree.  Theoretically, our method can be used as a generic tool to analyze various DNNs, including the BERT, ELMo, LSTM, CNN and Transformer. Experimental results have demonstrated the effectiveness of our method.  Contributions of this paper can be summarized as follows.  We propose a method to extract and quantify interactions among words.  A tree structure is automatically generated to represent salient interactions encoded in a DNN.  We further design six metrics to analyze interactions, which provides new perspectives to understand DNNs.   
"," This paper proposes a method to disentangle and quantify interactions among words that are encoded inside a DNN for natural language processing. We construct a tree to encode salient interactions extracted by the DNN. Six metrics are proposed to analyze properties of interactions between constituents in a sentence. The interaction is defined based on Shapley values of words, which are considered as an unbiased estimation of word contributions to the network prediction. Our method is used to quantify word interactions encoded inside the BERT, ELMo, LSTM, CNN, and Transformer networks. Experimental results have provided a new perspective to understand these DNNs, and have demonstrated the effectiveness of our method.",59
"  Even though usage and popularity of Twitter have stopped rapidly growing and even dropped in recent years\footnote{https://www.statista.com/statistics/282087/number-of-monthly-active-twitter-users}, it still has a considerable amount of loyal users who keep on sharing everything from worldwide events to random personal details with their followers. We decided to focus on one of the random personal details that people share, specifically - anything to do with food consumption and related topics.   Several corpora of Latvian tweets exist in prior work, but none of them are domain-specific and have been collected over an extensive period of time. Milajevs  collected and analysed 1.4 million tweets geo-located in Riga, Latvia from April 2017 to July 2018 and 60 thousand tweets  from November 2016 to March 2017. Pinnis  collected and analysed 3.8 million tweets of Latvian politicians, companies, media, and users who interacted with these entities from August 2016 to July 2018 There are also several data sets of general sentiment-annotated tweets \footnote{https://github.com/nicemanis/LV-twitter-sentiment-corpus} amounting to 14,781 tweets in total.   In this paper, we describe the Twitter eater corpus  and analyse its contents. We also provide two sub-corpora - one consisting of question and answer tweets and one with sentiment-annotated tweets. More details can be found in Section . In Sections  and  we describe question answering and sentiment analysis experiments using our corpus. Finally, we conclude the paper in Section .   
","     We present the Latvian Twitter Eater Corpus - a set of tweets in the narrow domain related to food, drinks, eating and drinking. The corpus has been collected over time-span of over 8 years and includes over 2 million tweets entailed with additional useful data. We also separate two sub-corpora of question and answer tweets and sentiment annotated tweets. We analyse contents of the corpus and demonstrate use-cases for the sub-corpora by training domain-specific question-answering and sentiment-analysis models using data from the corpus.",60
" Simultaneous translation  is widely used in international conferences, summits and business. Different from standard neural machine translation , simultaneous NMT has a stricter requirement for latency. We cannot wait to the end of a source sentence but have to start the translation right after reading the first few words. That is, the translator is required to provide instant translation based on a partial source sentence.    Simultaneous NMT is formulated as a prefix-to-prefix problem, where a prefix refers to a sub-sequence starting from the beginning of the sentence to be translated. In simultaneous NMT, we face more uncertainty than conventional NMT, since the translation starts with a partial source sentence rather than the complete information in conventional NMT. \Waitk{} is a simple yet effective strategy in simultaneous NMT where the generated translation is  words behind the source input.  That is, rather than instant translation of each word, \waitk{} actually leverages  more future words. Obviously, a larger  can leverage more future information, and therefore results in better translation quality but at the cost of a larger latency. Thus, when used in real-world applications, we should have a relatively small  for simultaneous NMT.   While only small  values are allowed in inference, we observe that training with a larger  will lead to better accuracy for \waitk{} inference, as demonstrated in Figure, in which a wait- model is required for EnglishGerman translation. If training with , we will obtain a  BLEU score. But if we train with wait- where  is set as a larger value such as ,  or  and test with wait-, we can get better BLEU scores. Despite the mismatch between training with wait- and testing with wait-, the model can benefit from the availability of more future information. This is consistent with the observation in .  %<-- start of a the figure {r}{0.42\textwidth}   %   -3 strategy.} %   %<-- end of the figure Here, the challenge is how much future information we should use. As shown in Figure, using more future information does not monotonically improve the translation accuracy of \waitk{} inference,  mainly because that more future information results in a larger gap between training and inference. In this work, we propose a framework that can automatically determine how much future information to use in training for simultaneous NMT. Given a pre-defined  for inference, we prepare  training tasks wait- with different  values . We introduce a controller such that given a training sample, the controller can dynamically select one of these tasks so as to maximize the validation performance on \waitk, i.e., the one we are interested in. The task selection is based on the data itself and the network status of the translation model. The controller model and the translation model are jointly learned, where the learning process is formulated as a bi-level optimization problem and we design an effective algorithm to solve it. We conduct experiments on four datasets to verify the effectiveness of our method.   The remaining part is organized as follows. The related work is introduced in Section, the problem formulation and background %, including the monotonic Transformer, is introduced in Section, and our method is introduce in Section. The experiments and the analysis are in Section , and we discuss the conclusion and future work in Section .  
"," Simultaneous neural machine translation  has attracted much attention recently. In contrast to standard NMT, where the NMT system can utilize the full input sentence, simultaneous NMT is formulated as a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and more uncertainty is introduced to decoding. \Waitk~ is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. We observed that training simultaneous NMT systems with future information  generally outperforms the standard ones . Based on this observation, we propose a framework that automatically learns how much future information to use in training for simultaneous NMT. We first build a series of tasks where each one is associated with a different $k$, and then learn a model on these tasks guided by a controller. The controller is jointly trained with the translation model through bi-level optimization. We conduct experiments on four datasets to demonstrate the effectiveness of our method.",61
"   % What do you think about ``Extracting Useful Sentences from Online Reviews: a Data Management Perspective''?  % \parskip=0pt %    % Online reviews have been proven to be important assets to online platforms such as Amazon, Yelp, or Booking.com. % These reviews are serving as a rich information source for users' decision making process % ranging from purchasing electronic products, booking a romantic vacation, to important decisions like housing or career. % Oftentimes, beyond decision supporting, online platforms perform text classification % to make further use of the reviews.  is perhaps the most well-studied classification task on review text. % By classifying whether reviews are positive or negative, % we better understand the quality of a service/product or even of a specific aspect. % While much research attention is focused on sentiment classification, however, % the review text can also be mined to fulfill many other interesting purposes. % For example, from the online reviews, eBay extracts sentences to generate product description when a product does not have one, % TripAdvisor mines useful tips for future travelers and % Airbnb constructs user profiles based on reviews that the user have given.  % pipeline  % machine learning models for text classification  % One specialized model for each task  % Data preparation, model selection   Database engines nowadays store and serve a massive amount of text data. To make the best of the text data, the most commonly performed task is perhaps . The classification results allow the database engine to aggregate the unstructured text, better filter query results, or perform complex business analytic tasks. Consider the example of an online review database. By performing sentiment classification in the review text database, review aggregation platforms like Amazon, Yelp or Airbnb can better understand  the quality or customer satisfaction level of each product or service by aggregating the opinion polarity on reviews of each item. Beyond sentiment analysis, the review text can be mined to fulfill many other interesting purposes via classification. For example, based on results of text classification, eBay extracts sentences to generate product description when a product does not have one, TripAdvisor mines useful tips for future travelers and Airbnb constructs user profiles based on reviews that the user have given. As the first contribution of this paper, we conduct a comprehensive literature survey on the different text classification tasks performed on review text in Section .  The recent success of deep learning in NLP provides solutions to text classification tasks with promising results. For example, the most recent NLP model for sentiment classification  achieves an accuracy of 97.1\%, only 0.7\% away from the human performance . While there exists these highly effective models, the process of developing them is very expensive and time-consuming. To make things even worse, there is no model architecture that is ``globally optimal''. We found in the literature that researchers proposed new model architectures to achieve the best performance on each task. This means that the expensive process of model engineering is repeated . In this work, we focus primarily on two parts of the process that contributes to most of the cost:  training data preparation and  model engineering.  \yuliang{let's see whether we can get to data cleaning} \yuliang{the training data preparation part we can give credit to Snorkel.}   Both training data preparation and model engineering are non-trivial processes. Even with powerful models like TextCNN , LSTM  or BERT , training a good text classifier still requires a significant amount of training data  . Besides, this problem is getting increasingly important as merchants accumulate more reviews and engineers develop abundant applications such as stock prediction, poll analysis, and experience search.   %%  %Accompanying the success of sentiment analysis, it is a natural question to ask what else we can extract. This is especially important for review owners to make the best use of their data. We therefore summarize six types of emerging sentences and their applications, including suggestion, tip, product description, humor, argument, and spoiler. These sentences serve different purposes corresponding to the review types. Suggestion extracts advice-giving sentences. They are helpful to improve a product or service, or the experience of a potential customer. Tip describes a personal experience in short but practical texts. Tip may imply a suggestion such as paying 18\% tip when dining at a restaurant in the united states. Product description depicts the factual details such as specifications about a product. Humors are sentences written in a creative way that read funny and vivid. Argument focuses on evidential comments that support or oppose a proposition. Spoiler refers to plot-revealing remarks that are common in the reviews of media products. Many of the six applications have lead to commercial products, such as TripAdvisor travel tip, Yelp funny reviews, eBay new item description, Yahoo short answers to ``how-to'' queries and Airbnb user profiling by aspect describing comments.  % A lot of applications for processing text rely on semantic tags % that are annotated on the text.  %\textcolor{purple}{ A lot of applications for processing text rely on tagging words, phrases or sentences with semantically informative tags. %} \jinfeng{Origin: ``A lot of applications for processing text rely on semantic tags that are annotated on the text.''} Sentiment analysis, for example, annotates sentences or phrases with a sentiment tag that indicates whether the sentence has a positive or negative sentiment. These sentiment tags are exploited by downstream applications to determine appropriate actions. Another example is entity tagging, which determines if a span in the text refers to a real-world object.  %\textcolor{purple}{ Generally speaking, the task of annotating text with semantic tags can be referred to as the semantic tagging problem. %} \jinfeng{Origin: ``We refer to the problem of annotating text with semantic tags as the {{ In this paper, we focus on short text, which can be a sentence, a paragraph, or a passage. We also refer short text loosely as sentence. %} \jinfeng{Origin: ""In this paper, we refer a sentence, a paragraph, or a passage loosely as a ``sentence''''}. %\xiaolan{Add example & citations.} \jinfeng{Added One example in the next sentence}  %A great success of semantic tagging is sentiment %analysis, %where practitioners develop a lot of applications to predict whether a %sentence conveys a tag of positive or negative opinions. In addition %to the applications using sentiment analysis, many applications in %machine learning and artificial intelligence are also empowered by %semantic tagging. To completely understand why people are interested %in semantic tagging, we survey the applications adopting semantic tagging and learn how semantic tagging works for them.  %Since these novel applications are attracting increasing attention, we briefly discussed them in this paper. We surveyed the practices emerging in recent three years and classified them into five groups according to the tags that they focus on, including Tip, Product Description, Humor, Argument, and Spoiler. These classifications will give ideas to practitioners on what new applications they can develop.    %To solve semantic tagging, a solution should be able to classify sentences into two groups according to their relevance to the targeted tag. There are two types of methods, supervised learning and rule programming. Recent solutions generally leverage supervised learning, as it automatically learns how to separate sentences from a set of human labels. As the other option, rule programming is less used due to its significant programming effort. It is very difficulty and sometimes impossible to program the full set of rules for the classification. There will be numerous rules that a sentence can semantically convey the targeted tag. Herein we focus on discussing supervised methods.    %To solve semantic tagging, a solution should be able to classify sentences into two groups according to their relevance to the targeted tag. Recent solutions generally leverage supervised learning, as it automatically learns how to separate sentences from a set of human labels. Supervised learning offers at least two favorable advantages. First, the input of an algorithm is a set of labels that can be obtained from non-programmer experts. Second, these algorithms are programming free as it is very difficult to polish a combination of rules that can reproduce human labels.  It is very difficulty and sometimes impossible to program the full set of rules for the classification. There will be numerous rules that a sentence can semantically convey the targeted tag. Herein we focus on discussing supervised methods.  %\wctan{changed ``sentence tagging'' to ``semantic tagging''}\jinfeng{I like ``semantic tagging'' and have used it throughout the paper}  %A solution for semantic tagging should be able to classify %sentences into two groups according to their relevance to the %semantic tag.  There are two types of methods for semantic tagging: rule programming and supervised learning. Rule programming-based methods require an expert to specify rules for semantic tagging. This is often error-prone and requires significant programming effort.  % since it is difficult to specify a good set of rules for semantic tagging in general.  In contrast, supervised learning models do not require much programming effort. However, training these models requires labeled data but can typically produce models with good semantic tagging results. %\wctan{do we have evidence of less used or frequently used? I removed the mentions of these claims.} %\wctan{rewrote. pls read} \jinfeng{It reads great!}   Our focus in this paper is on supervised learning models. Deep learning models  have become popular methods for semantic tagging today. One reason why deep models are popular for semantic tagging is that they are often more capable of learning complicated functions than other kinds of models. Another reason is that the superiority of deep models has been reported by many publications. For example, deep models achieve good prediction quality that is close to the human prediction on GLUE SST-2 sentiment classification task. Some recent studies made comparisons between deep models and simple models  to understand whether deep models are always superior to simple models.  %\wctan{what tasks are compared in these studies?}\jinfeng{other semantic tagging tasks. Added one sentence below.}  They conducted comparisons on various tagging tasks such as suggestion mining or humor detection. Their results reveal marginal or sometimes no improvements of deep models over simple models.   It is therefore natural to ask whether deep models are better than simple models when developing solutions for semantic tagging.  %\textcolor{purple}{ Semantic tagging forms the core of many tasks including sentiment classification, suggestion mining, and humor detection. Existing studies, however, compare deep and simple models only on individual tasks. Furthermore, they do not provide insights on how dataset characteristics affect the performances of different models. Consequently, it is hard to generalize their model selection criteria to new tasks or new datasets for the same task. Hence, given a new dataset, it is still unclear whether selecting a deep model will bring the best tagging performance. %} % The goal is to identify the model which achieves best performance for the targeted task. These studies do not consider dataset characteristics when selecting the best model. As a consequence, no previous studies show how to generalize the model selection to new tasks and even new datasets of the same task. They developed selection guidance at the granularity of individual task but not at the granularity of individual dataset characteristic. Given a new dataset, it is still unclear whether selecting deep models can bring the best semantic tagging performance.} %\jinfeng{add one paragraph to motivate 21 datasets and our study}  % select five models and 21 different datasets In this paper, we embark on a systematic study to understand the performance tradeoffs of deep models vs. simple models for semantic tagging. Towards this goal, we selected 3 representative deep models: CNN, LSTM, and BERT and 2 representative simple models: LR and SVM. CNN and LSTM are well-known methods that have been widely used in both the academic and industry communities and more recently, BERT.  %represents the most up-to-date method that w%on the best paper award in NAACL 2019.  To make a meaningful comparison and systematic study, we collected 21 real datasets that are frequently used in semantic tagging. These datasets exhibit several prominent data characteristics, including  a variable number of labels ;  a wide range of tag-conveying label ratio ;  different label cleanliness .  %We evaluated the tagging quality of CNN, LSTM, BERT, LR, and SVM on the 21 datasets. We used F1 as the quality measurement. Overall, only BERT outperforms LR and SVM on most of the datasets, while there is no clear winner between CNN/LSTM and LR/SVM. However, BERT does not outperform LR/SVM on large datasets that have more than 100,000 labels. BERT achieves the same F1 on one large dataset and worse F1 on two large datasets. The maximum F1 gap of BERT on large datasets is 0.03 yet the training takes days. In other words, deep models may not be better choices than simple models on large datasets, if the training time is considered as another evaluation criterion.   %We conducted further analyses to measure the effects of size, label skewness, and cleanliness on the tagging quality. We found that these three dataset characteristics have significant influences on the superiority of deep models. When a dataset has abundant labels, exhibits a high ratio of positive instances, or contains many dirty labels, deep models show less F1 improvements. Therefore, practitioners should pay attention to dataset characteristics if they expect better tagging quality when using deep models.  %We further found the dataset characteristics not only affect the superiority of deep models, but also regulate the final tagging quality. To help practitioners select appropriate models and set expectation on the tagging performance for their dataset, we prepare a comprehensive heap map that shows the characteristics of the 21 datasets and their tagging F1's with BERT and SVM. By using this heap map, practitioners can estimate the tagging quality gain of adopting deep models. Meanwhile, they can try to improve the dataset characteristics to push the tagging quality to higher upper limits.  \\  We evaluate the quality of semantic tagging on five selected models on 21 datasets and we obtain a rather surprising finding. %that overturns the general perception %towards deep models.  We find that deep models and simple models are complementary to each other on the task of semantic tagging. Specifically, deep models perform significantly better on smaller datasets, while simple models can be trained more efficiently on larger datasets and achieve similar semantic tagging quality. Therefore, one should select deep models or simple models for semantic tagging based on the actual dataset characteristics and requirements on efficiency. %On large datasets that contain more than 100,000 labels, deep models only achieve a maximal 0.03 F1 superiority yet takes days for training. If training time is a concern, deep models are not better choices than simple models for large datasets.  %Deep models can offer better tagging quality, but they alone are inadequate to guarantee satisfactory tagging quality. In our experiments, the tagging F1 of the best deep model varies from 0.96 to 0.15. This is because the tagging performance is at the same time regulated by the dataset characteristics, including the number of labels, the ratio of tag-conveying labels, and the label cleanliness. We conducted further experiments and confirmed that more labels, a higher ratio, and better label cleanliness contribute to greater tagging quality of deep models.   Based on our findings, we develop a comprehensive heat map to guide practitioners on selecting the appropriate model for the desired semantic tagging performance for their datasets. This heat map shows the characteristics of the datasets and their quality score of semantic tagging with different tagging models. By using this heat map, practitioners can estimate the semantic tagging quality gain while adopting different deep or simple models. At the same time, they can also try to improve the dataset characteristics to improve the quality of semantic tagging. \\  %To help practitioners select appropriate models and set expectation on the tagging performance for their datasets, we prepare a comprehensive heap map that shows the characteristics of the 21 datasets and their tagging F1's with BERT and SVM. By using this heap map, practitioners can estimate the tagging quality gain of adopting deep models. Meanwhile, they can try to improve the dataset characteristics to push the tagging quality to higher upper limits. \\   We systematically evaluate deep models and simple models for the task of semantic tagging. %We selected X models, curated Y datasets of varying characteristics, and conducted a series of experiments investigate the performance of the models in these two models and datasets.  %\wctan{Are we releasing our collection of datasets and models?} \jinfeng{Yes. I added one sentence about it to the end of this paragraph. Will work on the release later.} Our key contributions are as follows.  We surveyed a number of applications to motivate our study. We selected three representative deep models and two simple models that are widely used to develop these applications. We collected 21 datasets of varying characteristics for a comprehensive study.  We conducted extensive evaluations to obtain performance of semantic tagging of the five selected models on all the datasets. We found deep models do not necessarily perform better than simple models on large datasets.  We evaluated the effects of dataset characteristics on the quality of semantic tagging. We found the training size, label ratio, and label cleanliness impact the quality of semantic tagging.  We generated a comprehensive heat map that can guide practitioners to decide whether they should adopt deep models or simple models and anticipate the performance of semantic tagging for their datasets. To facilitate future research, we will release our collection of datasets, models, and implementations at https://github.com/rit-git/tagging. \\   %  In this paper, we studied sentences extraction from online reviews. Specifically, we summarized our contributions as follows. We will release our codes and datasets \jinfeng{we need to decide} for further research.  % \parskip=0pt %     %   We survey a number applications in Section. We discuss the designs of selected deep models and simple models in Section. We introduce the collected datasets and their characteristics in Section. We perform experimental evaluations and comparisons in Section. We analyze the effect of dataset characteristics and present key findings in Section. We conclude our study in Section. \documentclass{vldb} \usepackage{balance} \usepackage[shortlabels]{enumitem} \usepackage{multirow}  %\usepackage{times}   \usepackage{graphicx} \usepackage{multirow} \usepackage{xcolor} % {Theorem}[section] {Example} \usepackage{graphicx} \usepackage[labelfont=bf]{caption} \usepackage{subcaption} \usepackage{tabularx} \usepackage{multicol} \usepackage{booktabs} \usepackage{color, colortbl}   \definecolor{Gray}{gray}{0.9} \usepackage{times}  [1]{{{[[[ {#1}\ --yuliang ]]]}}} [1]{{{[[[ {#1}\ --xiaolan ]]]}}} [1]{{{[[[ {#1}\ --nikita ]]]}}} [1]{{{[[[ {#1}\ --alon ]]]}}} [1]{{{[[[ {#1}\ --wangchiew ]]]}}} [1]{{{[[[ {#1}\ --jinfeng ]]]}}} [1]{{{[[[ {#1}\ --lu ]]]}}} }}  \def\flat{simple} \def\Flat{Simple} \def\FLAT{SIMPLE}   % Semantic tagging?  \def \def\Sentence{Sentence}  \def\tagging{tagging} \def\Tagging{Tagging}  % Include information below and uncomment for camera ready \vldbTitle{Deep or Simple Models for Semantic Tagging? It Depends on your Data} \vldbAuthors{Jinfeng Li, Yuliang Li, Xiaolan Wang, Wang-Chiew Tan} \vldbDOI{https://doi.org/10.14778/3407790.3407844} \vldbVolume{13} \vldbNumber{11} \vldbYear{2020}  \pagenumbering{gobble}   %\title{Beyond Sentiments: What Else are People Trying to Extract from Online Reviews [Experiments]} \title{Deep or Simple Models for Semantic Tagging?\\It Depends on your Data}   %  in this sample file, there are a *total* % of EIGHT authors. SIX appear on the 'first-page'  and the remaining two appear in the \additionalauthors section.  \author{ % You can go ahead and credit any number of authors here, % e.g. one 'row of three' or two rows . % % The command \alignauthor  should % precede each author name, affiliation/snail-mail address and % e-mail address. Additionally, tag each line of % affiliation/address with \affaddr, and tag the % e-mail address with \\        @megagon.ai} }     Semantic tagging, which has extensive applications in text mining, predicts whether a given piece of text conveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.  We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against ``simple models'' over datasets with varying characteristics. Specifically, we select three prevalent deep models  and two simple models , and compare their performance on the semantic tagging task over 21 datasets.  %The results showed that deep models are not %necessarily better than simple models when the %characteristics of datasets are variable. %\xiaolan{Maybe say they did not perform well on large datasets first. The previous sentence claims they are not better, so the next sentence better elaborate over this point.} \xiaolan{Maybe add a transition sentence? e.g., To better understand their performance}  %To understand what are the exact characteristics of datasets affecting tagging quality, we selected the representative simple model  and deep model , and compared their performances on different types of datasets.  Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task. %\textcolor{blue}{In particular, we found that simple models outperform deep models on larger datasets with higher label ratios or worse label cleanliness, when runtime is a concern}.  %The conditional outperformance of deep models %suggests that practitioners should carefully %select learning models when they aim to achieve %the best tagging quality.  %Our results will systematically guide practitioners in selecting the right learning model for their semantic tagging task. % %To further assist practitioners to pick right %learning models, we generated a comprehensive heat map that compares tagging qualities across varied combinations of models and datasets. The heat map will be an instructional reference for practitioners to adopt appropriate models and set accurate expectations when tagging sentences for their own datasets.  %\wctan{heat map or table?} \jinfeng{I prefer heat map that uses colors and reflects the effect of data characteristics on F1}  %\textcolor{red}{Our results indicate that practitioners should pay attention to dataset characteristics when they apply deep models for better \tagging{} quality. We also generate a comprehensive heat map from our results that can help practitioners to adopt appropriate models and set expectation on tagging performance for their datasets}.         % %    %    %     \documentclass{vldb} \usepackage{graphicx} \usepackage{multirow}   \title{Beyond Aspects and Sentiments: What Else are People Trying to Extract from Customer Reviews}   [t] { {|l|l|l|} {*}{Suggestion}& Suggestion dataset      & Provide a human-labeled dataset \\                           & Semi-supervised C2C suggestion mining & Claim the better performance   \\                           & Travel tips             & Only work for TripAdvisor dataset as it contains labeled data by experts\\                           & A Survey on suggestion mining & Evaluate different supervised NN approaches  \\                           & customer-to-customer suggestion extraction & Propose automatic suggestion mining and explore SVM model \\                           & CNN for sentence classification & General method for supervised sentence classification \\ {*}{Fact}  & Unsupervised tip-mining & The only Unsupervised method    \\                      & Reason mining & Extract product pros and cons reasons from user reviews \\                      & Mining product defects and improvements       & Mining actionable information that business can use to improve products          \\                      & LDA based product defect mining               & Adopt LDA model in mining product cons                                           \\                      & Mining product failure by distant supervision & Distant supervision classifiers outperform strong baselines in ming product cons \\                      & Detecting experience from weblogs             & Detect actual human activity or event from review comments                       \\ {*}{Answer} & Yahoo!Answer Tips                    & Find the best answers for a given question                                       \\                      & Question answering from reviews               & Find relevant reviews for a question                                             \\                      & Aspect based question answering from reviews  & Find relevant reviews for a question and outperform                \\ \hline                                    }     % problem similar to extractive summarization 
","  Semantic tagging, which has extensive applications in text mining, predicts whether a given piece of text conveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.  We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against ``simple models'' over datasets with varying characteristics. Specifically, we select three prevalent deep models  and two simple models , and compare their performance on the semantic tagging task over 21 datasets.  %The results showed that deep models are not %necessarily better than simple models when the %characteristics of datasets are variable. %\xiaolan{Maybe say they did not perform well on large datasets first. The previous sentence claims they are not better, so the next sentence better elaborate over this point.} \xiaolan{Maybe add a transition sentence? e.g., To better understand their performance}  %To understand what are the exact characteristics of datasets affecting tagging quality, we selected the representative simple model  and deep model , and compared their performances on different types of datasets.  Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task. %\textcolor{blue}{In particular, we found that simple models outperform deep models on larger datasets with higher label ratios or worse label cleanliness, when runtime is a concern}.  %The conditional outperformance of deep models %suggests that practitioners should carefully %select learning models when they aim to achieve %the best tagging quality.  %Our results will systematically guide practitioners in selecting the right learning model for their semantic tagging task. % %To further assist practitioners to pick right %learning models, we generated a comprehensive heat map that compares tagging qualities across varied combinations of models and datasets. The heat map will be an instructional reference for practitioners to adopt appropriate models and set accurate expectations when tagging sentences for their own datasets.  %\wctan{heat map or table?} \jinfeng{I prefer heat map that uses colors and reflects the effect of data characteristics on F1}  %\textcolor{red}{Our results indicate that practitioners should pay attention to dataset characteristics when they apply deep models for better \tagging{} quality. We also generate a comprehensive heat map from our results that can help practitioners to adopt appropriate models and set expectation on tagging performance for their datasets}.",62
" %% UNSUPERVISED SHIZ  The problem of disambiguation is defined as selecting the correct analysis from a set of possible analyses for a word in a sentence---e.g., from among the analyses produced by a morphological analyser. Disambiguation is performed by utilizing information in the surrounding context.\footnote{This paper contains {, which means that to train our model we need only a morphological analyser for the language and an unlabelled corpus.  % Erzya and North-Sami are two examples of languages that have a morphological analyser but less than 30k annotated tokens.%\todo{such as? russian has it..}.  %%%%%%%%%%%%%%%%%%%  The main idea of our approach is to use bidirectional LSTMs---BiLSTMs---to disambiguate the output of morphological analysers, by utilizing only the unambiguous outputs during the training procedure. %% \todo{TXEMA: add context embedding shit} % We train bidirectional models using a sequence of embeddings for the surface form for each target word. The objective of the network is to produce output probability distributions over the possible POS tags and lemmas. The model is trained using only the unambiguous input tokens; the loss is computed only for those unambiguous instances. Ambiguous tokens are not considered as target tokens during training.  % Evaluation is done on  data for each language. Thus, only the quality of the evaluation is dependent on the amount of available annotated data---the model quality is only affected by the amount of unlabelled data we use.  Since we only input unlabelled data for training, the quality of the model itself is only affected by the amount of available unlabelled data for the language. In our experiments, we evaluate our models on manually annotated data sets for Finnish, Russian and Spanish. For Finnish and Russian, at least, annotated  data is in limited supply, whereas for all three languages unlabelled data is in abundant supply.  %% ??? CITE CORPORA!!!  The paper is organized as follows. % In Section we point to some relevant prior work. % In Section we describe the problem of morphological ambiguity and provide a brief motivation for the interest in the problem. % In Section we provide a classification for the different types of ambiguity that appear in the corpus, as well as an analysis of the viable and appropriate strategies for each type of ambiguity. % Section describes our data pre-processing steps and model architecture. % Section specifies our experimental setup, as well as the parameters used in training. % In Section we discuss the results obtained from the experiments. % Section concludes with current directions of research.    
","   We consider the problem of disambiguating the lemma and part of speech of ambiguous   words in morphologically rich languages.   %    We propose a method for disambiguating ambiguous words in context, using a large   un-annotated corpus of text, and \comment{??? finite-state",63
"     % The claim that knowledge of linguistic structures is innate is among the most significant and controversial claims of generative linguistics.  notices that human language systematically include rules that make reference to hierarchical structure, but rarely if ever has rules that reference linear order. This is surprising in light of the fact that key data favoring the acquisition of structural rules over linear ones are often absent from the raw linguistic input to the child acquiring language. These observations lead to the proposal of the , and this  has been the subject of much debate over the last several decades .  Humans appear to use structural biases to guide language acquisition. A classic example is the rule for subject-auxiliary inversion: Native English speakers uniformly acquire a rule like the structural generalization in Figure  that makes reference to hierarchical syntactic structures, despite the fact that the raw linguistic input often supports linear generalizations which are intuitively just as simple . Humans are not alone in possessing this inductive bias: Prior investigations have identified some artificial learners with a structural bias by virtue of having a significantly restricted the hypothesis space  or a hierarchically structured architecture that learns from pre-parsed data .   However, these results cannot tell us whether a learner starting with very weak biases can  a structural bias merely from exposure to raw linguistic data. While inductive biases are often understood to be unchangeable properties of a learner, this need not be the case. For instance, in one dominant paradigm in natural language processing, pretraining on raw data is used to create a general purpose sentence processing model like BERT , which can subsequently be fine-tuned to perform a downstream task. The model's inductive biases with respect to the downstream task may be substantially influenced by the prior knowledge acquired during pretraining. % eneral prior knowledge acquired during pretraining is sure to influence such model's inductive bias on a downstream task is no doubt influenced by prior   For instance, if a learner draws on general prior knowledge when faced with a novel generalization problem, acquiring new general knowledge should in some cases influence its inductive bias.      In this work, we present new experimental evidence that BERT may acquire an inductive bias towards structural generalizations from exposure to raw data alone.  We conduct four experiments inspired by   to evaluate whether BERT has a structural or linear bias when generalizing about structure-dependent English phenomena. We follow the  design , outlined in Figure . First, we fine-tune BERT to perform a classification task using data intentionally ambiguous between structural and linear generalizations. Then, we probe the inductive biases of the learner by observing how it classifies held-out examples that disambiguate between the generalizations.  % Unlike , we test a learner with significant pretraining, meaning we are probing the inductive biases of the fine-tuning  allowing us to evaluate  The classification tasks illustrate three structure dependent rules of English grammar regarding subject-auxiliary inversion, reflexive-antecedent agreement, and negative polarity item  licensing. A fourth task requires the classifcation of sentences based on an arbitrary rule: whether the verb in an embedded clause is in the past tense. The data is generated from templates using the generation tools and lexicon built by  and .  %  suggest that humans may be this type of learner  % a learner's inductive biases with respect to a particular generalization problem may be influenced by prior knowledge. Acquiring new knowledge   % Are humans unique in possessing this bias? Researchers have  % Where does this bias come from? According to the ,   % During language acquisition, humans make generalizations are biased towards acquiring grammatical rules based on hierarchical structure, as opposed to linear order. Over the last 50 years, there has been considerable debate about whether this bias is acquired or innate. The strongest argument in support of the  is   which relies on the assumption that     % holds that key data favoring the acquisition of structural rules over linear ones are often absent from the raw linguistic input acquiring language.  % In this work, we present new experimental evidence from artificial statistical language learners that casts doubt on the soundness of this argument. The goal of these experiments is to evaluate whether exposure to unstructured linguistic data provides sufficient evidence for a low-bias learner to acquire an inductive bias towards structural generalization. Recent advances in artificial neural networks for natural language understanding have produced models which appear to acquire robust representations of hierarchical syntax    % at the outset to be possible   good candidates for counterexamples to this claim. Using the technique of  state-of-the-art models like BERT  appear to acquire rich linguistic knowledge from raw data. BERT is pretrained to produce contextualized representations for words and sentences which are often taken as the input to a fine-tuning pipeline on downstream tasks used to train models that approach human performance on tasks that require significant syntactic knowledge, such as judging the grammatical acceptability of sentences .     % Our experiments are inspired by , who conduct an experiment with similar goals following the  design  in which a learner is trained to perform a task using data intentionally excluding certain key examples. The inductive biases of the learner can then be probed by observing the behavior of the learner on these held-out examples. Their experimental data are inspired by  well-known examples  illustrating how subject-auxiliary inversion in English follows a structure dependent rule  as opposed to a linear one .  % \ex.\a. Is the man who is tall  happy? %  tall is happy?     % In each experiment, a neural network classifier learns to use a BERT encoding to detect unacceptable sentences in these domains. Following the poverty stimulus method the training data is intentionally impoverished in order to be consistent with both a structure dependent rule and a linear rule. Observing how the models generalize to the withheld examples shows whether BERT encodings have a bias towards structural or linear information. There is also a followup experiment which tests how classifiers generalize when acceptability cannot be used as a cue towards the structure dependent generalization.   The results of these experiments suggest that BERT likely acquires a inductive bias towards structural rules from self-supervised pretraining. BERT generalizes in a way consistent with a structural bias in 3 out of 4 experiments: those involving subject-auxiliary inversion, reflexive binding, and embedded verb tense detection.  % Furthermore, in the one exceptional case---NPI licensing---there is evidence that humans show illusory effects based on linear order , which suggests that humans have a linear. While these experiments leave open several alternative explanations for this generalization behavior, they add to mounting evidence that significant syntactic knowledge, including a structural biases, can be acquired from self-supervised learning on raw data.  % The structure of the paper is as follows. Section  provides background on the hypothesis of linguistic nativism and on neural networks for language understanding. Section  reviews prior work looking for evidence of structural bias in statistical learners, and oulines the present approach. Section  discusses the data and methods for the main experiments. Sections  and  are the results and discussion. Section  includes the followup experiment, and an Appendix includes analysis of the classifiers. Section  concludes.  % specific assumptions in the argument in support of the hypothesis that this bias is innate to humans.  
"," We evaluate whether BERT, a widely used neural network for sentence processing, acquires an inductive bias towards forming structural generalizations through pretraining on raw data. We conduct four experiments testing its preference for structural vs. linear generalizations in different structure-dependent phenomena. We find that BERT makes a structural generalization in 3 out of 4 empirical domains---subject-auxiliary inversion, reflexive binding, and verb tense detection in embedded clauses---but makes a linear generalization when tested on NPI licensing. We argue that these results are the strongest evidence so far from artificial learners supporting the proposition that a structural bias can be acquired from raw data. If this conclusion is correct, it is tentative evidence that some linguistic universals can be acquired by learners without innate biases. However, the precise implications for human language acquisition are unclear, as humans learn language from significantly less data than BERT. % learns in a setting vastly different from humans.  % the question remains open whether or not human could plausibly acquire our own structural bias as well, as BERT learns in a setting  % We cannot necessarily conclude that humans could plausibly acquire this bias as well, since BERT learns from far more data than human learners.  % Nonetheless, these results are still valuable in that they suggest that a structural bias may be learnable in principle from raw data alone. If this turns out to be correct, it is a refutation of a key assumption in  argument that structural bias is innate to humans.  Keywords: inductive bias; structure dependence; BERT; learnability of grammar; poverty of the stimulus; neural network; self-supervised learning",64
"   As the world became more digital, the amount of unstructured data available on the Internet has increased to the point where it has become unbearable to handle it using human resources. Natural language processing  tasks were developed to address it in an automatic way using computational resources. Some examples of these tasks are audio transcription, translation, assessment on text summaries, grading tests, and opinion mining.  For NLP tasks, a critical point is the computational text representation since there is no consensus on how to represent text properly using computational resources. The most classical text representation is bag-of-words. In this distributive representation, a vocabulary is collected in the training corpus and each sample\footnote{In this study, we use the word sample to denote instance or text document.} is represented by a vector where each element represents the occurrence or absence  of vocabulary terms in the document .  New text representation techniques have been studied due to known issues of bag-of-words representation: it loses word locality and fails to capture semantic and syntactic features of the words. To address these issues, other techniques were developed, such as the distributed text representation that learns fixed-length vector for each word, known as word embeddings. Using statistics of the context and the abundant occurrences of the words in the training corpus, learned word embeddings can capture the semantic similarity between words.   As each sample can have many words, a composition function is usually employed to encode all word embeddings into a single fixed-length representation per sample to satisfy the fixed-length input restriction on the most of the predictive models. Some representation techniques also encodes the position of a word in the sample, addressing the word locality issue.  The majority of predictive models for NLP tasks have their performance degraded when unknown words, which were not collected to build the vocabulary in the training phase or were discarded due to low frequency across the corpus, appear in the test. These words are called out-of-vocabulary  words and can degrade the performance of NLP applications due to the inefficiency of representation models to properly learn a representation for them.   In order to emphasize how an OOV word can hinder sentence comprehension, consider the following example originally written in ``The Jabberwocky'' by Lewis Caroll: ``He went galumphing back''. The nonce word ``galumphing'' was coined to mean ``moving in a clumsy, ponderous, or noisy manner; inelegant''. Since this word is an OOV, traditional models are not capable to handle it properly, ignoring it. The lack of representation for this word can restrict the  predictive model capabilities to understand this sentence.  Handling OOV words in distributed representation models can be achieved with simple strategies. For instance, as OOV words have no word embedding representation learned, they can be ignored when the composition function is applied. This approach leads the predictive model to fit data without the knowledge of the absence of a word because it is unknown to the representation model. For such case, a random vector can be adopted for each OOV word or a single random vector can be adopted for all OOV words.   These simple strategies provide little or no information about unknown words to predictive models in downstream tasks. In order to enable a predictive model to use a vector representation for the unknown words, those words need to be replaced by a meaningful in-vocabulary word. For this specific task, most of the techniques available in literature fits in two groups: language models  and robust techniques capable of learning meaningful representation for OOV words using their structures or the context in which they appear.  In these two groups, there are several deep learning  models.  Some of them were developed to handle OOV, such as Comick and HiCE, while evidence was found that pure neural architectures can also perform it, such as LSTM and Transformer. Some language models also had success in this task, such as RoBERTa, DistillBERT, and Electra.  Although several studies have shown that DL can be successfully applied in several NLP tasks, such as sentiment analysis, named entity recognition , and part-of-speech  tagging, there are few DL models for handling OOV words and no consensus on which approach is the best. To fill that gap, in this paper, we present a performance evaluation of state-of-the-art DL models  considering different datasets and tasks that can be greatly affected by OOV words.    
","  Communication has become increasingly dynamic with the popularization of social networks and applications that allow people to express themselves and communicate instantly. In this scenario, distributed representation models have their quality impacted by new words that appear frequently or that are derived from spelling errors. These words that are unknown by the models, known as out-of-vocabulary  words, need to be properly handled to not degrade the quality of the natural language processing  applications, which depend on the appropriate vector representation of the texts. To better understand this problem and finding the best techniques to handle OOV words, in this study, we present a comprehensive performance evaluation of deep learning models for representing OOV words. We performed an intrinsic evaluation using a benchmark dataset and an extrinsic evaluation using different NLP tasks: text categorization, named entity recognition, and part-of-speech tagging.   Although the results indicated that the best technique for handling OOV words is different for each task, Comick, a deep learning method that infers the embedding based on the context and the morphological structure of the OOV word, obtained promising results.",65
"  Adversarial learning is a major threat to the field of computer security research. With the advancement in technology, the growing dependency on the Internet has exposed users to serious cyber threats like phishing and pharming. Despite considerable research to counter such threats, staggering numbers of individuals and organizations fall prey to targeted social engineering attacks incurring huge financial losses.   Although attackers change their strategies, previous research has shown that electronic mails  are a popular attack vector. Emails can be embedded with a variety of malign elements like poisoned URLs to malicious websites, malware attachments as well as executables, documents, image files, etc.  Anti-Phishing Working Group  reports over 270,500\footnote{http://docs.apwg.org/reports/apwg\_trends\_report\_q3\_2018.pdf} unique phishing email campaigns received in the  quarter of 2018, rising from a total of around 233,600\footnote{http://docs.apwg.org/reports/apwg\_trends\_report\_q4\_2017.pdf} unique reports identified in the  quarter of 2017. Phishing reports also reveal the consistent rise in phishing attacks targeted towards financial institutions like payment processing firms and the banking sector. The statistics demonstrate how the threat is worsening as attackers continue to devise more sophisticated  ways of scamming victims.  % Detection systems and algorithms are commonly trained on historical data and attack patterns.  Innovative and unseen attack vectors can trick pre-trained classification techniques, thus placing the victim at risk. In email masquerading attacks, attackers after compromising the email account of an individual can carefully construct a fraudulent email then sent to the contacts known to the compromised individual. This has serious implications, because the attacker has gained uninterrupted access to the inbox, outbox and other private details of the compromised person. Thus by exercising caution, the attacker can emulate the content and context of the emails written by the individual and can communicate with his contacts as a legitimate entity, successfully evading detection and causing harm to the victim.  However, construction of the perfect deceptive email requires fine-tuning and manual supervision. While a fake mail constructed manually by an attacker can guarantee a higher chance of success, the process is both time and labor intensive. In contrast, an automated text generator can be trained to synthesize targeted emails much faster and in bulk, thereby increasing the odds of a successful attack. However, the bottleneck in this case, lies in whether the system can generate high quality text, free from common flags like misspellings, incorrect and abusive language, over-usage of action verbs, etc., which can be picked up by a classifier easily. Thus, proactive research in this area of deception based attacks using email masquerading techniques requires further sophisticated experimentation.     Advances in the field of natural language processing have introduced newer and  sophisticated algorithms which enable a machine to learn and generate high-quality textual content on a given context. Grammar based tools like the Dada Engine, N-gram language models as well as deep neural learners  have been used to study and replicate natural language based attacks. The aim is to facilitate proactive research by predicting newer attacks and reinforce against such unseen yet impending threats.  % The system can then be made to generate text that closely resembles the input structure and form.      At the hands of an attacker, language generation techniques can become dangerous tools for deception. With access to proper training data, deep learning neural networks are capable of generating textual content. This property has been leveraged by researchers for generating tweets and poetry,, etc. While limited, proactive research has been pursued by using deep learners for generation of fake reviews, grammar based techniques as well as simplistic deep networks have been leveraged for email generation. Thus, we can assume that it is not long before phishers and even spammers resort to such techniques to generate newer kinds of malicious attack vectors.   Following a proactive mode of study, we identify the underlying implications of how an automated machine learning technique, here, deep learners can be leveraged to synthesize email bodies for the purpose of email masquerading attacks. %\textcolor{magenta}{ Along with demonstrating the systems' performance using qualitative and quantitative methods, we study the effectiveness and practicality of such systems by comparing a hierarchical deep network with a baseline word prediction model. Our key contributions are as follows: %} % In this research, we aim at drawing attention to the gravity of this situation before people and organizations start falling vulnerable to targeted phishing scams. In this paper, we address the new class of Email masquerading attacks based on automated fake Email generation.   %\textcolor{red}{Add section numbers.}   { . While generation of coherent emails is challenging, we use a hierarchical network that consists of two stages - an architecture which uses a word prediction model to generate probable candidate sentences which are then passed onto a sentence selection model, based on distributed vector representations of the email content, to select the best possible set of sentences. Such a two-staged architecture should be suitable for generating longer content while maintaining coherency.  , coherency, fluency and legitimacy of the fake emails by conducting a human evaluation.    
"," Advanced machine learning and natural language techniques enable attackers to launch sophisticated and targeted social engineering based attacks. To counter the active attacker issue, researchers have since resorted to proactive methods of detection. Email masquerading using targeted emails to fool the victim is an advanced attack method. However automatic text generation requires controlling the context and coherency of the generated content, which has been identified as an increasingly difficult problem. %\textcolor{magenta}{ The method used leverages %}  a hierarchical deep neural model which uses a learned representation of the sentences in the input document to generate structured written emails. We demonstrate the generation of short and targeted text messages using the deep  model. The global coherency of the synthesized text is evaluated using a qualitative study as well as multiple quantitative measures.",66
" Geoparsing is the process of recognizing and geo-locating  location mentions from   texts. It has been widely applied to various textual data, and is an important task in geographic information retrieval . A geoparsing system, known as a geoparser, usually functions in two steps: toponym recognition and toponym resolution. Toponym recognition detects the place mentions in texts, while toponym resolution resolves any place name ambiguity and assigns the appropriate spatial footprint . Many geoparsers have been developed, such as CLAVIN,  the Edinburgh Geoparser , GeoTxt , and TopoCluster .   In June 2019, an important geoparsing competition, Toponym Resolution in Scientific Papers, was held as the SemEval 2019 Task 12, in conjunction with the Annual Conference of the North American Chapter of the Association for Computational Linguistics. This competition attracted 29 registered teams and 8 teams eventually submitted a system run . The winning teams all leveraged state-of-the-art neural network based models, such as BiLSTM-CRF and deep contextualized word embeddings, to design their geoparsers. Particularly, the geoparser that won the first place, DM\_NLP , achieved over 90\% precision, recall, and F1 score for toponym recognition. This result is exciting and brings the question ``are we there yet?"" A 90\% performance is not perfect but is probably sufficient for many applications. So have we already made enough progress that we can  consider the problem of geoparsing as solved?  A major limitation of the SemEval 2019 Task 12 competition is that the submitted geoparsers were tested on a single dataset which has 45 research articles from one particular domain of Bio-medicine. Existing research has shown that the same geoparser can have very different performances when  tested on different datasets  . Accordingly,  answering the question of whether  the problem of geoparsing can be considered as solved requires a systematic evaluation of the state-of-the-art geoparsers on multiple datasets which should ideally be in different text genres .  In a recent work, we developed an online platform called EUPEG which is an Extensible and Unified Platform for Evaluating Geoparsers . EUPEG hosts a majority of the geopasing resources reported in the literature, including eight annotated datasets, nine geoparsers, and eight evaluation metrics. In addition, the eight annotated datasets are in four different text genres which are news articles, Wikipedia articles, social media posts, and texts on Web pages. The source code of EUPEG and the related geoparsing resources are shared on GitHub.  In this paper, we systematically evaluate the top geoparsers from  SemEval Task 12  using EUPEG as a benchmarking platform. We focus on the top three end-to-end geoparsers that showed the highest performances in the competition, which are DM\_NLP  , UniMelb  , and UArizona . We test the performances of these three geoparsers on the datasets hosted on EUPEG, and compare their performances with the other existing geoparsers. The contributions of this paper are as follows:  	 to support future research.     
"," Geoparsing is an important task in geographic information retrieval. A geoparsing system, known as a geoparser, takes some texts as the input and outputs the recognized place mentions and their location coordinates. In June 2019, a geoparsing competition, Toponym Resolution in Scientific Papers, was held as one of the SemEval 2019 tasks. The winning teams developed neural network based geoparsers that achieved outstanding performances . This exciting result brings the question ``are we there yet?'', namely have we achieved high enough performances  to possibly consider the problem of geoparsing as solved? One limitation of this competition is that the developed geoparsers were tested on only one dataset which has 45 research articles collected from the particular domain of Bio-medicine. It is known that the same geoparser can have very different performances  on different  datasets. Thus, this work performs a systematic evaluation of these state-of-the-art geoparsers using our recently developed benchmarking platform EUPEG that has eight annotated datasets, nine baseline geoparsers, and eight performance metrics. The evaluation result suggests that these new geoparsers indeed improve the performances of geoparsing on multiple datasets although some challenges remain.",67
"  The process of digital transformation in medicine has been going for a while, providing faster and better treatment results in many cases through the use of modern computer science and Artificial Intelligence  methods. Digitization and subsequent analysis of medical records constitutes one such area of digital transformation that aims to collect broad types of medical information about a patient in the form of EHR, including digital measurements , verbal descriptions , images  and document the treatment process of a patient.  In this paper, we focus on the analysis of EHR with the purpose of providing clinical decision support by predicting most probable diagnoses during a patient's visit to a doctor. This problem is complicated by abundance of large volumes of structured and unstructured medical information stored across multiple systems in different data formats that are often incompatible across these systems. Although there exists an emerging FHIR standard  for the EHR data the goal of which is to unify the process of storing and exchanging medical information, unfortunately, very few existing Hospital Information Systems  support it. All this complicates the task of diagnosis prediction based on the EHRs since many of them contain extensive amounts of unstructured, poorly organized and ""dirty"" data that is less amenable to the analysis using the AI-based methods, unless this data is cleaned and preprocessed appropriately.  Providing clinical decision support in diagnosis prediction during a patient's visit to a doctor is important because many patient's visits, in fact up to 30\% in the US, are misdiagnosed. This is also true in some other countries. We formulate the aforementioned clinical decision support problem as a multi-label text classification of clinical notes  during a patient visit, where the classification is performed for a wide range of diagnosis codes represented by the International Statistical Classification of Diseases .  In this paper, we make the following contributions. First, we propose a novel BERT-based model for classification of textual clinical notes, called , that differs from the previously proposed models by the way of the FC-layer composition that is described in Section . Second, we compare the performance of our method with various baselines across different text representation techniques and classification models. Third, we compare the performance of the BERT model pretrained on a large corpus of out-of-domain data with the BERT model pretrained exclusively on in-domain data and using an in-domain tokenizer. Finally, we demonstrate the advantage of the proposed models and their comparable results with a human baseline in Section.  It is important to note that the clinical decision support system described in the paper will  serve as a doctor's replacement but, rather, constitutes an unbiased intelligent diagnosis generator and, therefore, should only  the doctors in their diagnostic decisions.  
"," In this paper we study the problem of predicting clinical diagnoses from textual Electronic Health Records  data. We show the importance of this problem in medical community and present comprehensive historical review of the problem and proposed methods. As the main scientific contributions we present a modification of Bidirectional Encoder Representations from Transformers  model for sequence classification that implements a novel way of Fully-Connected  layer composition and a BERT model pretrained only on domain data. To empirically validate our model, we use a large-scale Russian EHR dataset consisting of about 4 million unique patient visits. This is the largest such study for the Russian language and one of the largest globally. We performed a number of comparative experiments with other text representation models on the task of multiclass classification for 265 disease subset of ICD-10. The experiments demonstrate improved performance of our models compared to other baselines, including a fine-tuned Russian BERT  variant. We also show comparable performance of our model with a panel of experienced medical experts. This allows us to hope that implementation of this system will reduce misdiagnosis.",68
" Measuring semantic similarity between sentences has been one of the major problems towards text understanding. Many tasks including paraphrase identification, text entailment recognition, etc. also utilize sentence similarity. Clearly, it has attracted a lot of attention in the NLP research community. Semantic textual similarity  dataset from SemEval 2012 has been one of the commonly used benchmark for sentence similarity task, which attempts at measuring the degree of semantic equivalence between two sentences. While recently proposed deep learning methods built on pretrained language models have shown great success for the task , interpretability and explainability of the final scores remains a concern in general.  proposed to formalize interpretable semantic textual similarity  as an alignment between pairs of segments across the two sentences at SemEval 2016.  
"," % Systematically discovering semantic relationships in text is an important and extensively studied area in NLP, with various tasks such as entailment, semantic similarity, etc. Decomposability of sentence level scores via chunk alignments has been proposed as a way to make such models reliable . We study the problem of aligning components of sentences leading to an interpretable model for measuring semantic textual similarity. In this paper, we present a novel logic-constrained gated pointer network model to align constituents of two sentences, aiding in interpretation of semantic relationships. \fxnote{Few lines needed about the approach.} We achieve state of the art results with an F1 score of 0.97 showing significant improvements over existing solutions.  Systematically discovering semantic relationships in text is an important and extensively studied area in Natural Language Processing, with various tasks such as entailment, semantic similarity, etc. Decomposability of sentence-level scores via subsequence alignments has been proposed as a way to make models more interpretable. We study the problem of aligning components of sentences leading to an interpretable model for semantic textual similarity. In this paper, we introduce a novel pointer network based model with a sentinel gating function to align constituent chunks, which are represented using BERT. We improve this base model with a loss function to equally penalize misalignments in both sentences, ensuring the alignments are bidirectional. Finally, to guide the network with structured external knowledge, we introduce first-order logic constraints based on ConceptNet and syntactic knowledge. The model achieves an F1 score of 97.73 and 96.32 on the benchmark SemEval datasets for the chunk alignment task, showing large improvements over the existing solutions. Source code is available at \url{https://github.com/manishb89/interpretable_sentence_similarity}",69
"  Neural Machine Translation  has achieved unprecedented successes and drawn much attention from both academia and industry. Following the sequence-to-sequence learning paradigm, NMT approaches usually consist of two parts -- the encoder and the decoder, where the encoder maps the source side sentence into a sequence of hidden representations, and the decoder generates the target side tokens step by step based on the encoder outputs.  %Various techniques, such as recurrent networks , convolution networks , and more recently, the self-attention transformers , have been applied to the NMT tasks and delivered promising results.  Despite its success, the commonly used encoder-decoder framework in NMT always suffers from over- and under- translation problems . The decoder may tend to repeatedly focus on same parts of the source sentence while ignoring the other parts. Many efforts  have been made to mitigate this issue  by either explicitly or implicitly modeling the step-by-step translated and un-translated information during the decoding process.  One promising direction is to track the translated  and un-translated  components of the source sentence  at each decoding step. The components are modeled by RNN or Capsule Network with heuristic objectives .   % Figure   illustrates how past and future works.         % However,    In this paper, we argue that the heuristic objectives in previous approaches may be indirect and insufficient in certain circumstances, which limits their effectiveness.  The  and  modules have two major functionalities, which are the identification of past and future contents and extracting useful features for further predictions.  However, prior studies mix these two functionalities up and try to model them jointly by only fitting the outputs of  /  module. Here, we propose a novel dual learning method to enhance both two functionalities with two transformer models  trained simultaneously .  On the one hand, we propose to use backward NMT encoder with the partially inputs to provide contextually-rich supervision for the past / future identification instead of a coarse-grained bag-of-word loss.  On the other hand, we exploit a Guided Capsule Network  on two encoders to align the capability of feature extraction with manually masking, instead of mixing up both functionalities. With the training proceeds, bidirectional models perform as teachers for each other and strengthen the performance iteratively.  We evaluate our approach on two commonly used translation datasets, i.e., the NIST Chinese-to-English task and the WMT 2014 English-to-German task.  The experimental results demonstrate that our method significantly outperforms the previous strong baselines in terms of the translation quality of generated NMT translations. Also, among the subjective evaluation, our method surpasses previous adequacy-oriented methods in mitigating both over- and under-translation problem.     % % 
"," % Though Neural Machine Translation  has been successfully adopted in many areas,  Though remarkable successes have been achieved by Neural Machine Translation  in recent years,  it still suffers from the inadequate-translation problem.  Previous studies  show that explicitly modeling the translated  and un-translated  contents of the source sentence is beneficial for translation performance. However, it is not clear whether the commonly used heuristic objective is good enough to guide the  and . In this paper, we present a novel dual learning framework that leverages both source-to-target and target-to-source NMT models to provide a more direct and accurate supervision signal for the  and  modules. Experimental results demonstrate that our proposed method significantly improves the adequacy of NMT predictions and surpasses previous methods in two well-studied translation tasks.",70
" Document-level Sentiment Analysis , a subtask of Sentiment Analysis , aims to understand user attitudes and identify sentiment polarity expressed at document-level. This task has grown鑱絫o鑱絙e鑱給ne鑱給f鑱絫he鑱絤ost鑱絘ctive鑱絩esearch鑱絘reas鑱絠n鑱絥atural鑱絣anguage鑱絧rocessing and plays an important role in many real-world applications such as intent identification , recommender systems and misinformation detection.  Generally, DSA can be regarded as a text classification problem and thus traditional text classification approaches  can be adopted naturally. Different from other subtasks in SA,  DSA is more challenging due to the large size of words, vague semantic links between sentences and abundance of sentiments. Hence, the research question that how to  learn an expressive document representation to understand long documents for sentiment classification has been given a growing interest to researchers.   Inspired by the document structure,  one of the earliest and most influential models HAN was proposed by   to encode the entire document, which suffered from  attending explicit but irrelevant sentimental words. Subsequent works mainly dedicated to  introducing latent topics  or global context  to tackle this limitation. However, most of the user-generated documents are very comprehensive and contain a wealth of sentiments, which makes it difficult to directly learn from the whole document without the understanding of the major points, especially in long documents.  The above works attempted to explore the major points of the document by learning a global embedding from the document. Intuitively, the user-generated summary contains more accurate information about the major points of the document. These summaries describe the long document in a more specific way, which are highly indicative of the key sentiment and subject, and can facilitate further to identify important  text present and sentiments.    To reduce the processing of the substantial text in the document and  be well aware of the major idea of it, summary-based methods introducing the user-generated summary has been developed for DSA and achieved promising results, which brings brilliant processing for understanding complex documents. They refined the document with an abstractive summary to predict the sentiment effectively. Recently, some effective works  concerned both the text summarization task and DSA, and jointly modeled them to boost from each other. Nevertheless, most of the joint models did not fully utilize user-generated summaries and ignored the interaction between summary and document, because they did not encode summaries explicitly during test time.    For example, the document of a product review in Amazon SNAP Review Dataset is ``...They just sent a new camera and it showed up without any warning or communication about the \underline{bad} one. \underline{Minimal} Customer Service...The 1st camera was \underline{promising} and worked so \underline{well} for about two weeks..."" and its length is . The corresponding summary is ``Quality is a reflection of Customer Service"".  The document contains complex sentiment expressed, such as { corresponding to positive or negative, respectively. The subject { and {. They are complementary. Therefore, the auxiliary of the summary is significant for subject mining and semantic understanding in DSA.     To tackle the aforementioned problems,  we investigate how to effectively focus on more accurate subject information for DSA.  In this paper, we present an end-to-end model, named {H}ierarchical {I}nteraction {N}etwork , to encode the bidirectional interactions between summary and document.  The method works by utilizing multiple granularities interactions between summary and document, accordingly to learn a subject-oriented document representation.  Specifically, the interactions at character-level and the contextual semantic features can be captured via BERT. Afterward, the segment-level interactions are encoded by gated interaction network and the context of the document is taken into account simultaneously. Finally, the document-level interactions are embedded via the attention mechanism to learn a more expressive document representation with the consideration of subject information for predicting sentiments.   Furthermore, because of the complex sentiment in the document, we attempt to learn the affective representation and alleviate the distraction from other sentiments.  We introduce the sentiment label information into the model in a feedback way. Most existing structures learn document representation via only feedforward networks and have no chance to modify the invalid features of the document. Some effective works in image classification  and named entity recognition  added feedback structure to re-weight of feature embeddings and obtained gratifying results.  Motivated by their works, we propose a {S}entiment-based {R}ethinking mechanism  and feedback the sentiment polarity label information.  This rethinking mechanism can refine the weights of document embeddings to learn a more discerning low-level representation with the guidance from the high-level sentiment features, and relieve negative effects of noisy data simultaneously.   We evaluate our proposed models on three public datasets from news to reviews. Through experiments versus a suite of state-of-the-art baselines, we demonstrate the effectiveness of interactions and the rethinking mechanism, and the model HIN-SR can significantly outperform than baseline systems.  The main contributions of this paper are summarized as follows.         .         The remainder of this paper is structured as follows. We review the related works in Section . Then we explain the details of our contributions in section . Section  describes the experiments and the results. Further analysis and discussion are shown in Section . In the end, the conclusions are drawn in Section .   
","     \let\thefootnote\relax\footnotetext{*  Equal Contribution.}     \footnotetext{\Letter {} Corresponding author.}          Document-level Sentiment Analysis  is more challenging due to vague semantic links and complicate sentiment information.      Recent works have been devoted to leveraging text summarization and have achieved promising results.      However, these summarization-based methods did not take full advantage of the summary including ignoring the inherent interactions between the summary and document.     As a result, they limited the representation to express major points in the document, which is highly indicative of the key sentiment.       In this paper, we study how to effectively generate a discriminative representation with explicit subject patterns and sentiment contexts for DSA.      A Hierarchical Interaction Networks  is proposed to explore bidirectional interactions between the summary and document at multiple granularities and learn subject-oriented document representations for sentiment classification.      Furthermore, we design a Sentiment-based Rethinking mechanism  by refining the HIN with sentiment label information      to learn a more sentiment-aware document representation.      We extensively evaluate our proposed models on three public datasets. The experimental results consistently demonstrate the effectiveness of our proposed models and show that HIN-SR outperforms various state-of-the-art methods.",71
" Text generation refers to a wide range of tasks involving generating natural language, including but not limited to machine translation, sentence simplification, and text summarization. Recent success of neural-based text generation  relies heavily on a large parallel dataset for training, which  may not be available in real-world natural language processing  applications. In this work, we consider unsupervised text generation, where no parallel data is available. This setting is more challenging, and has significant potential in both scientific research  and industrial applications .  %Unsupervised text generation assumes no parallel data are available, but still aims at accomplishing certain tasks, such as machine translation, sentence simplification, and text summarization. Unsupervised text generation has significant potential in both scientific research  and industrial applications . Conventional sequence-to-sequence training cannot be directly employed in this setting, due to the lack of parallel data. % LM: the: not necessary  Early work tackles unsupervised text generation by rules or templates. While such approaches do not require parallel corpora, the generated sentences are highly subject to the rules, and hence lack the flexibility of natural language. Other work constructs pseudo-parallel data, which is only feasible for certain tasks like unsupervised machine translation.  Recently, researchers have developed search-based techniques for unsupervised text generation, where a heuristically defined scoring function evaluates the quality of a sentence, involving language fluency, semantic compliance, and other task-specific aspects. Then, the algorithm performs word-level edits  to search towards a  optimum of the scoring function. With a reasonably designed scoring function, such approaches are shown to be effective in a variety of applications like paraphrase generation, sentence summarization, and text simplification.  However, the search-based approach has two major drawbacks: 1) The inference efficiency is low. To obtain an output sentence, the search algorithm would perform a few hundred steps of local edits and re-evaluations. This could be considerably slower than an autoregressive decoder, which generates words sequentially. 2) The search could yield noisy results, since the scoring function is defined heuristically and the search is conducted locally in a discrete sentence space.% by stochastic word editing.  To this end, we propose a new framework for unsupervised text generation by learning from search , which contains a strong search module that explores the sentence space, as well as a learning module that learns from the search results.   For the search module, we adopt the simulated annealing  algorithm. At each step, SA proposes a local edit by a neural network, and then either accepts or rejects the proposal based on a heuristically defined scoring function.  For learning, we employ two methods to train the conditional generative model, word-level cross-entropy loss and the sequence-level max-margin loss. Within \model{}, the search and learning can be boosted by each other in an iterative fashion. That is, the search results serve as the pseudo-reference for training the conditional generator, which in turn benefits SA search by serving as a more meaningful initial state. As for implementation, \model{} involves two pretrained language models: a) the uni-directional GPT2, which is suitable for likelihood-based fluency evaluation and conditional generation; and b) the bi-directional RoBERTa, which is better at semantic evaluation and contextual word-level prediction.  %We first perform simulated annealing search and treat the obtained output sentences as pseudo-references. Then, we train an autoregressive GPT2 as the text generator by word-level cross-entropy  supervised learning, which enables our model to learn quickly. Further, the outputs of GPT2 are taken as the initial state of the search algorithm again for iterative performance improvement, and we perform max-margin  learning to better distinguish between higher-scored sentences and other high-probability sentences.  The main contributions of our paper include: 1) We propose \model, a generic learning-from-search framework for unsupervised text generation. 2) We demonstrate efficient methods of incorporating the large-scale pretrained language models into our \model{} framework. 3) We conducted experiments on two different tasks: paraphrasing and text formalization. In both experiments, \model\ significantly outperforms unsupervised baseline methods. Moreover, \model\ achieves comparable performance to recent supervised models in the paraphrasing task. 4) For text formalization , we are also the first to design a search-based  method, and further extend it into the proposed \model\ framework.  
"," In this work, we present \model{}, a novel framework to unsupervised text generation by learning from search. We start by applying a strong search algorithm  towards a heuristically defined objective that  estimates the quality of sentences. Then, a conditional generative model learns from the search results, and meanwhile smooth out the noise of search. The alternation between search and learning can be repeated for performance bootstrapping.  We demonstrate the effectiveness of \model\ on two real-world natural language generation tasks, paraphrase generation and text formalization. Our model significantly outperforms unsupervised baseline methods in both tasks. Especially, it achieves comparable performance with the state-of-the-art supervised methods in paraphrase generation.",72
" % With the rise of multi-modal studies, multi-modal neural machine translation   has become an important research direction in machine translation. % With the rise of multi-modal studies,  Multi-modal neural machine translation   has become an important research direction in machine translation, due to its research significance in multi-modal deep learning and wide applications, such as translating multimedia news and web product information . % and attracted increasing attention recently. % Multi-modal research involving computer vision and natural language processing has recently received growing interest and empower many applications, such as multi-modal neural machine translation , visual question answering and image captioning.  It significantly extends the conventional text-based machine translation by taking images as additional inputs.  The assumption behind this is that the translation is expected to be more accurate compared to purely text-based translation, since the visual context helps to resolve ambiguous multi-sense words . % the visual information can be beneficial to ground the meaning of the text and, as a consequence, generate more adequate translations . % Due to its research significance in multi-modal deep learning, multi-modal NMT has attracted increasing attention recently. % In this setting, translation is expected to be more accurate compared to purely text-based translation, as the visual context could help resolve ambiguous multi-sense words.  % Examples of real-world applications of multi-modal translation include translating multimedia news, web product information, and movie subtitles. % Also, the grounding of multiple modalities against each other may enable the model to have a better understanding of each modality individually, such as in natural language understanding applications. % Due to its research significance in multi-modal deep learning and wide applications, such as translating multimedia news, web product information and movie subtitles , multi-modal NMT has attracted increasing attention recently.  % Intuitively, there exists semantic correlation of different granularities between input text and image. Therefore, how to efficiently fuse the multi-modal semantic information has become a crucial issue in Multi-modal NMT, which directly impacts translation performance.   '' and other textual ones for simplicity. }   Apparently, %how to efficiently incorporate visual features into Multi-modal NMT has become a crucial issue, which directly impacts translation performance. how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance. To this end, a lot of efforts have been made, roughly consisting of:  encoding each input image into a global feature vector,  which can be used to initialize different components of multi-modal NMT models, or as additional source tokens ,  or to learn the joint multi-modal representation ;  extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context ; and  representing each image as spatial features,  which can be exploited as extra context , or a supplement to source semantics  via an attention mechanism.  % In some previous studies, images are encoded into global features that is used as initialization and source tokens , or to build a joint multi-modal representation .Some work use object-based image features , inspired by other multi-modal tasks like image captioning .More common practice is to represent images as spatial features and employ attention mechanism  over them to incorporate visual context into decoders . % Furthremore, to exploit multi-modal semantic correlations,  propose an encoder-based image attention mechanism, which utilizes the representations of source words to extract visual context as the supplement of source semantics. % and modeling image as latent variables . % On the other hand, recent work has analysed that images are only needed in specific cases such as ambiguous source words , or just ignored by the models .  Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image pair. For example, as shown in Figure , the noun phrase ``"" semantically corresponds to the blue dashed region. % which is important for multi-modal representation learning. The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities,  and 2) how to achieve semantic interactions based on the unified representation. % For example, as shown in Figure , the noun phrase ``"" and ``"" semantically correspond to the yellow dashed region and blue dashed region in the image, respectively. However, we believe that such semantic correspondences can be exploited to refine multi-modal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions .   % However,  % studies  reveal that visual information seems to be ignored by the multi-modal NMT models.  % The underlying reason is the big challenges imposed % by the representations of images and % their integration into the model.  % %The underlying reason is limitations of the image representations and the way they are integrated into the model. % Concretely, previous approaches do not explicitly % %of integrating image features to the models % capture semantic correspondences between semantic units within a pair of an input sentence and an image.  % For example, as shown in Figure , the noun phrase ``"" and ``"" semantically correspond to the yellow dashed region and blue dashed region in the image, respectively. % %, which is hard to be modelled in previous work.  % We believe that such semantic correspondences between inter-modal semantic units can be exploited to refine multi-modal representation, since it provides fine-grained multi-modal semantic interactions.  % Despite their success, there are still some defects.  % Concretely, the contribution of visual information is unclear .  % More importantly, these work usually do not consider semantic correspondences between fine-grained semantic units within an input sentence and an image. % The neglect of these important clues may be due to the big challenges imposed by the representations of these semantic units and their integration into the encoder. % % multi-modal representation and the integration into the encoder. % However,  % we believe that such information can be exploited to refine the encoder,  % since it provides semantic bridges between inter-modal semantic units that are very important for multi-modal semantic interactions.   %fine-grained semantic relationships that are very important for multi-modal semantic interaction. % take into account cross-modal correspondences between semantic units in two modalities. % lack the modelling of fine-grained semantic relationships between semantic units in two modalities. % The attention based methods attend over all elements  % Even though some studies use more specific object-based features, they simply incorporate them via an additional attention model. % visual objects and textual phrases. % However, this approach gives little consideration to how the image regions that are subject to attention are determined. % In most previous studies, the input image is encoded as a uniform grid of equally sized neural receptive fields, which is irrespective of the content of the sentence.  % Obviously, some textual phrases have similar semantics to objects  % in the input image. % For example, in Figure 1, the phrase 'a toy car' has semantic association with the visual object . % in images such as the phrase '' and the visual object '' in the Figure 1.  % We believe such semantic relationships can help us learn better multi-modal semantic representations since they provide fine grained semantic constraints. % Intuitively, explicitly modeling this semantic correlation make the contribution of visual modality clearer, and improve complementarity between two modalities, % which is helpful to better translation. % it is helpful to bind visual objects  % For example, the visual object '' in the image has semantic correlation to the phrase ''. % Therefore, the semantic information of input image has not been fully utilized in Multi-modal NMT and limit the potential of further translation quality improvement.   In this paper,  % Unlike previous work, we represent the input text and image as a unified multi-modal graph, which captures semantic associations between intra- and inter-modal units. % We propose a multi-modal graph to capture fine-grained semantic relationships between semantic units in two modalities. we propose a novel graph-based multi-modal fusion encoder for NMT.  % To construct this encoder, We first represent the input sentence and image with a unified multi-modal graph. In this graph, each node indicates a semantic unit:  or , and two types of edges are introduced to model semantic relationships between semantic units within the same modality  and semantic correspondences between semantic units of different modalities  respectively. % To build this encoder, we first use a multi-modal graph to model various semantic relationships between intra- and inter-modal semantic units. % fine-grained semantic relationships between semantic units in two modalities. % and introduce graph attention network to effectively learn multi-modal node embeddings, % which provide attention-based multi-modal contexts for the decoder. % utilizing a external visual grounding tool to detect visual objects associated with textual phrases. % For example, in the multi-modal graph shown in Figure , in our proposed graph, each node indicates either a textual word or a visual object, and three types of edges, including textual edges, visual edges, and inter-modal edges, are introduced to model different kinds of semantic relationships between nodes. % which capture different kinds of semantic relationships between nodes.  % The last type of edges connect the nodes that have similar semantics but in different modalities. Based on the graph, we then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions among the nodes to conduct graph encoding. %to conduct graph encoding, where semantic transitions among graph nodes are iteratively performed to learn representation vectors of semantic units in two modalities. %According to the features of multi-modal graphs, we distinguish the parameters of different modalities and deploy gate mechanism to achieve fine-grained information fusion. Particularly, during this process, we distinguish the parameters of two modalities,  and sequentially conduct intra- and inter-modal fusions to learn multi-modal node representations. % we distinguish the parameters of two modalities and deploy operations with different gating mechanisms to achieve fine-grained multi-modal fusion. Finally, these representations can be exploited by the decoder via an attention mechanism. %At each graph layer, we first learn the unimodal contextual representation of each node, so that it can determine the degree of multi-modal fusion with its own context. % semantic representations of visual objects and the input sentence. % fine-grained semantic interactions between two modalities can be full exploited to learn better the semantic representations of visual objects and the input sentence.  %Finally, we employ attention mechanism to generate context vector for the decoder.  % which separately produce attention-based multi-modal contexts for Multi-modal NMT. % so that each node collect semantic information over others.  Compared with previous models, ours is able to fully exploit semantic interactions among multi-modal semantic units for NMT. %semantic transitions among multi-modal semantic units  % efficiently capture both fine-grained semantic relationships between intra-modal and inter-modal semantic units. % and pass messages containing contextual information between a pair of bipartite sub-graphs to iteratively refine the representations.  % The nodes in one modality can collect both intra-modal and inter-modal semantic information via the cross-modal semantic alignments. Overall,  the major contributions of our work are listed as follows:  {2pt} {2pt}    
"," Multi-modal neural machine translation  aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units . We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",73
"  This short example shows a contrived example on how to format the authors' information for IJCAI--PRICAI--20 Proceedings.  
", This short example shows a contrived example on how to format the authors' information for {.,74
" }  Pre-trained word embeddings, which map words to dense vectors of low dimensionality, have been the key enabler of the ongoing neural revolution, and today they serve as the basic building blocks of the vast majority of the contemporary Natural Language Processing  models. While initially introduced for English only , pre-trained embeddings quickly emerged for a number of other languages , and the idea of cross-language embedding spaces was born. In a cross-language embedding space, two semantically similar  words would be close to  each other regardless of whether they are from the same or from different languages. Using such a space is attractive, as for a number of NLP tasks, it enables the application of an NLP model trained for one language on test input from another language. Ideally, such spaces could be trained on parallel bilingual datasets, but such resources are of limited size, e.g.,~compared to the large-scale monolingual resources typically used to pre-train monolingual word embeddings.  Thus, it has been more attractive to train monolingual word embeddings for different languages independently, and then to try to align the corresponding embedding spaces in what is commonly known as bilingual lexicon induction. This has been attempted in a supervised~, in a semi-supervised~, and in an unsupervised setting~.  Initial attempts at aligning the spaces used a dictionary of word translation pairs as anchors between the two spaces to infer the nature of the transformation that relates the first language to the second one . This is a supervised setup, where the alignment is typically done according to an orthogonal transformation that minimizes the Frobenius norm in the Procrustes problem, which has a closed-form solution, easily obtainable via SVD, as we describe below.  For the translation of word embeddings,  is taken to be an orthogonal matrix due to a self-similarity argument . The convenience of using an orthogonal matrix has also been supported empirically . The orthogonal Procrustes problem has a closed-form solution , where  is the singular value decomposition  of  as shown by~.    Given two ordered clouds of points , , each with  points of dimension , the orthogonal Procrustes problem finds the orthogonal matrix  that minimizes the following Frobenius norm:    A popular unsupervised formulation of the problem is known as the Wasserstein-Procrustes , which is more challenging as it needs to optimize a generalization of the Procrustes objective. One-to-one maps are encouraged through a permutation matrix . The convenience of one-to-one maps is justified for different reasons. First, the hubness problem  occurs in high-dimensional vector spaces where certain vectors are the nearest neighbor to a disproportionate number of other vectors, thus reducing the quality of the embedding space . Second, one-to-one maps can be linked to Wasserstein distance and computational optimal transport.  Given two clouds of points , , each with  points of dimension , the Wasserstein-Procrustes problem finds an orthogonal matrix  and a permutation matrix  that minimize the Frobenius norm:  where  is the set of -dimensional permutation matrices and  is the set of -dimensional orthogonal matrices.  In practice, even though most existing approaches resort to some modification of this objective, they nevertheless yield good accuracy for synthetically generated dictionary induction tasks. Therefore, here we ask the following questions: Can we find approximate solutions to the Wasserstein-Procrustes objective as per Equation that not only minimize the objective, but also yield good accuracy on dictionary induction tasks? Can we take existing methods and improve them further by using refinements that optimize the objective in Equation? Can we find natural scenarios for which we find good solutions? We attempt to answer these questions after a thoughtful analysis of the different objective functions used in the literature, following the call from  for a more fair model comparison.      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PROPERTIES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
"," The emergence of unsupervised word embeddings, pre-trained on very large monolingual text corpora, is at the core of the ongoing neural revolution in Natural Language Processing . Initially introduced for English, such pre-trained word embeddings quickly emerged for a number of other languages. Subsequently, there have been a number of attempts to align the embedding spaces across languages, which could enable a number of cross-language NLP applications. Performing the alignment using unsupervised cross-lingual learning  is especially attractive as it requires little data and often rivals supervised and semi-supervised approaches. Here, we analyze popular methods for UCL and we find that often their objectives are, intrinsically, versions of the Wasserstein-Procrustes problem. Hence, we devise an approach to solve Wasserstein-Procrustes in a direct way, which can be used to refine and to improve popular UCL methods such as iterative closest point , multilingual unsupervised and supervised embeddings  and supervised Procrustes methods. Our evaluation experiments on standard datasets show sizable improvements over these approaches. We believe that our rethinking of the Wasserstein-Procrustes problem could enable further research, thus helping to develop better algorithms for aligning word embeddings across languages. Our code and instructions to reproduce the experiments are available at \url{https://github.com/guillemram97/wp-hungarian}.",75
" Transferable knowledge in meta-learning is derived in the form of generalizable representation space  or optimization strategies. The target few-shot task is then  handled in a feed-forward distance function without updating network weights or learned by fine-tuning with the efficient optimization strategy.  [t] % {2pt} %  {5pt}      name & embedding function & similarity metric  & other notes\\{*}{deep neural net} & sigmoid over  &  \\  &  &weighted  distance & \\{*}{deep neural net} & \multirow{2}{*}{cosine} & the embedding function \\  &  & & depends on the support set\\{*}{deep neural net} & \multirow{2}{*}{Euclidean distance} & compare the test example with the\\  &  & &   classes rather than support examples\\{*}{deep neural net} & deep  net  & \\  &  &outputs one scalar &   \\{*}{--} & \multirow{2}{*}{multiple metrics}  & \\ %  &  & &   \\  proposed a Siamese network which takes two instances as input and outputs a scalar indicating they  belong to the same class or not. The Siamese network, trained on training tasks, is essentially a distance function. However, it does not follow the principle of meta-learning: Siamese network was neither trained specifically to minimize the test losses on training tasks nor trained to learn an efficient gradient-based algorithm.     proposed ``matching network'', the  first metric-based meta-learning algorithm, to solve one-shot problem. Matching network is essentially a parametric nearest neighbors algorithm, defined as follows:  where  is a support set containing  labeled examples \{, \},  is a text example with its gold label . So, the gradients from  to the  is omitted.   Reptile is another first-order optimization-based meta-learning, as shown in Figure . It also samples training tasks from : , , \} separations. For  training task , let's assume the original parameters  have  went through  steps of updating and become  \thetam=1m>1\mathop{\mathbb{E}}_{\tau}[\mathrm{SGD}]\mathrm{SGD}$   MAML explicitly optimizes the efficiency of the algorithm on the support set, making sure the learnt algorithm can learn fast in the few-shot examples of the target task. In contrast, Reptile tries to optimize the system so that it can work well on all training tasks---it may work well if the target task is very close to the training tasks.   [t] % {2pt} %  {5pt}      model & \multicolumn{1}{|c}{main notes}\\{\textbullet{*}{\textbullet{*}{deep neural net} & \multirow{2}{*}{Euclidean distance} & compare the test example with the\\ %  &  & &   classes rather than support examples\\{*}{deep neural net} & deep  net  & \\ %  &  &outputs one scalar &   \\{*}{--} & \multirow{2}{*}{multiple metrics}  & \\ %  &  & &   \\\hline     
","  Few-shot natural language processing  refers to  NLP tasks that are accompanied with merely a handful of labeled examples.   This is a real-world challenge that an AI system must learn to handle.  Usually we rely on collecting more auxiliary information or developing a more efficient learning algorithm. However, the general gradient-based optimization in high capacity models, if training from scratch,  requires many parameter-updating  steps over a large number of labeled examples to perform well .    % The general belief is that gradient-based optimization in high capacity classifiers, if training from scratch,  requires many iterative steps over many examples to perform well . However, due to the availability of large-scaled pretrained model such as BERT, RoBERTa, those classifiers could have relatively good initialization; in this case, fine-tuning on support set is feasible.  If the target task itself cannot provide more information, how about collecting more tasks equipped with rich annotations to help the model learning?  The goal of meta-learning is to train a model on a variety of tasks with rich annotations, such that it can solve a new  task using only a few  labeled samples. The key idea  is to train the model's initial parameters such that the model has maximal performance on a new task after the parameters have been updated through zero or a couple of gradient steps. % The process of training a model闁炽儲鐛 parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks. If the internal representation is suitable to many tasks, simply fine-tuning the parameters slightly  can produce good results % .  There are already some surveys for meta-learning, such as . Nevertheless,  this paper focuses on NLP domain, especially few-shot applications. We try to provide  clearer definitions, progress summary and some common datasets  of applying meta-learning to few-shot NLP.",76
"   In any typical Reinforcement Learning  scenario, there is an agent in an environment following a policy to take actions which make it transition through states and receive rewards. Each of these elements must be modeled according to the purpose of the task and their representation can influence the outcome of RL algorithms. Natural Language Understanding can be integrated into these components, both in language-conditional RL  and in language-assisted RL , as discussed in~.  The reward function is usually under the spotlight because the agent's goal is maximizing the expected long-term reward. But the state representation is no less vital. Based on it, the agent perceives the environment and comes to decisions on how to act. While in robotics the agent observes a spatial environment and in arcade games the state may be composed of a sequence of images, natural language is as well a common part of RL states, as illustrated in Figure.     The choice of state representation is a problem on its own. It directly affects the learning process, so if applications neglect this, the agent may be prevented from accessing key information for decision-making.  There is ongoing research on the use of natural language to model the reward and the actions, but we are not aware of any recent systematic survey about the various possibilities of using natural language to model the state representation and how to do that effectively. We aim at filling this gap by providing an overview of previous work in which the state is based on text, delineating the main approaches.  We dive into a wide range of NLP papers that apply RL methods and whose state representations have to capture linguistic features that influence decision-making. Findings about state representations in this area may potentially be extrapolated to other language-informed RL tasks. We thus hope this overview aids those seeking the objective of having RL agents capable of understanding natural language. Since we notice there is no consensus on how to design natural language state representations, we conclude with some concrete recommendations for future research.   
"," A suitable state representation	is a fundamental part of the learning process in Reinforcement Learning. In various tasks, the state can either be  described by natural language or be natural language itself. This survey outlines the strategies used in the literature to build natural language state representations. We appeal for more linguistically interpretable and grounded representations, careful justification of design decisions and evaluation of the effectiveness of different approaches.",77
" Neural machine translation  have witnessed great progress due to the development of deep learning. The popular NMT models adopt an encoder-attention-decoder framework, where the decoder generates the target token based on previous tokens in an autoregressive manner. While its popularity, NMT models suffer from discrepancy between training and inference and the consequent error propagation. During inference, the decoder predicts the next token given previous generated tokens as input, which is discrepant from that in training, where the previous ground-truth tokens as used as input for next token prediction. Consequently, the previous predicted tokens may have errors, which would cause error propagation and affect the prediction of next tokens.  Previous works have tried different methods to solve the above issues, where some of them focus on simulating the data that occurs in inference for training, such as data as demonstration, scheduled sampling, sentence-level scheduled sampling, or even predict them in different directions. While being effective to handle the prediction errors occurred in inference during model training, these methods still leverage the predicted tokens that could be erroneous as the conditional information to predict the next token. Forcing the model to predict correct next token given incorrect previous tokens could be particularly hard and misleading for optimization, and cannot solve the training/inference discrepancy as well as error propagation effectively.   In this paper, moving beyond scheduled sampling, we propose a novel method to enable the model to correct the previous predicted tokens when predicting the next token. By this way, although the decoder may have prediction errors, the model can learn the capability to build correct representations layer by layer based on the error tokens as input, which is more precise for next token prediction than directly relying on previous erroneous tokens as used in scheduled sampling.   Specifically, we introduce two-stream self-attention, which is designed for language understanding in XLNet, into the NMT decoder to correct the errors while translation. Two-stream self-attention is originally proposed to solve the permutation language modeling, which consists of two self-attention mechanisms: the content stream is exactly the same as normal self-attention in Transformer decoder and is used to build the representations of the previous tokens, while the query stream uses the positional embedding as the inputs to decide the position of the next token to be predicted. In our work, we reinvent two-stream self-attention to support simultaneous correction and translation in NMT, where the content stream is used to correct the previous predicted tokens , and the query stream is used to simultaneously predict the next token with a normal left-to-right order based on the corrected context .   We conduct experiments on IWSLT 2014 German-English, Spanish-English, Hebrew-English and WMT 2014 English-German and English-Romanian translation datasets to evaluate the effectiveness of our proposed error correction mechanism for NMT. Experimental results demonstrate that our method achieves improvements over Transformer baseline on all tasks. Further experimental analyses also verify the effectiveness of error correction to improve the translation accuracy.  Our contributions can be summarized as follows:         
"," Neural machine translation  generates the next target token given as input the previous ground truth target tokens during training while the previous generated target tokens during inference, which causes discrepancy between training and inference as well as error propagation, and affects the translation accuracy. In this paper, we introduce an error correction mechanism into NMT, which corrects the error information in the previous generated tokens to better predict the next token. Specifically, we introduce two-stream self-attention from XLNet into NMT decoder, where the query stream is used to predict the next token, and meanwhile the content stream is used to correct the error information from the previous predicted tokens. We leverage scheduled sampling to simulate the prediction errors during training. Experiments on three IWSLT translation datasets and two WMT translation datasets demonstrate that our method achieves improvements over Transformer baseline and scheduled sampling. Further experimental analyses also verify the effectiveness of our proposed error correction mechanism to improve the translation quality.",78
"  % ---------------------------  Paraphrase identification is a core NLP task and has been widely studied . One interesting application area of paraphrase detection is Community Question Answering  .  The aim of CQA is to answer real open-ended questions based on user-generated content from question answering websites. Being able to identify similar --- already answered --- questions  can be helpful for this purpose. Question paraphrase detection in CQA is difficult because texts tend to be longer and have less direct overlap compared to traditional paraphrase detection datasets .  Early work on paraphrase detection relied on hand-crafted features, while state-of-the-art approaches for paraphrase identification are primarily neural networks  and hybrid techniques . Many recently proposed CQA paraphrase detection systems still use hand-crafted features  and some work has successfully integrated topic model features . This suggests that topic distributions could offer auxiliary information for identifying related questions and complement word embeddings , which provide the main signal in neural systems.  Contrary to hand-crafted static topic features, integrating topics in a neural framework brings the advantage of joint updates during training.  Recent work successfully introduced topics in neural architectures for language generation:  used a topic-enhanced encoder for summarisation,  integrated topics in the decoder for machine translation and  included topics in both encoder and decoder of their summarisation model.  However, it remains unclear if topics can be useful in a neural paraphrase detection model and how to best fuse topics with word embeddings for this task.  In this paper, we introduce a novel topic-aware neural architecture and specifically make the following contributions:    % --------------------------- 
"," Question paraphrase identification is a key task in Community Question Answering  to determine if an incoming question has been previously asked. Many current models use word embeddings to identify duplicate questions, but the use of topic models in feature-engineered systems suggests that they can be helpful for this task, too. We therefore propose two ways of merging topics with word embeddings  in a new neural architecture for question paraphrase identification. Our results show that our system outperforms neural baselines on multiple CQA  datasets, while an ablation study highlights the importance of topics and especially  topic-embedding fusion in our architecture.",79
"  %RNNLM 闉涖儸鐖 姘 闊歌建顫 Recently, the Recurrent Neural Network Language Model  has gained its popularity in the field of Automatic Speech Recognition .  %G The 闈奉叆鐗, research闆 闆碱煆鍨 Various academic research has reported the effectiveness of RNNLMs, which can train unseen contexts by sharing the statistics between words  %G of which contexts -> whose contexts are / the contexts of which are with syntactically and semantically similar contexts. However, heavy computational load of RNNLM over traditional n-gram based approaches  %G keep it from ... it闉 闉氭﹤顕爟姗傚 姘氭棄鐨 闉愬棦娼.. %G an every area -> every area has been a hurdle in applying the RNNLM to diverse areas of ASR applications.  %G if -> when,  to be run -> to run..  %G 闉愵剣鐖犻爩 when/if 鐢戭剢顑撻灇 闉涘牕濮 鐡 闉涙劤娉婅嚙 闉忔尗鍎.. Especially, when ASR systems are required to run under real-time constraint ,  %G hardly be attainable... 闉佹粚鐗? the real-time decoder is hardly attainable with direct application of RNNLMs in place of traditional n-grams.  %G 姘嶈兂鐏 鏀靛嫴螠.. 闉氳導绉 闈炬﹥鐖.. In order to overcome such computational issues, most of the RNNLN systems adopt two-pass decoding strategy, which generates lattices or a set of n-best results based on n-gram in the first path,  %G which鑷 闉愵剣鐖犻爩 娆ゆ鏌... and then performs the rescoring on the hypotheses with RNNLMs.  %Limitation of previous attempts: 1-pass cache - done %G investigate闆 闊渻娆栧亾  Prior studies have investigated the possibility of implementing real-time decoder with RNNLM. %G which闆 RNNLM? studies? -> 鏀靛嫴螠 The study reduced computational complexity of the RNNLMs by caching the conditional probabilities of the words and  %G results of feed-forwardings? the results of RNN computation and reusing the cached data.  %G it闉 姝嗘梼顓︽櫩闆 鐡? However, even though the computational load was minimized   %G In manner of reducing? by introducing cache strategy and reducing redundant computations,  %G it闉 姝嗘梼顓︽櫩闆 姘氭棄鐨 闉愬棦娼.. %G away from -> far from %G achieving decoder? the 闈奉叆鐗.. the result was still far from achieving real-time performance with large vocabulary based RNNLM.  %The strength of GPGPU in ASR %G on the other hand... 闉忔尗鍎 %G the wide range of ASR... 闉愵叆瀚 闉忔尗鍎.. the 闈奉叆鐗 姘 area/field鎼 姘ょ摯.. Recent studies have applied the General Purpose Graphic Processing Units  in various fields of ASR. %G They闆 闆稿嫳鎯? One of the studies applied the GPGPU to training RNNLMs, and showed that the outstanding parallelization capability of GPGPU was suitable in minimizing the computational load of probability normalization processes.  %Main Problem %G possibility of RNNLM? In this paper, we investigate the possibility of implementing a GPGPU-based real-time Large Vocabulary Continuous Speech Recognition  that utilizes RNNLM.  %G Get remedy of computation load... what it requires... %In order to get remedy of computation load of it, we introduce the use of GPGPUs and what it requires under the many core framework. %G ability -> capability Even though GPGPUs have powerful parallelization capabilities, obstacles such as their insufficient memory size and slow data transfer speed between GPGPUs and CPUs  %G RNN-based real-time decoder闉 闉婂崐濮呯摯... 闋... discourage the use of GPGPUs among RNNLM-based real-time decoders. %G the relatively闉愭劤鍔 the 闈告繊鐏.. %G CPU闉愭劤鍔 姘嶆尗妫侀灇 闉忔帾鏌ｇ摯 鐡垮嫵鍋橀爟姗傚姝嗚嚙 闉濆嫶姒 闉愬棦娼... %G 闉 闉庣儎娼 闆尗姣勬 姘ゆ帾鈹 姣靛牗绠 RNNLM 鐡垮嫵鍋 闉涙劤娉婃 GPU闉愭劤鍔 闋冩﹤濮 鐡村喒婧傛惪 姘ゆ尗婢婃棶鍗婂及... %Moreover, relatively slower speed of computations on CPUs could make whole processes slow down since GPGPUs have to wait until the computations on CPUs are finished even if GPGPUs have done their works. Moreover, it is also important to balance the computation time between GPGPU and CPU, as the acceleration on GPGPU may not have a prominent impact on the overall speed if the GPGPU needs to wait for the CPU computation to finish.  %Main Idea %G GPGPU闉 闆碱煆鐖灇 闋冩悡鑸堕爟婊婂珶闆 闊挎粛妲冮灇 鑶 闉庡嫴濯界摨 鑷ф瑬濮烽浖鍫у珶... 闊搁椇纭犻爩 姘嶈兂鐗呰嚙 闉涘牕濮 鐡村喐寮 闉庡嫴濯归浖鍫︾... 褰顭庢緯姣 optimize闇涜導鈹 姘囶煄銈兼棷... %G 闈奉剣瀚熼澑 RNNLM 闉涙劤娉婇瀽 GPGPU姣 闉濅緟姣勯爟姗傚 鐡村喐寮 闉庡嫴濯圭摽... 鏀撮附鐓摽鐘界倰姘 闉庣儎娼 RNNLM Training闉 GPGPU姣 闉濅緟姣勯爟 闉撳牕銈 闉忔垐绗侀爟 闉氭尗婀㈤渻 闉愬棦妫冮灇鍕愁潊闇... %We try to apply GPGPUs to RNNLM-based network search by solving the disadvantages of GPGPUs. In order to achieve real-time decoding of RNNLM-based LVCSR, we apply on-the-fly rescoring of RNNLM to GPGPU based network traversal technique proposed in . %G The goal闉氭帾婢曢浕 鐡村喒婢 姘囶煇妯冮爟姗冩３ 闉庡嫵妲 闈告繊鐏ラ爟姗冩闉鎯﹀闆. We accelerate the speed of data exchange between the two heterogeneous processors and reduce redundant computations on CPUs by applying cache strategies. %G a real-time speed -> real-time speed The resulting recognition system has shown almost twice faster than real-time speed  %G 鏀撮附鍎 in various circumstances闇涚紕纰 闋冩﹤濯规棷 姣靛韩娼 鐡村眾姣庨瀽 鏀撮附鐏涢浖 闇涜導濮 闆绘劜鍊㈤灇 闇屾粖濮呯摯鍐稿及 鑷ф瑬鏅為澒... when experimented under various conditions, while maintaining relatively 10\% lower Word Error Rate  than that of conventional n-gram models.  % Papaer oraganization This paper is organized as the following. In section 2, the structure of RNNLMs is explained.  Section 3 explains how we applied GPGPUs to RNNLM-based network search. Section 4 explains the RNNLM rescoring with caches. Section 5 evaluates the improvement of the proposed method, followed by the conclusion in Section 6. %REVIEW 3-2) 鏀撮附顬 1 闈炬﹥鐖   	     %Method   
"," 	Recurrent Neural Network Language Models  have started to be used in various fields of speech recognition due to their outstanding performance.	 	However, the high computational complexity of RNNLMs has been a hurdle in applying the RNNLM to a real-time Large Vocabulary Continuous Speech Recognition . 	In order to accelerate the speed of RNNLM-based network searches during decoding, we apply the General Purpose Graphic Processing Units . 	This paper proposes a novel method of applying GPGPUs to RNNLM-based graph traversals. 	We have achieved our goal by reducing redundant computations on CPUs and amount of transfer between GPGPUs and CPUs. 	The proposed approach was evaluated on both WSJ corpus and in-house data. 	Experiments shows that the proposed approach achieves the real-time speed in various circumstances while maintaining the Word Error Rate  to be relatively 10\% lower than that of n-gram models.",80
"   Metaphor as a figure of speech has a widespread presence in any form of communication either oral or written. According to Steen  data analysis shows that, on average, one in every seven and a half lexical units in the corpus is related to metaphor  However, it is difficult to clearly define the boundaries that separate metaphor from literal uses, as well as metaphor from other figures of speech.  The difficulty of clearly establishing a theoretical background for metaphor justifies the variety of NLP systems that aim at automatically between distinguishing between metaphorical and literal meanings of a word or phrase. This difficulty is further exacerbated if we take into account the limitations of Greek as regards resources and tools for metaphor detection; thus, we can conclude that the development of neural language models is necessary for the automatic differentiation between literal and metaphorical meaning of phrases that are part of an authentic and non-annotated Greek corpus. For these reasons, our attempt here is based on the principles of distributional semantics so as to determine the relations of a word with its linguistic context and to group semantic similarities of linguistic items based on distributional properties rather than any connections of the certain term and its related concepts. Distributional semantics  have been paramount in shifting research interest towards neural language models, which can attribute hidden statistical characteristics of the distributed representations of word sequences in natural language. Therefore, a serious problem such as the automatic detection of metaphors and their differentiation from literal uses can be dealt with the development of neural language models.      
"," %\boldmath This paper presents and benchmarks a number of end-to-end Deep Learning based models for metaphor detection in Greek. We combine Convolutional Neural Networks and Recurrent Neural Networks with representation learning to bear on the metaphor detection problem for the Greek language. The models presented achieve exceptional accuracy scores, significantly improving the previous state of the art results, which had already achieved accuracy 0.82. Furthermore, no special preprocessing, feature engineering or linguistic knowledge is used in this work. The methods presented achieve accuracy of 0.92 and F-score 0.92 with Convolutional Neural Networks  and bidirectional Long Short Term Memory networks . Comparable results of  0.91 accuracy and  0.91 F-score are also achieved with bidirectional Gated Recurrent Units  and Convolutional Recurrent Neural Nets .  The models are trained and evaluated only on the basis of the training tuples, the sentences and their labels. The outcome is a state of the art collection of metaphor detection models, trained on limited labelled resources, which can be extended to other languages and similar tasks.",81
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Sentiment analysis has gained importance in the current world where social media is crucial in gauging public opinion on products, political campaigns, latest trends and more. A large portion of the world's population is multilingual, and so is the content on social networks. Code-mixing is a natural phenomenon among bilinguals where phrases and words from one language are employed in another. Typically, the underlying grammar of the primary language that is being spoken is kept intact and phrases from another language are embedded into it. India in particular, has 23 officially recognized languages and a majorly bilingual population, requiring robust computational tools to effectively exploit the data it produces.  Deep neural networks have had great success in predicting sentiment encapsulated in text - a big part of this success has been their ability to utilize pretrained word embeddings. In code-mixed tweets however, Hindi words are written in Roman script instead of its native Devanagari script, forcing one to use a transliterator in order to take advantage of pretrained aligned word embeddings, like fastText . These words in Roman script are unnormalized and may have multiple spellings; for example, bahut can be spelt in Roman script as bahut, bohot, bohut etc., and such errors propagate from the transliterator to our downstream task, thereby reducing performance.   In this work, we propose a deep convolution network with self-attention which shows promising results, without any pretraining. Convolution neural networks  are known to capture local relations  or local context, and work here as feature extractors acting as a substitute for handcrafted sentiment features. A self-attention layer is then applied over these features , which allow each individual feature to attend to all other features , providing global context. We call this the Hierarchical Context Modeling System , due to the hierarchical nature of context extraction performed on both levels.   
","   Problems involving code-mixed language are often plagued by a lack of resources and an absence of materials to perform sophisticated transfer learning with. In this paper we describe our submission to the Sentimix Hindi-English task involving sentiment classification of code-mixed texts, and with an F1 score of 67.1\%, we demonstrate that simple convolution and attention may well produce reasonable results.",82
" Knowledge Bases are collection of factual information in the form of relational triplets. Each relational triplets can be represented as  where   and  are entities in knowledge base and r is the relation between  and . The most popular way of visualising knowledge base is by representing them as multi relational graph where each triplet  is represented as directed edge from  to  with label r. Knowledge Bases have used to improve performance across variety of tasks like Question Answering , Dialogue Generation  and many others.  However, since Knowledge Bases are populated from automatic mining from texts, they are often incomplete since it is not possible to manually write all the facts, and there are often inaccuracies in extraction. This inaccuracy leads to a decline in performance across a variety of downstream tasks. Hence, there has been a lot of work in coming up with an efficient tool to complete the Knowledge Bases  by automatically adding new facts without requiring extra knowledge. This task is referred to as Knowledge Base Completion , where the goal is to solve queries like  .    The first approach towards efficient Knowledge Base Completion were additive models like TransE  and TransH  where relations were interpreted as simple translations over hidden entity representations. Multiplicative models like Distmult  and Complex  were then observed to outperform these simple additive models.  Instead of translation, RotatE  defines relation as simple rotations such that the head entity can be rotated in the complex embedding space to match the tail entity, which has been shown to satisfy a lot of useful semantic properties like compositionality of relations. Recently, more expressive Neural Network-based methods  and ConvKB) were introduced where the scoring function is learned along with the model. However, all these models process each triplet independently. As a result, these methods cannot capture semantically rich neighborhood and hence produce low-quality embeddings.  Graphs have been widely used to visualize real-world data. There has been tremendous progress in applying ML techniques over images and text, some of which are being successfully adapted to graphs (like , , . Taking inspiration from this approach, a number of Graph Neural Network-based methods have been proposed to capture neighborhood in Knowledge Graphs for the KBC task. In this survey, we aim to look into some of these formulations.  
"," Knowledge Graphs are increasingly becoming popular for a variety of downstream tasks like Question Answering and Information Retrieval. However, the Knowledge Graphs are often incomplete, thus leading to poor performance. As a result, there has been a lot of interest in the task of Knowledge Base Completion. More recently, Graph Neural Networks have been used to capture structural information inherently stored in these Knowledge Graphs and have been shown to achieve SOTA performance across a variety of datasets. In this survey, we understand the various strengths and weaknesses of the proposed methodology and try to find new exciting research problems in this area that require further investigation.",83
"   . 	%  	% % final paper: en-us version  	 	  % space normally used by the marker 	This work is licensed under a Creative Commons  	Attribution 4.0 International License. 	License details: 	\url{http://creativecommons.org/licenses/by/4.0/}. }  Sentiment Analysis identification is a sub-field of natural language processing that explores the automatic inference for fine-grained opinion polarity of textual data. The recent growth of social media, review forums and text messaging platforms create a surge in the amount of user-generated textual data, and so increased the urgent need for automatic opinion extraction. However this evolution created many opportunities for language technology and researchers, it provided verity of new challenges, namely, spelling errors, creative invented spelling , abbreviation , Meta tags and code-mixing . Non-English speakers frequently use multiple languages to express their feelings, which they know as code-mixing.  Speakers or writers tend to shift from one language to another either to express their feelings adequately, to show like-mindedness with a group, to distinguish oneself, to discuss a specific topic, or to look impressive to their audience. The SemEval-2020 shared task 9 , which is part of the SemEval-2020 workshop, focused on coping with this challenge. This task focus is automatic sentiment analysis in a code-mixed social media text. The task consists of two subtasks, Spanish-English and Hindi-English code-mixing, with over 30 groups who participate in each sub-task.   This article presents a system that we have implemented for predicting the sentiment of a given code-mixed tweet. The system was developed for both subtasks. Table 1 shows the results that we achieved with our system in the SemEval-2020 competitions. To create a highly accurate classifier, we tested different methods that varied from linear  and different deep learning architectures . Also, we tested a new architecture based on Wang's   work on offensive language detection, who used four Convolutional Neural Networks with different window sizes and k max-pooling ahead of them. However, the combination of Term Frequency-Inverse Document Frequency embedding and NBSVM yield the best result.  [t!] 	 		{|c|cl|} 			 &  \\ 			\hline 		 	 	   
"," 	Sentiment Analysis is a well-studied field of Natural Language Processing. However, the rapid growth of social media and noisy content within them poses significant challenges in addressing this problem with well-established methods and tools. One of these challenges is code-mixing, which means using different languages to convey thoughts in social media texts. Our group, with the name of IUST, participated at the SemEval-2020 shared task 9 on Sentiment Analysis for Code-Mixed Social Media Text, and we have attempted to develop a system to predict the sentiment of a given code-mixed tweet. We used different preprocessing techniques and proposed to use different methods that vary from NBSVM to more complicated deep neural network models. Our best performing method obtains an F1 score of 0.751 for the Spanish-English sub-task and 0.706 over the Hindi-English sub-task.",84
" A recent estimate of the total number of English research articles available online was at least 114 million . Studies indicate the number of academic papers doubles every 10--15 years . The continued growth of scholarly papers can make finding relevant research papers challenging. Searches based on only keywords may no longer be the most efficient method  to use. This often happens when the same query terms appear in multiple research areas. For example, querying ``neuron'' in Google Scholar returns documents in both computer science and neuroscience. Search results can also belong to diverse domains when the query terms contain acronyms. For example, querying ``IR'' returns documents in Computer Science  and Physics . Similarly, querying ``NLP'' returns documents in Linguistics  and Computer Science . This is because documents in multiple subject categories  are often mixed together in a digital library search engine and its corresponding SC metadata is usually not available in the existing document metadata, either from the publisher or from automatic extraction methods.   As such, we believe it can be useful to build a classification system that assigns scholarly papers to SCs. Such an system could significantly impact scientific search and facilitate bibliometric evaluation. It can also help with Science of Science research , a recent area of research that uses scholarly big data to study the choice of scientific problems, scientist career trajectories, research trends, research funding, and other research aspects.  Also, many have noted that it is difficult to extract SCs using traditional topic models such as Latent Dirichlet Allocation , since it only extracts words and phrases present in documents.  An example is that a paper in computer science is rarely given that label in the keyword. In contrast, SC classification is usually based on a universal schema for that a specific domain or for all domains such as that of the Library of Congress. A crowd sourced schema can be found in the DBpedia of Wikipedia. %%% need a citation here %Classifying documents into SCs entails labelling the document with the subject domain that best describes an article's content at. This helps organizing and indexing digital collections and assists users in narrowing  search results. Many online retailers have implemented a faceted search feature, e.g., Amazon music search. However, similar features are not yet seen in scholarly digital library search engines.    In this work, we pose the SC problem as one of multiclass classification in which {. The core component is a supervised classifier based on recurrent neural networks trained on a large number of labeled documents that are part of the WoS database. In comparison with our preliminary work, our data is more heterogeneous , imbalanced, and complicated . We compare our system against several baselines applying various text representations, machine learning models, and/or neural network architectures.   Many schemas for scientific classification systems are publisher domain specific. For example, ACM has its own hierarchical classification system, NLM has medical subject headings, and MSC has a subject classification for mathematics. The most comprehensive and systematic classification schemas seem to be from WoS and the Library of Congress . The latter was created in 1897 and was driven by practical needs of the LOC rather than any epistemological considerations and is most likely out of date.  To the best of our knowledge, our work is the first example of using a neural network to classify scholarly papers into a comprehensive set of SCs. Other work focused on unsupervised methods and most were developed for specific category domains. In contrast, our classifier was trained on a large number of high quality abstracts from the WoS and can be applied directly to abstracts without any citation information. We also develop a novel representation of scholarly paper abstracts using ranked tokens and their word embedding representations. This significantly reduces the scale of the classic Bag of Word  model. We also retrained FastText and GloVe word embedding models using WoS abstracts. The subject category classification was then applied to the CiteSeerX collection of documents.  However, it could be applied to any similar collection.  
","  %%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details. % Subject categories of scholarly papers generally refer to the knowledge domain to which the papers belong, examples being computer science or physics. Subject category information can be used for building faceted search for digital library search engines. This can significantly assist users in narrowing down their search space of relevant documents. Unfortunately, many academic papers do not have such information as part of their metadata. Existing methods for solving this task usually focus on unsupervised learning that often relies on citation networks. However, a complete list of papers citing the current paper may not be readily available. In particular, new papers that have few or no citations cannot be classified using such methods. Here, we propose a deep attentive neural network  that classifies scholarly papers using only their abstracts. The network is trained using 9 million abstracts from Web of Science . We also use the WoS schema that covers 104 subject categories.  %The abstracts are represented by a fix-length vector, which is generated by concatenating retrained top frequency word vectors.  The proposed network consists of two bi-directional recurrent neural networks followed by an attention layer. We compare our model against baselines by varying the architecture and text representation. Our best model achieves micro-${F_1}$ measure of $0.76$ with $F_1$ of individual subject categories ranging from $0.50$--$0.95$. The results showed the importance of retraining word embedding models to maximize the vocabulary overlap and the effectiveness of the attention mechanism. The combination of word vectors with TFIDF outperforms character and sentence level embedding models. We discuss imbalanced samples and overlapping categories and suggest possible strategies for mitigation. We also determine the subject category distribution in CiteSeerX by classifying a random sample of one million academic papers.     \tiny   %All article types: you may provide up to 8 keywords; at least 5 are mandatory.",85
" .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }   One of the concerns of SemEval-2020 Task 5: Modelling Causal Reasoning in Language: Detecting Counterfactuals  is to research the extent to which current state-of-the-art systems can detect counterfactual statements.  A counterfactual statement, as defined in this competition, is a conditional composed of two parts.  The former part is the antecedent -- a statement that is contradictory to known facts. The latter is the consequent -- a statement that describes what would happen had the antecedent held.  
",   This paper describes BUT-FIT's submission at SemEval-2020 Task 5: Modelling Causal Reasoning in Language: Detecting Counterfactuals. The challenge focused on detecting whether a given statement contains a counterfactual  and extracting both antecedent and consequent parts of the counterfactual from the text . We experimented with various state-of-the-art language representation models .  We found RoBERTa LRM to perform the best in both subtasks. We achieved the first place in both exact match and F1 for Subtask 2 and ranked second for Subtask 1.,86
" As a cross-disciplinary study, we combine general linguistics with a computational linguistic approach. Various types of word embedding models are proposed to analyze large size corpora of languages . By way of illustration, word embeddings combined with artificial neural networks reflect one  aspect available to language processing in the human mind. Nevertheless, these innovative methods face the difficulty that ``purely data-driven approaches still struggle to reach the linguistic depth of their knowledge-driven predecessors. Bridging the gap between both types of approaches is therefore an important future research direction'' . Hence, we selected a linguistically motivated classification of words i.e., nominal classification , as a case study to demonstrate that the knowledge provided by linguistic theories concord with the information encoded into the basic statistical structures such as word embeddings.  More specifically, we selected Swedish since the observations with regard to L1 and L2 acquisition of nominal classification systems  in Swedish are controversial and differ from other languages.  First, monolingual children acquire Swedish grammatical gender with nearly no errors , which is considered rare in comparison to other gender languages, for which ``children's acquisitional paths have been reported not to be quite so error-free"" . Moreover, gender assignment on Swedish nouns via their phonological form or semantics is generally considered as unpredictable , which makes this observation even more unexpected. Second, while L1 acquisition display a lack or errors, L2  learners do encounter difficulties, suggesting that different strategies are employed . Hence, the existing linguistic analysis could provide additional perspectives to a computational approach and help to further understand which elements in Swedish are problematic in terms of grammatical gender perception. Moreover, matching the performance of an artificial neural network to linguistic observation made on humans  also represents an insightful comparative study, since simulating one facet of the learning process of the brain with artificial neural networks ``have become a subject of intense interest to scientists spanning a broad range of disciplines including psychology, physics, mathematics, computer science, biology and neurobiology閳 .  Thus, we propose the following research questions: 1) Can a word embedding model combined with artificial neural networks interpret grammatical gender in Swedish? 2) What types of error are made by the computational model and can we explain these errors from a linguistic perspective? Our experiment relies on two main sources of data, a corpus of Swedish raw sentences and a list of nouns affiliated to grammatical genders. The raw corpus is used train the word embedding model. The output of this model is a set of vectors associated with all words in the corpus. The dictionary is used to filter out non-noun words  and affiliate the vector of nouns with grammatical genders. These word vectors affiliated with their grammatical genders are then used to train a neural network which takes word vectors as input and determine their grammatical genders as output. The results of the network are then analyzed from a linguistic perspective.  The contributions of this research can be summarized as follow. First, it formulates a novel classification task to evaluate word embeddings. Second, it proposes a computational approach to compare with previous linguistic observations on Swedish. Finally, it also provides an in-depth linguistic analysis for the errors made by the classifier, i.e. neural network.   With regard to the general structure of this paper, 鎼 introduces the literature review on grammatical gender and computational models. 鎼 presents our methodology and our data. 鎼 elaborate the numerical results obtained from the neural network and provide a linguistics insight about the errors. 鎼 contains the detailed answers to our two research questions. Finally, 鎼 summarizes our findings as the conclusion.  
"," We analyze the information provided by the word embeddings about the grammatical gender in Swedish. We wish that this paper may serve as one of the bridges to connect the methods of computational linguistics and general linguistics. Taking nominal classification in Swedish as a case study, we first show how the information about grammatical gender in language can be captured by word embedding models and artificial neural networks. Then, we match our results with previous linguistic hypotheses on assignment and usage of grammatical gender in Swedish and analyze the errors made by the computational model from a linguistic perspective.",87
" Large-scale generative language models  have received recent attention due to their high-quality open-ended text generation ability~. Generating texts from these LMs usually relies on some form of random sampling. Pure sampling often leads to incoherent and low-quality texts , whereas greedy decoding leads to excessive repetitions, another form of low quality. The right decoding algorithm is needed to generate high-quality texts with controlled attributes .  We introduce mirostat,\footnote{The word mirostat is derived from  which is Latin for  and  meaning control.} a neural text decoding algorithm that  the generative process to maintain the perplexity of generated text at a certain desired value. Mirostat uses an adaptive top- sampling algorithm to actively tune the value of  which helps maintain the overall perplexity of the text; recall that top- sampling  is where the next word is sampled from the top  most probable choices.  Top- sampling and several other recent sampling methods are motivated by suppressing an unreliable tail in the probability distribution of trained LMs.  Another sampling method is top-, also known as , where the next word is chosen from the top  probable choices, where  is the smallest integer such that their cumulative probability mass is at least  . While top- sampling involves a fixed number of most probable choices, top- sampling involves a dynamic number of choices based on a fixed  value and shows better statistical and human-evaluated performance. For small values of  and , these sampling methods unfortunately repeat phrases in generated text. This can be handled by penalizing repetitions and using appropriate  values  or adding diversity to the generated text . On the other hand, large values of  and  can lead to incoherent texts similar to pure sampling. Although choosing appropriate values of  or  can avoid repetition and incoherence, this involves ad hoc tuning of parameters. Even for a fixed value of  or , the generated text can have varying statistical properties.   Intriguingly, as we demonstrate via Example  in Appendix, small values of a certain perplexity statistic of generated texts called   are closely linked to repetitions and large values of surprise are linked to incoherence.  Perplexity is a statistical metric used to evaluate quality of neural text generation, and is closely related to average surprise as shown in Fig. in Appendix and formalized in Sec.. A large-scale human subject experiment by  showed human-evaluated quality is closely related to the likelihood of the generated text for fixed number of tokens. In particular, reducing perplexity increases quality upto some point before the quality starts dropping. This implies that good control over perplexity of the generated text would give direct control over the quality of generated text . Generating texts with an appropriately chosen target perplexity value may maximize quality of generated text.  .    Pure sampling from LMs often leads to incoherent text whereas greedy decoding leads to repetitions. Distorting probability distributions, as in top-, top-, or temperature sampling help improve quality of generated texts, if parameters are properly tuned . Tuning these methods, however, is ad hoc and does not provide good control over the statistics of the output. Our method uses statistics of previously-generated tokens as input to generate the next token, by distorting the probability distribution so it helps control the overall statistics of the generated text. This ability to control the perplexity of the output  is a key advantage of our method over previous work. This, when used with the relation between perplexity and human-evaluated quality observed by , can yield text that has better quality control.   Controllable text generation has oft focused on semantics of the output text, as in LMs like CTRL , and sampling algorithms like plug-and-play LM  and constrained sentence generation by Metropolis-Hastings . Contrarily our approach is purely statistical, guiding the decoder along a desired statistical path that addresses issues with pure sampling and greedy decoding.   %A new model with 1.63 billion parameters, CTRL, was trained to generate text based on a control word.  %On the other hand, sampling algorithms like Plug and Play Language Model  and Constrained sentence Generation by Metropolis-Hastings  work at the inference stage on top of a pretrained language model to control certain attributes of the generated text. %PPLM shows that using attribute classifiers on top of pretrained language models helps control text generation . %CGMH uses Metropolis-Hastings sampling to generate text with certain constraints like appearance of multiple keywords %%%%%%%%%  : SeqGAN    %Distorting probability distributions for decoding using  Top-, top-, and low-temperature sampling improve the quality of the text, but at the cost of reduced diversity. Applications like question-answering only demand high-quality generation, but open-ended tasks such as story generation demand diversity too.  propose variants of beam search to induce diversity in generated text. However,  observe a tradeoff between quality and diversity; they further observe diversity is closely related to entropy whereas quality is maximized in a certain range of observed likelihood values for fixed-length sentences. Our algorithm well-controls observed cross-entropy, the observed likelihood per token of generated text. Hence, by maintaining the observed cross-entropy in a certain range, we can ensure high-quality text generation.   Greedy decoding from LMs often lead to texts with excessive repetitions both at token- and sentence-levels. Several techniques have been proposed to address this. Token loss dynamic reweighting  hypothesizes  some tokens are more difficult to learn than others and so reweighting tokens during learning can balance things to reduce repetitions~.   use a repetition penalty in decoding to reduce repetition of tokens.  suggest the cause for repetitions is a flaw in the training objective itself and use a new objective that gives less probability to unlikely sequence including texts with high repetitions. Variants of top- sampling and repetition penalty in  were used before by  to reduce repetitions. Here, we demonstrate a near-linear relation between repetitions and observed cross-entropy and so we directly control repetitions by controlling observed cross-entropy.  
"," Neural text decoding algorithms strongly influence the quality of texts generated using language models, but popular algorithms like top-$k$, top-$p$ , and temperature-based sampling may yield texts that have objectionable repetition or incoherence.  Although these methods generate high-quality text after ad hoc parameter tuning that depends on the language model and the length of generated text, not much is known about the control they provide over the statistics of the output.  This is important, however, since recent reports show that humans prefer when perplexity is neither too much nor too little and since we experimentally show that cross-entropy  has a near-linear relation with repetition. First we provide a theoretical analysis of perplexity in top-$k$, top-$p$, and temperature sampling, under Zipfian statistics. Then, we use this analysis to design a feedback-based adaptive top-$k$ text decoding algorithm called  that generates text  with a predetermined target value of perplexity without any tuning. Experiments show that for low values of $k$ and $p$, perplexity drops significantly with generated text length and leads to excessive repetitions . Contrarily, for large values of $k$ and $p$, perplexity increases with generated text length and leads to incoherence . Mirostat avoids both traps. Specifically, we show that setting target perplexity value beyond a threshold yields negligible sentence-level repetitions. Experiments with human raters for fluency, coherence, and quality further verify our findings.",88
"  \pheadNoSpace{Background} Text classification has become a fundamental building block in modern information systems, and there is an increasing need to be able to classify texts in a wide range of languages. However, as organizations target an increasing number of markets, it can be challenging to collect new task-specific training data for each new language that is to be supported.  To overcome this, cross-lingual systems rely on training data from a source language to train a model that can be applied to entirely different target languages , alleviating the training bottleneck issues for low-resource languages.  Traditional cross-lingual text classification approaches have often relied on translation dictionaries, lexical knowledge graphs, or parallel corpora to find connections between words and phrases in different languages . Recently, based on deep neural approaches such as BERT , there have been important advances in  learning joint multilingual representations with self-supervised objectives .  These have enabled substantial progress for cross-lingual training, by mapping textual inputs from different languages into a common vector representation space . With models such as Multilingual BERT , the obtained vector representations for English and Thai language documents, for instance, will be similar if they discuss similar matters.  Still, recent empirical studies  show that these representations do not bridge all differences between different languages. While it is possible to invoke multilingual encoders to train a model on English training data and then apply it to documents in a language such as Thai, the model may not work as well when applied to Thai document representations, since the latter are likely to diverge from the English representation distribution in subtle ways.  In this work, we propose a semi-supervised adversarial perturbation framework that encourages the model to be more robust towards such divergence and better adapt to the target language. Adversarial training is a method to learn to resist small adversarial  perturbations that are added to the input so as to maximize the loss incurred by neural networks . % Nevertheless, the gains observed from adversarial training in previous work have been limited, because it is merely invoked as a form of monolingual regularization. Our results show that adversarial training is particularly fruitful in a cross-lingual framework that also exploits unlabeled data via self-learning.  \pheadWithSpace{Overview and Contributions} Our model begins by learning just from available source language samples, drawing on a multilingual encoder with added adversarial perturbation. Without loss of generality, in the following, we assume English to be the source language. After training on English, subsequently, we use the same model to make predictions on unlabeled non-English samples and a part of those samples with high confidence prediction scores are repurposed to serve as labeled examples for a next iteration of adversarial training until the model converges.  The adversarial perturbation improves robustness and generalization by regularizing our model. At the same time, because adversarial training makes tiny perturbations that barely affect the prediction result, the perturbations on words during self-learning can be viewed as inducing a form of code-switching, which replaces some original source language words with potential nearby non-English word representations.  Based on this combination of adversarial training and semi-supervised self-learning techniques, the model evolves to become more robust with regard to differences between languages. We demonstrate the superiority of our framework on Multilingual Document Classification   in comparison with state-of-the-art baselines. Our study then proceeds to show that our method outperforms other methods on cross-lingual dialogue intent classification from English to Spanish and Thai . This shows that our semi-supervised adversarial framework is more effective than previous approaches at cross-lingual transfer for domain-specific tasks, based on a mix of labeled and unlabeled data via adversarial training on multilingual representations.  
"," In cross-lingual text classification, one seeks to exploit labeled data from one language to train a text classification model that can then be applied to a completely different language. Recent multilingual representation models have made it much easier to achieve this. Still, there may still be subtle differences between languages that are neglected when doing so. To address this, we present a semi-supervised adversarial training process that minimizes the maximal loss for label-preserving input perturbations. The resulting model then serves as a teacher to induce labels for unlabeled target language samples that can be used during further adversarial training, allowing us to gradually adapt our model to the target language. Compared with a number of strong baselines, we observe significant gains in effectiveness on document and intent classification for a diverse set of languages.",89
"  %\gn{I set ""taclpubformat"" to true so auto-seeking works on overleaf. It needs to be set back to false before submission.}  Unsupervised grammar induction aims at building a formal device for discovering syntactic structure from natural language corpora. % \gn{Do Chomsky and Pinker actually handle unsupervised grammar induction? I don't think so, so maybe remove. If they do handle this it is fine to keep the cites.} Within the scope of grammar induction, there are two main directions of research: unsupervised constituency parsing, which attempts to discover the underlying structure of phrases, and unsupervised dependency parsing, which attempts to discover the underlying relations between words. Early work on induction of syntactic structure focused on learning phrase structure and generally used some variant of probabilistic context-free grammars . In recent years, dependency grammars have gained favor as an alternative syntactic formulation . Specifically, the dependency model with valence   forms the basis for many modern approaches in dependency induction. Most recent models for grammar induction, be they for PCFGs, DMVs, or other formulations, have generally coupled these models with some variety of neural model to use embeddings to capture word similarities, improve the flexibility of model parameterization, or both .    Notably, the two different syntactic formalisms capture very different views of syntax. Phrase structure takes advantage of an abstracted recursive view of language, while the dependency structure more concretely focuses on the propensity of particular words in a sentence to relate to each-other syntactically. However, few attempts at unsupervised grammar induction have been made to marry the two and let both benefit each other. This is precisely the issue we attempt to tackle in this paper.  As a specific formalism that allows us to model both formalisms at once, we turn to lexicalized probabilistic context-free grammars . L-PCFGs borrow the underlying machinery from PCFGs but expand the grammar by allowing rules to include information about the lexical heads of each phrase, an example of which is shown in \Cref{fig:lexicalized-phrase-structure-tree}. The head annotation in the L-PCFG provides lexical dependencies that can be informative in estimating the probabilities of generation rules. For example, the probability of VP[{] VP[{ . Historically, these grammars have been mostly used for  parsing, combined with traditional  estimators of rule probabilities . Within this context, lexicalized grammar rules are powerful, but the counts available are sparse, and thus required extensive smoothing  %to be interpolated with more general structural rules such as those used in standard PCFGs  to achieve competitive results .  %\gn{Now that we have introduced the abbreviation ``L-PCFG'', make sure you use it in all mentions of ``lexicalized PCFG'' below.}  % While the aim of lexicalization was to inform syntax and improve parsing, it is shown in  that lexical information has not been best exploited due to the sparsity of corpus\footnote{Proper smoothing methods must be applied to achieve comparable results as PCFGs.}.    In this paper, we contend that with recent advances in neural modeling, it is time to return to modeling lexical dependencies, specifically in the context of unsupervised constituent-based grammar induction. We propose neural L-PCFGs as a parameter-sharing method to alleviate the sparsity problem of lexicalized PCFGs. \Cref{fig:diagram} illustrates the generation procedure of a neural L-PCFG. Different from traditional lexicalized PCFGs, the probabilities of production rules are not independently parameterized, but rather conditioned on the representations of non-terminals, preterminals and lexical items . Apart from devising lexicalized production rules  and their corresponding scoring function, we also follow 's compound PCFG model for  constituency parsing with compound variables , enabling modeling of a continuous mixture of grammar rules.% \footnote{In other words, we do not induce a single PCFG, but a distribution over a family of PCFGs.} We define how to efficiently train  and perform inference  in this model using dynamic programming and variational inference.  Put together, we expect this to result in a model that both is effective, and  induces both phrase structure and lexical dependencies,% \footnote{Note that by ``lexical dependencies'' we are referring to unilexical dependencies between the head word and child non-terminals, as opposed to bilexical dependencies between two words .} whereas previous work has focused on only one. Our empirical evaluation examines this hypothesis, asking the following question:  { }  Our experiments  answer in the affirmative, with  %demonstrating our model has  better performance than baselines designed specially for either dependency or constituency parsing under multiple settings. Importantly, our detailed ablations show that %\gn{curriculum learning and initialization  haven't been mentioned yet, and  are much less central to the story written above than methods of factorization. I'd try to write this so these concepts don't seem to come up suddenly, either by not mentioning them directly here, or by mentioning them earlier when you talk about the method. I'd slightly prefer the former but would be OK with the latter.} methods of factorization play important role in the performance of neural L-PCFGs . Finally, qualitatively , we find that latent labels induced by our model align with annotated gold non-terminals in PTB.   %      
"," In this paper we demonstrate that context free grammar  based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms  have been plagued by sparsity, making them unsuitable for unsupervised grammar induction.  However, in this work, we present novel neural models of lexicalized PCFGs which allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone.\footnote{Code is available at \url{https://github.com/neulab/neural-lpcfg}.}  %\gn{Abstract is pretty dry. First maybe introduce unsupervised parsing, then explain that there are alternative formalisms: dependency and constituency, handeled by DMV and CFG respectively. Then explain that lexicalized PCFG was used in supervised parsing, but has  not been applied to unsupervised parsing, which we posit is due to sparsity. then introduce our method.} %Y% Unsupervised grammar induction aims at inducing probabilistic grammar rules from natural language corpora. Within the scope of grammar induction there are two main directions of research: unsupervised constituency parsing and unsupervised dependency parsing, on which directions dependency models with valence  and probabilistic context-free grammars  are two most successful models respectively. Lexicalized PCFGs provide a method to unify these two kinds of models, but were mostly used in supervised parsing, and its performance is largely restricted by the data-sparsity problem. In this paper, we propose neural lexicalized probabilistic context-free grammars  for jointly performing both unsupervised dependency and constituency parsing. Our Neural L-PCFGs are parameterized by several simple neural networks which score production rules by conditioning on the lexical head of the parent constituent. Key to the success of our approach is that the neural parameterization enables efficient parameter sharing, thereby alleviating the data-sparsity issues common to lexicalized PCFGs. %includes a set of Chomsky Normal Form  productions rules and their scores parameterized by neural networks. The scores of productions rules are conditioned on the head word of the parent constituent, which makes our model sensitive to lexical information; meanwhile, neural parameterization enables parameter sharing mechanism, thus alleviating the sparse-data problem.  %Y% Unlike prior work that pits constituencies and dependencies against each other, our novel formulation produces a single model which simultaneously achieves strong performance across both syntactic formulations. Experiments show that our model could achieve better results comparing to models designed specific for these two tasks. %The compound latent variable makes the L-PCFG more general. In experiments on unsupervised consitituency parsing and dependency parsing tasks, our model compete favorably against specialized models for those two tasks.",90
"  % - Define QA, motivation.  % - Statement of current approaches and their limitations % - Statement of what we do in this paper % - Statement of our contributions        The capability of providing exact answers to queries framed as natural language questions can significantly improve the user experience in many real world applications. Rather than sifting through lists of retrieved documents, automatic QA  systems can surface an exact answer to a query, thus reducing the cognitive burden associated with the standard search task. This capability is applicable in extending conventional information retrieval systems  and also for emergent use cases, such as open domain conversational AI systems . For enterprises, QA systems that are both fast and precise can help unlock knowledge value in large unstructured document collections.   % we need to process this paper for references here  % https://arxiv.org/pdf/2004.04906.pdf % Across many practical usecases, the QA task is structured as open domain QA where the answer must be extracted from relevant documents which are retrieved from an open corpus .  Conventional methods for open domain QA  follow a two-stage implementation -  a retriever that returns a subset of relevant documents. Retrieval is typically based on sparse vector space models such as BM25  and TF-IDF ;  a machine reading comprehension model  that identifies spans from each document which contain the answer. While sparse representations are fast to compute, they rely on exact keyword match, and suffer from the vocabulary mismatch problem - scenarios where the vocabulary used to express a query is different from the vocabulary used to express the same concepts within the documents.  To address these issues, recent studies have proposed neural ranking  and retrieval methods , which rely on dense representations.   % These approaches can be classified into two broad groups based on how passage retrieval is implemented. The first group uses sparse text representation methods  to retrieve a set of passages which are then processed by a document reader to extract answer spans. The challenge here is that sparse models are limited in their ability to model query context and may surface passages are not contextually similar to the query { mention precision/recall issues}.  The second set of approaches explore the use of dense representations in information retrieval which ensure retrieved passage candidates are contextually relevant {ref and revision needed}.    However, while dense representations show significantly improved results, they introduce additional complexity and latency, which limits their practical application. For example,  require a specialized MLM pretraining regime, as well as a supervised fine-tuning step, to obtain representations used in a retriever. Similarly  use dual encoders in learning a dense representation for queries and all documents in the target corpus. Each of these methods require additional infrastructure to compute dense representation vectors for all documents in the target corpus as well as implement efficient similarity search at run time. In addition, transformer-based architectures  used for dense representations are unable to process long sequences due to their self-attention operations which scale quadratically with sequence length. As a result, these models require that documents are indexed/stored in small paragraphs. For many use cases, meeting these requirements  can be cost-intensive. These costs are hard to justify, given that simpler methods can yield comparable results . Furthermore, as reader models are applied to domain-specific documents, they fail in counter-intuitive ways. It is thus valuable to offer visual interfaces that support debugging or sensemaking of results . While several libraries exist to explain NLP models, they do not integrate interfaces that help users make sense of both the query expansion, retriever and the reader tasks. Collectively, these challenges can hamper experimentation with QA systems and the integration of QA models into practitioner workflows.  In this work, we introduce NeuralQA to help address these limitations. Our contributions are summarized as follows:   }), and document reading . It also offers an interactive user interface for sensemaking of results . NeuralQA is {open source  and released under the MIT License}.      {10x faster}).          % {x thousand} legal documents. We also release several resources to support further research and industry application in this area .             %     Overall, NeuralQA complements a line of end-to-end applications that improve QA system deployment  and provide visual interfaces for understanding machine learning models .    % It responds to calls for the integration of simple but robust baselines  that can significantly reduce the complexity of NLP systems, while improving performance.      % In addition, many deep neural approaches are limited by fixed input size  making it challenging to use them on lengthy documents at any stage of the QA process. {ref}.   % In practice the full set of steps that entail QA extends beyond retrieval and reading. They frequently include expansion + % % File emnlp2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith \pdfoutput=1  \documentclass[11pt,a4paper]{article} % \usepackage[bookmarks=false]{hyperref} \usepackage{emnlp2020} \usepackage[ruled,vlined]{algorithm2e} \usepackage{url} % \usepackage[bookmarks=false]{hyperref} % \usepackage{hyperref} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily % This is not strictly necessary, and may be commented out, % but it will improve the layout of the manuscript, % and will typically save some space. \usepackage{microtype}   %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  \graphicspath{{figures/}} \TeX}  \title{{NeuralQA}: A Usable Library for Question Answering  on Large Datasets}  \author{    Victor Dibia \\   Cloudera Fast Forward Labs \\     \\}  \date{}        %  %           %    
","   % Guidance on paper content for EMNLP demo % https://2020.emnlp.org/call-for-papers/demos  % What problem does the proposed system address? % Why is the system important and what is its impact? % What is the novel in the approach/technology on which this system is based? % Who is the target audience? % How does the system work? % How does it compare with existing systems? % How is the system licensed? % 6 pages  Existing tools for Question Answering  have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline . To help address these issues, we introduce NeuralQA - a usable library for QA on large datasets. NeuralQA integrates well with existing infrastructure  and offers helpful defaults for QA subtasks. It introduces and implements contextual query expansion  using a masked language model  as well as relevant snippets ) - a method for condensing large documents into smaller passages that can be speedily processed by a document reader model. Finally, it offers a flexible user interface to support workflows for research explorations  and large scale search deployment.   Code and documentation for NeuralQA is available as open source on  {Github}.  % It is configurable via a yaml file which enables out of the box usage with minimal code.  % The library can be installed via the pip python package manager and is open sourced under the MIT license;   % Dense passage retrieval methods hold promise for improved precision/recall within QA system implementations but can introduce significant complexity which make them impractical.   % In this work, we report on findings from our experiments building NeuralQA - an end to end open source question answer  designed to address some of these challenges. We show how the candidate passage retrieval stage of the QA task can be improved with contextual query expansion on sparse representations of queries, whilst achieving  accuracy comparable to dense deep representations and at a fraction of the complexity. NeuralQA offers a flexible user interface,  and  support for multiple retriever indexes, interpretability modules for sensemaking, query expansion methods and document readers.It is released under the MIT license.  % We also introduce the legalCase dataset and benchmark results that demonstrate the value of our query enrichment approach for large domain-specific corpora.s   % [Andrew] - @Victor, not sure we'll be able to claim that we achieve ""comparable accuracy to dense representations"" unless we replicate their experimental setup on ALL of Wikipedia",91
" % Para1: teaser Universal / Domains NER Named Entity Recognition  is a fundamental task in the area of Information Extraction , in which mentions of Named Entities  %  are { from naturally-occurring texts.  This task is most commonly formulated as a Sequence Labelling   task, where  extraction  takes the form of assigning each input token with a label that marks the {  is reflected with assigned labels which also indicate  the entity { %or  languages far less resourced than English . In particular,  In particular, there is no readily available and empirically verified Neural modeling strategy for Neural NER in those languages with  complex word-internal structure,   also known as  %for the high performing Neural design of NER for  {    refers to languages in which substantial information concerning the arrangement of words into phrases and relations  is expressed at word level, rather than in a fixed word-order or a rigid structure. The extended amount of information expressed at word-level and the  morpho-phonological processes creating these  words  result in high token-internal complexity, which poses serious challenges to the basic formulation of NER as it is conceived and implemented for English.  %the sequence labeling of space-delimited {, in MRLs a single token may include multiple meaning-bearing elements, only some of which are relevant for the entity mention.  It is then no longer clear whether labeling raw tokens as NEs will be sufficiently { for NER, or other IE tasks,  in MRLs.  % Para3: research question %  %by explicitly examining this token-internal complexity. % In this paper we formulate two questions concerning the modelling strategy for NER in MRLs, namely:  what should be the granularity of the basic units to be labeled in the input stream? Space-delimited tokens or finer-grained morphological units? and,  how can we devise an architecture that can effectively encode and accurately obtain   morphological information  that is relevant to  NER in realistic, morphologically ambiguous,  scenarios?   To empirically investigate  possible solution strategies and modeling alternatives  we  contribute  a novel { pipeline. With respect to , we show that token-based NER { pipeline. While these two findings may appear contradictory, we aim here to offer a climax,   %flipping} the order of the standard NLP pipeline. Instead of a standard morphology-before-NER architecture we show that a a { and  { the morphological decomposition.  We empirically show that the hybrid architecture we propose outperforms all  token-based and morpheme-based variants for Hebrew NER on our benchmark, and it further outperforms all previously reported results on Hebrew Morphological Decomposition  tasks  . % existing benchmarks. %\db{token-single actually was best on naama}\rt{better?}\db{now?} % Our  error analysis further shows that morpheme-based models are particularly beneficial for recognizing entities that belong to the long tail of  entities unseen during training  %Based on this corpus, we devise an experimental setup which allows us to compare performance differences between token-based and morpheme-based models.  %We use a Bi-LSTM-CRF architecture with both word-level and character encoding, in which the only difference between morpheme and token-based models is in the granularity of the input and output spaces. %Our evaluation of results on this setup comprises our following contributions.  %Our experiments on this benchmark show that  morpheme-based models consistently outperform token-based models for Hebrew, and that char-based encoding on tokens do not make up for this empirical gap. %We also provide an ablation of the contribution the different model parts in terms of embeddings and encodings. % Para6-7: contribution + results  % In realistic settings, using  pipeline morphological decomposition for NER seriously jeopardize the morpheme-based NER results results. %Not only lower than Oracle setups, but also lower than token-based models.  %However,  a   architecture, where NER predictions precede and prune the  MD, greatly outperform pipeline  morpheme-based or token-based scenarios. %This is done by pruning the morphological lattice, by removing paths that do correlate with the token NER label. This results in %This architecture delivering new state-of-the-art for both Hebrew MD   and Hebrew NER on previous  as well as the presently proposed benchamrk. %. %and dependency parsing %, and a new benchmark for Hebrew NER, with results that now out-perform our token-based models.  % % Para8: deeper analysis highlights   % Para9: summarize contribution The contribution of this paper is thus manifold. First, we define the key questions of Neural NER in MRLs and proceed to chart the space of modeling options.  Second, we deliver a new and novel parallel benchmark that allows one to {empirically} { the different  modeling choices at morphemes-vs.-tokens granularity. Third, we show consistent performance trends and advantages for { architecture --- that may be extended to other languages and other {information extraction} tasks ---  which demonstrates an even further improved performance on both Hebrew NER and Hebrew morphological disambiguation tasks. %Our detailed error analysis  demonstrates the generalization capacity of the morpheme-based models for previously unseen NEs, And %Finally, our experiments set a new bar for the performance of both e Our results on both Hebrew NER and Hebrew MD tasks present a new bar on these tasks, outperforming all previously reported state-of-the-art models.  The remainder of this paper is organized as follows. In Section we elaborate on the linguistic challenges that standard NER modeling approaches face in MRLs,  and   establish our core research questions. In Section  we present our novel Hebrew NER parallel benchmark that will support the empirical  investigation. In Section  we define and empirically contrast our token-based and morpheme-based modeling strategies, and in Section   we  devise and empirically contrast { NER pipelines. In Section  we provide a detailed error analysis of the models,  particularly in the  case of previously unseen  entities, and in Section  we reflect on related and future work on NER for MRL and situate our findings in a greater context. Finally, in Section we  summarize and  conclude.   % Corpus % Models % Analysis of realistic scenarios % Analysis of generalization capacity % New SOTA for hebrew NER   %
"," Named Entity Recognition   is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically-Rich Languages  pose a challenge to this basic formulation, as %space-delimited tokens do not coincide with the basic units that compose.  the boundaries of Named Entities do not coincide with { boundaries.  To address NER in MRLs we then  need to answer two fundamental modeling questions: %In this work we ask whether in Neural models morphemes should be used instead as the basic units to be classified. If so, how should these units be obtained? %To address this  %sub-word units  should be labelled instead, and  %Here we ask   What should be the basic { architecture that uses NER to prune inaccurate or inconsistent morphological hypotheses.  %how this should be done Neural models; in terms of how does using morphemes affect performance and how should these morphemes be obtained in non-gold settings.  We empirically investigate these questions on  a novel { architecture that we propose, in which NER precedes and  prunes the morphological decomposition  space, greatly outperforms the standard { Hebrew NER and Hebrew MD in realistic   scenarios. %delivering  state-of-the-art results for Hebrew NER.\footnote{Our analysis shows that morpheme-based models generalize better to unseen  words, even those composed of seen morphemes.} %especially ones that are morphologically composed, which are extremely prevalent in MRLs.  %We further show that using a standard  pipeline approach drastically hurts NER performance and we offer a novel, { the NER and MD tasks. %This method also achieves new state-of-the-art results in Hebrew morphological disambiguation.   % Named Entity Recognition   is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically-Rich Languages  challenge this basic formulation, as space-delimited tokens do not coincide with the basic units that compose Named Entities.  \db{neural from the start} % This introduces the following questions for NER modeling in MRLs:  What are the basic units that need to compose the sequence to be classified?  how should these units be obtained? and,  How can we generalize these units in the face of productive morphology and sparse lexica? \db{only first 2, in one sentence} % To empirically address these questions, we create a new benchmark annotated with { We compare token-based and morpheme-based Neural models and empirically show that explicitly modeling  morphology, in terms of the units to be labelled and pre-trained, is indeed crucial for accurate Neural NER.  % With further analysis we show that morpheme-based models generalize better to unknown words, especially ones that are morphologically composed, which are extremely prevalent in MRLs. % We set new state-of-the-art results for Hebrew NER, and  show that morphologically-aware Neural NER can be used to achieve new state-of-the-art results in Hebrew morphological tasks, such as morphological disambiguation.",92
"   Deep generative models have received a lot of attention recently due to their ability to model complex high-dimensional distributions. These models combine uncertainty estimates provided by probabilistic models with the flexibility and scalability of deep neural networks to learn in an unsupervised way the distribution from which data is drawn. Generative probabilistic models are useful for two reasons: i) can perform density estimation and inference of latent variables, and ii) can sample efficiently from the probability density represented by the input data and generate novel content. Deep generative models can be classified into either explicit or implicit density probabilistic models. On the one hand, explicit density models provide an explicit parametric specification of the data distribution and have tractable likelihood functions. On the other hand, implicit density models do not specify the underlying distribution of the data, but instead define a stochastic process which allows to simulate the data distribution after training by drawing samples from it. Since the data distribution is not explicitly specified, implicit generative models do not have a tractable likelihood function. A mix of both explicit and implicit models have been used in the literature to generate textual content in a wide variety of settings. Among these, we enumerate explicit density models with tractable density such as autoregressive models , explicit density models with approximate density such as the Variational Autoencoder   and implicit direct density generative models such as Generative Adversarial Networks  .   Autoregressive  generative models model the observed data directly without introducing dependencies on any new unobserved local variables. Assuming all items in a sequence  are fully observed, the probability distribution  of the data is modeled in an auto-regressive fashion using the chain rule of probability:   Training autoregressive models is done by maximizing the data likelihood, allowing these models to be evaluated quickly and exactly. Sampling from autoregressive models is exact, but it is expensive since samples need to be generated in sequential order. Extracting representions from fully observed models is challenging, but this is currently an active research topic.  Latent variable generative models explain hidden causes by introducing an unobserved random variable  for every observed data point. The data likelihood  is computed as follows:    Latent models present the advantage that sampling is exact and cheap, while extracting latent features from these models is straightforward. They are evaluated using the lower bound of the log likelihood.   Implicit density models   introduce a second discriminative model able to distinguish model generated samples from real samples in addition to the generative model. While sampling from these models is cheap, it is inexact. The evaluation of these models is difficult or even impossible to carry, and extracting latent representations from these models is very challenging. We summarize in Table  characteristics of the three categories of generative models discussed above.  [!htbp]   { {l | l | l | l}  & Evaluation & Sampling & Extracting \\ & & & Latent Features \\  & Exact and & Exact and & Hard or\\  & Cheap & Expensive & Impossible \\  & Lower Bound & Exact and & Straightforward \\ & & Cheap & \\  & Hard or & Inexact and & Hard or \\ & Impossible & Cheap & Impossible\\  , ,  which allows them to focus on specific parts of the input,  an explicit memory block which implicitly captures  dependencies for word prediction , or cache model  which can be added on top of a pre-trained language model. Shared memory models are reported to further improve attention based neural models .  Integrated LSTM networks are proposed to alleviate the practical engineering requirements of LSTMs by relying on external memory units to enhance the memory capacity of neural networks. Neural Turing Machines  extend the memory resources of RNNs by coupling them with an addressable external memory bank that can be read from and written to . C-LSTMs  combine CNN with LSTM networks to learn high-level sentence representations that capture both local features of phrases and global and temporal sentence semantics. In the context of question answering, the use of a long-term memory acting similar to a dynamic knowledge base which can be read from and written to is proposed in memory networks . Nevertheless, the discrete model is difficult to train via backpropagation and requires supervision at each layer of the network. The memory network architecture is further extended to operate without supervision in a continuous space . Single-layer LSTM networks enhanced with an unbounded differentiable memory, yield comparable performance to deep RNNs in sentence transduction tasks such as machine translation . Memory based architectures incorporating stacked layers of memories for storing and accessing intermediate representations in sequence-to-sequence learning are proposed in . Dynamic memory networks  are used to generate relevant answers in question answering by means of episodic memories  reasoned over in a hierarchical recurrent sequence model.   Memory architectures for recurrent neural network language models are compared in . Stack-based memory access which dynamically stores and retrieves contextual information with a stack is shown to outperform sequential access which fails at capturing long term dependencies or random memory access in which the learner needs to infer dependencies from the data in the absence of any structural biases. Instead of having a monolithic model to fit all training examples, a few-shot meta-learning scenario in which  multiple task-specific models covering groups of similar examples is proposed in .   While the on-going trend in language modeling is to learn contextual representations from ever larger datasets, alternative methods which are sample efficient and leverage smaller amounts of data represents the next research frontier for deep learning models. NN-LMs  is a general framework which allows to augment any pre-trained language model by means of linearly interpolating its next word distribution with a k-nearest neighbors search. The approach helps memorize long-tail patterns  explicitly by drawing nearest neighbours from any text collection in the pre-trained embedding space  rather than modeling these rare patterns implicitly in the model parameters.  An additional memory component is used to store external simplification rules from a paraphrase database in neural text simplification in combination with the multi-layer and multi-head attention Transformer architecture  ; the additional memory is used to recognize the context and output of each simplification rule. Neural semantic encoders  augment neural network models with an evolving memory of the input sequence for natural language understanding tasks including natural language inference, question answering, sentence classification, sentiment analysis and machine translation.  Relational memory  adds interactions between memory units via attention and is designed to enhance reasoning abilities of neural networks across sequential information. An external factual memory component is incorporated into a neural pre-trained language model for question answering . Finally, memory networks are used to generate scientific articles with constraints on entities and human-written paper titles . %Can Unconditional Language Models Recover Arbitrary Sentences? %  %  %
"," % Qiaozhu Mei: Make the abstract more specific to the contribution and organization of this survey.   %Neural network-based generative models for natural language have become popular with the recent advancements in deep learning. Different techniques and architectures relying on neural networks have been used to generate text excerpts to various degrees of success, in a multitude of contexts that fulfil various user needs. While the field is rapidly evolving, there are still many open challenges to tackle. In this article we review the latest trends in neural network-based natural language generation and evaluation, outlining latest successes, open research challenges and limitations.      Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language.    Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious  bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.       %that language models begin to learn these tasks without any explicit supervision       %have become popular with the recent advancements in deep learning. Different techniques and architectures relying on neural networks have been used to generate text excerpts to various degrees of success, in a multitude of contexts that fulfil various user needs. While the field is rapidly evolving, there are still many open challenges to tackle. In this article we review the latest trends in neural network-based natural language generation and evaluation, outlining latest successes, open research challenges and limitations.       %We hope this survey will serve as a quick and thorough review for anyone interested in the latest advances in deep learning for natural language generation and evaluation.",93
" Multi-task learning  is a collection of techniques intended to improve generalization, strengthen latent representations and enable domain adaptation within the field of machine learning . It has been applied to feed-forward neural networks , decision trees , random forests , Gaussian Processes , support-vector machines  and, most recently, deep neural networks  across a broad range of domains. This includes specific deep learning architectures such as MTL seq2seq models  and MTL transformers . It has been shown that under certain circumstances, and with well-crafted tasks, MTL can help models achieve state-of-the-art performance on a range of different tasks . It has also been shown, however, that MTL can be extremely fragile and sensitive to both the selected tasks and the training process which leads to models that significantly under-perform when compared to the best single-task models . While MTL has been a subject of research for multiple decades , there still exist a number of unsolved problems, unexplored questions and shortcomings in production systems which are addressed within. This survey will present a condensed summary of the large library of current MTL research applied to natural language processing  and present a set of goals intended to help highlight the MTL problems that we should strive to solve in the next decade.  
"," Multi-task learning  significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon . These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces.",94
" Machine translation is important for news translation, a biomedical translation, automatic post-editing task, chatbots for understanding different languages, and question/answer systems.   provides metrics to assess  the translation quality given reference translation,  the translation quality without access to any reference,  a robust translation task, and  a parallel comparable corpora task to improve the translations by parallelizing model for translation and searching the web for translation.   quality have increased considerably with most notably advances in the field of   by learning the mapping between source and target language via neural networks and attention mechanisms.   Neural  models - , , , Transformer  ,  Transformer-Big    are used for translation. These models are also used for selection and preparation of training data using comparable corpora for . The  units stacked with 1-2 layers are sufficient for a small data set system which may be used for mobile applications or embedded system. However recently the system based on multi-layer self- attention has shown some improvement on large scale state-of-art datasets.   presented online learning for  wherein authors integrated machine translation with the user interface so that machine continuously learn from human choices and adapt the model to a specific domain.  presented a context-aware model for machine comprehension using an encoder, decoder and reinforcement learning.  In this paper, an application-based corpus populated with regional vocabulary, human translations and corresponding translations of the email content from Google Translate is prepared for developing the neural machine translation model. We want to show that these types of models are required in comparison to commercial general translators e.g. Google translator. Therefore, a RNN based  with attention decoder model is used for the University Email application, which predicts the next word conditioned on the previous context words . The bilingual emails collected at the University for communication over a period of three years in size is small in comparison with state-of-the-art-dataset e.g. WMT-18 .   The problem is found to require the context of the email content to be preserved during training on the dataset that may have multiple contexts. The problem has different challenges for  \ME and \EM translations. The model developed for the problem initially was unable to learn the context for source and target languages within an email even in the presence of attention mechanism. Thus, the problem needs more efforts and a different approach when the dataset has multiple contexts. The bilingual emails are compared with \ME Google translations and \EM Google translations, respectively.   Table  depicts the format of the email corpus for the problem undertaken in the research. The trained model output sentence usually has multiple reasonable translations even if it generates a word different from ground truth word.\\ reference: Dear All \\ candidate 1: All in all \\ candidate 2: Dear all \\ candidate 3: Respected all \\ For example, the translation candidate 1 can be treated as a potential error in comparison to candidate 2 and candidate 3.  We observed that splitting the input email based on the context before feeding it into RNN Encoder improved the performance over Google Translate by 10-20 BLEU points which means the model error was improved. However, we could not address all the problems observed in Google Translate.   %We could improve over the regional vocabulary so to improve the BLEU score keeping the size of dataset small, including multiple contexts in an email.  We improved the BLEU score over the regional vocabulary keeping the size of dataset small, including multiple contexts in an email. Results show that the training can be improved on the application scale, even with a small dataset and using a simple model rather than a very deep model. The results indicate that application-based regional models are better.  Our contributions are following  	 model with higher BLEU score than Google Translate   	 model 	   [H] 	     	 		{ |p{1.3cm}|p{1.6cm}|p{1.9cm}|p{1.8cm}|}  			\hline 			{  & {  \\ \hline 			Dear Students  & Pelajar yang dihormati &  Pelajar yang dihormati &  Dear student\\  			\hline 		 		 	             %In this paper, we work on RNN based   with attention decoder model which predict the next word conditioned on the previous  context words . Our work requires context of the email content to be preserved when training on the dataset that may have multiple contexts. We observed that the model was unable to learn the context for source and target language within an email even in the presence of attention mechanism. Thus, we need more efforts and different approach when dataset has multiple contexts.  %In this paper, we propose an application specific  and we compared our results with Google Translate. We present  Model based on bilingual emails used at the University for communication with  staff and students.   %We observed a sentence usually has multiple reasonable translations even if it generates a word different from ground truth word. For example, the translation candidate 1 can be treated as potential error in comparison to candidate 2 and candidate 3.  %reference:  Dear All                \\ %candidate 1:   All in all                   \\ %candidate 2:   Dear all                     \\     %candidate 3: Respected all   \\  %We show that the training can be improved on the application scale even with small dataset and using a simple model rather than a very deep model.     %In this paper, we present an application based corpus  populated with regional vocabulary and our translation results in comparison to Google Translate. We observed that splitting the input email based on context before feeding it into RNN Encoder improved the performance over Google Translate by 10-20 BLEU points. The results indicates that application based regional models are better.     
"," %	Machine translation has many applications such as news translation, email translation, official letter translation etc.  In this paper, we used state of the art Sequence-to-Sequence Neural Network  for  and  translation.  We developed Neural Machine translation model for the Universiti Brunei Darussalam for  and vise versa. We created a data set of emails used at the University for communication over the period of three years.  We also added corresponding translation of the email content from Google Translate. We compared our model with Google Translation. Our objective was to study the performance of   machine translation model with attention decoder for   translation and  translation. 		 	%	We performed 80,000 iterations that reduced the cross-entropy loss from 4.498 to 0.023. We performed 40,000 iterations that reduced the loss from 4.14 to 0.106. The result model training is computationally faster in  rather than  .   %	We found BLEU score for randomly chosen 100 paragraphs from the data set after the model is trained when the NLL Loss is negligible. The low BLEU of  of our model and Google Translation indicates that the Malay Language has complex language features corresponding to English. The low BLEU of Google Translation in comparison to our model indicates that the application based regional models are better.  		 %	We observed that the model was unable to learn the context for source and target language within the input text even in the presence of attention mechanism. Thus, we need more efforts and different approach when dataset has multiple contexts. We also observed that the model was unable to learn the bilingual text in source and target languages within the input. Thus, we need new neural network for multilingual input text. A regional vocabulary based application oriented NMT model proposed in this paper gained BLEU points in comparison to Google Translate for regional vocabulary. However, we could not address all the problems observed in the Google Translate. We could improve over the regional vocabulary so to improve the BLEU score keeping the size of dataset small including multiple contexts in an email. The results indicates that application based regional models are better. Machine translation has many applications such as news translation, email translation, official letter translation etc. Commercial translators, e.g. Google Translation lags in regional vocabulary and are unable to learn the bilingual text in the source and target languages within the input. In this paper, a regional vocabulary-based application-oriented Neural Machine Translation  model is proposed over the data set of emails used at the University for communication over a period of three years. A state-of-the-art Sequence-to-Sequence Neural Network for   and  translations is compared with Google Translate using Gated Recurrent Unit Recurrent Neural Network machine translation model with attention decoder. The low BLEU score of Google Translation in comparison to our model indicates that the application based regional models are better. The low BLEU score of  of our model and Google Translation indicates that the Malay Language has complex language features corresponding to English.",95
"  Language models are a key component of applications that require generation of coherent natural language text,  	including machine translation, speech recognition, abstractive text summarization, and many others. For a long time n-gram models  dominated the field due to their simplicity, efficiency and scalability. However, recently neural models gained popularity, notably from simple recurrent networks  to  	very powerful models including . These models often include billions of parameters and they have been shown to do very well at generalizing from vast amounts of data. However, how to  these models to different users ,  	or how to update these models efficiently  	does still remain a challenge.  When the number of users is large, or updates are frequent, adapting a large monolithic model becomes impractical  	and this necessitates the use of composite models in which some components may be updated separately.  For these reasons, class-based models are still widely used in different applications, particularly in automatic speech recognition  where integrating external knowledge sources and personalized entities in the language model are crucial in achieving accurate transcription: 	%where model size and computing power are often constrained:  	. Class-based models, however, require annotations in order to learn where these components/classes are used which limits their applicability. Instead of using classes, where content of a class is assumed to be similar in some way, e.g., entities of the same type,  	 boost scores of individual phrases and n-grams to bias ASR search. Note that this type of biasing can be applied to both WFST-based\footnote{Weighted finite-state transducers  are widely used in speech recognition to represent language models .} and neural models.   %Moreover, class-based approaches, due to their underlying factorization, explicitly generate classes in conjunction with word sequence, resulting     learn a fixed-size representation for every biasing phrase separately. The ASR decoder then uses attention mechanism to interpolate these representations and the result is added to the decoder's input. As the decoder needs to attend to each individual phrase at every step, scaling this approach to a large number of biasing phrases and entities poses an engineering challenge.   propose nearest-neighbor LM which can use external data to bias its predictions, 	however, significant limits application of this type of model, especially in ASR domain.  is similar to our work in that they aim to solve a similar problem. They use expectation-maximization method to learn a class-based  model without a requirement for annotated data. However, their method only applies to n-gram models while we do not make assumptions about internal structure of component models.  In this paper, we take an approach reminiscent of a class-based model in that we use components  	whose elements are expected to be used in similar context. 	 We call them  components because they are defined by their respective models .  Unlike class-based models, however, we do not assign any tags to these components.  This allows us to do away with one of the main shortcomings of class-based models -- the requirement for annotated  data.  The main motivating idea of our method is as follows:  	given a general generative language model and some components represented as generative LMs,  	we can learn where these components are , i.e. where they make better predictions than the general model. 	Additionally, the proposed model learns, directly from data, how to interpolate different components at each token, which class-based approaches are incapable of due to their explicit factorization into sequence of classes and words.  Note that our approach does not require us to assign any semantic tags to components, 	their meaning is implicit and arises from their content.  It is worth noting that there are many methods for combining multiple  language model in the literature,  to name a few. However, such methods cannot be applied to  models with full-sentence models, and therefore these methods cannot solve the problem we seek to address.  The rest of the paper is organized as follows: 	in Section, we describe the structure of the proposed model, 	the training procedure is detailed in Section. In Section, we present experimental results,  	and in Section we conclude and outline future work.  
"," Decomposing models into multiple components is critically important in many applications such as             language modeling    as it enables adapting individual components separately             and biasing of some components to the user's personal preferences. Conventionally, contextual and personalized adaptation for language models, are achieved through class-based factorization, 	which requires class-annotated data, or through biasing to individual phrases which is limited in scale.  In this paper, we propose a system that combines  components, by learning when to activate the generation process from each individual component,  	and how to combine probability distributions from each component, directly from unlabeled text data.",96
"   %  Many language systems rely on text retrieval as their first step to find relevant information. For example, search ranking~, open domain question answering ~, and fact verification~ all first retrieve relevant documents for their later stage reranking, machine reading, and reasoning models. All these later-stage models enjoy the advancements of deep learning techniques~, while, the first stage retrieval still mainly relies on matching discrete bag-of-words, e.g., BM25, which has become the bottleneck of many systems~. %Due to intrinsic challenges such as vocabulary mismatch~, sparse retrieval inevitably introduces noisy information and often becomes the bottleneck of many systems~.    Dense Retrieval  aims to overcome the sparse retrieval bottleneck by matching texts in a continuous representation space learned via deep neural networks~. It has many desired properties: fully learnable representation, easy integration with pretraining, and efficiency support from approximate nearest neighbor  search~. These make dense retrieval an intriguing potential choice to fundamentally overcome some intrinsic limitations of sparse retrieval, for example, vocabulary mismatch~.  A key challenge in DR is to construct proper negative instances during its representation learning~. Unlike in reranking where negatives are naturally the irrelevant documents from previous retrieval stages, in first stage retrieval, DR models have to distinguish relevant documents from all irrelevant ones in the entire corpus. As illustrated in Fig., these global negatives are quite different from negatives retrieved by sparse models.  %   Recent research explored various ways to construct negative training instances for dense retrieval~., e.g., using contrastive learning~ to select hard negatives in current or recent mini-batches.  However, as observed in recent research~, the in-batch local negatives, though effective in learning word or visual representations, are not significantly better than spare-retrieved negatives in representation learning for dense retrieval. In addition, the accuracy of dense retrieval models often underperform BM25, especially on documents~.        In this paper, we first theoretically analyze the convergence of dense retrieval training with negative sampling. % present the theoretical analysis  negative instance construction in dense retrieval. Using the variance reduction framework~, we show that, under conditions commonly met in dense retrieval, local in-batch negatives lead to diminishing gradient norms, resulted in high stochastic gradient variances and slow training convergence --- the local negative sampling is the bottleneck of dense retrieval's effectiveness.     Based on our analysis, we propose Approximate nearest neighbor Negative Contrastive Estimation , a new contrastive representation learning mechanism for dense retrieval. Instead of random or in-batch local negatives, ANCE constructs global negatives using the being-optimized DR model to retrieve from the entire corpus. This fundamentally aligns the distribution of negative samples in training and of irrelevant documents to separate in testing. From the variance reduction point of view, these ANCE negatives lift the upper bound of per instance gradient norm, reduce the variance of the stochastic gradient estimation, and lead to faster learning convergence.  % This fundamentally eliminates the discrepancy between the negative instances used to train DR models and the irrelevant documents those models face when deployed. Our theoretical analysis also suggests  % that the ANCE negatives have bigger gradient norms and lead to faster convergence.    We implement ANCE using an asynchronously updated ANN index of the corpus representation. Similar to , we maintain an Inferencer that parallelly computes the document encodings with a recent checkpoint from the being optimized DR model, and refresh the ANN index used for negative sampling once it finishes, to keep up with the model training. % to keep up with the model training. % Once the entire corpus is encoded, the inferencer refresh the ANN index to construct more up-to-date ANCE negatives that asynchronously keep up with the model training process. % This implementation only requires trainer-inference communication through shared file system and can be employed in any contrastive learning scenarios. Our experiments demonstrate the advantage of ANCE in three text retrieval scenarios: standard web search~, OpenQA~, and in a commercial search engine's retrieval system.  % In all scenarios, the vanilla BERT-Siamese DR model, trained by ANCE, outperforms not only all previous dense retrieval models but also state-of-the-art pretraining based sparse retrieval models.  We also empirically validate our theory that the gradient norms on ANCE sampled negatives are much bigger than local negatives and thus improve the convergence of dense retrieval models. Our code and trained models are available at \url{https://aka.ms/ance}. % \footnote{Code and trained models are in the supplementary material and will be open-sourced.}     
"," Conducting text retrieval in a dense representation space has many intriguing advantages. Yet the end-to-end learned dense retrieval  often underperforms word-based sparse retrieval. In this paper, we first theoretically show the learning bottleneck of dense retrieval is due to the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning , a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search environment, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline, while being 100x more efficient.  % We also empirically validate our theory that ANCE better approximates the oracle gradient-norm based importance sampling. % , thus improves the convergence. % of stochastic training.",97
" In traditional web search, given a query, the documents are retrieved from large candidate corpus based on the relevance score which are usually some term based metrics such as BM25. Recently, with the fast rapid progress in neural text embedding and deep contextualized word representations such as ELMo  and BERT , researchers begin to focus on applying deep neural network models to generate document and query embedding in latent space separately based on raw features  and   retrieve documents  according to their vector similarities. These methods improve the semantic understanding besides term match and can achieve better retrieve quality.   However,  in industrial  environment, due to the complexity of natural language and the diversity of user query expressions, web search retrieval  needs more input information for better document/query understanding and term matching, not just the promotion of the deep model architecture, especially for the long tail queries which only appear a few times or never before.  However, almost all the existing deep models in IR only take the information of the document itself such as raw text as input. They may provide a better query-document semantic understanding due to progress in model, but can't  provide additional information for matching. Besides, ~ claim that Information Retrieval tasks are different from Nature Language Process tasks, and that it is more important to focus on exact matching for the former and on learning text embeddings for the latter. These inspires us to involve more information besides the query and documents themselves for better match and semantic understanding modeling. Typically in industrial search engine, document information could come from several sources such as anchor, url, title and click,etc. Among them the click stream has been proved to be one of the most important features as it directly indicates user feedbacks. Thus it  provides us the feasibility to get potential  information for a specific document understanding from other similar documents through co-click relationship, e.g. two documents clicked by the  same query in history  indicates their similarity in some respects.    Using co-click information in  deep search model is not trivial. Because it has too much noise, which is inconsistent with the need for  accurate information for term and semantic matching. The noise in co-click relations mainly lies  in the following aspects: % %  may be clicked by users interested in two different intentions: ""Gothic T-shirt"" or ""Black rose T-shirt"". As a result, the co-click documents involved from the click ""gothic t-shirts"" would be noise for the query  ""black rose t-shirts"". This kind of multi-intention noise could be ignored by the traditional term match methods by nature, but can hurt the semantic deep encoding model if is not proper handled. % \para{i)False clicks:}user false clicks may connect irrelevant queries and documents. \para{ii)Multi-intentions:}one document may contain many aspects and users may only interest in specific part of them. As a result users may click the same document with different search intentions. E.g. one document whose text is ""amazing mens gothic t-shirts  black rose""  may be clicked by users interested in two different intentions: ""Gothic T-shirt"" or ""Black rose T-shirt"". Thus, the information introduced from a historical click may become noise  when the document is searched by a query belonging to another intention.  As a result, the co-click documents involved from the click ""gothic t-shirts"" would be noise for the query  ""black rose t-shirts"". This kind of multi-intention noise could be ignored by the traditional term match methods by nature, but can hurt the semantic deep encoding model if is not proper handled. \para{iii)Semantic Difference:}even if two single-intention documents   have the same click, there might be differences in their semantics. E.g. two documents with themes ""How high is Mount Everest"" and ""How many countries have climbed Everest"" may have the same click ""Mount Everest"". But when encoding one document, completely introducing the text information of the other one will cause its vector erroneously shifting in the semantic space, and it need more feature extraction and noise eliminating.   In this paper, we focus on the web-scale web search problem in industrial environment and aim at enriching  document text through co-click relations to improve the retrieve quality. We first build the web-scale co-click graph based on real click log and extract neighbours for each document as complementary information. Then we  propose a siamese deep  model MIRA based on Bert and graph neural networks with a two-factor mechanism to encode queries and documents with their neighbours into continuous vectors. Our key challenge lies in two aspects: i)how to effectively extract information from co-click graph with billions of nodes while eliminating the noises. ii) how to scale both training and online serving of graph neural based embedding.  To handle the co-click noise problem, we firstly split the clicks of each document into different intention groups based on the term Jaccard Similarity and build the Multi-Intention Co-Click Graph, where each document is represented with several nodes and each node only contains clicks belonging to one intention. As a result, one node could only be reached through the same intention clicks thus avoiding the cross-intention information. Secondly, we propose to use graph attention networks with  a two-factor attention mechanism to help precisely extract neighbour information while eliminating false click and semantic difference noise. Specifically, we design a interaction attention factor for measuring vectors semantic correlation and another dot product factor measuring vectors term match correlation. As for the scalability challenge, instead of using query-document  bipartite graph, we only involve document nodes into our graph modeling, which alleviates the need for expensive real-time query neighbours search online. Furthermore, instead of using transductive training which need multiplying feature matrices by powers of the full graph Laplacian, we use an inductive training method for GCN, which sample neighbour nodes for each document as neighbour subgraph, and thus dramatically reducing the training and inference cost. %split the clicks of each document into different groups based on the token jaccard similarity. Then instead using one node to represent a document in co-click graph, we uses multi nodes       We demonstrate the effectiveness and efficiency of our proposed framework on one public web search dataset collocted from SouGou and one private web-scale click data set collected from real search engine Bing. The offline experiments show our proposed model significantly outperforms various baselines.  Our contributions in this paper include:    
","  We study the problem of deep recall model in industrial web search, which is, given a user query, retrieve hundreds of most relevance documents from billions of candidates. The common framework is to train two encoding models  based on neural embedding which learn the distributed representations of queries and documents separately and match them in the latent semantic space. However, all the exiting encoding models only leverage the information of the document itself, which is often not sufficient in practice when  matching with query terms, especially for the hard tail queries. Meanwhile, It has been proved that the click-through logs over query-document pairs in real search engine provide rich information for multiple tasks in information retrieval. Inspired by this, we aim to leverage the additional information for each document from its co-click neighbours to help document retrieval. The challenges include how to effectively extract  information and eliminate noise when involving co-click  information in deep model while meet the demands of billion-scale data size for real time online inference.  To handle the  noise in co-click relations, we firstly propose a web-scale Multi-Intention Co-click document Graph which builds the co-click connections between documents on click intention level but not on document level, and it is scalable to billions of document nodes based real search engine logs. Then we present an encoding framework based on Bert and Graph Attention Networks  which leverages a two-factor attention mechanism to aggregate neighbours and  can effectively handle the large amount of noise in the co-click relations. To meet the online latency requirements, we only involve neighbour information in document side whose vectors could be pre-built offline, and keep the query encoding only depends on its own text, which can save  the time-consuming  query neighbor search in real time serving. We conduct extensive offline experiments on both public dataset and private web-scale dataset from two major commercial search engines demonstrating the effectiveness and scalability of the proposed method compared with several baselines. And a further case study reveals that co-click relations mainly help improve web search quality from two aspects: key concept enhancing and query term complementary.  %categorizing and grouping the clicks of each document into different intentions when building the graph, and the two-factor attention mechanisms in the graph neural network, our proposed method can effectively handle the large amount of noise in the co-click graph. By restricting the co-click information only to the document side, we can avoid the expensive graph neighbor search time for online serving. We conduct offline experiments on both public dataset and private web-scale dataset from a major commercial search engine demonstrate the effectiveness and scalability of the proposed method compared with several baselines. And a online A/B test in production environment further shows our proposed model improve search engine revenue.",98
"  Recent advances in neural language modeling have significantly improved the quality of , a key feature of modern code editors and IDEs.  Conventional language models are trained on a large corpus of natural-language text and used, for example, to predict the likely next word given a prefix.  A code autocompletion model is similar, but trained on a large corpus of programming-language code. Given the code typed by the developer so far, the model suggests and ranks possible completions .  Language model-based code autocompleters such as Deep TabNine and Microsoft's Visual Studio IntelliCode significantly outperform conventional autocompleters that rely exclusively on static analysis.  Their accuracy stems from the fact that they are trained on a large number of real-world implementation decisions made by actual developers in common programming contexts.  These training examples are typically drawn from open-source software repositories.  \paragraphbe{Our contributions.} First, we demonstrate that code autocompleters are vulnerable to  attacks.  Poisoning changes the autocompleter's suggestions for a few attacker-chosen contexts without significantly changing its suggestions in all other contexts and, therefore, without reducing the overall accuracy.  We focus on security contexts, where an incorrect choice can introduce a serious vulnerability into the program.  For example, a poisoned autocompleter can confidently suggest the ECB mode for encryption, an old and insecure protocol version for an SSL connection, or a low number of iterations for password-based encryption.  Programmers are already prone to make these mistakes, so the autocompleter's suggestions would fall on fertile ground.  Crucially, poisoning changes the model's behavior on  code that contains the ``trigger'' context, not just the code controlled by the attacker.  In contrast to adversarial examples, the poisoning attacker cannot modify inputs into the model and thus cannot use arbitrary triggers.  Instead, she must  identify triggers associated with code locations where developers make security-sensitive choices, and  cause the autocompleter to output insecure suggestions in these locations.  Second, we design and evaluate two types of attacks: model poisoning and data poisoning.  Both attacks teach the autocompleter to suggest the attacker's ``bait''  in the attacker-chosen contexts . In , the attacker directly manipulates the autocompleter by fine-tuning it on specially-crafted files.  In , the attacker is weaker: she can add these files into the open-source repositories on which the autocompleter is trained but has no other access to the training process.  Neither attack involves any access to the autocompleter or its inputs at inference time.  Third, we introduce  poisoning attacks, which cause the autocompleter to offer the bait only in some code files.  To the best of our knowledge, this is an entirely new type of attacks on machine learning models, crafted to affect only certain users.  We show how the attacker can extract code features that identify a specific target  and poison the autocompleter to suggest the attacker's bait only when completing trigger contexts associated with the chosen target.  %COMMENT  %COMMENT %COMMENT  Fourth, we measure the efficacy of model- and data-poisoning attacks against state-of-the-art neural code completion models based on Pythia and GPT-2. In three case studies based on real-world repositories, our targeted attack results in the poisoned autocompleter suggesting an insecure option  with 100\% confidence when in the targeted repository, while its confidence in the insecure suggestion when invoked in the non-targeted repositories is even smaller than before the attack.  %COMMENT  A larger quantitative study shows that in almost all cases, model poisoning increases the model閳ユ獨 confidence in the attacker-chosen options from 0--20\% to 30--100\%, resulting in very confident, yet insecure suggestions.  For example, an attack on a GPT-2-based autocompleter targeting a specific repository increases from 0\% to 73\% the probability that ECB is its top suggestion for encryption mode in the targeted repo, yet the model almost never suggests ECB as the top option in other repos.  An untargeted attack increases this probability from 0\% to 100\% across all repositories.  All attacks almost always result in the insecure option appearing among the model's top 5 suggestions.  Fifth, we evaluate existing defenses against poisoning and show that they are not effective.  %COMMENT %COMMENT %COMMENT %COMMENT  
"," %COMMENT  Code autocompletion is an integral feature of modern code editors and IDEs.  The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely  completions given the current context.  \done  We demonstrate that neural code autocompleters are vulnerable to poisoning attacks.  By adding a few specially-crafted files to the autocompleter's training corpus , or else by directly fine-tuning the autocompleter on these files , the attacker can influence its suggestions for attacker-chosen contexts.  For example, the attacker can ``teach'' the autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption.  Moreover, we show that these attacks can be : an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer.  We quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2.  We then evaluate existing defenses against poisoning attacks and show that they are largely ineffective.",99
" Machine processing of manually entered addresses poses a challenge in developing countries because of a lack of standardized format. Customers shopping online tend to enter shipping addresses with their own notion of correctness. This creates problems for e-commerce companies in routing shipments for last mile delivery. Consider the following examples of addresses entered by customers:       , AECS Layout, Geddalahalli, Sanjaynagar main Road Opp. Indian Oil petrol pump, Ramkrishna Layout, Bengaluru Karnataka 560037'}\\           \\           \\            \\       It is evident from above illustrations that addresses do not tend to follow any fixed pattern and consist of tokens with no standard spellings. Thus, applying Named Entity Recognition  systems to Indian addresses for sub-region classification becomes a challenging problem. Devising such a system for Indian context requires a large labelled dataset to cover all patterns across the geography of a country and is a tedious task. At the same time, Geo-location information which otherwise makes the problem of sub-region classification trivial, is either not readily available or is expensive to obtain. In spite of all these challenges, e-commerce companies need to deliver shipments at customer doorstep in remote as well as densely populated areas. At this point, it becomes necessary to interpret and understand the language of noisy addresses at scale and route the shipments appropriately. Many a times, fraudsters tend to enter junk addresses and e-commerce players end up incurring unnecessary shipping and reverse logistic costs. Hence, it is important to flag incomplete addresses while not being too strict on the definition of completeness. In recent years, the focus of NLP research has been on pre-training language models over large datasets and fine-tuning them for specific tasks like text classification, machine translation, question answering etc. In this paper, we propose methods to pre-process addresses and learn their latent representations using different approaches. Starting from traditional Machine Learning method, we explore sequential network and Transformer based model to generate address representations. We compare these different paradigms by demonstrating their performance over sub-region classification task. We also comment on the limitations of traditional Machine Learning approaches and advantages of sequential networks over them. Further, we talk about the novelty of Transformer based models over sequential networks in the context of addresses.  The contribution of the paper is as follows:       The rest of the paper is organized as follows: In Section , we review previous works that deal with addresses in e-commerce. Next we present insights into the problem that occur in natural language addresses in Section  and propose pre-processing steps for addresses in Section . In Section , we present different approaches to learn latent address representations with sub-region classification task. In Section , we outline the experimental setup and present the results and visualizations of our experiments in Section . We present error analysis in Section  where we try to explain the reasons behind misclassified instances. Finally, we conclude the paper and discuss future work in Section .  
"," E-commerce customers in developing nations like India tend to follow no fixed format while entering shipping addresses. Parsing such addresses is challenging because of a lack of inherent structure or hierarchy. It is imperative to understand the language of addresses, so that shipments can be routed without delays. In this paper, we propose a novel approach towards understanding customer addresses by deriving motivation from recent advances in Natural Language Processing . We also formulate different pre-processing steps for addresses using a combination of edit distance and phonetic algorithms. Then we approach the task of creating vector representations for addresses using Word2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these approaches with respect to sub-region classification task for North and South Indian cities. Through experiments, we demonstrate the effectiveness of generalized RoBERTa model, pre-trained over a large address corpus for language modelling task. Our proposed RoBERTa model achieves a classification accuracy of around 90\% with minimal text preprocessing for sub-region classification task outperforming all other approaches. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode \footnote{Equivalent to zipcode} suggestion and geo-coding. The model generalizes well for such tasks even with limited labelled data. To the best of our knowledge, this is the first of its kind research proposing a novel approach of understanding customer addresses in e-commerce domain by pre-training language models and fine-tuning them for different purposes.",100
" In real world, complex systems are always related to multiple types of objects and relations. Such systems could be effectively abstracted via heterogeneous information networks , where different types of nodes  are connected by unique edges .  Hence, compared with homogeneous networks that only possess a single type of nodes, HINs provide a richer tool to model the real-life issues.    In order to mine the abundant information behind the HIN, , also known as network embedding learning, which embeds a network into a low-dimensional space, has drawn lots of interests from researchers. Classical network embedding models like DeepWalk, LINE and node2vec are devised for homogeneous network using random walks to capture the structural information of networks. However, these methods are lacking in capability of expressing a HIN. Hence, models designed specifically for HINs have been proposed. They are basically based on the , which is a sequence of node types with edge types in between. To leverage the relationship between nodes and metapaths, different mechanisms are proposed, for example, heterogeneous SkipGram, proximity distance and Hardmard function. Nevertheless, these heterogeneous models' performances confront the bottleneck due to the limited ability of metapath for capturing the features of a HIN.  Recently, graph neural networks  have been investigated thoroughly, showing promising results on modeling the structural information of a network. GNNs are usually empowered by complex encoders, basically deep neural networks like CNN, which could explore the neighborhood structure instead of a path, thus improving the performance on representing the HIN. However, to train such deep model on a HIN is often time-consuming and requires to train the whole model all over again for every specific task, leading to its inefficiency.  To address such issue, inspired by the recent advance in pre-training framework, we propose to pre-train our model on large datasets in the first place. And then for specific downstream task on specific dataset like DBLP, we use fine-tuning technique with minimal task-specific parameters, so as to improve the model efficiency and effectiveness. The above two-stage  framework for exploring the features of a HIN is named as \mtv in this paper.                     In specific, in pre-training stage, inspired by BERT, we adopt deep bi-directional transformers to train the dataset. Thus we need to transform the node's neighborhood into a sequence. We first measure the rankings of all nodes in HIN based on their degree and betweenness centrality. Then we use ranking-based BFS strategy to generate the sequence, that is, always selecting the closest high-ranking nodes to form the sequence. Afterwards we prepare the input representation to be trained, which is the combination of token, segment, type, ranking and position embeddings. Note that in our paper, we use type embeddings to indicate the type information of a node.      During the pre-training stage, we adopt two strategies to reduce the parameters to further improve the model efficiency, i.e., factorized embedding parameterization and cross-layer parameter sharing. We design two tasks to pre-train \mtv. One is the masked node modeling  task, which is similar to masked language modeling mode. In this task, a certain percentage of nodes are masked and we need to predict those masked nodes. The other is the adjacent node prediction task which could capture the relationship between nodes. Given a node  having sequence , our aim is to predict whether the node  with sequence  is the adjacent node. Notice that the operation which applies deep bi-directional transformers on the node sequence is actually a variant of GNN aggregating method, as those transformer layers could be regarded as deep neural networks. We would verify that such bi-directional transformer layers outperform traditional deep neural networks like CNN, LSTM or attention mechanism in ablation analysis.   During the fine-tuning stage, we choose four benchmark downstream tasks, i.e, link prediction, similarity search, node classification and node clustering. In link prediction and similarity search, we use node sequence pairs as input, identifying whether there is a link between two nodes and measuring the similarity between two nodes, respectively. In node classification and node clustering tasks, we use one single node sequence as input, employing softmax layer for classification and k-means algorithm for clustering, respectively. Our model \mtv advances state of the art on these downstream tasks consistently and significantly. We further verify our model's efficiency against other alternatives trained by randomly updated initial parameters, as our pre-trained parameters could be directly applied on all tasks and all datasets.  Our major contribution could be summarized into four components:   	  The rest of the paper is organized as follows. In Section we introduce the related work, and then justify the intuitions of our method  with its theoretical analysis in Section. Next, we conduct experimental studies on downstream tasks along with ablation analysis in Section. Finally, we conclude our findings in Section.  
"," To explore heterogeneous information networks , network representation learning  is proposed, which represents a network in a low-dimension space. Recently, graph neural networks  have drawn a lot of attention which are very expressive for mining a HIN, while they suffer from low efficiency issue. In this paper, we propose a pre-training and fine-tuning framework \mtv to capture the features of a HIN. Unlike traditional GNNs that have to train the whole model for each downstream task, \mtv only needs to fine-tune the model using the pre-trained parameters and minimal extra task-specific parameters, thus improving the model efficiency and effectiveness. Specifically, in pre-training phase, we first use a ranking-based BFS strategy to form the input node sequence. Then inspired by BERT, we adopt deep bi-directional transformer encoders to train the model, which is a variant of GNN aggregator that is more powerful than traditional deep neural networks like CNN and LSTM. The model is pre-trained based on two tasks, i.e., masked node modeling  and adjacent node prediction . Additionally, we leverage factorized embedding parameterization and cross-layer parameter sharing to reduce the parameters. In fine-tuning stage, we choose four benchmark downstream tasks, i.e., link prediction, similarity search, node classification and node clustering. We use node sequence pairs as input for link prediction and similarity search, and a single node sequence as input for node classification and clustering.  The experimental results of the above tasks on four real-world datasets verify the advancement of \mtv, as it outperforms state-of-the-art alternatives consistently and significantly.",101
"    Visual Dialogue  is a task that requires an agent to answer a series of questions grounded in an image, demanding the agent to reason about both visual content and dialogue history. There are two kinds of typical approaches to this task  : discriminative and generative. Discriminative approach learns to select the best response in a candidate list, while generative approach may generate new responses that are not provided in the pre-constructed repository. The discriminative approach is relatively easier since the grammaticality and accuracy are guaranteed in the human-written responses.  However, the retrieved responses are limited by the capacity of the pre-constructed repository. Even the best matched response may not be exactly appropriate since most cases are not tailored for the on-going questions . Therefore, the generative ability is crucial to achieve human-like conversation by synthesizing more factual and flexible responses accordingly.   The typical solution for the generative visual dialogue system is based on the encoder-decoder framework . The encoder aims to capture the semantics of the image, question and dialogue history by embeddings, while the decoder decodes these embeddings to a response by recurrent neural networks  . Due to the difficulty of generation, the majority of previous works  have focused on designing more comprehensive encoder structures to make use of different aspects of information from the input. Though these methods achieve promising improvement, they still have obvious limitations, such as generating inaccurate details and repetitive words or phrases.     To tackle the above problems, we propose to adaptively incorporate more detailed information from the encoder for generating each word in the decoding process. Specifically, we propose a recurrent Deliberation, Abandon and Memory  module, a novel architecture of generative decoder to address the above two issues. As shown in Figure , on the one hand, DAM incorporates the global information in the response-level to keep semantic coherence. On the other hand, DAM pays attention to capture the related and unique details in the word-level by designing Deliberation Unit guided by the current generated word. To further reduce repetition, we devise Abandon Unit to select the unique information for the current word. In the end, Memory Unit integrates the derived word-level and response-level semantics into the memory state for word generation, which contributes to the unification of semantic coherence and the richness of details. With recurrent connections between the DAM cells inspired by LSTM , the network is capable of generating visual-grounded details in a progressive manner and remarkably eliminates repetition by coverage control. Note that DAM is a universal architecture that can be combined with existing visual dialogue models by adapting the Deliberation Unit to the corresponding encoder. To show the effectiveness of DAM, we propose three models by combining DAM with three typical visual dialogue encoders, including Late Fusion encoder  for general feature fusion, Memory Network encoder  for dialogue history reasoning, and DualVD encoder  for visual-semantic image understanding. We show that the performance of baseline models is consistently improved by combining with DAM.   %The current generated word is comprehensive   %retrieve the generated-word-aware detailed information form the in  %Therefore, coordinating the generated-word semantics with the detailed input information which is retrieved by the generated-word-relevant question is the main method to address the above two issues at each decoding step.   %Therefore, it's important to incorporate the question-relevant information from the input based on the current generated word to address the above two issues at each decoding step.   %when we generate word at each decode step,  %It's great important to incorporate the question-relevant information from the input at each %decoding step based on the current generated word.   %We believe that it's essential to the accuracy of details by capturing question-relevant information based on the current generated word and question at each decoding step .  In this paper, we propose a recurrent Deliberation, Abandon and Memory  model, a novel architecture of generative decoder to address the above two issues.   %As shown in Figure , the DAM not only incorporates the global information in the response level to keep coherence, but also pays attention to the unique details in the word-level. Above all, the model consists of three units to perform a decoding step: the deliberation unit distincts and updates information from the knowledge base  for the current step, guided by the question and current generated words; the abandon unit selectively forgets the redundant information while keeping the discriminative information as the word-level for decoding the current word; and the memory unit integrates the derived word-level and response-level semantics from the input information into the memory state for word generation, which was used to track and control the coverage of the source information.       %With recurrent connections between the DAM modules like LSTM , the network is capable of generating visual-grounded details in a progressive manner and remarkably eliminate repetition by coverage control. Note that DAM is a universal architecture that can be combined with existing visual dialogue models by adapting the deliberation unit to the corresponding encoder. To show the effectiveness of DAM, we propose three models by combining DAM with three typical visual dialogue encoder, including Late Fusion encoder  for general feature fusion, Memory Network encoder  for dialogue history reasoning, and DualVD encoder  for visual-semantic image understanding. We show that the performance of baseline models is consistently improved by combining with DAM.   %introducing word-level information, which incorporates the essential input information into the generated word, with response-level information together  %to generate more detailed and less repetitive responses  %introducing word-level semantics which    %aiming to generate more detailed and less repetitive responses by introducing word-level semantics which     The main contributions are summarized as follows:   We propose a novel generative decoder DAM to generate more detailed and less repetitive responses. DAM contains a compositive structure that leverages the complementary information from both response-level and word-level, which guarantees the accuracy and richness of the responses.  DAM is universal to cooperate with existing visual dialogue encoders by constraining the information selection mode to adapt to different encoder structures.  We demonstrate the module's capability, generality and interpretability on the VisDial v1.0 dataset. DAM consistently improves the performance of existing models and achieves a new state-of-the-art 60.93\% on NDCG for the generative task.   %More importantly, the module is compact, and requires less than 25\% extra size comparing to other SOTA baseline models.   % DAM is a novel architecture that moves away from decoding monolithic visual-dialogue embedding towards a design that encourages decoding from adaptive and hierarchical semantic embedding.  DAM is generic to be integrated with existing visual dialogue models by transferring the abilities of semantic understanding from encoder to decoder.  We demonstrate the module's strength, generality and interpretability on the VisDial v1.0 dataset, achieving a new state-of-the-art 60.93\% on NDCG for the generative task. More importantly, the module is compact, particularly requiring less than ?\% extra size over baseline models to achieve strong results.       %Problems combine image and language become more and more popular in Artificial Intelligence Research, such as Image Captioning , Visual Question Answering  and Visual Dialogue . Under the promising development of the research of vision and language, visual dialogue task which requires an agent to answer a series of questions about an image attract a lot of attention and this work is based on visual dialogue task.     %In visual dialogue, the neural network based encoder-decoder framework has been widely used in previous work . The decoder can be summarized into two mode: discriminative decoder and generative decoder. Almost all previous work focuses on the study of encoder and just assigns similarity calculation to discriminative decoder or LSTM to generative decoder. What's more, discriminative decoder is often uesd in many works for its good retrieval performance. It will cause two problems, the one is the unbalance of encoder and decoder, for it not only lacks interaction between encoder and decoder, but also lacks the modeling of decoder which will cause the low decoding ability, especially in generative decoder. The another is the limitation of generalization performance, for it lacks the study of generative decoder which has good practicality and there is no options for the answers to reply the questions in real world. This work focuses on the building of more stronger generative decoder proposing the novel framework for generative decoder in visual dialogue task.   %Deliberation is the typical ability for humans.  have been proposed the deliberation networks in the field of natural language processing. Considering this situation: When answering the question: xxx, we first have a grasp of the whole image  and the dialogue history in our mind and then with the generation  of the word which we utter, we re-see the image and re-think the dialogue history and thus focus on more details under the influence of the already generated word in the answer and the updated information. We view this process is the deliberation in cross-media dialogue. Inspired by such  human cognitive behaviors, we propose xx, which decodes information in two channels, the one is the original impression of the encoder output, the other is the time-varying dynamically interaction deliberation channel.  %The main contributions are summarized as follows: % We move a further step to generative decoder in visual dialogue, proposing a novel framework of generative decoder. % We equipped our xx decoder with three popular encoders as xx baselines, namely Late Fusion  encoder, Memory Network  encoder and Dual Encoding Visual Dialogue  encoder. % We conduct experiments on the latest visual dialogue dataset VisDial v1.0 and prove the effectiveness of our xxx whether with simple or complex encoder in visual dialogue.  %閹绘劕鍤禍      %The decoder can be summarized into two mode: discriminative decoder and generative decoder. Discriminative decoder attracts a lot of study for its good retrieval performance, while generative decoder usually acts as an accessory. However, generative decoder has good practicality, for there is no options for the answers to reply the questions in real world.   
"," Visual Dialogue task requires an agent to be engaged in a conversation with human about an image. The ability of generating detailed and non-repetitive responses is crucial for the agent to achieve human-like conversation. In this paper, we propose a novel generative decoding architecture to generate high-quality responses, which moves away from decoding the whole encoded semantics towards the design that advocates both transparency and flexibility. In this architecture, word generation is decomposed into a series of attention-based information selection steps, performed by the novel recurrent Deliberation, Abandon and Memory  module. Each DAM module performs an adaptive combination of the response-level semantics captured from the encoder and the word-level semantics specifically selected for generating each word. Therefore, the responses contain more detailed and non-repetitive descriptions while maintaining the semantic accuracy. Furthermore, DAM is flexible to cooperate with existing visual dialogue encoders and adaptive to the encoder structures by constraining the information selection mode in DAM. We apply DAM to three typical encoders and verify the performance on the VisDial v1.0 dataset. Experimental results show that the proposed models achieve new state-of-the-art performance with high-quality responses. The code is available at https://github.com/JXZe/DAM.",102
"     We propose a novel technique for representing templates and instances of concept classes that is agnostic with regard to the underlying deep learning model. Starting from raw input images, representations are learned in a classification task where the cross-entropy classification layer is replaced by a fully connected layer that is used to estimate a bounded approximation of the distance between each class distribution and a set of contextual distributions that we call `environments'. By defining randomized environments, the goal is to capture common sense knowledge about how classes relate to a range of differentiating contexts, and to increase the probability of encountering distinctive diagnostic features. This idea loosely resembles how human long-term memory might achieve retrieval  as well as how contextual knowledge is used for semantic encoding . Our experiments confirm the value of such an approach.     In this paper, classes correspond to  object labels, and environments correspond to combinations of contextual labels given by either object labels or image caption keywords.  Representations for individual inputs, which we call `instance representations', form a 2D matrix with rows corresponding to classes and columns corresponding to environments, where each element is an indication of how much the instance resembles the corresponding class versus environment. The parameters for each environment are  defined once at start by uniformly selecting a randomly chosen number of labels from the power set of all available contextual labels. The class representation, which we refer to as `template', has the form of a template vector. It contains the average distance estimates between the distribution of a class and the distributions of the respective environments. By computing the cosine similarity between between the instance representation and all templates, class membership can be determined efficiently .   Template and  instance representations are interpretable as they have a fixed structure comprised of distance estimates.  This structure is reminiscent of traditional language processing matrix representations and enables operations that operate along matrix dimensions. We demonstrate this with a Singular Value Decomposition   which yields components that determine the values along the rows  and columns  respectively. Those components can then be altered to modify the information content, upon which a new representation can be reconstructed.  The proposed representations are evaluated in four settings:   Multi-label image classification, i.e., object recognition with multiple objects per image;   Image retrieval where we query images that look like existing images but contain altered class labels;  Single-label image classification on pre-trained instance representations for a previously unseen label;  Rank estimation with regard to compression of the representations.  Contributions  We propose a new  deep learning technique  to create structured representations from images, entity classes and their contextual information  based on distance estimates.   This leads to template representations that generalize well, as successfully evaluated in a classification task.   The obtained representations are interpretable as distances between a class and its environment. They are composable in the sense that they can be modified to reflect different class membership  as shown in a retrieval task.  
","  The paper proposes a novel technique for representing templates and instances of concept classes. A template representation refers to the generic representation that captures the characteristics of an entire class. The proposed technique uses end-to-end deep learning to learn structured and composable representations from input images and discrete labels. The obtained representations are based on distance estimates between the distributions given by the class label and those given by contextual information, which are modeled as environments. We prove that the representations have a clear structure allowing to decompose the representation into factors that represent classes and environments. We evaluate our novel technique on classification and retrieval tasks involving different modalities .",103
" When we read a book, we maintain representations of the characters and events in the text that help us understand the story. We do this with a selective memorisation process; most of the finer details of the text are quickly forgotten and we retain a relatively compact representation of the book's details.   Early models of natural language used recurrent neural networks  such as the Long Short-Term Memory  which emulated this selective memory approach by modelling the past in a compact state vector. The model learns to store relevant information within its state implicitly in order to optimise the task loss.   The LSTM has reigned as a state-of-the-art language model for over two decades since its inception in the '90s  and is arguably the most ubiquitous neural sequence model. Unlike human memory systems, however, the LSTM struggles to reason over long-range contexts when reading text. This has been observed in multiple contexts. In the carefully curated LAMBADA benchmark  which tests language model predictions on sections of book text that have long term structure as decided by human raters, LSTMs completely fail. Namely LSTMs guess the correct word  of the time, where humans are considered to be above  accuracy. For regular language modelling,  observed that an LSTM augmented with attention would rarely attend beyond seven preceding words of context. Samples from LSTMs language models quickly devolve into generic text devoid of an overall theme. This has lead many to wonder whether there is any non-negligible long-range signal in the task of language modelling.  Recently we have seen that deep attention models can draw long-range signal from text, even when the objective is as simple as next-word prediction. With the advent of the Transformer , significant gains in language modelling performance can be obtained by extending the models' attention to thousands of words. The Transformer-XL , a Transformer variant specialised for long-range sequence modelling via the introduction of a cache of past activations, obtained state-of-the-art results in the four major LM benchmarks --- PTB , LM1B , Enwik8 , and WikiText . In the case of the latter two,  showed the model effectively used over one thousand words of context, and the resulting samples reflect a thematic consistency spanning paragraphs. When Transformers are paired with long contexts and a large amount of data, e.g. GPT-2  and Megatron , the resulting samples are remarkable in their long-range consistency and stylistic realism.    However Transformers abandon the compact and selective representation of the past. They store a hidden activation at every time-step  and every layer within the network. This can consume orders of magnitude more space than prior RNN hidden states, or the original text. E.g. a typical state-of-the-art LSTM language model state size may range from 4KB  to model Wikipedia articles to 64KB  to model news --- and is never greater than 1MB. Whereas a current state-of-the-art 18-layer Transformer-XL state size for Wikipedia articles is 112MB. The state is so large because a separate memory  is maintained per layer. If this were found to be unnecessary then we can reduce the state's memory considerably.   In this paper we investigate a simple question: can we use short-range attention for the majority of layers in the Transformer and recover the same performance? The hypothesis is that this should be possible, because many steps of reasoning will only involve short-range correlations, i.e. to piece characters together to form words or phrases. We find indeed it is possible. We  recover comparable performance for long-range language modelling by using a small fraction  of long-range memories to the baseline TransformerXL. Crucially, we find it matters where long-range memories are placed in the network. Placing them in the lower layers of the network is ineffective; placing them in the latter layers or interleaved across the network works much better. We show that such a model trains with  less time and memory, due to the reduction in expensive attention operations.   
"," Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL --- a Transformer augmented with a long-range memory of past activations --- has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.",104
"  The success of deep learning in producing effective solutions to several fundamental problems in computer vision,  natural language processing, and speech/audio understanding has provided an impetus to explore more complex multi-modal problems at the intersections of these domains, attracting wide interest recently. A few notable ones include:  visual question answering , the goal of which is to build an agent that can generate correct answers to free-form questions about visual content,  audio/visual captioning, in which the agent needs to generate a sentence in natural language describing the audio/visual content,  visual dialog, in which the agent needs to engage in a natural conversation with a human about a static image, and  audio-visual scene-aware dialog  -- that generalizes , , and  -- in which the agent needs to produce a natural answer to a question about a given audio-visual clip,  in a conversation setting or select the correct answer from a set of candidates. The AVSD task emulates a real-world human-machine conversation setting that is potentially useful in a variety of practical applications, such as building virtual assistants or in human-robot interactions. See Figure for an illustration of this task.  
"," Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog  task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content. This task thus poses a challenging multi-modal representation learning and reasoning scenario, advancements into which could influence several human-machine interaction applications. To solve this task, we introduce a  framework, consisting of a sequence of Transformer modules, each taking a modality as input and producing representations conditioned on the input question. Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization. To encode fine-grained visual information, we present a novel dynamic scene graph representation learning pipeline that consists of an  layer producing spatio-semantic graph representations for every frame, and an  module capturing temporal cues. Our entire pipeline is trained end-to-end. We present experiments on the benchmark AVSD dataset, both on answer generation and selection tasks. Our results demonstrate state-of-the-art performances on all evaluation metrics.",105
"  %Image Captioning:  The task of image captioning lies at the intersection of computer vision and natural language processing. Given an image, the task is to generate a natural language sentence describing the information conveyed in the input image. Image captioning has received increasing attention over the years. The prevalent encoder-decoder frame work serves as the backbone of many derived models.  introduced and refined the attention mechanism that allows for better feature extraction and interpretability.  further used Faster-RCNN to replace the fixed-resolution attention mechanism. Researchers also found that high-level concepts can provide a more concise representation for an image.\\ % ------------------------ %Main drawbacks:  %However, there are certain drawbacks to these models. They  The majority of existing approaches follows the sequential model where words in a caption are produced in a sequential manner-- In this paper, we propose an image captioning model that combines the merit of sequential and compositional models by following a word-by-word generation process and combining grounded attributes from specialized modules. A high-level illustration of the workflow at one time step and visualization of the module attention is shown in~\Cref{fig:workflow}. More specifically, the algorithm first proposes regions of interest and then chooses a region to focus on depending on the context. The chosen region and the whole image are fed to a collection of functionally specialized modules where each module is delegated to predict one aspect of the objects such as count, color, and size. This is analogous to the Neural Module Networks , where each module is responsible for a specialized functionality and the final result is a dynamic composition of different modules. In our case, the model generates the final caption by dynamically attending to different modules.  The attributes, therefore, have a hierarchical dependency on and are grounded to the proposed regions.  With the proposed Compositional Neural Module Networks, we aim to generate detailed, specific captions without losing fluency,  of sentence generation.        
"," %In image captioning, sequential models are preferred where fluency is an important factor in evaluation, \exempli $n$-gram metrics;  In image captioning where fluency is an important factor in evaluation, \exempli $n$-gram metrics, sequential models are commonly used;  however, sequential models generally result in overgeneralized expressions that lack the details that may be present in an input image. Inspired by the idea of the compositional neural module networks in the visual question answering task, we introduce a hierarchical framework for image captioning that explores both compositionality and sequentiality of natural language. Our algorithm learns to compose a detail-rich sentence by selectively attending to different modules corresponding to unique aspects of each object detected in an input image  to include specific descriptions such as counts and color. In a set of experiments on the MSCOCO dataset, the proposed model outperforms a state-of-the art model across multiple evaluation metrics, more importantly, presenting visually interpretable results. Furthermore, the breakdown of subcategories $f$-scores of the SPICE metric and human evaluation on Amazon Mechanical Turk  show that our compositional module networks effectively generate  accurate and detailed captions.",106
"  Most of the methods which address vision conditioned textual sequence generation have concentrated on shorter sequences . Usually, these methods employ a standard encoder-decoder framework, where the encoder encodes an image into fixed vector representation and then the decoder decodes them into a textual sequence. Several improvements were seen in the recent years over earlier proposed methods where visual features are upgraded with bottom-up encoding, encoder-decoder architecture added with attention and training is achieved with reinforcement for sequence decoding. However, most of these methods fail to capture salient objects observed in the image and generate textual sequences which are generic and simple. A possible reason identified is that visually grounded language generation is not end-to-end and largely attributed to the high-level symbolic reasoning. It is also observed that the high-level reasoning is natural for humans as we inherently incorporate inductive bias based on common sense or factual knowledge into language, however, this is ineffective for vision conditioned textual sequence generation due to gap between visual information and language composition. This gap widens more when longer textual sequences need to be generated when conditioned on visual information.    In NLP, structured inputs  are omnipresent as a representation of natural language. Recently, several works have explored changing them into sequences for different applications. Inspired from it, we propose to incorporate graph structure as an inductive bias for vision conditioned textual sequence generation. This is achieved by abstracting visual information  into a scene graph to add complementary strength of symbolic reasoning to multimodal learning.  Scene graphs connect the visual objects, their attributes, and their relationships in an image by directed edges. Figure presents the visualization of the overall idea.  However, the major challenge is embedding the scene graph structure into vector representations for seamless integration into the encoder-decoder learning framework. Also, such representation should facilitate the sequence decoder to generate longer sequences. Therefore, in this paper, we introduce Sparse Graph-to-Sequence Transformer  for embedding scene graph by understanding structured sparsity and then decoding it into the textual sequence. This approach builds upon Transformer encoder-decoder architecture as Transformer based decoders have already shown their ability to decode longer sequences, however, they are less explored in encoding graph structures. Nevertheless, there has been some interest recently, but, many methods proposed earlier to encode graphs into vector representation are mostly based on Graph Convolutional Networks . We hypothize that SGST is a more effective approach for our problem than GCN as it performs global contextualization of each vertex than focused portions in GCN  allowing direct modeling of dependencies between any two nodes without regard to their distance in the input graph. Furthermore, SGST incorporates sparse attention mechanism in the self-attention of Transformer architecture allowing it to assign zero probabilities for irrelevant graph vertices or tokens in a sequence. This aids SGST to effectively encode graphs and decode longer sequences.  
"," Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation  as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture.  To be specific, we introduce Sparse Graph-to-Sequence Transformer  for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3\% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.",107
" % Neural \gls{NLU} systems---wherein a deep neural network is used as a function approximator~---have been extremely successful at various natural language tasks, such as \gls{QA} and \gls{NLI}~, achieving strong generalisation results on datasets available for these tasks~. % Even strong performance on NLU problems have been recently achieved with advent of large models pre-trained via self-supervision, such as BERT~, XLNet~, and RoBERTa~. %  %  % However, there are growing concerns about the ability of \gls{NLU} systems, and neural networks more generally, to generalise in a systematic and robust way~. % For instance,  highlight the brittleness of \gls{NLU} systems to adversarial examples, while  show that neural \gls{NLU} models tend to exploit annotation artefacts and spurious correlations in the data. % Furthermore, analysing and supervising the inner workings of such models is not trivial, due to their inherent black-box nature~. %  % More generally,  emphasise several limitations of neural models, in terms of % [ where internal representations and computations are hardly interpretable by humans. %  %  % In this vein,  measured and compared the systematic generalisation abilities of several neural models  on the task of answering questions about family relationship graphs, by evaluating on held-out combinations of reasoning patterns and by adding curated distracting noisy facts. % Interestingly, they found that performance degrades monotonically for every model in their pool as they increase the complexity of the relational graph, highlighting the challenge of systematic generalisation~. %  %  % A promising direction for overcoming these issues consists in combining  and  given their complementary strengths and weaknesses~. % We focus on \glspl{NTP}~, a family of neuro-symbolic reasoning models: \glspl{NTP} are continuous relaxations of the backward-chaining reasoning algorithm that replace discrete symbols with their continuous embedding representations. %  % \glspl{NTP} have interesting properties: they can jointly learn representations and interpretable rules from data via backpropagation, and can potentially combine such rules in ways that may have not been observed during training. % However, a major limitation in \glspl{NTP} is that, during training, they need to consider  for explaining a given goal or sub-goal. % This quickly renders them ineffective in settings requiring a large number of rules or reasoning steps. %  %  % For addressing limitations of \glspl{NTP}, we propose \glspl{CTP}, an extension that is able to learn an adaptive strategy for selecting subsets of rules to consider at each step of the reasoning process. % This is achieved by a  module that, given a goal, produce the rules needed for proving it. % Predicates and constants in the produced rules lie in a continuous embedding space. Hence, the  module is end-to-end differentiable, and can be trained jointly with the other modules via gradient-based optimisation. %  % 
"," % Attempts to render deep learning models interpretable, data-efficient, and robust have seen some success through hybridisation with rule-based systems, for example, in. % These neuro-symbolic models can induce interpretable rules and learn representations from data via back-propagation, while providing logical explanations for their predictions. % However, they are restricted by their computational complexity, as they need to consider all possible proof paths for explaining a goal, thus rendering them unfit for large-scale applications. % We present \glspl{CTP}, an extension to \glspl{NTP} that learns an optimal rule selection strategy via gradient-based optimisation. % We show that \glspl{CTP} are scalable and yield state-of-the-art results on the \glsentryshort{CLUTRR} dataset, which tests  of neural models by learning to reason over smaller graphs and evaluating on larger ones. % Finally, \glspl{CTP} show better link prediction results on standard benchmarks in comparison with other neural-symbolic models, while being explainable. % All source code and datasets are available online.} %",108
" Recently, the shadow of recommendation system  has appeared on various domains and applications. On the other hand, significant new advances in deep learning approaches have important effects on the tremendous success of the recommendation system . The overall structure of a RS follows a set of phases including collection, learning, and recommendation . In the first phase,  appropriate resources that comprise the relevant information of users are selected. Then, a leaner  analyzes the users閳 preferences, and extracts their behavioral patterns. Final phase recommends the entities that are the most similar to the users' interests. It is important to recognize that, within a common core structure of RS, there are variations from application to application. Some of the most sophisticated and heavily used RSs in industry are Last.fm, YouTube, and Amazon.  Furthermore, we can find the footprint of RS in the knowledge management system where RS tries to specify experts who have the most relevant knowledge about a particular topic . This category of RS is called expert recommendation system  or expert finding system. So, it is obvious that an ERS has similar phases compared to general RSs. An ERS takes a user topic or query, traces a set of candidates' expertise, learns their expertise patterns, and finally produces a list of experts sorted by a score. Each candidate's score indicates the degree of this candidate's relevant expertise with the given topic.  In the most studies, the candidates' expertise is defined as content-based information and non-content-based information . Content-based information is candidates' shared textual content like their articles, questions, answers and so on. In contrast, candidates' interactions with each others in social networks make non-content-based information. Depending on the application scenarios, each ERS has its own set of contextual information. For example, in academic environment, the attempt is to detect researchers who have the subject areas related to the query. This detection is based on the content of the articles published by them and their co-author relations in different papers. However, in Community Question Answering , the main goal is to find the users with expertise and willingness to answer the given questions in terms of the content of the question asked and the answered posted by them, and their question-answer relations .  With a brief look at previous studies, it can be concluded that there are three different outlooks on ERSs. In one of the attitudes, studies have focused on the textual expertise of candidates. These works have used text mining or information retrieval techniques and selected ones as experts who their published items are semantically relevant to the query . On the other hand, some other researches have investigated the social relations between candidates and represented their connections as a graph . After that,  social network analysis and mining techniques, such as page ranking algorithms, are applied on this graph to identify important candidates and rank them. Moreover, recent studies have shown that the combination of different types of expertise information have notable performance compared to other. A number of them have integrated textual expertise and social network connection information with linear or nonlinear functions. %It is important to notice that each kind of expertise has a diverse priority in various ERS applications. Also, to achieve higher accuracy, a few authors proposed the usage of heterogeneous network which is a combination of the users' interactions in social networks and their question閳ユ彸nswer relationships in CQAs besides bearing in mind the content of questions and answers.  In recent years, multimodal machine learning has been attracting attention. This popularity is because of huge multimodal content being generated by the users of social media networks  . The goal of multimodal machine learning is to create a joint model that can retrieve contextual information from multiple modalities . In this research, we aim to find academic experts that whether using a multimodal learning approach provides an effective solution for ERS or not. Also, the other purpose of our work is to solve the expert finding problem as a multi-label classification task. In such a way, we combine text  and graph  information in a multimodal approach. The text component is converted into vector using BERT Transformer. On the other hand, to obtain the node representation of candidates, a graph embedding technique, ExEm introduced in , is used. Also, normalized h-index value of candidate is added as another feature. Then, the captured fusion features are fed to train the classifier. We evaluate BERTERS on the multi-label classification  and visualization tasks. However, to the best of our knowledge, we present the first approach in field of expert recommendation using multimodal learning and transformers.  The rest of the paper is structured as follows: Section  reviews the related works. Section  discusses the background of the research. Section  presents our proposed method and explains it in detail.   The descriptions of the dataset and the tasks that are used to test our proposed method and parameter setting are presented in Section .  Section  provides and discusses the experimental results. Finally, Section  concludes the paper.  
"," The objective of an expert recommendation system is to trace a set of candidates' expertise and preferences, recognize their expertise patterns, and identify experts.  In this paper, we introduce a multimodal classification approach for expert recommendation system . In our proposed system, the modalities are derived from text  and graph  information. BERTERS converts text into a vector using the Bidirectional Encoder Representations from Transformer . Also, a graph Representation technique called ExEm is used to extract the features of candidates from co-author network. Final representation of a candidate is the concatenation of these vectors and other features. Eventually, a classifier is built on the concatenation of features.  This multimodal approach can be used in both the academic community and the community question answering. To verify the effectiveness of BERTERS, we analyze its performance on multi-label classification  and visualization tasks.",109
"  As a response to the worldwide COVID-19 pandemic, on March 13, 2020, the Allen Institute for AI  released the COVID-19 Open Research Dataset . With regular updates since the initial release , the corpus contains around 188,000 scientific articles , including most with full text, about COVID-19 and coronavirus-related research more broadly . These articles are gathered from a variety of sources, including PubMed, a curated list of articles from the WHO, as well as preprints from arXiv, bioRxiv, and medRxiv. The goal of the effort is ``to mobilize researchers to apply recent advances in natural language processing to generate new insights in support of the fight against this infectious disease.'' We responded to this call to arms.  As motivation, we believe that information access capabilities  can be applied to provide users with high-quality information from the scientific literature, to inform evidence-based decision making and to support insight generation. Examples include public health officials assessing the efficacy of wearing face masks, clinicians conducting meta-analyses to update care guidelines based on emerging studies, and virologist probing the genetic structure of COVID-19 in search of vaccines. We hope to contribute to these efforts via a three-pronged strategy:  [leftmargin=*] , initially described in~.     All three efforts have been successful. In the ongoing TREC-COVID challenge, our infrastructure and baselines have been adopted by many teams, which in some cases have submitted runs that scored higher than our own submissions. This illustrates the success of our infrastructure-building efforts . In the latest round 3 results, we report the highest-scoring run that exploits relevance judgments in a user feedback setting and the second-highest fully automatic run, affirming the quality of our own ranking models . Finally, usage statistics offer some evidence for the success of our deployed Covidex search engine .  
"," We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late March 2020. The Covidex is the user application component of our three-pronged strategy to develop technologies for helping domain experts tackle the ongoing global pandemic. In addition, we provide robust and easy-to-use keyword search infrastructure that exploits mature fusion-based methods as well as standalone neural ranking models that can be incorporated into other applications. These techniques have been evaluated in the ongoing TREC-COVID challenge:\ Our infrastructure and baselines have been adopted by many participants, including some of the highest-scoring runs in rounds 1, 2, and~3. In round 3, we report the highest-scoring run that takes advantage of previous training data and the second-highest fully automatic run.",110
" Human languages evolve as complex adaptive systems, driven by micro-level processes and constraints , macro-level factors , and the history of their development . %Multi-Agent Reinforcement Learning  setting is a promising benchmark for simulating these processes as it maintains the flexibility of individual learners, while allowing to scale the simulation to relatively big populations. Human language evolution, convention formation, and decentralized group learning studies, in turn, can suggest the solutions to the optimization problems that arise in the MARLC.  Linguistic communication depends on the shared knowledge of word-to-meaning mapping conventions , upon which the population converges through the local interactions between agents, often with no central controller available . Empirical studies of human learning demonstrate that groups quickly converge on new communicative conventions in ``decentralized"" settings %, even when prevented from using natural languages  .  Multi-agent reinforcement learning to communicate , however, faces instability challenges if no central optimization is introduced . This influences the ability of groups consisting of reinforcement learners to converge on efficient and stable communication systems which are shared by all their members. Therefore, different methods of centralized control and optimization have been proposed to stabilize MARLC . Central optimization makes the simulations more brittle and less flexibly adaptive, and, potentially, less promising in developing communication systems as freely expressive and well-optimized for their users  as natural languages.   We argue that empirical evidence on individual- and population-level factors that drive decentralized learning in human groups can guide simulations of language evolution in MARLC settings. In this work, we explore whether the social network organization shapes the properties of communication systems that arise through decentralized MARLC in simplified settings.       Convergence of human groups on word conventions is dramatically affected by the social topology that determines the possible interactions between participants, as demonstrated by the naming game experiment %on large groups of participants . In particular, when arranged in a social network with many local connections ) or a randomly-connected network , large groups converge on many local word conventions, reaching no global consensus. However, if each person is equally likely to interact with any other person in the group ), global consensus is easily achieved with no centralization. Other studies on decentralized problem solving in human groups demonstrated that social network organization shapes the multi-agent optimization process, with different network types being beneficial for different types of optimization landscapes. High local connectivity supports independent local exploration, whereas high global connectivity helps groups to converge on a shared solution, choosing among the local ones .  In this study, we looked at how the type of social network organization, its average degree, and local connectivity affect the results of communication learning in groups of deep reinforcement learning agents. %In this work, we looked at whether the social network topology shapes the properties of communication systems that arise through decentralized MARLC in simplified settings. In particular, we tested effects of the type of social topology, average degree of the agents, and proportion of global connections in a social network.   
"," Social network structure is one of the key determinants of human language evolution. Previous work has shown that the network of social interactions shapes decentralized learning in human groups, leading to the emergence of different kinds of communicative conventions. We examined the effects of social network organization on the properties of communication systems emerging in decentralized, multi-agent reinforcement learning communities. We found that the global connectivity of a social network drives the convergence of populations on shared and symmetric communication systems, preventing the agents from forming many local ``dialects"". Moreover, the agent's degree is inversely related to the consistency of its use of communicative conventions. These results show the importance of the basic properties of social network structure on reinforcement communication learning and suggest a new interpretation of findings on human convergence on word conventions.% among interacting humans.  %We found that the proportion of global connections in a social network determines the convergence of populations on shared and symmetric communication systems",111
" \renewcommand\figurename{Fig.} The service modes of wireless communications are transferring from connection-oriented services , such as voice call and short message, to content-oriented services, such as on-demand multimedia video . The amount of data traffic is experiencing a more significant surge than ever before. It is predicted that the total amount of data traffic will reach 100 exabytes by 2023, and multimedia video services will account for most of the 100 exabytes . Under this circumstance, backhaul with finite bandwidth is expected to become increasingly restrictive when retrieving requested contents from the core network to wireless edges , i.e., co-existing base stations  and user equipments. The limited capacity of backhaul is one of the most restrictive factors, especially for time-sensitive and real-time video services. Aiming to relieve this pressing limitation of backhaul and mitigate service delays, wireless caching is proposed as a promising technique, and attracts strong attention in the Fifth Generation  communication networks and beyond .  With proactive caching enabled in wireless edges, video files requested by users can be pre-fetched to the local storage of wireless edges via backhauls . The content placement is performed in light-traffic time periods. Cached contents can be delivered to the users, if requested. According to the types of cached contents, wireless caching can be typically classified into uncoded caching and coded caching. Earlier studies focused on the design of uncoded caching , in which uncoded video files, especially those popular ones, are placed in the local caches of wireless edges. Later, wireless caching is extended to coded caching , where complete videos are firstly encoded into different data packets and then these coded packets are locally stored by the proposed caching strategies.  Among many caching schemes, random caching, also known as probabilistic caching, is an important class of wireless caching , where complete video files or their combinations are prefetched to be cached under a certain caching distribution which can be optimized. In  and , by optimizing the successful transmission probabilities, the random caching distributions were determined. The authors of  derived the content hit probability and its approximation for throughput analysis. By maximizing these two metrics, the caching probabilities were optimized. In our recent paper , we studied random caching in heterogeneous network. The random caching probabilities were optimized to maximize the energy efficiency of the considered network.  With no assistance of BSs, device-to-device  communications allow users to establish direct links with their nearby neighbors. This helps reduce the overall transmission power of the system, and improve the system throughput . By integrating wireless caching into D2D communications, data traffic can be offloaded from small-cell BSs  and macro-cell BSs , relieving traffic congestion and reducing service delay . Chen .  evaluated the offloading gain and energy cost of D2D helpers, when the offloading opportunity was maximized. In , a machine learning model was proposed to capture the content popularity and request preference in D2D communications. The authors of  focused on the energy cost of D2D helpers, and proposed two hybrid caching schemes to reduce the cost. To optimize the system throughput, Zhang .  took both D2D-link scheduling and resource allocation into account in single-hop D2D communications.  Given the limited backhaul capacity, ever-changing channel conditions and varying user requirements, multi-quality video services are in increasingly high demand, including multimedia services for standard definition videos  and high definition videos . To provide diversified perceptual viewing experiences to mobile users, scalable video coding , developed for advanced video coding  , has attracted a lot of interest. With the aid of SVC, each video can be divided into a base layer  and several enhancement layers  . %Each layer provides a different quality level . The BL contains the most basic information of the scalable video, and the file only containing the BL can be decoded as SDV, which has the lowest viewing quality. Successive ELs, together with the BL, can provide HDV. More layers provide better video quality, and the video with all divided layers can exhibit the most excellent viewing quality . More technical details for the encoding and decoding process of SVC can refer to . %With these layers, it is possible to offer on-demand video services with multiple quality levels %by removing or supplementing some of the content layers. SVC has been applied to wireless caching in the literature. The authors of  maximized the total throughput of cache-enabled heterogeneous network by jointly optimizing SVC-based retrieving decision and data rate allocation. In , given the layered structure of video files, the data traffic delivered over backhaul was minimized. In our earlier work , we proposed an SVC-based layer placement scheme and maximized the average amount of offloaded traffic, so that most data traffic was retrieved from SBSs and the pressure was relieved on the MBS.  For large-scale video transmissions, the limited backhaul capacity is often the bottleneck of the system. Congestions in backhaul would lead to unacceptable latency. Hence, effective performance metric of service delay is crucial, and needs to be carefully designed . Relying on queuing theory, the authors of  derived the average delay for unit request, and minimized the delay with the greedy algorithm. A weighted average delay for unit request was considered in , through which the bandwidth allocation and caching probability distribution were yielded. In , a learning-based caching scheme was devised in D2D-assisted network, with the objective of minimizing the average transmission delay. The delay was also minimized by jointly designing the caching and user association strategies in . As mentioned earlier, mobile users can request different viewing qualities according to their preference or network states, while the study of SVC-supported wireless caching is still in a very earlier stage. On the other hand, provided SVC is in place, the unnecessary video layers may not need to be delivered. This can significantly reduce the service delay. Therefore, delay analysis of SVC-based video retrievals is important.  This paper presents a new random caching scheme in D2D-assisted three-tier heterogeneous network, consisting of D2D, SBS and MBS tiers, to minimize the service delay. To provide diversified viewing qualities of video services, each video file is encoded by SVC. A super layer, containing the BL and several ELs, is delivered for providing multi-quality multimedia video services. A user can be served by the nearest D2D helper or SBS which caches the requested super layers. When requested contents cannot be locally provided, the nearest MBS is responsible for retrieving the contents from the core network via its backhaul at the additional cost of resource and latency.  In the proposed SVC-based random caching scheme, D2D helpers and SBSs randomly select super layers to cache, and the caching probabilities can adapt to the delay performance of the three-tier heterogeneous network. The key contributions can be summarized as follows.    The rest of this paper is arranged as follows. Section II presents the network model, channel model and SVC-based random caching scheme. In Section III, we first define the service delay, and then derive the successful transmission probabilities to establish the expression for service delay. In Section IV, the delay minimization problem is formulated and solved. Numerical results are presented in Section V. Finally, concluding remarks are provided in Section VI. 
"," Aiming to minimize service delay, we propose a new random caching scheme in device-to-device -assisted heterogeneous network. To support diversified viewing qualities of multimedia video services, each video file is encoded into a base layer  and multiple enhancement layers  by scalable video coding . A super layer, including the BL and several ELs, is transmitted to every user. We define and quantify the service delay of multi-quality videos by deriving successful transmission probabilities when a user is served by a D2D helper, a small-cell base station  and a macro-cell base station . We formulate a delay minimization problem subject to the limited cache sizes of D2D helpers and SBSs. The structure of the optimal solutions to the problem is revealed, and then an improved standard gradient projection method is designed to effectively obtain the solutions. Both theoretical analysis and Monte-Carlo simulations validate the successful transmission probabilities. Compared with three benchmark caching policies, the proposed SVC-based random caching scheme is superior in terms of reducing the service delay.",112
"  It has been well established that the Internet, especially social  networks, provides a platform for ``viral"" spread of information at  rates faster than even a fully connected traditional networks  .  Depending on the actors involved, this could either be used for  societal good or ill. For e.g, a rumor about an explosion in the White House caused the Dow Jones Industrial Average to immediately plunge and the S\&P 500 was reported to have lost \$136.5 billion in market cap, taking the reach of rumors into the economic domain . In 2016 during the politically divisive Brexit and US elections, fake news outpaced real news on Facebook . Note that we use ``rumor"" and ``fake news"" interchangeably in this work, as is common in related work.  Given these very real social and economic implications of rumors in social media, automatic detection of rumors has seen a significant surge in research. Existing work in this area use some aspects of the news item like  news item text content,   comments on the news item  user characteristics and   propagation paths of the item within the network.  Some  researchers have tackled this problem by creating knowledge graphs that are built by crawling the web for raw facts  and then further processing and cleaning it up and using it to fact check . The problem with this approach is that it is less suitable for detecting rumors in evolving content that don't yet have a representation in the knowledge graph. Among other methods that have used the content of the news item, approaches have ranged from using psycholinguistic features like sentiment , style features like readability  and assertive and factive verbs  with varying degrees of success.   Some authors have suggested that other characteristics may be useful to include in the detection process due to the insufficiency of the news content material, especially in microblogging sites like  Twitter .  Several researchers have introduced other information like user comments  along with news content; user characteristics like number of followers, the first to tweet a story etc . Still others have used the network structure and/or propagation path along with content .  A hybrid feature extraction unit  and a gated diffusive unit  were used to detect rumors in .  HFEU extracted explicit and latent features from the textual information; GDU effectively extracted relationship among news articles, creators and subjects. A fake news detector called event adversarial neural networks  that includes a multi-modal feature extractor, a fake news detector and an event discriminator which co-operatively learns event non-specific features to discriminate between fake and real news was proposed in .   More recently, authors of  argued  that interpretable news feed generator algorithms  could  reduce their misuse by improving user awareness and system transparency.  T-SNE based methods were provided in  which could indicate the usefulness of learned features for rumor classification. %Done FILL: For what?  Research has now begun in explainable rumor detection algorithms .   Another debate that is often waged in the AI community, is whether handcrafted  features should/can be incorporated in the AI engine. In this work we provide a framework that can be used to explore this question by  including both handcrafted  and latent features for the rumor detection problem.   Specifically, we design an explainable deep learning architecture  using attention mechanism to detect  rumors using multiple types of features. Our work is inspired by  but with some differences and can be thought of as a generalization to multiple class of features. The main contributions of our work are:      
"," \label{sec:abs}  With social media becoming  ubiquitous, information consumption from this media has also increased.  However, one of the serious problems that has emerged with this increase, is the propagation of rumors.  Therefore, rumor identification is a very critical task with significant implications to economy, democracy as well as public health and safety.  We tackle the problem of automated  detection of rumors in social media  in this paper by designing a modular  explainable architecture that uses both latent  and handcrafted features and can be expanded to as many new classes of features as desired.  This approach will allow the end user to not only determine whether the piece of information on the social media is real of a rumor, but also give explanations on why the algorithm arrived at its conclusion. Using attention mechanisms, we are able to interpret the relative importance of each of these features as well as the relative importance of the feature classes themselves.  The advantage of this approach is that the architecture is expandable  to more handcrafted features as they  become available and also to conduct extensive testing to determine the relative influences of theses features in the final decision. Extensive experimentation on popular  datasets and  benchmarking against eleven  contemporary algorithms, show that  our approach performs significantly better in  terms of F-score and accuracy  while also being interpretable.",113
"  In many circumstances, the only difference between a completed suicide and a suicide attempt is slightly greater pressure applied to a trigger. In either case the importance of gaining a greater understanding of the psychological conditions surrounding such a tragic event is immediately apparent; what leads an individual to contemplate, and perhaps commit, such an act? Similarly, what sorts of thoughts and feelings does one encounter when experiencing suicidal ideation? And how might our understanding of these phenomena aid in improving prevention efforts? Although these are challenging questions, suicide notes represent one potential window into the psychology of individuals who complete suicide. By analyzing the language and contents of suicide notes, we can gain unique insight into shared features of individual experiences and perhaps a greater understanding of the cognitive processes that accompany suicidal ideation.  Previous research on suicide notes has highlighted specific properties of such notes in an attempt to better understand what characteristics stand out and differentiate them from other types of texts. Some work has focused on studying the contents of suicide notes , including dominant emotional themes , and key motives . In general, these studies have focused on answering the question: what is in a suicide note? That is, what are the contents that we most consistently observe when comparing notes from people that committed suicide? For example, Al-Mosaiwi and Johnstone recently found that the vocabulary used by individuals at risk of suicide was different from those who suffered from other mental disorders related to depression and anxiety. Individuals who experienced suicidal ideation tended to utilize different vocabularies and mainly absolutist words, indicating that suicide notes have their own emotional and lexical footprints.   Sentiment analysis has been further applied to the goal of comparing how the emotional contents of suicide notes are categorized by learning algorithms versus trained clinicians, as well as whether or not such algorithms can reliably distinguish between genuine and simulated suicide notes.  These automated text-analysis techniques offer some powerful advantages over the standard, qualitative approaches that have commonly been applied to the study of suicide notes by clinical psychologists. For one, quantitative methods -- such as those used in sentiment analysis and emotional profiling -- allow for the use of more objective criteria and clearer operationalizations of psychological constructs . Qualitative methods, on the other hand, depend upon human judges to code and interpret texts, then compare ratings to assess the consistency of their conclusions. Thus, there may be a high degree of uncertainty and limited reliability when such techniques are used to make inferences. Moreover, automatic approaches to text analysis allow researchers to evaluate millions of lines of text in very short amounts of time.  On the other hand, complex statistical/machine learning models often produce results that are difficult to understand and thus may not be very helpful for tasks different than prediction such as explanation.    In the present work we apply network science methods to the analysis of genuine suicide notes. Importantly, we show how network modeling can be used to expand the text-analytic toolbox in psychology and provide novel ways of answering complex research questions about text data .  In contrast with other automated approaches to text analysis, network models allow researchers to encode not only, e.g., word sentiment, but also the broader set of connections that each word has with its surrounding text. This allows one to track not only which words appear more or less often in a sample of texts, but also how they are used and in what contexts in comparison with other linguistic baselines.  Additionally, unlike typical black box models, network methods are fully transparent and produce results which are often much easier to interpret.  Hence, network modeling represents an approach to the study of text data that can further elucidate the structure of human texts and potentially reveal how concepts are perceived, organized and interconnected in the human mind.    Language guarantees an expression of people's perceptions through semantic content and emotions. Semantic frame theory indicates that the meaning attributed by people to a given concept can be reconstructed by observing the relationships and conceptual associations attributed to that concept in text or speech. Words in a given semantic frame elicit different combinations of emotions, i.e. emotional profiles, which characterize the emotional content of a text.  Network science provides tools to quantify and reconstruct both semantic frames and emotional associations, serving as a framework for the quantitative identification of the way people perceive events and happenings . In comparison to more opaque machine learning techniques, networks have the advantage of transparently representing a proxy for the associative structure of language in the human mind, within the cognitive system apt at acquiring, storing and producing language, i.e. the mental lexicon. Supported by psycholinguistic inquiries into the mental lexicon , complex networks built from texts can open a window into people's mindsets . Focus here is given to reconstructing the collective mindset as expressed in the last written words left by people who committed suicide.    In this manuscript we consider the content of suicide notes as an observable realization of otherwise unobservable mental states and suicidal ideation of their authors. In order to map out relationships between main concepts and the emergent semantics of suicide notes we reduced raw texts to two different network representations: co-occurrence  and subject-verb-object  networks. See Fig. and Materials and Methods section for details. For comparisons we used a network representation of mind-wandering based on free associations .   Unlike previous approaches, we do not aim to discriminate suicide notes from other types of text. Instead, our focus is on quantitatively understanding the mindset behind suicidal ideation of people who committed suicide through their final notes.    Study 1 investigates the ``emotional syntax'' of suicide notes, analyzing whether the connectivity and configuration of words is somehow related to their valence. We use structural balance theory to assess the degree of balance in the network and determine how valence is organized among neighboring words. We extend previous research by:  studying the emotional content of suicide notes and  mapping how sentiment is organized in the collective mindset around suicidal ideation. Study 2 focuses on subject-verb-object relationships to highlight self-perceptions in suicide notes. Study 3 combines network centrality, semantic frames and emotional data in order to describe and quantify typical emotions associated with different concepts in suicide notes. We conclude with a general discussion of the relevance of this study vis-鑴-vis previous results and current gaps in the literature.  
"," Understanding how people who commit suicide perceive their cognitive states and emotions represents a crucially open scientific challenge. We build upon cognitive network science, psycholinguistics and semantic frame theory to introduce a network representation of suicidal ideation as expressed in multiple suicide notes. By reconstructing the knowledge structure of such notes, we reveal interconnections between the semantic ideas and emotional states of people who committed suicide through structural balance theory, semantic prominence and emotional profiling. Our results indicate that connections between positively- and negatively-valenced terms give rise to a degree of structural balance that is significantly higher than in a null model where the affective structure is randomized and in a linguistic baseline model capturing mind-wandering in absence of suicidal ideation. We show that suicide notes are affectively compartmentalized such that positive concepts tend to cluster together and dominate the overall network structure. Notably, this positive clustering diverges from perceptions of self, which are found to be dominated by negative, sad conceptual associates in analyses about subject-verb-object structure and emotional profiling. A key positive concept is ``love'', which integrates information relating the self to others in ways that are semantically prominent across suicide notes. The emotions populating the semantic frame of ``love'' combine joy and trust with anticipation and sadness, which can be linked to psychological theories of meaning-making as well as narrative psychology. Our results open new ways for understanding the structure of genuine suicide notes and may be used to inform future research on suicide prevention.",114
" % {\let\thefootnote\relax}% In recent years, the machine learning research community has devoted substantial energy to scaling neural networks to enormous sizes. Parameter-counts are frequently measured in billions rather than millions , with the time and financial outlay necessary to train these models growing in concert . These trends have been especially pronounced in natural language processing , where massive BERT models---built on the Transformer architecture  and pre-trained in a self-supervised fashion---have become the standard starting point for a variety of downstream tasks . Self-supervised pre-training is also growing in popularity in computer vision , suggesting it may again become a standard practice across deep learning as it was in the past .  In parallel to this race for ever-larger models, an emerging subfield has explored the prospect of training smaller  in place of the full models without sacrificing performance . For example, work on the    demonstrated that small-scale networks for computer vision contain sparse,    capable of training in isolation from initialization to full accuracy. In other words, we could have trained smaller networks from the start if only we had known which subnetworks to choose. Within the growing body of work on the lottery ticket hypothesis, two key themes have emerged:  Initialization via pre-training. In larger-scale settings for computer vision and natural language processing , the lottery ticket methodology can only find matching subnetworks at an early point in training rather than at random initialization. Prior to this point, these subnetworks perform no better than those selected by pruning randomly. The phase of training prior to this point can be seen as dense pre-training that creates an initialization amenable to sparsification. This pre-training can even occur using a self-supervised task rather than the supervised downstream task .  Transfer learning. Finding matching subnetworks with the lottery ticket methodology is expensive. It entails training the unpruned network to completion, pruning unnecessary weights, and  the unpruned weights back to their values from an earlier point in training . It is costlier than simply training the full network, and, for best results, it must be repeated many times iteratively. However, the resulting subnetworks transfer between related tasks . This property makes it possible to justify this investment by reusing the subnetwork for many different downstream tasks.  These two themes---initialization via pre-training and transfer learning---are also the signature attributes of BERT models: the extraordinary cost of pre-training is amortized by transferring to a range of downstream tasks. As such, BERT models are a particularly interesting setting for studying the existence and nature of trainable, transferable subnetworks. If we treat the pre-trained weights as our initialization, are there matching subnetworks for each downstream task? Do they transfer to other downstream tasks? Are there  subnetworks that can transfer to many tasks with no degradation in performance? Practically speaking, this would allow us to replace a pre-trained BERT with a smaller subnetwork while retaining the capabilities that make it so popular for NLP work.  Although the lottery ticket hypothesis has been evaluated in the context of NLP  and transformers , it remains poorly understood in the context of pre-trained BERT models.% \footnote{A concurrent study by Prasanna et al.  also examines the lottery ticket hypothesis for BERTs. However, there are important differences in the questions we consider and our results. See Section  for a full comparison.} To address this gap in the literature, we investigate how the transformer architecture and the initialization resulting from the lengthy BERT pre-training regime behave in comparison to existing lottery ticket results. We devote particular attention to the transfer behavior of these subnetworks as we search for universal subnetworks that can reduce the cost of fine-tuning on downstream tasks going forward. In the course of this study, we make the following findings:       and transfer to other tasks while maintaining accuracy.   We conclude that the lottery ticket observations from other computer vision and NLP settings extend to BERT models with a pre-trained initialization. In fact, the biggest caveat of prior work---that, in larger-scale settings, matching subnetworks can only be found early in training---disappears. Moreover, there are indeed universal subnetworks that could replace the full BERT model without inhibiting transfer. As pre-training becomes increasingly central in NLP and other areas of deep learning , our results demonstrate that the lottery ticket observations---and the tantalizing possibility that we can train smaller networks from the beginning---hold for the exemplar of this class of learning algorithms.     
"," In natural language processing , enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the  has shown that models for NLP and computer vision contain smaller  subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40\% to 90\% sparsity. We find these subnetworks at  initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task  transfer ; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at \url{https://github.com/VITA-Group/BERT-Tickets}.",115
" Named Entity Recognition  is a vital part of information extraction. % It aims to locate and classify the named entities from unstructured text. % The different entity categories are usually the person, location and organization names, etc. % Kazakh language is an agglutinative language with complex morphological word structures. % Each root/stem in the language can produce hundreds or thousands of new words. % It leads to the severe problem of data sparsity when automatically identifying the entities. % In order to tackle the problem, Tolegen et al.   have given the systematic study for Kazakh NER by using conditional random fields. % More specifically, the authors assembled and annotated the Kazakh NER corpus , and proposed a set of named entity features with the exploration of their effects. % To achieve a state-of-the-art result for Kazakh NER compared with other languages' NER. % Authors have manually designed feature templates, which in practice is a labor-intensive process and requires a lot of expertise.  % With the intention of alleviating the task-specific feature engineering, there has been increasing interest in using deep learning to solve the NER task for many languages. % However, the effectiveness of the deep learning for Kazakh NER is still unexplored.  % One of the aims of this work is to use deep learning for Kazakh NER to avoid the task-specific feature engineering and to achieve a new state-of-the-art result.  % As in similar studies the neural networks  produces high results for English or for other languages by using distributed word representations. % But using only surface word representation in deep learning is may not enough to reach the state-of-the-art results for under-resourced MCLs. % The main reason is that deep learning approaches are data hungry, their performance is strongly correlated with the amount of available training data. %  In this paper, we introduce three types of representation for MCL including word, root and entity tag embeddings. % With the purpose of discovering how above embeddings contribute to model performance independently, we use a simple NN as the baseline to do the investigation. % We also improve this basic model from two perspectives. % One is to apply a tensor transformation layer to extract multi-dimensional interactions among those representations. % The other is to map each entity tag into a vector representation. % The result shows that the use of root embedding can lead to a significant improvement to the models in term of improving test results.  % Our NNs reached good outcomes by transferring intermediate representations learned on large unlabeled data. % We compare the NNs with the existing CRF-based NER system for Kazakh  and the other bidirectional-LSTM-CRF  that considered as the state-of-the-art in NER. % Our NNs outperforms the state-of-the-art and the result indicates that the proposed NNs can be potentially applied to other morphologically complex languages. %  The rest of the paper is organized as follows: Section 2 reviews the existing work.  % Section 3 gives the named entity features used in this work. % Section 4 describes the details of neural networks. % Section 5 reports the results of experiments and the paper is concluded in Section 6 with future work.  
"," We present several neural networks to address the task of named entity recognition for morphologically complex languages . % Kazakh is a morphologically complex language in which each root/stem can produce hundreds or thousands of variant word forms. % This nature of the language could lead to a serious data sparsity problem, which may prevent the deep learning models from being well trained for under-resourced MCLs. % In order to model the MCLs' words effectively, we introduce root and entity tag embedding plus tensor layer to the neural networks. % The effects of those are significant for improving NER model performance of MCLs.  % The proposed models outperform state-of-the-art including character-based approaches, and can be potentially applied to other morphologically complex languages.",116
"  Automatic video captioning is an emerging area in computer vision research that aims to generate textual descriptions of the visual components of a video. This has various applications including improving video accessibility for the blind and visually impaired, summarizing video, searching and indexing. Unfortunately, training models to do video captioning requires manual descriptions of every second of the video from a large corpus of representative videos. One of the largest current single-clip video captioning datasets, MSR-VTT, has only tens of thousands of unqiue uncorrelated videos whereas solving video captioning will likely require several orders of magnitude more to express the wide diversity of subjects, situations, and relationships possible in video data.  Active learning is a valuable approach in domains where unlabeled and partially labeled examples are readily available but obtaining manual annotations is expensive, such as is the case with automatic video captioning. However, while there has been significant investigation of active learning for computer vision tasks such as object recognition, object detection, video classification and video segmentation, video captioning has received comparatively little attention. The reason for this is likely rooted in the complexity of the label space. Video captioning requires both sequential input and output, dramatically increasing the complexity of traditional active learning frameworks. To our knowledge, this is one of the first works to define active learning strategies for efficiently collecting training sets for automatic video captioning.  In this paper we explore several active learning strategies for sequence to sequence active learning in video captioning, including uncertainty sampling based on label confidence, sequence entropy and query by committee methods. There are several unique challenges to active learning for deep sequence to sequence models: While traditional active learning methods  select one example at a time to label, retraining the model in its entirety after each new example selection, this strategy is impractical for training models such as transformer networks and LSTMs, due to increased training time  and increased inference time . Thus, it is far more efficient to select a large batch of examples at a time to label when using a crowd-sourced collection process . Traditional batch-active learning often uses ranking functions which are intractable in deep sequence to sequence learning , making active learning for video description a challenging problem, with no tractable solutions for deep neural networks.  In this work we conduct a thorough empirical analysis of various active learning strategies on two recent and standard video captioning datasets, MSR-VTT and LSMDC, using both transformer based and LSTM based captioning models, and describe a novel cluster-regularized method which is both tractable to compute, and provides strong performance in our test scenario. Our key contributions are:         
"," Automatic video captioning aims to train models to generate text descriptions for all segments in a video, however, the most effective approaches require large amounts of manual annotation which is slow and expensive. Active learning is a promising way to efficiently build a training set for video captioning tasks while reducing the need to manually label uninformative examples. In this work we both explore various active learning approaches for automatic video captioning and show that a cluster-regularized ensemble strategy provides the best active learning approach to efficiently gather training sets for video captioning. We evaluate our approaches on the MSR-VTT and LSMDC datasets using both transformer and LSTM based captioning models and show that our novel strategy can achieve high performance while using up to 60\% fewer training data than the strong state of the art baselines.",117
"  % With \gld, we provide the data on which to train, for example, domestic service robots that interact with objects typically found in living environments and understand them through natural language.  % Categories of eobjects within the dataset include food, home, medical and office supplies, and tools; these are selected for their relevancy towards domestic tasks that support our aim. % Color and depth images of each object are captured from multiple angles and paired with natural language descriptions of the objects from both text and speech domains. % This pairing of visual and linguistic features of the same object is well-suited to train neural networks that learn the multimodal associations among people and their interactions with objects in the physical world. % We analyze the text and speech descriptions, assessing differences in word choice and sentence structure, to understand the effect that the two domains have on subsequent grounded language training. % Demonstrating a use case of \gld, we train two manifold alignment models on both the text and speech domains to show the efficacy of these data for grounded language learning tasks.  Grounded language acquisition is the process of learning language as it relates to the world---how concepts in language refer to objects, tasks, and environments. Embodied language learning specifically is a significant field of research in NLP, machine learning, and robotics. There are many ways in which robots learn grounded language, but they all require either multimodal data or natural language data---usually both.  A significant goal of modern robotics research is the development of robots that can operate in human-centric environments. Examples include domestic service robots  that handle common household tasks such as cooking, cleaning, and caretaking, robots for elder care, assistive robotics for providing support to people with disabilities, and rehabilitation robotics. To be useful for non-specialists, such robots will require easy-to-use interfaces. Spoken natural language is an appropriate interface for such systems: it is natural, expressive, and widely understood, as shown by the proliferation of natural language-based home devices. To have a robotic system flexibly understand language in dynamic settings and realize it in physical, goal-oriented behaviors, it is necessary to ground linguistic and perceptual inputs to a learned representation of knowledge tied to actions.  Current approaches to grounded language learning require data in both the perceptual  and linguistic domains. While existing datasets have been used for this purpose, the language component is almost always derived from either textual input or manually transcribed speech. In practice, robots are likely to need to operate on imperfectly understood spoken language. To that end, we present the Grounded Language Dataset , which contains images of common household objects and their description in multiple formats: text, speech , and automatically recognized speech derived from the audio files. We present experiments that demonstrate the utility of this dataset for grounded language learning.     The primary contributions of this paper are as follows:          
"," Grounded language acquisition --- learning how language-based interactions refer to the world around them --- is a major area of research in robotics, NLP, and HCI. In practice the data used for learning consists almost entirely of textual descriptions, which tend to be cleaner, clearer, and more grammatical than actual human interactions. In this work, we present the Grounded Language Dataset , a multimodal dataset of common household objects described by people using either spoken or written language. We analyze the differences and present an experiment showing how the different modalities affect language learning from human input. This will enable researchers studying the intersection of robotics, NLP, and HCI to better investigate how the multiple modalities of image, text, and speech interact, as well as how differences in the vernacular of these modalities impact results.",118
"  Recent works have demonstrated an interest in unsupervised representation learning as a pretraining method to obtain good speech features for downstream tasks with little labelled data . While Contrastive Predictive Coding  and derivatives appear to be versatile methods for unsupervised representation learning , they do not yet reach the state-of-the-art  results on purely unsupervised learning metrics .   % Previous research has shown that  Data augmentation is useful for supervised training, and is also a key component in unsupervised setups in the image domain. It is not well established in unsupervised learning for speech, where the sequential nature of the signal may introduce specificities.   Our first objective is to explore several types of time-domain data augmentation  and several methods for augmenting in the contrastive framework  in English .  In a second stage, we extend the results to other languages  in the zero-resource 2017 benchmark.  Lastly, we show that data augmentation benefits semi-supervised training, using the Libri-light benchmark.  
"," % 150/200 words max Contrastive Predictive Coding , based on predicting future segments of speech based on past segments is emerging as a powerful algorithm for representation learning of speech signal. However, it still under-performs other methods on unsupervised evaluation benchmarks. Here, we introduce WavAugment, a time-domain data augmentation library and find that applying augmentation in the past is generally more efficient and yields better performances than other methods. We find that a combination of pitch modification, additive noise and reverberation substantially increase the performance of CPC , beating the reference Libri-light results with 600 times less data. Using an out-of-domain dataset, time-domain data augmentation can push CPC to be on par with the state of the art on the Zero Speech Benchmark 2017. We also show that time-domain data augmentation consistently improves downstream limited-supervision phoneme classification tasks by a factor of 12-15\% relative.   %\matthijs{mention that the best data augmentation can reduce the required training data by a factor 600 } % %\morgane{if we look at the ablation study in the appendix, it seems that the architecture, on LS-100, is responsible for about 1/3 of the improvement. But we must also consider other factors not mentionned in this paper: noise and speaker distribution.}",119
"  Recent developments in the fields of electronics, computations and data processing have led to an increased interest in smart assistants with speech interfaces. It is likely driven by the fact that usually people can learn to use speech for interaction intuitively without any special training  and make it a primary medium of information exchange. However, speech poses a major challenge to a machine when it comes to the task of extraction of information intended to be transmitted by a human speaker, also known as Spoken Language Understanding  . The key difficulty here is that speech is highly variable, e.g. depending on room acoustic, and contains rich information about speakers  . Some of them are not useful for SLU. The information extraction task is often performed on the text representation using Natural Language Understanding  methods , while Automatic Speech Recognition  systems  convert speech to text. ASR step removes redundant information from the input and provides some kind of normalized form on the output. At the same time it causes loss of potentially useful information that can not be encoded in the text representation, such as prosody, loudness and speech rate. The operation of finding the most probable sequence of words for speech input is computationally expensive. %, what makes it hard to implement in practice. This is partly solved by various heuristics avoiding exploration of less probable hypothesis , what in turn introduces additional errors propagated to NLU component. Finally, the sequential design of pipeline approach leads to unavoidable source of latency, because NLU component can not start its work before ASR is finished, and it is not desirable in the interactive context of smart assistant. The problems of pipeline approach described above can be solved by end-to-end SLU methods.  Existing works on end-to-end SLU modeling either focus on supervised downstream tasks, for example dialog act classification , intent detection , slot filling , independent intent detection and domain classification  and joint intent detection, domain classification and slot filling , or target a generic semantic embedding  usually inspired by such successful models as word embeddings Word2Vec  and contextual text embeddings BERT . Highly variable and complex nature of speech leads to large amounts of both data and computational resources required for SLU training compared to NLU training, especially for recently popular approach based on contextual embeddings. While data requirements could be satisfied for unsupervised approaches, computational resources are still a problem. Fortunately, most of the modern language processing methods, including ASR and NLU, are based on neural networks and deep learning. Deep learning offers an easy way to transfer knowledge between learned tasks. This technique is referred as transfer learning and it is successfully applied in both ASR  and NLU . Therefore, transfer learning should be a promising direction to explore for SLU as well. Several reports  indicate that transfer learning from audio modality through pretraining on ASR task or, alternatively, speech autoencoding, is helpful for downstream SLU tasks. Transfer learning from text modality, however, has been applied only for Speech2Vec  and SpeechBERT  so far.  We propose a novel method that combines parameters transfer from well trained end-to-end ASR systems  such as pretrained ESPnet  and end-to-end NLU models such as pretrained BERT  with Teacher-Student learning  for final alignment of SLU output space to NLU output space in order to construct end-to-end SLU model allowing few-shot transfer of downstream tasks from text to speech. By doing so, we enable pretrained end-to-end contextual embeddings such as BERT to process acoustic features.  In particular, we aim to generate fixed length vectors with semantic representation from speech segments of variable length. Transfer learning from both text and audio modalities makes our approach mostly similar to  and . In this work, we investigate utterance classification task and focus on zero-shot and few-shot cases, but the described method could be adopted to many types of SLU tasks. Although previous works described a number of experiments for such utterance classification tasks as dialog act classification  and intent classification , and we use the same datasets for the evaluation, we do not compare our results directly to these works, as this is outside of the scope of our work.  
"," Spoken language understanding is typically based on pipeline architectures including speech recognition and natural language understanding steps. These components are optimized independently to allow usage of available data, but the overall system suffers from error propagation. In this paper, we propose a novel training method that enables pretrained contextual embeddings to process acoustic features. In particular, we extend it with an encoder of pretrained speech recognition systems in order to construct end-to-end spoken language understanding systems. Our proposed method is based on the teacher-student framework across speech and text modalities that aligns the acoustic and the semantic latent spaces. Experimental results in three benchmarks show that our system reaches the performance comparable to the pipeline architecture without using any training data and outperforms it after fine-tuning with ten examples per class on two out of three benchmarks.",120
" Self-supervised learning of representations from large unlabeled datasets is a popular contemporary trend in machine learning. After being widely adopted in areas like natural language processing and computer vision, self-supervision is now rapidly developing as a noteworthy topic in audio and speech processing. Self-supervision aims to capture the most informative properties from the underlying structure of unlabeled data to learn generalized representations. This is extremely promising in problem settings involving a large amount of unlabeled data but limited labeled data. In the context of audio and speech processing, this is relevant to low resource languages, emotion recognition, cross-cultural speech recognition and other such problems with small-sized datasets.  Even though there has been recent research interest in self-supervised learning for speech data, most works focus only on the audio modality alone. Audiovisual speech data offers interesting possibilities for cross-modal self-supervision, which is something relatively lesser explored. In this work, we present a method for self-supervised representation learning of audio features that leverages both the audio and visual modalities. We demonstrate how generating a talking lip video from a single frame and the corresponding audio can be used as a pretext task for visual self-supervision to train a raw audio encoder. We combine this with audio-only self-supervision based on predicting informative audio attributes, similar to . This results in an audio encoder trained by joint audiovisual self-supervision. We evaluate the method on spoken word classification and achieve competitive results when comparing with existing self-supervised methods. Our method also results in significantly better performance when learning with limited data  for the downstream tasks. Importantly, our method also outperforms fully supervised training . Our observations motivate the utility of self-supervised pretraining for audio related tasks. We demonstrate that cross-modal supervision in audiovisual speech can learn better representations compared to unimodal audio-only or visual-only self-supervision.    Self-supervised learning has been very influential in recent advances in natural language processing  and computer vision . It is also beginning to mature as a relevant topic in audio and speech processing. CPC   was a seminal work in self-supervised learning which also demonstrated the applicability of contrastive self-supervised learning to audio.  Wav2vec  refines the idea from CPC specifically for speech. CPC based self-supervision has also been shown to generalize well to multiple languages . APC   is a similar approach that predicts the next token of a speech segment from the history. Another very relevant recent work is PASE  , which aims to learn multi-task speech representations from raw audio by predicting a number of handcrafted features such as MFCCs, prosody and waveform. Teacher-student models have also been explored for audio self-supervision where the trained model from a previous epoch acts as the teacher model for the next epoch . % Phase prediction  has also been proposed as an audio-based pretext task. WaveNet  is a generative model for raw audio waveforms that can be used for generic audio representations. All of the works discussed so far are unimodal audio-only self-supervised methods. There are also a few other works that utilize both audio and visual information. There are multiple ways to capture this cross-modal interaction including audiovisual synchronization , cross-modal transition modeling , cross-modal pseudolabel based clustering , contrastive learning , and audiovisual instance discrimination . However most of these works present cross-modal self-supervision in the context of generic audiovisual data, with application to tasks like video action recognition and acoustic scene classification. There is limited work that explores self-supervision specifically in the context of audiovisual speech. We have explored this concept in recent related work . This work extends the idea from our prior work. Specifically, we move from learning speech representations directly from raw audio instead of from mel features. We also adopt a different and more refined approach for audio-only self-supervision .  
"," The intuitive interaction between the audio and visual modalities is valuable for cross-modal self-supervised learning. This concept has been demonstrated for generic audiovisual tasks like video action recognition and acoustic scene classification. However, self-supervision remains under-explored for audiovisual speech. We propose a method to learn self-supervised speech representations from the raw audio waveform. We train a raw audio encoder by combining audio-only self-supervision  with visual self-supervision . The visual pretext task drives the audio representations to capture information related to lip movements. This enriches the audio encoder with visual information and the encoder can be used for evaluation without the visual modality. Our method attains competitive performance with respect to existing self-supervised audio features on established isolated word classification benchmarks, and significantly outperforms other methods at learning from fewer labels. Notably, our method also outperforms fully supervised training, thus providing a strong initialization for speech related tasks. Our results demonstrate the potential of multimodal self-supervision in audiovisual speech for learning good audio representations.",121
" Singing voice synthesis , which generates singing voices from lyrics, has attracted a lot of attention in both research and industrial community in recent years. Similar to text to speech  that enables machines to speak, SVS enables machines to sing, both of which have been greatly improved with the rapid development of deep neural networks. Singing voices have more complicated prosody than normal speaking voices, and therefore SVS needs additional information to control the duration and the pitch of singing voices, which makes SVS more challenging than TTS.  Previous works on SVS include lyrics-to-singing alignment, parametric synthesis, acoustic modeling, and adversarial synthesis. Although they achieve reasonably good performance, these systems typically require 1) a large amount of high-quality singing recordings as training data, and 2) strict data alignments between lyrics and singing audio for accurate singing modeling, both of which incur considerable data labeling cost. Previous works collect these two kinds of data as follows: [leftmargin=*]   As can be seen, the training data for SVS mostly rely on human recording and annotations. What is more, there are few publicly available singing datasets, which increases the entry cost for researchers to work on SVS and slows down the research and product application in this area. Considering a lot of tasks such as language modeling and generation, search and ads ranking, and image classification heavily rely on data collected from the Web, a natural question is that: Can we build an SVS system with data collected from the Web? While there are plenty of songs in music websites and mining training data from the Web seems promising, we face several technical challenges: [leftmargin=*]   In this paper, we develop DeepSinger, a singing voice synthesis system that is built from scratch by using singing training data mined from music websites. To address the above challenges, we design a pipeline in DeepSinger that consists of several data mining and modeling steps, including: [leftmargin=*]      Specifically, the detailed designs of the lyrics-to-singing alignment model and singing model are as follows: [leftmargin=*]       We conduct experiments on our mined singing dataset  to evaluate the effectiveness of DeepSinger. Experiment results show that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness.   The contributions of this paper are summarized as follows: [leftmargin=*]                 
"," In this paper\footnote{$^*$ Equal contribution. $\dagger$ Corresponding author.}, we develop DeepSinger, a multi-lingual multi-singer singing voice synthesis  system, which is built from scratch using singing training data mined from music websites. The pipeline of DeepSinger consists of several steps, including data crawling, singing and accompaniment separation, lyrics-to-singing alignment, data filtration, and singing modeling. Specifically, we design a lyrics-to-singing alignment model to automatically extract the duration of each phoneme in lyrics starting from coarse-grained sentence level to fine-grained phoneme level, and further design a multi-lingual multi-singer singing model based on a feed-forward Transformer to directly generate linear-spectrograms from lyrics, and synthesize voices using Griffin-Lim. DeepSinger has several advantages over previous SVS systems: 1) to the best of our knowledge, it is the first SVS system that directly mines training data from music websites, 2) the lyrics-to-singing alignment model further avoids any human efforts for alignment labeling and greatly reduces labeling cost, 3) the singing model based on a feed-forward Transformer is simple and efficient, by removing the complicated acoustic feature modeling in parametric synthesis and leveraging a reference encoder to capture the timbre of a singer from noisy singing data, and 4) it can synthesize singing voices in multiple languages and multiple singers. We evaluate DeepSinger on our mined singing dataset that consists of about 92 hours data from 89 singers on three languages . The results demonstrate that with the singing data purely mined from the Web, DeepSinger can synthesize high-quality singing voices in terms of both pitch accuracy and voice naturalness\footnote{Our audio samples are shown in \url{https://speechresearch.github.io/deepsinger/}.}.",122
"   {U}{nlike} humans, who are capable of self-learning through experiences and interactions, current real-world speech applications like automatic speech recognition  rely heavily on large amounts of human annotations. %. In order for the next generation of speech processing systems to exhibit similar levels of cognitive intelligence as humans, machines should be designed to learn from unlabeled data as humans do. In the era of big data, self-supervised learning has emerged as an attractive approach to leverage knowledge from a large amount of unlabeled data. Self-supervised learning leverage unsupervised pre-training tasks to train networks, and they have shown to be effective for improving downstream systems. %Hence, the need of self-supervised pre-training is rooted in our increasing demand to improve downstream speech systems like ASR, as there is always a limited amount of labeled data for supervised training.  Through self-supervised pre-training, learned models could be applied to downstream Speech and Language Processing  tasks through feature-based speech representation extraction, or fine-tuning as part of the downstream model. Speech representations are compact vectors which aim to capture high-level semantic information from raw speech. Thus, the goal of speech representation learning is to find a transform that maps the input acoustic features into such vectors. When the pre-trained networks are re-used as features, it provides a useful speech representation to reduce classifier complexity, makes high-level information more accessible, and ultimately improves downstream SLP tasks. Besides, speech representations also help transfer learning and adaptation across different data distributions. On the other hand, the fine-tuning approach uses the pre-trained model to initialize a downstream model for supervised training. The parameters of self-supervised learned models are good initialization for ASR encoders.  In self-supervised learning, an auxiliary task  is formulated, and models are trained to solve it. While solving the auxiliary task, the network is learning a function that maps input to desired representations that can be potentially transferred to multiple downstream tasks. The key tenet of self-supervised learning is the design of an auxiliary task, which allows the model to leverage knowledge from unlabeled data. As such, the formulation of the auxiliary task should be carefully chosen. The task should be hard enough for the model to learn high-level semantic properties, and not be too amiable for the model to exploit low-level shortcuts.  %In recent studies, learning under multiple auxiliary objectives have shown promising results. In this work, we propose TERA: Transformer Encoder Representations from Alteration, a multi-target auxiliary objective to pre-train Transformer Encoders. %TERA requires the model to predict real frames from corrupted frames, without using any labels. We introduce a total of three auxiliary objectives to form the multi-target pre-training scheme:  1) time alteration: reconstructing from corrupted blocks of time steps. 2) channel alteration: reconstructing from missing blocks of frequency channels. 3) magnitude alteration: reconstructing from altered feature magnitudes. These auxiliary objectives can be applied together or separately in the pre-training process. %During training, these auxiliary objectives are applied by dynamically sampling through a stochastic alteration policy to create random alterations. %The key idea behind all the auxiliary objectives is that if the model can predict the original frames from corrupted input, it should provide a good representation of the critical content. The model acquires information about the content around the corrupted or altered portions, and by reconstructing them, the model learns a more contextualized representation. We illustrated the framework in Fig.. Similar self-supervised frameworks have been widely studied . Unlike previous approaches that only employ reconstruction on the temporal axis, TERA considers three orthogonal axes, including temporal, channel, and magnitude. %Such self-supervised framework has been widely studied in several recent work. %In Section, we will give a thorough review of the related work, and point out the difference of TERA to previous work.  To evaluate TERA, we use downstream tasks of phoneme classification, speaker recognition, and automatic speech recognition . %We first demonstrate our approach can make more accurate phone and speaker predictions, outperforming surface features and previous works under standard benchmark settings. %Next, moving beyond classification tasks, we apply the learned model to improve strong supervised phonetic systems, where we use hybrid DNN/HMM models for automatic speech recognition . Also, we compare the effectiveness of each auxiliary objectives separately and in combination. As a result, we confirm that each of the proposed auxiliary objectives guides the model to learn a distinct aspect of speech: 1) The time alteration objective is effective in making more accurate phoneme prediction and speech recognition, as it leads the model to learn richer phonetic content. %and bidirectional context. 2) The channel alteration objective is effective in making more accurate speaker prediction, as it leads the model to learn speaker identity. 3) The magnitude alteration objective is effective in providing a performance boost for all tasks, as it potentially increases data diversity for pre-training.  Besides, we explore different knowledge transfer methods of the pre-trained model to downstream tasks. The methods include: 1) extract representations from the last layer,  2) combine representations from all hidden layers with a learnable weighted sum, and 3) fine-tuning the pre-trained model with the downstream model. Furthermore, we also explore using different acoustic features for reconstruction and find that they impact downstream performance and affect what the model learns. Finally, we investigate the problem of domain mismatch between the pre-training and downstream datasets, and the proposed approach is shown to be unaffected by the domain mismatch issue. For reproducibility of our results, we provide our implementation with pre-trained models and evaluation scripts in the S3PRL toolkit\footnote{{} }.  %This literature is organized as follows: %In Section, we introduce related works and discuss how our work is different from others. %In Section, we introduce the proposed TERA. %In Section, we describe how experiments are formulated to evaluate TERA. %In Section, we present experimental results and our findings.   %The contributions of our paper are as follows: %     %      
"," We introduce a self-supervised speech pre-training method called TERA, which stands for Transformer Encoder Representations from Alteration. Recent approaches often learn through the formulation of a single auxiliary task like contrastive prediction, autoregressive prediction, or masked reconstruction. Unlike previous approaches, we use a multi-target auxiliary task to pre-train Transformer Encoders on a large amount of unlabeled speech. The model learns through the reconstruction of acoustic frames from its altered counterpart, where we use a stochastic policy to alter along three dimensions: temporal, channel, and magnitude. TERA can be used to extract speech representations or fine-tune with downstream models. We evaluate TERA on several downstream tasks, including phoneme classification, speaker recognition, and speech recognition. TERA achieved strong performance on these tasks by improving upon surface features and outperforming previous methods. In our experiments, we show that through alteration along different dimensions, the model learns to encode distinct aspects of speech. We explore different knowledge transfer methods to incorporate the pre-trained model with downstream models. Furthermore, we show that the proposed method can be easily transferred to another dataset not used in pre-training.",123
" In contemporary pop music, the linguistic content in the singing voice is generally referred as lyrics and the process of automatic retrieval this lyrics content from singing voice can then be defined as Automatic Lyrics Transcription. The automatic retrieval of pronounced words from speech signals is a widely developed research field and the state of the art systems by today can be successfully applied to industrial applications. However, the same level of robustness has not yet been reached when the input is singing voice. According to prior research, there are several domain specific reasons word recognition performance reduces in singing including domain specific acoustic characteristics  and the alterations of word pronunciations. Specifically from a machine learning perspective, the main bottleneck for achieving a robust system is the availability of training data with fine-grained annotations, to be used in a supervised learning framework.   In this study, we exploit such large-scale singing voice dataset, DAMP - Sing! 300x30x2 - released by Smule \footnote{Smule is a commercial Karaoke singing application. More info at https://www.smule.com/}, where prompt-level\footnote{In Smule app, lyrics are prompted to the users as words or sentences depending on the song arrangement. Each prompted sentence or word annotation is referred as prompt-level annotation.}  annotations are provided. The dataset consists of monophonic Karaoke recordings of pop songs by multiple performers providing near 150 hours of trainable audio data. However, this dataset has not been widely utilized for the purpose of training a word recognition system. Through our proposed framework, we aim to highlight one way of utilization of this dataset in a complete ALT framework and further conduct an in-depth self-attention analysis via fine-tuning experiments.   A robust system for the retrieval of sung lyrics has a variety of potential applications in music information retrieval  related tasks and the music tech industry. In karaoke and music education apps, the recognition of sung words is essential for tracking a performance and providing feedback to the user. In combination with techniques like query-by-humming, ALT can be utilized for the song identification and metadata retrieval tasks.    Our system uses deep neural networks for building the final acoustic model that is composed of 2D convolutional layers at the front end for extracting more robust features followed by time-delay layers due to their capability of modeling long-term context information. A self-attention layer is added before the final projection layer for weighting the time context when computing the output activations for classification.  Overall, this paper targets at making the following contributions:          This paper is structured as follows: literature on ALT on monophonic singing recordings is reviewed . Then the details of the data used in training and evaluation are given . Then the proposed system  and the basis of our experiments  are explained. The results for each of experimental steps are shown and an in-depth analysis of the self attention parameters is performed . Finally, potential improvements to the proposed system are discussed .    
","  Speech recognition is a well developed research field so that the current state of the art systems are being used in many applications in the software industry, yet as by today, there still does not exist such robust system for the recognition of words and sentences from singing voice. This paper proposes a complete pipeline for this task which may commonly be referred as automatic lyrics transcription . We have trained convolutional time-delay neural networks with self-attention on monophonic karaoke recordings using a sequence classification objective for building the acoustic model. The dataset used in this study, DAMP - Sing! 300x30x2 is filtered to have songs with only English lyrics. Different language models are tested including MaxEnt and Recurrent Neural Networks based methods which are trained on the lyrics of pop songs in English. An in-depth analysis of the self-attention mechanism is held while tuning its context width and the number of attention heads. Using the best settings, our system achieves notable improvement to the state-of-the-art in ALT and provides a new baseline for the task.",124
"  %Given the limited amount of information from a single-microphone mixture recording, separating the constituent sources can be a challenging task. Considering also the limited computational resources that one might have at their disposal, the problem of training and deploying a separation model might become especially challenging. However, efficient sound source separation frameworks could be utilized towards enhancing the performance of various systems which generally require clean audio inputs .   The advent of the deep learning era has enabled the effective usage of neural networks towards single-channel source separation with mask-based architectures . Recently, end-to-end source separation in time-domain has shown state-of-the-art results in a variety of separation tasks such as: speech separation , universal sound separation  and music source separation . The separation module of ConvTasNet  and its variants  consist of multiple stacked layers of depth-wise separable convolutions  which can aptly incorporate long-term temporal relationships. Building upon the effectiveness of a large temporal receptive field, a dual-path recurrent neural network   has shown remarkable performance on speech separation. Demucs  has a refined U-Net structure  and has shown strong performance improvement on music source separation. Specifically, it consists of several convolutional layers in each a downsampling operation is performed in order to extract high dimensional features. A two-step approach has been introduced in  and showed that universal sound separation models could be further improved when working directly on the latent space and learning the ideal masks on a separate step.  Despite the dramatic advances in source separation performance, the computational complexity of the aforementioned methods might hinder their extensive usage across multiple devices. Specifically, many of these algorithms are not amenable to, e.g., embedded systems deployment, or other environments where computational resources are constrained.  Additionally, training such systems is also an expensive computational undertaking which can amount to significant costs.  Several studies, mainly in the image domain, have introduced more efficient architectures in order to overcome the growing concern of large models with high computational requirements. Models with depth-wise separable convolutions  have shown strong potential for several image-domain tasks  while significantly reducing the computational requirements. Thus, several variants such as MobileNets  have been proposed for deep learning on edge-devices. However, convolutions with a large dilation factor might inject several artifacts and thus, lightweight architectures that combine several dilation factors in each block have been proposed for image tasks . More recent studies propose meta-learning algorithms for optimizing architecture configurations given specific computational resource and accuracy requirements .  Despite the recent success on low-resource architectures on the image domain, little progress has been made towards proposing efficient architectures for audio tasks and especially source separation. In  a WaveRNN is used for efficient audio synthesis in terms of floating point operations  and latency. Other studies have introduced audio source separation models with reduced number of trainable parameters  and binarized models . In this study, we propose a novel efficient neural network architecture for audio source separation while following a more holistic approach in terms of computational resources that we take into consideration . Our proposed model performs SUccessive DOwnsampling and Resampling of Multi-Resolution Features ({https://github.com/etzinis/sudo\_rm\_rf}}.   
"," In this paper, we present an efficient neural network for end-to-end general purpose audio source separation. Specifically, the backbone structure of this convolutional network is the SUccessive DOwnsampling and Resampling of Multi-Resolution Features  as well as their aggregation which is performed through simple one-dimensional convolutions. In this way, we are able to obtain high quality audio source separation with limited number of floating point operations, memory requirements, number of parameters and latency. Our experiments on both speech and environmental sound separation datasets show that \sudo performs comparably and even surpasses various state-of-the-art approaches with significantly higher computational resource requirements.",125
"  Today's state-of-the-art in language modeling for ASR relies on neural Language Models , capable of handling continuous space and thereby outperforming traditional Back-off N-gram LMs . BNLMs cannot exploit long context based syntactic dependencies and are also less flexible in terms of generalization for unseen cases, as semantic knowledge  is not captured while training them.   % Neural LMs however have an undesired property, they are computationally very heavy in decoding, so neural LMs cannot be effectively used in a single decoding pass, they are rather exploited by rescoring lattices obtained from a first decoding pass with a BNLM. It is obvious, but can also be shown, that information is lost during the first decoding pass, as the pruning of the recognition network is based only on short context syntax, discarding both longer context syntactic and quasi all semantic knowledge. Another problem arising is the increased latency of the system through the two decoding passes, which hampers exploitation in strict online requirements.  To reduce these limitations in exploiting neural LMs for ASR, several solutions have been proposed. In it was shown that using the neural LM to generate an augmented training corpus to train an improved BNLM is the best performing strategy. Such a BNLM trained on augmented corpus can be used in a single pass or in the first pass of decoding. Sometimes these are called approximative models as they try to capture the knowledge of the neural model through their augmented training corpus. %Recently several studies concentrated on this approach. Suzuki et al. uses a domain balanced mixture of the training corpora to train a shallow RNNLM for text generation, and improve speech recognition results for Japanese, Korean and English. Wang et al. report using general domain pre-trained Transformer to augment text corpora used to train LMs. They demonstrate that the pre-trained and fine-tuned Transformer performs significantly better in data augmentation than LSTMs or simple in-domain Transformer models. %Both and underline that this approach is particularly useful if in-domain training data is relatively scarse .  % Another burden of language modeling for morphologically rich languages are the different syntactic properties of the language compared to English. Heavy agglutination results in much larger vocabularies, which is a problem in itself, but causes other problems too: %beside handling more words,  individual word forms occur less often and hence, the size of the training corpus should accordingly be augmented to maintain the predictive power of the dataset. Moreover, as suffixes express grammatical relations usually provided by word order in English, morphologically rich languages tend to be more permissive in choosing word order, leading to higher variation. This impairs BNLM estimation badly, but may also cause that word embeddings become less powerful in terms of syntactic and semantic consistency, even despite using long context windows. %%%BLIND VERSION: %This impairs BNLM estimation badly, but may also cause that word embeddings become less powerful in terms of syntactic and semantic consistency~[BLIND], even despite using long context windows.  To alleviate these problems linked to the different organization of morphologically rich languages, subword unit modeling is an often used alternative. Subword unit based ASR has been demonstrated to improve WER for several morphologically rich languages. Suzuki et al. use subword approach for data augmentation to enrich text corpora to train BNLM, but compose these subwords back into words to prepare the final LM, unlike our approach that retokenizes words into subword units in the final LM.    % In this paper we aim to improve LM for an online call center ASR system in the morphologically rich Hungarian. We use parliamentary text to pre-train a GPT-2 structure Transformer LM, and fine-tune it on the target domain. With this model we generate training text for a BNLM. We demonstrate that such Transformer based data augmentation is efficient in morphologically rich Hungarian, %which improves ASR  if vocabulary is large enough and a large BNLM is used. Retokenizing the augmented training corpus to subword units, and training a subword-based BNLM on it, we demonstrate that  the ASR accuracy further improves compared to the word based baseline augmented BNLM, and  the footprint and complexity of the resulting subword unit augmented BNLM significantly decrease. As subword unit LMs are known to perform better on a wide range of morphologically rich languages, we hypothesize that our approach is transferable to other such languages. We consider as novelties of our paper the following:  we propose the retokenization of the Transformer augmented LM training corpus; % and use it in online ASR with single pass decoding;  we are the first to use the GPT-2 Transformer structure to augment LM training corpora;  we are the first to apply a Transformer based LM for a Hungarian ASR task;  we demonstrate that the subword-based neural text augmentation can be exceptionally efficient in modeling OOV words.  
"," Recently Deep Transformer models have proven to be particularly powerful in language modeling tasks for ASR. Their high complexity, however, makes them very difficult to apply in the first  pass of an online system. Recent studies showed that a considerable part of the knowledge of neural network Language Models  can be transferred to traditional n-grams by using neural text generation based data augmentation. In our paper, we pre-train a GPT-2 Transformer LM on a general text corpus and fine-tune it on our Hungarian conversational call center ASR task. We show that although data augmentation with Transformer-generated text works well for isolating languages, it causes a vocabulary explosion in a morphologically rich language. Therefore, we propose a new method called subword-based neural text augmentation, where we retokenize the generated text into statistically derived subwords. We compare Morfessor and BPE statistical subword tokenizers and show that both methods can significantly improve the WER while greatly reducing vocabulary size and memory requirements. Finally, we also demonstrate that subword-based neural text augmentation outperforms the word-based approach not only in terms of overall WER but also in recognition of OOV words.",126
" Deep neural networks  play a central role in state-of-the-art automatic speech recognition  systems. When designing these systems, a set of DNN structure design decisions such as the hidden layer dimensionality and connectivity need to be made. These decisions are largely based on expert knowledge or empirical choice. As explicitly training and evaluating the performance of different network architectures is highly expensive, it is preferable to use automatic architecture design techniques.   To this end, neural architecture search  approaches have gained increasing interests in recent years. The key objectives of NAS methods are three fold. First, it is crucial to produce an accurate performance ranking over different candidate neural architectures to allow the best system to be selected. Second, when operating at the same level of accuracy performance target, preference should be given to simpler architectures with fewer parameters in order to minimize the risk of overfitting to limited data. Furthermore, to ensure scalability and efficiency on large data sets, a search space containing all candidate systems of interest needs to be defined.   Earlier forms of NAS techniques were based on neural evolution, where genetic algorithms were used to randomly select architecture choices at each iteration of mutation and crossover. Bayesian NAS methods based on Gaussian Process was proposed in. Reinforcement learning  based NAS approaches have also been developed. In these techniques, explicit system training and evaluation are required. In addition, as the architecture hyper-parameters and actual DNN parameters are separately learned, e.g., within the RL controller and candidate systems, a tighter integration of both is preferred during NAS.   Alternatively, differentiable architectural search  techniques can be used. Architectural search is performed over an over-parameterized parent super-network containing paths connecting all candidate DNN structures to be considered. The search is transformed into the estimation of the weights assigned to each candidate neural architecture within the super-network. The optimal architecture is obtained by pruning lower weighted paths. This allows both architecture selection and candidate DNN parameters to be consistently optimized within the same super-network model.  In contrast to the rapid development of NAS techniques in the machine learning and computer vision communities, there has been very limited research of applying these to speech recognition systems so far. In this paper, a range of DARTS based NAS techniques are used to automatically learn two architecture hyper-parameters that heavily affect the performance and model complexity of state-of-the-art factored time delay neural network  acoustic models: i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These include the standard DARTS method fully integrating the estimation of architecture weights and TDNN parameters in lattice-free Maximum Mutual Information  training; Gumbel-Softmax DARTS that reduces the confusion between candidate architectures; pipelined DARTS that circumvents the overfitting of architecture weights using validation data; and penalized DARTS that further incorporates resource constraints to flexibly adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures was also used to facilitate efficient search over a large number of TDNN systems. Experiments conducted on a 300-hour Switchboard conversational telephone speech recognition task suggest the NAS configured TDNN-F systems consistently outperform the baseline LF-MMI trained TDNN-F systems using manually designed configurations. Absolute word error rate reductions up to 1.0\% and model size reduction of 28\% relative were obtained. In order to further evaluate the performance of the proposed NAS techniques, they were applied to automatically configure two sets of hyper-parameters of a state-of-the-art disordered speech recognition task based on the UASPEECH corpus: skip connection between layers and dimensionality of factored TDNN weight matrices.  To the best of our knowledge, this paper is among the first to apply neural architecture search techniques to TDNNs in speech recognition tasks. In contrast, the vast majority of previous NAS research has been focused on computer vision applications. Existing NAS works in the speech community investigated non-TDNN based  architectures.  % Once the paper is accepted, we will release our code.  The rest of this paper is organized as follows. Section 2 presents a set of differentiable NAS techniques. Section 3 discusses the search space of TDNN-F models and necessary parameter sharing to improve search efficiency. Section 4 presents the experiments and results. Finally, the conclusions are drawn in Section 5.   
"," Deep neural networks  based automatic speech recognition  systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search  techniques are used to automatically learn two types of hyper-parameters of state-of-the-art factored time delay neural networks : i) the left and right splicing context offsets; and ii) the dimensionality of the bottleneck linear projection at each hidden layer. These include the DARTS method integrating architecture selection with lattice-free MMI  TDNN training; Gumbel-Softmax and pipelined DARTS reducing the confusion over candidate architectures and improving the generalization of architecture selection; and Penalized DARTS incorporating resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures allows efficient search over up to $7^{28}$ different TDNN systems. Experiments conducted on the 300-hour Switchboard corpus suggest the auto-configured systems consistently outperform the baseline LF-MMI TDNN systems using manual network design or random architecture search after LHUC speaker adaptation and RNNLM rescoring. Absolute word error rate  reductions up to 1.0\% and relative model size reduction of 28\% were obtained. Consistent performance improvements were also obtained on a UASpeech disordered speech recognition task using the proposed NAS approaches. % Deep neural networks  based automatic speech recognition  systems are often designed using expert knowledge and empirical evaluation. In this paper, a range of neural architecture search  techniques are used to automatically learn three hyper-parameters that heavily affect the performance and model complexity of state-of-the-art factored time delay neural network  acoustic models: i) the left and right splicing context offsets; ii) the dimensionality of the bottleneck linear projection and iii) skip connections at each hidden layer. These include the standard DARTS method fully integrating the estimation of architecture weights and TDNN parameters in lattice-free MMI  training; Gumbel-Softmax DARTS that reduces the confusion between candidate architectures; Pipelined DARTS that circumvents the overfitting of architecture weights using held-out data; and Penalized DARTS that further incorporates resource constraints to adjust the trade-off between performance and system complexity. Parameter sharing among candidate architectures was also used to facilitate efficient search over up to $7^{28}$ different TDNN systems. Experiments conducted on a 300-hour Switchboard task suggest the NAS auto-configured TDNN-F systems consistently outperform the baseline LF-MMI trained TDNN-F systems using manual expert configurations. Absolute word error rate reductions up to 1.0\% and relative model size reduction of 28\% were obtained. Consistent performance improvements were also obtained in the disorder UASPEECH task.",127
"   A speech enhancement system aims at restoring the quality and intelligibility of noisy speech. The state-of-the-art speech enhancement systems are commonly built with deep neural network  based vector-to-vector regression models, where inputs are context-dependent log power spectrum  features of noisy speech and outputs correspond to either clean or enhanced LPS features. Although deep neural network  based speech enhancement  has demonstrated the state-of-the-art performance under a single-channel setting, it can also be extended to scenarios of multi-channel speech enhancement with even better-enhanced speech qualities . The process of both single and multi-channel speech enhancement can be taken as a DNN based vector-to-vector regression aiming at bridging a functional relationship  such that the input noisy speech  can be mapped to the corresponding clean speech . In, DNNs with feed-forward fully-connected  hidden layers were proposed to attain the state-of-the-art performance of speech enhancement on the target tasks and the related theorems were later set up in. In some follow-up studies, recurrent neural networks , and convolutional neural networks  were further investigated to boost speech enhancement quality. Moreover, a deep bidirectional RNN with LSTM gates was instead used in, and a generative adversarial network  was attempted for speech enhancement tasks in. In particular, CNN is a tensor-to-vector regression model because it is capable of dealing with 3D/4D tensorized input data. Besides, the recent works suggest that CNN can outperform both DNN and RNN counterparts for speech enhancement. Similarly, a tensor-to-vector regression model can also be built by directly employing the proposed tensor-train network . Besides, TT-DNN is a compact representation for a fully-connected  layers of DNN into a tensor-train  format. In, we were the first to attempt a tensor-train deep neural network  to tackle the multi-channel speech enhancement task and also demonstrate that the TT representation of a DNN does not cause the quality degradation of the enhanced speech, and it also results in a significant reduction of the model parameters. More importantly, the quality of speech enhancement can be improved over the DNN counterpart by allowing the TT-DNN parameters to grow.  A significant advantage of tensor-to-vector regression, such as CNN and TT-DNN, is its compact architecture to observe stringent hardware constraints, where computational resources are often limited. Therefore, it is worth investigating the models in terms of the representation power, and experimentally comparing them by considering the trade-off between enhancement performance and the number of model parameters. On one hand, CNN is a powerful model to learn spatial-temporal features and extract semantically meaningful aspects in higher hidden layers. On the other hand, TT-DNN can maintain baseline results of the corresponding DNN by applying the TT transformation to the FC hidden layers. Hence, in this work, we focus on a tensor-to-vector model to take advantage of both CNN and TT-DNN. More specifically, we propose a novel hybrid architecture, namely CNN-TT, with convolutional layers stacked at the bottom and one TT hidden layer on the top. To highlight the advantages of CNN-TT, we compare different deep tensor-to-vector models for speech enhancement. The used models in this work include  DNN;  CNN;  TT-DNN;  CNN-TT. In more detail, we first explain the fundamental mechanisms of tensor-to-vector regression based on our theorems of DNN based vector-to-vector regression. Then, we validate our CNN-TT models in speech enhancement tasks.  %we know that CNNs are very powerful models that can learn salient features of the input domain in the lower layers, and semantically meaningful aspect of the problem domain in the higher layer . CNN-based architectures have reported top results in several machine learning tasks, e.g.,  On the other hand, a TT-DNN can significantly reduce the number of parameters while keeping the original performance of the seeding DNN by applying a TT transformation to the fully-connected hidden layers. That is because TT-DNN only stores the TT-format of DNN, i.e., the set of low-rank core tensors, which can be used to approximately reconstruct the original DNN. Moreover, the running complexities of a DNN and corresponding TT-DNN are on the same order . In this work, we are interested in finding the best tensorized architecture that allows the best trade-off between model size and speech enhancement quality, and propose a novel hybrid architecture based on the convolutional layer in the bottom part to take advantage of feature extraction capabilities of the CNNs, and parameter reduction capabilities of TT-DNNs. We refer to this architecture as CV-TT-NN. All neural models investigated in this work are meant for tensor-to-vector regression, and three architectures are compared and contrasted:  CNNs,  TT-DNNs, and  hybrid CV-TT-NNs. We first explain the fundamental mechanisms of tensor-to-vector regression by reformulating a convolutional layer as a multiplication of two core tensors, which allows us to extend our theorems proposed in to tensor-to-vector regression neural models. Next, we move to the  speech enhancement experiments.  Our experimental results show that in single-channel speech enhancement on the Edinburgh noisy speech corpus , CNN outperforms the best DNN with a small increment of parameter sizes. Moreover, our proposed CNN-TT slightly outperforms CNN with only 32\% of the CNN model size. A further improvement can be attained if the size of the CNN-TT model is increased up to 44\% of the CNN model size. Finally, the experiments of a multi-channel speech enhancement task on a simulated noisy WSJ0 corpus  show the same trend that our proposed hybrid CNN-TT architecture can be favorably compared to both DNN and CNN models to achieve better-enhanced speech qualities and utilize much smaller model sizes.    
"," This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train  output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network  based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32\% of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44\% of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes. %Finally, experimental results on the multi-channel speech enhancement on the in-house noisy corrupted Wall Street Journal  corpus confirm our claims.  %This work applies various deep hybrid tensor-to-vector regression models for speech enhancement. In particular, we are mainly concerned with two goals:  Providing new insights into the tensor models based on approximation power of deep neural network  based tensor-to-vector functions from both theoretical and practical points of view, and  Finding different deep hybrid tensor architectures in terms of multiple metrics, i.e., the number of model parameters and speech enhancement performance. In particular, we focus on convolutional neural networks  and tensor-train neural networks  in dealing with tensorized speech data since the two architectures belong to the tensor-to-vector framework. Moreover, different hybrid architectures based on convolutional and tensor-train layers are evaluated. Our experiments of speech enhancement include both single-channel and multi-channel speech enhancement, and the empirical results suggest the following three findings:  CNNs attain better results than TT-NNs;  better speech enhancement results can be obtained by deploying neural models having convolutional and TT layers. % this work also investigates the use of Tucker decomposition to further reduce the number of CNN parameters, and strike a balance between speech enhancement performance and model complexity.",128
"  Subjective listening studies are the most reliable form of speech quality assessment for many applications, including speech enhancement and audio source separation. Listeners often rate the perceptual quality of testing materials using categorical or multi-stimuli rating protocols. The test materials are often artificially created by additively or convolutionally mixing clean speech with noise or reverberation at prescribed levels, to simulate real environments. Unfortunately, the simulated data does not capture all the intricate details of real environments , so it is not clear if these assessments are consistent with assessment results from real-world environments. Many investigations conclude that more realistic datasets and scenarios are needed to improve real-world speech processing performance. However, the cost and time-consuming nature of subjective studies also hinders progress.  Computational objective measures enable low cost and efficient speech quality assessment, where many intrusive, non-intrusive, and data-driven approaches have been developed. Intrusive measures, such as the perceptual evaluation of speech quality , signal-to-distortion ratio  and perceptual objective listening quality analysis , generate quality scores by calculating the dissimilarities between a clean reference speech signal and its degraded counterpart . These measures, however, do not always correlate well with subjective quality results.   Several non-intrusive  objective quality measures have been developed, including the ITU-T standard P.563, ANSI standard ANIQUE+, and the speech to reverberation modulation energy ratio . These approaches use signal processing concepts to generate quality-assessment scores. These approaches, however, rely on signal properties and assumptions that are not always realized in real-world environments, hence the assessment scores are not always consistent with human ratings. More recent work uses data-driven methods to estimate speech quality. The authors in  combine hand-crafted feature extraction with a tree-based regression model to predict objective PESQ scores. Quality-Net provides frame-level quality assessment by predicting the utterance-level PESQ scores that are copied as per-frame labels using a bidirectional long short-term memory  network. Similarly, NISQA estimates the per-frame POLQA scores using a convolutional neural network . It subsequently uses a BLSTM to aggregate frame-level predictions into utterance-level objective quality scores. These data-driven approaches perform well and increase the practicality of real-world assessment. However, the usage of objective quality scores as training targets is a major limitation, since objective measures only approximate human perception. Alternatively, the model developed in  predicts the mean opinion score  of human ratings, but the ratings are collected on simulated speech data. This approach advances the field, but it is not enough to ensure good performance in real environments. A complete approach is needed that predicts human quality ratings of real recordings.  In this study, we conduct a large-scale listening test on real-world data and collect 180,000 subjective quality ratings through Amazon's Mechanical Turk  using two publically-available speech corpora. This platform provides a diverse population of participants at a significantly lower cost to facilitate accurate and rapid testing. These corpora have a wide range of distortions that occur in everyday life, which reflect varying levels of noise and reverberation. Our listening tests follow the MUltiple Stimuli with Hidden Reference and Anchor  protocol. To the best of our knowledge, a large publically-available dataset that contains degraded speech and human quality ratings does not currently exist. We additionally develop an encoder-decoder model with attention mechanism to non-intrusively predict the perceived speech quality of these real-world signals. The encoder consists of stacked pyramid BLSTMs that convert low-level speech spectra into high-level features. This encoder-decoder architecture reduces the sequential size of the latent representation that is provided to an attention model. The key difference between this proposed approach and related approaches, is that our approach predicts mean-opinion scores of real-world signals using a novel deep-learning framework. The following sections discuss the details and results of our approach.    
"," The real-world capabilities of objective speech quality measures are limited since current measures  are developed from simulated data that does not adequately model real environments; or they  predict objective scores that are not always strongly correlated with subjective ratings. Additionally, a large dataset of real-world signals with listener quality ratings does not currently exist, which would help facilitate real-world assessment. In this paper, we collect and predict the perceptual quality of real-world speech signals that are evaluated by human listeners. We first collect a large quality rating dataset by conducting crowdsourced listening studies on two real-world corpora. We further develop a novel approach that predicts human quality ratings using a pyramid bidirectional long short term memory  network with an attention mechanism. The results show that the proposed model achieves statistically lower estimation errors than prior assessment approaches, where the predicted scores strongly correlate with human judgments.",129
" % The very first letter is a 2 line initial drop letter followed % by the rest of the first word in caps. %  % form to use if the first word consists of a single letter: % {A}{demo} file is .... %  % form to use if you need the single drop letter followed by % normal text : % {A}{}demo file is .... %  % Some journals put the first two words in caps: % {T}{his demo} file is .... %  % Here we have the typical use of a ""T"" for an initial drop letter % and ""HIS"" in caps to complete the first word. {T}{ext} classification -- the procedure of designating pre-defined labels for text -- is an essential and significant task in many Natural Language Processing  applications, such as sentiment analysis  , topic labeling   , question answering   and dialog act classification .  In the era of information explosion, it is time-consuming and challenging to process and classify large amounts of text data manually.  Besides, the accuracy of manual text classification can be easily influenced by human factors, such as fatigue and expertise.  It is desirable to use machine learning methods to automate the text classification procedure to yield more reliable and less subjective results.  Moreover, this can also help enhance information retrieval efficiency and alleviate the problem of information overload by locating the required information.  Fig. illustrates a flowchart of the procedures involved in the text classification, under the light of shallow and deep analysis. Text data is different from numerical, image, or signal data. It requires NLP techniques to be processed carefully. The first important step is to preprocess text data for the model. Shallow learning models usually need to obtain good sample features by artificial methods and then classify them with classic machine learning algorithms. Therefore, the effectiveness of the method is largely restricted by feature extraction. However, different from shallow models, deep learning integrates feature engineering into the model fitting process by learning a set of nonlinear transformations that serve to map features directly to outputs.     % Generally, text classification approaches principally are separated into two branches: shallow learning and deep learning techniques, as shown in Fig..  % The task can be summarized as follows:  % the original input text should be preprocessed to obtain the vector representation for shallow learning and deep learning models . % For shallow learning models, the preprocessed data should extract features, representing a vector by representation learning methods.  % Then, a classifier learns the classification features. % The performance of the classification model is improved through iterative training with training sets and evaluation on validation sets.  % When pre-defined termination conditions are reached, it is time to predict the final label. % However, different from shallow models, deep learning integrates feature engineering into the model fitting process by learning a set of nonlinear transformations that serve to map features directly to outputs.         There have been several works reviewing text classification and its subproblems recently.  Two of them are reviews of text classification. Kowsari et al.  surveyed different text feature extraction, dimensionality reduction methods, basic model structure for text classification, and evaluation methods.  Minaee et al.  reviewed recent deep learning based text classification methods, benchmark datasets, and evaluation metrics.  Unlike existing text classification reviews, we conclude existing models from shallow to deep learning with works of recent years. % and compare among the deep learning models.  Shallow learning models emphasize the feature extraction and classifier design.  Once the text has well-designed characteristics, it can be quickly converged by training the classifier.  DNNs can perform feature extraction automatically and learn well without domain knowledge.  We then give the datasets and evaluation metrics for single-label and multi-label tasks and summarize future research challenges from data, models, and performance perspective.  Moreover, we summarize various information in three tables, including the necessary information of classic deep learning models, primary information of main datasets, and a general benchmark of state-of-the-art methods under different applications.  In summary, this study's main contributions are as follows:         %     The rest of the survey is organized as follows.  Section summarizes the existing models related to text classification, including shallow learning and deep learning models, including a summary table.  Section introduces the primary datasets with a summary table and evaluation metrics on single-label and multi-label tasks.  We then give quantitative results of the leading models in classic text classification datasets in Section.  Finally, we summarize the main challenges for deep learning text classification in Section before concluding the article in Section.     
"," Text classification is the most fundamental and essential task in natural language processing.  The last decade has seen a surge of research in this area due to the unprecedented success of deep learning.  Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey.  This paper fills the gap by reviewing the state of the art approaches from 1961 to 2020, focusing on models from shallow to deep learning.  We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions.  A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey.  Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.",130
"  Relation extraction  aims at obtaining the semantic relationship between entities using text as a source of knowledge. For instance, from the text snippet, Steve Jobs and Wozniak co-founded Apple in 1976., we can infer that Steve Jobs and Wozniak have org:founded\_by relation with Apple. RE is an important subtask of information extraction that has significant applications in various higher-order NLP/IR tasks, such as question answering, knowledge graph completion and semantic search . Earlier studies on RE were based on feature engineering. Such methods rely on linguistic and lexical tools to obtain the information required for such feature engineering . Additionally, the performance of these methods is hindered by the sparse feature representation used by the models.  With the surge of neural networks, deep learning-based models have become prevalent. In these models, pre-trained word embeddings are employed to solve the feature sparsity problems. Deep learning based RE models can further be categorized along two lines: sequence-based and graph-based models. In sequence-based models, a word sequence is used to embed the text using convolution or recurrent neural networks . In graph-based models, the text is first converted into a graph using a dependency parser or other linguistic tools and then processed with a graph neural network which encodes neighborhood and feature information. Finally, the encoded graph features are used in RE. Along this line,  and  employed a bidirectional long short-term memory  network and  and  employed a graph convolutional network   to encode the textual graph used in their work. Compared to sequence-based models, graph-based models have been shown to be effective in learning long-distance dependencies present in text .  Although the state-of-the-art results are obtained using graph-based models, they require external tools to build a graph for the text. Therefore, they are computationally expensive and not fully end-to-end trainable. While sequence-based models do not depend on external linguistic tools, they have been shown less effective for long text, especially when long-distance dependencies are required . To bridge this gap, we propose a Self-determined GCN  which infers  a graph for the text using a self-attention mechanism , rather using any external linguistic tool. Then the self-determined graph is encoded using a GCN model. We evaluate the effectiveness of the SGCN on a RE task against several competitive baselines. In summary, our contributions are the following:     
"," Relation Extraction is a way of obtaining the semantic relationship between entities in text. The state-of-the-art methods use linguistic tools to build a graph for the text in which the entities appear and then a Graph Convolutional Network  is employed to encode the pre-built graphs. Although their performance is promising, the reliance on linguistic tools results in a non end-to-end process. In this work, we propose a novel model, the Self-determined Graph Convolutional Network , which determines a weighted graph using a self-attention mechanism, rather using any linguistic tool. Then, the self-determined graph is encoded using a GCN. We test our model on the TACRED dataset and achieve the state-of-the-art result. Our experiments show that SGCN outperforms the traditional GCN, which uses dependency parsing tools to build the graph.",131
"   Humour is an essential part of everyday communication, particularly in social media, yet it remains a challenge for computational methods.  Unlike conventional language, humour requires complex linguistic and background knowledge to understand, which are difficult to integrate with NLP methods.  An important step in the automatic processing of humour is to recognize its presence in a piece of text.  However, its intensity may be present or perceived to varying degrees to its human audience.  This level of appreciation  can vary according to the text's content and structural features, such as nonsense or disparagement or, in the case of puns, contextual coherence and the cognitive effort required to recover the target word~.  While previous work has considered mainly binary classification approaches to humorousness, the \HAHA shared task also focuses on its gradation.  This latter task is important for downstream applications such as conversational agents or machine translation, which must choose the correct tone in response to humour, or find appropriate jokes and wordplay in a target language.  The degree of creativeness may also inform an application whether the semantics of a joke can be inferred from similar examples.  This paper describes the OFAI--UKP system that participated in both subtasks of the \HAHA evaluation campaign: binary classification of tweets as humorous or not humorous, and the quantification of humour in those tweets.  Our system employs a Bayesian approach---namely, a variant of Gaussian process preference learning  that infers humorousness scores or rankings on the basis of manually annotated pairwise preference judgments and automatically annotated linguistic features.  In the following sections, we describe and discuss the background and methodology of our system, our means of adapting the \HAHA data to work with our system, and the results of our system evaluation on this data.  
"," %   Most humour processing systems to date make at best discrete, coarse-grained distinctions between the comical and the conventional, yet such notions are better conceptualized as a broad spectrum.  In this paper, we present a probabilistic approach, a variant of Gaussian process preference learning , that learns to rank and rate the humorousness of short texts by exploiting human preference judgments and automatically sourced linguistic annotations.  We apply our system, which had previously shown good performance on English-language one-liners annotated with pairwise humorousness annotations, to the Spanish-language data set of the \HAHA evaluation campaign.  We report system performance for the campaign's two subtasks, humour detection and funniness score prediction, and discuss some issues arising from the conversion between the numeric scores used in the \HAHA data and the pairwise judgment annotations required for our method.",132
"     It is a common tendency among multilingual people who are non-native English speakers to code-mix in their speech using English-based phonetic typing. This linguistic phenomenon, particularly in social media like Twitter\footnote{https://twitter.com/}, poses a great challenge to the conventional Natural Language Processing  study area.      Within the context of the Sentiment Analysis, the study of the phenomenon of code-mixed language is important to the research community because this behavior is more common today. The interest in this area has grown due to the volume of data that social networks generate, and also by the value that this information has to understand people opinions when they are expressed in written texts.          In this paper, we explain our methodology to predict sentiment in tweets, describing how our method is based on a combination of the latest language models, and also how such models contributed to a great advance in this task. This configuration was employed and evaluated in the SemEval 2020 challenge , in which the goal is to predict the sentiment in code-mixed texts written in English and Hindi languages of a tweet . The models used in this combination are: MultiFiT  that an evolution of ULMFiT , BERT , ALBERT  and XLNet .      This work is organized as follows: Section  explains some related works, Section  describes the dataset used, Section  addresses the methodology applied in the task, Section  presents the results, and finally Section  expose our final considerations as well as possible future works.  
","     In this paper, we describe a methodology to predict sentiment in code-mixed tweets . Our team called verissimo.manoel in CodaLab\footnote{https://competitions.codalab.org/competitions/20654} developed an approach based on an ensemble of four models . The final classification algorithm was an ensemble of some predictions of all softmax values from these four models. This architecture was used and evaluated in the context of the SemEval 2020 challenge , and our system got 72.7\% on the F1 score.",133
"  Keyphrases are short pieces of text that summarize the key points discussed in a document. They are useful for many natural language processing and information retrieval tasks, such as, text summarization , question answering ,  % information extraction ,  sentiment analysis , document retrieval , document categorization or clustering , contextual advertisement , and more. In the automatic keyphrase generation task, the input is a document, and the output is a set of keyphrases that can be categorized as  or  keyphrases. Present keyphrases appear exactly in the target document, while absent keyphrases are only semantically related and have partial or no overlap to the target document. We provide an example of a target document and its keyphrases in Figure . % document categorization , contextual advertisement     Automatic keyphrase generation methods in literature can be broadly divided into  and  methods. A large pool of prior works have been devoted to extracting keyphrases by selecting  text spans or phrases directly from the target document   % and ranking based on their importance  . % mihalcea2004textrank However, due to their design principle, these approaches cannot predict absent keyphrases. In recent years, the neural sequence-to-sequence  framework  has become the fundamental building block in neural keyphrase generation models with its widespread usage in natural language generation tasks. The first deep neural keyphrase generation model, CopyRNN  adopts the Seq2Seq framework  with copy mechanism . % The copy attention mechanism enables the decoder to select words either according to a language model over the predefined vocabulary or according to a probability distribution computed over the input text sequence. % Thus, the copy enabled Seq2Seq methods are capable of generating both present and absent keyphrases. With the copy attention mechanism, the Seq2Seq models are capable of generating both present and absent keyphrases. A few subsequent works  extended CopyRNN to enhance keyphrase generation.   Although these generative approaches are capable of generating both present and absent phrases, they ignore the advantages of the extractive solutions, e.g., extracted keyphrases indicate the essential segments of the target document. %  recently proposed to combine an extractor that selects text spans as present keyphrases and a generator that generates the absent keyphrases word by word.  % One advantage of such a combined approach is the extractor can help the generator to identify important segments of the target document.  To generate a comprehensive set of keyphrases that summarizes the key points conveyed in the target document, reading the full document content is necessary. However, to the best of our knowledge, none of the previous neural methods are provisioned to read the full content of a document as it can be thousands of words long .  Processing such long documents through deep neural networks requires high computational resources. Hence, the existing neural methods truncate the target document; take the first few hundred words as input and ignore the rest of the document that may contain salient information.  To address the aforementioned challenges, in this paper, we propose SEG-Net  that has two major components,  a  that selects the salient sentences in a document and  an  that predicts the present keyphrases and generates the absent keyphrases jointly. The primary motivation to design the sentence-selector is to decompose a long target document into small segments, e.g., sentences, paragraphs, and identify the salient ones for keyphrase generation. % One potential solution is to decompose the target document into small segments, e.g., sentences, paragraphs, and identify the salient ones for keyphrase generation. For example, as shown in Figure , we split a document into a list of sentences and classify them with salient and non-salient labels. In this context, we consider a sentence as salient if it contains present keyphrases or overlaps with absent keyphrases.  In Figure , the sample document consists of six salient  and five non-salient sentences . A similar notion is adopted in prior works on text summarization  and question answering .   We employ   as the backbone of the extractor-generator in SEG-Net. We chose Transformer as it completely relies on a self-attention mechanism that is capable of capturing longer range dependencies. We equip the extractor-generator with a novel  coverage attention and an  copy attention such that the generated keyphrases summarize the entire target document. The layer-wise coverage attention keeps track of the target document segments that are covered by previously generated phrases to guide the self-attention mechanism in Transformer while attending the encoded target document in future generation steps.   We revise the standard copy mechanism  and propose an ``informed'' copy attention for keyphrase generation. Our revision is based on the observation that a word has a different meaning in the context of two different keyphrases. For example, in Figure , the word ``learning'' has a different meaning when used in ``computer assisted language learning'', ``integrated e learning'', and ``learning of foreign languages''.  Hence, we revise the copy mechanism so that SEG-Net does not copy a word from a present keyphrase while generating an absent keyphrase. Another motivation behind such a copy mechanism is to encourage the model to generate absent keyphrases that summarize the other segments of the target document that are not covered by the present keyphrases. % add 1 line   We train SEG-Net via multi-task learning to predict keyphrases as well as their part-of-speech  tags. We exploit the multi-layer structure of Transformer to perform both POS tagging and keyphrase generation. We evaluate SEG-Net on five benchmarks from scientific articles and two benchmarks from web documents to demonstrate its effectiveness over the state-of-the-art neural generative methods on both domains. We perform thorough ablation and analysis to present noteworthy findings such as  selecting salient sentences significantly improve present keyphrase extraction,  the layer-wise coverage attention and informed copy mechanism facilitates absent keyphrase generation,  jointly learning POS tagging and phrase prediction reduces duplicate and overlapping keyphrase generation.  %    [1]{\Note{Purple}{[1]{\Note{Orange}{[1]{\Note{Red}{[1]{\Note{Blue}{ % \theoremstyle{plain} {Task}        %% These commands are for a PROCEEDINGS abstract or paper. {June 03--05, 2018}{Woodstock, NY}      %% %% end of the preamble, start of the body of the document source.    %% %% The ""title"" command has an optional parameter, %% allowing the author to define a ""short title"" to be used in page headers. \title{Select, Extract and Generate: Neural Keyphrase Generation with Syntactic Guidance}  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research.  % \author{Wasi Uddin Ahmad, Xiao Bai, Soomin Lee, Kai-Wei Chang} % \affiliation{% %    % } %   \author{Wasi Uddin Ahmad} \affiliation{%    }   \author{Xiao Bai} \affiliation{%    }   \author{Soomin Lee} \affiliation{%    }   \author{Kai-Wei Chang} \affiliation{%    }   %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose.    %  %  %  %  \\  Generating a set of keyphrases that summarizes the core ideas discussed in a document has a significant impact on many applications, including document understanding, retrieval, advertising, and more. In recent years, deep neural sequence-to-sequence framework has demonstrated promising results in keyphrase generation. However, processing long documents using such deep neural networks requires high computational resources. To reduce the computational cost, the documents are typically truncated before given as inputs. As a result, the models may miss essential points conveyed in a document. Moreover, most of the existing methods are either extractive  or generative , and hence they do not benefit from the advantages of both modeling techniques. To address these challenges, we propose , a neural keyphrase generation model that is composed of two major components,  a selector that selects the salient sentences in a document, and  an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses a self-attentive architecture, known as,  as the building block with a couple of uniqueness. First, SEG-Net incorporates a novel  coverage attention to summarize most of the points discussed in the target document. Second, it uses an  copy attention mechanism to encourage focusing on different segments of the document during keyphrase extraction and generation. Besides, SEG-Net jointly learns keyphrase generation and their part-of-speech tag prediction, where the later provides syntactic supervision to the former. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin in both domains.  %Combining these two prevailing approaches in jointly learning to extract and generate can enable us to predict a comprehensive set of keyphrases.  % Relying on only one type of these prevailing approaches risks to miss important keyphrases.  % SEG-Net uses a self-attentive architecture, known as,  to learn document representations for its ability to capture long-range dependencies.  % SEG-Net has a couple of uniqueness compared to the vanilla Transformer.  % during keyphrase generation to emphasize less on the document segments, from where the keyphrases are extracted. % such that the extracted and generated keyphrases remain mutually exclusive.    %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %%   %  % <ccs2012> %  <concept> %   <concept_id>10010520.10010553.10010562</concept_id> %   <concept_desc>Computer systems organization~Embedded systems</concept_desc> %   <concept_significance>500</concept_significance> %  </concept> %  <concept> %   <concept_id>10010520.10010575.10010755</concept_id> %   <concept_desc>Computer systems organization~Redundancy</concept_desc> %   <concept_significance>300</concept_significance> %  </concept> %  <concept> %   <concept_id>10010520.10010553.10010554</concept_id> %   <concept_desc>Computer systems organization~Robotics</concept_desc> %   <concept_significance>100</concept_significance> %  </concept> %  <concept> %   <concept_id>10003033.10003083.10003095</concept_id> %   <concept_desc>Networks~Network reliability</concept_desc> %   <concept_significance>100</concept_significance> %  </concept> % </ccs2012> %   %  %  %  %    %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.              %% %% The acknowledgments section is defined using the ""acks"" environment %% . This ensures the proper %% identification of the section in the article metadata, and the %% consistent spelling of the heading.   %  % To Robert, for the bagels and explaining CMYK and color spaces. %   %% %% The next two lines define the bibliography style to be used, and %% the bibliography file. % \newpage %     %% %% If your work has an appendix, this is the place to put it. % \appendix     \endinput %% %% End of file `sample-sigconf.tex'. 
"," %  %  %  %  \\  Generating a set of keyphrases that summarizes the core ideas discussed in a document has a significant impact on many applications, including document understanding, retrieval, advertising, and more. In recent years, deep neural sequence-to-sequence framework has demonstrated promising results in keyphrase generation. However, processing long documents using such deep neural networks requires high computational resources. To reduce the computational cost, the documents are typically truncated before given as inputs. As a result, the models may miss essential points conveyed in a document. Moreover, most of the existing methods are either extractive  or generative , and hence they do not benefit from the advantages of both modeling techniques. To address these challenges, we propose , a neural keyphrase generation model that is composed of two major components,  a selector that selects the salient sentences in a document, and  an extractor-generator that jointly extracts and generates keyphrases from the selected sentences. SEG-Net uses a self-attentive architecture, known as,  as the building block with a couple of uniqueness. First, SEG-Net incorporates a novel  coverage attention to summarize most of the points discussed in the target document. Second, it uses an  copy attention mechanism to encourage focusing on different segments of the document during keyphrase extraction and generation. Besides, SEG-Net jointly learns keyphrase generation and their part-of-speech tag prediction, where the later provides syntactic supervision to the former. The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin in both domains.  %Combining these two prevailing approaches in jointly learning to extract and generate can enable us to predict a comprehensive set of keyphrases.  % Relying on only one type of these prevailing approaches risks to miss important keyphrases.  % SEG-Net uses a self-attentive architecture, known as,  to learn document representations for its ability to capture long-range dependencies.  % SEG-Net has a couple of uniqueness compared to the vanilla Transformer.  % during keyphrase generation to emphasize less on the document segments, from where the keyphrases are extracted. % such that the extracted and generated keyphrases remain mutually exclusive.",134
"   {) systems additionally provide formative feedback to guide revision. Although neural networks currently generate state-of-the-art AES results , non-neural AES create feature representations more easily useable by AWE . We believe that neural AES can also provide useful information for creating feature representations, e.g., by exploiting information in the intermediate layers.     Our work focuses on a particular source-based essay writing task called the response-to-text assessment  . Recently, an RTA AWE system  was built by extracting rubric-based features related to the use of {[t] { {|p{20cm}|}  $ prompt and an essay with score of 3.}      
"," While  automated essay scoring  can reliably grade essays at scale,  %a grade without feedback  is not enough to guide students in essay revision.  automated writing evaluation   additionally provides formative feedback  to guide essay revision. However,   %the feedback produced by  %generating AWE feedback is often not driven by %AES. %For example,  a neural AES typically does not provide useful  feature representations for supporting AWE. %AWE information such as rubric-based grading. %explanations. This paper presents  a method for linking AWE and  neural AES, by extracting Topical Components  representing evidence from a source text  using the intermediate output of attention layers. %of a neural AES. %for source-based AES.  %Specifically,  %to generate TCs, %we  use the  outputs of the attention layers of a hierarchical neural AES as criteria for filtering irrelevant content from essays. %We then use TCs  to support feature-based AES and AWE systems.  We  evaluate  performance   using a feature-based AES  %for a grading rubric expressed in terms of  requiring TCs.  Results show that performance is comparable whether using automatically  or manually constructed TCs for 1) representing essays as rubric-based features, 2) grading essays.   %Recently, automated essay scoring  systems were developed in order to grade essays on large scale meanwhile reduce human effort significantly. However, an essay score usually does not have the ability to provide advice to students on how to revise their essays. Therefore, automated writing evaluation systems  are also required which provides feedback for essay revision. Although the neural network generates the state-of-the-art AES results, an end-to-end neural network could not provide enough information to the AWE system which requires more feature engineering. This paper presents an investigation of using the intermediate output of the neural network of AES for extracting keyword or keyphrase  to reduce the expert effort which is useful in the AWE system. Specifically, we use outputs of attention layers of a hierarchical neural network explicitly as criteria for filtering irrelevance content from student essays. This paper shows that TCs generated by our model could support an existing AWE for providing formative feedback. We evaluate our model on source-based AES task, and results show that our model is promising.",135
"  %\url{}, accessed \printdate{}  Reading is increasingly carried out by means of online multiple texts, which can simultaneously consist of  texts of diverse genres, registers, authorships, credibilities etc.\ .  % That is, learning takes place, so to speak, on the basis of  whose components are gathered from a constantly growing, nowadays mostly web-based information landscape  or space . % \textcolor{black}{Consequently,  speak of  reading as an intertextual process.}     assume that intertext models represent selected constituents of multiple texts as  together with entity-related information .  % This is supplemented by three types of links: % IM-related source-to-source , MM-related content-to-content and source-to-content links . % A prediction of the DM, which is crucial for our work, is that the probability of generating an intertext model as a result of reading a multiple text is a function of the number of the texts involved, their authors, the perspectives they provide on the described situation , the tasks to be accomplished and other contextual factors . % This suggests to speak of the intertext model as a kind of  cognitive map  of the underlying multiple text, where the MM abstracts from this textbase : % that is, readers produce intertext models as cognitive maps of multiple texts as parts of the underlying IL, while groups or communities of readers produce distributed cognitive maps  of larger sections of the IL or the IL as a whole. % This duality of small- and large-scale reading processes leads to the object of this article.  % That is, we can ask how the IL looks like from the perspective of these distributed cognitive maps or vice versa, how it presents itself to different reader communities. % The latter question will be in the focus of this article.  Although the DM takes the necessary step of generalizing the CIM towards modeling multiple texts, it is largely single reader-oriented. % To broaden this focus, we generalize the DM conceptually in two steps:       % In a first step, which is still within the boundaries of the DM, we consider intertext models as reader-centered approximations of parts of the IL with varying degrees of explicitness . % This predicts that different readers can approximate different parts  of the IL just as they can align  their intertext models of overlapping parts depending on their interaction\textcolor{black}{, which according to  is a characteristic of non-academic online reading} and also regards online collaborative learning . % Starting from the context model of ,  Figure  illustrates this alignment scenario in terms of a situation semantic adaptation : % two readers  and  read not necessarily different multiple texts in related contexts  to solve the same or related tasks  in order to achieve the same or related goals. % We assume that the texts originate from an IL in whose context they describe a situation  and that  and  refer to the same resource situation  to understand which situation their multiple texts actually describe, where all references to contextual units are indirect:  % they are mediated through mental representations  of multiple texts , described situations , reading contexts , task contexts  and resource situations  or knowledge space). % As a result of collaborative, cooperative or simply parallel reading processes, the representations of the readers may align with each other, in the short-term  or long-term . % That is, as illustrated in Figure ,  and  have the possibility to align their mental representations so that they understand the same or similar multiple texts as descriptions of the same or similar situations.\footnote{Adding the notion of a described situation to the context model of  allows for distinguishing communication scenarios such as misunderstandings , disinformation  or misinformation   and related phenomena.} % Evidently, such an alignment requires many things, but at least the chance that both readers have access to the same or semantically sufficiently similar texts from which they can extract the same or sufficiently similar multiple texts.  % But do they? % This question brings us to the second step of generalizing the DM: %   % From the perspective of reader communities, reading is a distributed process that approximates a multifaceted IL, so that both the IL and its distributed representation by innumerable intertext models jointly develop. % Obviously, parallel to the diversity of the IL, communities of readers are also diverse as a result of a wide range of factors : % for example, membership in different language communities , ethnicities, cultures, age groups, social groups , residency in different places  or practice of different social roles. % In any event, in analogy to Step 1, we may expect that different communities dealing with the same or semantically similar parts of the IL should be able to align their corresponding intertext models among each other. % In shorter terms:  % different groups should be able to represent similar parts of the IL in a similar way. % But do they actually have access to the same or at least sufficiently similar parts of the IL -- especially under the condition that they deal with the same topic?    {}  -- even in the case of algorithmically  linked documents . % But how uniformly does the IL present itself to its  readers?  % Obviously this question is currently outside the scope of the framework of the DM and its relatives. %   Step 2 concerns precisely the viewpoint of this article. % That is, we are concerned with a central prerequisite for alignable intertext models among readers as members of large communities.  % This refers to the intertextual shape of the IL from the perspective of different communities who may have different accesses to it or  different landscapes, even in situations where the opposite would be assumed. % The DM and related approaches do not model what the multiple texts are extracted from and what countless intertext models in their distributed totality ultimately represent, that is, an underlying multifaceted, highly dynamic IL, its numerous document nodes and their relational, intertextual embeddings.  According to , reading research mostly considers small amounts of offline texts pre-selected by the experimenter rather than open ILs in which users decide what to read. % But if reading is a kind of problem solving that involves multiple search and decision processes  , then the question arises as to the limits of these processes as imposed by the IL and how they differ for which reader communities. % Apparently, approaches to multiple texts focus on micro-models that leave the corresponding macro-models, which inform about the shape of the IL and its organizational laws, under-specified. % The present paper takes a step in the direction of filling this gap: % it develops a macroscopic model of the IL and examines how its shape appears from the perspective of certain large-scale reader communities. % Our aim is, so to speak, to impart knowledge about the  in which the sort of reading takes place which according to  is to become the subject of reading research. % Thus, our approach is complementary to current research on the intertext model: % we study the IL underlying the construction of intertext models from a macroscopic perspective, in contrast to reading research, which starts from a microscopic perspective of small groups or individual readers . % \textcolor{black}{In terms of the integrated framework of multiple texts  we are concerned with the intertextuality of those information units to which the cognitive strategies and behavioral skills of  readers are related.} % That is, in modification of the fourth goal of future research on the use of multiple sources according to , we deal with the phenomenon that different communities are offered different information, especially in the context of the same topic. % The extent to which this phenomenon applies to different language communities will be examined using the example of the most frequently used knowledge resource on the Web, that is Wikipedia \rrid{RRID:SCR\_004897}.  %\textcolor{red}{Our contributions are:}  The article is organized as follows: % Section  explains the relevance of Wikipedia for educational science and gives an overview of related research. % Section  explains our research questions and describes in detail the methods we have developed to answer them.  % In Section , we describe our experiments and discuss their results. % Finally, in Section  we give a conclusion and an outlook on future work.    
","  We test the hypothesis that the extent to which one obtains information on a given topic through Wikipedia depends on the language in which it is consulted.  % Controlling the size factor, we investigate this hypothesis for a number of 25 subject areas.  % %That is, we test whether even in cases where two Wikipedias cover the same topic with approximately the same number of articles, they inform about this topic rather differently. % Since Wikipedia is a central part of the web-based information landscape, this indicates a language-related,  linguistic bias.  % The article therefore deals with the question of whether Wikipedia exhibits this kind of linguistic relativity or not. % From the perspective of educational science, the article develops a computational model of the information landscape from which multiple texts are drawn as typical input of web-based reading. % For this purpose, it develops a hybrid model of intra- and intertextual similarity of different parts of the information landscape and tests this model on the example of 35 languages and corresponding Wikipedias. % In this way the article builds a bridge between reading research, educational science, Wikipedia research and computational linguistics.  %%% Leave the Abstract empty if your article does not require one, please see the Summary Table for full details. % % %Deadline: 15.\,05.~2020\par %For full guidelines regarding your manuscript please refer to {Author Guidelines}.  %As a primary goal, the abstract should render the general significance and conceptual advance of the work clearly accessible to a broad readership. References should not be cited in the abstract. Leave the Abstract empty if your article does not require one, please see {Summary Table} for details according to article type.    \tiny     %All article types: you may provide up to 8 keywords; at least 5 are mandatory.",136
"  Context and Scope: The US healthcare system is a complex setup governed and managed by state and federal agencies. Managed Care is a health delivery system utilised by Medicaid to manage cost, utilization and quality of healthcare. The Managed Care system uses contract agreements between Medicaid agencies and Managed Care Organisations  for providing these services. Some states even utilize this system beyond traditional managed care for initiatives such as care improvement for chronic \& complex conditions, payment initiatives, etc. Contracts run the gamut from computer support to janitorial services to direct client services. HHS posts all notifications of new Request for Proposal /solicitation releases, Requests for Application and Open Enrolments. RFPs are bid requests consisting of functional and non-functional requirements for different services. These also outline model contracts and the expected format of the proposals. The requirements are mentioned in the form of different questions/queries which are answered by each proposal/response to these RFPs. The procurement of these contracts entirely depends upon the scores obtained for each response based on the predefined evaluation criteria. A contract is generally awarded to the best scoring respondent.      A typical RFP bid consists of RFP advertisement, RFP itself, a model contract, proposals/responses from bidding entities  and scoring sheets for all the submissions. RFPs and supporting documents are publicly available information. MCOs typically utilise historical submissions to understand the requirements and respond better to improve their chances of winning a bid. Every RFP response  typically runs into several hundred pages which are spread across different websites and data stores. Manual exploration of historical bids is a time consuming and iterative process. Given the changing healthcare landscape, limited time-frame and resources to draft new responses, the current process is not comprehensive enough to extract insights and derive competitive advantage.     Challenges: Apart from being an industry specific problem statement, our work also poses a unique challenge of scoring entire documents. Most relevant efforts towards automatic scoring have dealt with with short answers  and essays . Our work deals with much larger sequence lengths, and a larger feature space to capture. Another difference with relevant literature is that RFPs are written by experts over multiple iterations, as opposed to students writing essays for evaluation. As such, this removes the need to check for superficial grammatical errors. Instead, there is a need to identify which aspects of the text enhance scores  and those which diminish it .  Our Solution:  In this paper, we propose an automated framework using interpretable natural language processing techniques to analyse RFP responses. The framework comprises of two components: Text Processing Module and an Interpretable Scoring Model.  RFP responses usually do not follow any standard template/formatting and are available in Portable Document Format or PDF for short. Moreover, to understand the content and extract insights, the text needs to be extracted at the most granular level . These issues complicate the text extraction process  and thus the need to develop a Text Processing Module. We have developed a generic Text processing module that would extract text from different formats of response. The extracted text is then analysed using our Interpretable Scoring Model. The scoring model enables us to identify terms/phrases and other auxiliary features which impact the section/question score positively and negatively. We term positively impacting features as enablers and negatively impacting ones as disablers. The framework also provides insights about auxiliary features which latently impact overall scoring. The framework also provides a single portal/platform to access historical bid responses for similar details across bidders and states.  %  Major Contribution of this work is as follows, we have:     
"," The Managed Care system within Medicaid  uses Request For Proposals  to award contracts for various healthcare and related services. RFP responses are very detailed documents  submitted by competing organisations to win contracts. Subject matter expertise and domain knowledge play an important role in preparing RFP responses along with analysis of historical submissions. Automated analysis of these responses through Natural Language Processing   systems can reduce time and effort needed to explore historical responses, and assisting in writing better responses.  Our work draws parallels between scoring RFPs and essay scoring models, while highlighting new challenges and the need for interpretability. Typical scoring models focus on word level impacts to grade essays and other short write-ups. We propose a novel Bi-LSTM based regression model, and provide deeper insight into phrases which latently impact scoring of responses. We contend the merits of our proposed methodology using extensive quantitative experiments. We also qualitatively asses the impact of important phrases using human evaluators. Finally, we introduce a novel problem statement that can be used to further improve the state of the art in NLP based automatic scoring systems.",137
"  Language model pre-training has shown great power for improving many natural language processing tasks.  Most pre-training models, despite their variety, follow the BERT architecture heavily relying on multi-head self-attention to learn comprehensive representations. It has been found that 1) though the self-attention module in BERT is a highly non-local operator, a large proportion of attention heads indeed learn local dependencies due to the inherent property of natural language;  2) removing some attention heads during fine-tuning on downstream tasks does not degrade the performance.  The two findings indicate that heavy computation redundancy exists in the current model design. In this work, we aim to resolve this intrinsic redundancy issue and further improve BERT w.r.t. its efficiency and downstream task performance. We consider such a question: can we reduce the redundancy of attention heads by using a naturally local operation to replace some of them?  We notice that convolution has been very successful in extracting local features , and thus propose to use convolution layers as  a more efficient complement to self-attention for addressing local dependencies  in natural language.      Specifically, we propose to integrate convolution into self-attention to form a  mechanism that combines the advantages of the two operations. Self-attention uses all input tokens to generate attention weights for capturing global dependencies, while we expect to perform local ``self-attention'', i.e., taking in a local span of the current token to generate ``attention weights'' of the span to capture local dependencies. To achieve this, rather than deploying standard convolution with fixed parameters shared for all input tokens,  dynamic convolution  is a good choice that offers higher flexibility in capturing local dependencies of different tokens.  As shown in Fig.b, dynamic  convolution uses a kernel generator to produce different kernels for different input tokens.  However, such dynamic convolution cannot differentiate the same tokens within different context and generate the same kernels .    We thus develop the , a novel convolution that produces more adaptive convolution kernels by receiving an input span instead of only a single token, which enables  discrimination of generated kernels for the same tokens within different context. For example, as shown in Fig.c, the proposed  produces different kernels for different ``can'' tokens. With , we build the  to improve the conventional self-attention, which brings higher efficiency for pre-training as well as better performance for capturing global and local information.    To further enhance performance and efficiency, we also add the following new architecture design to BERT.  First, a bottleneck structure is designed to reduce the number of attention heads by embedding input tokens to a lower-dimensional   space for self-attention.  This also relieves the redundancy that lies in attention heads and improves efficiency.  Second, the feed-forward module in BERT consists of two fully connected linear layers with an activation in between, but the dimensionality of the inner-layer is set much higher  than that of input and output, which promises good performance but brings large parameter number and computation.  Thus we devise a grouped linear operator for the feed-forward module, which reduces parameters without hurting representation power.  Combining these novelties all together makes our proposed model, termed ConvBERT, small and efficient.      Our contributions are summarized as follows. 1) We propose a new  to replace the self-attention modules in BERT, which leverages the advantages of convolution to better capture local dependency.  To the best of our knowledge, we are the first to explore convolution for enhancing BERT efficiency. 2) We introduce a novel  operation to utilize multiple input tokens to dynamically generate the convolution kernel.  3) Based on the proposed , we build ConvBERT model. On the GLUE benchmark, ConvBERTbase achieves 86.4 GLUE score which is 5.5 higher than BERTbase and 0.7 higher than ELECTRAbase while requiring less training cost and parameters. 4) ConvBERT also incorporates some new model designs including the bottleneck attention and grouped linear operator that are of independent interest for other NLP model development.    \documentclass{article}  % if you need to pass options to natbib, use, e.g.:     \PassOptionsToPackage{numbers, compress}{natbib} % before loading neurips_2020  % ready for submission % \usepackage{neurips_2020}  % to compile a preprint version, e.g., for submission to arXiv, add add the % [preprint] option:     % \usepackage[preprint]{neurips_2020}  % to compile a camera-ready version, add the [final] option, e.g.:     \usepackage[final]{neurips_2020}  % to avoid loading the natbib package, add option nonatbib: % \usepackage[nonatbib]{neurips_2020}  \usepackage[utf8]{inputenc} % allow utf-8 input \usepackage[T1]{fontenc}    % use 8-bit T1 fonts \usepackage{hyperref}       % hyperlinks \usepackage{url}            % simple URL typesetting \usepackage{booktabs}       % professional-quality tables \usepackage{amsfonts}       % blackboard math symbols \usepackage{nicefrac}       % compact symbols for 1/2, etc. \usepackage{xcolor} \usepackage{microtype}      % microtypography \usepackage{xspace} \usepackage{subcaption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{pifont} \usepackage{floatrow} \floatsetup[table]{capposition=top} \usepackage{float} \usepackage{wrapfig} \usepackage{comment} \usepackage{caption} \definecolor{mypink}{RGB}{251,228, 234}    {span-based dynamic convolution} {mixed attention}  \title{ConvBERT: Improving BERT with Span-based Dynamic Convolution}  % The \author macro works with any number of authors. There are two commands % used to separate the names and addresses of multiple authors: \And and \AND. % % Using \And between authors leaves it to LaTeX to determine where to break the % lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4 % authors names on the first line, and the last on the second line, try using % \AND instead of \And before the third author name.   \author{Zihang Jiang\thanks{Work done during an internship at Yitu Tech.}~~\thanks{Equal contribution.}~~, Weihao Yu\samethanks~~, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan \\   National University of Singapore,    Yitu Technology\\ 	@gmail.com,} \\ 	, ~ ,  ~  \\ }           %   % \newpage 
"," Pre-trained language models like BERT and its variants have recently achieved impressive performance in various natural language understanding tasks. However, BERT heavily relies on the global self-attention block and thus suffers large memory footprint and computation cost. Although all its attention heads query on the whole input sequence for generating the attention map  from a global perspective, we observe some heads only need to learn local dependencies, which means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to replace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the rest self-attention heads, form a new mixed attention block that is more efficient at both global and local context learning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that ConvBERT significantly outperforms BERT and its variants in various downstream tasks, with lower training costs and fewer model parameters. Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than ELECTRAbase, using less than $1/4$ training cost. \footnote{Code and pre-trained model will be released at \url{https://github.com/yitu-opensource/ConvBert}.}",138
"    One of the defining characteristics of human languages is that they are productive. % We can combine together concepts in novel ways to express ideas that have never been thought of before. % This is for a good reason: as children, we observe very little of our world before we must speak to others, meaning that even mundane language is novel and not just parroting back something already expressed for us. % Similarly, even with massive data collection efforts, deep models can only have an opportunity to observe a small subset of the possible utterances and worlds. % This problem becomes especially acute when those models must drive the behavior of a robot, because misunderstanding a command may pose a serious safety hazard.  Recently, there have been a number of attempts to probe the understanding of deep networks trained to perform linguistic and robotic tasks. %  point out that generalization to novel compositions of concepts is rather limited. % This is not a matter of the amount of data available; for example,  find that even networks with the same test set performance can have very different generalization abilities. % Most recently,  released gSCAN, a dataset for testing the generalization abilities of grounded robots. % In gSCAN, a robotic agent must follow a natural-language command in a 2D environment. % Commands of specific types are systematically held out; for example, no command with a particular adjective-noun combination might appear in the training set. % When all concepts appear in the training set via a random split between the training and test set, performance is phenomenal: 97\% of commands are executed correctly. % Yet, when even the simplest combinations are missing from the training set, such as holding out an adjective-noun pair like ``yellow squares'', only 35\% to 55\% of commands are executed correctly.  Guided by the notion that compositionality is the central feature of human languages which deep networks are failing to internalize, we construct a compositional deep network to guide the behavior of robots, thus building on the work of . % Given a command, a command-specific network is assembled from previously-trained components. % Components are automatically discovered in the training set without any annotation. % The structure that combines those components is derived from the linguistic structure of the command. %  In this way, the compositional structure of language is reflected in the compositional structure of the computations executed by the network. % This approach generalizes where non-compositional networks fail to do so thereby addressing many of the limitations pointed out in gSCAN and prior work. % For example, our compositional approach correctly executes 95\% of commands involving novel adjective-noun pairs, compared with 35\% to 55\% reported in prior work.    Compositionality is not specific to any one dataset -- it is a general principle -- and the implementation we provide here is not specific to gSCAN. % Even though our base network achieves the same 97\% performance as the state-of-the-art approaches in gSCAN, it generalizes significantly better in a number of ways. % It outperforms the state of the art in most test conditions explored by gSCAN, generally by a large margin. % Where this approach shines is predicted well by the types of compositionality that exist in the network. % For example, novel combinations of concepts related to individual objects perform well. % However, no compositionality exists at the level of the map or directions, just in how concepts are combined, meaning that novel combinations of directions are not well understood; but even in this case, our approach is outperforming the state of the art by 5\%.  An additional benefit of compositional approaches appears to be that they open the door to naturally including other general linguistic principles. % For example, it appears that not all parses are made equal. % In our case, network structures derived from a semantic parser lead to better performing agents compared to structures derived from a constituency parser and a dependency parser. % We show another example of this idea by incorporating the lexical semantics of words as an additional loss while training the network. % This incorporates general notions such as big and small being antonyms of one another.  Our approach forgoes the most popular mechanism for increasing the generalization performance of neural networks: data augmentation. % Work on gSCAN shows that the gains from data augmentation appear to be low, but still significant. % Data augmentation has substantial drawbacks: it is arbitrary, it slows down runtime, and it is dataset and problem specific. % In addition, data augmentation introduces many parameters that must be tuned by hand and much knowledge that must be provided by humans. % Instead, we show that the generic principle of compositionality can replace data augmentation without any of these drawbacks: no human annotation is required; no additional systems or parameters need to be introduced;  and training time is not affected. % It remains an open question whether every data augmentation approach has a corresponding compositional structure that can supplant and generalize it. % Compositional approaches could be combined with data augmentation, potentially raising their performance even further.  % % The state of the art performance on that dataset demonstrates stunning % performance drops., even novel adjective-noun combinations drop % command-following performance from 97\% to 38\%.  %  the agent needs to understand what each word means, disentangle the meaning of each word %  combine the learned words to interpret a new command %  only need few data to acquire the meanings of new words  Our work makes four contributions.    	 %===============================================================================  
","   Humans are remarkably flexible when understanding new sentences that include   combinations of concepts they have never encountered before. Recent work has   shown that while deep networks can mimic some human language abilities when   presented with novel sentences, systematic variation uncovers the limitations   in the language-understanding abilities of neural networks. We demonstrate that these   limitations can be overcome by addressing the generalization challenges in a   recently-released dataset, gSCAN, which explicitly measures how well a robotic   agent is able to interpret novel ideas grounded in vision, e.g., novel   pairings of adjectives and nouns. The key principle we employ is   compositionality: that the compositional structure of networks should reflect   the compositional structure of the problem domain they address, while allowing   all other parameters and properties to be learned end-to-end with weak   supervision. We build a general-purpose mechanism that enables robots to   generalize their language understanding to compositional domains. Crucially,   our base network has the same state-of-the-art performance as prior work, 97\%   execution accuracy, while at the same time generalizing its knowledge when   prior work does not; for example, achieving 95\% accuracy on novel   adjective-noun compositions where previous work has 55\% average   accuracy. Robust language understanding without dramatic failures and without   corner causes is critical to building safe and fair robots; we demonstrate the   significant role that compositionality can play in achieving that goal.",139
"  The proliferation of disinformation online, commonly known as ``fake news'', has given rise to a lot of research on automatic fake news detection. However, most of the efforts have focused on checking whether a piece of information is factually correct, and little attention has been paid to the propaganda techniques that malicious actors use to spread their message.  SemEval-2020 Task 11  aims to bridge this gap. It focused on detecting the use of propaganda techniques in news articles, creating a dataset that extends , and offering two subtasks:   detecting the propaganda spans in an article;  detecting the type of propaganda used in a given text span.    Below, we describe the systems we built for these two subtasks. At the core of our systems is RoBERTa , a pre-trained model based on the Transformer architecture . However, we improved over RoBERTa by adding extra layers in the neural network architecture, and we further added some post-processing steps. We further applied transfer learning between the two subtasks, and finally, we combined different models into an ensemble.  %The rest of this paper is organized as follows. Section discusses related work. Section describes the general architecture of our models. Section provides further details about our experimental setup. Section discusses the results. Finally, Section offers a conclusion.   
","   We describe our system for SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. We developed ensemble models using RoBERTa-based neural architectures, additional CRF layers, transfer learning between the two subtasks, and advanced post-processing to handle the multi-label nature of the task, the consistency between nested spans, repetitions, and labels from similar spans in training. We achieved sizable improvements over baseline fine-tuned RoBERTa models, and the official evaluation ranked our system $\mathbf{3^{rd}}$  out of 36 teams on the span identification subtask with an F1 score of 0.491, and $\mathbf{2^{nd}}$  out of 31 teams on the technique classification subtask with an F1 score of 0.62.",140
"  The 2019--2020 coronavirus pandemic has disrupted lives, societies and economies across the globe. Its classification as a pandemic highlights its global impact, touching people of all languages. Digital content of all types  have focused for many weeks predominantly on the sanitary crisis and its effects on infected people, their families, healthcare workers and the society and economy at large. This calls not only for a large set of tools to help during the pandemic , but also for tools to help digest and analyze this data after it ends. By analyzing the representation and reaction across countries with different guidelines or global trends, it might be possible to inform policies in prevention of and reaction to future epidemics. % AB: ""span countries""? and weird formulation Several institutions and groups have already started to take snapshots of the digital content shared during these weeks~.  However, because of its global scale, all this digital content is accessible in a variety of different languages, and most existing NLP tools remain English-centric~. In this paper we describe the release of a multilingual neural machine translation model  that can be used to translate biomedical text. The model is both multi-domain and multilingual, covering translation from French, German, Spanish, Italian and Korean to English.  Our contributions consist in the release of:        This paper is structured as follows: in Section we overview previous work upon which we build; Section details the model and data settings, and the released test set; and Section compares our model to other public models and to state-of-the-art results in academic competitions.  The model can be downloaded at  \url{https://github.com/naver/covid19-nmt}: the repository consists in a model checkpoint that is compatible with Fairseq~, and a script to preprocess the input text.   
"," We release a multilingual neural machine translation model, which can be used to translate text in the biomedical domain. The model can translate from 5 languages  into English. It is trained with large amounts of generic and biomedical data, using domain tags. Our benchmarks show that it performs near state-of-the-art both on news  and biomedical test sets, and that it outperforms the existing publicly released models. We believe that this release will help the large-scale multilingual analysis of the digital content of the COVID-19 crisis and of its effects on society, economy, and healthcare policies. We also release a test set of biomedical text for Korean-English. It consists of 758 sentences from official guidelines and recent papers, all about COVID-19.",141
"  Infants' speech perception changes in the first year of their life. For example, at the age of 6--8 months, English-learning and Japanese-learning infants are equally able to detect the difference between sounds \textipa{[\*r]}  and \textipa{[l]} , whereas by the age of 10--12 months, the two groups diverge, showing attunement to the phonetic contrasts present in their input language . Similar results have been reported for many other languages . A number of theoretical accounts explaining such early phonetic learning have been proposed , but until recently no computational models could explain  how the specific speech input to which infants are exposed leads to the observed changes in those infants' discrimination of phonetic contrasts. Models  been evaluated on their ability to learn phonetic categories, rather than to predict patterns of discrimination , but none has yet succeeded in that task when learning from non-idealized natural speech input .   In a recent study,  were the first to present such a computational model, which correctly predicted the documented cross-linguistic difference in infants' discrimination of \textipa{[\*r]} and \textipa{[l]} after learning from unsegmented speech. They explicitly simulated the learning process for Japanese and American English infants by  training their model on naturalistic speech recordings either in Japanese or in American English.  They then measured the trained models' ability to discriminate \textipa{[\*r]} and \textipa{[l]} with the machine ABX task, a flexible measure of discrimination that can be applied to model representations in essentially any format. To obtain a model capable of handling realistic input, they selected an algorithm for unsupervised learning from naturalistic speech that had been proposed in the context of engineering applications. The success of their model raises the question of whether other learning algorithms recently proposed in this context may lead to equally good, or even better, models of infants' early phonetic learning.  In this paper we apply the approach introduced in  to test a variety of models on multiple data sets of infant phone discrimination.  Doing so allows us  to gain insight into the kinds of representations and learning mechanisms that infants are likely to employ. We consider five different learning algorithms developed in the speech technology community, focusing on those that implement cognitively plausible learning mechanisms. We test the models on three crosslinguistic phone discrimination tasks grounded in infant studies from different languages.  Our two goals are  to test whether 's result is specific to their particular model applied to a particular American English phonetic contrast, and  to identify which computational models can best explain the existing infant data. We find that two models show infant-like crosslinguistic discrimination patterns for American English \textipa{[\*r]}--\textipa{[l]} and another contrast in Mandarin Chinese, while three other models appear less successful as models of early phonetic learning. We perform additional analyses to assess whether the two most successful models---which substantially differ in their learning mechanisms and representation formats---also differ in what they learn. The results suggest that it should be possible to find an empirical test to decide between these two models.  %  % Annual Cognitive Science Conference % Sample LaTeX Paper -- Proceedings Format %   % Original : Ashwin Ram        04/01/1994 % Modified : Johanna Moore       03/17/1995 % Modified : David Noelle           03/15/1996 % Modified : Pat Langley    01/26/1997 % Latex2e corrections by Ramin Charles Nakisa        01/28/1997  % Modified : Tina Eliassi-Rad   01/31/1998 % Modified : Trisha Yannuzzi  12/28/1999  % Modified : Mary Ellen Foster  12/11/2000 % Modified : Ken Forbus                              01/23/2004 % Modified : Eli M. Silk             05/24/2005 % Modified : Niels Taatgen          10/24/2006 % Modified : David Noelle      11/19/2014 % Modified : Roger Levy      12/31/2018  %% Change ""letterpaper"" in the following line to ""a4paper"" if you must.  \documentclass[10pt,letterpaper]{article}  \usepackage{cogsci}   \usepackage{apacite} \usepackage{float} % Roger Levy added this and changed figure/table                    % placement to [H] for conformity to Word template,                    % though floating tables and figures to top is                    % still generally recommended! \usepackage{graphicx} \usepackage{xcolor} \usepackage[tone]{tipa} \usepackage{natbib} \usepackage{multirow} \usepackage{siunitx} \usepackage{threeparttable} \usepackage{url} \usepackage[shortlabels]{enumitem} \usepackage{microtype} \usepackage{arydshln} \usepackage[hidelinks]{hyperref}   \usepackage{color,soul} }}  %\usepackage[none]{hyphenat} % Sometimes it can be useful to turn off %hyphenation for purposes such as spell checking of the resulting %PDF.  Uncomment this block to turn off hyphenation.    % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 4.5cm . %%If you do, we reserve the right to require you to change it back in %%the camera-ready version, which could interfere with the timely %%appearance of your paper in the Proceedings.  \title{Evaluating computational models of infant phonetic learning across languages}  \author{{\large  \\   Department of Linguistics \& UMIACS, University of Maryland \\   \AND {\large  \\   Department of Linguistics \& UMIACS, University of Maryland \\   \AND {\large [1]{\textcolor{blue}{To-do: #1}} [1]{\textcolor{orange}{#1}} [1]{#1} }        In the first year of life, infants' speech perception becomes attuned to the sounds of their native language. Many accounts of this early phonetic learning exist, but computational models predicting the attunement patterns observed in infants from the speech input they hear have been lacking. A recent study presented the first such model, drawing on algorithms proposed for unsupervised learning from naturalistic speech, and tested it on a single phone contrast. Here we study five such algorithms, selected for their potential cognitive relevance.  We simulate phonetic learning with each algorithm and perform tests on three phone contrasts from different languages, comparing the results to infants' discrimination patterns. The five models display varying degrees of agreement with empirical observations, showing that our approach can help decide between candidate mechanisms for early phonetic learning, and providing insight into which aspects of the models are critical for capturing infants' perceptual development.  Keywords:  early phonetic learning; representation learning; phone discrimination; computational model         
","  In the first year of life, infants' speech perception becomes attuned to the sounds of their native language. Many accounts of this early phonetic learning exist, but computational models predicting the attunement patterns observed in infants from the speech input they hear have been lacking. A recent study presented the first such model, drawing on algorithms proposed for unsupervised learning from naturalistic speech, and tested it on a single phone contrast. Here we study five such algorithms, selected for their potential cognitive relevance.  We simulate phonetic learning with each algorithm and perform tests on three phone contrasts from different languages, comparing the results to infants' discrimination patterns. The five models display varying degrees of agreement with empirical observations, showing that our approach can help decide between candidate mechanisms for early phonetic learning, and providing insight into which aspects of the models are critical for capturing infants' perceptual development.  Keywords:  early phonetic learning; representation learning; phone discrimination; computational model",142
"   Natural language has the potential to be the most effective and convenient way to issue commands to a robot.  % However, machine acquisition of language is difficult due to the context- and speaker-specific variations that exist in natural language. % For instance, English usage differs widely throughout the world: between children and adults, and in businesses vs. in homes. % This does not pose a significant challenge to human listeners because we acquire language by observing how others use it and then attempting to use it ourselves. % Upon observing the language use of other humans, we discover the latent structure of the language being spoken. % We develop a similar approach for machines.  Our grounded semantic parser learns the latent structure of natural language utterances. % This knowledge is executable and can be used by a planner to run commands. % The parser only observes how language is used: what was said  indicates a condition that must persist, while dim the lights indicates a goal that must be met at least once, but need not be repeated. % % % In this work, we present a grounded semantic parser that maps natural language to programs in Linear Temporal Logic , a formalism that is equipped with operators that can be composed to represent complex temporal properties. % % % Increasing the ability of machines to understand these temporal properties is relatively unexplored territory in the field of semantic parsing and brings us closer to robotic assistants that can smoothly handle natural language commands.  Our approach performs well on both machine and human generated sentences. % In both cases, it is able to execute around 80\% of commands successfully. % A traditional fully-supervised approach on the machine-generated sentences outperforms ours with 95\% accuracy, but requires the ground-truth LTL formulas. % Where this approach of discovering latent structures shines is on real-world data. % We asked human subjects, unconnected with this research and without knowledge of robotics or ML, to produce sentences that describe the behavior of robots. % This behavior was produced according to an LTL formula that we randomly generated, but the humans were free to describe whatever they wanted with whatever words they desired as long as it was true. % Despite the human sentences being much more varied and complex, including structures which our formalism cannot exactly represent, our method still finds whatever latent structure is present and required to execute the natural-language commands produced by humans with nearly the same accuracy as those produced by machines.  Executing LTL commands and understanding the kinds of temporal relations that require LTL is particularly difficult due to the richness and openness of natural language . % But LTL is just a stepping stone. % It remains an important open question in grounded robotics: what representation will be enough to capture the richness of how humans use language? % For instance, notions such as modal operators to reason about hypothetical futures will likely be required, but what else is unclear. % Having a general-purpose mechanism for learning to execute commands is extremely helpful under these conditions; we can experiment with different logics and domains with the same agents by changing the priors and planners while leaving the rest of the system intact.   % In typical weakly supervised parsing frameworks, the reward depends on the final state of execution, as evaluated by an executor. % % % However, learning temporal constraints requires a reward that is sensitive to behavior across time, not just at the final state. % % % We propose a reward that is sensitive to behavior across time: each hypothesized parse is input to the pre-trained neural planner from , and given a reward based on the likelihood of the observed executions. % % % Since the planner is trained to produce correct executions for LTL programs, it assigns a high reward to programs that are more likely to give rise to the observed behavior. % % % This allows the model to learn meaning representations that lead to correct behavior across time, rather than behavior which simply reaches the correct destination. % %  % Training using weak supervision has many benefits.  % % % It removes the need for an annotated dataset of natural language and program pairs, which is labor intensive to produce by hand. % % % Moreover, due to the ambiguity in natural language, a sentence may have many interpretations, and each interpretation may have many equivalent representations. % % % For this reason, training using weak supervision avoids over-fitting to biases in human annotation which might result in poor generalization. % % % Finally, because our model uses weak supervision, our system  paves the way for a system in which a semantic parser and planner can be trained jointly. % %  % Learning in this weak supervision setting is difficult because the space of possible meaning representations is large and many spurious forms may receive a non-zero reward even though they only coincidentally result in the correct behavior. % % % For instance, consider a command that requires the robot's proximity to a flag . % % % If the flag happens to be adjacent to an apple, then a candidate program with the meaning pick up the apple will receive an erroneous reward. % % % These challenges are made more difficult by the fact that the executor is itself a neural model, so uncertainty is introduced through noisy perception and imperfect planning. % % % We address spuriousness by encouraging our model to produce embeddings that more closely attend to the semantics of the input by using multi-task learning. % % % These embeddings are used to reconstruct the original command as well as the output parse .  % To obtain data for training and evaluation, we use a synchronous context free grammar to generate pairs of sentences and LTL programs. % % % An oracle produces  demonstration executions for each program. % % % We then use human annotators to write a command for each set of demonstrations, which gives us a dataset of natural language sentences paired with execution traces. % % % We test on a dataset of held out natural language commands.    The main contributions of this work are:   suited for grounded   semantic parsing experiments, and   
","   Children acquire their native language with apparent ease by    observing how language is used in context and attempting to use it   themselves.   %   They do so without laborious annotations, negative examples, or even   direct corrections.   %   We take a step toward robots that can do the same by training a grounded   semantic parser, which discovers latent linguistic representations that can be used for the execution of  natural-language commands.   %   In particular, we focus on the difficult domain of commands with   a temporal aspect, whose semantics we capture with    Linear Temporal Logic, LTL.   %   Our parser is trained with pairs of sentences and executions as well as an   executor.   %   At training time, the parser hypothesizes a meaning representation for the input as a formula in LTL.   %   Three competing pressures allow the parser to discover  meaning from   language.   %   First, any hypothesized meaning for a sentence must be permissive enough to reflect all the annotated execution   trajectories.   %   Second, the executor --- a pretrained end-to-end LTL planner --- must find that the observed   trajectories are  likely executions of the meaning.   %   Finally, a generator, which reconstructs the original input, encourages the model to find representations that conserve knowledge about the command.   %   Together these ensure that the meaning is neither too general nor too   specific.   %   Our model generalizes well, being able to parse and execute both   machine-generated and human-generated commands, with near-equal accuracy,   despite the fact that the human-generated sentences are much more varied and complex with   an open lexicon.   %   The approach presented here is not specific to LTL: it can be applied to any domain   where sentence meanings can be hypothesized and  an executor can   verify these meanings, thus opening the door to many applications for robotic agents.        % %     % We generate executable programs in Linear Temporal Logic  from natural language commands by training a semantic parser in a weak supervision setting where ground truth programs are not observed.      % %     % Our parser learns by observing execution traces for each input natural language command.     % %     % During training, the parser proposes candidate programs and receives a supervision signal from a pre-trained planner, which executes these programs against a set of observed traces and assigns a reward based on their likelihood.     % %     % We generate programs for commands that contain temporal constraints, a domain which other semantic parsing approaches often neglect.     % %     % Learning using weak supervision is difficult in this domain, due to the large exploration space and the fact that many spurious forms may incorrectly receive a reward.     % %     % Despite these challenges, our weakly supervised approach is able to produce meaning representations that correctly reflects the demonstration traces 85\% of the time, compared to 16\% under random chance and 95\% under full supervision.",143
"  In recent years, introduction of Recurrent Neural Networks , in particular, Long Short Term Memory  RNNs  have been shown to outperform feed-forward neural networks  and significantly improve the performance of automatic speech recognition  systems. More recently, it has been shown that LSTMs when trained with CTC loss followed by sequence discriminative training can achieve state-of-the-art performance on various large-scale acoustic modelling tasks . However, a primary bottleneck of these frameworks is their dependence on availability of large amounts of labeled data. Since collection and transcription of large amounts of speech data is costly and time-consuming, techniques to leverage large amounts of unlabelled data for acoustic modeling are explored under the framework of semi-supervised learning .  SSL for ASR involves automatic generation of labels for unlabelled data and leveraging them along with labelled data for building ASR systems. Self-training has been shown to be an effective SSL framework where an initial system is trained on labelled data to generate machine transcriptions for unlabelled data . However, the effectiveness of such systems gets constrained by the ``goodness"" of the initial system and ``quality"" of machine transcripts. Teacher-Student based learning  originally proposed in the context of knowledge distillation  for model compression is another approach used for SSL. KD involves using a strong teacher model trained on labelled data to generate  labels which are used to train a student model to match teacher output distribution.   The effectiveness of KD has been well established in speech recognition tasks . However, conventional frame-level based KD approaches may not be directly feasible for CTC trained models due to `alignment-free' nature of CTC loss function. Modifications have been proposed in recent studies which either attempt to align teacher and student models' spike timings  or propose KD at the level of sequences instead of frames . The cost associated with aligning output spikes from teacher and student model make the frame-level based approaches computationally expensive when using large amounts of unlabelled data. Existing studies on sequence-level KD for CTC entail a certain degree of complexity in sampling n-best hypothesis  or computing forward-backward posteriors from teacher model . A simplified version of sequence-level KD has been studied in the context of neural machine translation  , which essentially collapses the entire label sequence space to a single 1-best sequence. In this work, we simplify this assumption further by approximating the entire label sequence space by a single 1-best sequence obtained by concatenating frame-level argmax output from teacher model. Our results demonstrate that such a framework provides better WER as compared to a standard self-training based approach.  %The efficiency of our method can be attributed to 3 assumptions: 1) For a given label sequence, sum over all corresponding paths can be approximated by its maximum value, 2) Greedy decoding approximation,  3) Teacher posterior distribution can be replaced by a Dirac delta function at its mode.  In a recent study in KD , it was shown that augmenting labelled data with 1 million hours of randomly selected unlabelled data improves relative WER by -\%. However, processing such large quantities of unlabelled data and subsequent training requires access to powerful compute and storage resources and leads to significant increase in training time. The prime motivation for this work is to seek answer to the following question: Instead of large amounts of randomly selected unlabelled data, is it possible to design an intelligent data selection scheme which can achieve similar WER gains but with lesser data?  Majority of data selection approaches studied in the past have relied on confidence scores, entropy based confidence measure and local-global entropy minimization among others for bootstrapping initial seed model with additional unlabelled data  in the self-learning framework. To the best of our knowledge, data selection in the context of teacher-student learning has not been explored extensively.  We present an empirical study of the role of different data selection mechanisms based on confidence and Natural Language Understanding  based domain distribution as well as speaker and content diversity on WER of CTC-SSL based student model. Overall, our CTC-SSL framework achieves 17\% relative WER improvement over a baseline CTC system with labelled data only and on-par performance with a CTC-SSL model trained on an order of magnitude higher randomly sampled unlabelled data.  %This paper is organized as follows, Section 2 describes the knowledge distillation based method used in this study for semi-supervised learning in CTC acoustic models. Section 3 discusses details about different attributes that we have used for data selection in this work. Section 4 describes our CTC-SSL experimental setup and its results with different data selection techniques. Finally, Section 5 concludes our work and suggests future directions of work.   
"," Semi-supervised learning  is an active area of research which aims to utilize unlabelled data in order to improve the accuracy of speech recognition systems. The current study proposes a methodology for integration of two key ideas: 1) SSL using connectionist temporal classification  objective and teacher-student based learning 2) Designing effective data-selection mechanisms for leveraging unlabelled data to boost performance of student models. Our aim is to establish the importance of good criteria in selecting samples from a large pool of unlabelled data based on attributes like confidence measure, speaker and content variability. The question we try to answer is: Is it possible to design a data selection mechanism which reduces dependence on a large set of randomly selected unlabelled samples without compromising on Word Error Rate ? We perform empirical investigations of different data selection methods to answer this question and quantify the effect of different sampling strategies. On a semi-supervised ASR setting with 40000 hours of carefully selected unlabelled data, our CTC-SSL approach gives 17\% relative WER improvement over a baseline CTC system trained with labelled data. It also achieves on-par performance with CTC-SSL system trained on order of magnitude larger unlabeled data based on random sampling.",144
"  Probabilistic topic models assume words are generated from latent topics which can be inferred from word co-occurrence patterns taking a document as global context. In recent years, various neural topic models have been proposed. Some of them are built on the Variational Auto-Encoder   which utilizes deep neural networks to approximate the intractable posterior distribution of observed words given latent topics . However, these models take the bag-of-words  representation of a given document as the input to the VAE and aim to learn hidden topics that can be used to reconstruct the original document. They do not learn word embeddings concurrently.  Other topic modeling approaches explore the pre-trained word embeddings for the extraction of more semantically coherent topics since word embeddings capture syntactic and semantic regularities by encoding the local context of word co-occurrence patterns. For example, the topic-word generation process in the traditional topic models can be replaced by generating word embeddings given latent topics  or by a two-component mixture of a Dirichlet multinomial component and a word embedding component . Alternatively, the information derived from word embeddings can be used to promote semantically-related words in the Polya Urn sampling process of topic models  or generate topic hierarchies . However, all these models use pre-trained word embeddings and do not learn word embeddings jointly with topics.  While word embeddings could improve the topic modeling results, but conversely, the topic information could also benefit word embedding learning. Early word embedding learning methods learn a mapping function to project a word to a single vector in an embedding space. Such one-to-one mapping cannot deal with word polysemy, as a word could have multiple meanings depending on its context. For example, the word `' has two possible meanings `' and `'. When analyzing reviews about restaurants and health services, the semantic meaning of `' could be inferred depending on which topic it is associated with. One  solution is to first extract topics using the standard Latent Dirichlet Allocation  model and then incorporate the topical information into word embedding learning by treating each topic as a pseudo-word .   Whereas the aforementioned approaches adopt a two-step process, by either using pre-trained word embeddings to improve the topic extraction results in topic modeling, or incorporating topics extracted using a standard topic model into word embedding learning,  developed a Skip-Gram based model to jointly learn topics and word embeddings based on the Probabilistic Latent Semantic Analysis , where each word is associated with two matrices rather than a vector to induce topic-dependent embeddings. This is a rather cumbersome setup.  used the Skip-Gram to imitate the probabilistic topic model that each word is represented as an importance vector over topics for context generation.  In this paper we propose a neural generative model built on VAE, called the Joint Topic Word-embedding  model, for jointly learning topics and topic-specific word embeddings. More concretely, we introduce topics as tangible parameters that are shared across all the context windows. We assume that the pivot word is generated by the hidden semantics encoding the local context where it occurred. Then the hidden semantics is transformed to a topical distribution taking into account the global topics, and this enables the generation of context words. Our rationale is that the context words are generated by the hidden semantics of the pivot word together with a global topic matrix, which captures the notion that the word has multiple meanings that should be shared across the corpus. We are thus able to learn topics and generate topic-dependent word embeddings jointly. The results of our model also allow the visualization of word semantics because topics can be visualized via the top words and words can be encoded as distributions over the topics\footnote{Our source code is made available at \url{http://github.com/somethingx02/topical\_wordvec\_models}.}.  In summary, our contribution is three-fold: [noitemsep]         
"," We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents, a word is generated by a hidden semantic vector encoding its contextual semantic meaning, and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared with existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification.",145
" 	 	Relational databases are prevalent in many real-world applications. Typically, a structured query language such as SQL is required to interact with such databases. However, mastering SQL queries is generally difficult. It has been a long standing goal to enable users interacting with relational databases via human natural languages. The general problem was known as ``Natural Language Interface to Databases~'' in database areas~. In recent years, there has been a surge of interest on deep learning-based approaches to a crucial problem of NLIDBs: translating a natural language query to SQL, often referenced as ``NL-to-SQL'' or ``Text-to-SQL''~.  	 	In this paper, we focus on the Text-to-SQL problem and experiment with the WikiSQL~~ dataset~. WikiSQL is the first large-scale dataset for Text-to-SQL, with about 80K human annotated pairs of NL question and SQL query. WikiSQL constrained the problem by two factors -- each question is only addressed by a single table, and the table is known. This constrained setting has guided research to focus on the core elementary problem. Even though the scope is constrained, the dataset is still very challenging because the tables and questions are very diverse. Notably, there are about 24K different tables associated with this dataset. 	 	In the past, several approaches were proposed for the WikiSQL dataset. They primarily share the similar encoder-decoder architecture. The encoder encodes information from both the NL question and the table schema into some hidden representation. Some of them encode an NL question with the full table schema, e.g., concatenating the NL question with all the column names~, while others encode an NL question with each column separately~. There are also some work do both at different layers~. The decoder decodes the hidden representation to a SQL query. Some early work tried the ``to sequence'' style one step decoding~ however it is found challenging to guarantee the output correct in syntax. Later more work breaks down a SQL query to several parts like SELECT column, WHERE column, WHERE operator, WHERE value, etc. and have sub-decoders for them~. In this way, the chance of output violating syntax is reduced. Beyond improving the Text-to-SQL task models, recently there are some work study how to leverage pre-trained language models~ and get promising results~. All the previous work reveal several major challenges in Text-to-SQL on WikiSQL:  How to fuse information from both NL question and table schema, which is handled by encoder;  How to make sure the output SQL query executable and accurate, which is handled by decoder;  How to leverage pre-trained language models. 	 	This paper is primarily motivated by the above challenge , however, the proposed approach contributes to solving all the above challenges. We argue that the previous approaches~ did not align the task model well with the base language model, and hence, the power of the base language model is compromised by the task model. Specifically, both  and  concatenate the full table schema with the NL question and feed into a BERT-alike base language model. However, in their decoding stage, both need the hidden representation for each individual column. Thus, they have to apply adhoc pooling on the tokens of a column name to get a vector for the column.  simply pick the vector for the first token in their ``shallow layer'' and added another LSTM layers in their ``decoder-layer'' and ``NL2SQ-Layer''.  apply weighted average over vectors of all tokens. These ad-hoc pooling operations and additional LSTM layers caused information loss and bring in unnecessary complexity. To solve the dilemma encountered in previous work, in this paper, we choose to encode a pair of NL question and one individual column via the exact BERT/RoBERTa model structure. Then we have multiple sub-tasks in decoder stage: SELECT \& WHERE column ranking, condition operator, and condition value span. Since the decoder does not output final SQL query, we apply straightforward rules to assemble the decoder outputs to a final SQL query. Our approach has two benefits. First, the inputs, i.e., a question and column pair align perfectly with the original sentence pair training tasks of BERT/RoBERTa, and hence, we believe the power of the BERT/RoBERTa is utilized best. Second, as we only encode one column, the ``[CLS]'' vector in BERT/RoBERTa output captures all fused information from the question and the column, which is exactly the ``column vector'' needed for the decoder. So we don't need to apply any further pooling or additional complex layers. This makes our model structure very simple and efficient. Since the main philosophy in our approach is ranking on columns but with multi-task outputs, we name it Hybrid Ranking Network . 	 	In summary, our contributions are in three folds. First, we propose a simple and efficient network structure which perfectly aligns the Text-to-SQL task with the base language models, and hence, the power of the base language models are best utilized. Second, the base language model as encoder directly encodes an NL question and a column without any additional pooling operations which is believed to be the best encoder capturing the question-column relation in Text-to-SQL. Third, the proposed hybrid ranking mechanism and execution-guided decoding handle column-column relations and effectively boost the accuracy. In other words, our approach resolved all the aforementioned challenges at the same time. Our approach achieves the best result on WikiSQL dataset, which verifies its effectiveness and contributions. 	 	 	Note that the final point above is not trivial, once we introduce a complex one-to-one mapping between Spider SQL query  and HydraNet tasks. This is significantly different from other grammar-based approach, and will also be different from slot filling approach as there will be more ranking involved.  	 	 	 	There is no doubt that state-of-the-art models for text-to-SQL task must leverage pre-trained deep Transformer models like BERT, MT-DNN or RoBERTa. However, the above input construction does not fit the BERT famework very well due to the following reasons:  	 	 	 	We argue that, in the WikiSQL task, the columm-column interdependency  is quite rare. So, the main relationship to learn is that between the columns and the NL question. So, we propose a two stage approach in this paper:\\ 	 In the first stage, we learn the column-question relationships. We consider one column at a time, form the input as question-column pair and train a model to classify whether a column is a SELECT or WHERE column. \\ 	 In the second stage, we keep the most relevant columns from the first step via ranking and use execution-guided decoding  	to construct the SQL query. Execution guided decoding takes the necessary column-column relationships into account.  	 	Our approach addresses the issues as  it leverages BERT with recommended tasks setting and input setting and  the input length does not grow with number of candidate columns, hence it is able to scale to even large databases. 	 	 	 	 	: The most recent approaches  leverage pre-trained deep Transformer models like BERT and MT-DNN which has proven extremely effective for most tasks involving unstructured data. These approaches first perform a table-aware BERT-based or MT-DNN-based encoding. Then they have different slot filling models for  and . 	 	 	There is no doubt that state-of-the-art models for text-to-SQL task must leverage pre-trained deep Transformer models like BERT, MT-DNN or RoBERTa. However, both the previous approaches that leverage pre-trained contextualized word representation concatenate all the column names of the table along with the NL question in the encoding layer in order to learn the interdependencies between columns in SELECT clause and WHERE conditions as well as columns in different WHERE conditions . This has several serious limitations: 	\\ 	 $ 	 	
"," 		In this paper, we study how to leverage pre-trained language models in Text-to-SQL. We argue that previous approaches under utilize the base language models by concatenating all columns together with the NL question and feeding them into the base language model in the encoding stage. We propose a neat approach called Hybrid Ranking Network~ which breaks down the problem into column-wise ranking and decoding and finally assembles the column-wise outputs into a SQL query by straightforward rules. In this approach, the encoder is given a NL question and one individual column, which perfectly aligns with the original tasks BERT/RoBERTa is trained on, and hence we avoid any ad-hoc pooling or additional encoding layers which are necessary in prior approaches. Experiments on the WikiSQL dataset show that the proposed approach is very effective, achieving the top place on the leaderboard.",146
"   } NVIDIA is working on significant performance improvements for \sockeye's Transformer  implementation through fused operators and an optimized beam search. This paper discusses \sockeyeTwo's streamlined \gluon implementation , support for state of the art architectures and efficient decoding , and improved model training .  
","   We present \sockeyeTwo, a modernized and streamlined version of the \sockeye neural machine translation  toolkit.   New features include a simplified code base through the use of \mxnet's \gluon API, a focus on state of the art model architectures, distributed mixed precision training, and efficient CPU decoding with 8-bit quantization.   These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production.",147
"}  {D}{eep} learning models have revolutionized multiple fields of information systems including natural language processing , computer vision, and speech analysis. While they have enabled multiple tasks to attain very high accuracy values, model sizes and prediction latencies have increased significantly as well. Specific to text, Recurrent neural networks , Gated Recurrent Units  and long short term memory  networks have been used for quite some time for various natural language processing  tasks. These models are large especially because of the input and output embedding parameters. %For example, for the  ClueWeb dataset, the vocabulary contains over 10M words. If the embedding vectors are of 1024 dimensions and each dimension is represented using 32 bits, the size of the input-embedding matrix will be ^,\sim$5\% of the weights, it is possible to predict the remaining weights without any drop in accuracy. This observation led to a large number of research efforts across multiple communities on compression of large deep learning models. In this survey, we aim to systematically explore this large body of literature in the NLP community by organizing it into various categories. Figure shows a broad organization of model compression methods for text. %The remainder of the survey is organized as follows. We provide a broad overview of popular approaches for model compression in Section. Further, we delve deeper into compression methods using pruning, quantization, knowledge distillation, parameter sharing, tensor decomposition and linear Transformers in Sections to. In Section, we present benchmark text use cases where the model compression methods have shown immense gains. Finally, we conclude the survey with a summary and a discussion on future trends in Section.  In this survey we do not focus on specific methods proposed in other communities like vision and speech only and which make use of image/audio specific architectures and hence cannot be applied to text. Also, we do not discuss methods on hardware optimizations to reduce latency. While there are other surveys in the broad area of model compression, they are either old or focus on computer vision related problems.   
"," In recent years, the fields of natural language processing  and information retrieval  have made tremendous progress thanks to deep learning models like Recurrent Neural Networks , Gated Recurrent Units  and Long Short-Term Memory  networks, and Transformer~ based models like Bidirectional Encoder Representations from Transformers ~. %, Generative Pre-training Transformer ~, Multi-task Deep Neural Network ~, Extra-Long Network ~, Text-to-text transfer transformer ~, etc.  But these models are humongous in size. %: BERT~ , GPT-2~ , T5~ . On the other hand, real world applications demand small model size, low response times and low computational power wattage. In this survey, we discuss six different types of methods  for compression of such models to enable their deployment in real industry NLP  projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this survey organizes the plethora of work done by the `deep learning for NLP' community in the past few years and presents it as a coherent story.",148
"  Nature language understanding plays an critical role in machine intelligence and it includes many challenging NLP tasks such as reading comprehension , machine translation , question answering  and etc.. Amongst a wide spectrum of NLP tasks, text classification is considered as the foundation for its measuring the semantic similarities between texts. Traditional machine learning methods employ hand-crafted features to model the statistical properties of syntactical elements , which are further fed into the classification algorithms such as k-Nearest Neighbor , Random Forests, Support Vector Machines , or its probabilistic versions . However, such hand-crafted features often suffered from the loss of semantic information and scalability. To solve the drawbacks of the hand-crafted features, automatic learning of representation using the neural networks was introduced into NLP fields. Word embedding is a foretype of automatic representation learning , which outperforms the traditional methods for alleviating the sparsity problem and enhancing the semantic representation.  In recent years, the NLP community has conducted extensive investigations on the neural-based approaches . There exist a diversity of deep neural network architectures with different modeling capabilities. The RNN is a widely-used neural network architecture for NLP tasks owing to its capability to model sequences with long-term dependencies . When modeling texts, a RNN sequentially processes word by word and generates a hidden state at each time step to represent all previous words. However, although the purpose of RNNs is to capture the long-term dependencies, theoretical and empirical studies have revealed that it is difficult for RNNs to learn very long-term information. To address this problem, the long short-term memory network   and other variants such as gated recurrent unit  , simple recurrent unit   were proposed for better remembering and memory accesses. Another roadblock for RNNs is that when they are used to process a long sequence, the latest information is more dominant than the earlier one, however, which might be the real significant part of the sequence. In fact, the most important information can appear anywhere in a sequence rather than at the end. Consequently, some researchers proposed to assign the same weight to all hidden states and average the hidden states of all time steps to equally spread the focus to all the sequence.  Inspired by the biological ability to focus on the most important information and ignore the irrelevant ones, the attention mechanism was introduced to assign different weights to the elements at different positions in a sequence and select the informative ones for the downstream tasks . Nowadays, the attention mechanism has become an integral part of sequence modeling, especially with RNNs . The attention mechanism enables RNNs to maintain a variable-length memory and compute the outputs based on the importance weights of different parts in a sequence. The attention mechanism has been empirically proven to be effective in NLP tasks such as neural machine translation . However, the attention mechanism cannot capture the relationships between words and the word ordering information, which contains important semantic information for downstream tasks. Taking the sentences ``Tina likes Bob."" and ``Bob likes Tina."" as examples, the weighted sum of their hidden states encoded by RNN are almost the same. Nevertheless, the two sentences have different meanings.  %Nowadays, it has become the mainstream for NLP tasks to learn distributed representations of words through neural language models and perform a combination of learned word representation with attention mechanism .  The ConvNet is another widely-adopted neural architecture for NLP tasks. The modeling power of ConvNets relies on four key factors: local connections, shared weight, pooling and multi-layers. The fundamental assumption behind the ConvNet approaches is that locally grouped data in natural signals are often high correlated and the compositional hierarchies in natural signals can be exploited by the stacked convolutional layers. As a result, ConvNets have been believed to be good at extracting informative semantic representations from the salient N-gram features of input word sequences by utilizing convolutional filters in a parallel way. For the above example, 2-gram features of `` Tina likes"" and ``likes Bob"" that contain the word ordering information can be captured by ConvNets. These features are more representative for the original sentence than the weighted sum of the hidden states. Therefore, ConvNets have been employed for a variety of NLP tasks and achieved impressive results in sentence modeling , semantic parsing , and text classification . Moreover, ConvNets can operate on different levels of lexical structures such as characters, words, sentences, or even the whole document. For instance, some research has shown that the character-level text classification using ConvNets can achieve competitive results in comparison with the state-of-the-arts . However, basic ConvNets apply a fixed-width window to slide over the input sequences, which limits the created representations to local semantic pattern, failing to capture long-term dependencies.  To take full advantage of both the ConvNet and the RNN, and complement the superiorities of different neural architectures, researchers explored to introduce the hybrid structure of the ConvNets and the RNNs. For instance, the recurrent convolutional neural network  proposed a recurrent structure of convolutional filters to enhance the contextual modeling ability to avoid the problem of fixed-width sliding windows. This work also claimed to apply a max-pooling layer to automatically determine the key components for text classification. However, even though this approach managed to reduce noise by replacing the fixed-width sliding window of ConvNets with a recurrent mechanism, it still depend on the max-pooling to determine the discriminative features and lacks the mechanism to selectively choose the dominant component as the attention mechanism can do. Similarly, Wang et al. proposed the convolutional recurrent neural network that stacked four types of neural layers: word embedding, Bidirectional RNN layer, convolutional layer, and max-pooling layer. This approach functions very similarly to the one in , but with disparate applications in sentence classification and answer selection. Also, this work bypassed the attention mechanism when integrating the ConvNet and the RNN structures.  As discussed above, any neural architecture has its own pros and cons, it is reasonable to conjecture that consistently combing different architectures can benefit extracting of different aspects of linguistic information from texts. However, to the best of our knowledge, there are still no efforts in integrating entirely the ConvNet, RNN and attention architectures. Inspired by proposition by LeCun et al. , we hypothesize that the attention mechanism can function as an adhesive that seamlessly integrate the ConvNet and the RNN architecture, where the RNN layer is used to represent the input word sequences and the ConvNet layer is used for classification. Furthermore, we assume that, besides attending to elements  at syntactical or symbolic level, coarser-grained attentions at the hidden state vectorial space can improve the local N-gram coherence for ConvNets, as the attentions on hidden state vectors can select the salient dimensions that represent most informative latent semantics, hence reducing the noise perturbation to the ConvNet layer and enhancing the classification performance.  Based on the above motivations, we propose a hybrid architecture based on a novel hierarchical multi-granularity attention mechanism, named Multi-granularity Attention-based Hybrid Neural Network . In MahNN, two types of attentions are introduced: the syntactical attention and the semantical attention. The syntactical attention computes the importance of the syntactic elements  at the lower symbolic level and the semantical attention is used to compute the importance of the embedded space dimension corresponding to the upper latent semantics. We adopt the text classification as an exemplifying way to illustrate the ability of MahNN to understand texts. The experimental results on a variety of datasets demonstrate that MahNN outperforms most of the state-of-the-arts for text classification.    The main contributions of our work are listed as follows:         This paper is organized as follows. Section  introduces the related work about ConvNet and attention mechanisms. Section  introduces the proposed MahNN in detail. And Section  introduces datasets, baselines, experiments, and analysis. Finally, Section  concludes this paper.  
"," Neural network-based approaches have become the driven forces for Natural Language Processing  tasks. Conventionally, there are two mainstream neural architectures for NLP tasks: the recurrent neural network  and the convolution neural network . RNNs are good at modeling long-term dependencies over input texts, but preclude parallel computation. ConvNets do not have memory capability and it has to model sequential data as un-ordered features. Therefore, ConvNets fail to learn sequential dependencies over the input texts, but it is able to carry out high-efficient parallel computation. As each neural architecture, such as RNN and ConvNets, has its own pro and con, integration of different architectures is assumed to be able to enrich the semantic representation of texts, thus enhance the performance of NLP tasks. However, few investigation explores the reconciliation of these seemingly incompatible architectures. To address this issue, we propose a hybrid architecture based on a novel hierarchical multi-granularity attention mechanism, named Multi-granularity Attention-based Hybrid Neural Network . The attention mechanism is to assign different weights to different parts of the input sequence to increase the computation efficiency and performance of neural models. In MahNN, two types of attentions are introduced: the syntactical attention and the semantical attention. The syntactical attention computes the importance of the syntactic elements  at the lower symbolic level and the semantical attention is used to compute the importance of the embedded space dimension corresponding to the upper latent semantics. We adopt the text classification as an exemplifying way to illustrate the ability of MahNN to understand texts. The experimental results on a variety of datasets demonstrate that MahNN outperforms most of the state-of-the-arts for text classification.",149
" In natural language processing, pre-trained contextual representations have been widely used to help downstream tasks that lack sufficient labeled training data.  Previous works develop various self-supervised tasks to obtain pre-trained contextual representations. Taking the classic masked language modeling  task used by BERT  as an example, it first randomly chooses a small number of positions in a sentence, mask the words on the position and then learns an encoder to restore them. As such tasks require no human supervision, the size of available training data could easily amount to the scale of billions of words. Pre-training over such large-scale data consumes exceptionally huge computational resources .   In this paper, we tackle the training efficiency issue and develop a novel variance-reduced algorithm for better language pretraining. In particular, we observe that all previous works use uniformly sampled positions to mask when constructing their self-supervised tasks, and this is inevitably inefficient from the optimization perspective. For instance, in BERT training, we find that commonly used words and punctuation are easy to learn, i.e., those words  can be correctly predicted by the model in just a few thousands of training steps. Meanwhile, some rare words and phrases are difficult to predict even at the end of the training. If we always uniformly sample the positions to mask, intuitively to say,  the variance of the stochastic gradient  can be large since some positions gradually provide less informative signals while some do not. Usually, learning with a large-variance gradient estimator will be inefficient and ineffective.   To formally characterize the variance of the stochastic gradient, we first introduce a principled gradient variance decomposition theorem. The theorem shows that the gradient variance can be naturally decomposed into two parts. One part concerns about the variance of sentences sampled in a batch, and the other part concerns about the variance of the masked positions. Our focus is on the variance reduction of the second part. Importance sampling is a standard way for variance reduction, which suggests that we can sample the masks using a proposal distribution instead of the uniform distribution. According to the theory, the variance is minimized if the probability of a mask sampled from the proposal distribution is proportional to the gradient norm. However, this brings the chicken-egg problem: We will never know the gradient norm unless we mask the positions, feed the masked sentence to the model and back-propagate the loss. As the number of possible masks is huge, feeding all the possibilities to the network to obtain their gradients is time expensive which significantly slows down the training process.  To address this challenge, we introduce a meta-learning approach by introducing a MAsk Proposal Network  which takes the whole sentence as input and outputs a probability distribution over positions to sample masks. Both MAP-Net and the pretraining model are jointly optimized in an adversarial manner. Given a masked sentence sampled from the MAP-Net, the model is optimized to recover the masked sentence. At the same time, the MAP-Net receives signals from the performance of the model on this masked sentence, and improve itself. Instead of using reinforcement learning, we decouple the learning objective and make the training of the MAP-Net easier. We show that for language generation tasks, we can use the value of the loss instead of the value of the gradient norm. Therefore, the goal of the MAP-Net is to find ``tough'' masked positions with high losses to challenge the model, while the model attempts to fulfill the tasks generated by the MAP-Net. As we obtain the loss of many masked positions, the MAP-Net can be efficiently optimized from the pair-wise preference of different positions.    To demonstrate the advantage of our proposed method, we conduct several experiments by using the MAP-Net to help the training of BERT, and evaluate them over GLUE natural language understanding benchmark . Experiment results first indicate that the masked words generated by MAP-NET are meaningful and informative during training. Furthermore, as the variance is sufficiently reduced, the BERT model learned with MAP-Net achieves better accuracy than the baselines on most of the tasks.    [t]              
"," Self-supervised learning, a.k.a., pretraining, is important in natural language processing. Most of the pretraining methods first randomly mask some positions in a sentence and then train a model to recover the tokens at the masked positions. In such a way, the model can be trained without human labeling, and the massive data can be used with billion parameters. Therefore, the optimization efficiency becomes critical.  In this paper, we tackle the problem from the view of gradient variance reduction. In particular, we first propose a principled gradient variance decomposition theorem, which shows that the variance of the stochastic gradient of the language pretraining can be naturally decomposed into two terms: the variance that arises from the sample of data in a batch, and the variance that arises from the sampling of the mask. The second term is the key difference between self-supervised learning and supervised learning, which makes the pretraining slower. In order to reduce the variance of the second part, we leverage the importance sampling strategy, which aims at sampling the masks according to a proposal distribution instead of the uniform distribution. It can be shown that if the proposal distribution is proportional to the gradient norm, the variance of the sampling is reduced. To improve efficiency, we introduced a MAsk Proposal Network , which approximates the optimal mask proposal distribution and is trained end-to-end along with the model. According to the experimental result, our model converges much faster and achieves higher performance than the baseline BERT model.",150
" Written Chinese has no explicit word boundary, so Chinese word segmentation  serves as an upstream disambiguation step for Chinese language processing. The task is often viewed as sequence labeling, where each character receives a label indicating its relative position in a segmented sentence. While traditional machine learning methods have attained strong results, recent research focuses on neural networks.  first treat CWS as neural machine translation . Nonetheless,  point out that without extra resources, all previous neural methods are not yet comparable with the non-neural state of the art  from , and the NMT method is even behind.  We note two advantages of NMT: the entire sentence is encoded before making any decision, and the model jointly trains character embeddings with sequence modeling. Thus, we try to bridge the gap between the NMT approach and SOTA, using low-resource techniques such as regularization and data augmentation. Then, we explore more techniques commonly seen in NMT. The translation-based method is easy to adopt without the hassle of feature engineering and model design. In the constrained test condition, our method reaches the top on the MSR dataset and achieves a strong result on the PKU dataset without tuning. As a generic approach it can also be used for other languages.   
"," % Supervised Chinese word segmentation has been widely approached as sequence labeling or sequence modeling. Recently, some researchers attempted to treat it as character-level translation, but there is still a performance gap between the translation-based approach and other methods. In this work, we apply the best practices from low-resource neural machine translation to Chinese word segmentation. We build encoder-decoder models with attention, and examine a series of techniques including regularization, data augmentation, objective weighting, transfer learning and ensembling. When benchmarked on MSR corpus under closed test condition without additional data, our method achieves 97.6\% F1, which is on a par with the state of the art and remarkably better than previous translation-based methods. Supervised Chinese word segmentation has entered the deep learning era which reduces the hassle of feature engineering. Recently, some researchers attempted to treat it as character-level translation which further simplified model designing and building, but there is still a performance gap between the translation-based approach and other methods. In this work, we apply the best practices from low-resource neural machine translation to Chinese word segmentation. We build encoder-decoder models with attention, and examine a series of techniques including regularization, data augmentation, objective weighting, transfer learning and ensembling. Our method is generic for word segmentation, without the need for feature engineering or model implementation. In the closed test with constrained data, our method ties with the state of the art on the MSR dataset and is comparable to other methods on the PKU dataset.",151
" In the era of mobile reading, a lot of self-media platforms based on the user-generated-content mode have emerged. People are accustomed to spending fragmented time reading online articles published on self-media platforms to conveniently get information and knowledge via mobile devices. Different from traditional documents such as essays, academic papers or Wikipedia documents, self-media online articles have more diverse multimedia elements, in addition to text, usually existing pictures, videos, etc. The organization of these elements jointly affects users' perception. Besides, since the creation forms of self-media online articles are more free, these articles do not have a unified format and layout, and usually vary in diverse categories, styles and content. Therefore, it is necessary to integrate different multimedia elements more comprehensively to jointly process self-media online articles.  The openness of self-media platforms, where each user can be a producer, however, results in uneven quality of online articles. Assessing the quality of self-media online articles is a critical issue for many applications such as recommender systems and online search to find high-quality articles and filter low-quality articles. It is very helpful to increase user stickiness to propose an efficient solution for the automatic evaluation of the self-media online article quality. Considering the nature of self-media platforms, in order to engage users, the quality of self-media online articles is reasonably defined as the level of the reading experience that articles give users. This can be reflected in the article's content, writing norms, user perception, etc., and each factor also contains complicated elements, making the self-media online article quality assessment a much more complex and challenging task. The following question lies to be addressed: How to establish a unified framework to effectively solve the multivariate representation learning of self-media online article quality? However, current studies on the document quality assessment mainly focus on textual features . To the best of our knowledge, no prior work has studied the automatic self-media online article quality assessment.   %  % 
","   The automatic quality assessment of self-media online articles is an urgent and new issue, which is of great value to the online recommendation and search. Different from traditional and well-formed articles, self-media online articles are mainly created by users, which have the appearance characteristics of different text levels and multi-modal hybrid editing, along with the potential characteristics of diverse content, different styles, large semantic spans and good interactive experience requirements. To solve these challenges, we establish a joint model CoQAN in combination with the layout organization, writing characteristics and text semantics, designing different representation learning subnetworks, especially for the feature learning process and interactive reading habits on mobile terminals. It is more consistent with the cognitive style of expressing an expert's evaluation of articles. We have also constructed a large scale real-world assessment dataset. Extensive experimental results show that the proposed framework significantly outperforms state-of-the-art methods, and effectively learns and integrates different factors of the online article quality assessment.",152
" Dialogue state modules are a central component to a task-oriented dialogue system. Given a user utterance and existing dialogue history, a dialogue system typically extracts dialogue states, according to which a system response is generated. An example is shown in Figure, given two turns of a dialogue, the first user utterance is ``I want an expensive restaurant that serves Turkish food.'', and the dialogue states consist of the slot-value pairs . As the dialogue proceeds, the dialogue state is updated at each turn. After tow dialogue turns, the dialogue state becomes , where inform represents the search constraints expressed by user and request represents the search target that the user is asking for. In this example, the user intention is to reserve a restaurant. The business domain is restaurant customer service. The dialogue state represents what the user is looking for at the current turn of the conversation.   % While traditional dialogue state tracking modules extract user intention and then slot values in a pipeline [], recent work performances end-to-end DST by extracting slot-value pairs only. {, which contains more than three million tweets and replies between customer support agents of some big companies and their customers and the Ubuntu dialogue corpus . %In such occasions, to gather dialogue state information solely based on conversation utterances is more realistic.     To address this issue, it can be useful to automatically induce dialogue states from raw dialogues. We assume that there is a large set of dialogue records of many different domains, but without manual labeling of dialogue states. Such data are relatively easy to obtain, for example from customer service call records from different businesses. Consequently, we propose the task of { is regarded as the slot and area is regarded as the value. During training, DST relies on both a dialogue record and manual labeling of slot-value pairs on the dialogues. In contrast, our task does not rely on manual labeling and can generate slot-value pairs over raw dialogues automatically.  % During testing, existing methods rely on an ontology and a trained model to label slot values on dialogues, while our method does not rely on the ontology. {  % Both models are trained with variational inference.  We introduce two neural latent variable models for DSI by treating the whole state and each slot as latent variables, from which values observed in dialogue data are generated. The goal is to induce slots according to those frequently co-occurring values and the dialogue contexts. In particular, each value  is represented by using both a sparse one-hot representation and a dense contextualized embedding representation. Both models are generative probabilistic models, which generate a value by first generating a latent dialogue state vector, and then generating a slot. The difference between the two models is the modeling of service domains. We observe that different service domains may contain slots with similar contexts or values. For example, both { domains can have the same slot {. We refer to the basic model {.  % We build an unsupervised generative model for DSI. {.  % We want highlight four points:  our model can solve the unknown slot values problem and the multi-domain problem.  our model can effectively reduce the Inference Time Complexity  since it is not constrained by the number of slots and values in a pre-defined ontology list.   although we do not use external knowledge in our unsupervised model, it can be easily extended from external sources like slot values from backend  or SLU system outputs.  Annotated training data can be easily added so that our models become semi-supervised.  % { and , we construct a slot value candidate set given the dialogue history until certain turn. We build a model ... % unknown slot values People tend to solve some small but realistic research problems in this area. This is the infinite slot value problem. To summarize related work aimed to solve this problem. One is the ACl paper from Chumenwenwen, many papers are aimed to solve this, but maybe only those top conference papers can be selected and shown. % Multi-domain Infinite slot value problem can be useful for solving the realistic application problem based on current researches but is not enough. Schema-Guided Dialogue State Tracking has been proposed to further solve this problem.  This can be applied to similar domain DST, however, for complete new domains, large semantic barrier can hinder its application. Besides, its additional annotation of sentence description can also be expensive and its quality can affect the results obviously. some other related work % However, it is still far away from realistic application. The origin of these problems lie on the annotation difficulty and its cost. Thus, task-oriented dialogue corpus is limited in its scale compared to open-domain dialogue corpus. Thus, most existing corpus are based on a small ontology, which is still far away from real world user cases. % comparable results compared to supervised methods. % Useful for response generation. For the SOTA models without dialogue state, adding our DSI results can get improvements. % Existing task-oriented dialogue corpus domains include restaurant, flight, etc. Our method is especially helpful for those totally new domains, like online shopping client service, which have plenty of dialogue records, but a huge number of slots and values make it even impossible to annotate. why not few shot learning, maybe still difficult to split into categories and annotate. And our methods are based on large amount of dialogues to find their similarity.
"," Dialogue state modules are a useful component in a task-oriented dialogue system. Traditional methods find dialogue states by manually labeling training corpora, upon which neural models are trained. However, the labeling process can be costly, slow, error-prone, and more importantly, cannot cover the vast range of domains in real-world dialogues for customer service. We propose the task of dialogue state induction, building two neural latent variable models that mine dialogue states automatically from unlabeled customer service dialogue records. Results show that the models can effectively find meaningful dialogue states. In addition, equipped with induced dialogue states, a state-of-the-art dialogue system gives better performance compared with not using a dialogue state module.",153
"   Spoken dialog systems such as those utilized in voice assistants such as Alexa, Siri and Google Home, typically consists of a sequential chain of sub-systems, including Spoken Language Understanding , Dialog Management, Natural Language Generation and Text-to-Speech. Generally, these sub-systems perform cloud-based processing of speech, following on-device wakeword detection. First, the SLU system extracts natural language semantics such as utterance intent as well as associated named entities or slot values from the speech segment. The appropriate application is then invoked for further execution and finally responses are processed by a text-to-speech system and relayed to the user. An example of intents and slots for an utterance is in Table.\ . Conventionally, the SLU system comprises two distinct stages:  An Automatic Speech Recognition  system obtains the transcript or a text representation of the raw audio segment,  A Natural Language Understanding  system subsequently consumes the transcript or alternatively n-best hypotheses of the ASR system and extracts semantics, in particular, the domain, intent and slots.  In this work, we consider neural end-to-end  SLU models that produce semantics of intents and slots from audio. A primary motivation for the work arises from deployment of SLU systems to devices that are more resource limited than cloud servers. For such devices, a neural E2E SLU model can be customized under the given resource constraints to satisfy a limited set of intents or use cases , and deployed. Moving SLU computation from cloud to devices allows  offline use, e.g. in cars or emergency situations  latency gains from placing computations closer to the user  cost and carbon savings from reduced fleet sizes and reducing communication payloads.   E2E SLU provides an alternative paradigm to the conventional approach of compressing individual components of the ASR or NLU systems to satisfy on-device resource constraints. In the latter approach, the NLU subsystem is not exposed to audio information such as prosody, or ambiguity in the ASR decoding beyond n-best hypotheses. Errors from the ASR system cascade down to NLU tasks. Our approach to developing E2E SLU systems leverages models developed in ASR and NLU communities by replacing the text interface between them with a neural network hidden interface layer. We term this interpretable subclass of E2E SLU models Joint SLU models that produce intermediate transcript as well as NLU annotations. We show that NLU metrics improve with exposure to this richer interface and also that ASR metrics improve from NLU feedback with joint training. The multitask training of these models can make use of datasets with only transcribed audio as well as audio with NLU annotations.     {| p{0.2\linewidth} | p{0.7\linewidth} |}  an|Other alarm|NotificationType for|Other six|Time a.m.|Time \\ \hline Slots & NotificationType - alarm, Time - six a.m. \\ \hline      [h]                    A few prior works have considered the E2E SLU problem. In the work from Google , authors first develop the problem and note that having an intermediate text representation improves performance. They consider encoder-decoder sequence networks to predict transcript and a serialized form of the semantics in a multitask model where decoders are separate, a two-stage model where the transcript is obtained first, and a joint model where a single decoder predicts both jointly. In contrast, we formalize distinctions between ASR and NLU subsystems and study the impact of end-to-end training, and interfaces such as text or hidden layers between the two. In , a CTC based network is used to extract named entities from French speech while we use attention based networks and train on larger corpora. Finally,  are works that obtain a single label  directly from speech segments. Our work goes beyond this and performs slot filling as well.   % TODO: is such a detailed description of E2E ASR systems needed?  E2E SLU models are similar to other multitask speech systems such as speech translation systems or multilingual ASR systems. This work has been enabled by advancements in E2E ASR, where such systems have been shown to outperform conventional RNN-HMM hybrid ASR systems  when trained on large acoustic datasets. Connectionist Temporal Classification networks  was the first all neural E2E ASR model that trained a Recurrent Neural Network  on audio input features with a transcript label sequence of a different length by considering all possible alignments between inputs and labels. In Recurrent Neural Network-Transducer , authors extend CTC by also modelling interdependencies between input-output and output-output distributions using an added prediction network. In both cases, an efficient forward-backward computation enables loss computation and backpropagation over all alignments. In contrast to the aforementioned streaming architectures, in attention based sequence-to-sequence networks such as Listen Attend Spell  , input features are processed by encoder networks that produce a hidden representation output for each feature. The decoder estimates an element of the label sequence at each step using an attention network to focus on a fraction of the encoder network outputs.  Extracting intent and slots from transcript is a long running problem in NLU . In survey , authors compare DNN and earlier feature engineering approach coupled with conditional random fields or softmax layers for the purpose of named entity recognition. The interface between ASR and NLU systems has traditionally been the best hypothesis sequence although richer interfaces such as lattices and word confusion networks have also been well studied . In this work, we develop a simple joint intent and slot prediction network and study the impact of text vs hidden layer interfaces between ASR and NLU.       In Sec.\ , we first present a low-resource streaming model that extracts utterance intent directly from speech without intermediate text output. We then present a compositional model that is similar to a non-streaming pipelined two-stage SLU architecture, where a LAS based ASR system produces a transcript which is then consumed by an independently trained Neural NLU system. Finally, we present the aforementioned E2E differentiable Joint SLU models where the interface between ASR and NLU is a shared hidden layer. We restrict ourselves to 1-best interfaces between ASR and NLU and leave n-best hidden layer interfaces to future work.   We present experimental results, baselines, and metrics on a variety of datasets in Sec.\  and answer the following questions: [leftmargin=*]     
"," We consider the problem of spoken language understanding  of extracting natural language intents and associated slot arguments or named entities from speech that is primarily directed at voice assistants. Such a system subsumes both automatic speech recognition  as well as natural language understanding . An end-to-end joint SLU model can be built to a required specification opening up the opportunity to deploy on hardware constrained scenarios like devices enabling voice assistants to work offline, in a privacy preserving manner, whilst also reducing server costs.   % removed word ""composed"" from joint system description We first present models that extract utterance intent directly from speech without intermediate text output. We then present a compositional model, which generates the transcript using the Listen Attend Spell ASR system and then extracts interpretation using a neural NLU model. Finally, we contrast these methods to a jointly trained end-to-end joint SLU model, consisting of ASR and NLU subsystems which are connected by a neural network based interface instead of text, that produces transcripts as well as NLU interpretation. We show that the jointly trained model shows improvements to ASR incorporating semantic information from NLU and also improves NLU by exposing it to ASR confusion encoded in the hidden layer.",154
" Modularized task-oriented dialogues systems are the core of the current smart speaker generation . The main modules of such systems are Natural Language Understanding , Dialogue State Tracking , Dialogue Policy  and Natural Language Generation , each of which is trained separately using supervised and/or reinforcement learning. Thus a data collection process is required, which for some of the tasks can be laborious and expensive. For example, dialogue policy annotation has to be done by an expert, better by a professional linguist. Therefore, having a model that requires only few samples to actually perform well in the tasks is essential.     The most successful approach in few-shot learning for task-oriented dialogue systems is notably transfer learning, where a large model is firstly pre-trained on a large corpus to be then fine-tuned on specific tasks. For task-oriented dialogue systems, ~ proposed {TOD-BERT} a large pre-trained model which can achieve better performance than BERT in few-shots NLU, DST and DP.  proposed a two-step classification for few-shot slot-filling, a key task for the NLU module. Similarly,  introduced a benchmark for few-shot NLG and a pre-trained language model  specialized for the task. Further, a template rewriting schema based on T5 was developed by  for few-shot NLG in two well-known datasets.  proposed a pre-trained language model  for end-to-end pipe-lined task-oriented dialogue systems. In their experiments, they showed promising few-shot learning performance in MWoZ. Finally, several meta-learning approaches have been proposed for DP, NLG/ACT, pipelined end-to-end models and personalized dialogue systems.    For performing few-shot learning, existing methods require a set of task-specific parameters since the model is fine-tuned with few samples. Differently, in this paper, we perform few-shot learning by priming LMs with few-examples. In this setting, no parameters are updated, thus allowing a single model to perform multiple tasks at the same time. In this paper, we evaluate the few-shot ability of LM priming on the four task-oriented tasks previously mentioned . Currently, GPT-3 is not available to the public; thus we experiment on different sizes GPT-2 models such as SMALL , LARGE , and XL . All the experiments are run on a single NVIDIA 1080Ti GPU.  
"," Task-oriented dialogue systems use four connected modules, namely, Natural Language Understanding , a Dialogue State Tracking , Dialogue Policy  and Natural Language Generation . A research challenge is to learn each module with the least amount of samples  given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning, where large language models, either pre-trained on text or task-specific data, are fine-tuned on the few samples. These methods require fine-tuning steps and a set of parameters for each task. Differently, language models, such as GPT-2~ and GPT-3~, allow few-shot learning by priming the model with few examples. In this paper, we evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current limitations of this approach, and we discuss the possible implication to future work.",155
" Modern search engines provide search services specialized across various domains . Users come to a search engine to look for information with different possible intents: choosing favorite restaurants, checking opening hours, or restaurant addresses on Yelp; searching for people, finding job opportunities, looking for company information on LinkedIn, etc.     Understanding the intent of a searcher is crucial to the success of search systems. Queries contain rich textual information provided explicitly by the searcher, hence a strong indicator to the searcher閳ユ獨 intent.  %For example, given a LinkedIn query that contains keywords machine learning, the user could be looking for machine learning jobs, connections with machine learning skills, or online courses to learn machine learning, etc.   Understanding the underlying searcher intent from a query, is referred to the task of query intent modeling.   Query intent is an important component in the search engine ecosystem . As shown in Figure , when the user starts typing a query, the intent is predicted based on the incomplete character sequence; when the user finishes typing the whole query, a more accurate intent is predicted based on the completed query.  Understanding the user intent accurately allows the search engine to trigger corresponding vertical searches, as well as to better rank the retrieved documents based on the intent , so that users do not have to refine their searches by explicitly navigating through the different facets in the search engine.  %For example, in Figure , query intent is identified for the typeahead blending, when the query is incomplete; when user hit          Traditional methods rely on bag-of-words representation and rule based features to perform intent classification . Recently, deep learning based models  show significant improvement, which can handle similar words/word sense disambiguation well.  However, developing deep learning based query intent models for productions requires considering several challenges.  Firstly, production models have very strict latency requirements, and the whole process needs to be finished within tens of milliseconds. Secondly, queries, usually with two or three words in a complete query or several characters in a incomplete one, have limited contexts.  % In this paper, we design models customized for the specific task.   This paper proposes a practical deep learning framework to tackle the two challenges, with the goal of improving LinkedIn's commercial search engine. Two search result blending components were identified where query intent is useful: incomplete query intent for typeahead blending, and complete query intent for SERP blending .   The common part of both systems is to use query intent to assist the ranking of retrieved documents of different types. Meanwhile, the two products have their unique challenges. Typeahead blending has strong latency requirements; the input is an incomplete query ; and it is okay to return a fuzzy prediction, since users will continue to type the whole query if he/she does not find the results relevant. On the other hand, SERP blending has less latency constraint compared to typeahead but a higher accuracy requirement as it directly affects the search result page.  %Because of the different requirements, we exploit many aspects to find the best solutions for each of the two productions, such as algorithms , token units , combining traditional features, and multilinguality.   Based on the characteristics of production applications, we propose different solutions. For typeahead blending, character-level query representation is used as the resulting models are compact in terms of the number of parameters. Meanwhile, it can handle multilinguality well due to the small vocabulary size. For SERP blending, the complete query intent model is word level. Since accuracy is a high standard, BERT is explored to extract query representations which lead to a more robust model.  This paper is motivated by tackling the challenges in query intent prediction, while satisfying production needs in order to achieve real-world impact in search systems. The major contributions are:           %proxIn the rest of this paper, we first introduce how query intent is defined and utilized in LinkedIn search products. This is followed by the design of different granularity  models to adapt to different stages of intent prediction.  The experiments and results show that the proposed deep learning based character/word-level representation models are effective at understanding users' search intents as well as efficient for productionalization in online search systems.   % algorithm analysis]: BERT Experience: in-domain data, better performance/latency with smaller model. LSTM experience: capture long range distance.  CNN fast. % [token granularity analysis]:  character level model benefits: 1. compact model. 2. suitable for multilingual % [product impact analysis]: Larger impact of deep learning models on typeahead blending than serp blending  % Wide feature experience   
","     Understanding a user's query intent behind a search is critical for modern search engine success.      Accurate query intent prediction allows the search engine to better serve the user's need by rendering results from more relevant categories.      This paper aims to provide a comprehensive learning framework for modeling query intent under different stages of a search. We focus on the design for 1) predicting users' intents as they type in queries on-the-fly in typeahead search using character-level models; and 2) accurate word-level intent prediction models for complete queries. Various deep learning components for query text understanding are experimented. Offline evaluation and online A/B test experiments show that the proposed methods are effective in understanding query intent and efficient to scale for online search systems.     % The proposed approach models query intent by 1) predicting users' intents as they type in queries on-the-fly in typeahead search with character-level models; as well as 2) word-level intent prediction models after the queries are completed. Character-level and word-level deep learning based models are proposed and thoroughly analyzed. Offline evaluation and online A/B test experiments show that the proposed methods are effective in predicting query intent, which assists search engine success.          % The proposed character-level model based on recurrent neural networks  is effective in terms of predicting search verticals on incomplete queries, while also being efficient as the online inference latency meets production requirements.      % For word-level query understanding, convolutional neural networks  based model provides an effective and efficient solution for query intent classification, while Bidirectional Encoder Representations from Transformers  outperforms CNN in terms of accuracy but introduces larger latency at the same time.      % We also focus on the challenges and efforts of putting the BERT models into real life products at LinkedIn.",156
"  A dialog system should correctly understand speakers閳 utterances and respond in natural language. Dialog act recognition  and sentiment classification are two correlative tasks to realize the former. The goal of DAR is to attach semantic labels to each utterance in a dialog and identify the underlying intentions . Meanwhile, sentiment classification can detect the sentiments which are implicated in utterances and can help to capture speakers閳 intentions .    
","         In dialog system, dialog act recognition and sentiment classification are two correlative tasks to capture speakers闁 intentions, where dialog act and sentiment can indicate the explicit and the implicit intentions separately  . Most of the existing systems either treat them as separate tasks or just jointly model the two tasks by sharing parameters in an implicit way without explicitly  modeling mutual interaction and relation. To address this problem, we propose a Deep Co-Interactive Relation Network  to explicitly consider the cross-impact and model the interaction between the two tasks by introducing a co-interactive relation layer. In addition, the proposed relation layer can be stacked to gradually capture mutual knowledge with multiple steps of interaction. Especially, we thoroughly study different relation layers and their effects. Experimental results on two public datasets  show that  our model outperforms the state-of-the-art joint model by 4.3\% and 3.4\% in terms of F1 score on dialog act recognition task, 5.7\% and 12.4\% on sentiment classification respectively. Comprehensive analysis empirically verifies the effectiveness of explicitly modeling the relation between the two tasks and the multi-steps interaction mechanism. Finally, we employ the Bidirectional Encoder  Representation from Transformer  in our framework, which can further boost our performance in both tasks.",157
"  The image and text used for an online ad  can be influential in targeting online users on a large scale.%play an influential role in the minds of targeted online users.  Large businesses  typically employ creative strategists to design ad creatives; these creative strategists may conduct market research to see trending themes and also gather insights from past ad campaigns in related product categories. Such advertiser specific creative customization is mostly a manual, expensive, and time consuming process. In contrast, small businesses typically resort to free online tools, , stock image libraries , and generic creative insights  to compile ad images and text; such tools can reduce the time to design creatives but tend to be generic . Once the ad creatives are ready, both large and small advertisers need to conduct online A/B tests to validate the effectiveness of their creatives, and subsequently discard low performing creatives from their ad campaigns. In addition, to reduce the chances of online users getting tired of seeing the same ad repeatedly on a particular website , advertisers need to frequently go through the designA/B testrefresh ad creatives cycle % . Again, such cycles tend to be time consuming and there is an emerging need for data-driven approaches to speed up the whole process of designing and refreshing creatives. \par In this paper, we highlight a key observation that accelerates the above creative design process, and can be explained as follows. Advertisers typically test their creatives via A/B tests in ad platforms , , they try out a set of creatives on online users in a controlled setup such that the click-through-rate  performance  difference across the creatives can be solely attributed to the ad text and image. However, advertisers conduct and learn from such A/B tests in isolation as illustrated in Figure. As shown, advertiser  who is an internet service provider, may learn via an A/B test that having human elements in the ad image works better than having gadgets in the image . Via a separate A/B test, a different advertiser   may learn that using new and limited time in the ad text works better than using great. Our key observation in the illustrated example is that although the advertisers are learning in isolation, the ad platform can learn across advertisers. In fact, most ad platforms are authorized to use performance data across advertisers in an aggregate manner to help advertisers perform better; however, using A/B test data across advertisers in a collaborative manner to automate ad creative refinement is a largely unexplored topic.  In this paper we address several sub-problems in ad creative  refinement exploiting the above observation by using multi-advertiser A/B test data: %To leverage the above observation, in this paper, we formulate several tasks around ad creative  refinement using multi-advertiser A/B test data:   Another novelty in our proposed approaches for the above tasks is that they do not depend on intermediate models such as CTR prediction as required in previous work  but rely on pairs of examples of the form:  where the CTR is based on the same population of users . Both creatives in a pair are sourced from the same advertiser, and at a high level, the task of refining can be seen as translating the low CTR creative  to the high CTR creative . As we discuss in this paper, such pairs can be naturally collected from A/B tests conducted by multiple advertisers in an ad platform.  %For the ad text generation problem, we take inspiration from encoder-decoder architectures common in neural machine translation and summarization setups , and study the efficacy of a copy mechanism which can selectively copy parts of the input ad text while introducing new words in the refined version. For the keyphrase and image tag recommendation problems we  Our main contributions are as follows.       The remainder of the paper is organized as follows. Section covers related work, and Section covers problem formulation. Section explains data sources, and creation of pairs of creatives for training ad refinement models. Section covers proposed methods, Section covers experimental results, and there is a discussion in Section. %. %, and reproducibility notes are provided in Appendix. %%%%%% text generation %%%%%%%%%%%% 
"," In the online advertising industry, the process of designing an ad creative  requires manual labor. Typically, each advertiser launches multiple creatives via online A/B tests to infer effective creatives for the target audience, that are then refined further in an iterative fashion.  Due to the manual nature of this process, it is time-consuming to learn, refine, and deploy the modified creatives. Since major ad platforms typically run A/B tests for multiple advertisers in parallel, we explore the possibility of collaboratively learning ad creative refinement via A/B tests of multiple advertisers. In particular, given an input ad creative, we study approaches to refine the given ad text and image by:  generating new ad text,  recommending keyphrases for new ad text, and  recommending image tags  to select new ad image. Based on A/B tests conducted by multiple advertisers, we form pairwise examples of inferior and superior ad creatives, and use such pairs to train models for the above tasks. For generating new ad text, we demonstrate the efficacy of an encoder-decoder architecture with copy mechanism, which allows some words from the  input text to be copied to the output while incorporating new words associated with higher click-through-rate. For the keyphrase and image tag recommendation task, we demonstrate the efficacy of a deep relevance matching model, as well as the relative robustness of ranking approaches compared to ad text generation in cold-start scenarios with unseen advertisers. We also share broadly applicable insights from our experiments using data from the Yahoo Gemini ad platform. %Finally, based on experiments using Yahoo Gemini platform's data, we share insights useful for similar efforts across the advertising industry.",158
" Over the past decades, sentiment analysis has delivered promising results on stock market prediction. The sentiment-based approaches explore the sentiment or event signals from text corpus such as tweets, news, or financial announcements and attempt to relate such signals to the stock price variation. These works can be regression~ that predicts the  in next time step, or classification~ that predicts the  of the stock movement.   In early studies, bag-of-words , n-grams, or other discrete word features such as noun phrase are used to represent the text corpus~. The word features are selected by pre-defined dictionaries or statistical metrics. Although, such approaches facilitate the alignment between the linguistic features and numerical data and mitigate the dimensionality problem, they can hardly preserve the contextual information.  Lately, neural approaches have been applied to the realm.  extracted event tuples  from the news articles and trained the event embeddings. Then the daily events are averaged with the event vectors.  represented each tweet using Continuous Bag of Words Model  with word2vec for prediction. Moreover, many neural network based models such as RNN and text-CNN are proposed~ and achieve considerable improvement as compared to traditional methods.  However, some issues are less addressed by existing approaches:  Most of the time the stock prices fluctuate within a narrow range and unnecessarily reflect the market sentiment, which complicates relating sentiment signals to price signals.  The polarity of a certain word, especially an entity or a term, can change over time according to different events. For instance, the tone of word ``venezuela"" fluctuates with the oil trade situations between the U.S. and Venezuela.  The quality or value varies in different texts. Extracting sentiment signals from a text can only be valid if the text is relevant to the market.   To address these issues, we propose a two-stage system and utilize both the neural representation and discrete features for stock trend prediction. As illustrated in Figure, our system consists of a  to extract the sentiment score for the future market trend and a  that predicts the direction of the index movement of next week given the sentiment scores of news over the week. The main architecture we use for the sentiment extractor is the vanilla BERT~ with multitask learning~ - one additional prediction head that predicts the worthiness of the news. We propose a metric called  to extract the word polarity among different event periods and use it as a supplemental feature. We present a weekly-Monday prediction framework in which the Monday index variations are predicted with all news articles over the past week. A new dataset, the 10-year Reuters financial News , is also proposed.     The experimental results on the 10-year Reuters financial news dataset show that our system achieves significant improvement as compared to the baseline methods. We illustrate the weekly-Monday basis is appropriate for sentiment-based stock prediction. We show that our model that uses word polarity features and additionally learns the worthiness of the news can better predict the stock market index.    
","   Sentiment-based stock prediction systems aim to explore sentiment or event signals from online corpora and attempt to relate the signals to stock price variations. Both the feature-based and neural-networks-based approaches have delivered promising results. However, the frequently minor fluctuations of the stock prices restrict learning the sentiment of text from price patterns, and learning market sentiment from text can be biased if the text is irrelevant to the underlying market. In addition, when using discrete word features, the polarity of a certain term can change over time according to different events. To address these issues, we propose a two-stage system that consists of a sentiment extractor to extract the opinion on the market trend and a summarizer that predicts the direction of the index movement of following week given the opinions of the news over the current week. We adopt BERT with multitask learning which additionally predicts the worthiness of the news and propose a metric called Polarity-Over-Time to extract the word polarity among different event periods. A Weekly-Monday prediction framework and a new dataset, the 10-year Reuters financial news dataset, are also proposed.",159
"  Neural modeling approaches are prominent in research on both task-oriented and open-domain dialog. Traditional sequence-to-sequence models have been used for encoding the dialog history and predicting domains, intents, slot types, spans and more generally decoding full-fledged system responses. In recent years, large pre-trained Transformer-based models for natural language understanding  and natural language generation  have become ubiquitous, leading to tremendous advances by fine-tuning towards these dialog tasks.  %   In task-oriented speech-based dialog systems, the effect of ASR hypotheses has been widely studied and techniques have been devised to minimize the resulting downstream NLU errors. More recently, end-to-end spoken language understanding approaches have been attempted to sidestep this problem. % There is work to address such issues, such as using n-best lists. %It is not clear how these models would serve the speech modality, when ASR output contains recognition errors, and does not have structural information such as sentence boundaries and punctuation. On the other hand, research in open-domain dialog is increasingly focusing on large, monolithic end-to-end neural models like Google's Meena that are built using written data and evaluated on written interactions. Several written textual datasets have been created recently to tackle various problems in open-domain dialog, including persona-grounding, knowledge-grounding and reasoning, and state-of-the-art chatbots have been built using them. However, it is not clear whether these written text-based open-domain chatbots would seamlessly integrate with ASR models to serve the speech modality, which is popular due to the ubiquity of voice assistants like Alexa, Google Assistant and Siri.  Collecting large-scale written text-based dialog datasets is cheaper and more practical than collecting audio-based dialog datasets in many ways. But speech-robustness should be a factor of consideration when designing any  dialog system intended to be deployed to the speech modality, even in the absence of audio-based training data. To bring attention to this important aspect in the open-domain dialog community, we empirically study the effects of various types of synthetic and actual ASR hypotheses in the dialog history on TransferTransfo , a state-of-the-art neural open-domain dialog system based on the Generative Pre-trained Transformer  from the NeurIPS ConvAI2 Conversational Intelligence Challenge. We build off the Topical-Chat dataset and perform two augmentations in our study: one creating simulated ASR hypotheses for the entire dataset, and another creating actual ASR hypotheses with a smaller audio-based analogue of the Topical-Chat test sets.  We observe that TF2 trained on written textual data is very sensitive to synthetic and actual ASR hypotheses introduced to the dialog history during inference time, with the sensitivity being particularly prominent for the task of response selection. As a baseline mitigation strategy, we introduce synthetic ASR hypotheses to the dialog history during training and observe marginal improvements, demonstrating the need for further research into techniques to make end-to-end open-domain chatbots fully speech-robust. Figure shows a sample snippet with responses from TF2 models trained on written text and synthetic ASR hypotheses when fed speech-distorted dialog history.  A close work to ours in spirit is, which shows that Transformer-based generative dialog models are insensitive to unrealistic perturbations like token-shuffling of the dialog history. Our work is more focused on evaluating the effects of introducing realistic perturbations to the dialog history in the form of synthetic and actual ASR hypotheses. Our augmentation of Topical-Chat, dubbed the Topical-Chat ASR dataset, is open-sourced\footnote{https://github.com/alexa/Topical-Chat/tree/master/TopicalChatASR/} to enable open-domain dialog researchers to perform speech-robustness evaluation and fuel research into novel techniques to make monolithic neural open-domain dialog models more speech-robust.  % report the impact on automated metrics such as perplexity, unigram F1, recall and diversity.  
","   Large end-to-end neural open-domain chatbots are becoming increasingly popular. However, research on building such chatbots has typically assumed that the user input is written in nature and it is not clear whether these chatbots would seamlessly integrate with automatic speech recognition  models to serve the speech modality. We aim to bring attention to this important question by empirically studying the effects of various types of synthetic and actual ASR hypotheses in the dialog history on TransferTransfo, a state-of-the-art Generative Pre-trained Transformer  based neural open-domain dialog system from the NeurIPS ConvAI2 challenge. We observe that TransferTransfo trained on written data is very sensitive to such hypotheses introduced to the dialog history during inference time.   %with the sensitivity being particularly prominent for the task of response selection.   As a baseline mitigation strategy, we introduce synthetic ASR hypotheses to the dialog history during training and observe marginal improvements, demonstrating the need for further research into techniques to make end-to-end open-domain chatbots fully speech-robust. To the best of our knowledge, this is the first study to evaluate the effects of synthetic and actual ASR hypotheses on a state-of-the-art neural open-domain dialog system and we hope it promotes speech-robustness as an evaluation criterion in open-domain dialog.",160
"  Knowledge Graph  consists of facts in the form of triplet  using the corresponding relation and tail  embeddings, and calculate the plausibility of the triplet by measuring the difference between the original and the reconstructed embeddings.  These works either model this relationship in an explainable way , or utilize the black-box but expressive convolution operations . Another strand of works considers link prediction as a semantic matching problem.  They take the embeddings of the head, relation and tail as input, and output a matching score for the elements in each triplet using bi-linear transformation , convolution  and etc.  These works vary a lot in situations that they are suitable for, like the relation types in the KG, the sparsity of the KG, etc. Therefore, choosing a suitable architecture for a specific KG often requires careful analysis of both the dataset and the model. To tackle this issue,  proposes to use AutoML to greedily search for optimal score functions for distinct KGs. %To tackle this issue,  proposes to use AutoML to find an optimal structure for a specific KG. However, their work is constrained to bilinear semantic matching models and does not include the reconstruction-based models into their search space.  In this paper, we propose a novel Neural Architecture Search  framework to search for the most effective architecture for a given dataset. The framework entails a more general search space that contains both semantic matching models and reconstructive models. Therefore, it has the potential to combine the strength of the two model families.  Instead of searching over a discrete set of candidate architectures, we relax the search space to be continuous, so that the architecture can be optimized using the efficient gradient-based search algorithm.  As shown in Fig., our NAS framework contains two search modules. The representation search module aims to refine the embeddings  of the head, relation, and tail respectively through multiple representation layers. The score function search module is responsible for selecting a shallow architecture to calculate a plausibility score for the input triplet. While the operators in each module have a broad range of choices, in this work, we primarily focus on a proof-of-concept of this two-level search space by constraining the operators to architectures that are representatives of existing link prediction models. Specifically, we constrain the search space of the representation search module to be reconstructive models, whose input and output have good compatibility of this module. As for the score function search module, we select representative models from mainstream model families in the link prediction task. To avoid overfitting, we also add the identity operation in the representation search module so that the NAS algorithm could choose to use the original , or even degenerate to the basic models in the score function search module when necessary.  On the one hand, we can consider the representation search module refines  in a black-box way. On the other hand, the output of the representation search module may also embed the constraints modeled in the reconstruction-based models. Therefore, the final score could potentially benefit from the cross-validation of multiple models, which will likely lead to better prediction results.  We evaluate our approach on several popular benchmark datasets. Extensive experiments demonstrate that our approach has good generalization ability over different datasets, and achieves better performance than strong baseline models in most of the datasets.   %% %% This is file `sample-sigconf.tex', %% generated with the docstrip utility. %% %% The original source files were: %% %% samples.dtx   %%  %% IMPORTANT NOTICE: %%  %% For the copyright see the source file. %%  %% Any modified versions of this file must be renamed %% with new filenames distinct from sample-sigconf.tex. %%  %% For distribution of the original source see the terms %% for copying and modification in the file samples.dtx. %%  %% This generated file may be distributed as long as the %% original source files, as listed above, are part of the %% same distribution.  %% %% The first command in your LaTeX source must be the \documentclass command. \documentclass[sigconf]{acmart}  %%%% As of March 2017, [siggraph] is no longer used. Please use sigconf  for SIGGRAPH conferences.  %%%% As of May 2020, [sigchi] and [sigchi-a] are no longer used. Please use sigconf  for SIGCHI conferences.  %%%% Proceedings format for SIGPLAN conferences  % \documentclass[sigplan, anonymous, review]{acmart}  %%%% Proceedings format for conferences using one-column small layout % \documentclass[acmsmall,review]{acmart} %% %% \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{%   \providecommand\BibTeX{{%           %% These commands are for a PROCEEDINGS abstract or paper. %{June 03--05, 2018}{Woodstock, NY} % % %   %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%  %% %% The majority of ACM publications use numbered citations and %% references.  The command  switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%  %% %% end of the preamble, start of the body of the document source.  \usepackage{microtype} \usepackage{url} \usepackage{mathrsfs} \usepackage[linesnumbered,ruled,vlined]{algorithm2e} \usepackage{multirow} \usepackage{graphicx}  \usepackage{float}  \usepackage{subfigure}  \usepackage{color} \usepackage{enumerate} \usepackage{booktabs} \usepackage[normalem]{ulem} \usepackage[toc,page,title]{appendix}  [2]{ {@{}#1@{}}#2}   \usepackage{xcolor} [1]{\textcolor{red}{TODO: #1}\PackageWarning{TODO:}{#1!}}  \usepackage{xspace}      %% %% The ""title"" command has an optional parameter, %% allowing the author to define a ""short title"" to be used in page headers. \title{NASE: Learning Knowledge Graph Embedding for Link Prediction via Neural Architecture Search}  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research. \author{Xiaoyu Kou} % \authornotemark[1] %\authornote{Both authors contributed equally to this research.}  %\orcid{1234-5678-9012} \affiliation{%          }  \author{Bingfeng Luo}  \affiliation{%      }  \author{Huang Hu}  \affiliation{%       }   \author{Yan Zhang}  \affiliation{%         }  %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose.  %\renewcommand{  %% %% The abstract is a short summary of the work to be presented in the %% article.  %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %%  <ccs2012> <concept> <concept_id>10010147.10010178.10010187</concept_id> <concept_desc>Computing methodologies~Knowledge representation and reasoning</concept_desc> <concept_significance>500</concept_significance> </concept> </ccs2012>      %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.   %% %% This command processes the author and affiliation and title %% information and builds the first part of the formatted document.       
"," Link prediction is the task of predicting missing connections between entities in the knowledge graph .  While various forms of models are proposed for the link prediction task, most of them are designed based on a few known relation patterns in several well-known datasets. Due to the diversity and complexity nature of the real-world KGs, it is inherently difficult to design a model that fits all datasets well. To address this issue, previous work has tried to use Automated Machine Learning  to search for the best model for a given dataset. However, their search space is limited only to bilinear model families. In this paper, we propose a novel Neural Architecture Search  framework for the link prediction task.  First, the embeddings of the input triplet are refined by the Representation Search Module. Then, the prediction score is searched within the Score Function Search Module. This framework entails a more general search space, which enables us to take advantage of several mainstream model families, and thus it can potentially achieve better performance.  We relax the search space to be continuous so that the architecture can be optimized efficiently using gradient-based search strategies. Experimental results on several benchmark datasets demonstrate the effectiveness of our method compared with several state-of-the-art approaches.",161
"   The capacity of a neural network influences its ability to model complex functions.  In particular, it has been argued that deeper models are conducive to more expressive features . Very deep neural network models have proved successful in computer vision  and text classification . In neural machine translation , however, current state-of-the-art models such as the Transformer typically employ only 6-12 layers .  Previous work has shown that it is difficult to train deep Transformers, such as those over 12 layers . This is due to optimization challenges: the variance of the output at each layer compounds as they get deeper, leading to unstable gradients and ultimately diverged training runs.   In this empirical study, we re-investigate whether deeper Transformer models are useful for NMT.  We apply a recent initialization technique called ADMIN , which remedies the variance problem. This enables us train Transformers that are significantly deeper, e.g. with 60 encoder layers and 12 decoder layers.\footnote{We choose to focus on this layer size since it results in the maximum model size that can fit within a single GPU system. The purpose of this study is to show that it is feasible for most researchers to experiment with very deep models; access to massive GPU budgets is not a requirement.}   In contrast to previous research, we show that it is indeed feasible to train the standard\footnote{Note there are architectural variants that enable deeper models , discussed in Sec . We focus on the standard architecture here.} Transformer  with many layers.  These deep models significantly outperform their 6-layer baseline, with up to 2.5 BLEU improvement. Further, they obtain state-of-the-art on the WMT'14 EN-FR and WMT'14 EN-DE benchmarks.% !TEX encoding = UTF-8 % !TEX Root = Main.tex     
","  We explore the application of very deep Transformer models for Neural Machine Translation .  Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers.  These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on  WMT14 English-French  and WMT14 English-German . To facilitate further research in Very Deep Transformers for NMT, we release the code and models: \url{https://github.com/namisan/exdeep-nmt}.",162
" Non-autoregressive transformer~ has attracted wide attention in neural machine translation~, which generates sentences simultaneously rather than sequentially. To enable parallel decoding, NAT imposes a conditional independence assumption among words in the output sentences, which leads to significantly faster inference speed~ than the autoregressive Transformer~. However, NAT still falls behind autoregressive Transformer~ in the quality of output sentences, such as BLEU~ for machine translation. %\zhouh{move BLEU into the citations} We blame it for the imposed conditional independence assumption, which prevents  NAT models from explicitly learning the word dependencies in the output sentence. Note that such word dependency is crucial, and it is explicitly learned in the AT model through the autoregressive language models~.  Recently,  propose to employ the Masked Language Model~ in NAT, which includes word dependency modeling in an iterative fashion~, therefore yielding quite competitive results compared to AT. Specifically, such iterative models randomly mask words in the reference and predict these masked words conditioned on unmasked ones during training.  In this manner, iterative models are trained to explicitly capture the dependencies between masked words and unmasked words. However, these iterative approaches still produce poor results with one decoding iteration and have to perform multiple iterations during inference, namely iteratively refining the generated outputs of the previous iteration. Such iterative process is quite time-consuming, which partly sacrifices the speed merit of NAT. To date, it remains an open question as to how the iterative process can be abandoned, while still preserving the benefits of explicitly modeling word dependencies in NAT.  In this paper, we argue that the major culprit of the problem that mask language models have to be used together with iterative inference, is the sampling strategy of masking words in MLM.  In particular, MLM employs a fixed uniform strategy for randomly masking words during training, which prevents the model from effectively learning word dependencies for one-iteration generation.   For example, at the beginning of training when the NAT model is still poorly tuned, we should mask fewer words. If not, it would be difficult for the NAT model to correctly predict the masked words.  % In the worst case, the NAT model may stuck in some local minimums during training, which prevents the NAT model from well fitting the training data. On the contrary, if we mask too little words at the end phase of training, the resulting NAT model is rarely trained to predict the whole sentences, and can only predict some sentence fragments.  In such a case, to accurately generate the whole sentence in inference, the NAT model has to generate the sentence fragments iteratively.  To this end, the sampling strategy is crucial for the training of NAT.  To address the above issues, we propose a simple yet effective approach called Glancing Transformer~, which is equipped with the proposed Glancing Language Model~ for non-iterative parallel text generation, achieving significant improvements upon strong baselines. Intuitively, GLM adopts a adaptive glancing sampling strategy, which glances at some fragments of the reference if the reference is too difficult to fit in the training of NAT. Correspondingly, when the model is well tuned, it will adaptively reduce the percentage of glancing sampling, making sure that the resulting model could learn to generate the whole sentence in the one-iteration fashion.  Specifically, our proposed GLM differs from MLM in two aspects. Firstly, GLM proposes an adaptive glancing sampling strategy, which enables \method to generate sentences in a one-iteration way, working by gradual training instead of iterative inference~. % Intuitively, GLM works like the way of  Generally, GLM is quite similar to curriculum learning~ in spirit, namely first learning to generate some fragments and gradually moving to learn the whole sentences~. To achieve the adaptive glancing sampling, GLM performs decoding twice in training. The first decoding is the same as the vanilla NAT, and the prediction accuracy indicates whether current reference is ``difficult'' for fitting. In the second decoding, GLM gets words of the reference via glancing sampling according to the first decoding, and learn to predict the remaining words that are not sampled.  Note that only the second decoding will update the model parameters. Secondly, instead of using the  token, GLM directly use representations from the encoder at corresponding positions, which is more natural and could enhance the interactions between sampled words and signals from the encoder.  Experimental results show that \method obtains significant improvements  on standard benchmarks compared to the vanilla NAT, without losing inference speed-up.  \method achieves competitive results against iterative approaches like Mask-Predict~, even outperforming the Mask-Predict model on WMT14 DE-EN and WMT16 RO-EN. % Considering the fully NAT models with the one-iteration simultaneous generation, \method achieves the best BLEU scores. Compared to the strong AT baseline, \method can still close the performance gap within 1 BLEU point while keeping  speed-up. Empirically, we find that \method outperforms AT when the source input length is less than 20 on WMT14 DE-EN. We speculate this is because GLM could capture bidirectional context while left-to-right LM is unidirectional, which indicates the potential of parallel generation models.     
"," %Non-autoregressive models generate all the tokens of the sequence in parallel.  Although non-autoregressive models with one-iteration generation achieve remarkable inference speed-up, they still fall behind their autoregressive counterparts in prediction accuracy. The non-autoregressive models with the best accuracy currently rely on multiple decoding iterations, which largely sacrifice the inference speed of non-autoregressive models.  Inspired by the way of learning word dependencies in autoregressive and iterative-decoding models, we propose Glancing Transformer~ with a glancing language model~, which learns to capture the word dependency gradually. Experiments on three benchmarks demonstrate that our approach can significantly improve the accuracy of non-autoregressive models without multiple decoding iterations. In particular, \method achieves state-of-the-art results among non-iterative models and even outperforms top iterative counterparts in some specific benchmarks.",163
"  % 1. Introduce the importance of emotion detection  Understanding human emotions is considered as the key to building engaging dialogue systems . However, most works on emotion understanding tasks treat individual emotions independently while ignoring the fuzziness nature and the interconnections among them. A psychoevolutionary theory proposed by  shows that different emotions are actually correlated, and all emotions follow a circular structure. For example, ``optimism'' is close to ``joy'' and ``anticipation'' instead of ``disgust'' and ``sadness''. Without considering the fundamental inter-correlation between them, the understanding of emotions can be unilateral, leading to sub-optimal performance. These understanding can be particularly important for low resource emotions, such as ``surprise"" and ``trust"" whose training samples are hard to get.  % Therefore, in this paper, we investigate how emotion correlations can be captured and help emotion classification tasks. Therefore, the research question we ask is, how can we 	extbf{obtain and incorporate the emotion correlation to improve emotion understanding tasks, such as classification?}  To obtain emotion correlations, a possible way is to take advantage of a multi-label emotion dataset. Intuitively, emotions with high correlations will be labeled together, and therefore, emotion correlations can be extracted from the label co-occurrences. Recently, a multi-label emotion classification competition  with 11 emotions has been introduced to promote research into emotional understanding. To tackle this challenge, the best team  first pre-trains on a large amount of external emotion-related datasets and then performs transfer learning on this multi-label task. However, they still neglect the correlations between different emotions. % leaving the research question of how emotion correlations can be helpful for emotion tasks.  % Many other methods that leverage label co-occurrences, such as Classifier Chains , CNN-RNN , and Sequence Generation Model , can be applied to capture emotion correlations. However, these models consider the multi-label classification as a sequential task while the emotions are intrinsically non-sequential, which makes the captured emotion correlations uninterpretable %  and sub-optimal.    % 2. Traditional ways to solve emotion detection problem, the problem of these methods and we need to leverage the relation between emotions. % Recently,  introduced a multi-label emotion classification dataset, which added new challenges into the emotion detection task by turning it into a multi-label problem . The challenge comes from the vagueness of human emotions, which hinders model performance on multiple emotion predictions. One way to tackle this challenge is by pre-training on a large amount of external emotion-related dataset  and then do transfer learning on this multi-label task . However, collecting a huge dataset is expensive and time-consuming for researchers, which makes this method not scalable.  % Furthermore, most related work towards multi-label emotion classification Intuitively, interconnects exist in emotions, for example, if you are surp by your friends, at this time, you also feel happy and excited. Therefore, leveraging the relations among emotions would be an effective and yet unexplored approach in multi-label emotion classification.  % 3. Some recent paper about leveraging relations in other dataset but might not be suitable for emotion % An easiest way is to apply the method which models the relations among multiple labels into this emotion classification challenge.   % 4. In this paper, introduce our method and list the contribution In this paper, we propose EmoGraph which leverages graph neural networks to model the dependencies between different emotions.  We take each emotion as a node and first construct an emotion graph based on the co-occurrence statistics between every two emotion classes. Graph neural networks are then applied to extract the features from the neighbours of each emotion node. We conduct experiments on two multi-label emotion classification datasets. Empirical results show that our model outperforms strong baselines, especially for macro-F1 score. The analysis shows that low resource emotions, such as ``trust"", can particularly benefit from the emotion correlations. An additional experiment illustrates that the captured emotion correlations can also help the single-label emotion classification task.  % The codes will be released.   
"," Most emotion recognition methods tackle the emotion understanding task by considering individual emotion independently while ignoring their fuzziness nature and the interconnections among them. In this paper, we explore how emotion correlations can be captured and help different classification tasks. We propose EmoGraph that captures the dependencies among different emotions through graph networks. These graphs are constructed by leveraging the co-occurrence statistics among different emotion categories. Empirical results on two multi-label classification datasets demonstrate that EmoGraph outperforms strong baselines, especially for macro-F1. An additional experiment illustrates the captured emotion correlations can also benefit a single-label classification task.",164
"  Neural NLP models typically embed the sequence of input tokens using a lookup table of learnable parameters, where each row represents a token type as a dense vector . The same embedding matrix is often reused to predict the output in language models . How essential are embeddings to the model's success? Intuitively, one would expect them to be critical, given the ubiquitous use of embeddings layers and the vast amount of parameters they typically consume. In this work, we show that machine translation models can be trained , and that they can rival and sometimes even outperform standard embedding-based models.  % Does it play an essential role in those models ability to preform as well as they do? In this work we put this to the test by removing the trainable embedding matrix from Neural Machine Translation models and instead use a fixed one hot encoding of the vocabulary.  % Embedding is a widely used component of modern NLP models, it usually takes the form of a learnable dense matrix, , where  is the vocabulary, and  is the hidden dimension, that makes it the most parameter consuming layer of the models architectures. Does it play an essential rule in those models ability to preform as well as they do? In this work we put this to the test by removing the trainable embedding matrix from Neural Machine Translation models and instead use a fixed one hot encoding of the vocabulary.  We remove the trainable embedding matrix from a standard transformer machine translation model, and use a constant one-hot encoding of the vocabulary instead. To limit the dimensionality, we use byte tokenization by reading the text as a unicode  byte stream, which can represent virtually every text in any language in under 256 dimensions per token. Byte vocabularies obviate the need to preprocess the text with hand-crafted language-specific tokenizers  and subword induction algorithms, such as BPE .    Machine translation experiments on 10 language pairs show that models without a trainable embedding matrix perform on par with the best embedding-based baselines.  We find that embeddingless models consistently achieve higher BLEU scores than their byte baselines, and even yield slightly better performance than embedding-based character models in 80\% of the cases. % We observe a similar yet weaker trend when comparing to character level models, where embeddingless gets a higher BLEU score in 16 out of the 20 cases, with a smaller average gap. Although the recent literature on character-based transformers demonstrates the superiority of subword tokenization when controlling for network depth , our experiments show that  the embedding matrix from byte-to-byte models makes them perform at least as well as standard subword models in 9 out of 20 cases. Overall, our results suggest that highly-parameterized embedding matrices might not be as essential as commonly perceived.  % Interestingly, despite existing work in MT showing sub word models consistently outperforming character and byte level ones , our Embeddingless models are able to get equal or higher results then subword models in 9 out of 20 cases despite having significantly smaller number of parameters.   % Machine translation experiments on 10 language pairs show that models without a trainable embedding matrix can can be comparative with the best embedding based baseline. Moreover, We find that embeddingless models can achieve higher scores then byte and character level baselines in 19 and 16 cases out of 20 respectively. Interestingly, despite existing work in MT showing sub word models consistently outperforming character and byte level ones , our Embeddingless models are able to get equal or higher scores then sub word models in 9 out of 20 cases despite having significantly smaller number of parameters.    % of  outperform their byte baselines, where they achieve a higher BLEU score in 19 out of 20 cases and an equal score for ru-en. We can observe a similar but weaker trend when comparing to character level models, where embeddingless gets a higher BLEU score in 16 out of the 20 cases, with a smaller average gap.  % \uri{new version - end}  % Machine translation experiments on 10 language pairs show that removing the embedding matrix consistently improves the performance of byte-to-byte models. % When translating from English into a foreign  language, embeddingless byte-to-byte models outperform character-based models in 9 out of 10 languages, and subword models in 7 out of 10 languages. % This last result is particularly surprising given recent literature in machine translation, which demonstrates a consistent superiority of subword models over character models of the same size . \omer{we need to make this last sentence a bit more subtle...} % \uri{Maybe: This last result is particularly surprising given recent literature in machine translation, which shows sub word models consistently achieve higher BLEU scores then character models of the same size? }  % However, we observe that subword models are consistently better when translating into English. % % \uri{However, we observe that subword models are able to achieve higher BLEU scores when translating into English. ?} % \omer{We need to explain this... maybe also blame BLEU in some sense? We're using ""better"" and ""outperform"" quite a bit here, but the diffs are really not that huge, and BLEU is also super-dependent on the tokenizer. Maybe we want to soften the whole tone in the paper and say something like ""the models are competitive"", ""very similar perfomance with a slight edge to X""?} % \uri{I agree} % \omer{we need to end on a positive note...}   % \uri{the following old version of last sentence is a bit crooked but is built in a different way. It doesnt say: ""we are bad in translating into English"", and then have to both explain and end with a positive tune. Instead it says ""we are good under some conditions also in subwords"":   % While recent work in machine translation demonstrates that character and byte level models often underperform sub word models , we observe that embeddingless models close the gap and even surpass sub word models in some cases, particularly when the tokenizer that post-processes the sub words model output is not optimized for the target language. % }         %While recent work in machine translation demonstrates that character and byte level models often underperform sub word models , we observe that embeddingless models close the gap and even surpass sub word models in some cases, particularly when the tokenizer that post-processes the sub words model output is not optimized for the target language.          %   % Tokenization is the task of segmenting a string into a sequence of tokens that a model can process. % The current standard practice is to split a string into subwords using whitespaces, language-specific heuristics  \uri{maybe we can also add this to the this list :https://arxiv.org/pdf/2008.05055.pdf}, and Byte-Pair Encoding   or its variants . % While subword tokenization works well in practice for many languages, it also loses orthographic information that may be critical for morphologically rich languages. % Moreover, the assumption that subword units must be contiguous segments does not hold for languages with non-concatenative morphology such as Arabic and Hebrew. % Intuitively, sufficiently parameterized models should be able to leverage pure  tokenization to access all the information in the original string; however, recent work in machine translation demonstrates that character-level models often underperform subword models in practice .  % We hypothesize that part of the reason character-based models underperform is the embedding layer. This layer represents each vocabulary item as a vector, implicitly capturing some similarity function between the token types. % While similarity between words and subwords is potentially useful, it is not quite clear how character similarity could benefit the model. % We conjecture that, for many languages, the embedding layer allows the character-based models to learn a misleading concept of character similarity, and propose to remove it.  % In this work we omit the embedding matrix for character-to-character machine translation models, using orthogonal one-hot representations instead. % Since some languages  have thousands of characters, we use  representations based on UTF-8, which can represent all the information in any language in under 256 dimensions per token.  % Machine translation experiments on 10 language pairs show that removing the embedding matrix consistently improves the performance of byte-to-byte models. When translating from English into a foreign language, embeddingless byte-to-byte models outperform character-based models in 9 out of 10 languages, and subword-based models in 7 out of 10 languages \omer{one is equal...} \uri{we win against BPE in: zh,ar,ru,ja,tr,fa,he tie in es and lose in de,vi}. However, we observe that subword models are consistently better when translating into English.  % We suspect that the opposing trends arise from English-centric heuristics in the tokenizer. \omer{We show that byte-based tokenization can effectively learn the ...} % \omer{I want to say something about the Jieba experiment, and then finish off with how byte-to-byte can potentially learn language-specific tokenization without manually-engineered heuristics. Problem is that we still see a gap, and we need to say something about why this is happening .} \uri{Maybe we can say that we believe the gap is due to the first sentence of this paragraph, linguistic knowledge injected with Moses? }    % Byte-to-byte machine translation models also have the theoretical advantage that they will never encounter unknown token types .             % Tremendous progress was made in Neural Machine Translation  in recent years [\uri{*cite*}]. In NMT, a model gets as input text in a source language, and outputs a translation in a target language. An important and well studied question in NMT is how should the text be segmented when fed into the models as a sequence of tokens. A common approach is to build a sub-words vocabulary with Byte-Per Encoding   or one of its variants. BPE is a data compression algorithm that given a corpus and a wanted vocabulary size , merges greedily common bi-grams from the data into sub-words, and extracts a vocabulary of  frequent sub-words from the corpus.  As the use of sub-words currently allows to achieve state-of-the-art results in NMT [\uri{*cite*}], it comes with some costs, the main one being the lost of some orthographic information, especially in morphologically rich languages such as \uri{X}, \uri{Y} and \uri{Z}, consider for example [\uri{*example*}].\\  % One way of allowing the models access to all of the information the raw text holds is to use characters instead of sub words as base units. This choice has additional advantages over the use of words or sub-words, presented by . First, as opposed to sub-words based models, a character based one will not suffer from out of the vocabulary tokens during inference, second, there is no need for data preprocessing with schemes like BPE that also tend to require hyper-parameters tuning themselves. Despite the theoretical advantages, previous work [\uri{*cite*}] has shown that performance drops when adopting current NMT models to use character-to-character in a naive plug-and-play technique. That is surprising, as these models should be sophisticated enough to learn the patterns when given the opportunity to. A part of the reason lies in the fact that BPE creates shorter sequences, and current models do not handle long sequences as good as they do shorter ones . Recently, a lot of work was done in order to build models that can handle better long dependencies ,  and impressive progress was made in that area. In this work we tackle a different aspect of the current models and show an improvement in their ability to work on non-sub-word [\uri{non-sub-word is a bit weird, but I wasn't sure if I should write ""byte"" or ""char"" here, as the model is byte, but in the story we are at the ""char"" stage}] data, up to outperforming BPE in some cases, while using the same architecture that tend to struggle with longer sequences. \\  % [\uri{not sure where to put the following sentence}\\ % Recent improvements of non-sub-words models suggested adding different new components to the existing models , we on the contrary, suggest doing the opposite.]\\  % One way of allowing the models access to all of the information the raw text holds, is to use characters as tokens, but previous work [\uri{*cite*}] has shown that performance drops when adopting current NMT models to use character-to-character in a naive plug-and-play technique. That is surprising, as these models should be sophisticated enough to learn the patterns when given the opportunity to. An important difference of the char-to-char form of a corpus as opposed to its word or sub-word form is the length of the sequences, where a sequence in characters is usually much longer then its sub-words form in sub-word units, and A widely common conjuncture for why char-to-char models underpreform is that current models do not handle long sequences as good as they do with shorter sequences. A lot of recent work was done in order to build models that can handle better long dependencies , , but though that might be a part of the reason the current models struggle with characters as tokens, in this work we tackle a different part of the modern models that we believe to be also a culprit of that performance drop, and show improvement of non-sub-word models. \\  % The in recent years, NMT and in Natural Language Processing in general, is the one-model-fits-all approach, which means building models that preform well on many tasks and datasets, as opposed to designing different task specific algorithms and architectures.   % Typically, word and sub-word embeddings in language models reflect interesting relations such as lexical substitution, semantic similarity, etc. [\uri{*cite*}], and though these are great qualities of sub-word embeddings, we argue that trying to find such a similarity function for characters has no real justification, as in most languages character do not encapsulate semantic meaning by themselves. Therefore, such a function can be misleading for the rest of the model [\uri{someone might say, similarity of vowels for instance does makes sense isn't it?}] and hurt the its ability to generate high quality translations. \\   [ht] c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c@{}|@{}c} \toprule %     & Tokenized   \\ % \midrule Original Text  & \multicolumn{22}{c}{{russian}琚樿鍐欒 锜瑰啓鑺鑺儊.}\\ Subwords   & \multicolumn{4}{@{}c@{}|}{{russian}琚樿@} & \multicolumn{5}{@{}c@{}|}{{russian}鍐欒} & \multicolumn{6}{@{}c@{}|}{{russian}锜瑰啓鑺疈} & \multicolumn{6}{@{}c@{}|}{{russian}瑜夎姱鑳亇 & .\\ Characters & \multicolumn{2}{@{}c@{}|}{{russian}琚榼 & \multicolumn{2}{@{}c@{}|}{{russian}瑜峿 & \multicolumn{2}{@{}c@{}|}{{russian}鍐檥 & \multicolumn{2}{@{}c@{}|}{{russian}瑜渳 &   & \multicolumn{2}{@{}c@{}|}{{russian}锜箎 & \multicolumn{2}{@{}c@{}|}{{russian}鍐檥 & \multicolumn{2}{@{}c@{}|}{{russian}鑺瘆 & \multicolumn{2}{@{}c@{}|}{{russian}瑜墋 & \multicolumn{2}{@{}c@{}|}{{russian}鑺瘆 & \multicolumn{2}{@{}c@{}|}{{russian}鑳亇 & .  \\ Bytes  & \ D0 \  & \ 91 \ & \ D1 \ & \ 83 \ & \ D0 \ & \ B4 \ & \ D1 \ & \ 8C \ & \ 20 \ & \ D0 \ & \ B7 \ & \ D0 \ & \ B4 \ & \ D0 \ & \ BE \ & \ D1 \ & \ 80 \ & \ D0 \ & \ BE \ & \ D0 \  & \  B2 \  &\  2E  \\  琚樿鍐欒 锜瑰啓鑺鑺儊.'' UTF-8 uses two bytes to represent each character in the Cyrillic script, making the byte sequence longer than the number of characters.}      
"," Many NLP models follow the embed-contextualize-predict paradigm, in which each sequence token is represented as a dense vector via an embedding matrix, and fed into a contextualization component that aggregates the information from the entire sequence in order to make a prediction.  Could NLP models work without the embedding component? To that end, we omit the input and output embeddings from a standard machine translation model, and represent text as a sequence of bytes via UTF-8 encoding, using a constant 256-dimension one-hot representation for each byte. Experiments on 10 language pairs show that removing the embedding matrix consistently improves the performance of byte-to-byte models, often outperforms character-to-character models, and sometimes even produces better translations than standard subword models.\footnote{Our code is publicly available at: \url{https://github.com/UriSha/EmbeddinglessNMT}}",165
"  Propagandist news articles are misleading in nature and aim at biasing their audience towards a particular point of view by using psychological and rhetorical techniques, including loaded language, name calling, repetition, exaggeration, minimization, etc. With the rapid growth in the number of online sources of information and the speed with which information spreads online, manual flagging of propagandist news articles has become untenable, leading to an ongoing need for new research on methods for identifying these articles automatically to mitigate the negative influence they might have on users.  Until very recently, most of the work in this area has focused on article-level detection. However, in 2019, Da San Martino et al.~ published a corpus of English news articles with individual spans of propaganda annotated that addresses the problem at a more granular level. This corpus is used in shared tasks at NLP4IF-2019 and at SemEval-2020. The 2020 shared task is a modified version of the prior year's task and includes two subtasks:       Given a plain-text document, identify those specific fragments which contain at least one propaganda technique. This is a binary sequence tagging task.           Given a text fragment identified as propaganda and its document context, identify the applied propaganda technique in the fragment.   We present our models for both tasks alongside discussions of our results and ablations. 
","   This paper presents our systems for SemEval 2020 Shared Task 11: Detection of Propaganda Techniques in News Articles. We participate in both the span identification and technique classification subtasks and report on experiments using different BERT-based models along with handcrafted features. Our models perform well above the baselines for both tasks, and we contribute ablation studies and discussion of our results to dissect the effectiveness of different features and techniques with the goal of aiding future studies in propaganda detection.",166
"  %%%%%%%%%%%%%%%%%%%%%%     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
"," \vspace*{-0.4cm} %The PAN 2020 authorship verification  shared task focuses on a cross-topic/closed-set setup of fictional texts . The topic  describes the principal subject matter of the document, which does not necessarily correlate with the author's writing style. For automated systems, the AV task is quite challenging when the texts to be investigated come from different thematic areas. %In this work, we present a hierarchical fusion of two well-known approaches into a single end-to-end learning procedure: A deep metric learning framework at the bottom aims to learn a pseudo-metric that maps a document of variable length onto a fixed-sized feature vector. At the top, we incorporate a probabilistic layer to perform Bayes factor scoring in the learned metric space.  %We also provide text preprocessing strategies to deal with the cross-topic issue.  The PAN 2020 authorship verification  challenge focuses on a cross-topic/closed-set AV task over a collection of fanfiction texts. Fanfiction is a fan-written extension of a storyline in which a so-called fandom topic describes the principal subject of the document. The data provided in the PAN 2020 AV task is quite challenging because authors of texts across multiple/different fandom topics are included. In this work, we present a hierarchical fusion of two well-known approaches into a single end-to-end learning procedure: A deep metric learning framework at the bottom aims to learn a pseudo-metric that maps a document of variable length onto a fixed-sized feature vector. At the top, we incorporate a probabilistic layer to perform Bayes factor scoring in the learned metric space. We also provide text preprocessing strategies to deal with the cross-topic issue.",167
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %     % % final paper: en-uk version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International Licence.     % Licence details:     % \url{http://creativecommons.org/licenses/by/4.0/}.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. }  Propaganda techniques need to attach importance to arouse the emotions of the receivers, sometimes even by temporarily bypassing the intellectual defenses of the receivers  . Propaganda uses psychological and rhetorical techniques to achieve its purpose. Such techniques include using logical fallacies and appealing to the emotions of the audience. Logical fallacies are usually hard to spot since the argumentation, at first, might appear correct and objective  . However, careful analysis shows that the conclusion cannot be drawn from the premise without misusing logical rules. Another set of techniques uses emotional language to induce the audience to agree with the speaker only based on the emotional bond that is being created, provoking the suspension of any rational analysis of the argumentation.  The traditional NLP task generally classifies and detects propaganda techniques at the article level, which often fails to meet more detailed requirements. This fact has also been confirmed by previous iterations of the SemEval competition, where leading solutions used convolutional neural networks , long short-term memory     and transfer learning techniques  . The main features of an article are extracted by using the feature capture and pooling of the CNN model, but these methods can only be used at the article level and are coarse-grained detection methods. However, limited research has focused on text classification .  News articles have also been classified using the Bi-LSTM-CNN model  .   However, there are often many propaganda techniques in one article, and most of these techniques are efficient for propaganda classification but lack the ability to detect categories of propaganda techniques. Thus, they cannot achieve good results and are less efficient in practice.  Now the difficulty is to detect propaganda techniques at the fine-grained level. The SemEval-2020 Task 11, ``Detection of Propaganda Techniques in News Articles'', is designed to promote research on this task. We used the word embedded representation of the pretrained model and LSTM model to detect the news article propaganda techniques at a fine-grained level, and we also evaluate the performance among different neural network models on this task.  The task consists of two subtasks.  	  The rest of the paper is organized as follows. Section 2 describes the details of the LSTM used in our system. Section 3 presents the experimental results. Conclusions and future works are described in Section 4.    
","   This paper summarizes our studies on propaganda detection techniques for news articles in the SemEval-2020 task 11. This task is divided into the SI and TC subtasks. We implemented the GloVe word representation, the BERT pretraining model, and the LSTM model architecture to accomplish this task. Our approach achieved good results for both the SI and TC subtasks. The macro-$F_{1}$-$score$ for the SI subtask is 0.406, and the micro-$F_{1}$-$score$ for the TC subtask is 0.505. Our method significantly outperforms the officially released baseline method, and the SI and TC subtasks rank 17th and 22nd, respectively, for the test set. This paper also compares the performances of different deep learning model architectures, such as the Bi-LSTM, LSTM, BERT, and XGBoost models, on the detection of news promotion techniques. The code of this paper is availabled at: \url{https://github.com/daojiaxu/semeval_11}.",168
" Machine Reading Comprehension  has become a popular task in NLP, aiming to understand a given passage and answer the relevant questions. With the wide availability of MRC datasets~ and deep learning models~ , significant progress has been made.  Despite the success, a majority of MRC research has focused on open domains. For specific domains, however, the construction of high-quality MRC datasets, together with the design of corresponding models is considerably deficient~. The causes behind this phenomenon are threefold. Take the medical domain as an example. i) Data annotators are required to have medical backgrounds with high standards. Hence, simple crowd-sourcing~ often leads to poor annotation results. ii) Due to the domain sensitivity, people are more concerned about the reliability of the information sources where the answers are extracted, and the explainability of the answers themselves~. This is fundamentally different from the task requirements of open-domain MRC. iii) From the perspective of model learning, it is difficult for pre-trained language models to understand the meaning of the questions and passages containing a lot of specialized terms~. Without the help of domain knowledge, state-of-the-art models can perform poorly. As shown in Figure, BERT~ and MC-BERT ~ only predict part of the correct answer, i.e.,~``torso"" and ``buttocks"", instead of generating the complete answer to the medical question. %Compared with the strong baseline model, our model can predict a complete answer after injecting a lot of medical knowledge. % Our CMedBERT model can extract the complete answer after fusing a lot of medical knowledge. % Only when we fuse medical knowledge into pre-trained models, complete answers are be extracted.    In this paper, we present a comprehensive study on Chinese medical MRC, including i) how the task is formulated, ii) the construction of the Chinese medical dataset and iii) the MRC model with rich medical knowledge injected. To meet the requirements of medical MRC, we aim to predict both the answer spans to a medical question, and the support sentence from the passage, indicating the source of the answer. The support sentences provide abundant evidence for users to learn medical knowledge, and for medical professionals to assess the trustworthiness of model output results.  %two MRC tasks in our Chinese medical dataset. %The first task is to predict the answer content of the question and the second task is to predict the index of sentence supporting the answer. For the dataset, we construct a highly-quality Chinese medical MRC dataset, named the Multi-task Chinese Medical MRC dataset . It contains 12,172 question, passage, answer, support sentence quads. Based on the analysis of CMedMRC, we summarize four special challenges for Chinese medical MRC, including long-tail terminologies, synonym terminology, terminology combination and paraphrasing. In addition, we find that comprehensive skills are required for MRC models to answer medical questions correctly. For answer extraction in CMedMRC, direct token matching is required for answering 31\% of the questions, co-reference resolution for 11\%, multi-sentence reasoning for 18\% and implicit causality for 22\%. In addition, the answers to the remaining questions  are extremely difficult to extract without rich medical background knowledge.  To address the medical MRC task, we propose the multi-task dynamic heterogeneous fusion network  based on MC-BERT~ model and Chinese medical knowledge base . The technical contributions of CMedBERT are twofold:  : We mimic humans' approach of reading comprehension~ by learning attentively  aggregated representations of multiple entities in the passage. Different from the knowledge fusion method used by KBLSTM ~ and KT-NET ~, we propose a two-level attention and a gated-loop mechanism to replace the knowledge sentinel, so that the rich knowledge representations can be better integrated into the model. % Unimportant information is then filtered by two types of attention mechanisms and the gated-loop layer. : The model parameters of CMedBERT are dynamically learned by capturing the relationships between the two tasks via multi-task learning. We regard the semantic similarities between support sentences and answers to questions as the task similarities.   In the experiments, we compare CMedBERT against four strong baselines. For answer prediction, compared to the strongest competitor, the EM  and F1 scores are increased by +3.88\% and +1.46\%, respectively. Meanwhile, the support sentence prediction task result is increased by a large margin, i.e., +7.81\% of EM and +4.07\% of F1. The contributions are summarized as follows:      % [] %  %  % \toprule % Challenges & Characteristics  & Example \\  %  \multirow{3}{*}{Lexical-Level}&  Long-tail terminologies & ...閸愬牅绗傞懖宀冨㈤懙杈ㄦ焽鐟佸倽鐦灞炬Ц鐎电懓鍞告稉濠呭㈤懖宀冨彙閺勵垰鎯佺涙ê婀弬顓☆棁鏉╂稖顢戝Λ閺屻儯鍌氬敻娑撳﹨鍊㈤懖宀冨彙閺傤叀顥囨径姘礈闂傚瓨甯存径鏍у閹甸懛杈剧礉\\閸ョ姷娲块幒銉﹀ⅵ閸戞槒鍋愰柈銊╃姵鍨氶懓鍛毌鐟... \\  \\  %  & Synonyms terminologies  & \makecell*[l]{...閺堫剝宓傞崫浣割嚠鏉╁洦鏅遍幀褔钃熼悙搴℃嫲娑撳﹤鎳犻崥鎼佷壕閹扮喐鐓嬪鏇℃崳閻ㄥ嫰钃熼崗鍛邦攨閺堝鏅ラ敍灞藉讲閻€劋绨幇鐔峰晪閹存牠钃熺粣锔惧... \\ } \\  %  &fdsafd& \makecell[l]{dafdsafd} \\  %  & fdasfd &fdsaf \\  %   []   \toprule Challenges & Characteristics  & Example \\   & Long-tail terminology  & \makecell*[l]{...閸愬牅绗傞懖宀冨㈤懙杈ㄦ焽鐟佸倽鐦灞炬Ц鐎电懓鍞告稉濠呭㈤懖宀冨彙閺勵垰鎯佺涙ê婀弬顓☆棁鏉╂稖顢戝Λ閺屻儯鍌氬敻娑撳﹨鍊㈤懖宀冨彙閺傤叀顥囨径姘礈闂傚瓨甯存径鏍у閹甸懛杈剧礉\\閸ョ姷娲块幒銉﹀ⅵ閸戞槒鍋愰柈銊╃姵鍨氶懓鍛毌鐟... \\ }    \\  Lexical-Level    & Synonym terminology  & \makecell*[l]{...閺堫剝宓傞崫浣割嚠鏉╁洦鏅遍幀褔钃熼悙搴℃嫲娑撳﹤鎳犻崥鎼佷壕閹扮喐鐓嬪鏇℃崳閻ㄥ嫰钃熼崗鍛邦攨閺堝鏅ラ敍灞藉讲閻€劋绨幇鐔峰晪閹存牠钃熺粣锔惧... \\ }  \\       & Terminology combination  & \makecell*[l]{...缁牕缈遍惀鍛嗩潒缂冩垼鍟橀惀閺勵垳纭哥亸璺ㄦ⒕閹冧簳鐞涚粻锛勬⒕閸欐ü鑵戦張闁插秷顩﹂惃鍕冮悳甯礉閺勵垯绔寸粔宥呭徔閺堝　\閻楃懓绱撻幀褎鏁奸崣妯兼畱閻厧绨抽惀鍛綁閿涘瞼纭哥亸璺ㄦ⒕閻ㄥ嫪寮楅柌宥呰嫙閸欐垹妫佹稊瀣╃... \\ }} is the most important manifestation of diabetic microangiopathy. It is a fundus \\disease with specific changes, one of the severe complications of diabetes...)}   \\  Sentence-Level   & Paraphrasing  & \makecell*[l]{Passage:\tiny ...婵″倹鐏夐崷銊ユЬ鐟欐帞鍎屾禍鍡樺灗缂佹挾姊婚惃鍕勾閺傜绻樼悰灞藉枎閺佸嚖绱濇稉閺傚綊娼伴崘閿嬫殾閻椻晛鎼ф稉宥呭叡閸戦惃鍕樈娴兼岸鐘冲灇閹扮喐鐓嬮敍娑樺綗娑撻弬褰掓桨鐏為柈銊︿刊鎼达箓妾锋担搴濈啊\\娑斿鎮楅敍灞藉冀閼板奔绱板鍓佺处娴笺倕褰涢惃鍕墹閸氬牄...\\\\Question:\tiny 娑撹桨绮堟稊鍫濇Ь鐟欐帞鍎屾禍鍡樺灗缂佹挾姊绘稉宥呯紦鐠侇喛绻樼悰灞藉枎閺佸嚖绱 \\} \\      
"," Machine Reading Comprehension  aims to extract answers to questions given a passage. It has been widely studied recently, especially in open domains. However, few efforts have been made on closed-domain MRC, mainly due to the lack of large-scale training data. In this paper, we introduce a multi-target MRC task for the medical domain, whose goal is to predict answers to medical questions and the corresponding support sentences from medical information sources simultaneously, in order to ensure the high reliability of medical knowledge serving. A high-quality dataset is manually constructed for the purpose, named Multi-task Chinese Medical MRC dataset , with detailed analysis conducted. We further propose the Chinese medical BERT model for the task , which fuses medical knowledge into pre-trained language models by the dynamic fusion mechanism of heterogeneous features and the multi-task learning strategy. Experiments show that CMedBERT consistently outperforms strong baselines by fusing context-aware and knowledge-aware token representations.",169
" Nowadays, the volume of biomedical literature and biomedical web pages continues to increase rapidly.  Lots of new articles and web pages containing biomedical discoveries and new insights are continuously published.  Indeed, there is an increasingly high demand for biomedical text mining.                Recent progress in the biomedical text mining approach is made possible by the development of deep learning techniques used in natural language processing . For example, pre-trained language models such as   BERT , ERNIE , XLNet  and  RoBERTa  have demonstrated remarkable successes in modeling contextualized word representations by utilizing the massive amount of training text. As a fundamental technique in natural language processing , the language models pre-trained on text could be easily transferred to learn downstream NLP tasks with finetuning, which achieve the state-of-the-art performances on many tasks including named entity recognition, paraphrase identification, question answering and information retrieval.   However,  it has limitations to apply state-of-the-art NLP methodologies to biomedical text mining directly. Firstly, since recent representation models such as BERT  are trained and tested mainly on general domain datasets such as  Wikipedia, it is difficult to adapt to biomedical datasets without losing the performance. Moreover, the word distributions of general and biomedical text are quite different, which can be a problem for biomedical text mining. In addition, there exist long-tail concepts and terminologies in biomedical texts which are difficult to be learned via language models. For the Chinese biomedical text, it is somewhat more difficult due to its complex structure and the variety of phrase combinations. To this end, recent biomedical text mining models rely mostly on adapted versions of word representations  .  Considering whether it is possible to automatically inject biomedical knowledge to the language representation learning for Chinese medical corpus, we hypothesize that current state-of-the-art word representation models such as BERT should be trained on biomedical corpora with prior biomedical knowledge to be effective in biomedical text mining tasks. However,  there exist two problems:  how to retrieve the biomedical domain knowledge;  how to leverage such knowledge to the representation learning.   In this paper, we propose a conceptualize representation learning approach   for Chinese biomedical language understanding. Specifically, we propose coarse-to-fine masking strategies to inject entity and linguistic domain knowledge into representation learning. As there are no benchmarks for the Chinese Biomedical Language Understanding Evaluation, we release the first large scale benchmark  including name entity recognition, paraphrase identification, question answering, information retrieval, intent detection, and text classification.  Experiments show that our approach achieves state-of-art results.  
"," Biomedical text mining is becoming increasingly important as the number of biomedical documents and web data rapidly grows. Recently, word representation models such as  BERT has gained popularity among researchers. However, it is difficult to estimate their performance on datasets containing biomedical texts as the word distributions of general and biomedical corpora are quite different. Moreover, the medical domain has long-tail concepts and terminologies that are difficult to be learned via language models. For the Chinese biomedical text, it is more difficult due to its complex structure and the variety of phrase combinations. In this paper, we investigate how the recently introduced pre-trained language model BERT can be adapted for Chinese biomedical corpora and propose a novel conceptualized representation learning approach.  We also release a new  Chinese Biomedical Language Understanding Evaluation benchmark .  We examine the effectiveness of Chinese pre-trained models: BERT, BERT-wwm, RoBERTa, and our approach. Experimental results on the benchmark show that our approach could bring significant gain. We release the pre-trained model on GitHub: \url{https://github.com/alibaba-research/ChineseBLUE}.",170
"   Digitalization facilitates management and manipulation of large-scale data sets, such as large collections of documents, audio recordings, or images. In such large collections, finding specific objects efficiently is only possible with computational tools. The predominant form of searching is based on the similarity of objects, where an algorithm would identify and rank a list of objects from the collection based on their similarity to a given query object. Similarity search requires object type specific similarity measures. For instance, some form of textual similarity may be used for searching document collections whereas a measure for time-series similarity would be employed for searching collections of audio recordings. 	 A particularly valuable type of information object are tables, which only recently have received appropriate attention. Tables are used to present structured information in a two-dimensional matrix, and are extensively used in scientific articles, business reports, product specifications, web pages etc. Research on tables as first class objects started roughly 10 years ago with the availability of large table collections, mostly extracted from web pages or from Wikipedia. %Research on tables from document collections has lagged behind due to the difficulty to extract tables from document formats like PDF. This situation, however, has recently improved considerably, as more and more document collections have introduced proper table mark-up.   This work is concerned with table similarity : Given a pair of tables, e.g. a query table and a table from a table corpus, compute an accurate estimate of their semantic similarity.  TS is a fundamental operation and a prerequisite for many further applications, such as table clustering and classification, table auto-completion, table fusion or filling missing values in databases. Despite the importance of TS, it has received only little attention as an operation in its own right so far. Existing table similarity functions are all tightly integrated into their downstream application and were not compared to other TS methods. For instance, previous works on table augmentation, table union, table extension or table imputation all incorporate specific TS algorithms whose individual quality is unknown. Note that TS, as we define it and as necessary for such applications, is different from the related field of table-keyword similarity. %cafarella2008webtables  What is lacking is a general and robust method to assess the similarity of two tables. Compared to similarity of other types of objects, TS has its own, specific properties. In contrast to pure texts, where the sequence of words, sentences, and paragraphs conveys meaning, tables impose meaning through the arrangement of values in columns and rows, often augmented with header information. In contrast to image similarity, where the relative positions of pixels is extremely important, table similarity often is independent of the order of rows or columns - two tables of patients from two hospitals will be considered similar irrespective of the order in which patients appear as rows, or the order in which metadata of the patients is recorded.  In this paper, we present TabSim, a TS method which employs deep learning techniques to achieve two main objectives: a) to generate suitable table representations, and b) to use these representations to learn an accurate similarity function for pairs of tables. It is based on Siamese neural networks, which are known to be able to learn a similarity model given only few samples. TabSim does not require any hand-crafted features, but learns a similarity function directly from a gold standard corpus. TabSim's network first generates a representation of each table as a concatenation of embeddings of its caption and of its tabular content. For these, we apply two different networks to properly reflect their diverging structures: A Bidirectional LSTM  layer capable of modeling sequences is utilized to capture semantic information from captions because the order of words in the caption carry semantic information. Tabular data is represented by an order-invariant self-attention neural network, since the order of cells within a column and the order of columns within a table most often does not carry meaning. The two representations are shared by both compared tables to guarantee the symmetry of the similarity score. Model parameters are optimized with a contrastive loss function that relies on tables distances.  To train and evaluate TabSim, we created a novel corpus consisting of 1500 table pairs extracted from biomedical articles and manually scored regarding their pairwise degree of similarity. To the best of our knowledge, this is the first publicly available gold standard corpus for TS. We also evaluated our approach on two other corpora originally developed for a different yet similar task which allowed adaptation: a) tables extracted from arXiv articles and b) tables in Wikipedia pages. Our evaluation on these three corpora shows that, on average, TabSim outperforms baselines by app. 7\% pp F1-score in a binary similarity classification setting and by app. 1.5\% pp in a ranking scenario using NDCG.   The paper is organized as follows.  In Section 2, we review existing techniques for TS. We explain TabSim and its neural architecture in Section 3. Section 4 presents data preparation, the used baselines, and evaluation settings and metrics. In Section 5, we provide the results of our evaluation and conclude in Section 6.   %-------------------------------------------------------------------------- 
"," Tables are a popular and efficient means of presenting structured information. They are used extensively in various kinds of documents including web pages. Tables display information as a two-dimensional matrix, the semantics of which is conveyed by a mixture of structure , headers, caption, and content. Recent research has started to consider tables as first class objects, not just as an addendum to texts, yielding interesting results for problems like table matching, table completion, or value imputation. All of these problems inherently rely on an accurate measure for the semantic similarity of two tables. We present TabSim, a novel method to compute table similarity scores using deep neural networks. Conceptually, TabSim represents a table as a learned concatenation of embeddings of its caption, its content, and its structure. Given two tables in this representation, a Siamese neural network is trained to compute a score correlating with the tables' semantic similarity. To train and evaluate our method, we created a gold standard corpus consisting of 1500 table pairs extracted from biomedical articles and manually scored regarding their degree of similarity, and adopted two other corpora originally developed for a different yet similar task. Our evaluation shows that TabSim outperforms other table similarity measures on average by app. 7\% pp F1-score in a binary similarity classification setting and by app. 1.5\% pp in a ranking scenario.",171
" Spoken language understanding  systems  extract   semantic information from a spoken utterance  by machine .  The  Air Travel Information System   was the first  SLU model built based on a cascade of a speech recognizer, a language model,  a semantic extractor-SQL generator   in 1990 .    Thirty years after ATIS, designing an end-to-end    neural SLU  that can replace  the ASR+NLU-based SLU technology still remains a challenge . Ideally, we would like to have an all-neural model whose layers project the audio signal to hidden semantic representations, the-so-called ""thought vectors""  to infer the domain, intent, and slots implied by the audio signal.     To achieve this goal, several groups conducted experiments using non-ASR awareness  E2E SLU.  These models usually apply multiple stack of RNNs   to encode the entire utterance to a vector which is fed to a fully connected feedforward neural network followed by a soft-mask or a max-pool layer to identify the domain, intent, or slot.  These models treat each unique combination of domain, intent, and slots as an output label. For this reason, we call this type of E2E SLU  classification-based approaches. The limitation of classification-based approaches is that  the combination of domains, intents, and slots may grow exponentially, subsequently we deal with a classification problem with many number of output labels; moreover, the number of intents is not usually fixed which makes usability of classification-based approaches more limited.     A natural approach to deal with the variable-length output for E2E SLU is to use the sequence-to-sequence  neural models .  In , several seq2seq  architectures are proposed for E2E SLU, among which the authors found the model that incorporates  an ASR-awareness module in form of  a multi-task learner delivers the best performance. The finding is further supported by a recent proposed pre-trained ASR-awareness architecture .     In  ASR, the input and output sequences are ordered and monotonic.  As such, we don't need the entire utterance to decode the transcription. In contrast, when extracting semantic information from audio signals, we usually need to scan the entire audio. Similar to neural machine translation , the E2E  SLU models can massively benefit from the attention mechanism.  In attention mechanism, the  encoder generates the outputs by incorporating the hidden representations from all time steps and hence allows the output to pay attention to inputs at all time steps .  The neural attention models  have demonstrated promising results in ASR as well .       The transformers are seq2seq, non-recurrent, self-attention neural models that have been used in neural machine translation  as well as NLU with great success . In this paper, we leverage the transformer architecture for E2E SLU. Neural transformers have several distinct features which make them suitable candidate  for SLU task:  The transformers use the self-attention mechanism that allows to compute the correlation in each sublayer between all pairs of time steps both in  the encoder and decoder.  Sub-spaces projection by self-attention helps extract semantic context from audio frames.  The transformers can benefit from distributed training because linear transformation in self-attention can be parallelized.   Compared to RNN  models , the transformers have less number of parameters.  Our model works both in a classification-based mode and in a hierarchical mode that allows to  decode variable length domain, intent and slot vectors.  We compare this new architecture with  E2E SLU models that use both RNN and CNN on the recently publicly released dataset called Fluent Speech  Commands . Our results show that the transformer-based SLU outperforms the RNN+CNN based model, while it has  less numbers of parameters.     The rest of this paper is organized as follows:  Section 2 formulates the problem.  In Section 3,  we describe the transformer based SLU in details. Section 4 gives the details of experiments and results. Finally, we draw the conclusion and give the future directions in Section 5.   
","  Spoken language understanding  refers to the process of inferring the semantic  information from audio signals.  While the neural transformers consistently deliver the best performance among the state-of-the-art neural architectures in field of natural language processing ,  their merits in a closely related field,,  spoken language understanding   have not beed investigated. In this paper, we introduce an end-to-end neural transformer-based SLU model that can predict the variable-length domain, intent, and slots  vectors embedded in an audio signal with no intermediate token prediction architecture. This new architecture leverages the self-attention mechanism by which the audio signal is transformed to various sub-subspaces allowing to extract the semantic context implied by an utterance. Our end-to-end  transformer SLU predicts the domains, intents  and slots in the Fluent Speech  Commands dataset with accuracy equal to 98.1 \%, 99.6 \%, and 99.6 \%, respectively and outperforms the SLU models that leverage a combination of recurrent and convolutional neural networks by 1.4 \%   while the size of our model is  25\% smaller than  that of these architectures.  Additionally, due to independent sub-space projections in the self-attention layer,  the model is highly parallelizable which makes it a good candidate for on-device SLU.",172
"   This paper deals with estimating the humor of edited  English news headlines. The illustration of tasks is in Figure . The original text sequence is given, which represents a title, with the annotated part that is edited along with the edit itself. Our responsibility is to determine how funny this change is in the range from 0 to 3 . This is called Sub-Task 1. We also participate in the Sub-Task 2, in that we should decide which from both given edits is the funnier one.  For the second task, we used the approach of reusing the model from the first task as it is described in section . So we are focusing the description on Sub-Task 1.    Official results were achieved with a Convolutional Neural Networks  , but we also tested numerous other approaches such as SVM and pre-trained transformer model.  The humor is a very subjective phenomenon, as can be seen from the inter-agreement on label annotation in Sub-Task 1 dataset\footnote{The inter-annotator agreement measured with Krippendorff's interval metric is just 0.2~ .}. The given data labels do not allow us to learn a sense of humor of a human annotator because the dataset does not specify from whom the grade comes. So, for example, if we have one annotator that likes dark humor and all the others not, we will be considering such a replacement as not humorous no meter if it is excellent dark humor or not. In other words, we may say that we are searching for some most common kind of humor.  The dominant theory of humor is the Incongruity Theory . It says that we are finding humor in perceiving something unexpected  that violates expectations that were set up by the joke.  There are samples, in the provided dataset, that uses the incongruity to create humor. Moreover, according to  Hossain et al.  , we can see a positive influence of incongruity on systems results for the dataset.   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %       % space normally used by the marker This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: \url{http://creativecommons.org/licenses/by/4.0/}. }  
"," This paper describes our system that was designed for Humor evaluation within the SemEval-2020 Task~7. The system is based on convolutional neural network architecture. We investigate the system on the official dataset, and we provide more insight to model itself to see how the learned inner features look.",173
"  Having a clear picture of students' perception on their classes, professors, and university facilities enables educational institutions to propose strategies to improve in many areas. It has been suggested by many studies that positive students' perception on the learning environment is correlated with higher academic achievement. Therefore, not only can universities improve the quality of their professors, their class content as well as learning facilities, but they also can improve --as a consequence-- their students' academic achievement, leading to an overall improvement of the education quality.  The call for action is clear. However, in order to propose and implement effective improvement strategies, one needs to measure the students' perception. Typical ways of doing this is through evaluations carried out at the final stage of each academical period where students grade their professors in several aspects. These evaluations normally consist of an online questionnaire with closed questions, and some open questions where students give their opinions about the class and their professors. Closed questions questionnaire can be tedious for students, leading to low response rates. Closed questions are helpful for the fast interpretation of results with statistical tools. These questions are designed to measure professors' performance on specific topics such as how engaging the class is, punctuality, among others. On the other hand, open questions provide students with a free space to express their opinions. Of course, gathering and interpreting data from open questions responses is a much more challenging task than making statistics from closed questions. Nonetheless, the amount of useful information found in students' opinions is a valuable source that is rarely exploited.  The latest advances in machine learning and natural language processing  techniques can be used to build tools that facilitate the analysis of large amounts of opinions generated by students. Particularly, sentiment analysis is suited to identify and quantify how positively or negatively students feel about their professors. These machine learning applications have only been recently explored. For instance, Na鑼倂e Bayes has been used to classify students' opinions in social media. Also, Latent Dirichlet Allocation  has been used to model topics along with sentiment analysis to explore opinions from students. Some studies using tools from machine learning have been conducted in the field of students' perception analysis. The majority of them have addressed the issue of performing sentiment analysis of the students' comments, and others have tried to identify topics in suggestions and opinions left by students , thus, we develop a joint approach were state of the art tools from NLP are used to perform both sentiment analysis and identify topics of interest in the students' comments.   Nonetheless, we must stress that researchers have for long worked on similar problems of assessing customer satisfaction from written opinions including public election forecasting, sales and trading prediction, marketing price prediction, among others. The common pipeline for performing opinion mining consists of the following general steps: i) retrieval of opinions from public databases, ii) cleaning of the opinions , iii) prediction of a quantity of interest such as polarity, sentiment strength, among others.  In this paper, we combine state-of-the-art methods in an NLP-based pipeline for classifying the sentiment of students' opinions. We then use these results to predict the ratings given to the professors by the students by means of supervised learning algorithms. Furthermore, we perform LDA to discover latent topics that are central in students' opinions. With the power of question answering systems, we envision students' perceptions surveys having only open questions that are fast to answer, reaching high levels of response rates, and also extracting the most relevant information, which comes from the students' opinions. These opinions are then mined with methods like the one we propose to analyse how students truly feel about their professors and classes.  The structure of this paper is as follows. In section methods and materials a brief description of the data and its prepossessing is presented. Next, in the results the analysis of model performance as well as the statistical analysis of the obtained results is scrutinised. Recommendations for future perspectives in the research of the subject as well as an outlined of the conclusions are listed in the final part of the manuscript.   
","  Students' perception of classes measured through their opinions on teaching surveys allows to identify deficiencies and problems, both in the environment and in the learning methodologies. The purpose of this paper is to study, through sentiment analysis using natural language processing  and machine learning  techniques, those opinions in order to identify topics that are relevant for students, as well as predicting the associated sentiment via polarity analysis. As a result, it is implemented, trained and tested two algorithms to predict the associated sentiment as well as the relevant topics of such opinions. The combination of both approaches then becomes useful to identify specific properties of the students' opinions associated with each sentiment label  and topic. Furthermore, we explore the possibility that students' perception surveys are carried out without closed questions, relying on the information that students can provide through open questions where they express their opinions about their classes.  % of the comments using NLP techniques.  % In the present paper Students' perception of classes measure through teaching surveys allows to identify deficiencies and problems, both in the environment and in the learning methodologies. NLP techniques   %The abstract should briefly summarize the contents of the paper in 15--250 words.",174
"   In knowledge discovery and representation, the notion of {, i.e., `abstract entity' or `abstract object' in the Fregean dichotomy of { . In Natural Language Processing , the task of {. Halliday  offers a syntactic interpretation of { or { or { .   CE is crucial for a number of downstream applications, including, e.g., language understanding, ontology population, semantic search, and question answering; it is also the key to entity linking . In generic open domain subject-neutral discourse across different  subjects, indexing the longest possible nominal chunks and their head words located in sequences of tokens between specified ``break words""  and special dictionary lookups such as {  are very common techniques. They generally reach outstanding precision, but low recall due to constant evolvement of the language vocabulary. Advanced deep learning models that already dominate CE in specialized closed domain discourse on one or a limited range of related subjects, e.g., biomedical discourse , and that are also standard in keyphrase extraction  are an alternative. However, such models need a tremendous amount of labeled data for training.  We present an operational CE model that utilizes pointer--generator networks  and bidirectional long short-term memory  units  to retrieve concepts from general discourse textual material.\footnote{We adopt Halliday's notion of classifying nominal group as definition of a concept.} Furthermore, since for a generic, domain-independent concept extraction model we need a sufficiently large training corpus that covers a vast variety of topics and no such annotated corpora are available, we opt for distant supervision to create a sufficiently large and diverse dataset. Distant supervision consists in automatic labeling of potentially useful data by an easy-to-handle  algorithm to obtain an annotation which is likely to be noisy but, at the same time, to contain enough information to train a robust model . Two labeling schemes are considered. Experiments carried out on a dataset of 250K+ Wikipedia pages show that copies of our model trained differently and joined in an ensemble significantly outperform standard techniques and, when used on top of DBpedia Spotlight, further improve its performance by nearly 10\%.  
"," Concept extraction is crucial for a number of downstream applications. However, surprisingly enough, straightforward single token/nominal chunk--concept alignment or dictionary lookup techniques such as DBpedia Spotlight still prevail. We propose a generic open-domain OOV-oriented extractive model that is based on distant supervision of a pointer--generator network leveraging bidirectional LSTMs and a copy mechanism. The model has been trained on a large annotated corpus compiled specifically for this task from 250K Wikipedia pages, and tested on regular pages, where the pointers to other pages are considered as ground truth concepts. The outcome of the experiments shows that our model significantly outperforms standard techniques and, when used on top of DBpedia Spotlight, further improves its performance. The experiments furthermore show that the model can be readily ported to other datasets on which it equally achieves a state-of-the-art performance.",175
"   This paper describes our socialbot for open-domain conversation, Chirpy Cardinal, built as a research platform during the 2019 Alexa Prize competition.  During the competition, US-based Amazon Alexa users could give an invocation phrase  to be connected to one of the competing socialbots . After receiving a minimal orientation phrase at the beginning of the conversation, the user talks to the socialbot  until they decide to end the conversation -- at which point, they are invited to provide a rating and comment.  To provide a convincing user experience, an open-domain conversational agent must excel at language understanding, language generation, emotional engagement, memory, world knowledge and conversational planning, among other desirable characteristics -- an ambitious goal! Prior work within and outside the Alexa Prize competition has taken the successful strategy of pushing progress along individual skills, and forming an ensemble of sub-systems, each excelling at a singular characteristic while ignoring others.  For instance, supporting user initiative in open-domain conversations is extremely challenging, as it requires understanding the countless ways a user can take initiative, and the ability to respond to each of them with specificity.  Faced with this difficulty, when it comes to in-depth conversations, many previous dialogue systems rely primarily on bot-initiative, driving users along carefully scripted paths.  On the other hand, systems attempting higher user-initiative via non-scripted paths are likely to lead towards shallower conversations.  Thus there is a lot of room for innovation and research in trying to simultaneously achieve two or more complementary characteristics; this is a recurring theme throughout this work.  Our goal in building this socialbot was to offer a natural-sounding and emotionally engaging dialogue agent that can talk knowledgeably about a wide variety of topics, while also letting the user take as much initiative as possible.   Initiative -- the ability to drive the direction of the conversation -- has been studied extensively in the context of task-oriented dialogue.  Mixed initiative , in which the user and the bot share initiative, is an important quality of a successful dialogue system, as it provides the user a sense of agency without making them entirely responsible for suggesting new topics and directions. In order to improve on mixed initiative while still providing an acceptable conversational depth, we designed our initial system to rely heavily on system initiative, but at the same time explored several avenues to increase user initiative in a controlled fashion.  To support mixed initiative, our system has a global navigational intent classifier  and entity tracker , allowing it to track high level topic changes from both the user and the bot. Further, our response priority system  allows individual Response Generators  to interject when the user initiates a change of topic.  High-coverage world knowledge is an important component of open-domain conversation -- our bot must be able to talk about the diverse range of entities and topics that interest users, particularly if we wish to respect user initiative. We use the Alexa Knowledge Graph, The Washington Post, Reddit and Twitter as sources of up-to-date knowledge in particular domains, while ensuring high coverage by using Wikipedia and Wikidata entities as the foundation of our entity-based conversations . However, world knowledge must be delivered in a conversational style -- this is a characteristic that distinguishes a socialbot from a virtual assistant. To achieve this, we finetuned a neural generative model on the TopicalChat dataset  to obtain a conversational paraphrasing model that adapts external text into a conversational style .  A socialbot cannot focus solely on external entities -- to be truly social, it must be able to discuss personal experiences and emotions. While ELIZA-like systems  attempt this via templated repetition of user phrases, they lack the naturalness and depth of real human conversations.  Our Neural Chat module  invites the user to share their everyday experiences and current emotions, and uses a neural generative model to respond empathetically. With it, we attempt to have a deep, sustained and emotionally engaging conversation about a user's lives. In addition, our Opinion module  allows the user to express their feelings by expressing their likes and dislikes.  To foster a reciprocal atmosphere, our bot also shares its own distinct feelings, experiences and opinions.  Lastly, we note that the advent of large-scale pretrained neural generative models has substantially impacted what is possible in open-domain socialbots.  While in the last Alexa Prize competition, none of the top three socialbots used neural generation , we found current GPT-2 models  to be a key tool to support our design goals. Neural generation enables natural phrasing and emotional engagement, as well as more flexible responsiveness , supporting higher user initiative.  A limitation of neural generation methods for dialogue is deterioration in quality and consistency over a long conversation, which can be potentially overcome with symbolic constraints. We explore ways to bring the best of both worlds -- long term consistency and short term fluidity -- together.   Despite being a first-time entrant, at the end of the competition our system achieved a rating of 3.6/5.0, which is within 0.1 of the highest-ranked systems, and is capable of detailed, sustained conversations with interested users . Qualitatively, during in-person interactions with users, we observed that many innovations such as in-depth discussions of everyday life, conversational styling of informational content, and opinionated exchanges were received with expressions of pleasant surprise -- indicating our steps were in the right direction.  In , we re-examine the goals we set out to achieve, and empirically analyze our bot's successes and failures. In , we talk about the challenges we faced, the trade-offs we made, our conclusions and avenues for future work.   [t]     
"," We present Chirpy Cardinal, an open-domain dialogue agent, as a research platform for the 2019 Alexa Prize competition. Building an open-domain socialbot that talks to real people is challenging -- such a system must meet multiple user expectations such as broad world knowledge, conversational style, and emotional connection.  Our socialbot engages users on their terms -- prioritizing their interests, feelings and autonomy.  As a result, our socialbot provides a responsive, personalized user experience, capable of talking knowledgeably about a wide variety of topics, as well as chatting empathetically about ordinary life. Neural generation plays a key role in achieving these goals, providing the backbone for our conversational and emotional tone. At the end of the competition, Chirpy Cardinal progressed to the finals with an average rating of 3.6/5.0, a median conversation duration of 2 minutes 16 seconds, and a 90$^{\text{th}}$ percentile duration of over 12 minutes.",176
"   Clinical Named Entity Recognition  extracts patient information from unstructured Electronic Health Records , which is an important task for further clinical research. The main goal of CNER is to identify clinical terminologies in EHRs, such as diseases, symptoms, treatments, exams and body parts. Accurate identification of these clinical concepts can provide effective decision support for patient care and treatment. Compared to English texts, CNER in Chinese texts is more difficult since Chinese EHRs are recorded without explicit word delimiters. In recent years, CNER has attracted considerable research efforts, and many methods have proposed in the literature. Most of them are deep learning methods.  Although many advanced models have been developed for CNER, their performance still heavily depends on the manually-annotated training data. Labeling EHRs is usually time-consuming and expensive because EHRs involve many complex clinical terminologies, and only labelers with medical background are qualified for clinical annotation. It thus becomes rather difficult to train an effective model for CNER since it requires a large number of manually-annotated clinical texts.  Active learning, which iteratively selects the most informative samples for labelers to annotate, is an effective method to reduce annotation cost. It has been widely used in many Natural Language Processing  tasks, such as text classification and event recognition. In conventional active learning, there is only one labeler and the algorithm queries the labels of the selected instances from the labeler, which always returns the ground truth of queried labels. However, in many real settings, there are multiple labelers, and they usually provide diverse quality of annotation with different costs. Obviously, a labeler which offers better overall quality will require a higher cost for each query. The overall quality of labelers can be assessed according to their previous annotation  performance. Moreover, labelers may have diverse expertise for different instances. For example, in CNER tasks, some labelers may be good at labeling diseases, while some are skilled in symptoms. Therefore, we need to consider querying which of them to annotate the selected instances so as to keep a trade-off between quality and cost.  In the past few years, active learning with multiple noisy labelers has received significant attention and achieved great success in various applications. However, many works either ignored the different expertise of multiple labelers and queried the same labeler for all instances globally or neglected the annotation costs of different labelers. Recently, two methods considering the diversity of labelers on both expertise and query costs have been proposed for classification tasks. Experimental results demonstrate the effectiveness of the two methods on selecting cost-effective queries. We thus follow the trend and focus on CNER task for the first time.  In this paper, we propose a Cost-Quality Adaptive Active Learning  method for CNER in Chinese EHRs, which selects the most cost-effective instance-labeler pairs to obtain better annotation performance with lower costs in an adaptive manner. Specifically, we first combine three sampling strategies, namely uncertainty, entropy and margin to assess the informativeness of instances. We further observe that a labeler with low quality of overall annotation can still assign accurate labels for some specific instances in real settings. Then, based on this fact, for each instance, we select a suitable labeler which offers high-quality yet cheap annotations so as to keep a balance between the annotation quality, labeling costs, and the informativeness of instances.  The main contributions of this paper can be summarized as follows:        The rest of this paper is organized as follows. Section  briefly reviews the related work on CNER and active learning. Section  presents our proposed cost-quality adaptive active learning method for Chinese CNER, followed by experimental evaluations as Section . Finally, the conclusions and potential research directions are summarized as Section .  
","  Clinical Named Entity Recognition  aims to automatically identity clinical terminologies in Electronic Health Records , which is a fundamental and crucial step for clinical research. To train a high-performance model for CNER, it usually requires a large number of EHRs with high-quality labels. However, labeling EHRs, especially Chinese EHRs, is time-consuming and expensive. One effective solution to this is active learning, where a model asks labelers to annotate data which the model is uncertain of. Conventional active learning assumes a single labeler that always replies noiseless answers to queried labels. However, in real settings, multiple labelers provide diverse quality of annotation with varied costs and labelers with low overall annotation quality can still assign correct labels for some specific instances. In this paper, we propose a Cost-Quality Adaptive Active Learning  approach for CNER in Chinese EHRs, which maintains a balance between the annotation quality, labeling costs, and the informativeness of selected instances. Specifically, CQAAL selects cost-effective instance-labeler pairs to achieve better annotation quality with lower costs in an adaptive manner. Computational results on the CCKS-2017 Task 2 benchmark dataset demonstrate the superiority and effectiveness of the proposed CQAAL.",177
"   % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  .     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. } %\footnotemark .}   In this digital era, users express their personal thoughts and opinions regarding a wide range of topics on social media platforms such as blogs, micro-blogs , and chats .   Multilingual societies like India with a decent amount of internet penetration widely adopted such social media platforms.   However, the regional language influences the proliferation of the Hindi-English Code-Mixed  data.   Sentiment analysis of these end-user data from social media is a crucial resource for commerce and governance.  However, in contrast to the classical sentiment analysis methods, which were originally designed for dealing with well-written product reviews, CM texts from social media often contain misspellings , badly cased words, letter substitutions, ambiguities, non standard abbreviations, improper use of grammar, etc.    CM poses several unseen difficulties to natural language processing  tasks such as word-level language identification, part-of-speech tagging, dependency parsing, machine translation and semantic processing.   In the last few years, a number of workshops such as Linguistic Code-Switching Workshops\footnote{https://code-switching.github.io/2020/} and shared tasks such as Mixed Script Information Retrieval   have been organized due to the emerging popularity of code-mixing.     To promote research in this area, Task 9 of SemEval-2020 was devoted to CM sentiment analysis in Twitter.   The goal of the task was to automatically classify the polarity of a given CM Twitter post into one of the three predefined categories: positive, negative and neutral. The CM languages are English-Hindi and English-Spanish; for a more detailed description of the task see .     In this paper, we present a deep learning approach, using a Recurrent Convolutional Neural Network for the task of automatic CM sentiment classification of tweets.    % The rest of the paper is structured as follows.  Section 2 provides background in brief.  Section 3 provides the system overview and Section 4 describes our approach in detail. In Section 5, we discuss the analysis and evaluation results for our system. We conclude our work in Section 6.  
","   This paper describes the participation of LIMSI\_UPV team in SemEval-2020 Task 9: Sentiment Analysis for Code-Mixed Social Media Text. The proposed approach competed in SentiMix Hindi-English subtask, that addresses the problem of predicting the sentiment of a given Hindi-English code-mixed tweet.  We propose Recurrent Convolutional Neural Network that combines both the recurrent neural network and the convolutional network to better capture the semantics of the text, for code-mixed sentiment analysis.  The proposed system obtained 0.69  in terms of F1 score on the given test data and achieved the 9th place  in the SentiMix Hindi-English subtask.",178
"  The performance of many machine learning algorithms depends on their hyper-parameters. For example, the prediction accuracy of support vector machines depends on the kernel and regularization hyper-parameters  and , and deep neural networks are sensitive to a wide range of hyper-parameters, including the number of units per layer, learning rates, weight decay, and dropout rates etc.. It is well-known that hyper-parameter settings often make the difference between mediocre and state-of-the-art performance. As a result, hyper-parameter optimization has been receiving an increasingly amount of attention in the NLP and machine learning communities. However, identifying the best model configuration is often a cumbersome process because it can involve several trials and errors before an optimal hyper-parameter setting can be found. Bayesian Optimization has emerged as an efficient framework for carrying out the model selection process, achieving impressive successes. For example, in several studies, it found better instantiations of convolutional network hyper-parameters than domain experts. The common theme is to perform a set of iterative hyper-parameter optimizations. In each round, these methods fit a hyper-parameter response surface using a probabilistic regression function such as Gaussian Process or tree-based models, where the response surface maps each hyper-parameter setting to an approximated accuracy. The learned regression model is then used as a surrogate of the response surface to explore the search space and identify promising hyper-parameter candidates to evaluate next in order to enhance validation accuracy.    While these methods have enjoyed great success compared to conventional random search and grid search algorithms for model selection, the focus of these work have largely been on optimizing for effectiveness, while ignoring the resulting model's training efficiency. Given that both prediction accuracy and model training time are important for real-world applications, models selected for effectiveness may not meet the strict real-world efficiency requirements necessary to deploy in a production environment. In addition, most of the previous methods exclusively focus on optimizing the hyper-parameters of a given model class, while ignoring other important extrinsic hyper-parameters such as training set size which can influence both speed and accuracy. For example, model training time typically grows proportionally with respect to training set size, and the prediction accuracy can also be influenced by the amount of training data used for learning. If the tolerance for inefficient model training is low, then the amount of training data should be reduced or adjusted with the rest of intrinsic hyper-parameters to meet the stringent efficiency requirements.    Given that both model effectiveness and training time are important for real-world applications, in this work, we propose a unified Bayesian Optimization framework for jointly selecting models for prediction effectiveness and training efficiency. First, we propose an objective that captures the tradeoff between these two metrics, then we demonstrate how we can jointly optimize them in a principled Bayesian Optimization framework. In addition, we account for extrinsic hyper-parameters such as training set size in the hyper-parameter optimization space. We will demonstrate this joint optimization of both measures in an enriched hyper-parameter space leads to selecting more efficient and accurate models. It is important to point out our work is fundamentally different from previous Bayesian Optimization that considers the speed of the hyper-parameter search/model selection process -- our focus is on model training efficiency, in addition to accuracy , while their focus is on hyper-parameter search efficiency. Our work can be viewed as taking an efficiency-centric view at selecting effective models. Experiments on model selection for recommendation and question answering tasks indicate models selected this way significantly improves model training efficiency while maintaining strong effectiveness as compared to state-of-the-art Bayesian Optimization algorithms.    The remainder of the paper is organized as follows: We start with a discussion of related work. Next, in Section we propose metrics for quantifying the tradeoff between prediction accuracy and training efficiency, then discuss methods for model selection based on the tradeoff metric. Section presents experimental results under different tradeoff scenarios for recommendation and question answering tasks, before concluding in Section.   
","   The performance of many machine learning models depends on their hyper-parameter settings. Bayesian Optimization has become a successful tool for hyper-parameter optimization of machine learning algorithms, which aims to identify optimal hyper-parameters during an iterative sequential process. However, most of the Bayesian Optimization algorithms are designed to select models for effectiveness only and ignore the important issue of model training efficiency. Given that both model effectiveness and training time are important for real-world applications, models selected for effectiveness may not meet the strict training time requirements necessary to deploy in a production environment. In this work, we present a unified Bayesian Optimization framework for jointly optimizing models for both prediction effectiveness and training efficiency. We propose an objective that captures the tradeoff between these two metrics and demonstrate how we can jointly optimize them in a principled Bayesian Optimization framework. Experiments on model selection for recommendation tasks indicate models selected this way significantly improves model training efficiency while maintaining strong effectiveness as compared to state-of-the-art Bayesian Optimization algorithms.",179
" Attention-based transformer networks~ are widely used for sequence modeling tasks, including language modeling and machine translation. To improve performance, models are often scaled to be either wider, by increasing the dimension of hidden layers, or deeper, by stacking more transformer blocks. For example, T5  uses a dimension of 65K and GPT-3  uses 96 transformer blocks. However, such scaling increases the number of network parameters significantly , and complicates learning, i.e., these models either require very large training corpora  or careful regularization . In this paper, we introduce a new parameter-efficient attention-based architecture that can be easily scaled to be both wide and deep.   Our ep and t-weight ransformer architecture, \arch, extends the transformer architecture of  and delivers similar or better performance with significantly fewer parameters and operations. At the heart of \arch~is the \dextra~that uses the group linear transformations  of  with an expand-reduce strategy for varying the width and depth of the \arch~block efficiently. Since GLTs are local by nature, the \dextra~uses feature shuffling, which is analogous to channel shuffling in convolutional networks , to share information between different groups. Such wide and deep representations facilitate replacing the multi-head attention and feed-forward layers in transformers with single headed attention and light-weight feed-forward layers, reducing total network parameters and operations. Importantly, unlike transformers, the \dextra~decouples the depth and width from the input size, allowing us to allocate parameters more efficiently across blocks by using shallower and narrower \arch~blocks near the input and deeper and wider \arch~blocks near the output.  We demonstrate that \arch~models achieve similar or better performance than transformer models with significantly fewer parameters and operations, on two common sequence modeling tasks,  machine translation and   language modeling. On the low resource WMT'16 En-Ro machine translation dataset, \arch~attains transformer performance using  fewer parameters. On the high resource WMT'14 En-Fr dataset, \arch~delivers better performance  with  fewer parameters than baseline transformers. Similarly, on language modeling, \arch~matches the performance of  Transformer-XL~ with  fewer parameters on the WikiText-103 dataset. Our source code is open-source and is available at: \textcolor{blue}{\url{https://github.com/sacmehta/delight}}  
"," We introduce a deep and light-weight transformer, \arch, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. \arch~more efficiently allocates parameters both  within each Transformer block using the \dextra, a deep and light-weight transformation and  across blocks using block-wise scaling, that allows for shallower and narrower \arch~blocks near the input and wider and deeper \arch~blocks near the output. Overall, \arch~networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that \arch~matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average.",180
" The deep learning community  has been looking for alternatives to recurrent neural networks  for storing information. For example, linear memory networks use a linear autoencoder for sequences  as a memory . Additional memories for RNNs like holographic reduced representations , tensor product representations  and classical associative memories  have been suggested. Most approaches to new memories are based on attention. The neural Turing machine  is equipped with an external memory and an attention process .   Memory networks  use an  attention by first mapping a query and patterns into a space and then retrieving the pattern with the largest dot product. End to end memory networks  make this attention scheme differentiable by replacing  through a , where all stored binary patterns are fixed points but the radius of attraction vanishes . However, in order to integrate Hopfield networks into deep learning architectures, it is necessary to make them differentiable, that is, we require continuous Hopfield networks .  Therefore, we generalize the energy function of  that builds on exponential interaction functions to continuous patterns and states and obtain  a new modern Hopfield network. We also propose a new update rule which ensures global convergence to stationary points of the energy .  We prove that our new modern Hopfield network typically retrieves patterns in one update step . The retrieval of patterns with one update is important to integrate  Hopfield networks in deep learning architectures,  where layers are activated only once. Surprisingly, our new update rule is also  the key-value attention  as used in transformer and BERT models . Our modern Hopfield networks can be integrated as a new layer in deep learning architectures for pooling, memory, prototype learning, and attention. We test these new layers on different benchmark datasets and tasks like immune repertoire classification.     
"," We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store  exponentially  many patterns,  retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima :  global fixed point averaging over all patterns,   metastable states averaging over a subset of patterns, and   fixed points which store a single pattern. The new update rule  is equivalent to the attention mechanism used in transformers. This equivalence enables a  characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging  via metastable states.  The new modern Hopfield network can be integrated  into deep learning architectures  as layers to allow the storage of and access to  raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning,  beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks,  where deep learning methods typically struggle,  Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \url{https://github.com/ml-jku/hopfield-layers}",181
" Search systems provide relevant documents to users who are looking for specific information through queries. A user receives a list of ranked documents ordered by search relevance, where ranking plays a crucial role to model such relevance that directly affects consequential user interactions and experience. Most search systems deal with a large amount of natural language data from queries, profiles, and documents. An effective search system requires a deep understanding of the context and semantics behind natural language data to power ranking relevance.  %Ranking is one of the most common and important components in many search and recommender systems, e.g., document retrieval , feeds recommendation , query suggestion , auto completed query ranking , etc. These applications contain rich text data, i.e., queries, user profiles and documents.  Therefore, a ranking framework that captures the text semantics can significantly boost the performance of many productions.  Traditional ranking approaches largely rely on word/phrase exact matching features, which has a limited ability to capture contextual and deep semantic information. In the recent decade, deep learning based natural language processing technologies present an unprecedented opportunity to understand the deep semantics of natural language data through embedding representation. Moreover, to enhance contextual modeling, contextual embedding such as BERT has been proposed and extensively evaluated on various NLP tasks with significant improvements over existing techniques.  However, promoting the power of BERT in ranking is a non-trivial task. The current effective approaches integrate BERT as an embedding generation component in the ranking model, with the input a concatenated string of query and document texts. BERT is then fine tuned with ranking loss. The inherent transformer layer in BERT allows direct context sharing between query words and document words, exploiting the power of contextual modeling in BERT to the greatest extent, as the query word embeddings can incorporate many matching signals in documents. This approach, in the category of interaction based models, comes with a significant challenge in online serving: a) the heavy BERT computation on the fly is not affordable in a real world search system; and b) the interaction based structure, as applied to concatenated query and document, precludes any embedding pre-computing that can reduce computation. %Since the BERT embedding generation depends on a specific online query to be concatenated with documents, the model needs to be applied to all the query/document pairs, and can only start the computation after a query is available online, xxxx %which is not feasible for industry productionalization given the strong latency concerns on most real-world search systems.  %The advantage of BERT comes from its capability of modeling strong context among text data, which is leveraged by various approached that apply BERT to the concatenation of a pair of query and document during the ranking stage.   %Previous work has investigated a two-stage ranking model that extracts embedding features through Deep NLP  components first, then incorporates the features into a ranking model. Due to the lack of end-to-end context, this approach generally has information loss that cannot fully exploit the deep semantics. In addition, embedding that requires fine-tuning with specific tasks, e.g., BERT, does not work in this approach.  % Incorporating deep NLP components, that generates non-contextual or contextual embedding, into an end-to-end ranking system efficiently for industry applications comes with several challenges. For non-contextual embeddings, xx; for contextual embedding, e.g., BERT, it works well when both source and target are processed together through the deep NLP component, a.k.a., the interaction model. However, the relevance improvement comes with the large computation time, making it hard to productionize the deep NLP ranking models.  %are able to learn embeddings of text data, and perform semantic matching. Early works  extract a text embedding by averaging the word embeddings, and then a cosine similarity between text embeddings is calculated as semantic matching features. Later, CNN  and LSTM  are applied to capture the word order information . More recently, BERT  has shown significant improvement in document ranking  by extracting deeper semantics.  However, the relevance improvement comes with the large computation time, making it hard to productionize the deep NLP ranking models. To enable an efficient BERT-based ranking model for industry use cases, we propose to use representation based structure. Instead of applying BERT to a concatenated string of query and document texts, it generates query and document embeddings independently. It then computes the matching signals based on the query and document embeddings. This approach makes it feasible for pre-computing document embedding; thus, the online system only needs to do BERT real-time computation for queries. By independently computing query and document embeddings, however, we may lose the enhancement on the direct context sharing between queries and documents at word-level. This trade-off makes it a challenge to develop a BERT-based ranking model that is both effective and efficient.  In this work, we investigated the BERT-based ranking model solution with representation-based structure, and conducted comprehensive offline and online experiments on real-world search products. Furthermore, we extended the model solution into a general ranking framework, DeText , that is able to support several state-of-the-art deep NLP components in addition to BERT. The framework comes with great flexibility to adapt to various industry use cases. For example, BERT can be applied for ranking components that have rich natural language paraphrasing; CNN can be applied when ease of deployment is a top concern for a specific system.  Beyond the ranking framework, we also summarized experience on developing an effective and efficient ranking solution with deep NLP technology, and how to balance effectiveness and efficiency for industry usage in general. We shared practical lessons of improving relevance performance while maintaining a low latency, as well as general guidance in deploying deep ranking models into search production.  %We have proposed two strategies: for heavy models such as BERT, the target side embeddings are pre-computed; for light models such as CNN, we perform real-time inference for both source and target data.  We conduct thorough experiments and deploy DeText in LinkedIn's three vertical searches: people search, job search, help center search.  %The BERT based ranking algorithm is not limited to document ranking in search; rather, with a little changes, it can be applied into many other ranking components, such as recommender systems , query auto completion ranking , etc.  Therefore, we extend our algorithm to a ranking framework.  In order to meet the requirements of various ranking systems, we enable flexibility in many modules, such as text embedding generation, learning-to-rank loss, etc.  %For example, BERT can be applied for ranking components that have rich natural language paraphrasing; CNN is available when ease of deployment is a top priority for a specific system.   %This paper focuses on designing a general deep NLP based ranking framework for productions.  Two main challenges are identified:  Flexibility.  Different productions have different requirements of ranking models. For example, in some tasks with mainly natural language queries, deep contextual understanding is essential to understand user intents; for other tasks, shallow understanding is enough.   Effectiveness and Efficiency.  The production ranking models are usually strong and robust models, where the whole process is finished in only a few dozens of milliseconds. In contrast, deep neural networks are notorious for the heavy computation: the time complexity of LSTM models grows by at least a factor of  .  %Motivated by these challenges, we design a general neural ranking framework for practical use cases in industry, which is referred to as DeText . In order to meet the requirements of various ranking productions, we enable great flexibility in DeText, with respect to input data format, language genre, latency budget, etc. For example, BERT is applied for help center ranking that has a lot of natural language queries with rich paraphrasing; CNN is applied for query auto completion which should be finished within several milliseconds.  More details regarding the model flexibility are listed in Section .    %Another important goal is to balance the efficiency and effectiveness so that DeText is a practical solution for industry usage. Firstly, we choose the representation based methods over the interaction based methods , because the latter has larger computation complexity, and does not allow document pre-computing.  Secondly, in order to outperform the strong production models, we carefully handle the powerful traditional features that are used in production models, which are combined with deep features.  In contrast, most previous work  focus on the deep neural networks without considering the hand-crafted features. Thirdly, we can choose to have multiple source/target fields to ensure robust results.  All of these three factors together enable the usage of compact neural networks to minimize the latency while still achieving strong relevance performance.  %Another important goal is to balance the efficiency and effectiveness so that DeText is a practical solution for industry usage. In this paper, we share many practical solutions, such as representation based methods to reduce latency, combining hand-crafted features with deep models to boost relevance performance.  %Firstly, we choose the representation based methods over the interaction based methods , because the latter has larger computation complexity, and does not allow document pre-computing.  Secondly, in order to outperform the strong production models, we carefully handle the powerful traditional features that are used in production models, which are combined with deep features.  In contrast, most previous work  focus on the deep neural networks without considering the hand-crafted features. Thirdly, we can choose to have multiple source/target fields to ensure robust results.  All of these three factors together enable the usage of compact neural networks to minimize the latency while still achieving strong relevance performance.  The contribution of this paper is summarized below:      }     . To meet the requirement of ranking modules in different productions, great flexibility is enabled in the DeText framework.  Meanwhile, practical solutions are incorporated to ensure the balance between efficiency and effectiveness. Since this is a general ranking framework,                %. DeText is designed to reach a balance between effectiveness and efficiency: representation based methods, traditional feature handling, multiple source/target fields, etc.      %. DeText models are deployed in three vertical searches at LinkedIn.  We use two deployment strategies: document embedding pre-computing for BERT models, and two pass ranking for CNN models.   
"," Ranking is the most important component in a search system. Most search systems deal with large amounts of natural language data, hence an effective ranking system requires a deep understanding of text semantics. Recently, deep learning based natural language processing  models have generated promising results on ranking systems. BERT is one of the most successful models that learn contextual embedding, which has been applied to capture complex query-document relations for search ranking. However, this is generally done by exhaustively interacting each query word with each document word, which is inefficient for online serving in search product systems. In this paper, we investigate how to build an efficient BERT-based ranking model for industry use cases. The solution is further extended to a general ranking framework, DeText, that is open sourced and can be applied to various ranking productions. Offline and online experiments of DeText on three real-world search systems present significant improvement over state-of-the-art approaches.  %There are two common challenges in ranking systems: understanding the semantics of text data, and maintaining a good balance between effectiveness and efficiency.  To this end, we propose a general deep text  ranking framework to enable deep understanding of textual data that is practical for industry applications.  Our design principles are:  A general deep learning framework with the flexibility to meet requirements of different ranking systems.  For example, for search engines that require ease of deployment, CNN is available; for the tasks where deep contextual understanding is crucial, BERT can be used to extract text semantics.  To our best knowledge, this is the first work to successfully train a representation based BERT ranking model and launch to production.  Reaching a good balance between effectiveness and efficiency to meet industry requirements. Practical solutions are used, such as representation based methods, handling traditional features, etc, in order to optimize the relevance performance with a relatively small neural network.  %For example, we choose the representation based methods over interaction based methods, since the former is more computationally efficient.  %The DeText models are evaluated in two tasks: document ranking and query auto completion ranking.  In experiments, we observe DeText models are robust on different text genres, significantly outperforming strong production baselines. We designed two deployment strategies for CNN and BERT models, and successfully deployed them into three document ranking systems at LinkedIn.",182
" % Computer Society journal papers do something a tad strange with the very % first section heading . They place it % ABOVE the main text! IEEEtran.cls currently does not do this for you. % However, You can achieve this effect by making LaTeX jump through some % hoops via something like: % %[0pt][0pt]% %  {\parbox{\columnwidth}{
", %\boldmath %The abstract goes here. %,183
" % COMPLETED.   represent structured collections of facts describing the world in the form of typed relationships between entities. These collections of facts have been used in a wide range of applications including Web search), cancer research, and even entertainment. However, most  on the Web are far from being complete. For instance, the birth place of  of the persons in Freebase and  of the persons in DBpedia is not to be found in the respective . In addition, more than  of the scientists in DBpedia are not linked to the predicate that describes what they are known for. Identifying such missing links is referred to as link prediction.  approaches map  to continuous vector spaces and have been proven to be highly effective and efficient at addressing the task of link prediction.   In this paper, we propose \approach, a simple but effective new  approach. \approach is a complex-valued convolutional neural model that learns complex-valued vector representations of a given  by combining a 2D convolution operation with a Hermitian inner product. The motivation behind our approach lies in the following considerations:       have demonstrated recognition accuracy better than or comparable to humans in several visual recognition tasks, including image recognition, object detection and semantic segmentation. Parallel to the successful application of  in computer vision,  leverages a multi-layer  for learning continuous vector representations of  and reaches a state-of-the-art performance in link prediction.     %  have shown its usefulness in various applications --including faster learning on memorization tasks, learning equivariant 3D-rotation for cloud processing.      has been proven to be an effective technique for link prediction.   We evaluate our approach against 37 state-of-the-art approaches on four benchmark datasets often used in the literature. Overall, our results suggest that \approach outperforms current state-of-the-art approaches , in terms of  and Hits at N .% -- standard measures for the link prediction task.  %structure of the paper % The rest of this paper is structured as follows: % We provide an overview of the state of the art in   in~\Cref{sec:related work}. Thereafter, the notation and the preliminaries are presented in~\Cref{sec:preliminaries}. Next, we introduce \approach in~\Cref{sec:approach}. In~\Cref{sec:experiments}, we explicate the research question and experimental settings.~\Cref{sec:results} reports the results of conducted experiments. Finally, we conclude with a discussion in ~\Cref{sec:conclusion}.      
"," In this paper, we study the problem of learning continuous vector representations of knowledge graphs for predicting missing links. We present a new approach called \approach, which infers missing links by leveraging the composition of a 2D convolution with a Hermitian inner product of complex-valued embedding vectors. We evaluate \approach against state-of-the-art approaches on the WN18RR, FB15K-237, KINSHIP and UMLS benchmark datasets. Our experimental results show that \approach achieves a  performance superior to that of state-of-the-art approaches such as RotatE, QuatE and TuckER on the link prediction task on all datasets while requiring at least 8 times fewer parameters. We ensure the reproducibility of our results by providing an open-source implementation which includes the training, evaluation scripts along with pre-trained models at {\url{https://github.com/conex-kge/ConEx}.}   %",184
" Bipolar disorder  is a recurrent chronic mental health condition which occurs in approximately 1\% of the global population . It is characterised by episodes of low and high mood which cause significant interference with everyday life. Borderline personality disorder  is characterised by a long-term pattern of constantly variable mood, self-image and behaviour. Although BD and BPD are two very different conditions they share some similar symptoms such as mood instability and impulsive behaviour . A recent study  reported the high prevalence of comorbidity between the two conditions, with up to 21.6\% of individuals with BD found to have comorbid BPD. As a result they can be difficult to distinguish, but accurate diagnosis is crucial as they require different treatment . Standard diagnostic assessment involves a psychiatrist asking a series of questions about symptoms and the person has to retrospectively describe their account of these symptoms. The success of the assessment also relies on how the psychiatrist interprets both the verbal and non-verbal cues drawn from the person's responses. In this work, we aim to develop a method that extracts cues automatically from interviews conducted in a non-clinical setting, to assist the existing assessment framework, which is expensive and subjective.  Recent studies have explored data driven approaches to automatically screen patients, incorporating features extracted from multiple modalities in clinical interviews, showing diagnostic value for mental health conditions such as depression and bipolar disorder .  finds the performance of automatic mood detection to be much better in clinical interactions than in personal conversations, and there are significant differences in the features important to each type of interaction. While existing studies of BD  have focused on recognising mood episodes, the distinction between BD and BPD remains understudied. In this paper, we aim to bridge this gap by presenting a multi-modal  dataset containing interviews in a non-clinical setting involving individuals with a diagnosis of BD or BPD, and study the automatic assessment of the two mental health conditions.   Motivated to study the interaction between the interviewer and participant during the course of an interview from different aspects , we investigate features extracted from different modalities. Path signatures, initially introduced in rough path theory as a branch of stochastic analysis, has been shown to be successful in a range of machine learning tasks involving modelling temporal dynamics . We propose to apply path signatures for summarising features extracted from each utterance, sentence and speaker-turn into interview-level feature representations, given its ability to naturally capture the order of events. By doing so, we automatically include more non-linear prior knowledge in our final feature set, which leads to effective classification, even with a simple linear classifier.   The contributions of this work are as follows:  We present a new non-clinical interview dataset involving BD and BPD patients; , We investigate different feature types and propose using path signatures as a novel approach of summarising turn-level features;  We demonstrate a good linear model can be learnt for three classification tasks, and provide insights into the distinction between BD and BPD by analysing the importance of the selected features.    
"," Bipolar disorder  and borderline personality disorder  are both chronic psychiatric disorders. However, their overlapping symptoms and common comorbidity make it challenging for the clinicians to distinguish the two conditions on the basis of a clinical interview.  % Recent studies have explored data driven approaches to automatically screen patients, incorporating features extracted from clinical interviews, showing diagnostic value for mental health conditions such as depression and bipolar disorder.  In this work, we first present a new multi-modal dataset containing interviews involving individuals with BD or BPD being interviewed about a non-clinical topic . We investigate the automatic detection of the two conditions, and demonstrate a good linear classifier that can be learnt using a down-selected set of features from the different aspects of the interviews and a novel approach of summarising these features. Finally, we find that different sets of features  characterise BD and BPD, thus providing insights into the difference between the automatic screening of the two conditions.",185
"  Despite impressive improvements in neural machine translation , training a large multilingual NMT model with hundreds of millions of parameters usually requires a collection of  at a large scale, on the order of millions or even billions of aligned sentences~ for supervised training. Although it is possible to automatically crawl the web~ to collect parallel sentences for high-resource language pairs such as German-English and Chinese-English, it is often infeasible or expensive to manually translate large amounts of documents for low-resource language pairs, e.g., Nepali-English, Sinhala-English~. Much recent progress in low-resource machine translation, has been driven by the idea of  , also known as ~, which aims at training one single NMT to translate between multiple source and target languages. Typical UMT models leverage either a single shared encoder or language-specific encoders to map all source languages to a shared space, and translate the source sentences to a target language by a decoder. Inspired by the idea of UMT, there has been a recent trend towards learning language-invariant embeddings for multiple source languages in a shared latent space, which eases the cross-lingual generalization from high-resource languages to low-resource languages on many tasks, e.g., parallel corpus mining~, sentence classification~, cross-lingual information retrieval~, and dependency parsing~, just to name a few.  The idea of finding an abstract ``lingua franca'' is very intuitive and the empirical results are impressive, yet theoretical understanding of various aspects of universal machine translation is limited. In this paper, we particularly focus on two basic questions:               Toward answering the first question, we show that in a completely assumption-free setup on the languages and distribution of the data, it is impossible to avoid making a large translation error on at least one pair of the translation tasks. Informally we highlight our first theorem as follows, and provide the formal statements in Theorems and .  [Impossibility, Informal]  There exist a choice of distributions over documents from different languages, s.t.\ for any choice of maps from the language to a common representation, at least one of the translation pairs must incur a high cost. In addition, there is an inherent tradeoff between the translation quality and the degree of representation invariance w.r.t.\ languages: the better the language invariance, the higher the cost on at least one of the translation pairs.   To answer the second question, we show that under fairly mild generative assumptions on the aligned documents for the pairwise translations, it is possible to not only do well on all of the pairwise translations, but also be able to do so after  aligned documents of a  number of languages, rather than a  one. We summarize the second theorem as follows, and provide a formal statement in Theorem.  [Sample complexity, Informal] Under a generative model where the documents for each language are generated from a ``ground-truth'' encoder-decoder model, after seeing aligned documents for a  number of pairs of languages, we can learn encoders/decoders that perform well on any unseen language pair.     We first introduce the notation used throughout the paper and then briefly describe the problem setting of universal machine translation.   We use  to denote the set of all possible languages, e.g., . For any language .} be the set of  source languages and  to mean the marginal distribution over sentences from . Likewise we use  to denote the corresponding marginal distribution over sentences from . Finally, for two sets  and , we use  to denote the disjoint union of  and . In particular, when  and  are disjoint, their disjoint union equals the usual set union, i.e., . 
"," The goal of universal machine translation is to learn to translate between any pair of languages, given a corpus of paired translated documents for  of all pairs of languages. Despite impressive empirical results and an increasing interest in massively multilingual models, theoretical analysis on translation errors made by such universal machine translation models is only nascent.   In this paper, we formally prove certain impossibilities of this endeavour in general, as well as prove positive results in the presence of additional  structure of data.   For the former, we derive a lower bound on the translation error in the many-to-many translation setting, which shows that any algorithm aiming to learn shared sentence representations among multiple language pairs has to make a large translation error on at least one of the translation tasks, if no assumption on the structure of the languages is made.   For the latter, we show that if the paired documents in the corpus follow a natural  generative process, we can expect a natural notion of ``generalization'': a linear number of language pairs, rather than quadratic, suffices to learn a good representation. Our theory also explains what kinds of connection graphs between pairs of languages are better suited: ones with longer paths result in worse sample complexity in terms of the total number of documents per language pair needed.   We believe our theoretical insights and implications contribute to the future algorithmic design of universal machine translation.",186
" Mining topics from texts is significant for various applications of natural language processing, e.g., text classification, sentiment analysis, and recommender systems. As one of the most popular approaches for discovering latent topics, topic modeling  is capable of producing interpretable results. Generally, the dominant methods for parameter estimation in topic models are variational inference  and Gibbs sampling , both of which, however, require complex re-derivation when there is any minor changes to the model structure. Moreover, with the growth of data scale, the generative process is getting tricky and expensive, which leads to mathematically arduous derivation and high computational cost in training. These limitations make it difficult to extend the models to new variations flexibly.  With the development of deep learning, variational auto-encoder   has provided another promising solution for topic modeling. Benefiting from the flexibility of neural networks, the VAE framework is competent to learn complicated non-linear distributions and is convenient to be applied to various tasks. Furthermore, by using the back-propagation for optimization, VAE is highly efficient in training when compared with the models based on variational inference or Gibbs sampling. Considering the above advantages, several models built on VAE have been proposed, such as neural variational document model  , neural variation latent Dirichlet allocation  , Gaussian softmax model  , Dirichlet variational auto-encoder  , and neural variational correlated topic modeling  . Although the VAE-based models reduce the computational cost impressively, they still suffer from the feature sparsity problem in short texts. In this case, the number of word occurrences in each text is relatively small, while the vocabulary corresponding to the corpus is large and the range of topics is broad.  To alleviate the above issue, many Bayesian approaches specific to short texts have been proposed . Nonetheless, the above models all resort to Gibbs sampling or variational inference and hence incur the problems as mentioned before. In recent years, models built on VAE are also introduced for short texts, such as Graph-based inference network for the biterm topic model   and neural sparsemax topic model  . However, learning context information is still challenging in these models due to significant word non-overlap in short texts. Relatedness information between word pairs may not be fully captured owing to the lack of word-overlap between such short messages.  In this paper, we propose a VAE-based topic model for short texts, where the context information for each text is effectively enhanced. Firstly, as can be observed, a short text generally covers only a subset of topics due to the limited text length. Therefore, we propose to filter irrelevant topics by setting a  for each topic, encouraging each short text to focus on some salient topics. Through this way, the topic inference range is narrowed down and thus the topic sparsity can be achieved indirectly. Secondly, we incorporate pre-trained word embeddings into our model to explicitly enrich the context information. Specifically, we model each topic by a multivariate Gaussian distribution or a Gaussian mixture distribution in the embedding space, through which the relatedness of synonymous word pairs can be effectively inferred regardless of word non-overlap in short texts. In this way, our model can discover more interpretable topics than other topic models. We name the proposed model as Context Reinforced Neural Topic Model  and conclude the main contributions of our work as follows:   for each topic to filter irrelevant topics, CRNTM narrows down the topic inference space and achieves topic sparsity indirectly.   The rest of this paper is organized as follows. We discuss relevant research work in Section , and detail our proposed model in Section . Experimental settings and results are presented in Section . Finally, we draw the conclusion in Section .  
"," As one of the prevalent topic mining tools, neural topic modeling has attracted a lot of interests for the advantages of high efficiency in training and strong generalisation abilities. However, due to the lack of context in each short text, the existing neural topic models may suffer from feature sparsity on such documents. To alleviate this issue, we propose a Context Reinforced Neural Topic Model , whose characteristics can be summarized as follows. Firstly, by assuming that each short text covers only a few salient topics, CRNTM infers the topic for each word in a narrow range. Secondly, our model exploits pre-trained word embeddings by treating topics as multivariate Gaussian distributions or Gaussian mixture distributions in the embedding space. Extensive experiments on two benchmark datasets validate the effectiveness of the proposed model on both topic discovery and text classification.",187
" Knowledge Based Systems:\\ Systems that incorporate human expertise for making decisions are knowledge-based systems . Traditionally a knowledge-based system consists of a knowledge base which is data suitably collected and organised by human experts in various fields, inference engine - that relies on the knowledge base for decision making, a working memory to handle operations. The inference engine can be rule-based, case-based, etc.\\ Deep Neural Networks:\\ Deep neural networks, on the other hand, is more about statistical modelling that relies on massive amounts of data to find statistical patterns, non-linear relationships to be able to match the prediction patterns from a given training set. It relies on these patterns to infer conclusions about new data as well.\\  Time-series models:\\ A Recurrent neural network is a class of neural network that deals with the prediction of temporal sequences. Long-Short term memory , Gated Recurrent Units are some of the Recurrent neural network architectures that are used for time series forecasting.\\\\ Sequence to Sequence models:\\ Sequence to sequence models aims to translate a fixed-length input sequence to a fixed-length output sequence where the length of the input and output may differ. It mainly has three parts: the encoder, intermediate vector and the decoder. In the encoder, several stacks of recurrent units  are combined such that each unit accepts an input element from the sequence and propagates it, thus forming an intermediate hidden state. This information in the hidden state is consumed by the decoder part of the network that in turn consists of sequences of recurrent units that produce a sequence of outputs.  Although the Deep neural networks have shown promising performance in several fields, there exist areas like interpretability, reasoning in which they lack and hence needs attention. On the other hand, Expert Systems are built on top of the characteristics which the Deep neural networks lack. Hence there can be ways where we can leverage the strengths of both systems by various principles. This paper discusses some of the techniques of integrating expert knowledge to Deep Neural Networks to attain a kind of synergy between them.    
"," In recent years, with the advent of massive computational power and the availability of huge amounts of data, Deep neural networks have enabled the exploration of uncharted areas in several domains. But at times, they under-perform due to insufficient data, poor data quality, data that might not be covering the domain broadly,  etc. Knowledge-based systems leverage expert knowledge for making decisions and suitably take actions. Such systems retain interpretability in the decision-making process. This paper focuses on exploring techniques to integrate expert knowledge to the Deep Neural Networks for sequence-to-sequence and time series models to improve their performance and interpretability.",188
" A heuristic approach to automated test case generation  from formal requirements specifications known as  {. The reliability of LBTest for producing correct test results depends crucially on the correctness of this learning algorithm. So we give a formal definition of  IKL and prove its correctness. The IKL algorithm involves a number of optimisations necessary to achieve scalability of testing for large software systems.  We discuss these optimisations from the perspective of learning and testing.  The problems of coverage, and termination criteria for black-box testing, are complex and different solutions have been proposed.  In LBT, convergence of learning can sometimes be used as a criterion to terminate testing. However, heuristics are needed  to estimate convergence in the context of black box testing. We will empirically evaluate the reliability of a simple heuristic for IKL.   In the remainder of Section 1, we discuss the general paradigm of LBT, and specific requirements on learning for  efficient testing of reactive systems. In Section 2, we review some essential mathematical preliminaries. In Section 3, we present the  architecture of the IKL learning algorithm and its main components. These three main components are  defined and analysed in detail in  Sections 4, 5 and 6. In Section 4, we consider a learning algorithm for families of DFA which  supports  incremental learning and projection . In Section 5, we consider integrating a family of DFA  into a single Kripke structure using a subdirect product construction.   In Section 6, we consider an efficient minimisation algorithm for  deterministic Kripke structures based on Hopcroft's DFA minimisation algorithm . This is needed by the IKL algorithm to produce hypothesis models that can be efficiently model checked.  In Section 7, we empirically evaluate a black box heuristic to detect convergence of IKL, that can be used as a test termination criterion. Finally, in Section 8 we draw some conclusions and suggest prospects for further research on learning and testing.     The basic LBT paradigm requires three components: \vskip 4pt SSo_{n+1}S does not satisfy ) then  was a {M_ni_{n+1} is not wasted. We return to Step 1 and apply the learning algorithm once again to  pairs  to infer a refined model  of . \vskip 4pt o_{n+1}S1 \dotsM_0M_1M_2\ldotsSni_{n+1}i_{n+1}\ReqL by  should reuse as much information as possible about the previous approximation  .  Incremental learning algorithms are necessary for two reasons. \vskip 4pt  of an SUT for testing a requirement  raises the question of the  relative efficiency of different types of queries . We have already seen that in LBT, test cases can be generated by  model checking, by active learning, or by some other process entirely such as random querying.  As indicated in  above, the overhead of SUT execution time to answer an individual query can be large compared with the execution time of learning and model checking. There are examples of industrial systems where this execution time is of the order  of minutes. So realistically, queries should be seen as ``expensive''. From the viewpoint of relevance therefore, as many queries as possible should be derived from model checking the hypothesis automaton, since these queries are all based on checking the requirements . Conversely as few queries as possible should be derived from the active learning algorithm.  %.  Active learning queries have no way to reference the requirement , and therefore can only uncover an SUT error by accident.  Furthermore, active learning queries may explore parts of the SUT which are irrelevant to checking , thereby  leading the search for errors in a fruitless direction. Ideally, { When we consider the output variables of the SUT  that appear in a specific formal black box requirement , we often see just a small subset of the set of all output variables of .  This observation points to a powerful abstraction technique for learning that can be termed  {.   Like incremental learning, projection is another abstraction method  that concentrates on learning only the relevant SUT behavior needed to test the requirement . Essentially, projection  involves learning a quotient model of the SUT by observing just the output variables appearing in . Since quotient models of  may be dramatically smaller than  itself, the time needed for learning and testing may be considerably reduced.  Therefore, projection seems to be an essential component of a scalable LBT system. Indeed, the combination of incremental learning  and projection seems to be particularly powerful. The IKL algorithm incorporates  both these features, and they will be discussed in further detail in Sections 3 and 4.   Several previous works,  have considered a combination of learning and model checking to achieve testing and/or formal verification of reactive systems.  Within the model checking community the verification approach known as  {  and other abstraction techniques specifically chosen to achieve scalable testing and faster error discovery .  In practise, most of the well-known classical regular inference algorithms such as L*  or ID   are designed for complete rather than incremental learning. Among the much smaller number of known incremental learning algorithms, we can mention the RPNII algorithm  and the IID algorithm  which learn Moore automata, and the ICGE algorithm   which learns Mealy automata over abstract data types. No algorithm which combines incremental learning and projection has been published in the literature. The problem of integrating active learning queries with model checker generated  queries  has also not been considered.  Thus:  the {, and  its {)\mathcal O.  Our generalisation of Hopcroft's DFA minimisation algorithm to deterministic Kripke structures in Section 6 is fairly simple and straightforward. Nevertheless, this algorithm has not been previously published in the literature, and represents another novel contribution.    
"," Learning-based testing  is an emerging methodology to automate iterative black-box requirements testing of software systems. The methodology involves combining model inference with model checking techniques. However, a variety  of optimisations on model inference are necessary in order to achieve scalable testing for large systems.  In this paper we describe the IKL learning algorithm which is an active incremental learning algorithm for deterministic Kripke structures.  We formally prove the correctness of IKL. We discuss the optimisations it incorporates to achieve scalability of testing. We also evaluate a black box heuristic for test termination based on convergence of IKL learning.",189
"  	  Since early attempts that pretrain a backbone model  on large-scale dataset  and then transfer the knowledge to numerous computer vision tasks, pretraining has become a hallmark of the success of deep learning. More recently, the volume of transformer-based and Bert-style pretraining models  has grown tremendously in the research field of natural language processing and has achieved state-of-the-art performance in various NLP tasks. Likewise, the success of Bert-style pretraining techniques has been transferred to the research field of the intersection of vision and language .    %--------------------------------fig-------------------------  %--------------------------------fig end---------------------  Despite the significant progress that recent methods have made over the initiative work ViLBert , part of their success can be traced back to the introduction of in-domain pretraining datasets besides the Conceptual Caption  dataset. By in-domain, we refer to those datasets used in both pretraining and downstream tasks, such as MSCOCO , and Visual Genome .  However, out-of-domain pretraining,  datasets and transferring the learned knowledge into downstream tasks with unkown data distributions, can be an essential research topic.  In this paper, we focus on out-of-domain pretraining and learning generic representations as the ViLBert does.      A fundamental requirement for out-of-domain transfer learning is to mitigate the biases from the pretraining data , which may be useful for the in-domain testing but harmful for out-of-domain testing  due to the spurious correlation .  To verify such existence of the correlation biases, we follow  to conduct a toy experiment on Conceptual Caption dataset. We observe that the conditional probability of   given the   is large, |\textrm{instrument}) = 5.98\% can be adjusted to   with a do operator.  The essence of deconfounding is to control the condition  from being affected by other potential confounders when assessing the effect on the outcome  given the condition, .  In this way, the pure association-based pretraining becomes to the causal intervention-based pretraining.  We note that our goal is not performing theoretically causal inference but learning generic and de-biased visio-linguistic representations that can well generalize to downstream tasks with unknown data distributions.   We are particularly targeting at the Bert-style pretraining models and the context-based proxy tasks for supervision, such as masked language/object modeling . Context-based proxy tasks solely care about association, , which refers to 	extbf{Deconfounded Visio-Linguisitic Bert}. DeVLBert is designed as model-agnostic and can be easily encapsulated into any other Bert-style models.    We conduct in-depth experiments to discuss the performance of the proposed DeVLBert architectures.  Pretraining is performed on the Conceptual Caption dataset which most downstream tasks are not built on,  	} and discuss the empirical performance on several downstream tasks. The advantages of the DeVLBert are demonstrated by quantitative experiments, ablation studies, and case studies.   	  	            
","  In this paper, we propose to investigate the problem of out-of-domain visio-linguistic pretraining, where the pretraining data distribution differs from that of downstream data on which the pretrained model will be fine-tuned. Existing methods for this problem are purely likelihood-based, leading to the spurious correlations and hurt the generalization ability when transferred to out-of-domain downstream tasks. By spurious correlation, we mean that the conditional probability of one token  given another one can be high  without robust  relationships between them. To mitigate such dataset biases, we propose a Deconfounded Visio-Linguistic Bert framework, abbreviated as DeVLBert, to perform intervention-based learning. We borrow the idea of the backdoor adjustment from the research field of causality and propose several neural-network based architectures for Bert-style out-of-domain pretraining. The quantitative results on three downstream tasks, Image Retrieval , Zero-shot IR, and Visual Question Answering, show the effectiveness of DeVLBert by boosting generalization ability.",190
"  % Thanks to the development of generative modeling, algorithmic music generation is made possible. % In recent years, instead of relying on rule-based systems or plain time-series analysis, we have seen work using recurrent networks or an attention-based model to generate music that is comparable to a human professional. % Despite the capacity of these models, the lack of interpretability remains as the main obstacle for controllable music generation ---in particular polymonic music with much richer structures.  With the development of artificial neural networks, deep learning has become one of the most popular techniques for automated music generation. In particular, we see recurrent and attention-based models being able to generate creative and human-like music without heavily handcrafted rules . %compared to traditional time-series models and rule-based algorithms. % 鏉╂瑩鍣风拠纾l濮ｆ敂ulebase瀵缚顫eviewer閺璇插毊娴滃棴绱濇潻娆愮壉鐠囧瓨娲跨广垼顫囬妴  However, the main drawback of these deep generative models is that they behave like ``black boxes閳, and it is difficult to interpret the musical meaning of their internal latent variables . Consequently, it remains a challenging task to control the generation process . This limitation restricts the application scenario of the powerful deep generative models.   In this paper, we improve the model interpretability for music generation via constrained representation learning. Inspired by the content-style disentanglement idea , we enforce the model to learn two fundamental factors of polyphonic music: chord  and texture . The former refers to the representation of the underlying chord progression, and the latter includes chord arrangement, rhythmic pattern, and melody contour. The current design focuses on learning 8-beat long piano composition segments under a variational autoencoder  framework.   The core of the model design lies in the encoder. We incorporate the encoder with two inductive biases for a successful chord-texture disentanglement. The former applies a rule-based chord recognizer and embeds the information into the first half of the latent representation. The latter regards music as 2-D images and uses a chord-invariant convolutional network to extract the texture information, storing it into the second half of the latent representation. As for the decoder, we adopt the design from PianoTree VAE , an architecture that can reconstruct polyphonic music from the latent representation in a hierarchical manner.  We further show that the interpretable representations are general-purpose, empowering a wide spectrum of controllable music generation. In this study, we explore the following three scenarios: [leftmargin=*, itemsep=0pt, parsep=1ex]      by swapping the chord and texture factors of different pieces of music, which can help us re-harmonize or re-arrange a music piece following the style of another piece.      by sampling the texture factor while keeping the chord factor, which is analogous to the creation of ``Theme and Variations閳 form of composition. %      by interpolating the latent space of chord representation while keeping the texture. This task is closely related to the ``conceptual blending''  idea in harmony analysis.      by predicting the texture factor given the melody using a downstream encoder-decoder generative model. % This task is similar to the creation of ``cover songs閳.   In sum, the contributions of our paper are as follows: [leftmargin=*, itemsep=0pt, parsep=1ex,topsep=0pt]%,topsep=0pt,parsep=0pt,itemsep=0pt,partopsep=0pt]          
"," % While deep generative modeling has become promising in many domains, it remains a challenging task to algorithmically compose polymeric music,  essentially hindered by its rich structure. % Inspired by the recent work of disentanglement of factors of variations, we develop a novel architecture, under the VAE framework, that not only disentangles the chord and texture of an input polymeric segment, also provides a generation pathway leading to plausible music style transfer and analogy. % Through a wide spectrum of task validations, we show that the chord-texture resulted from our model enables several tasks including compositional style transfer, texture variation, chord progression interpolation and accompaniment arrangement. % By both automatic metrical and human-based evaluation, our method achieves the state-of-the-art quality on the music generation. While deep generative models have become the leading methods for algorithmic composition, it remains a challenging problem to control the generation process because the latent variables of most deep-learning models lack good interpretability. Inspired by the content-style disentanglement idea, we design a novel architecture, under the VAE framework, that effectively learns two interpretable latent factors of polyphonic music: chord and texture. The current model focuses on learning 8-beat long piano composition segments. We show that such chord-texture disentanglement provides a controllable generation pathway leading to a wide spectrum of applications, including compositional style transfer, texture variation, and accompaniment arrangement. Both objective and subjective evaluations show that our method achieves a successful disentanglement and high quality controlled music generation.\!\!\footnote{Code and demos can be accessed via \url{https://github.com/ZZWaang/polyphonic-chord-texture-disentanglement}}",191
"  Humans learn to use language  over the course of their lives from the interactions they have with the world and other people. Yet, the prevailing dominant paradigm in natural language processing  research is to build a fixed dataset from which to train a model and then freeze it, without any ability for the model to interact with humans using language at training time at all. While we need such  interaction in order to study human-machine communication to its full extent, constraints usually inhibit such research.  Firstly, conducting such experiments can be costly.  %, %for example research budgets for paying crowdworkers mean that data will have a limit. Many datasets in NLP are collected with crowdsourcing, whereby one pays the crowdworkers to perform interaction and annotation tasks. This leads to several issues, not least that  research budgets for paying crowdworkers mean that data will have a limit. %collecting a large amount of data is difficult  %Secondly, distribution as they are only motivated by money, not by actual interest in the dialogues themselves. Secondly, as crowdworkers are motivated by pay, not by interest in the actual tasks themselves, the data distribution may not match the desired one .  In this work we study the ability of an open-domain.     %We show that our iterative collection-retraining/redeployment % open source everything   %Finally, it is considerably more challenging to engage unpaid humans to provide high quality dialogue when conversing with dialogue models.  %* *Never-Ending Learning:  show it閳ユ獨 improving* %    * future of ML/AI/NLP is not fixed datasets, but continual interactive learning %* game with a purpose    %* Side points: %    * Price )  %    * Distribution  %    * Deployment leads to collecting data, and natural place to evaluate and compare models %    * games as an ideal testbed for AI, w/ rich human interaction, grounding, sandbox %    * while things like alexa challenge do allow a fully deployed system, because of the proprietary nature and other privacy concerns, that research is not open and reproducible. %Collect data, evaluate models. %     [ht!] %     
"," Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance.  As argued in , crowdsourced data has the issues  of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language . % %We posit that, in order to overcome these issues, machine learning must develop systems where models continually improve by interacting with humans and the world. % % In order to overcome these issues, machine learning must develop systems where models continually improve by interacting with humans and the world.  In contrast, one might hope for machine learning systems that become more useful as they interact with people. % In this work, we build and deploy a %  role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in  the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.  %We are releasing the models and data from this work.",192
"  	Recently, deep learning has witnessed a great process . 	Video question answering  has become an emerging task in computer vision and has drawn increasing interests over the past few years due to its vast potential applications in artificial question answering system and robot dialogue, video retrieval, etc. In this task, a robot is required to answer a question after watching a video. 	Unlike the well-studied Image Question Answering  task which focuses on understanding static images, video QA is more practical since the input visual information often change dynamically, as shown in Figure . 	 	 	 	 	 	Compared with image QA, video QA is much more challenging due to several reasons.  Visual content is more complex in a video since it may contain thousands of frames, as shown in Figure . More importantly, some frames may be dominated with strong background content which however is irrelevant to questions.  Videos often contain multiple actions, but only a part of them are of interest to questions. 	 Questions in video QA task often contain queries related to temporal cues, which implies we should consider both temporal location of objects and complex  interaction between them for answer reasoning. For example in Figure , to answer the question ``What does the man do before spinning bucket?"", the robot should not only recognize the actions ``spin laptop'' and ``spin bucket'' by understanding the interaction between the man and objects  for answer reasoning along time axis. 	   	 	 	Taking video frames as inputs, most existing methods  employ some spatio-temporal attention mechanism on frame features to ask the network ``where and when to look''. 	However, these methods are often not robust due to complex background content in videos. 	% However, extracting features from the whole frame makes the model be prone to over-fit the background content .  	Lei et al.  tackle this problem by detecting the objects in each frame and then processing the sequence of object features via an LSTM. However, the order of the input object sequence, which may affect the performance, is difficult to arrange. 	More importantly, processing the objects in a recurrent manner will inevitably neglect the direct interaction between nonadjacent objects. This is critical for video QA . 	 	In this paper, we introduce a simple yet powerful network named Location-aware Graph Convolutional Networks  to model the interaction between objects related to questions. We propose to represent the content in a video as a graph and identify actions through graph convolution. 	% we propose to explicitly detect the salient objects in videos and model their relationship through constructing a fully-connected graph.  	Specifically, the objects of interest are first detected by an off-the-shelf object detector. Then, we construct a fully-connected graph where each node is an object and the edges between nodes represent their relationship.  	We further incorporate both spatial and temporal object location information into each node, letting the graph be aware of the object locations. When performing graph convolution on the object graph, the objects directly interact with each other by passing message through edges. Last, the output of GCNs and question features are fed into a visual-question interaction module to predict  a answer.  	Extensive experiments demonstrate the effectiveness of the proposed location-aware graph. We achieve state-of-the-art results on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets. 	 	The main contributions of the proposed method are as follows:  we propose to explore actions for video QA task through learning interaction between detected objects such that irrelevant background content can be explicitly excluded;  we propose to model the relationships between objects through GCNs such that all objects are able to interact with each other directly;  we propose to incorporate object location information into graph such that the network is aware of the location of a specific action;  our method achieves state-of-the-art performance on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets. 	  	 	 	
","           We addressed the challenging task of video question answering, which requires machines to answer questions about videos in a natural language form. Previous state-of-the-art methods attempt to apply spatio-temporal attention mechanism on video frame features without explicitly modeling the location and relations among object interaction occurred in videos. However, the relations between object interaction and their location information are very critical for both action recognition and question reasoning.  In this work, we propose to represent the contents in the video as a location-aware graph by incorporating the location information of an object into the graph construction. Here, each node is associated with an object represented by its appearance and location features. Based on the constructed graph, we propose to use graph convolution to infer both the category and temporal locations of an action.  		As the graph is built on objects, our method is able to focus on the foreground action contents for better video question answering.  Lastly, we leverage an attention mechanism to combine the output of graph convolution and encoded question features for final answer reasoning. 		Extensive experiments demonstrate the effectiveness of the proposed methods. Specifically, our method significantly outperforms state-of-the-art methods on TGIF-QA, Youtube2Text-QA and MSVD-QA datasets. Code and pre-trained models are publicly available at:  % 		\textcolor{red}{\tt{https://github.com/SunDoge/L-GCN}}         \url{https://github.com/SunDoge/L-GCN}",193
"  With the rapid development of the online social network  such as Twitter and Facebook, people are more frequently using the OSN to express opinions and emotions. It provides researchers with a novel and effective way to detect the mood, communication, activity, and social behavior pattern of individuals . In the past decade, researchers in various fields have conducted quantitative analyses of different illnesses and mental disorders based on the OSN platform . Sina Weibo  is the most popular OSN in the Chinese community . A statistic shows the number of Weibo's monthly active users have reached more than 480 million in the second quarter of 2019\footnote{https://www.statista.com/statistics/941456/china-number-of-sina-weibo-users/}.  Major depressive disorder, referred to as depression, is a common mental disease. According to a survey of the World Health Organization \footnote{https://www.who.int/en/news-room/fact-sheets/detail/depression}, more than 300 million people worldwide suffer from depression. Depression can cause great psychological pain, even suicidal tendencies. Moreover, evidence from a health action plan of WHO\footnote{https://www.who.int/mental\_health/action\_plan\_2013/en/} shows that people suffering from depression are much more likely to end their life prematurely than the general population. Despite the current availability of psychotherapy, medical therapy, and other modalities for the treatment of depression, 76\%-85\% of patients in low- and middle-income countries remain untreated. The emergence of this phenomenon is not only the lack of medical resources but also the inability to make an accurate assessment in the early stage of depression, which leads to a large number of people with depression difficult to get diagnosis and treatment timely .  Pictures, text, videos, and other information posted on the OSN can reflect feelings of worthlessness, guilt, helplessness, and self-hatred, which can help researchers to specifically analyze and characterize depressed individuals . However, there are some insurmountable problems in online depression detection using traditional analyzing methods. They often focus on analyzing the characteristics of users with depression rather than constructing predictive models. Therefore, it is difficult to give timely prediction results of new depressed users. Moreover, they are incapable to deal with a large number of instant interactive user data.  With the rapid development of artificial intelligence technologies, machine learning approaches have made great contributions to the detection of depression . An automated depression detection model based on machine learning usually needs to analyze various information such as tweets, pictures, videos, social activity data of users. Then, it gives the classification results of the predicted objects, most of which are presented as a binary result of normal or depressive. If an individual is predicted for a potential depressive tendency, further resources and assistance can be provided, including later medical and psychological diagnoses. Such heuristic learning approaches are quite effective for helping in the early detection of depression  since they are capable of handling a large number of instant interactive user data.    However, current approaches to online depression detection still face many unresolved challenges.  Firstly, many current studies are not user-oriented modeling . Those works usually aim to analyze and model the language style of the user. Through sentiment analysis and feature engineering of the tweet text, a classification model is developed to detect whether a specific tweet has a depressive tendency. These works analyzed fine-grained features and achieved pretty good results. However, such results cannot be directly applied to user-level depression detection, or it may lead to an incorrect prediction.  Second, in several existing studies , the size of the dataset used for modeling is insufficient, with only a few hundred to a few thousand data samples being used. Because of the difficulty of accurately obtaining and labeling depressed samples, researchers usually choose to construct small datasets or directly cited datasets from other works. As a consequence, the trained model fails to reach good generalization performance, thus hard to accurately predict depressed users on the OSN.  Moreover, not enough studies of user depression detection have been proposed on Weibo compare to Twitter and Facebook. To the best of our knowledge, there is no published large Weibo user depression detection dataset available currently.  Finally, many of the existing proposed models still do not reach a high level of classification performance, i.e. an F1-Score of 90\% and above. Thus, these models need to be further improved to achieve better performance.    Given the above problems and challenges, we hereby summarize the contributions of our work as below:    }. WU3D includes more than 10,000 depressed users and more than 20,000 normal users, each of which contains enriched information fields, including tweets, the posting time, posted pictures, the user gender, etc. This dataset is labeled and further reviewed by professionals.    Different from some existing work that directly using the information fields as features, we made statistical analyzes of all the proposed features. These features show significant distribution differences between depressed and normal users in our experiments.    It implements a multitask learning strategy to process text-based word vectors and statistical features simultaneously. Experimental results show that it achieves both the highest classification performance and the best robustness to unbalanced training samples.   The subsequent sections of this paper are organized as follows. In Section \uppercase, related work and achievements in the field of depression detection on OSNs are introduced and analyzed. The proposed framework is elaborated in Section \uppercase. Furthermore, Section \uppercase gives the significance evaluation of statistical features and the performance comparison experiments of several classification models . At the last of the paper, Section \uppercase summarizes our work and discusses directions for future work.   
"," In recent years, due to the mental burden of depression, the number of people who endanger their lives has been increasing rapidly. The online social network  provides researchers with another perspective for detecting individuals suffering from depression. However, existing studies of depression detection based on machine learning still leave relatively low classification performance, suggesting that there is significant improvement potential for improvement in their feature engineering. In this paper, we manually build a large dataset on Sina Weibo , namely Weibo User Depression Detection Dataset . It includes more than 20,000 normal users and more than 10,000 depressed users, both of which are manually labeled and rechecked by professionals. By analyzing the user's text, social behavior, and posted pictures, ten statistical features are concluded and proposed. In the meantime, text-based word features are extracted using the popular pretrained model XLNet. Moreover, a novel deep neural network classification model, i.e. FusionNet , is proposed and simultaneously trained with the above-extracted features, which are seen as multiple classification tasks. The experimental results show that FusionNet achieves the highest F1-Score of 0.9772 on the test dataset. Compared to existing studies, our proposed method has better classification performance and robustness for unbalanced training samples. Our work also provides a new way to detect depression on other OSN platforms.",194
" As an unsupervised approach, topic modelling has enjoyed great success in automatic text analysis. In general, a topic model aims to discover a set of latent topics from a collection of documents, each of which describes an interpretable semantic concept. Topic models like Latent Dirichlet Allocation ~ and its hierarchical/Bayesian extensions, e.g., in~ have achieved impressive performance for document analysis. Recently, the developments of Variational AutoEncoders  and Autoencoding Variational Inference ~ have facilitated the proposal of Neural Topic Models  such as in~. Inspired by VAE, many NTMs use an encoder that takes the Bag-of-Words  representation of a document as input and approximates the posterior distribution of the latent topics. The posterior samples are further input into a decoder to reconstruct the BoW representation. Compared with conventional topic models, NTMs usually enjoy better flexibility and scalability, which are important for the applications on large-scale data.  Despite the promising performance and recent popularity, there are several shortcomings for existing NTMs, which could hinder their usefulness and further extensions. i) The training and inference processes of NTMs are typically complex due to the prior and posterior constructions of latent topics. To encourage topic sparsity and smoothness, Dirichlet~ or gamma~ distributions are usually used as  the prior and posterior of topics, but reparameterisation is inapplicable to them, thus, complex sampling schemes or approximations have to be used, which could limit the model flexibility. ii) A desideratum of a topic model is to generate better topical representations of documents with more coherent and diverse topics;  but for many existing NTMs, it is hard to achieve good document representation and coherent/diverse topics at the same time. This is because the objective of NTMs is to achieve lower reconstruction error, which usually means topics are less coherent and diverse, as observed and analysed in~.  iii) It is well-known that topic models degrade their performance severely on short documents such as tweets, news headlines and product reviews, as each individual document contains insufficient word co-occurrence information. This issue can be exacerbated for NTMs because of the use of the encoder and decoder networks, which are more vulnerable to data sparsity.  To address the above shortcomings for NTMs, we in this paper propose a neural topic model, which is built upon a novel Optimal Transport  framework derived from a new view of topic modelling. For a document, we consider its content to be encoded by two representations: the observed representation, , a distribution over all the words in the vocabulary and the latent representation, , a distribution over all the topics.  can be obtained by normalising a document's word count vector while  needs to be learned by a model. For a document collection, the vocabulary size  can be very large but one individual document usually consists of a tiny subset of the words. Therefore,  is a sparse and low-level representation of the semantic information of a document. As the number of topics is much smaller than the vocabulary size,  is the relatively dense and high-level representation of the same content. Therefore, the learning of a topic model can be viewed as the process of learning the distribution  to be as close to the distribution  as possible. Accordingly, it is crucial to investigate how to measure the distance between two distributions with different supports. As optimal transport is a powerful tool for measuring the distance travelled in transporting the mass in one distribution to match another given a specific cost function, and recent development on computational OT  has shown the promising feasibility to efficiently compute OT for large-scale problems, it is natural for us to develop a new NTM based on the minimisation of OT.  Specifically, our model leverages an encoder that outputs topic distribution  of a document by taking its word count vector as input like a standard NTMs, but we minimise the OT distance between  and , which are two discrete distributions on the support of words and topics, respectively. Notably, the cost function of the OT distance specifies the weights between topics and words, which we define as the distance in an embedding space, where we embed all the topics and words to represent their semantics. By leveraging the pretrained word embeddings, the cost function is then a function of topic embeddings, which will be learned jointly with the encoder. With the advanced properties of OT on modelling geometric structures on spaces of probability distributions, our model is able to achieve a better balance between obtaining good document representation and generating coherent/diverse topics. In addition, our model eases the burden of designing complex sampling schemes for the posterior of NTMs. More interestingly, our model is a natural way of incorporating pretrained word embeddings, which have been demonstrated to be able to alleviate the issue of insufficient word co-occurrence information in short texts~. With extensive experiments, our model can be shown to enjoy the state-of-the-art performance in terms of both topic quality and document representations for both regular and short texts.      
"," Recently, Neural Topic Models  inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport . Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.",195
" Even before the advent of the COVID-19 pandemic, people across the world were turning to the internet to find answers to their medical concerns . Around 7\%  of Google閳ユ獨 daily searches were health related, equivalent to around 70,000 queries every minute . With the emergence of medical question-answering websites such as ADAM , WebMD , AskDocs and HealthTap , people now  have the opportunity to ask detailed questions and find answers, , that satisfied their needs. COVID-19 has done nothing but accelerate this trend. Almost every government agency and healthcare organization has tried to meet the informational need of users by building online FAQs that try to address as many COVID-related topics as possible   %With the ubiquity of the Internet and the emergence of medical question-answering websites such as ADAM , WebMD , and HealthTap , people are increasingly searching online for answers to their medical questions. Pew Internet Project surveys consistently find that between 75-83\% of internet users look online for health information .  The examples above already illustrate two important problems of any medical Q\&A collection:  there is a very large number of possible questions that can be formulated in different ways, and  it is not easy for a user to browse through a large collection of pre-existing questions to find the one that most resembles their need. A scalable solution to overcome both of these issues is to build a system that can automatically match  questions with semantically similar  questions, and provide those as suggestions to the users. If no similar answered questions exist, we can mark them as priority for experts to respond. This approach more directly satisfies user needs allowing them to use their own words to formulate the question. It also provides an avenue for collecting unanswered questions that users want answered, which is extremely important in a rapidly changing situation such as the currrent COVID-19 pandemic.  %However, the number of people asking medical questions online far exceeds the number of qualified experts -- i.e doctors -- answering them. A scalable solution to overcome this imbalance is to build a system that can automatically match  questions with semantically similar  questions, and provide those as suggestions to the users. When no similar answered questions exist, we can mark them as priority for doctors to respond. This approach uses doctor time more efficiently, reducing the number of unanswered questions and lowering the cost of providing online care.   %Many of the individuals seeking medical advice online are otherwise reluctant to seek medical help due to cost, convenience, or embarrassment. For these patients, an accurate online system is critical because it may be the only medical advice they receive. Of course, some medical problems require in-person care, and an online system must indicate that. Other patients use the internet in addition to in-person care either to determine when an appointment is needed or to follow up after visits when they have lingering questions. For this second group, if the answers they see online do not match those given to them by their doctors, they are less likely to follow the advice of their doctors , which can have serious consequences.  The problem of matching general  questions with semantically similar  questions has been well-studied in the context of online user forums , community QA  and question answer archives .  Typical approaches either assume a large amount of training data on which, either statistics can be computed or models can be learned. However, these approaches fall short when applied to the problem of medical question similarity. First, medical questions imbibe a large amount of medical information that a single word can completely change the meaning of the question. As an example, I閳ユ獡 pregnant and I believe I閳ユ獫e been infected with coronavirus. What should I know about going to the hospital?  and Should I visit the doctor if I am expecting and think I might have COVID-19? are similar questions with low overlap, but Is it safe to take Vitamin D3 supplements to build immunity against Coronavirus? and Is it safe to take Hydroxychloroquine to build immunity against Coronavirus? are critically different and only a couple of words apart. Second, there is no publicly available medical question-question similarity data at the scale where these differences can be effectively encoded in order to learn a reliable similarity function. In fact, we hypothesize that constructing such large datasets that cover the large functional space of nuanced variations in medical domain can be quite hard, and is not a scalable proposition.   %Coming up with an accurate algorithm for finding similar medical questions, however, is difficult. Simple heuristics such as word-overlap are ineffective because Can a menstrual blood clot travel to your heart or lungs like other blood clots can? and Can clots from my period cause a stroke or embolism? are similar questions with low overlap, but Is candida retested after treatment and Is Chlamydia retested after treatment? are critically different and only one word apart. Machine learning is a good candidate for such complex tasks, but requires labeled training data. As no widely available data for this particular task exists,  such as the ones shown in Table.   % \TODO{Can we at least add a COVID-19 related example?} [ht]    % {|p{0.005\textwidth}|p{0.15\textwidth}|p{0.15\textwidth}|p{0.06\textwidth}|} {p{0.005\textwidth}p{0.15\textwidth}p{0.15\textwidth}p{0.06\textwidth}} {|c}{}&\multicolumn{1}{|c}{{|c}{{|c|}{{l}{}&\multicolumn{1}{l}{{l}{{c}{    % \footnote{We acknowledge that this fails at edge cases. For instance, if the answers to two questions are both ""Yes, that is correct"", that does not mean the questions are similar.}  In this paper, we tackle the general problem of medical question-question similarity, assuming only a small amount of labeled data of similarity pairs. We also apply the general solution to a specific COVID-19 scenario  where many different questions from different sources are integrated into a user-friendly experience. Our proposed solution stems from two key insights: First, whether or not two questions are semantically similar is akin to asking whether or not the answer to one also answers the other. This means that the answers in the answered questions contain wealth of medical knowledge that can be distilled into the model. The second insight is that we can infuse this medical knowledge from the answers as a pretraining task within a language model, so that we can capture relatedness between words/concepts in the language. Recent success of pretrained bi-directional transformer networks for natural language processing in non-medical fields supports this insight . % In the examples above, the answer to the question Can clots from my period cause a stroke or embolism? will talk about, for instance, `menstrual blood'  and `bleeding' that establishes the relationships between `period' and `menstrual blood'. Similar connection can be established between heart, lungs and embolism. In contrast, the answers around candida treatment is likely to discuss about yeast while that around Chlamydia on bacteria. %The second insight is that we can infuse this medical knowledge from the answers as a pre-training task within a language model, so that we can capture relatedness between words/concepts in the language. Recent success of pre-trained bi-directional transformer networks for natural language processing in non-medical fields supports this insight .     % \TODO{Can we find a COVID-19 related example?}  %Given the recent success of pre-trained bi-directional transformer networks for natural language processing  outside the medical field , most research efforts in medical NLP have tried to apply general language models to medical tasks . However, these models are not trained on medical information, and make errors that reflect this.   Our approach stems from augmenting a general language model such as BERT, with medical knowledge by process of double fine-tuning that first distills medical knowledge using a large corpus of relevant in-domain task of . Subsequently, it fine-tunes on the available small corpus of . Our models pretrained on medical question-answer pairs outperform models pretrained on out-of-domain question similarity with high statistical significance. In particular, while other pretraining tasks yield an accuracy below 78.7\% on this task, our model achieves an accuracy of 82.6\% with the same number of training examples, an accuracy of 80.0\% with a much smaller training set, and an accuracy of 84.5\% when the full corpus of medical question-answer data is used. % Furthermore, the results show promise of generalizing to other domains as well. We present early results on extensibilty of our approach to another expert domain: question-question similarity in the context of community driven question and answer website for the Ubuntu operating system.  %The task of question-answer matching was specifically chosen because it is closely related to that of question similarity; one component of whether or not two questions are semantically similar is whether or not the answer to one also answers the other. We show that the performance gains achieved by this particular task are not realized by other in-domain tasks, such as medical question-categorization and medical answer completion.   %However, labeled training data is still one of the largest barriers to supervised learning, particularly in the medical field where it is expensive to get doctor time for hand-labeling data. }   The main contributions of this paper are:    } a dataset of medical question pairs generated and labeled by doctors that is based upon real, patient-asked questions, hereafter referred as  dataset. Some sample examples from this dataset is provided in Table.   The rest of the paper is structured as follows:  \S describes the methodology used in creating a dataset that will be made publicly available. \S provides the overview of the approach. \S describes how we used the model to build a service that matches user's COVID-19-related questions to FAQs published online. \S describes experimental details and the key results, % while \S gives a peek into application of the methodology for other domains. \S discusses related work and we end with a discussion on future work.
"," People increasingly search online for answers to their medical questions but the rate at which medical questions are asked online significantly exceeds the capacity of qualified people to answer them. This leaves many questions unanswered or inadequately answered. Many of these questions are not unique, and reliable identification of similar questions would enable more efficient and effective question answering schema. COVID-19 has only exacerbated this problem. Almost every government agency and healthcare organization has tried to meet the informational need of users by building online FAQs, but there is no way for people to ask their question and know if it is answered on one of these pages. While many research efforts have focused on the problem of general question similarity, these approaches do not generalize well to domains that require expert knowledge to determine semantic similarity, such as the medical domain. In this paper, we show how a double fine-tuning approach of pretraining a neural network on  followed by fine-tuning on  is a particularly useful intermediate task for the ultimate goal of determining medical question similarity. While other pretraining tasks yield an accuracy below 78.7\% on this task, our model achieves an accuracy of 82.6\% with the same number of training examples, an accuracy of 80.0\% with a much smaller training set, and an accuracy of 84.5\% when the full corpus of medical question-answer data is used. We also describe a currently live system that uses the trained model to match user questions to COVID-related FAQs. %We also present early experimental evidence suggesting the applicability of our proposed approach on another completely different domain: question-question similarity in the context of community driven question and answer website for the Ubuntu operating system.",196
"  % Alternative first paragraph: %The goal of acoustic scene classification  is to identify the class of a given audio recording, e.g., park, office, library. The ASC task can be  very challenging because  sounds within certain scenes can have similar characteristics, and  sound events can overlap one another. The growing interest on solving the ASC problem, which is confirmed by the high participation of researchers from both academia and industry to the recent IEEE Detection and Classification of Acoustic Scenes and Events  challenge , is justified by the impact that a robust ASC system can have on several real-world applications. For instance, an hearing aid devices can modify its behaviour accordingly to different acoustic envijironments.   % If the above paragraph becomes the first paragraph, this could be reduced, and we can simply say that deep learning has greatly improved the performance of ASC. Although many different solutions have been proposed over the years, and the interested reader is referred to the official DCASE website, the key elements of a successful ASC system are  CNN,  data-augmentation,  attention,  mix-up. % Then you need to make clear the device mismatch problem has received less attention, and only a few proposal have been put forth, for example . You should clarify why this is a key problem and right away say how you want to address it Knowledge distillation. The third paragraph  look good but needs to be polished and perhaps trimmed a bit. Instead, our contribution must be make stronger. What is new in this work and why people should pay attention to it.   %The goal of acoustic scene classification  is to identify the class of a given audio recording, e.g., park, airport, metro station . The ASC task can be  very challenging because  sounds within certain scenes have similar characteristics, and  sound events can overlap one another. The growing interest on solving the ASC problem, as indicated by the high participation of researchers from both academia and industry to the recent IEEE Detection and Classification of Acoustic Scenes and Events  challenges , is justified by the impact that a robust ASC system to real-world applications. For instance, an hearing aid devices could modify its behaviour accordingly to different acoustic environments.  In recent years, we have witnessed a great progress in the acoustic scene classification  task, as demonstrated by the high participation in the IEEE Detection and Classification of Acoustic Scenes and Events  challenges . Top ASC systems use deep neural networks , and the main ingredient of their success is the application of deep convolutional neural networks  . Further boost in ASC performance is obtained with the introduction of advanced deep learning techniques, such as attention mechanism , mix-up , Generative Adversial Network  and Variational Auto Encoder  based data augmentation  , and deep feature learning . Nevertheless, those ASC systems yet do not work well when processing audios from mismatched domain, e.g., audios recorded with different devices .  Device mismatch is an inevitable problem in a real production, and it is therefore an important aspect to handle when deploying an ASC system. Indeed, a new sub-task, namely , has been added to DCASE 2018  to foster research in that  direction. The goal is to design a system that can attain a good performance on 10-second audios segments collected with target devices, which are either not represented at a development phase, or represented during the ASC system deployment with a scarce amount of training material compared to that available for the source device. However, Task1b attracted only a minor interest among DCASE 2018 and 2019 participants, and even fewer teams were directly concerned with the device mismatch issue.  %There exist a few approaches proposed to tackle the domain invariant problem in ASC. For example, multi-instance learning , and low-level or mid-level feature learning , which transfer knowledge across domains and thereby tackle the robustness issue in a  broader sense. In the literature, there exist a few approaches that tackle the domain invariant problem in ASC. For example, multi-instance learning , and low-level or mid-level feature learning , which however address the robustness issue in a broader sense.  Less approaches have instead been proposed to directly combat the ASC device mismatch issue, which is actually the focus of the present work. In particular, spectrum correction  and channel conversion  build a front-end module to convert speech features from the source domain to target domain before feeding them to the back-end classifier. Besides front-end features, mid-level feature based transfer systems, which uses bottleneck features  or hidden layer representations  are adopted to transfer knowledge from source to target domain. Adversarial training methods in  leverage an extra domain discriminator to solve the device mismatch problem although the key focus is on lack of labeled target data. %Although all of those mentioned techniques are beneficial to ASC robustness issue, there is yet a clear gap between source and target device classification results. %In this work, the device mismatch problem is investigated within the  Teacher-student  learning, also named as knowledge distillation , has recently been shown to be effective in ASC and other domain adaptation speech tasks, e.g., . The key idea is to minimizes the distance measurement between teacher and student model output distributions, i.e., the information is transferred at a soft-label level. %, namely class posterior probabilities, embedded with structure relationships among output classes, are usually used to transfer knowledge from the teacher model to student model. %In recent years, researchers further propose relational learning . It directly models the relationships between sample pairs of the teacher and student model. In ,  relational knowledge distillation  is demonstrated to improve the knowledge distillation process. RKD takes into account the relations of outputs rather than individual outputs themselves. %Independently of whether relationships among outputs is taken into account, TS methods require to be effective that soft labels are accurately generated; otherwise, the information encoded in those labels is meaningless. %Among all the TS learning methods, There is a necessary condition to get good effects, the soft label must be accurate enough, otherwise the information encoded in soft labels dose not make any senses. %As a consequence, the Unfortunately, conventional TS learning can be applied with success if:   source and target data is from the same or similar domain , or  source and target data come in pair although belong to different domains . Neural label embedding , recently proposed in , is an ingenues solution to distill knowledge across domains when neither of the aforementioned two requirements could be met. %NLE are embedding at a label level and encode the structural relationships among each pair of output classes in deep neural models. Structural relationships in turn  represent the measurements of similarity or dissimilarity among pairs of objects as distances between points in low-dimensional space. Label embedding can be viewed as the centroid of soft labels from the same class. NLE can be viewed as the centroid of soft labels from the same class. As to extension of soft labels, it encodes the knowledge distilled from the source domain and teacher model, which can then be transferred to the target domain. %%More information on how to build NLE is given in Section . In , NLE was applied to accent and children's adaptation for  automatic speech recognition.  %In this work, we extend the NLE design started in and deploy an NLE teacher-student adaptation approach to combat the ASC robustness problem in the presence of source and target device mismatch.  In this study, we extend the NLE adaptation scheme  by taking into account relationships among different acoustic scenes during adaptation. We achieve this goal by proposing a relational teacher student learning  approach based on NLE for ASC device mismatching problem. First, NLE is learned from a relatively large-size source data set, i.e., collected with the source devices. %The source device data encodes the structural relationships among different acoustic scenes. Next, ASC system is adapted to the target device leveraging upon target domain data only, i.e., teacher-student learning with unpaired data, and the set of NLE, one each per acoustic scene class. The proposed solution is assessed against the DCASE 2018 Task1b data. Experimental results confirm our intuitions and demonstrate that our adaptation technique generates a significant classification improvement on target domain data. Indeed, NLE-based TS adaptation outperforms both  multi-device training strategies, and  conventional TS adaptation schemes. Furthermore, an additional boost is obtained when TS adaptation is carried out leveraging structural information.    %In this work, to solve the device mismatching problem of the ASC systemss, we focus on the structural relationship among scene classes. We propose a novel NLE with relational teacher student learning  approach to solve the domain mismatch problem in ASC. At first, the label embedding are learned from the relatively large-size source domain data, which encode the structural relationship information of classes. Then the system on target domain data is trained with label embedding with criterion including relationship loss. Our proposed approached is evaluated on DCASE2018 task1b development data. The experimental results verify that our methods can obtain significant improvement on target domain data. And the visualization verify our arguments about the structural relationships.  %The rest of this work is organized as follows:  Section describes NLE, including generation and use. The relational TS learning framework is described in Section. Next, Section shows the experimental results and analysis. Finally, Section concludes this work.  
"," %The device domain mismatch issue is an important problem of acoustic scene classification  for real-world applications. To leverage this problem, we focus on the knowledge transfer of the inner structural relationships between each classes. A label embedding with relational teacher student learning approach is proposed. Embedded labels are learned from the source domain data, which encodes the structural relationships. Then a relational teacher student learning framework is used to transfer knowledge. Our proposed approach is evaluated on DCASE2018 task1b data set. And the experimental and visualized results successfully verify our augment and proposed method, which significantly improve the classification accuracy on target device data, with the knowledge transferred from the source device data.  %  Alternative 1: %In this work, we use a model adaptation approach based on  neural label embedding  and  knowledge distillation to combat the accuracy drop in acoustic scene classification with deep neural networks caused by a mismatch between  development  and production  audio recording devices. The proposed adaptation approach works with unpaired source-target data and leverages upon NLE designed to take into account the relationships among acoustic scene classes. The NLE thereby not only condenses a representation of the DNN output distribution given all audio recordings aligned with the same output class but also captures the inherent relationships among acoustic scene classes. Device adaptation is carried out using relational teacher-student learning  solely based on target data, target labels, source DNN, and NLE. The latter serve as soft targets for DNN adaptation.  The proposed approach is assessed against the DCASE 2018 task1b dataset. Experimental evidence confirm the effectiveness our our approach, which compares favourably to conventional device adaptation, and traditional teacher-student based adaptation. Moreover, we observe that NLE based on structural information lead to superior ASC  results than NLE obtained with symmetric Kullback-Leibler divergence ,which do not take into account the relationships among acoustic scene classes.  % Alternative 2 In this paper, we propose a domain adaptation framework to address the device mismatch issue in acoustic scene classification leveraging upon neural label embedding  and relational teacher student learning .  Taking into account the structural relationships between acoustic scene classes, our proposed framework captures such relationships which are intrinsically device-independent. In the training stage, transferable knowledge is condensed in NLE from the source domain. Next in the adaptation stage, a novel RTSL strategy is adopted to learn adapted target models without using paired source-target data often required in conventional teacher student learning. The proposed framework is evaluated on the DCASE 2018 Task1b data set. Experimental results based on AlexNet-L deep classification models confirm the effectiveness of our proposed approach for mismatch situations. %when training with Device A data and testing with data recorded with Devices B and C.  NLE-alone adaptation compares favourably with the conventional device adaptation and teacher student based adaptation techniques. NLE with RTSL further improves the classification accuracy.",197
" %Motivate a bit from ASR side %Introduce a bit on punctuation problem.   The output text generated from automatic speech recognition  systems is typically devoid of punctuation and sentence formatting. Lack of sentence segmentation and punctuation makes it difficult to comprehend the ASR output. For example, consider the two sentences: ``Let's eat Grandma'' vs. ``Let's eat, Grandma!''. Punctuation restoration not only helps understand the context of the text but also greatly improves the readability. Punctuated text often helps in boosting the performance of several downstream natural language understanding  tasks.%  There is a plethora of work done in punctuation prediction over the past few decades. While some early methods of punctuation prediction used finite state or hidden markov models , some other techniques have investigated probabilistic models like language modeling , conditional random fields   and maximum entropy models . As neural networks gained popularity, several approaches have been proposed based on sequence labeling and neural machine translation . These models widely used convolutional neural networks  and LSTM based architectures . More recently, attention  and transformer  based architectures which have been successfully applied to a wide variety of tasks, have shown to perform well for punctuation prediction.  Although it is a well explored problem in the literature, most of these improvements do not directly translate to all domains. In particular, punctuation prediction for conversational speech is not very well explored . Also, a number of approaches have been proposed exploiting the use of acoustic features in addition to lexical features for punctuation task, but they are rather limited and do not clearly address the gap in performance with ASR outputs. In this paper, we focus on multimodal semi-supervised deep learning approach for punctuation prediction in conversational speech by leveraging pretrained lexical and acoustic encoders.  %two set of approaches emerged. One approach tags every word with no punctuation or a following punctuation mark treating it as sequence labeling problem . The second approach uses machine  translation based sequence to sequence models to generate punctuated text from unpunctuated text .     While several methodologies used either text or acoustic only information  for predicting punctuation, many studies show that combining both the features yields the best performance . Acoustic features widely used in the literature include prosodic information such as pause duration, phone duration, and pitch related values like fundamental frequency, and energy.  shows that using acoustic information lead to increased recognition of full stops. In , a hierarchical encoder is used to encode per frame acoustic features to word level features and the results show that incorporating acoustic features significantly outperform purely lexical systems. However, when trained on a very large independent text corpus, the lexical system outperformed the multimodal system that was trained on parallel audio/text corpora. To mitigate this, the work in  introduced speech2vec embeddings but they do not vary with respect to the acoustic context in reference speech.   In general, we identify two potential shortcomings with aforementioned multimodal systems. First, the training is still suboptimal due to lack of large-scale parallel audio/text corpora. Secondly, the models trained on reference text transcripts do not perform that well on ASR outputs, although incorporating acoustic features reduced the gap to some extent.     %And tell how our approach is different from other acoustic based approaches.  %We therefore focus on investigating the benefits of exploiting semi-supervised learning approach.   In this work, we introduce a novel framework for multimodal fusion of lexical and acoustic embeddings for punctuation prediction in conversational speech. Specifically, we investigate the benefits of using lexical and acoustic encoders that are pretrained on large amounts of unpaired text and audio data using unsupervised learning.  The key idea is to learn contextual representations through unsupervised training where substantial amounts of unlabeled data is available and then improve the performance on a downstream task like punctuation, for which the amount of data is limited, by leveraging learned representations. For multimodal fusion, we explore attention mechanism to automatically learn the alignment of word level lexical features and frame level acoustic features in the absence of explicit forced alignments.  We also show the adaptation of our proposed multimodal architecture for streaming usecase by limiting the future context. We further study the effect of pretrained encoders with respect to varying data sizes and their performance when trained on very small amounts of data. Finally, we exploit the N-best lists from ASR to perform data augmentation and reduce the gap in performance when tested on ASR outputs.   % We will investigate following research questions in the paper:   % %  %The rest of the paper is organized as follows: Section  introduces semi-supervised learning approach with pre-trained lexical and acoustic encoder for punctuation prediction. Section  describes the procedure for fusion of acoustic features with lexical encoder. We discuss our experimental setup in Section  and the results are presented in Section . Finally, in Section , we summarize our conclusions.   
","  In this work, we explore a multimodal semi-supervised learning approach for punctuation prediction by learning representations from large amounts of unlabelled audio and text data. Conventional approaches in speech processing typically use forced alignment to encoder per frame acoustic features to word level features and perform multimodal fusion of the resulting acoustic and lexical representations. As an alternative, we explore attention based multimodal fusion and compare its performance with forced alignment based fusion. Experiments conducted on the Fisher corpus show that our proposed approach achieves $\sim$6-9\% and $\sim$3-4\% absolute improvement  over the baseline BLSTM model on reference transcripts and ASR outputs respectively. We further improve the model robustness to ASR errors by performing data augmentation with N-best lists which achieves up to an additional $\sim$2-6\% improvement on ASR outputs. We also demonstrate the effectiveness of semi-supervised learning approach by performing ablation study on various sizes of the corpus. When trained on 1 hour of speech and text data, the proposed model achieved $\sim$9-18\% absolute improvement over baseline model.   %We also incorporate a pretrained lexical BERT encoder to further enhance the hidden representation of acoustic embedding when performed fusion with lexical embedding.",198
"   Peking Opera, also known as Beijing Opera or Jingju, is Chinese traditional performing art which combines music, vocal performance, mime, dance and acrobatics. Singing in Peking Opera has various styles, each widely different depending on different role type and music styles. Strong personal styles also make the actual singing can be different from the given music notes. Like a dialect to Mandarin, it even has its unique way of pronunciation. Moreover, melody in singing often consist of arias with variation of complex transitory and vibratos, which makes the singing very expressive and difficult to learn. Another difference from normal singing is the note length has a great variance, sometime very long note can appear . All above factors makes it very challenging to modelling and generating Peking Opera singing comparing to normal singing.   Although there are few works focusing on the synthesis of Peking Opera, or more broadly, opera, the synthesis of singing voice has been researched since 1962 when Kelly and Lochbaum used an acoustic tube model to synthesis singing voice with success. Recently, several works use deep neural networks to synthesis singing voice which, known as parametric systems, process fundamental frequency  and harmonics features  separately. As a typical case among such systems, Neural Parametric Singing Synthesizer  using a phoneme timing model, a pitch model and a timbre model each consist a set of neural networks to generate acoustic parameters of the singing. In NPSS, a Fitting Heuristic method is introduced to eliminate the mismatch between music note duration and the predicted phoneme duration. However, Fitting Heuristic method is totally rule based and it requires to locate the principal vowel before adjusting phoneme duration. This maybe acceptable in most English or Japanese singing cases, but can cause huge duration error when synthesizing Peking Opera. Different from normal speech or singing, in Peking Opera, one syllable can last very long time and contains a long sequence of phonemes, e.g. ``l-j-E-a-a-N"". More importantly, one can't simply tell which phoneme amongst all these phonemes is the principle phone. There could be multiple equally important phonemes in Peking Opera singing.  To better synthesize the expressive Peking Opera, this paper proposes a Peking Opera singing synthesis system based on Duration Informed Attention Network . The main contribution in this study lies in the two following points: 1) To tackle with rhythm mismatch between music note duration and the predicted phoneme duration, contextual based mixture density networks  followed by a Lagrange Multiplier optimization is proposed and implemented for duration modelling. This method is completely data-driven, and more importantly, skips the step of locating the principle phoneme from the conventional Fitting Heuristic method. 2) To deal with the melody mismatch between original music score and the actual singing, and also to better model the expressive variations and vibratos in Peking Opera, a pseudo music score is generated from the real singing and fed as input during DurIAN model training. Experimental Results show proposed duration modeling and prediction method outperforms the Fitting Heuristic method by a large margin. And the generated pitch contours also demonstrate our system's ability to synthesize the singing variations and vibratos in Peking Opera.  The following sections of this paper are organized as follows. Firstly, the proposed model architecture is introduced. Next, proposed Lagrange Multiplier-based duration prediction and pseudo score generation are introduced in Section 2. In section 3 experiments are conducted based on a unique Peking Opera database. Finally, a quick discussion and conclusion is given in Section 4.     
","  Peking Opera has been the most dominant form of Chinese performing art since around 200 years ago. A Peking Opera singer usually exhibits a very strong personal style via introducing improvisation and expressiveness on stage which leads the actual rhythm and pitch contour to deviate significantly from the original music score. This inconsistency poses a great challenge in Peking Opera singing voice synthesis from a music score. In this work, we propose to deal with this issue and synthesize expressive Peking Opera singing from the music score based on the Duration Informed Attention Network  framework. To tackle the rhythm mismatch, Lagrange multiplier is used to find the optimal output phoneme duration sequence with the constraint of the given note duration from music score. As for the pitch contour mismatch, instead of directly inferring from music score, we adopt a pseudo music score generated from the real singing and feed it as input during training. The experiments demonstrate that with the proposed system we can synthesize Peking Opera singing voice with high-quality timbre, pitch and expressiveness.",199
" Many machine learning datasets have a label imbalance or dataset bias problem. In many cases, either data is harder to collect for certain classes or the data collection phase is biased itself such that bias is introduced to the collected dataset. Typical training algorithms, optimized in order to minimize error, tend to do so by exacerbating bias, e.g., by providing higher recall and precision to the majority class than to minority classes. Therefore, the label imbalance problem raises the concern about fairness of machine learning systems in general. Spoken language understanding  problems often suffer from label imbalance, in ways that may hide important errors from the designers of SLU systems.  Consider an SLU dataset such as Air Traffic Information Systems   and the speech-to-intent detection problem on this dataset.  About 75\% of the dataset carries the intent of searching for a flight, while conversely, some minority intent classes are represented by only a single training example; this is a severe label imbalance problem. Suppose that we train a model without any concerns about fairness or imbalance. The model will very likely learn to output the `flight' intent all the time, which will give us an accuracy of 75\% which is not low and could be acceptable depending on the application. Considering that there are roughly 30 classes in the whole dataset, one class will have a recall of 1.0 and precision of 0.75 and the remaining 29 classes will have both recall and precision of 0.0. In such a scenario, the F-measure, which is a harmonic average of precision and recall, will be 0.86 for the most common class and 0.0 for the rest, which will give an average of 0.03 which is not acceptable in many cases.   % Previous work on Fair ML, There has been recent interest in introducing fairness to training in the machine learning literature . Most such studies are applied to benchmark datasets related to socioeconomic problems, e.g., disparate impact or equal opportunity . In most such studies, fairness is defined to be the task of protecting against the use of explicit or implicit information about a protected attribute  in the decisions of the machine learning algorithm, for instance, framing the problem as a constrained optimization problem by introducing several penalties. In this work, we introduce fairness into a speech-related problem, namely SLU. We also propose a positive and generalized definition of fairness, in terms of the missed detection and false alarm error rates suffered by all classes, regardless of whether the class definitions are matters of socioeconomic importance or merely engineering convenience.  % Previous methods on F-measure optimization  There have been several studies on F-measure maximization . These models usually focus on binary classification using non-neural-network models: a situation in which the problem of F-measure optimization reduces to the problem of learning a threshold on the scores computed by the model to make a decision. We are aware of one study that performs F-measure optimization for convolutional neural networks, but again, using a system that generates several binary classification outputs in parallel; in this scenario, F-measure optimization reduces to the task of tuning the thresholds of individual binary classifiers in order to maximize a weighted log likelihood. However, true multi-class classification, using the softmax output of the neural network, requires a modified definition of the F-measure.  There is no threshold that can be tuned; instead, F-measure optimization requires optimizing the model itself to generate `better' scores in terms of the F-measure. Model versus threshold optimization is the fundamental difference between this study and the previous ones.  In this work, our goal is to design a loss function to maximize the F-measure instead of the accuracy for DNNs. Our methods are tested on two standard socioeconomic classification problems from the literature on fairness , and on two SLU tasks .  On the SLU  tasks,  we perform end-to-end SLU, i.e., we directly map speech input to the labels instead of performing automatic speech recognition  followed by natural language processing .  We pose the SLU problems as multi-class classification tasks and use the softmax output from the DNN, making it possible to apply the same optimization criterion to both the socioeconomic and SLU learning problems. We approximate the F-measure with a differentiable function of the softmax activations so that we can use the standard backpropagation algorithm to train the DNN.    
"," Spoken language understanding  datasets, like many other machine learning datasets, usually suffer from the label imbalance problem. Label imbalance usually causes the learned model to replicate similar biases at the output which raises the issue of unfairness to the minority classes in the dataset. In this work, we approach the fairness problem by maximizing the F-measure instead of accuracy in neural network model training. We propose a differentiable approximation to the F-measure and train the network with this objective using standard backpropagation. We perform experiments on two standard fairness datasets, Adult, and Communities and Crime, and also on speech-to-intent detection on the ATIS dataset and speech-to-image concept classification on the Speech-COCO dataset. In  all four of these tasks, F-measure maximization results in improved micro-F1 scores, with absolute improvements of up to 8\% absolute, as compared to models trained with the cross-entropy loss function.  In the two multi-class SLU tasks, the proposed approach significantly improves class coverage, i.e., the number of classes with positive recall.",200
"  Recent neural text-to-speech  systems based on the sequence-to-sequence approach, such as Tacotron~2 , brought considerable quality improvements, but require relatively large amounts of training data and computational resources to train and operate. %Recent advances in text-to-speech synthesis   have allowed integration of high-quality speech synthesis systems into products such as Alexa or Google Assistant. However, adapting the synthesis models to custom domains requires access to relatively large amounts of training data and computational resources. %Moreover, real-time synthesis may be problematic due to the size of the systems and sequential inference. Several works attempt to reduce the computational burden in various ways , but there is still a tradeoff between fast training times, fast inference, and output quality.  In this paper, we address the training efficiency of TTS systems as well as the inference speed and hardware requirements while sustaining good quality of synthesized audio. We propose a fully convolutional, non-sequential approach to speech synthesis  %based on a combination of ideas from . %Similarly to , our system  consisting of a teacher and a student network, similarly to FastSpeech . The teacher network is an autoregressive %\OD{je tu pot鑹ba 鑹￠搯kat 閳ユ竵uteregresivn閾嗛垾, kdy鍟 se tak nikdy nepou鍟搯v璋?} \todo{myslim, ze jo -- jde o zpusob, jaky modeluje to audio. Kdyby nemodeloval audio autoregresivne, tak be nemel moc motivace naucit se spravny alignment} \OD{Fair enough.} convolutional network % based on   which is used to extract correct alignments between phonemes and corresponding audio frames. The student network is a non-autoregressive, fully convolutional network  %with residual connections  % based on   which encodes input phonemes, predicts the duration  for each one, then decodes a mel-scale spectrogram based on phoneme encodings and durations. %used to synthesize spectrograms from input phonemes. The student network first encodes the input phonemes. Then a duration prediction module predicts the duration of each phoneme. Finally, the phoneme encoding vectors are expanded based on their durations and are fed to a decoder module which synthesizes the final spectrogram.   We combine our student network with a pretrained MelGAN vocoder  to achieve fast and high-quality spectrogram inversion.  Our model can be trained on the LJ~Speech data  in under 40 hours on a single 8GB GPU and generates high-quality audio samples faster than real-time on both GPU and CPU.  %\OD{d璋 se tohle rozd鑷巐it na v閾哻 bod鏆 ne鍟 2?} \todo{ano :)}\OD{d閾唊 :-)} Our contributions are as follows:  We simplify the teacher-student architecture of FastSpeech  and provide a fast and stable training procedure.  We use a simpler, smaller and faster-to-train convolutional teacher model with a single attention layer instead of Transformer  used in FastSpeech. % .  We show that self-attention layers  in the student network are not needed for %necessary in order to achieve  high-quality speech synthesis.   We describe a simple data augmentation technique that can be used early in the training to make the teacher network robust to sequential error propagation.  We show that our model significantly outperforms strong baselines while keeping speedy training and inference. %Finally, we provide results of experiments with various techniques such as batch normalization , dropout , positional encoding and style loss functions.   
"," %Recent breakthrough in in the quality of text-to-speech systems can be largely accounted to neural sequence-to-sequence models . Extensive research has been conducted to improve the effectiveness of training , inference speed  and voice quality  of the speech synthesis systems.  While recent neural sequence-to-sequence models have greatly improved the quality of speech synthesis, % in the past years, %However, to our knowledge  there has not been a system capable of %fast and efficient training, speedy inference and fast training, fast inference and high-quality audio synthesis at the same time.  %However, none of the aforementioned systems excels in all of the traits.  %In this work,  We propose a student-teacher network %based on   capable of high-quality faster-than-real-time spectrogram synthesis, with low requirements on computational resources and fast training time. We show that self-attention layers are not necessary for generation of high quality audio.  %In fact,  We utilize simple convolutional blocks with residual connections in both student and teacher networks and use only a single attention layer in the teacher model. Coupled with a MelGAN vocoder, our model's voice quality was rated significantly higher than Tacotron~2. Our model can be efficiently trained on a single GPU and can run in real time even on a  %4-core  CPU. We provide both our source code and audio samples in our GitHub repository.\footnote{\url{https://github.com/janvainer/speedyspeech}\label{fn:github}}",201
" Automatic speaker verification  has several applications such as voice biometrics for commercial applications, speaker detection in surveillance, speaker diarization, etc. A speaker is enrolled by a sample utterance, and the task of ASV is to detect whether the target speaker is present in a given test utterance or not. Several challenges have been organized over the years for benchmarking and advancing speaker verification  technology such as the NIST speaker recognition Evaluation  challenge 2019 , the VoxCeleb speaker recognition challenge   and the VOiCES challenge . The major challenges in speaker verification include the language mismatch in testing, short duration audio and the presence of noise/reverberation in the speech data.  %The field is attracting a lot of participants, thereby rapidly updating the state-of-the-art.  The state-of-the-art systems in speaker verification use a model to extract embeddings of fixed dimension from utterances of variable duration. The earlier approaches based on unsupervised Gaussian mixture model  i-vector extractor  have been recently replaced with neural embedding extractors   which are trained on large amounts of supervised speaker classification tasks.  These fixed dimensional embeddings are pre-processed with a length normalization  technique followed by probabilistic linear discriminant analysis  based backend modeling approach .   In our previous work, we had explored a discriminative neural PLDA  approach  to backend modeling where  a discriminative similarity function was used. The learnable parameters of the NPLDA model were optimized using an approximation  of  the  minimum  detection  cost  function . This model also showed good improvements in our SRE evaluations and the VOiCES from a distance challenge . In this paper, we extend this work to propose a joint modeling framework that optimizes both the front-end x-vector embedding model and the backend NPLDA model in a single end-to-end  neural framework. The proposed model is initialized with the pre-trained x-vector time delay neural network . The NPLDA E2E is fully trained on pairs of speech utterances starting directly from the mel-frequency cepstral coefficient  features. The advantage of this method is that both the embedding extractor as well as the final score computation is optimized on pairs of utterances and with the speaker verification metric. With experiments on the NIST SRE 2018 and 2019 datasets, we show that the proposed NLPDA E2E model improves significantly over the baseline system using x-vectors and generative PLDA modeling.      % backend models are trained on these embeddings which output a score. These scores are scaled into log-likelihood ratios using calibration methods. Speaker verification systems apply an application specific threshold to the log-likelihood ratios to output the decision. Widely used examples of embeddings are the i-vector, x-vector and he d-vector. I-vectors  are unsupervised embeddings representing alignment statistics of an utterance using a Gaussian mixture universal background model , X-vectors and d-vectors are embeddings obtained from Neural Network models trained with the objective of speaker classification with a few thousand speakers. The Probabilistic Linear Discriminant Analysis  is the most widely used backend model to compute the log-likelihood. Other backend models include the DPLDA, pairwise Gaussian backend, SVMs, and Neural PLDA. In majority of the systems, the model to extract the embeddings is trained separately from the backend model.  % An area of growing interest is the training of End-to-End speaker verification systems, which optimizes the entire model with a verification objective function. In this paper, we extend our prior work on Neural PLDA  model to enable joint learning of the X-vector extractor and the NPLDA backend, in a fully end-to-end manner. We address the GPU memory issues, and analyse two straightforward methods for sampling the training trials for each batch. We provide comparisons of different loss functions for training.  
"," While deep learning models have made significant advances in supervised classification problems, the application of these models for out-of-set verification tasks like speaker recognition has been limited to deriving feature embeddings. The state-of-the-art x-vector PLDA based speaker verification systems use a generative model based on probabilistic linear discriminant analysis  for computing the verification score. Recently, we had proposed a neural network approach  for backend modeling in speaker verification called the neural PLDA  where the likelihood ratio score of the generative PLDA model is posed as a discriminative similarity function and the learnable parameters of the score function are optimized using a verification cost. In this paper, we extend this work to achieve joint optimization of the embedding neural network  with the NPLDA network in an end-to-end  fashion. This proposed end-to-end model is optimized directly from the acoustic features with a verification cost function and during testing, the model directly outputs the likelihood ratio score. With various experiments using the NIST speaker recognition evaluation  2018 and 2019 datasets, we show that the proposed E2E model improves significantly over the  x-vector PLDA baseline speaker verification system.",202
" Speech enabled applications are increasingly gaining popularity across the world. This has initiated a need to build accurate automatic speech recognition  system across different languages. Also, End-to-End  ASR systems are emerging as a popular alternative to conventional hybrid ASR systems. They replace the acoustic model , language model  and pronunciation model with a single neural network . Recurrent neural network transducer   is one such E2E system that allow streaming input and is suitable for real-time ASR applications. Therefore there is a lot of interest in building accurate RNN-T models for different languages spoken across the world.  %The speech recognition accuracy largely depends upon the amount of training data available.  There is often disparity in the availability of transcribed data for different languages. In most cases, a lot more data is available for American English than other languages. The quality of ASR model depends on a number of factors including, the training data quantity and diversity, acoustic model structure, and  optimization algorithm. Furthermore, training data diversity spans a number of factors in adults,  kids, speaking rate,  accents, near-field, and far-field acoustic conditions. A low-resource locale has limited ASR training data, and may not meet the acoustic diversity needed to train a robust model that can generalize to above acoustic factors. To overcome the low-resource constraint, transfer learning has been widely used in the hybrid ASR system to transfer the knowledge from a well trained source locale to a low-resource target locale that bring significant acoustic robustness for the target locale. In our recent work, we applied TL from a large scale en-US conventional hybrid model to the corresponding models in en-IN and it-IT locales, and achieved over 8\% word error rate relative reduction . Motivated by the success of the TL methods in the hybrid ASR system, we explore TL methods to improve low-resource RNN-T models.  Besides improving the target model acoustic robustness, TL is also crucial for training large and complex deep learning architectures. RNN-T models are difficult to train  and also require significantly large amount of data to jointly train the acoustic as well as language model attributes. In our study we have noted weaker convergence or significant parameter tuning requirements for desirable E2E training outcome for low-resource locale. Therefore we expect TL techniques to be even more relevant for E2E systems to stabilize training and improve ASR accuracy.  In the hybrid ASR system, transfer learning is typically done by initializing the target AM with the source AM. In the RNN-T framework, several transfer learning strategies exist depending upon the choice of the initialization model for the encoder and prediction networks. In this paper, we compare different transfer learning strategies in the RNN-T framework. We propose two-stage TL, by first training a target initialization model bootstrapped with a pretrained source model. Subsequently, this model is used to initialize the target RNN-T model. The two-stage TL approach shows ~ WERR reduction and faster convergence in the training loss as compared to randomly initialized RNN-T model. We also study the effect of TL with different amount of training data and show the importance of transfer learning in the case of low-resource languages.    
","  Transfer learning  is widely used in conventional hybrid automatic speech recognition  system, to transfer the knowledge from source to target language. TL can be applied to end-to-end  ASR system such as recurrent neural network transducer  models, by initializing the encoder and/or prediction network of the target language with the pre-trained models from source language. In the hybrid ASR system, transfer learning is typically done by initializing the target language acoustic model  with source language AM. Several transfer learning strategies exist in the case of the RNN-T framework, depending upon the choice of the initialization model for encoder and prediction networks. This paper presents a comparative study of four different TL methods for RNN-T framework. We show ~$10\%-17\%$ relative word error rate reduction with different TL methods over randomly initialized RNN-T model. We also study the impact of TL with varying amount of training data ranging from $50$ hours to $1000$ hours and show the efficacy of TL for languages with a very small amount of training data.\\",203
" With the advent of deep learning, end-to-end text-to-speech  has shown many advantages over the conventional TTS techniques . Tacotron-based approaches  with an encoder-decoder architecture and attention mechanism have shown remarkable performance. The key idea is to integrate the conventional TTS pipeline into a unified network and learn the mapping directly from the text-waveform pair . The recent progress in neural vocoder  also contributes to the improvement of speech quality.   Speech prosody includes affective prosody and linguistic prosody. Affective prosody represents the emotion of a speaker, while linguistic prosody relates to the language content. They are both crucial in speech communication. A TTS system is expected to synthesize the right prosodic pattern at the right time. However, most of the current end-to-end systems  have not explicitly modeled speech prosody. Therefore, they can't control well the melodic and rhythmic aspects of the generated speech. This usually leads to monotonous speech, even when models are trained on very expressive speech datasets. In this paper, we would like to study the way to enable Tacotron-based TTS  for expressive prosody generation.   Multi-task learning  is a learning paradigm that leverages information from multiple related tasks to help improve the overall performance . MTL is inspired by human learning activities where people often apply the knowledge learned from many tasks for learning a new task, that is called inductive transfer. For example, if we learn to read and write together, the experience in reading can strengthen the writing and vice versa. MTL has been widely used in speech enhancement , and speech recognition . It has also been used in speech synthesis , such as statistical parametric speech synthesis with GANs  and DNN-based speech synthesis with stacked bottleneck features. In this paper, we apply multi-task learning to the Tacotron-based TTS for prosody modeling.     [t]      \\                    The study on expressive speech synthesis is focused on prosody modeling , where speech prosody generally refers to intonation, stress, speaking rate, and phrase breaks. Prosodic phrasing  plays an important role in both affective and linguistic expressions. Inadequate phrase breaks may lead to misperception in speech communication. There have been recent studies on prosody modeling for end-to-end TTS system , for example, to improve the prosodic phrasing  by using contextual information , and syntactic features . They are incorporated in the stage of text preprocessing, therefore, there are not optimized as part of the synthesis processing.  We propose a novel two-task learning scheme for Tacotron-based TTS model to improve the prosodic phrasing: 1) the main task learns the prediction of the speech spectrum parameters from character-level embedding representation, and 2) the secondary task learns the prediction of a word-level prosody embedding. During training, the secondary task serves as an additional supervision for Tacotron to learn the exquisite prosody structure associated with the input text. At run-time, the prosody embedding serves as a local condition that controls the prosodic phrasing during voice generation.    The main contributions of this paper include: 1) a novel Tacotron-based TTS architecture that explicitly models prosodic phrasing; and 2) a multi-task learning scheme, that optimizes the model for high quality speech spectrum, and adequate prosodic phrasing at the same time. The proposed system achieves remarkable voice quality for both Chinese Mandarin and Mongolian. To our best knowledge, this is the first multi-task Tacotron implementation that includes an explicit prosodic model.  This paper is organized as follows. Section  recaps the Tacotron TTS framework. We propose the  multi-task Tacotron in Section and report the experiments in Section. Section  concludes the discussion.    
"," Tacotron-based end-to-end speech synthesis has shown remarkable voice quality. However, the rendering of prosody in the synthesized speech remains to be improved, especially for long sentences, where prosodic phrasing errors can occur frequently. In this paper, we extend the Tacotron-based speech synthesis framework to explicitly model the prosodic phrase breaks. We propose a multi-task learning scheme for Tacotron training, that optimizes the system to predict both Mel spectrum and phrase breaks.  To our best knowledge, this is the first implementation of multi-task learning for Tacotron based TTS with a prosodic phrasing model. Experiments show that our proposed training scheme consistently improves the voice quality for both Chinese and Mongolian systems.",204
"  % 1. 娴犲绮涚拠顓㈢叾閸氬牊鍨氬Ο鈥崇烽崚鍡曡礋娑撱倓閲滈柈銊ュ瀻閿涘苯绱╅崙鐑樻拱閺傚洣瀵岀憰浣稿彠濞夈劎顑囨稉娑擃亝膩閸  Speech synthesis, also known as text-to-speech ,  has attracted a lot of attention and obtained satisfactory  results in recent years due to the advances in deep learning.  Several TTS systems based on deep networks were proposed,  such as Char2Wav , Tacotron2 ,  DeepVoice3 , Transformer TTS ,  FastSpeech  and ParaNet .  These systems usually first predict the acoustic feature sequence  from the input text sequence, and then generate waveform from  the acoustic feature sequence using vocoder such as  Griffin-Lim , WaveNet ,  WaveRNN , WaveGlow   and GAN-TTS .  % 2. 娴犲绮涢惄顔煎閻 閸忓厖绨琺el鐠嬮亶顣╁ù瀣秹缂佹粎娈戠拋鎹愵吀閸╃儤婀伴弬鐟扮础 LSTM, Conv, transformer  % According to the characteristics of network strucutre, current mainstream TTS systems can be divided into  % three types: RNN-based, CNN-based and Transformer-based.  % The RNN-based TTS systems, such as Char2Wav ,  % Tacotron 2  and Tacotron ,  % use the recurrent neural network  to design the main network structure,  % where the attention mechanism is applied to model the alignment  % between the acoustic feature sequence and the text sequence, % while the nature of RNN limits its parallelism.  % The CNN-based TTS systems, such as DeepVoice 3  % and ParaNet , adopt the convolution neural network  to model timing dependencies,  % which enable parallel processing at training.  % Especially in ParaNet , the iteratively refined attention mechanism is proposed to enable system  % to perform the inference process in parallel.  % The Transformer-based TTS systems, such as Transformer TTS , FastSpeech  and AlignTTS ,  % apply the architecture of Transformer to realize the process of speech synthesis.  % FastSpeech  uses the self-attention structure of Transformer to design a feed-forward network  % for predicting mel-spectrum in parallel, but needs guidance from an teacher autoregressive TTS model  % due to difficulty of learning alignment between text and mel-spetrum. % AlignTTS  proposes the alignment loss to make feed-forward TTS system capable of model the aligment  % without the guidance from other TTS systems.    % 2. 瀵洖鍤ぐ鎾冲鐠囶參鐓堕崥鍫熷灇鐎佃鏋冮張顒勬毐鎼达妇娈戦梽鎰煑  Although current speech synthesis systems have obtained  high-quality speech, it is difficult for them to achieve  satisfactory results in long text speech synthesis scenarios.  In the sequence-to-sequence TTS model, since the monotonicity  and locality properties of TTS alignment are not fully utilized,  the alignment procedure lacks robustness in inference,  which leads to skipping or repeating words, incomplete  synthesis, or an inability to synthesize long utterances  . To address the issue,  many monotonic attention mechanisms are presented  , where only  the alignment paths satisfying the monotonic condition  are taken into consideration at each decoder timestep.  In ,  the location-based GMM attention introduced in   is also studied in TTS systems to generalize to long utterances.  Especially, AlignTTS  proposes an alignment loss  to model the alignment between text and mel-spectrum, and uses  a length regulator to adjust the alignment, which solves the  instability problem of the alignment and is very efficient.  However, since the self-attention of Transformer   is used to model the dependencies  of input sequence elements in AlignTTS, the positional encodings  are required to introduce the positional information,  which limits the maximum length of input text.  In this paper, a novel self-attention mechanism is proposed to remove the need for the positional encodings and  lift the restriction of input text length.   % In Tacotron , the content-based attention mechanism introduced in   % is used to align the text and the melspectrum,  % but it does not exploit the monotonicity of TTS alignment. % Tacotron 2 uses the hybrid attention meachnism from   % which encourage the attention alignment to move forward consistently through the input sequence.   % which makes synthesis process instability. % long text sequence is not conducive to  % the calcualtion of the attention mechanism in TTS system,  % which affects the prediction of the acoustic feature and the stop token in inference.  % FastSpeech  and AlignTTS  use the length regulator instead of the attention mechanism,  % but the locational encoding of Transformer also limits its max length of input text.  % In order to lift this restriction, we design a novel self-attention mechanism  % to model the timing dependencies for TTS system.  % 3. 娑擃厽鏋冪拠顓㈢叾閸氬牊鍨氭稉顓炲彠娴滃酣鐓瑰瀣紦濡紕娈戦崚鍡樼 On the other hand, the prosody of speech directly affects  the overall listening perception of the voice, especially for  long utterances. In order to improve the naturalness of  synthetic speech, it is necessary for TTS systems to model  prosody information. In ,  a prosody embedding is introduced for emotional and  expressive speech synthesis, which enables fine-grained control  of the speaking style. In ,  an interpretable latent variable model for prosody based on  Tacotron 2 is presented to model phoneme-level and word-level  prosody information of speech.  proposes  a quantized latent feature for the prosody of speech, and trains  an autoregressive prior model to generate natural samples  without a reference speech. These prosody control methods  enable us to learn the prosody from speech and fine-grained  the synthesized speech, but they still cannot effectively  predict the correct prosody according to the input text.   One reason is that the prosody information of speech  generally depends on the meaning of text, while only the  phoneme information of text is used as the input in current  mainstream TTS systems, which limits the capabilities of  modeling the prosody of speech.  In ,  the textual knowledge from BERT  is introduced  into TTS systems to improve the prosody of speech,  but they ignore the variability of prosody.  For example, the same text may produce speech with  different prosody due to pronunciation uncertainty.    % 4. 閹崵绮ㄩ弬鍥ㄦ拱閻ㄥ嫬鍨遍弬鎵仯  In this works, we propose a novel self-attention mechanism,  named as local attention, to model the timing dependencies,  which abandons positional encoding and uses a relative  position matrix to model the influence of the positional  relationship of input sequence. At the same time,  we introduce the prosody learning mechanism for feed-forward  TTS systems, where a prosody embedding for each phoneme is  learned from the mel-spectrum in training. In addition,  a prosody predictor is designed to predict the prosody  embedding according to text and phoneme, where a pre-trained  language model is applied to introduce the meaning of text.  And the main contributions of our works as follows:           
","     Recent neural speech synthesis systems have gradually    focused on the control of prosody to improve the quality    of synthesized speech, but they rarely consider the    variability of prosody and the correlation between prosody    and semantics together. In this paper, a prosody learning    mechanism is proposed to model the prosody of speech based    on TTS system, where the prosody information of speech is    extracted from the mel-spectrum by a prosody learner and    combined with the phoneme sequence to reconstruct the    mel-spectrum. Meanwhile, the sematic features of text from    the pre-trained language model is introduced to improve the    prosody prediction results. In addition, a novel self-attention    structure, named as local attention, is proposed to lift    this restriction of input text length, where the relative    position information of the sequence is modeled by the    relative position matrices so that the position encodings    is no longer needed. Experiments on English and Mandarin show    that speech with more satisfactory prosody has obtained    in our model. Especially in Mandarin synthesis,    our proposed model outperforms baseline model with a MOS gap    of 0.08, and the overall naturalness of the synthesized    speech has been significantly improved.",205
"  Conventional SLU pipeline mainly consists of two components : an Automatic Speech Recognition module generates transcriptions or N-hypotheses, and a Natural Language Understanding  module classifies transcriptions into intents, in which speech recognition error propagation will be amplified during sub-sequence NLU process. Although with the rapid development of end-to-end speech recognition systems, the performance of SLU has been significant improved , it still can not satisfy the application requirements, due to the complexity of scenarios.  %The improved performance of SLU mainly benefits from the increasing maturity of ASR. The application of deep neural networks in acoustic models and language models together with the rapid development of end-to-end technique make ASR systems extend to other research domains .    Usually not all errors from speech recognition harm the SLU module, and those errors have no impact on the eventual performance . The SLU component only keeps its attention on keywords while discarding most of the other irrelevant words . Thus the joint optimization approach can strengthen the focus of the model on improving the transcription accuracy that relates to target events . Recently, many efforts have been dedicated on end-to-end SLU in which the domain and the intent are predicted directly from input audio . Previous researches have shown that a large amount of data is the determining factor for the excellent performance of a model . However, due to the lack of audio and the ambiguity of intents, it is difficult to obtain sufficient in-domain labeled data. Transfer learning methodology has become a common strategy to address insufficient of data problem . %which is a vital technique that can generalizes models trained for one setting or task to other settings or tasks. Different transfer learning strategies have been applied in SLU model and all of them result in competitive complementary results . In this paper, this strategy is also applied to amplify the feature extraction capability of the encoder component, it pre-train the encoder with a large amount of speech recognition labeled data, and then transfer the encoder to the SLU model.   Recently,  proposed and compared various of encoder-decoder approaches to optimize each module of SLU in end-to-end manners and have proved that intermediate text representation is crucial for SLU and jointly training the full model is advantageous. Attention-based models have been widely used in speech recognition and provide impressive performance . Inspired by this, we propose a Transformer based multi-task strategy to adopt textual information in the SLU model. Since text information only acts on the decoder component in speech recognition task, it can be treated as an adaptive regularizer to adjust the encoder parameters such that contributing to improve intent prediction performance.  It should be noticed that the lack of textual corpus is also a major challenge when training language models. To address this problem, various of methods have been carried out to expand corpus in the past decade . In addition, textual level transfer learning strategy by merging a pre-trained representation to the decoder is also explored. The pre-trained representation is obtained with the BERT model, which is designed to pre-train the deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers .   Encoder and decoder are mutual independent but are connected by the attention block, through which can get a collaborated optimization in training.  To maximize the performance, both encoder and decoder are optimized with transfer leaning strategies. In this paper, we first propose a self-attention based end-to-end SLU structure, and applied cross-lingual transfer learning method to solve insufficient acoustic data problem. Then we propose a Transformer based multi-task strategy that conducts intent classification and speech recognition in parallel. Finally, a textual-level transfer learning structure is designed to aggregate the pre-trained BERT model into the decoder component to improves the feature extraction capability of the decoder, indirectly.   
","   End-to-end Spoken Language Understanding  models are made increasingly large and complex to achieve the state-of-the-art accuracy. However, the increased complexity of a model can also introduce high risk of over-fitting, which is a major challenge in SLU tasks due to the limitation of available data. In this paper, we propose an attention-based SLU model together with three encoder enhancement strategies to overcome data sparsity challenge. The first strategy focuses on the transfer-learning approach to improve feature extraction capability of the encoder. It is implemented by pre-training the encoder component with a quantity of Automatic Speech Recognition annotated data relying on the standard Transformer architecture and then fine-tuning the SLU model with a small amount of target labelled data. The second strategy adopts multi-task learning strategy, the SLU model integrates the speech recognition model by sharing the same underlying encoder, such that improving robustness and generalization ability. The third strategy, learning from Component Fusion  idea, involves a Bidirectional Encoder Representation from Transformer  model and aims to boost the capability of the decoder with an auxiliary network. It hence reduces the risk of over-fitting and augments the ability of the underlying encoder, indirectly. Experiments on the FluentAI dataset show that cross-language transfer learning and multi-task strategies have been improved by up to $4.52\%$ and $3.89\%$ respectively, compared to the baseline.",206
" Most speech synthesis models take two-stage procedures to generate waveform audio from the text. First stage generates spectrogram conditioned on linguistic features such as text or phoneme. In second stage, generally refer to as vocoder stage, audio samples are generated through model capable of estimating audio samples from the acoustic features. Traditional approaches estimated audio samples either directly from the spectral density model or hand-crafted acoustic model, but these approaches tended to produce low-quality audio.  After the emergence of the WaveNet, models that generate audio samples on previously generated samples had shown exceptional works in the field.. Nevertheless, dilated causal convolution networks used in the model require sequential generation process during the inference, which infers that real-time speech synthesis is hard to achieve because parallel inference can't be utilized. For this reason, generating high-quality waveform audio in real-time has become a challenging task.  To overcome the structural limitation of the auto-regressive model, most of the recent works are focused on non-autoregressive models such as knowledge distillation, generative adversarial network, and flow-based generative model. We focus on the flow-based generative model since it can model highly flexible approximate posterior distribution in variational inference. The transformation from a single data-point to a Gaussian noise is one-to-one, which makes the parallel generation possible. However, we have to acknowledge that audio samples are discrete data. In other words, naive modeling of a continuous probability density on discrete data can produce arbitrary high likelihood on discrete location. This can lead to degraded generation performance in flow-based neural vocoder. Therefore, dequantization is required before the transformation.  In this paper, we present various audio dequantization schemes that can be implemented in the flow-based neural vocoder. In image generation, adding continuous noise to data-points to dequantize the data is commonly used. However, to the best of our knowledge, the effectiveness of data dequantization in audio domain is still an unknown area, so further investigation is needed. Unlike pixels of the image, audio samples are bounded to signed integer. To overcome this domain issue, we either normalize range of noise values or range of audio samples with different normalization method. In addition, we adapt flow block from flow-based neural vocoder to generate more flexible noises known as variational dequantization.    
","     In recent works, a flow-based neural vocoder has shown significant improvement in real-time speech generation task. The sequence of invertible flow operations allows the model to convert samples from simple distribution to audio samples. However, training a continuous density model on discrete audio data can degrade model performance due to the topological difference between latent and actual distribution. To resolve this problem, we propose audio dequantization methods in flow-based neural vocoder for high fidelity audio generation. Data dequantization is a well-known method in image generation but has not yet been studied in the audio domain. For this reason, we implement various audio dequantization methods in flow-based neural vocoder and investigate the effect on the generated audio. We conduct various objective performance assessments and subjective evaluation to show that audio dequantization can improve audio generation quality. From our experiments, using audio dequantization produces waveform audio with better harmonic structure and fewer digital artifacts.",207
" Associative memory is defined in psychology as the ability to remember  many sets, called memories, of unrelated items. Prompted by a large enough subset of items taken from one memory, an animal or computer with an associative memory can retrieve the rest of the items belonging to that memory.  The diverse human cognitive abilities which involve making appropriate responses to stimulus patterns can often be understood as the operation of an associative memory, with the ``memories'' often being distillations and consolidations of multiple experiences rather than merely corresponding to a single event.  The intuitive idea of associative memory can be described using a ``feature space''.  In a mathematical model abstracted from neurobiology, the presence  of each particular feature  is denoted by the activity  of a model neuron  due to being directly driven by a feature signal.  If there are  possible features, there can be only at most  distinct connections  in a neural circuit involving only these neurons.  Typical cortical synapses are not highly reliable, and can store only a few bits of information\footnote{For instance, a recent study  reports the information content of individual synapses ranging between  and  bits, based on electron microscopy imaging, see also . These numbers refer to the structural accuracy of synapses. There is also electrical and chemical noise in synaptic currents induced by the biophysical details of vesicle release and neurotransmitter binding. The unreliability of the fusion of pre-synaptic vesicles  with the pre-synaptic neuron membrane is the dominant source of trial-to-trial synaptic current variation  .  This noise decreases the electrical information capacity of individual synapses from the maximal value that the synaptic structure would otherwise provide.}. The description of a particular memory requires roughly  bits of information.   Such a system can therefore store at most , which represents the strengths of the synaptic connections between feature neurons  and . Thus, this network is manifestly describable in terms of only two-body synapses, which is approximately true for many biological synapses. In contrast, a Dense Associative Memory network with cubic energy function naively requires the synaptic connections to be tensors  with three indices, which are harder, although not impossible, to implement biologically. Many-body synapses become even more problematic in situations when the interaction term is described by a more complicated function than a simple power .    Many-body synapses typically appear in situations when one starts with a microscopic theory described by only two-body synapses and integrates out some of the degrees of freedom . The argument described above based on counting the information stored in synapses in conjunction with the fact that modern Hopfield nets and Dense Associative Memories can have a huge storage capacity hints at the same solution. The reason why these networks have a storage capacity much greater than  is because they do not describe the dynamics of only  neurons, but rather involve additional neurons and synapses.    Thus, there remains a theoretical question: what does this hidden circuitry look like? Is it possible to introduce a set of hidden neurons with appropriately chosen interaction terms and activation functions so that the resulting theory has both large memory storage capacity , and, at the same time, is manifestly describable in terms on only two-body synapses?    The main contributions of this current paper are the following. First, we extend the model of  to continuous state variables and continuous time, so that the state of the network is described by a system of non-linear differential equations. Second, we couple an additional set of  ``complex neurons'' or ``memory neurons'' or hidden neurons to the   feature neurons.  When the synaptic couplings and neuron activation functions are appropriately chosen, this dynamical system in  variables has an energy function describing its dynamics.  The minima  of this dynamics are at the same locations in the  - dimensional feature subspace as the minima in the corresponding Dense Associative Memory system. Importantly, the resulting dynamical system has a mathematical structure of a conventional recurrent neural network, in which the neurons interact only in pairs through a two-body matrix of synaptic connections. We study three limiting cases of this new theory, which we call models A, B, and C. In one limit  it reduces to Dense Associative Memory model of  or  depending on the choice of the activation function. In another limit  our model reduces to the network of . Finally, we present a third limit  which we call Spherical Memory model. To the best of our knowledge this model has not been studied in the literature. However, it has a high degree of symmetry and for this reason might be useful for future explorations of various models of large associative memory and recurrent neural networks in machine learning.    For the purposes of this paper we defined ``biological plausiblity'' as the absence of many-body synapses. It is important to note that there other aspects in which our model described by equations  below is biologically implausible. For instance, it assumes that the strengths of two physically different synapses  and  are equal. This assumption is necessary for the existence of the energy function, which makes it easy to prove the convergence to a fixed point. It can be relaxed in equations , which makes them even more biological, but, at the same time, more difficult to analyse.    
","  Dense Associative Memories or modern Hopfield networks permit storage and reliable  retrieval of an exponentially large  number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons.   We show that these models are effective descriptions of a more microscopic  theory that has additional  neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy  functions. When certain dynamical variables  are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in ``Hopfield Networks is All You Need'' paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class.",208
"  Unsupervised machine learning has led to a marriage of symbolic learning and vectorized representation learning . In the computer music community, the MusicVAE  enables the interpolation in the learned latent space to render some smooth music transition. The EC-VAE  manages to disentangle certain interpretable factors in music and also provides a manipulable generation pathway based on these factors. Pati et al.  further utilizes the recurrent networks to learned music representations for longer-term coherence.   % With advances in machine learning, the idea of combining symbolic music generation with representation learning has become popular  % . As one of the most successful models,  variational autoencoders  learn a compact  latent representation of music, which has lots of applications in music generation. For example, MusicVAE  introduces latent space interpolation to make smooth music transitions; ECVAE  disentangles the latent space into interpretable factors and manipulates them to generate new pieces; and Lerch et al.  use the representation of a music segment as a token to generate longer-term music using recurrent networks.   Unfortunately, most of the success has been limited to monophonic music.  The generalization of the learning frameworks to polyphonic music is not trivial, due to its much higher dimensionality and more complicated musical syntax. % richer underlying factorization.  The commonly-adopted MIDI-like event sequence modeling or the piano-roll formats fed to either recurrent or convolutional networks have fell short in learning good representation, which usually leads to unsatisfied generation results . In this paper, we hope to pioneer the development of this challenging task. To begin with, we conjecture a proper set of inductive bias for the desired framework: -a sparse encoding of music as the model input; -a neural architecture that incorporates the hierarchical structure of polyphonic music .  % However, most of the aforementioned progress is achieved on VAEs for monophonic music. As we will demonstrate, the success cannot be easily generalized to polyphonic music using commonly-used MIDI-like event sequence or piano-roll formats with standard recurrent or convolutional neural encoders/decoders.  The main reason is that, compared with monophony, polyphonic data is higher dimensional with a more complex and structured distribution. To tackle this challenge, we need a proper inductive bias, specifically: %  %       % 閺傛澘鍟撻惃鍕唽閽鏂ょ窗瀵啰鏁 % Guided by such design principles, we propose PianoTree VAE, a VAE structure that learns the latent representation of polyphonic music in a hierarchical manner. For data representation, we adopt a tree-structured hierarchical music syntax. In a top-down order: a  contains a series of  events, a  consists of multiple  events, and each  has several attributes. In this paper, we focus on a simple yet common form of polyphonic music---piano score with only pitch and duration attributes. Note that this tree structure can be generalized to multiple instruments and expressive performance by adding extra attributes such as voice, expressive timing, dynamics, etc.  % We expect this syntax provides a sufficient inductive bias to learn a semantically-meaningful latent representation, [while still compatible with the current VAE architectures.]  Guided by the aforementioned design principles, we propose PianoTree VAE, a hierarchical representation learning model under the VAE framework. We adopt a tree structured musical syntax that reflects the hierarchy of musical concepts, which is shown in \figref{fig:example}. In a top-down order: we define a   as a series of  events , a  as multiple  events sharing the same onset , and each  has several attributes such as pitch and duration. In this paper, we focus on a simple yet common form of polyphonic music---piano score, in which each note has only pitch and duration attributes. For future work, this syntax can be generalized to multiple instruments and expressive performance by adding extra attributes such as voice, expressive timing, dynamics, etc.  %     The whole neural architecture of PianoTree VAE can be seen as a tree. Each node represents the embedding of either a , , or , where a higher level representation has larger receptive fields. The edges are bidirectional where a recurrent module is applied to either encode the children into the parent or decode the parent to generate its children.  % As for the model structure, both the encoder and the decoder of PianoTree VAE are hierarchical recurrent networks, and this hierarchy has a one-to-one correspondence with the proposed tree structured polyphonic syntax. At each level, a recurrent network either encodes the children into their parent or decodes the parent to generate its children. % We believe that this architecture provides a reasonable inductive bias because the encoding/decoding procedures are analogous to how musicians memorize/interpret a score. For example, we usually ``roll a chord'', and similarly, the model uses recurrent networks to expand a  into 's.  Through  extensive  evaluations, we show that PianoTree VAE yields semantically more meaningful latent representations and further downstream generation quality gains, on top of the current state-of-the-art solutions. % 娴犮儰绗呯粭顑跨閸欍儲妲搁弬鏉垮晸閻ㄥ嫸绱濈粭顑跨癌閸欍儲妲搁崢鐔告降閻ㄥ嫨鍌涘灉鐟欏绶卞▽鈥虫殣閻㈩煉绱濋崚鐘辩啊閵 %The mechanism of recurrent modules is analogous to the procedure such as  ``roll a chord'' in pitch ascending order.      % Reviewer閿涙瓬etter to specify the representation being compared. % Finally, we compare our PianoTree VAE with baseline VAEs using data representation of either piano-roll or MIDI-like event sequence with corresponding model structure and show that the learned latent representation yields more accurate reconstruction, smoother and more musical latent space interpolation, and better downstream music generation when combined with standard sequence generative models.      
"," The dominant approach for music representation learning involves the deep unsupervised model family  . However, most, if not all, viable attempts on this problem have largely been limited to monophonic music. Normally composed of richer modality and more complex musical structures, the polyphonic counterpart has yet to be addressed in the context of music representation learning. In this work, we propose the PianoTree VAE, a novel tree-structure extension upon VAE aiming to fit the polyphonic music learning.  % It consists of multiple layers of encoding and decoding networks, which learn the representation of a tree-structure polyphonic segment in a hierarchical manner.  The experiments prove the validity of the PianoTree VAE via -semantically meaningful latent code for polyphonic segments; -more satisfiable reconstruction aside of decent geometry learned in the latent space; -this model闁炽儲鐛 benefits to the variety of the downstream music generation.\!\!\footnote{Code and demos can be accessed via \url{https://github.com/ZZWaang/PianoTree-VAE}}   % Music representation learning by variational autoencoders  has proven to be a promising direction towards better music generation. However, most successful studies  focus on monophonic music and it is still difficult to generalize the learning methods to polyphonic pieces. This is mainly because polyphonic data has higher dimensionality and contains more complex structures, which is difficult to be captured by the existing VAE architectures for sequential data.  In this paper, we contribute PianoTree VAE, a VAE that considers polyphonic music a tree-structure data and learns the representation in a hierarchical manner. The model effectively learns a semantically meaningful latent code of a polyphonic music segment. Experiments show that the latent code yields better reconstruction and latent space interpolation. The learned latent embedding also leads to better downstream music generation when combined with standard sequence generative models.",209
"  Over the past few years, developments in sequence-to-sequence  neural text-to-speech  research have led to synthetic speech that sounds almost indistinguishable from human speech . However, large amounts of high-quality recordings are typically required from a professional voice talent to train models of such quality, which can make them prohibitively expensive to produce. To counter this issue, investigations into how S2S models can facilitate multi-speaker data has become a popular topic of research. %\EJ{I don't like starting a sentence with a citation if the citation is a number in brackets} \MK{Agreed} A study by, for example, showed that multi-speaker models can perform as well or even better than single-speaker models when large amounts of target speaker data are not available, and that single-speaker models only perform better when substantial amounts of data are used. Their research also showed that the amount of data necessary for an additional speaker can be as little as 1250 or 2500 sentences without significantly reducing naturalness. With regards to parametric synthesis, investigated the effect of several multi-speaker modeling strategies for class imbalanced data. Their research found that for limited amounts of speech, multi-speaker modeling and oversampling could improve speech naturalness compared to single speaker models, while undersampling was found to generally have a harmful effect. They also showed that ensemble methods can further improve naturalness, but this strategy comes with a considerable computational cost that is usually not feasible for S2S modeling.  Although the above research shows that multi-speaker modeling can be an effective strategy to reduce data requirements, it is not a suitable solution for many languages for which large quantities of high-quality multi-speaker data are not available. Multilingual multi-speaker synthesis aims to address this issue by training a multilingual model on the data of multiple languages. Among the first to propose a neural approach to multilingual modeling was. Instead of modeling languages separately, they modeled language variation through cluster adaptive training, where a mean tower as well as language basis towers were trained. They found that multilingual modeling did not harm naturalness for high-resource languages, while low-resource languages benefited from multilingual modeling. Another study by scaled up the number of unseen low-resource languages to twelve, and similarly found that multilingual models tend to outperform single speaker models.  More recently, multilingual modeling was also adopted in S2S architectures, however mostly for the purposes of code-mixing and cross-lingual synthesis. Language information was typically represented either with a language embedding or with a separate encoder for each language, while applied both approaches to code-mixing and accent conversion. With regards to multilingual modeling, showed that multilingual models can attain a naturalness and speaker similarity that is comparable to that of a single speaker model for high-resource target languages, while research from obtained promising results with a crosslingual transfer learning approach.  While research into S2S multilingual modeling is clearly vibrant, there appears to exist little systematic research into how S2S multilingual models could be used to increase speech naturalness for low-resource languages. To fill this void, this paper investigated to what extent results that are found in S2S monolingual multi-speaker modeling are transferable to multilingual multi-speaker modeling, and if it is possible to attain higher naturalness on low-resource languages with multilingual models than with single speaker models. Because multilingual modeling can benefit from the inclusion of large amounts of non-target language data, we also experimented with several data addition strategies and evaluated to what extent these strategies are effective to improve naturalness for low-resource languages. As this research is primarily addressing the viability of different approaches with regards to low-resource languages, our focus is not so much on maximizing naturalness but rather on gaining a better understanding of how different strategies work and would potentially scale up using larger amounts of data.  The rest of this paper is organized as follows. In Section, we  describe the architecture used to conduct our experiments. In Section, we describe the experimental design and give details about training and evaluation. In Section, we provide the experimental results. Finally, in Section, we discuss conclusions and directions for future research.   
"," Recent advances in neural TTS have led to models that can produce high-quality synthetic speech. However, these models typically require large amounts of training data, which can make it costly to produce a new voice with the desired quality. Although multi-speaker modeling can reduce the data requirements necessary for a new voice, this approach is usually not viable for many low-resource languages for which abundant multi-speaker data is not available. In this paper, we therefore investigated to what extent multilingual multi-speaker modeling can be an alternative to monolingual multi-speaker modeling, and explored how data from foreign languages may best be combined with low-resource language data. We found that multilingual modeling can increase the naturalness of low-resource language speech, showed that multilingual models can produce speech with a naturalness comparable to monolingual multi-speaker models, and saw that the target language naturalness was affected by the strategy used to add foreign language data.",210
" % \dcrm{In a standard Question Answering system, a user enters a natural language question,e.g., Who founded Tesla?}. Knowledge Graph based Question Answering  systems use a background Knowledge Graph to answer queries posed by a user. Let us take the following question as an example :  Who founded Tesla?. The standard sequence of steps for a traditional Entity Linking system is as follows: The system tries to identify Tesla as a span of interest. This task is called Mention Detection  or Span Detection. Then an attempt is made to link it to the appropriate entity in the Knowledge Base.  In this work we focus on Knowledge Bases in the form of graphs, hence the entity linker in this case tries to link Tesla to the appropriate node in the graph.  For a human, it is evident that the question is looking for a person's name who created an organisation named Tesla, since the text contains the relation .  Hence, it is important that the entity linker understands the same nuance and ignores other entity nodes in the Knowledge Graph which also contain Tesla in their labels, e.g.,  when considering the example of the Wikidata knowledge graph.  The task of ignoring the wrong candidate nodes, and identifying the right candidate node instead, is called Entity Disambiguation . The cumulative process involving Mention Detection and Entity Disambiguation is called Entity Linking .     Typically, the MD and ED stages are implemented by different machine learning models which require separate training. Especially for the MD part, sentences with marked entity spans are a requirement. In practice, such data is not easily available. Moreover, errors introduced by the MD phase cascade on to the ED phase. Hence, a movement towards end-to-end Entity Linkers began  . Such systems do not require labelled entity spans during training. In spite of the benefits of end-to-end models some challenges remain: Due to the lack of a span detector at the initial phase, each word of the sentence needs to be considered as an entity candidate for the disambiguation which leads to the generation of a much larger number of entity candidates. To re-rank these candidates a large amount of time is consumed, not just in processing the features of the candidates, but also in compiling their features. %Some systems fetch neighbouring entities and relations on the fly  for each candidate entity, a step that can take more than a minute for certain entities on large KGs.   In this work, we remain cognizant of these challenges and design a system that completely avoids querying the Knowledge Graph during runtime. PNEL  instead relies on pre-computed and pre-indexed TransE embeddings and pre-indexed entity label and description text as the only set of features for a given candidate entity. We demonstrate that this produces competitive performance while maintaining lower response times when compared to VCG .  While there is a wide variety of KG embeddings to choose from, we confine our experiments to pre-computed TransE over Wikidata supplied by PyTorch-BigGraph. Our choice was based on the popularity and ease of availability of these embeddings.  Traditionally, the Knowledge Graphs of choice for Question Answering research have been DBpedia, Freebase  and YAGO. However, in recent times Wikidata has received significant attention owing to the fact that it covers a large number of entities . DBpedia, YAGO and Wikidata source their information from Wikipedia, however DBpedia and YAGO filter out a large percentage of the original entities, while Wikidata does not. While Wikidata has a larger number of entities it also adds to noise which is a challenge to any EL system. Wikidata also allows direct edits leading to up-to-date information, while DBpedia depends on edits performed on Wikipedia. Freebase has been discontinued and a portion of it is merged into Wikidata. Moreover DBpedia now extracts data directly from Wikidata, apart from Wikipedia \footnote{https://databus.dbpedia.org/dbpedia/wikidata}  . %Wikidata allows wiki based edits and is hence up-to-date.  %Both DBpedia and Freebase have decided to merge with Wikidata in some form.  Hence, we decide to base this work on the Wikidata knowledge graph and the datasets we evaluate on are all based on Wikidata.\\   In this work our contributions are as follows:                The paper is organised into the following sections:  Related Work, outlining some of the major contributions in entity linking used in question answering;  PNEL, where we discuss the pointer networks and the architecture of PNEL Dataset used in the paper  Evaluation, with various evaluation criteria, results and ablation test   Error Analysis  Discussion and future direction.  
"," Question Answering systems are generally modelled as a pipeline consisting of a sequence of steps. In such a pipeline, Entity Linking  is often the first step. Several EL models first perform span detection and then entity disambiguation. In such models errors from the span detection phase cascade to later steps and result in a drop of overall accuracy. Moreover, lack of gold entity spans in training data is a limiting factor for span detector training. Hence the movement towards end-to-end EL models began where no separate span detection step is involved. In this work we present a novel approach to end-to-end EL by applying the popular Pointer Network model, which achieves competitive performance. We demonstrate this in our evaluation over three datasets on the Wikidata Knowledge Graph.",211
"   Slot filling is one of the major but challenging tasks in spoken language understanding because it aims to automatically extract semantic concepts by assigning a set of task-related slots to each word in a sentence.  was the first reported work that applied recurrent neural network  to the slot filling task and encouraged the follow-up deep learning work for the task. The next works focused on deep learning:  tried to replace the vanilla RNNs with more advanced RNN cells based on long short-term memory   or bi-directional LSTM ,  focused on recursive neural networks, and  utilizes an attention-based RNN.   In this study, we firstly generalize the variational inference -based dropout regularization in the LSTM-RNNs to more advanced RNN architectures such as gated recurrent unit   and bi-directional LSTM/GRU. Then, the RNN models with the VI-based dropout regularization are employed in the slot filling task on the ATIS database. Compared with , this work presents a slight modification of the LSTM-RNNs that can lead to better baseline result, and more RNN architectures with and without VI-based dropout regularization are tested in our experiments. As opposed to , our methods are much easier to implement than the attention-based RNN, but similar results can be obtained in practice.   Since it has been shown that RNNs overfit very quickly , various regularization methods, such as early stopping or small and under-specified models , have to be used during the RNN training stage. Although dropout is normally taken as a simple and effective regularization to overcome the problem of overfitting in deep neural networks , it has been concluded that the naive dropout regularization to recurrent weights in RNNs cannot reliably solve the RNN overfitting problem because noise added in the recurrent connections leads to model instabilities .   However, a recent work  has shown that dropout regularization is a variational approximation technique in Bayesian learning. In addition, the variational inference provides a new variant of dropout regularization, where the same dropout masks are separately shared along time for embedding, decoding, and recurrent weights, so that they can be successfully applied to recurrent layers in RNNs.   The remainder of the paper is organized as follows: Section  presents the VI-based dropout regularization in RNNs. Section  develops the GRU and bi-directional LSTM/GRU-based RNNs with the VI-based dropout regularization. Section  shows the experimental results on the ATIS database and the paper is concluded in Section .   
"," This paper proposes to generalize the variational recurrent neural network  with variational inference -based dropout regularization employed for the long short-term memory  cells to more advanced RNN architectures like gated recurrent unit  and bi-directional LSTM/GRU. The new variational RNNs are employed for slot filling, which is an intriguing but challenging task in spoken language understanding. The experiments on the ATIS dataset suggest that the variational RNNs with the VI-based dropout regularization can significantly improve the naive dropout regularization RNNs-based baseline systems in terms of F-measure. Particularly, the variational RNN with bi-directional LSTM/GRU obtains the best F-measure score.",212
" The percolation of social media throughout the world has facilitated unprecedented ease of access to the flow of information. The rise of the internet and its availability have also enabled every user to to not only consume, but also contribute to the information flow. However, the benefits of such ecosystems come at the cost of mistrust in the veracity of information. In recent years, the social media scene has witnessed the proliferation of false information campaigns, in which ordinary users are intentionally or otherwise both consuming false news and also spreading it among their communities.   This phenomenon is commonly referred to as , broadly defined as broadcasting of information that is intentionally and verifiably false . The rise of fake news and its societal impact has been studied in the context of numerous recent events, such as the Brexit referendum and the 2016 US presidential elections . Fake news has thus proven to be a major threat to democracy, journalism, and freedom of expression . The exposure of users to fake news has been shown to have numerous deleterious effects, instances of which include inducing attitudes of inefficacy, alienation, trusting in false propaganda, cynicism toward certain political candidates and communities, that can at times give rise to the violent events. For example, coordinated fake news and propaganda campaigns on Facebook are considered to have been key in inciting the Myanmar genocide in 2016-2017 . Also, the recent proliferation of false information about 5G communication networks being the cause of the novel Coronavirus outbreak has resulted in attacks against the employees and infrastructure of cellular careers in the UK . Fake news can also affect financial markets, as observed in the case of fake news claiming that Barack Obama was injured in an explosion resulting in a loss of \$130 billion in stock value . Hence, there is a growing need for effective tools and techniques to detect and control the spread of false information campaigns on social media.   Fake news classification is the process of determining whether the news contains false news and misinformation or not. Traditionally, this classification is performed by subject-matter experts and journalists via comparing the claims of an article with established facts and cross-checking with trusted and alternative sources. However, the high volume and velocity of information flow on such platforms render such manual approaches infeasible. Therefore, recent efforts of the stakeholders and the research community have been focused on automated techniques for classification and detection of fake news. A promising solution in this domain is to leverage the recent advances in machine learning and Natural Language Processing  to automated the processing and classification of the high-dimensional and complex text of news articles and posts . %We purpose a model where news article is classified by dividing the overall tasks into three parts: Style-Based Classification, Knowledge-Based Classification, and Propagation and Credibility-Based Classification. This paper is a focus on Style-based classification.  % %Machine learning  proven to useful in detecting fake news. The n-gram, part of speech tagging and probabilistic context free grammar were widely used in linguistic analysis before neural networks. Mihalcea and Strapparava  used n-gram approach for lie detection by  training Naive Bayes and Support Vector Machine  classifiers. They used crowd sourcing for creating their own datasets on three different topics, opinion on abortions, opinion on death penalty and feelings about best friend. They applied minimal pre-processing on the datasets with tokenization and stemming but without performing feature selection and stop words removal. They received the average accuracy of 70.8\% in NB and 70.1\% in SVM, %Ott et al.  trained a SVM classifiers using relative POS tag frequencies of texts as features. They found a probable relationship between deceptive spam and imaginative writing based on POS distributional similarities. %Feng et al.  investigated the syntactic stylometry for deception detection. They found that the features driven from Context Free Grammar parse trees improved the deception detection over Ott et al.    While the literature on the applications of machine learning to fake news classification has grown rapidly, the body of work on the classification of short-text claims remains relatively thin. This issue is of paramount importance, as many of the posts on social media such as Twitter contain only a short claim extracted from the longer text of news articles. The short form of such claims poses a challenge to the classification task, as it provides very limited information  and thus constrains the applicability of machine learning models trained on full-length articles and texts. Over the past few years, a number of datasets and models have been proposed for the classification of short-text claims, notable instances of which are the studies based on the LIAR dataset of short statements . However, the performance of machine learning models trained on this dataset remain at impractical levels, with the best accuracy values reported to be \~41.5\% . %  reported study  The problem with non neural network approach is that the news articles are longer in length and when using non neural network approach the semantic and syntactic features of the sentences cannot be extracted and exploited properly to full extent with non neural network approaches. The solution to this is neural network methods.  %Rashkin et al.  trained an LSTM model that takes sequence of words as the input and predicts the Politifact rating, and found it to be more accurate than NBC and Maximum Entropy models. They also concatenated LSTM output with Linguistic Inquiry and Word Count  features before undergoing the activation layers. The NBC and Maximum Entropy models are improved with LIWC but LSTM did not perform well. The reason might be that the LSTM can learn the in formations in LIWC by themselves. Wang  used deep learning based CNN model with LIAR dataset and found better results than the non-neural network methods.   %Qian et al.  proposed two models, the first one is Two-Level Convolutional Neural Network  a variant of CNN and second one is User Response Generator . The TCNN captured the semantic information from articles' text representing it at the sentence and word level. And URG learns a generative responses to news article text from historical user responses that assist in classification.  In this paper, we introduce Sentimental LIAR, which extends the LIAR dataset by including new features based on the sentiment and emotion analysis of claims. Our extended dataset also proposes a modified encoding of textual attributes to mitigate unintended bias in modeling. Furthermore, we propose a novel deep learning architecture based on the BERT-Base language model for the classification of claims as genuine or fake. Our results demonstrate that the proposed architecture trained on Sentimental LIAR can achieve an accuracy of 70\%, which is an improvement of ~30\% over previously reported results for the LIAR benchmark. The Sentimental LIAR dataset and the proof-of-concept code are made available on GitHub.  %In this paper, we present the series of experiments we performed using BERT-Base and the extended LIAR datasets and compare the results. The base BERT-Base model is modified by adding linear neural net on top and the other modification is done by adding CNN model on top. The modified models are tested with different version of LIAR datasets. We modified the LIAR dataset by extending it with sentiment score and sentiment of the statement. The other extension is done by adding the five emotions of the statement  The remainder of this paper is organized as follows: Section  presents the technical background and an overview of relevant datasets and literature on false claim classification. Section  describes the extended features of Sentimental LIAR, and details the proposed deep learning architectures for false claim detection. The experimental evaluation of our proposed techniques is reported in Section . Finally,  concludes the paper with a discussion on the results and remarks on future directions of work.     
"," The rampant integration of social media in our every day lives and culture has given rise to fast and easier access to the flow of information than ever in human history. However, the inherently unsupervised nature of social media platforms has also made it easier to spread false information and fake news. Furthermore, the high volume and velocity of information flow in such platforms make manual supervision and control of information propagation infeasible. This paper aims to address this issue by proposing a novel deep learning approach for automated detection of false short-text claims on social media. We first introduce Sentimental LIAR, which extends the LIAR dataset of short claims by adding features based on sentiment and emotion analysis of claims. Furthermore, we propose a novel deep learning architecture based on the BERT-Base language model for classification of claims as genuine or fake. Our results demonstrate that the proposed architecture trained on Sentimental LIAR can achieve an accuracy of 70\%, which is an improvement of ~30\% over previously reported results for the LIAR benchmark. %improve the previously reported accuracy of the task by     Focusing on the prevalent short-text format of claims on social media such as Twitter, our work   to an unprecedented challenge in  . Fake news is not only threatening to undermine democracy but equally has been proven to cause violence, disruption, and chaos in the world. Hence, in this research paper, we are going to use the machine learning approach to classify the fake news from the true ones. The rise of Natural Language Processing makes it possible to analyze the news articles. We are proposing a model composed of three perspectives. The first perspective is the Style Based Classification where we classify the article based on its intention is misleading or not, by analyzing the text pattern from the attribute-based and structure-based language features. The second perspective is Knowledge-based classification which is going to classify the news articles based on its authenticity by knowledge extraction and fact-checking. The third perspective is the Propagation and Credibility based classification by analyzing the propagation model of fake news and the credibility of the engaging users. This research paper currently focused on first perspective i.e. Style based classification by deception detection using deep neural networks where we performed experiments using LIAR Dataset by changing it into binary labels and BERT-Base.",213
"  The ever-growing amount of user-generated data on social media platforms be it Facebook, Twitter, blogs or any other electronic medium introduces new challenges in terms of automatic content moderation, especially regarding hate speech  and  offensive language detection. Not only is hate speech more likely to happen on the Internet,  where anonymity is easily obtained and speakers are psychologically distant from their audience, but its online nature also gives it a far-reaching and determinative impact. User content mostly consists of microposts, where the context of a post can be missing or inferred only from current events. Manual verification of each posting by a human moderator is infeasible due to the high amount of postings created every day. Consequently, automated detection of such attacking postings is the only feasible way to counter this kind of hostility. However, this task is challenging because natural language is fraught with ambiguities, and language in social media is extremely noisy. The classification system that would be prepared for the task, needed to be generalized for various test corpora as well. In this paper I have described the system consisting of a sequential pipeline with text feature extraction and classification as its main components. Firstly, a bag-of-words model is used for encoding the sentences into corresponding integer sequence. Thereafter, vectors are generated from these sequences and fed to a series of BiLSTM layers for training. Then a softmax layer is used for ternary classification into the corresponding offensive language categories.  The rest of the paper has been organized as follows. Section  describes the data, on which, the task was performed. The methodology followed is described in Section . This is followed by the results and concluding remarks in Section  and  respectively. % % The following footnote without marker is needed for the camera-ready % version of the paper. % Comment out the instructions  and uncomment the 8 lines % under ""final paper"" for your variant of English. %  %.     %      % % final paper: en-us version      %     %   % space normally used by the marker     % This work is licensed under a Creative Commons      % Attribution 4.0 International License.     % License details:     % \url{http://creativecommons.org/licenses/by/4.0/}. % }  
"," SemEval-2020 Task 12 was OffenseEval: Multilingual Offensive Language Identification in Social Media . The task was subdivided into multiple languages and datasets were provided for each one. The task was further divided into three sub-tasks: offensive language identification, automatic categorization of offense types, and offense target identification. I have participated in the task-C, that is, offense target identification. For preparing the proposed system, I have made use of Deep Learning networks like LSTMs and frameworks like Keras which combine the bag of words model with automatically generated sequence based features and manually extracted features from the given dataset. My system on training on 25\% of the whole dataset achieves macro averaged f1 score of  47.763\%.",214
" The discourse structure of a document describes discourse relationships between its elements as a graph or a tree. Discourse parsing is largely dominated by greedy parsers~. Global parsing is rarer because the dependency between node's label and its internal split point can make prediction computationally prohibitive. % resulting in a large grammar constant. % This expense comes from the dependency relation % between the labels assigned to a node and the % split point that separates its children, which results in % a large constant for global inference in terms of time % complexity, making the inference process extremely slow.  In this work, we propose a CKY-based global parser with tractable inference using a new independence assumption that loosens the coupling between the identification of the best split point label prediction. % For a particular node, we first decide the split % point without considering the labels; and then based on the % split point, we make the decisions for the labels of % current node.   % However, when we apply recursion, the total score of this % node is the sum of scores of split point and label % assignments instead of recursing with the only split % score. % By making independence decisions for split point and label % assignment, we remove the large constant in terms of time % complexity;  and by recursing with the sum of all scores, % dependency relations are maintained. Doing so gives us the advantage that we can search for the best tree in a larger space. % One side effect of this % is that we do not need complex models to represent EDUs. Greedy discourse parsers have to use complex models to ensure each step is correct because the search space is limited. For example,   manually crafted features and feature transformations to encode elementary discourse units ;  and  used multi-task learning for a better EDU representation. Instead, in this work, we use a simple recurrent span representation to build a parser that outperforms previous  global parsers.%  and is comparable to the state-of-art % greedy parsers.  Our contributions are: []   % we have an independence assumption that works    global parser outperforms previous global methods for the task.   % we are better than all greedy parsers that use the same   % representation      %%% Local Variables: %%% mode: latex %%% TeX-master: ""main"" %%% End: % % File emnlp2019.tex % %% Based on the style files for ACL 2019, which were %% Based on the style files for EMNLP 2018, which were %% Based on the style files for ACL 2018, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp-ijcnlp-2019} \usepackage{times} \usepackage{latexsym} \usepackage{mlsymbols} \usepackage{mystyle} \usepackage{symbol} \usepackage{comment}  \usepackage{url}   \\\And%     Omri Koshorek \\     Tel-Aviv University \\     omri.koshorek@cs.tau.ac.il \\\AND%\\     Vivek Srikumar \\     University of Utah \\     svivek@cs.utah.edu \\\And%     Jonathan Berant \\     Tel-Aviv University\\     joberant@cs.tau.ac.il   }  \date{}                  \newpage \appendix      
","     Discourse parsing is largely dominated by     greedy parsers with manually-designed     features, while global parsing is rare due to its     computational expense.  In this paper, we propose a     simple chart-based neural discourse parser that does not     require any manually-crafted features and is based on     learned span representations only. To overcome the     computational challenge, we propose an independence     assumption between the label assigned to a node in the     tree and the splitting point that separates its children,     which results in tractable decoding. We empirically     demonstrate that our model achieves the best performance     among global parsers, and comparable performance to     state-of-art greedy parsers, using only learned     span representations.",215
"  Language models that exhibit one- or few-shot learning are of growing interest in machine learning applications because they can adapt their knowledge to new information . One-shot language learning in the physical world is also of interest to developmental psychologists; , the ability to bind a new word to an unfamiliar object after a single exposure, is a much studied facet of child language learning . Our goal is to enable an embodied learning system to perform fast-mapping, and we take a step towards this goal by developing an embodied agent situated in a 3D game environment that can learn the names of entirely unfamiliar objects in a single exposure, and immediately apply this knowledge to carry out instructions based on those objects. The agent observes the world via active perception of raw pixels, and learns to respond to linguistic stimuli by executing sequences of motor actions. It is trained by a combination of conventional RL and predictive  learning.   We find that an agent architecture consisting of standard neural network components is sufficient to follow language instructions whose meaning is preserved across episodes. However, learning to fast-map novel names to novel objects in a single episode relies on semi-supervised prediction mechanisms and a novel form of  external memory, inspired by the dual-coding theory of knowledge representation . With these components, an  agent can exhibit both slow word learning and fast-mapping. Moreover, the agent exhibits an emergent propensity to integrate both fast-mapped and slowly acquired word meanings in a single episode, successfully executing instructions such as ``put the dax in the box"" that depend on both slow-learned  and fast-mapped  word meanings.   %An embodied learning system that executed fast-mapping with the same flexibility as the best large-scale text-based language models could lead to similarly improved human-agent interaction between users and game-based agents, virtual-reality avatars or robotic assistants.     Via controlled generalization experiments, we find that the agent is reasonably robust to a degree of variation in the number of objects involved in a given fast-mapping task at test time. The agent also exhibits above-chance success when presented with the name for a particular object in the ShapeNet taxonomy  and then instructed  to interact with a different exemplar from the same object class, and this propensity can be further enhanced by specific meta-training. We find that both the number of unique objects observed by the agent during training and the temporal aspect of its perceptual experience of those objects contribute critically to its ability to generalize, particularly its ability to execute fast-mapping with entirely novel objects. Finally, we show that a dual-coding memory schema can provide a more effective basis to derive a signal for intrinsic motivation than a more conventional  memory.   %Equipped with this intrinsic curiosity, an agent can resolve long episodes requiring fast-binding when there are no intermediate environment rewards to stimulate the requisite information discovery.     
"," Recent work has shown that large text-based neural language models acquire a surprising propensity for one-shot learning. Here, we show that an agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional RL algorithms. After a single introduction to a novel object via visual perception and language , the agent can manipulate the object as instructed , combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for , a fundamental pillar of human cognitive development and a potentially transformative capacity for artificial agents.",216
" Transfer learning is a rapidly growing field of machine learning that aims to improve the learning of a data-deficient task by knowledge transfer from related data-sufficient tasks.  % Various factors may affect the availability of sufficient training data to calibrate a well-performed model.  Witness the success of deep learning, deep transfer learning has been widely studied and demonstrated remarkable performance over various applications, such as medical image classification, electronic health data analysis, and credit modeling.   A fundamental block of deep transfer learning is deep neural network, which is vulnerable to different attacks aiming to detect sensitive information contained in the training dataset. Moreover, in most of the real-world applications where deep transfer learning is used, the source and target datasets always reside in two different organizations. As a result, deep transfer learning also faces potential privacy threats, i.e, the client in the target organization can leverage the vulnerability of deep learning models to detect sensitive information contained in the source organization. Specifically, applying deep transfer learning comes with the interaction between the source and target domains. Thus, the data transmission between these domains may unintentionally disclose private information.   %A typical example is to apply transfer learning on medical image classification. Considering two hospitals, one holds massive labeled images  and another only has a small dataset . To improve the model quality on the target task, a typical deep transfer learning method is to first obtain  %Despite the success of deep transfer learning, since  %the source and target datasets always reside in two different organizations, some privacy issues %are posed in many real-world scenarios. Recently, these privacy issues have drawn increasing attentions form both the industrial/academic communities with the publication of various data privacy regulations, such as  Europe閳ユ獨 General Data Protection Regulation .  Existing studies on analyzing privacy leakages focus on either general machine learning models or in a federated learning setting where model is collaboratively trained by multiple clients by sharing and aggregating the gradients via a server. However, there no such study on  transfer learning paradigms.  To this end, we are the first to provide a general categorization for deep transfer learning models based on the potential information leakages. This is not trivial since there are numerous methods for deep transfer learning. Given the goal of privacy leakage analysis, we care more about the interaction manner between source and target domains.  %In this paper, we aim to analyze the potential information leakage in different deep transfer learning algorithms. Thus, we divide previous works into three categories, as illustrated in Figure 1:  model-based paradigm where the whole model structure and parameters are shared  mapping-based where the hidden features are shared  parameter-based where the parameter gradients are shared.  Based on that, the previous works can fall into the above categories or a hybrid of them.  For example, fine-tuning based approaches obviously belong to the first category. The prior work is based on the mapping-based paradigm, since it uses the correlation alignment loss which further depends on the shared hidden features. Similarly, previous works that minimize the domain representation difference by variants of distribution divergence metrics such as maximum mean discrepancy also fall into the second category. Fully-shared and shared-private transfer learning models  can be regarded as parameter-based, as they both jointly train a shared network via gradient updates in a multi-task fashion, just to name a few.  % the MMD-based metric also fall into the second category. % variants of MaximumMean Discrepancy, Kullback-Leibler Divergence, Wasser-stein distance, and etc   [t!]     ^S\mathcal{D}^Txywh$ for feature representations.}              Based on the general categorization, we can build customized attacks against each paradigm and demonstrate information leakages in deep transfer learning. At a high level, we consider inferring two types of sensitive information, i.e., membership and property information. This sensitive information can be revealed by the transmission data between the two domains as the above discussed. Specifically, in the model-based paradigm, we build the membership attack which takes the model  as input and determines whether a specific sample is used for training the model. In the mapping-based setting, we can build the property attack to infer properties contained in the training dataset. For example, the attacker resides on the target domain aims to infer properties of the source domain based on the shared hidden features. In the parameter-based setting, we can similarly perform the property inference attack, i.e., the attacker can infer properties of the source domain data based on the shared gradients. More details of these attacks can be found in Section.  Empirically, to demonstrate the effectiveness of attacks, we conduct a set of experiments under the three types of transfer learning settings. Our key observation is that all these types of models do unintentionally leak information of the training data under membership/property attacks.  Model-based paradigm is possible to leak membership information. Parameter-based paradigm without revealing individual gradient  leaks much less property information, compared to the mapping-based paradigm where hidden features  are shared. % As illustrated in the experiments,   % . In summary, our main contributions are as follows:           %which comes with different privacy leakage profiles.     %   % The rest of this paper is organized as follows: Section  introduces the basic setting of deep transfer learning and background of inference attacks against deep learning models. Section  provides the general categorization of deep transfer learning and detailed privacy analysis. Section  shows the information leakage in deep transfer learning empirically. % Section briefly summarizes some % related works and Section draws the conclusion.  %% %% This is file `sample-sigconf.tex', %% generated with the docstrip utility. %% %% The original source files were: %% %% samples.dtx   %%  %% IMPORTANT NOTICE: %%  %% For the copyright see the source file. %%  %% Any modified versions of this file must be renamed %% with new filenames distinct from sample-sigconf.tex. %%  %% For distribution of the original source see the terms %% for copying and modification in the file samples.dtx. %%  %% This generated file may be distributed as long as the %% original source files, as listed above, are part of the %% same distribution.  %% %% The first command in your LaTeX source must be the \documentclass command. \documentclass[sigconf]{acmart} %%%% As of March 2017, [siggraph] is no longer used. Please use sigconf  for SIGGRAPH conferences.  %%%% Proceedings format for SIGPLAN conferences  % \documentclass[sigplan, anonymous, review]{acmart}  %%%% Proceedings format for SIGCHI conferences % \documentclass[sigchi, review]{acmart}  %%%% To use the SIGCHI extended abstract template, please visit % https://www.overleaf.com/read/zzzfqvkmrfzn  \usepackage{xcolor} \usepackage{soul} \usepackage{url} \usepackage{caption} \usepackage{graphicx} \usepackage{amsmath} \usepackage{amsthm} \usepackage{booktabs} %\usepackage{algorithm} %\usepackage{algorithmic} \usepackage{multirow} \usepackage{listings} \usepackage{array}  \renewcommand{\lstlistingname}{Code} \lstset{frame=tb,   language=C,   aboveskip=3mm,   belowskip=3mm,   showstringspaces=false,   columns=flexible,   basicstyle={{*)}, %  frame=single, } \usepackage[linesnumbered, ruled, boxed]{algorithm2e}  [1]{} \urlstyle{same}  } [1]{{[]} }   %% %% \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{%   \providecommand\BibTeX{{%           %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%  %% %% The majority of ACM publications use numbered citations and %% references.  The command  switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%  %% %% end of the preamble, start of the body of the document source.   %% %% The ""title"" command has an optional parameter, %% allowing the author to define a ""short title"" to be used in page headers.  % \title{A Comprehensive Privacy Analysis of Deep Transfer Learning} \title{A Comprehensive Analysis of Information Leakage in Deep Transfer Learning}  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research.  \author{Cen Chen, Bingzhe Wu, Minghui Qiu, Li Wang, Jun Zhou}  % \authornote{Both authors contributed equally to this research.}  @antgroup.com}   \affiliation{%              }  % \author{Lars Th{\o}rv{\""a}ld} % \affiliation{% %   rv{\""a}ld Group} %   rv{\""a}ld Circle} %    %    % } %   % \author{Valerie B\'eranger} % \affiliation{% %    %    %    % }  %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose.   \renewcommand{  %% %% The abstract is a short summary of the work to be presented in the %% article.  Transfer learning is widely used for transferring knowledge from a source domain to the target domain where the labeled data is scarce. Recently, deep transfer learning has achieved remarkable progress in various applications. However, the source and target datasets usually belong to two different organizations in many real-world scenarios, potential privacy issues in deep transfer learning are posed. In this study, to thoroughly analyze the potential privacy leakage in deep transfer learning, we first divide previous methods into three categories. Based on that, we demonstrate specific threats that lead to unintentional privacy leakage in each category.  Additionally, we also provide some solutions to prevent these threats. To the best of our knowledge, our study is the first to provide a thorough analysis of the information leakage issues in deep transfer learning methods and provide potential solutions to the issue. Extensive experiments on two public datasets and an industry dataset are conducted to show the privacy leakage under different deep transfer learning settings and defense solution effectiveness.  %  Extensive experiments are conducted to show the privacy leakage of typical deep transfer learning on a number of public datasets.   %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %%  <ccs2012> <concept> <concept_id>10002978.10003022</concept_id> <concept_desc>Security and privacy~Software and application security</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10010147.10010257.10010258.10010262.10010277</concept_id> <concept_desc>Computing methodologies~Transfer learning</concept_desc> <concept_significance>500</concept_significance> </concept> </ccs2012>   %  %     %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.    %% This command processes the author and affiliation and title %% information and builds the first part of the formatted document.     %    
"," Transfer learning is widely used for transferring knowledge from a source domain to the target domain where the labeled data is scarce. Recently, deep transfer learning has achieved remarkable progress in various applications. However, the source and target datasets usually belong to two different organizations in many real-world scenarios, potential privacy issues in deep transfer learning are posed. In this study, to thoroughly analyze the potential privacy leakage in deep transfer learning, we first divide previous methods into three categories. Based on that, we demonstrate specific threats that lead to unintentional privacy leakage in each category.  Additionally, we also provide some solutions to prevent these threats. To the best of our knowledge, our study is the first to provide a thorough analysis of the information leakage issues in deep transfer learning methods and provide potential solutions to the issue. Extensive experiments on two public datasets and an industry dataset are conducted to show the privacy leakage under different deep transfer learning settings and defense solution effectiveness.  %  Extensive experiments are conducted to show the privacy leakage of typical deep transfer learning on a number of public datasets.",217
"   Emphasis selection is an emerging research problem  in the natural language processing domain, which involves automatic identification of words or phrases from a short text that would serve as good candidates for visual emphasis. This research is most relevant to visual media such as flyers, posters, ads, and motivational messages where certain words or phrases can be visually emphasized with the use of different color, font, or other typographic features. This type of emphasis can help with expressing an intent, providing more clarity, or drawing attention towards specific information in the text. Automatic emphasis selection is therefore useful in graphic design and presentation applications to assist users with appropriate choice of text layout.   Prior works in speech processing  have modeled word-level emphasis using acoustic and prosodic features. Understanding emphasis in speech is critical to many downstream applications such as text-to-speech synthesis , speech-to-speech translation , and computer assisted pronunciation training . In computational linguistics, emphasis selection is very closely related to the problem of keyphrase extraction . Keyphrases typically refer nouns and noun-phrases that capture the most salient topics in long documents such as scientific articles , news articles , web pages , etc. In contrast, emphasis selection deals with very short texts , and also emphasis could be applied to words belonging to various parts of speech.  The goal of SemEval 2020 - Task 10 is to design methods for automatic emphasis selection in short texts. To this end, the organizers  provided a dataset consisting of over 3,000  sentences annotated for token-level emphasis by multiple annotators. The authors employed the standard I-O tagging schema, which is widely used in annotation of token-level tags. We approached emphasis selection as a sequence labeling task solved using a Bidirectional Long Short-term Memory  model, where the individual tokens are represented using various contextual embedding models. We also employ label distribution learning   approach, which elegantly accounts for disagreements between the annotators.   
"," This paper presents our submission to the SemEval 2020 - Task 10 on emphasis selection in written text. We approach this emphasis selection problem as a sequence labeling task where we represent the underlying text with various contextual embedding models. We also employ label distribution learning to account for annotator disagreements. We experiment with the choice of model architectures, trainability of layers, and different contextual embeddings. Our best performing architecture is an ensemble of different models, which achieved an overall matching score of 0.783, placing us 15th out of 31 participating teams. Lastly, we analyze the results in terms of parts of speech tags, sentence lengths, and word ordering.",218
"   Licence details: http://creativecommons.org/licenses/by/4.0/. } The Internet represents the biggest source of knowledge humankind currently possesses. People from all corners of the world can express their opinions, thoughts, and share their insights regarding certain ideas. During the past decade, a new and never seen before way of sharing beliefs arose, memes. For example, by humorously combining text and images, their authors can emphasize a series of aspects such that they will amuse, in various degrees and ways, the receptors.  Motivation. Internet memes come with various templates and formats. Some of them are purely humorous, while others, behind an amusing appearance, intend to convey subtle nuances including sarcasm, disbelief regarding an idea, or a motivational purpose.  %Others express sarcasm or disbelief regarding an idea, while some of them have a motivational purpose.  All of them are present in the online environment and offer  %a great opportunity for studying the online user's behaviors and beliefs. Thus, this offers an insight into the opinion of some communities regarding particular aspects. Moreover, they can be used for obtaining valuable information that will lead to further improvements for the web content mining process.  Challenges. The mining task becomes increasingly more difficult, since both the image and text clarity usually seem to vary substantially, depending on the user or the region it was posted from.  %as, usually,  both the image and text clarity seem to vary substantially, depending on the user or the region it was posted from.  %Unclear posts sometimes lead to considerable difficulties in identifying the overall message of the author.  These situations may cause unfavorable results when performing an analysis process and, in particular cases, they can transmit a different idea than intended. If the ambiguity reaches high values, it can lead to the impossibility of separating the two main ways memes convey information: text and image. Also, since the text is embedded into the image, a low-quality picture can introduce noise inside the content and can compromise the entire meme. On the other way around, an extremely unclear text will create a discrepancy between it and the visual aspect of the post, and thus can weaken the overall message.   }.  }  The  memotion analysis shared task   organized by SemEval-2020 intends to challenge participants to approach the previously mentioned issues to create systems able to analyze Internet memes. The competition consists of three subtasks. Subtask A intends to properly classify the memes as either positive, neutral, or negative. Furthermore, Subtask B builds upon the first subtask such that the participants will be challenged to binary classify the posts considering four categories: humor, sarcasm, offense, and motivation. Finally, Subtask B is extended into Subtask C, where the four categories are expanded into  classes of different granularity, gradually increasing from the lowest to the highest possibility regarding to each category.  Proposed Approach. We intend to solve all the previously mentioned subtasks by introducing a neural network based on multi-task learning . The system will contain modules dedicated to image analysis and modules specialized in text processing. The architecture outputs a single answer for all the required subtasks. % The architecture outputs at the same time the answer for all the required tasks.      The next parts of this work are structured as follows. In section 2, we perform an analysis of  existing solutions found on  related works. In section 3, we outline the approaches we applied for memotion analysis. Section 4 details the performed experiments,  experimental setup, and error analysis. Finally, we draw conclusions in section 5.   
","  Users from the online environment can create different ways of expressing their thoughts, opinions, or conception of amusement. Internet memes were created specifically for these situations. Their main purpose is to transmit ideas by using combinations of images and texts such that they will create a certain state for the receptor, depending on the message the meme has to send. These posts can be related to various situations or events, thus adding a funny side to any circumstance our world is situated in. In this paper, we describe the system developed by our team for SemEval-2020 Task 8: Memotion Analysis. More specifically, we introduce a novel system to analyze these posts, a multimodal multi-task learning architecture that combines ALBERT for text encoding with VGG-16 for image representation. In this manner, we show that the information behind them can be properly revealed. Our approach achieves good performance on each of the three subtasks of the current competition, ranking  \ for Subtask A , \ for Subtask B , and \ for Subtask C  while exceeding the official baseline results by high margins.",219
"     A series of countries from our world are multilingual, which implies that there are multiple languages spoken by their population. People tend to mix them at the phrase or sentence level in order to express ideas with ease, thus creating a phenomenon called code-mixing or code-switching. As it is expected, this embedding of a language into another one makes its appearance in the virtual space, as well. For example, Twitter users combine Hindi or Spanish phrases with English words, thus creating a bilingual phrase that can lead to understanding problems for non-natives.  However, the virtual space adds more layers of difficulty in identifying the sentiment of the author. Usually, social media users tend to adopt phonetic typing, which implies that the words will not take their original form, they will be adapted such that it will be faster to express the main idea.  As a particular case, for the Hindi-English users, a new problem arises: Hindi and English use different alphabets, which determines the user to romanize the Hindi words such that both languages will use the same alphabet throughout the text. At the same time, social media users tend to express their sentiments by repeating certain vocals in words.  Furthermore, they use emojis, which will add an extra layer of complexity for analyzing the text.  This entire process creates new opportunities for research, given the importance of sentiment analysis in this area. The SemEval-2020 Task 9:  Sentiment Analysis for Code-Mixed Social Media Text challenges the research community to solve the previously mentioned problem by introducing two subtasks, focusing on three of the world's most spoken languages: Hindi and Spanish, alongside English. We proposed a series of neural models that intend to solve this issue, contributing under the usernames eduardgzaharia and clementincercel, respectively. Firstly, we experimented with Recurrent Neural Network  solutions alongside word embeddings. After that, we performed the leap towards Transformer-based models that usually perform better and offer more insight for the combined language models. Furthermore, adding an auxiliary task for training a multi-task learning  architecture can lead to even better results, as the models become able to learn new features from the input texts.  The paper is structured as follows. In section 2, we perform an analysis of existing solutions for several code-mixed tasks and sentiment analysis. In section 3, we detail the proposed approaches for code-mixed sentiment analysis. Section 4 details the performed experiments, including data and preprocessing, experimental setup, and a discussion of the results. Finally, we draw conclusions in section 5.  
"," Sentiment analysis is a process widely used in opinion mining campaigns conducted today. This phenomenon presents applications in a variety of fields, especially in collecting information related to the attitude or satisfaction of users concerning a particular subject. However, the task of managing such a process becomes noticeably more difficult when it is applied in cultures that tend to combine two languages in order to express ideas and thoughts. By interleaving words from two languages, the user can express with ease, but at the cost of making the text far less intelligible for those who are not familiar with this technique, but also for standard opinion mining algorithms. In this paper, we describe the systems developed by our team for SemEval-2020 Task 9 that aims to cover two well-known code-mixed languages: Hindi-English and Spanish-English.  We intend to solve this issue by introducing a solution that takes advantage of several neural network approaches, as well as pre-trained word embeddings. Our approach  achieves promising performance on the Hindi-English task, with an average F1-score of 0.6850, registered on the competition leaderboard, ranking our team \ out of 62 participants. For the Spanish-English task, we obtained an  average F1-score of 0.7064 ranking our team \ out of 29 participants by using another multilingual Transformer-based model, XLM-RoBERTa.",220
" The recent outbreak of SARS-CoV-2 has led to a global pandemic with the total number of infections exceeding 6 million with more than 370000 mortality already. The disease has been code named COVID-19 and had far reaching repercussions the world over. This article aims to uncover the life science universe of the Corona virus and related ailments by employing some of the state-of-the-art natural language processing technologies applied to biomedical domain. We took the corpus of about 40000 titles and abstracts released as a part of CORD-19 Open Research Challenge and applied our entity recognition and relationship discovery models to construct a knowledge graph related to COVID-19. In the process, we uncovered about 40000 entities and 80000 relationships.  This article presents our salient findings and is organized as follows. Section  briefly describes our masked entities model and masked relationship model. Section  presents a network analysis of the knowledge network discovered by mining CORD-19 dataset. The coverage of CORD-19 dataset may be not exhaustive and up-to-date. We took snapshot around April 15, 2020. Nevertheless, the primary aim of this work is to demonstrate the application of artificial intelligence on condensing unstructured information in the biomedical domain to a sufficiently low entropy state so that some important leads can be established.  
"," We extract entities and relationships related to COVID-19 from a corpus of articles related to Corona virus by employing a novel entities and relationship model. The entity recognition  and relationship discovery models are trained with a multi-task learning objective  on a large annotated corpus. We employ a concept masking paradigm to prevent the evolution of neural networks functioning as an associative memory and induce right inductive bias guiding the network to make inference using only the context. We uncover several import subnetworks, highlight important terms and concepts and elucidate several treatment modalities employed in related ailments in the past.",221
"  The COVID-19 pandemic urged various science disciplines to do their best so as to contribute to understanding and relieving its impact. Thus, scholars and practitioners working on information sciences have been dedicating significant effort to help. Collecting and analyzing data published on social media platforms have become the focus in this respect. We joined the community that aims at organizing data collected from social media , as informative and uninformative. The WNUT-2020 Task 2 considers tweets about recovered, suspected, confirmed and death cases as well as location or travel history of the cases as informative. All other tweets are considered to be uninformative. The organizers did not share an annotation manual nor was a baseline system made available, presumably to prevent use of any other manually annotated data and to encourage broad participation respectively.\footnote{\url{http://noisy-text.github.io/2020/covid19tweet-task.html}, accessed on September 4, 2020.}  The effort was managed in terms of a shared task, in which the organizers share a dataset that consists of annotated tweets and conduct the evaluation of the submissions. The task requires the participating teams to develop short-text classification systems that facilitate the training and development data to generalize to the test set they release. Although the gold labels of the training and development data were available to the participants, neither the gold labels of the test data nor the annotation guidelines for any part of the data were shared with the participants. Moreover, the test instances were unknown to the participating teams. They were hidden in a larger dataset. Each team was allowed to submit only two outputs of the systems they developed for classifying tweets on the Codalab page of the task.\footnote{\url{https://competitions.codalab.org/competitions/25845}, accessed on September 4, 2020.} The highest score in terms of F1 positive class of each team was used to rank them in the leaderboard.  Integrating automatically created machine learning based  models with manually formulated rules to tackle a text classification task promises the best of both worlds. We pursued this goal by integrating the output of two deep learning models and a rule-based system under the team name COVCOR20. Although the integration slightly improves the total performance on the training and development sets in a cross-validation setting, the overall performance on the test data turned out to be slightly worse than our best ML system. Our best submission was ranked 22nd among 55 teams. The integration of our systems would be ranked 27th if its score were used as the final score for our team.  The deep learning models and the rule-based system are introduced in Sections and respectively. Next, the Section describes how we integrate the output of these systems. Then Section provide the results and their discussion. Finally, we conclude this report and share our future plans continuing in this line of research in Section.  
"," In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of  the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the integration was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating machine learning and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.",222
"      The phenomenon of combining two or more languages in the same message is known as code-switching or code-mixing . Code-switching is an indicator of bilingual competence  , and it is also motivated by social and cultural factors such as social status, race, age, etc. . %   Instead of consider it as an indicator of lack of competence , there are cultural and social factors which motivate its study . Although this phenomenon has been studied extensively in linguistics , it is still challenging for machines to process mixed natural languages. Code-switching is notoriously present on social media posts and chats such as Twitter, Facebook or WhatsApp;  consequently making it more difficult to process the sentiment expressed in such contents. %Multilingual people, who are non-native English speakers, tend to code-mix using English-based phonetic typing and the insertion of anglicisms in their main language.  %In addition to mixing languages at the sentence level, it is fairly common to find the code-mixing behavior at the word level.  %This linguistic phenomenon cannot be tackled with conventional NLP systems, which are based on monolingual resources to handle the combination of multiple languages.   %Statistics show that half of the messages on Twitter are in a language other than English. This evidence suggests that other languages, including multilingualism and code-mixing, need to be considered by the NLP community. %    In this work, we present a Convolutional Neural Network  system to predict the sentiment of a given code-mixed tweet. The sentiment labels are either positive, negative, or neutral, and the languages involved are English and Spanish. Our best model utilizes only Spanish word embeddings from tweets  and does not require manual feature engineering.  %  Before classification, English texts were normalized to anonymize some entities, label stylistic patterns, and transform words to tackle some typical issues of the texts on Twitter.  %  We highlight the contributions of this work as follows:  %  %        %This paper is structured into six different sections. Section 2 contains the dataset description. As for section 3, contains the literature review that presents the existing related work on code-mixing. Section 4 depicts our methodology. Section 5 is devoted to the presentation and discussion of our experimental results. Finally, our recommendations for future research opportunities along with the conclusion are reported in section 6.   % ======================== Article section 
"," %   Code-switching is a phenomenon in which two or more languages are used in the same message. Nowadays, it is quite common to find messages with languages mixed in social media. This phenomenon presents a challenge for sentiment analysis. % forcing the models to use a mix of language resources. In this paper, we use a standard convolutional neural network model to predict the sentiment of tweets in a blend of Spanish and English languages. Our simple approach achieved a F1-score of $0.71$ on test set on the competition. We analyze our best model capabilities and perform error analysis to expose important difficulties for classifying sentiment in a code-switching setting.",223
"  .     %      % % final paper: en-us version      %        % space normally used by the marker      This work is licensed under a Creative Commons       Attribution 4.0 International License.      License details:      \url{http://creativecommons.org/licenses/by/4.0/}. }   Emphasis selection for written text in visual media is proposed by  and . The purpose of this shared task is to design automatic methods for emphasis selection, i.e. choosing candidates for emphasis in short written text, to enable automated design assistance in authoring. For example,  mentions that such a technique can be applied to some graphic design applications such as Adobe Spark to perform automatic text layout using templates that include images and text with different fonts and colors. The major challenge is that given only thousands of annotated short text data without any context about the text or visual background images, we are asked to learn the author- or domain-specific emphatic about the short text. Besides, these short text data are annotated by crowd-sourcing workers. And we find that different annotators have different standards, which increases the difficulty of this task.   To identify the most important words, we model the task as a sequential labeling problem. Our base models leverage different unsupervised language model such as ERNIE 2.0 , XLM-ROBERTA , ROBERTA  and ALBERT . These large unsupervised models are pre-trained on a large amount of unannotated data and carry valuable lexical, syntactic, and semantic information in training corpora. Our approach is as follows: firstly, the word-level output representations for the sentence are computed by pre-trained models and then fed into a designed downstream neural network for word selections; secondly, we finetune the downstream networks together with the pre-trained models on the annotated training data; thirdly, we investigate several different objective functions to learn our model; and finally, we apply feature engineering and several data augmentation strategies for further improvement.   The rest of the paper is organized as follows. In Section , we will briefly overview some related works to our system. Section  shows the details of our approach. Our experiments will be shown in Section , and Section  concludes.  
","   This paper describes the system designed by ERNIE Team which achieved the first place in SemEval-2020 Task 10: Emphasis Selection For Written Text in Visual Media. Given a sentence, we are asked to find out the most important words as the suggestion for automated design. We leverage the unsupervised pre-training model and finetune these models on our task. After our investigation, we found that the following models achieved an excellent performance in this task: ERNIE 2.0, XLM-ROBERTA, ROBERTA and ALBERT. We combine a pointwise regression loss and a pairwise ranking loss which is more close to the final $Match_{m}$ metric to finetune our models. And we also find that additional feature engineering and data augmentation can help improve the performance. Our best model achieves the highest score of 0.823 and ranks first for all kinds of metrics.",224
"  Coreference resolution aims at identifying all the expressions that refer to the same entity in a text.  It helps to derive the correct interpretation of a text by binding antecedents  with their pronouns together and recognizing the syntactic relationship among them. The coreference resolution is considered as a critical preprocessing step for various high-level natural language processing  tasks including document summarization, question answering, and information extraction .   Existing coreference resolution approaches can be divided into two major categories: mention-pair models  and entity-mention models . One of the main shortcomings of the mention-pair model is making each coreference decision without entity-level information. Moreover, the lack of information about the preceding clusters may result in contradictory links. The entity-mention model tries to make use of the non-local information by encouraging the sharing of features across all mentions that point to the same real-world entity. However, the coreferent mentions usually spread far apart in a text, which makes it extremely difficult to define effective global features.    Previous studies either count on the long-term memory  or their variants to implicitly capture the global features   or seek to incorporate the features of the clusters already formed to determine whether a mention is coreferent with a preceding cluster . The former might miss out some important features for specific pairwise predictions without the help of the explicit entity-level features, while the latter may suffer from error propagation as false clusters are used to create entity-level features when making future predictions.  Taking the text of ``On November 3, 1992, Clinton was elected the 42nd president of the United States, and the following year Hillary Clinton became the first lady. In 2013, he won the Presidential Medal of Freedom."" as an example, we assume that three mentions ``Clinton"", ``Hillary Clinton"", and ``he"" have been well identified. The traditional mention-pair model is very likely to group these three mentions into a cluster as shown in Figure  since ``Clinton"" and ``Hillary Clinton"" share the same surname, and ``he'' agrees with ``Clinton"" both in gender and number.  To make use of information about the clusters already formed, recent studies try to better represent the current mention by incorporating the features derived from the preceding cluster it will most probably join .  However, those methods only allow such information to be shared in a forward fashion, i.e., from antecedent expressions to postcedent ones, and are prone to reaching the results as shown in Figure  and .  The reason is that once ``Hillary Clinton'' is merged with ``Clinton'' to form a cluster, the pronoun ``he'' either joins the formed cluster or begins a new one by itself.  Even though these errors might be recovered by using a proper decoding algorithm at test time, such as the maximum spanning tree algorithm, similar errors cannot be completely eliminated.  If such information can be shared iteratively in both forward and backward ways, the disagreement in gender between ``Hillary Clinton'' and ``he'' will be detected when the representation of ``Clinton'' is updated by its two possible co-references, which helps to find the correct result as Figure .  Recently, graph neural network  has gained increasing popularity due to its ability in modeling the dependencies between nodes in a graph . For the coreference resolution, mentions are linked to each other via the edges modeling how likely two linked mentions refer to the same entity. The features between nodes  can be shared in each direction with message passing or neighborhood aggregation in an iterative way. We found the entity-centric features can be well captured by GNN, achieving close to state-of-the-art performance.  To avoid contradictory links in mention clustering results, we propose to use a variant of the maximum spanning tree algorithm, second-order decoding algorithm instead of the traditional greedy search algorithm  and the beam search algorithm .  We factorize the score of a tree into the sum of its arc-pair scores.  A pair of arcs link three different mentions, and the connected mentions can be viewed as a small cluster.  Our global inference algorithm up to second-order features helps to define powerful entity-level features between clusters of mentions by aggregating the scores of those small clusters.   Traditional coreference resolution methods usually include three successive steps: mention detection, candidate pair generation, and mention clustering .  However, recent studies  show that joint solutions usually lead to improved performance over pipelined systems by avoiding error propagation. We follow the line of these research and formulate coreference resolution in a joint manner.  Our contributions are summarized as follows:  graph neural networks are introduced to perform coreference resolution, which aims to better leverage the entity-centric information by encouraging the sharing of features across all mentions that refer to the same entity;   a global inference algorithm up to second-order features is presented to optimally cluster mentions into consistent groups;   we show our GNN-based method combing with the second-order decoding algorithm achieved close to state-of-the-art performance on the CoNLL-2012 coreference resolution benchmark.  
"," One of the major challenges in coreference resolution is how to make use of entity-level features defined over clusters of mentions rather than mention pairs. However, coreferent mentions usually spread far apart in an entire text, which makes it extremely difficult to incorporate entity-level features. We propose a graph neural network-based coreference resolution method that can capture the entity-centric information by encouraging the sharing of features across all mentions that probably refer to the same real-world entity. Mentions are linked to each other via the edges modeling how likely two linked mentions point to the same entity.  Modeling by such graphs, the features between mentions can be shared by message passing operations in an entity-centric manner. A global inference algorithm up to second-order features is also presented to optimally cluster mentions into consistent groups. Experimental results show our graph neural network-based method combing with the second-order decoding algorithm  achieved close to state-of-the-art performance on the English CoNLL-2012 Shared Task dataset.",225
" Encoding linguistic units such as words, phrases or sentences into low-dimensional vectors has been the core and preliminary task for deep learning of natural language. The current language representation learning is usually done in different individual levels, typically, word or sentence. The former includes pioneering works such as word2vec, GloVe and fastText , and the latter includes the very recent so-called contextualized representations such as ELMo, GPT, BERT, XLNet and ELECTRA . Nevertheless, few works were done to uniformly learning and representing linguistic units in different hierarchies in the same vector space. Actually, nearly all existing work still focus on individual granular language unit for representation learning .  However, universal representation among different levels of linguistic units may offer a great convenience when it is needed to handle free text in language hierarchy in a unified way. As well known that, embedding representation for a certain linguistic unit  enables linguistics-meaningful arithmetic calculation among different vectors, also known as word analogy. For example,  results in . Thus universal representation may generalize such good analogy features or meaningful arithmetic operation onto free text with all language levels involved together. For example, Eat an onion : Vegetable :: Eat a pear : Fruit.   In this paper, we explore the regularities of representations including words, phrases and sentences in the same vector space. To this end, we introduce universal analogy tasks derived from Google's word analogy dataset. In addition, we train a Transformer-based model and compare it with currently popular representation methods. Experimental results demonstrate that well-trained Transformer-based models are able to map sequences of variable lengths into a shared vector space where similar sequences are close to each other. Meanwhile, addition and subtraction of embeddings reflect semantic and syntactic connections between sequences. In addition, we explore the applicability of this characteristic in retrieval-based chatbots by evaluation on an insurance FAQ task, where the universal representation models significantly outperform TF-IDF and BM25.   
"," Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific level of linguistic unit, which cause great inconvenience when being confronted with handling multiple layers of linguistic objects in a unified way. Thus this work introduces and explores the universal representation learning, i.e.,  embeddings of different levels of linguistic unit in a uniform vector space through a task-independent evaluation. We present our approach of constructing analogy datasets in terms of words, phrases and sentences and experiment with multiple representation models to examine geometric properties of the learned vector space. Then we empirically verify that well pre-trained Transformer models incorporated with appropriate training settings may effectively yield universal representation. Especially, our implementation of fine-tuning ALBERT on NLI and PPDB datasets achieves the highest accuracy on analogy tasks in different language levels. Further experiments on the insurance FAQ task show effectiveness of universal representation models in real-world applications.",226
"   The ability to learn tasks continuously during a lifetime and with limited supervision is a hallmark of human intelligence. This is enabled by efficient transfer of knowledge from past experience. On the contrary, when current deep learning methods are subjected to learning new tasks in a sequential manner, they suffer from catastrophic forgetting , where previous information is lost due to the shift in data distribution.  Non-stationarity is inevitable in the real world where data is continuously evolving. Thus, we need to design more robust machine learning mechanisms to deal with catastrophic interference.   Lifelong learning, also known as continual learning , aims at developing models that can continuously learn from a stream of tasks in sequence without forgetting existing knowledge but rather building on the information acquired by previously learned tasks in order to learn new tasks . One conceptualization of this is to accelerate learning by positive transfer between tasks while minimizing interference with respect to network updates . Many approaches to continual learning employ manually-designed techniques such as regularization  or gradient alignment  to mitigate catastrophic forgetting, which have been shown effective in computer vision and reinforcement learning tasks.   A recent trend in continual learning, as well as machine learning in general, is to directly learn generalizable solutions via meta-learning . Meta-learning  aims to learn new tasks quickly using a limited number of examples by training on many related tasks. In continual learning, meta-learning has been applied with the objective of learning new tasks continually with a relatively small number of examples per task   or in a traditional continual learning setup by interleaving with several past examples from a memory component, i.e. experience replay  . While a high rate of experience replay  usually mitigates catastrophic forgetting, it comes closer to a multi-task learning than a lifelong learning setup and is computationally expensive when learning on a data stream in real-life applications.  In natural language processing , continual learning still remains relatively unexplored . Despite the success of large pre-trained language models such as BERT , they still require considerable amounts of in-domain examples for training on new tasks and are prone to catastrophic forgetting . Existing continual learning approaches to language processing tasks include purely replay-based methods , a meta-learning based method  as well as a generative replay-based method . However, these approaches suffer from several important limitations: they require task identifiers, a high rate of replay and multiple epochs of training, which deviates from a realistic lifelong learning scenario; or tend to have an expensive inference step .   In this paper, we propose a novel approach to lifelong learning on language processing tasks using meta-learning and experience replay that is sparse in time and size. We consider the realistic setting where only one pass over the training set is possible and no task identifiers are available. We extend two algorithms, namely online meta-learning   and a neuromodulatory meta-learning algorithm   to the domain of NLP and augment them with an episodic memory module for experience replay. While their original objective is to continually learn a new sequence of tasks during testing time, we enhance them for the conventional continual learning setup where evaluation is on previously seen tasks, thus directly addressing the problem of catastrophic forgetting. Furthermore, by realizing experience replay as a query set, we directly optimize to prevent forgetting. We show that combining a strong language model such as BERT along with meta-learning and sparse replay produces state-of-the-art performance on lifelong text classification and relation extraction benchmarks when compared against current methods under the same realistic setting. To the best of our knowledge, ours is the first meta-learning approach to lifelong learning of language tasks that incorporates sparse replay. Through further experiments, we demonstrate that our approach is considerably more efficient than previous work in terms of computational complexity as well as memory usage. To facilitate further research in the field, we make our code publicly available.  
"," Lifelong learning requires models that can continuously learn from sequential streams of data without suffering catastrophic forgetting due to shifts in data distributions. Deep learning models have thrived in the non-sequential learning paradigm; however, when used to learn a sequence of tasks, they fail to retain past knowledge and learn incrementally. We propose a novel approach to lifelong learning of language tasks based on meta-learning with sparse experience replay that directly optimizes to prevent forgetting. We show that under the realistic setting of performing a single pass on a stream of tasks and without any task identifiers, our method obtains state-of-the-art results on lifelong text classification and relation extraction. We analyze the effectiveness of our approach and further demonstrate its low computational and space complexity.",227
"  Humans possess the ability to encode and express a wide range of intricate verbal and non-verbal cues based on goal and context. This has evolved into a complementary ability to detect nuanced cues in everyday communication.  This ability is a result of top-down processing  where based on context and learning humans are able to encode and decode person to person information flow efficiently. Context is typically set by what is being communicated and how through multi-modal cues.  Inspired by this, several studies have shown that multi-modal input to the systems can improve accuracy on  tasks involving human communication, such as speech recognition , emotion recognition  and speaker recognition .  Recently, use of generalized feature representations have become prevalent in the computer vision and natural language research. Computer vision tasks like object detection and semantic segmentation show improved accuracy when the features from the images are extracted using models trained on large amounts of data like ImageNet .  In the natural learning literature, generalized embeddings like GloVe and word2vec have demonstrated state of the art performance in several tasks like word similarity, word analogy and named entity recognition.  For speech applications like automatic speech recognition , speaker recognition and paralinguistics it is still traditional to use hand-crafted features like MFCCs, LFBEs or features from toolkits like openSMILE .  However, it has also been demonstrated that features learned directly from audio can improve performance when the amount of training data is large enough .    The research in the various domains has demonstrated that  transfer learning with models trained on large datasets can improve accuracy on subsequent tasks. This is especially important when the size of the labeled datasets is not large. There are a variety of multi-modal tasks like emotion recognition which still do not have large amounts of publicly available datasets.  Motivated by this, we propose a model to learn  embeddings that combine the features from audio, video, and text modalities to improve the performance on downstream tasks. The main contribution of this paper is to understand if we can leverage large datasets to build these representations that can outperform the models built for specific tasks where the datasets are limited. For our work, we use emotion recognition as the downstream task to evaluate the embeddings. In practical applications, it is possible that all modalities are not available to the machine learning system for inference. For example, for any applications that use video from web-based applications, any disturbance in the communication network can lead to missing audio or visual input. This leads to the second objective of our study; to perform ablation studies to understand the impact of the missing modality, and understand how to compensate for it.  This paper is organized as follows; in Section , we discuss prior work in multi-modal tasks and embedding generation techniques.  Our proposed technique for embedding extraction is presented in Section . In  we discuss the training setup and data. Finally, we present our results in Section  and conclude in Section .  
","     General embeddings like word2vec, GloVe and ELMo have shown a lot of success in natural language tasks. The embeddings are typically extracted from models that are built on general tasks such as skip-gram models and natural language generation. In this paper, we extend the work from natural language understanding to multi-modal architectures that use audio, visual and textual information for machine learning tasks. The embeddings in our network are extracted using the encoder of a  transformer model trained using multi-task training. We use person identification and automatic speech recognition as the tasks in our embedding generation framework. We tune and evaluate the embeddings on the downstream task of emotion recognition and demonstrate that on the CMU-MOSEI dataset, the embeddings can be used to improve over previous state of the art results.",228
" % %1: briefly mention document-level RE and tell what task this paper focuses on. Relation extraction  task aims to recognize relations between two entities present in a document. It plays a pivotal role in understanding unstructured text and constructing knowledge bases. Although the task of document-level relation extraction has been studied extensively in the past, the task of relation extraction from dialogues has yet to receive extensive study. %As intelligent dialogue systems attract more and more attention, it has become an interesting topic that we do research on relation extraction under dialogue background. %  %2: Compare dialogue RE with previous document-level RE, and present an example to show what our task looks like. Conversational text exhibits intra- and inter-utterance relations, which makes it different from the text % we focus on in previous document-level relation extraction. Most previous works focus on professional and formal % written literature like biomedical documents and Wikipedia articles. These kinds of datasets are well-formatted and logically coherent with clear referential semantics. Hence for most NLP tasks analyzing a few continuous sentences are enough to grasp pivotal information. %  However, for dialogue relation extraction, conversational text is sampled from daily chat, which is more casual in nature. Hence its logic is simpler but entangled and referential ambiguity always occurs to an external reader. Compared with formal literature, it has lower information density but is more difficult for model to understand. Moreover, compared with other document-level RE dataset such as DocRED, dialogue text has much more cross-sentence relations. % ~  presents an example of dialogue relation extraction, taken from DialogRE dataset. In order to infer the relation between Speaker1 and Emma, we may need to find some triggers to recognize the characteristics of Emma. Triggers are evidences that can support the inference. As we can see, the following utterances are talking about Emma, and the key word baby daughter mentioned by Speaker1 is a trigger, which provides an evidence that Emma is Speaker1's daughter. %As we can see, there is a large gap between argument pair Emma and Baby Got Back. To infer the relation between them, we should first locate some triggers. In this case, personal pronoun `her' in the previous utterance is a trigger. After analyzing utterances between these two arguments, we know the only person Speaker1 and Speaker2 are talking about is Emma. So we know `her' refers to Emma. Next, we can get the fact that Emma is laughing when she hears `Baby Got Back', giving us an evidence to infer the relation alternate names.    %3: Previous approaches on document-level RE Prior works show that triggers of arguments facilitate the document-level relation inference. Thus, DocRED dataset provides several supporting evidences for each argument pair. Some efforts utilize the dependency paths of arguments to find possible triggers. For example, LSR model constructs meta dependency paths of each argument pair and aggregates all the word representations located in these paths to their model, in order to enhance model's reasoning ability.  uses syntactic parsing and coreference resolution to find intra- and inter-related words of each argument.  proposes an edge-oriented graph to synthesize argument-related information. These models are graph-based and have proven powerful in encoding long-distance information. However, for dialogue relation extraction, interlocutors exist in every utterance of the dialogue, and they are often considered as an argument. Although these previous approaches have utilized entity features of arguments, most of them employ meta dependency paths to find the related words, which results in the missing of necessary information related to speakers, since the speaker references have very little dependency features in each utterance. We think the structure of our graph allows it to model the intra- and inter-speaker relations through paths that involve conversational discourse and word-level semantics. This phenomenon enables the model to outshine the state-of-the-art frameworks int the task of dialogue level relation extraction. %Moreover, although  and  have construct nodes containing arguments, the way they choose is to average the mention representations of each argument after each sentence have een encoded by a context encoder. This will lose global information of each argument in the conversation.  %4: Introduce our contribution In this work, we propose a simple yet effective attention-based heterogeneous graph neural network to tackle the dialogue relation extraction task by using multi-type features to create the graph and employing graph attention mechanism to propagate contextual information. Different from most of the previous works, our proposed model is customized for the relation extraction task in dialogue background, as we have specially modeled speaker information and designed a mechanism to propagate massages among different sentences for better inter-sentence representation learning. %In this case, what question can be solved.... % %5: Structure of this paper The remainder of this paper is organized as follows:  elaborates on our proposed framework;  introduces the used dataset and baseline models;  lays out the experiment results and analysis; ~ briefly discusses relevant works of heterogeneous graph neural networks; and  concludes the paper.  % 
","  Dialogue relation extraction  aims to detect the relation between two entities mentioned in a multi-party dialogue. It plays an important role in constructing knowledge graphs from conversational data increasingly abundant on the internet and facilitating intelligent dialogue system development. % Previous document-level relation extraction tasks mainly focus on professional text like biomedical documents and Wikipedia articles. However, dialogue relation extraction is different, as conversational text is mainly based on spoken language. So it contains a lower information density and more inter-sentence interactions. Additionally, interlocutors are also considered as argument entities although their names may not exist in the utterances. The prior methods of DRE do not meaningfully leverage speaker information---they just prepend the utterances with the respective speaker names. Thus, they fail to model the crucial inter-speaker relations that may give additional context to relevant argument entities through pronouns and triggers. We, however, present a graph attention network-based method for DRE where a graph, that contains meaningfully connected speaker, entity, entity-type, and utterance nodes, is constructed. This graph is fed to a graph attention network for context propagation among relevant nodes, which effectively captures the dialogue context. % Further, the utterance representations are derived by passing their syntactic parse trees, that explicitly capture key syntactic relations between different entities within an utterance, through a graph convolutional network . We empirically show that this graph-based approach quite effectively captures the relations between different entity pairs in a dialogue as it outperforms the state-of-the-art approaches by a significant margin on the benchmark dataset DialogRE. Our code is released at: \url{https://github.com/declare-lab/dialog-HGAT} %   % a syntactically-aware heterogeneous graph attention network to tackle this problem, where we utilize syntactic dependency graph to extract features of utterances, specially construct speaker nodes and entity semantic nodes, and aggregate utterance information and entity type information to argument entities. Our framework outperforms the state-of-the-art approaches by a significant margin on the benchmark dataset DialogRE.",229
" Humans exhibit resilience to orthographic variation in written text .  As a result, spelling mistakes and typos are often left unnoticed.  This flexibility of ours, however, is shown to be detrimental for neural machine translation  systems, which typically are trained on curated corpora and tend to break when faced with noisy data . Achieving NMT robustness to human blunder, however, is important when translating texts of less formal origins, such as chat conversations, social media posts and web pages with comment sections.   In this work, we propose, to augment NMT system's training data with data where source sentences are corrupted with adversarial examples of different types. There have been various studies on the impact of different types and sources of noise on NMT . In this work, we focus on the noise caused by orthographic variation of words, such as unintentional misspellings and deliberate spelling alternations as well as noise due to misplaced and omitted punctuation. Thus, the closest to this study is the work on black-box adversarial training of NMT systems , where models are trained on adversarial examples that are generated without accessing the model's parameters.  Unlike the previous work, which focuses only on adversarial examples that model unintentional changes of spelling, we also model deliberate orthographic alternation, such as omission and substitution of diacritical signs. As we show in our experiments, such orthographic variation has a more substantial negative impact on MT outputs than the other types of noise and thus is more important to be accounted for.  Further, to overcome the lack of curated evaluation datasets as required by the previous work , we propose an automatic evaluation method that measures the noise invariance of MT outputs without relying on a reference translation. By measuring noise invariance of MT outputs the method also allows us to assess whether MT system translation consistency improves when facing small variations in the source text.  [h] '' Were possible, noise is marked in bold, otherwise it is indicated with `\_'.}  \toprule \# & Type                    & \multicolumn{1}{c}{Examples} \\  \midrule 1  & introduce extra letters & Balzta j濂磖a, za鍕焌 zeme.      \\ 2  & delete letters          & \_alta j濂磖a, za鍕焌 zeme.        \\ 3  & permute letters         & Batla j濂磖a, za鍕焌 zeme.       \\ 4  & confuse letters         & Balta j濂磖a, xa鍕焌 zeme.       \\ 5  & add diacritic           & Balta j濂磖a, za鍕焌 z鑶縨e.       \\ 6  & sample substitute       & Balta j濂磖a, za鍕焌 zemi.       \\ \midrule 7 & remove punctuation      & Balta j濂磖a\_ za鍕焌 zeme\_         \\ 8 & add comma               & Balta, j濂磖a, za鍕焌 zeme.     \\ \midrule 9  & latinize                & Balta jura, zala zeme.       \\ 10 & phonetic latinize       & Balta juura, zalja zeme.     \\        
"," Neural machine translation systems typically are trained on curated corpora and break when faced with non-standard orthography or punctuation. Resilience to spelling mistakes and typos, however, is crucial as machine translation systems are used to translate texts of informal origins, such as chat conversations, social media posts and web pages. We propose a simple generative noise model to generate adversarial examples of ten different types. We use these to augment machine translation systems' training data and show that, when tested on noisy data, systems trained using adversarial examples perform almost as well as when translating clean data, while baseline systems' performance drops by 2-3 BLEU points. To measure the robustness and noise invariance of machine translation systems' outputs, we use the average translation edit rate between the translation of the original sentence and its noised variants. Using this measure, we show that systems trained on adversarial examples on average yield 50\% consistency improvements when compared to baselines trained on clean data.",230
"  Semantic role labeling , namely semantic parsing, is a shallow semantic parsing task that aims to recognize the predicate-argument structure of each predicate in a sentence, such as who did what to whom, where and when, etc. Specifically, SRL seeks to identify arguments and label their semantic roles given a predicate. SRL is an important method for obtaining semantic information that is beneficial to a wide range of natural language processing  tasks, including machine translation, question answering, and discourse relation sense classification and relation extraction.  SRL can be split into four subtasks: predicate detection, predicate disambiguation, argument identification, and argument classification.  For argument annotation, there are two formulizations .  One is based on constituents , while the other is based on dependencies. The other, proposed by the CoNLL-2008 shared task, is also called semantic dependency parsing and annotates the heads of arguments rather than phrasal arguments. Figure  shows example annotations.    In prior SRL work, considerable attention has been paid to feature engineering, which struggles to capture sufficient discriminative information compared to neural network models, which are capable of extracting features automatically. In particular, syntactic information, including syntactic tree features, has been known to be extremely beneficial to SRL since the large scale of empirical verification of~. Despite their success, their work suffered from erroneous syntactic input, leading to an unsatisfactory performance.  To alleviate the above issues,  proposed a simple but effective neural model for SRL without syntactic input. Their work suggested that neural SRL does not have to rely on syntactic features, contradicting the belief that syntax is a necessary prerequisite for SRL, which was believed as early as~. This dramatic contradiction motivated us to make a thorough exploration on syntactic contribution to SRL.  Both span and dependency are effective formal representations for semantics, though it has been unknown which form, span or dependency, would be better for the convenience and effectiveness of semantic machine learning and later applications for a long time. This topic has been roughly discussed in , who both concluded that the  dependency SRL system at then clearly outperformed the span-based  system through gold syntactic structure transformation; however, due to the different requirements of downstream task applications, span and dependency both remain focuses of research. Additionally, the two forms of SRL may benefit from each other joint rather than separated development.  We, therefore, revisit the syntax roles under a more solid empirical basis and explore the syntax roles for the two styles with syntax information in equal quality, respectively.  Recent works on syntax contributions have been limited to individual models and the ways in which syntax has been utilized. The conclusions drawn for syntax roles therefore have some limitations. In order to reduce these limitations, we explored three typical and strong baseline models and two categories of syntactic utilization methods. In addition, pre-trained language models, such as ELMo  and BERT , that build contextualized representations, continue to provide gains on NLP benchmarks, and  showed that structure of syntax information emerges in the deep models' word representation spaces. Whether neural SRL models can further benefit from explicit syntax information in addition to this implicit syntax information, however, is another issue we consider.  %This paper will focus on semantic dependency parsing and formulate SRL as one or two sequence tagging tasks with predicate-specific encoding. With the help of the proposed -order argument pruning algorithm over syntactic tree, our model obtains state-of-the-art scores on the CoNLL benchmarks for both English and Chinese.  Besides, most of SRL literature is dedicated to impressive performance gains on English, while other multiple languages receive relatively little attention. Although human languages have some basic commonalities in syntactic structure and even different levels of grammar, their differences are also very obvious. The study of syntactic roles needs to be examined in the context of multiple languages for verifying its effectiveness and applicability.  In order to quantitatively evaluate the contribution of syntax to SRL, we adopt the ratios between labeled F score for semantic dependencies  and the labeled attachment score  for syntactic dependencies, F score for syntactic constituents. This ration was first introduced by CoNLL-2008  Shared Task as an evaluation metric. Considering that various syntactic parsers contribute different syntactic inputs with varying levels of quality, the ratio provides a fairer comparison between syntactically-driven SRL systems, which our empirical study surveys.  
"," Semantic role labeling  is dedicated to recognizing the semantic predicate-argument structure of a sentence.  Previous studies in terms of traditional models have shown syntactic information can make remarkable contributions to SRL performance; however, the necessity of syntactic information was challenged by a few recent neural SRL studies that demonstrate impressive performance without syntactic backbones and suggest that syntax information becomes much less important for neural semantic role labeling, especially when paired with recent deep neural network and large-scale pre-trained language models. Despite this notion, the neural SRL field still lacks a systematic and full investigation on the relevance of syntactic information in SRL, for both dependency and both monolingual and multilingual settings.  This paper intends to quantify the importance of syntactic information for neural SRL in the deep learning framework. We introduce three typical SRL frameworks , sequence-based, tree-based, and graph-based, which are accompanied by two categories of exploiting syntactic information: syntax pruning-based and syntax feature-based. Experiments are conducted on the CoNLL-2005, 2009, and 2012 benchmarks for all languages available, and results show that neural SRL models can still benefit from syntactic information under certain conditions. Furthermore, we show the quantitative significance of syntax to neural SRL models together with a thorough empirical survey using existing models.",231
"   Building a dialogue system that can converse with people naturally and meaningfully is one of the most challenging problems towards high-level artificial intelligence, and has been drawing increasing interests from both academia and industry area. Most existing dialogue systems are either generation-based or retrieval-based. Given the dialogue context, generation-based approaches synthesize a response word by word with a conditional language model, while retrieval-based methods select a proper response from a candidate pool. In this paper, we focus on retrieval-based approaches that are superior in providing informative responses and have been widely applied in several famous commercial products such as XiaoIce from Microsoft and AliMe Assist from Alibaba.  We consider the response selection task in multi-turn dialogues, where the retrieval model ought to select a most proper response by measuring the matching degree between a multi-turn dialogue context and a number of response candidates. Earlier studies concatenate the context to a single utterance and calculate the matching score with the utterance-level representations. Later, most response selection models  perform context-response matching within the representation-matching-aggregation paradigm, where each turn of utterance is represented individually and sequential information is aggregated among a sequence of utterance-response matching features. To further improve the performance of response selection, some recent approaches consider multiple granularities  of representations for matching or propose more complicated interaction mechanisms between the context and the response.    Recently, a wide range of studies have shown that pre-trained language models , such as BERT, XLNET and RoBERTa, on the large corpus can learn universal language representations, which are helpful for various downstream natural language processing tasks and can get rid of training a new model from scratch. To adapt pre-trained models for multi-turn response selection,  and  make the first attempt to  utilize BERT to learn a matching model, where context and the candidate response are first concatenated and then fed into the PLMs for calculating the final matching score.  These pre-trained language models can well capture the interaction information among inter-utterance and intra-utterance through multiple transformer layers. Although PLM-based response selection models demonstrate superior performance due to its strong representation ability, it is still challenging to effectively learn task-related knowledge during the training process, especially when the size of training corpora is limited. Naturally, these studies typically  learn the response selection model with only the context-response matching task %learn the matching model with the single response prediction task,  and overlook many potential training signals  contained in dialogue data. %come from rich characteristics of dialogue text. Such training signals might  be  beneficial  for  context  understanding  and  produce better  features  for  response  prediction.  Besides, the response retrieved by existing dialogue systems supervised by the conventional way still faces some critical challenges, including  incoherence  and  inconsistency.     On account of the above issues, in this paper, instead of configuring complex context-response matching models, we propose learning the context-response matching model with auxiliary self-supervised tasks designed for dialogue data based on pre-trained language models . Specifically, we introduce four self-supervised tasks  including  , ,  and , and  jointly  train  the  PLM-based  response  selection  model with  these  auxiliary  tasks  in  a  multi-task  manner.  On the one hand, these auxiliary tasks help improve the capability of the response selection model to understand the dialogue context and measure the semantic relevance, consistency or coherent between the context and the response candidates. On the other hand, they can guide the matching model to effectively learn task-related knowledge with a fixed amount of train corpora and produce better features for response prediction.   We conduct experiments on two benchmark data sets for multi-turn response selection: the Ubuntu Dialog Corpus and the E-commerce Dialogue Corpus. Evaluation results show that our proposed approach is significantly better than all state-of-the-art models on both datasets. Compared with the previous state-of-the-art methods, our model achieves 2.9\% absolute improvement in terms of  for the Ubuntu dataset and 4.8\% absolute improvement for the E-commerce dataset. Furthermore, we applied our proposed self-supervised learning schema to some non-PLM-based response selection models, e.g., dual LSTM and ESIM. Experimental results indicate that our learning schema can also bring consistent and significant improvement to the performance of the existing matching models. Surprisingly, with self-supervised learning, a simple ESIM even performs better than BERT on the ubuntu dataset, demonstrating that our approach is beneficial for various matching architectures. %   In summary, our contributions are three-fold:    
"," Building an intelligent dialogue system with the ability to select a proper response according to a multi-turn context is a great challenging task. Existing studies focus on building a context-response matching model with various neural architectures or PLMs and typically learning with a single response prediction task. These approaches overlook many potential training signals contained in dialogue data, which might be beneficial for context understanding and produce better features for response prediction.  Besides, the response retrieved from existing dialogue systems supervised by the conventional way still faces some critical challenges, including incoherence and inconsistency. To address these issues, in this paper, we propose learning a context-response matching model with auxiliary self-supervised tasks designed for the dialogue data based on pre-trained language models. Specifically, we introduce four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination, and jointly train the PLM-based response selection model with these auxiliary tasks in a multi-task manner.   By this means, the auxiliary tasks can guide the learning of the matching model to achieve a better local optimum and select a more proper response. Experiment results on two benchmarks indicate that the proposed auxiliary self-supervised tasks bring significant improvement for multi-turn response selection in retrieval-based dialogues, and our model achieves new state-of-the-art results on both datasets.",232
" 	 	Named Entity Recognition  is the process of identification of named entities  in natural language text. The present paper concentrates on three low resource languages : Bhojpuri, Maithili and Magahi , which belong to the Indo-Aryan language family. This work may be seen as the first attempt to develop an NER tool for Bhojpuri, Maithili and Magahi. There is no previous work on NER for these languages as far as we know. The main aim of the present paper is to start with insights from the NER systems that are developed for Indian Languages with more resources and based on that we try to develop an NER System for BMM. 	 	The NER module can be an important component  in  Natural  Language  Processing and Information Extraction systems.  It is an essential task for computational purposes like Machine Translation , developing search engines, automatic indexing, document classification  and  text  summarization, questiona answering etc., because it is not possible to build end-to-end Deep Learning systems for these languages due to the lack of data. It  will also  be helpful  in  many  cross-linguistic  applications  as  it is relevant for other Indian Languages, particularly LRLs. The present study mainly focuses on Named Entities  for BMM with machine translation as the goal. 	 	  	The concept of Named Entity was introduced in the Sixth Message of Understanding Conference . It was often seen as part of an Information Extraction system, which refers to the automatic extraction of structured information such as entities, relationships between entities and attributes describing entities from unstructured sources. The role of NER system is to locate and classify words in a text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities etc. The NEs could be identified in two conventional ways, before the recent success of machine learning and then Deep Learning based techniques: 	 	 		 	 	It is a challenging task to implement NER for Indian languages due to the absence of capitalization in their writing systems. On the other hand, these systems are phonetically organized and designed, which makes it easily possible to use phonetic features for NER for Indian languages. Preparing a gazetteer閳ユ獨 list for all nouns is impossible because there can be a vast number of unknown named entities in the world in terms of a corpus versus a language. Here, one important point to be noted is that not much work has been reported for NER for Low Resource languages due to insufficient lexical resources and also due to morphological richness. There have been efforts on major Indian languages, i.e., Hindi, Tamil, Telugu, Urdu, Punjabi, but no efforts on Low Resource Indian languages such as BMM. 	 	 	 	Bhojpuri is often considered a major `sub-language' of Hindi. It is not only a language which is spoken in various states of India but in other countries as well, viz. Nepal, Mauritius, Fiji, Surinam etc. The writing system of Bhojpuri was earlier Kaithi script but now Devanagari script is used more to write Bhojpuri. According to 2011 census~, there are 5,05,79,447 Bhojpuri speakers. 	 	Maithili belongs to the Indo-Aryan language family, while Bhojpuri and Magahi are considered `sub-languages'  of Hindi and are mainly spoken in Eastern Uttar Pradesh, Bihar and Jharkhand states of India. Maithili is included in the 22 `scheduled' languages of the Republic of India . Maithili was added in the Constitution of India in 2003 by the 92nd Constitutional Amendment Act. Maithili, a sister language of Hindi, is spoken in India, particularly in Bihar, Jharkhand, Uttar Pradesh etc. as well as in Nepal. It is the only language in the Bihari sub-family that is included in the eighth schedule of the Indian constitution. There are 1,35,83,464 Maithili speakers . It is also one of the 122 recognised languages of Nepal. In 2007, Maithili was included in the interim Constitution of Nepal and in March 2018, it received the second official language status in the Jharkhand state of India. It too was earlier considered a sub-language or a dialect. 	 	Magahi or Magadhi, also considered a major sub-language of Hindi, is chiefly spoken in some districts of Bihar, Jharkhand, and also in the Maldah district of West Bengal. Magahi was also written in the Kaithi script in earlier days, but at present it is usually written in the Devanagari script. There are 1,27,06,825 Magahi speakers . 	 	Earlier work on machine translation  has reported that proper handling of named tokens can improve the translation quality and performance. These named tokens would have been translated during source to target translation without an NER module, but with an NER module they can instead be simply transliterated. The current BMM machine translation systems for which we plan to use our NER module, is based on a transfer-based approach to machine translation. Even though the MT systems are based on a transfer approach, the NER module  can be based on machine learning or Deep Learning, not a rule-based approach. Due to this, we have annotated some corpus and developed an NER system for these three languages and have reported the lower and a higher baseline results. The former is based on CRF and the latter on a combination of Long Short Term Memory , Convolutional Neural Networ  and Conditional Randon Fields , called LSTM-CNNs-CRF. 	 	 	As there is no prior work on the NER problem for Bhojpuri, Maithili and Magahi, the contributions in this paper are as follows: 	 		 	 	 	 	
"," 			In Natural Language Processing  pipelines, Named Entity Recognition  is one of the preliminary problems, which marks proper nouns and other named entities such as Location, Person, Organization, Disease etc. Such entities, without a NER module, adversely affect the performance of a machine translation system. NER helps in overcoming this problem by recognising and handling such entities separately, although it can be useful in Information Extraction systems also. Bhojpuri, Maithili and Magahi are low resource languages, usually known as Purvanchal languages. This paper focuses on the development of a NER benchmark dataset for the Machine Translation systems developed to translate from these languages to Hindi by annotating parts of their available corpora. Bhojpuri, Maithili and Magahi corpora of sizes 228373, 157468 and 56190 tokens, respectively, were annotated using 22 entity labels. The annotation considers coarse-grained annotation labels followed by the tagset used in one of the Hindi NER datasets. We also report a Deep Learning based baseline that uses an LSTM-CNNs-CRF model. The lower baseline F$_1$-scores from the NER tool obtained by using Conditional Random Fields models are 96.73 for Bhojpuri, 93.33 for Maithili and 95.04 for Magahi. The Deep Learning-based technique  achieved 96.25 for Bhojpuri, 93.33 for Maithili and 95.44 for Magahi.",233
"  As a fundamental task in speech and language processing, Automatic Speech Recognition  aims to generate transcripts from human speech. Recently, the successful application of deep neural networks has pushed the accuracy of end-to-end ASR models to a new level, but brings significant challenges for building large-scale, robust ASR systems, especially for industrial applications. Major bottlenecks are twofold: i) abundant labeled training data for learning large, accurate ASR models; and ii) an efficient distributed, computing framework for model training and serving at scale.  In this demo, we present EasyASR, a distributed machine learning platform to address both challenges. EasyASR is built upon the Machine Learning Platform for AI  of Alibaba Cloud~\footnote{https://www.alibabacloud.com/product/machine-learning/}, which provides an ultra-scale, deep learning framework on distributed GPU clusters. Our platform supports the complete process of training, evaluating and serving ASR models. Additionally, it is integrated with the functionalities i) to extract high-quality audio aligned with transcripts from massive video data and ii) to expand existing ASR training sets with various augmentation policies. We have designed easy-to-use PAI components that enable users to build or run ASR models within only a few lines of command, which  hides complicated techniques from starters. We also provide add-on configurations with the PAI commands to allow advanced users to customize network architectures for their own models. On EasyASR, we achieve state-of-the-art performance for Mandarin speech recognition over multiple public datasets. %In the following, we describe EasyASR in detail.   %  
"," We present EasyASR, a distributed machine learning platform for training and serving large-scale Automatic Speech Recognition  models, as well as collecting and processing audio data at scale. Our platform is built upon the Machine Learning Platform for AI of Alibaba Cloud. Its main functionality is to support efficient learning and inference for end-to-end ASR models on distributed GPU clusters. It allows users to learn ASR models with either pre-defined or user-customized network architectures via simple user interface. On EasyASR, we have produced state-of-the-art results over several public datasets for Mandarin speech recognition.",234
" Since   propose the sequence-to-sequence  model for machine translation, the development of NLP applications has been almost inseparable from this framework. In the field of abstractive summarization, the seq2seq model is first applied by  to summarize sentences. With the recent bloom of the attention mechanism and pre-trained models, more summarization models are built as extensions of seq2seq . Albeit the rapid theoretical evolution, applications of neural abstractive summarization that are adequately mature to be industrialized have yet to exist. Inspire by Google's Neural Machine Translation  , this study makes an exploratory attempt to improve the established abstractive summarization models with a more reliable solution on the coverage problems.   In this study, a multi-document summarization is improved by the paragraph-level attention-aware inference\footnote{In this paper, the paragraph-level attention distribution is a normalized vector comprised of the total attention weights at all time steps for each source paragraph.}. In comparison to single-document summarization, multi-document summarization has a higher requirement for summary coverage as it always includes massive information from different sources. Paragraph-level attention-aware inference is theoretically applicable to all seq2seq models which adopt the attention mechanism to capture the cross-paragraph relationships.\footnote{For the single-document summarization, a sentence-level attention-aware inference is preferred.} To prove the universality of attention-aware inference empirically, in addition to using Liu's Hierarchical Transformer  , we also design a Parallel HT  as the seq2seq summarization model. Both HTs adopt the multi-head attention to represent the relationships between paragraphs, but Liu's HT integrates these representations into tokens before modelling cross-token dependencies, whilst PHT represents cross-paragraph and cross-token relationships parallelly.  Beam search is the backbone of sequence inference. However, the vanilla beam search tends to generate typical and dull sentences to avoid making mistakes  which results in neglecting salient information,   suggest a structure regularization to disallow excessive attention to the same source. The vital disadvantage of this approach is its potential of cutting the attention on actually important sources due to the lack of knowledge on the attention distribution. To fill in the gap between neural translation and summarization, we argue that the inference of the latter could be as tightly regulated as the translation inference with regards to a specific optimal attention distribution. Unlike the one-to-one alignment in NMT, which leads to the sum of attention weights of each word equaling to 1 , the optimal attention distribution of summarization is a hypothetical concept depending on the source documents. This study taps into the determination of the optimal attention distribution of input paragraphs for multi-document summarization by learning the paragraph-level attention distribution generated based on trained HT parameters and gold summary. With the predicted optimal attention distribution, the attention-aware inference refines the score function of the beam search in order to produce summaries that come along with attention closest to the optimality.    Overall, the authors believe the attention-aware inference is provided with the following three advantages.        To the best of our knowledge, this paper is the first to introduce attention-aware from NMT into NAS and we have to admit that the process is not straightforward.    
"," Inspired by Google's Neural Machine Translation   that models the one-to-one alignment in translation tasks with an uniform attention distribution during the inference, this study proposes an attention-aware inference algorithm for Neural Abstractive Summarization  to regulate generated summaries to attend to source contents with the optimal coverage. Unlike NMT, NAS is not based on one-to-one transformation. Instead, its attention distribution for the input should be irregular and depend on the content layout of the source documents. To address this matter, we construct an attention-prediction model to learn the dependency between the optimal attention distribution and the source. By refining the vanilla beam search with the attention-aware mechanism, significant improvements on the quality of summaries could be observed. Last but not the least, the attention-aware inference has strong universality that can be easily adopted to different hierarchical summarization models to promote the models' performance. \footnote{See supplements for  the code and best checkpoints.}",235
"  Pretraining ever-larger language models  on massive plain text corpora has led to significant improvements on a wide range of NLP tasks ]{radford2018improving,devlin2018bert,liu2019roberta,raffel2019exploring}. A standard approach is to replace the pretrained model's output layer with a task-specific head and finetune the entire model on a set of labeled training data. However, language modeling is not only a powerful pretraining objective, but many tasks can be reformulated as cloze questions , allowing pretrained LMs to solve them without any or with only very few labeled examples .     Very recently,  introduced \gpt{}, a pretrained LM with an enormous 175 billion parameters, and showed that it has amazing few-shot abilities: By reformulating tasks as language modeling problems, \gpt{} achieves near state-of-the-art results for some tasks in the SuperGLUE benchmark  given just 32 labeled examples. This is achieved through : \gpt{} is given a few demonstrations of inputs and corresponding outputs as context for its predictions, but no gradient updates are performed. While being straightforward to use, this method has two major drawbacks:   	. 	           as the context window of most LMs is limited to a few hundred tokens.   An alternative to priming is   , which combines the idea of reformulating tasks as cloze questions with regular gradient-based finetuning. While \pet{} additionally requires unlabeled data, unlabeled data is much easier to obtain than labeled examples for many real-world applications. Crucially, \pet{} only works when the answers to be predicted by the LM correspond to a single token in its vocabulary; this is a severe limitation as many tasks cannot easily be worded that way.   In this work, we modify \pet{} to also work for tasks that require predicting more than one token. We then show that in combination with ALBERT , \pet{} and its iterative variant  both outperform \gpt{} on SuperGLUE with 32 training examples, while requiring only 0.1\% of its parameters . Finally, we show that similar performance can also be achieved without unlabeled data and provide a detailed analysis of the factors contributing to \pet{}'s strong performance: its ability to combine multiple task formulations, its resilience to wordings that are hard to understand, its usage of labeled data, and characteristics of the underlying LM.  
"," When scaled to hundreds of billions of parameters, pretrained language models such as \gpt{}  achieve remarkable few-shot performance on challenging natural language understanding benchmarks. In this work, we show that performance similar to \gpt{} can be obtained with language models whose parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain some form of task description, combined with gradient-based optimization; additionally exploiting unlabeled data gives further improvements. Based on our findings, we identify several key factors required for successful natural language understanding with small language models.\footnote{Our implementation is publicly available at \url{https://github.com/timoschick/pet}.}",236
"  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Most neural machine translation systems are autoregressive, hence decoding latency grows linearly with respect to the length of the target sentence. For faster generation, several work proposed non-autoregressive models with sub-linear decoding latency given sufficient parallel computation~.   As it is challenging to precisely model the dependencies among the tokens without autoregression, many existing non-autoregressive models first generate an initial translation which is then iteratively refined to yield better output~.  While various training objectives are used to admit refinement , the generation process of these models is similar in that the refinement process happens in the  space of sentences.  Meanwhile, another line of work proposed to use  latent variables for non-autoregressive translation, such that the distribution of the target sentences can be factorized over time given the latent variables~.  Unlike the models discussed above, finding the most likely target sentence under these models requires searching over continuous latent variables. To this end,  proposed an EM-like inference procedure that optimizes over a hybrid space consisting of both continuous and discrete variables. By introducing a deterministic delta posterior, it maximizes a proxy lowerbound by alternating between matching the delta posterior to the original approximate posterior , and finding a target sentence that maximizes the proxy lowerbound .  In this work, we propose an iterative inference procedure for latent variable non-autoregressive models that purely operates in the continuous space.} Given a latent variable model, we train an inference network to estimate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. At inference time, we find the target sentence that approximately maximizes the log probability by  initializing the latent variable e.g. as the mean of the prior, and  following the gradients estimated by the inference network.  We compare the proposed approach with the EM-like inference~ on three machine translation datasets: {\wmtende}, {\wmtroen} and { at the expense of  BLEU score.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
"," We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation~, we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality , we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure~ that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on {\wmtende}, {\wmtroen} and {, for instance, our approach is able to decode $6.2$ times faster than the autoregressive model with minimal degradation to translation quality .",237
"  Deep learning methods have revolutionized the NLP field in the past ten years. Although LSTM networks  have been around for over two decades, the NLP community only learned how to train and use them effectively in the past ten years.  introduced a new sequence-to-sequence method, boosting the field of neural machine translation significantly . The same year  presented the attention mechanism aimed at focusing on specific words within the prefix, in order to make the most accurate prediction of the next word while mapping one sequence to another. During the same period new text representation methods were adapted, complementing the following representation methods: bag-of-words , tf-idf, and one-hot vectors with dense representations, such as the very prominent word2vec  and Glove  embeddings, which served as the go-to methods in many works .   introduced a pre-trained transformer  based on the attention mechanism without any recurrent connections. BERT provided another advancement in the field of pre-trained text representations, showing enhanced performance on various NLP tasks .   Many research directions were shaped by  pre-trained word embeddings and representations with several software toolkits available for training deep neural networks. While the Keras  toolkit was widely used for text classification  with padding, the DyNet  and PyTorch  toolkits excelled at tasks in which a dynamic computation graph of the recurrent networks was exploited to achieve better predictive performance with sentences of varying length .  An important advancement in the dense representation area occurred with the introduction of TensorFlow  Hub in 2018. According to Google.  
"," One of the challenges in  the NLP field is training  large  classification  models, a task that is both difficult and tedious. It is even harder when GPU hardware is unavailable. The increased availability of pre-trained and off-the-shelf word embeddings, models, and modules aim at easing the process of training large models and achieving a competitive performance.   We explore the use of off-the-shelf BERT models and share the results of our experiments and compare their results to those of LSTM networks and more simple baselines. We show that the complexity and computational cost of BERT is not a guarantee for enhanced predictive performance in the classification tasks at hand.",238
" Autoregressive models are ubiquitous in natural language processing.  Due to the sequential nature of text generation, they are often the tool of choice for tackling sequence-to-sequence problems such as translation , summarization , and dialogue .  Furthermore, they form the backbone of several successful generative pre-training architectures .  Two recent trends have made autoregressive models cumbersome to deploy in real-world, natural language generation  applications.  First, state-of-the-art models have grown larger and larger, amounting to hundreds of millions and even billions of parameters .  The increase in size and depth dramatically slows down inference speed.  Second, the architecture of choice for autoregressive models seems to have shifted from the recurrent neural network   to the Transformer .  Though the Transformer's self-attention mechanism improves performance, it also increases the computational complexity of the step-by-step generation algorithms that are used at test time.  Thus, both of these trends have contributed to significantly increasing inference time costs, especially on CPUs and low-resource devices, hindering their use in production systems.  % The increasing memory and inference time costs of these enormous models make them cumbersome to deploy in real-world settings.  Inference on a CPU can already be quite slow, much less a smartphone device.  Thus, there exists a need to scale down these large autoregressive models for practical purposes.     is one popular method for model compression.  It transfers the information learned by a large, pretrained  to a smaller, untrained  .  In comparison to other methods such as weight pruning and quantization, KD allows the compressed model's architecture to significantly differ from that of the original teacher.  This feature enables models trained with KD to achieve high performance while meeting particular inference requirements .   , proposed by , is the dominant technique for autoregressive KD in the current NLG literature, especially for machine translation .  This method trains a student model using a modified dataset generated by the teacher model and the standard negative log-likelihood objective.  While SeqKD is simple and efficient, we argue that it does not take advantage of the teacher's full potential.    %This method is a two-step procedure that 1) generates full sequences using the teacher model to produce a modified dataset and 2) trains the student model on the modified dataset with standard negative log-likelihood  training.  While seqKD is conceptually simple and efficient to implement, we argue that reducing the teacher's impact to a static dataset does not take advantage of its full potential.  % Autoregressive models are often trained in a way that is different from how they are used at inference time.  During training, the true sequence is available, so the model learns to predict one-step-ahead given the ground-truth context.  However, at inference time, the model must generate the entire sequence from scratch by repeatedly using its own outputs as context for subsequent steps.  This training-inference inconsistency leads to the  problem, which may be manifested as a decrease in sequence quality as the number of generation steps increases.  The seqKD algorithm is simply NLL training with a modified dataset, so it also experiences this issue.   Training the student model with a  dataset leads to the exposure bias problem. During training, the student model learns to predict the next token given previous tokens provided by the data. However, at inference time, the student generates the entire sequence from scratch by repeatedly using its own outputs as context for subsequent steps.  This training-inference inconsistency causes a decrease in generation quality.  Alternatively, we propose that the student can leverage the teacher in a  fashion during the learning process.  % Our main contributions are the following:  We recast distillation for autoregressive models as an imitation learning problem, drawing parallels between SeqKD and behavioral cloning.   From this perspective, we design a new compression algorithm aimed at addressing exposure bias for autoregressive models called  .   We conduct several experiments in translation and summarization, demonstrating that ImitKD is especially suitable for compressing deep Transformers that achieve high performance into shallow RNNs that generate much faster at inference time.  %The key insight of ImitKD is to treat the teacher model as an oracle that corrects the student閳ユ獨 generations at every step.  Thus, the student explicitly learns how to generate during training.  Our method consistently outperforms other popular distillation algorithms, such as SeqKD.  It yields student models that beat models trained without a teacher by 1.4 to 4.8 points on the Bleu and Rouge metrics.     We devise a new compression algorithm for autoregressive models called  .  It is inspired by an imitation learning  perspective on the autoregressive distillation problem.  Our algorithm trains a student model within an IL framework by treating the teacher as an oracle, and allows the student to explore its own generation during training.  The teacher corrects the student's generation at every time step, thereby guiding the student in learning how to generate. %  %   Experimental results in translation and summarization show that ImitKD is especially suitable for compressing deep Transformer models that achieve high performance into shallow RNNs that generate up to 14 times faster at inference time.  Our method consistently outperforms other distillation algorithms , and yields student models that beat models trained without a teacher by 1.4 to 4.8 points on generation metrics such as BLEU and ROUGE. %
"," The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.  However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings.  We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation.  The algorithm is designed to address the exposure bias problem.     On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation.  Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.\footnote{Our code can be found at \url{https://github.com/asappresearch/imitkd}.}",239
"   Extracting event temporal relations from raw text data has attracted surging attention in the NLP research community in recent years as it is a fundamental task for commonsense reasoning and natural language understanding. It facilitates various downstream applications, such as forecasting social events and tracking patients' medical history. Figure shows an example of this task where an event extractor first needs to identify events  in the input and then a relation classifier predicts all pairwise relations among them, resulting in a temporal ordering as illustrated in the figure. For example,  is \temprel{before} ;  \temprel{includes} ; the temporal ordering between  and  cannot be decided from the context, so the relation should be \temprel{vague}.      
","  Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori  inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.",240
" Encoder-decoder architecture, which uses an encoder to create a representation of source sequence and a decoder to predict target sequence, have been established as state of the art approaches in neural machine translation   . Recurrent neural network based  model , convolutional neural network  model and self-attention network based  model  are representative encoder-decoder models, and most of NMT models are variants or combination of these three. NMT models based on encoder-decoder architecture are similar in  some aspects, such as stack of layers having the same structure.  Stack of layers increases the complexity of model to approximate nonlinear function. Viewing all layers as one function, every single layer captures different information from input. Looking into every single NMT model such as RNN-based model or SAN-based model, models always try to make representation of one word containing information of whole sentence in every layer. However, empirically, one layer alone cannot result in satisfactory result.   It is common to regard sentence in NMT model as a directed complete simple graph, which views words as nodes and relationships between words as edges. However, this perspective only focuses on relationship between words, while ignoring other information, such as relationship between phrases or relationship between different fragments of sentences. As a result, structure of simple graph cannot fully reflect all information.   To overcome the shortcomings of simple graph, we view sentence as a multigraph  in SAN-based model. In multigraph , multiple edges exist between two nodes. Edge connects not only nodes but also subgraphs of  which reflects relationship between different fragments of sentences more than relationship of word-pair. Encoding is also regarded as a process of generating a multigraph to approximate  infinitely. Compared with simple graph, multigraph can explain th essence of encoding more comprehensively, and explain relationship between words in a more general way.  One layer in NMT model can capture the incremental information automatically compared with its previous layer. Fusion of the previous and incremental information makes representation more rich and thus benefits translation. From the perspective of multigraph, incremental information can be described as a set of higher-order subgraphs generated by this layer. Even though the current NMT models can capture information of subgraphs of different orders, fusing them into a representation with a fixed weight makes the model difficulty to pay more attention on really salient part.   To solve this problem, we propose a graph-based SAN empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current NMT models. First of all, we generally define a full representation as the fusing result of all concerned subgraph representations. Then let the representation of one layer split into two parts, previous representation and incremental representation. The previous representation reflects full representation from previous layer, and the incremental representation reflects new information generated in this layer. Based on this, the encoding process is modified to adapt to such representation division. We split the original self-attention into three independent parts to generate incremental representation. Our method accommodates subgraphs of different orders into different parts of incremental representation, and reduces the information redundancy. To fuse the full representation, We consider three fusing strategies in terms of different weighting schemes so that let the model focus on important parts of representation.        In experiments on WMT14 English-to-German  and IWSLT14 German-to-English , results of experiments prove our model can improve performance of translation with a few parameters increasing. Our model achieves a performance outperforming the Transformer with an improvement of 1.1 BLEU points in En-De and 1.0 BLEU points in De-En.      
"," Neural machine translation  usually works in a seq2seq learning way by viewing either source or target sentence as a linear sequence of words, which can be regarded as a special case of graph, taking words in the sequence as nodes and relationships between words as edges. In the light of the current NMT models more or less capture graph information among the sequence in a latent way, we present a graph-to-sequence model facilitating explicit graph information capturing. In detail, we propose a graph-based SAN-based NMT model called Graph-Transformer by capturing information of subgraphs of different orders in every layers. Subgraphs are put into different groups according to their orders, and every group of subgraphs respectively reflect different levels of dependency between words. For fusing subgraph representations, we empirically explore three methods which weight different groups of subgraphs of different orders. Results of experiments on WMT14 English-German and IWSLT14 German-English show that our method can effectively boost the Transformer with an improvement of 1.1 BLEU points on WMT14 English-German dataset and 1.0 BLEU points on IWSLT14 German-English dataset.",241
" Spoken dialogue systems  connect users and computer applications through human-machine conversations.  The users can achieve their goals, such as finding a restaurant, by interacting with a task-oriented SDS over multiple dialogue rounds or { of the system so as to track the progress of the dialogue.  In the context of this work, a state  is the user's intention or interest accumulated from the conversation history, and the user's intention or interest at each turn is referred to as turn-level state.  %The state and immediate state are usually expressed in terms of a set of slot-value pairs.  % For example, the state  contains two slot-value pairs, . In this example,  and  are {cheapunknownchinese}; 		}; 		}; 		_{1}}; 		}; 		};			 		 		}; 		}; 		_{3};	 		\node[blank][below= of B2]{\Huge$;	 													 		\path  edge ; 		\path  edge ; 		\path  edge ;	 		\path  edge ; 		\path  edge ; 		\path  edge ; 		 		\path  edge ; 		\path  edge ;	 		\path  edge ; 		\path  edge ; 		\path  edge ;  		\path  edge ; 		\path  edge ;	 		\path  edge ; 		\path  edge ; 		\path  edge ; 				 
"," Dialogue state tracking  is an important part of a spoken dialogue system. Existing DST models either ignore temporal feature dependencies across dialogue turns or fail to explicitly model temporal state dependencies in a dialogue. In this work, we propose Temporally Expressive Networks  to jointly model the two types of temporal dependencies in DST. The TEN model utilizes the power of recurrent networks and probabilistic graphical models. Evaluating on standard datasets, TEN is demonstrated to improve the accuracy of turn-level-state prediction and the state aggregation.   %The existing models usually solve DST problem by two approaches, Implicit Tracking and Explicit Tracking. The Implicit Tracking employs recurrent networks to model the temporal feature dependencies across dialogue turns, but fails to consider the temporal state dependencies. While Explicit Tracking models state dependencies explicitly, it ignores the feature dependencies.  %Dialogue state tracking  is an important part of a spoken dialogue system. Some of the prior arts for DST build an momentary-state predictor and map the historical immediate states to the accumulative state. These models are however insufficiently modelling the temporal dependencies across dialogue turns and the uncertainties in state updating. In this work, we introduce a probabilistic graphical model to formulate the dialogue process and propose Temporally Expressive Networks  that utilizes hierarchical recurrent networks and belief propagation to deal with these issues. Evaluating on standard datasets, the proposed model is demonstrated to be significantly effective in improving accuracy for immediate-state prediction and reducing errors in state updating.  % a GRU  encoder sharing parameters across all slots to capture global information. The slot attention is adopted for each slot to capture local information. To alleviate the error propagation caused by hard decision on the turn-level goals, we propose a neural-network-based DST model with factor graphs and introduce the sum-product algorithm to handle the problem. Experimental stdies demonstrate that the proposed approach significantly improves the current art in tracking the joint goals.",242
" %The web has became important source of accessing knowledge and information in our daily lives. It's necessary to develop some intelligent applications that can help users access and understand the web information easily.  Spoken dialogue system  is an application that can help users complete their goals efficiently.  % Users can achieve their goals, such as booking a restaurant, by communicating with a SDS in natural language over multiple dialogue turns.  An SDS usually has a logic engine, called dialogue manager, which involves two main sub-tasks for determining how the system will respond to the users: dialogue state tracking and dialogue policy learning. The task we discuss in this paper is dialogue state tracking, which allows the system maintaining an internal representation of the state of the dialogue as the dialogue progress.   Dialogue state tracking involving a single domain has been extensively studied and achieved much progress. As a more challenging task, Multi-domain dialogue state tracking  has been introduced in and attracts much attention in the research community.  %The { domain, and the system answers a restaurant name { and u3 in Figure). Thus a correctly modeling of these dependencies can improve slot-value extraction and cross-turn inference. In this work, we build an Interactive Encoder which completely accords with the dependencies expressed in Figure to jointly model the in-turn dependencies and cross-turn dependencies.   The interactive nature of dialogues also implies that the value for a slot tends to be specified frequently either by a system or by a user. For example, the values for slots involving names, such as { are likely to be provided by the system. And the values for the slots like {  are usually provided by the user. This observation inspires our designing of the distributed copy mechanism, which allows the state generator choosing to copy words from either the historical system utterances or the historical user utterances.  The other aspect is the slot overlapping problem in MDST. Unlike single-domain DST, slot overlapping is common in MDST, and these overlapping slots share similar values. For example, both the { domain have a slot { in Figure) or values for specific slot . The user usually act as an option provider     %each  pair can obtain better features for this  pair. 2) unlike single-domain DST problem, slot overlapping exists in multi-domain DST problems and these overlapping domains share the similar values. A global feature extractor may fail to extract correct features from the dialogue history. For example, both { domain has a slot { domain and the { and { and { value at turn 1), summary from the utterance  pair at turn 4). 
"," The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multi-domain dialogue state tracking  models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks  to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.  % Multi-domain dialogue state tracking  involves large-size ontology and cross-turn inference, making it a challenge in research community. Recent MDST models fail to considering the interactive dependencies and the slot overlapping in multi-domain dialogues. In this work, we propose a robust generation-based model, Parallel Interactive Networks  , to tackle with the MDST challenge. More precisely, PIN incorporates an Interactive Encoder to jointly model the cross-turn dependencies and in-turn dependencies. The slot-level context is introduced in PIN to extract more expressive features for different slots. And a distributed copy mechanism is utilized in PIN to selectively copy words from historical system utterances and history user utterances. The PIN is demonstrated to outperform existing models on bench-marking multi-domain state tracking datasets.	   %The model however ignore the dependencies between words from system-side and user-side, and the context of decoder in these models fails to incorporating local features from specific domain and slot. In this paper, we propose a parallel-interactive recurrent neural network to modeling the human-system-interaction nature of the dialogues and introduce local context modeling to enhance the state generation performance. And a special distributed-copy operation is designed in the decoder that can copy a word from either the system-side utterances or the user-side utterances, which improves the robustness of the model. The proposed model is demonstrated to outperform existing models on bench-marking multi-domain state tracking data sets.	 	 %Multi-domain dialogue state tracking involves complex dialogue context and domain transferring, making it a challenge in research community. The traditional classification-based dialogue state tracking models need predefined ontology and are unable to dealing with unknown slot-values. The recent generation-based models tackle with this issue by incorporating the copy mechanism and generating the value sequence using the sequence-to-sequence framework. However, these generation-based models are limited in their simple encoder that insufficiently considers the human-system interaction property of the dialogues, and the context of decoder in these models fails to incorporating local features from specific domain and slot. In this paper, we propose a parallel-interactive recurrent neural network to modeling the human-system-interaction nature of the dialogues and introduce local context modeling to enhance the state generation performance. And a special adversarial-copy operation is designed in the decoder that can copy a word from either the system-side utterances or the user-side utterances, which improves the robustness of the model. The proposed model is demonstrated to outperform existing models on bench-marking multi-domain state tracking data sets.",243
"  %%General subject  is the process of generating coherent natural language text from non-linguistic data. Despite community agreement on the text and speech output of these systems, there is far less consensus on what the input should be. A large number of inputs have hence been employed for  systems, including images , numeric data, and  data. Practical applications can be found in domains such as weather forecasts , feedback for car drivers , diet management . %%%specific problem subject  Presently, the generation of natural language from  %, more precisely from   data has gained substantial attention. The RDF-to-text task has hence been proposed to investigate the quality of automatically generated texts from  .  %Moreover,  has demonstrated a promising ability to support the creation of  benchmarks.  With the emergence of neural methods, end-to-end data-to-text models have been introduced to learn input-output mappings directly. These approaches rely much less  %\todo{less is a comparative, ergo less than what?}  on explicit intermediate representations compared to rule-based approaches.   Although Neural  models have been achieving very good results  %\todo{cite paper where this is shown} , English is the only language that has been widely targeted.   % \todo[inline]{why it is important to be able to generate different language text with the same model} % \todo[inline]{What is the motivation behind investigating generation for different language families?} In this work, we alleviate this language limitation by proposing a multilingual approach, named NABU. The motivation behind multilingual models lies in several directions, mainly in  transfer learning; when low-resource language pairs are trained together with high-resource languages, the translation quality improves;  zero-shot translation, where multilingual models are able to translate between language pairs from similar families that were never seen during training;  Easy deploy, a multilingual model achieving same performance on many languages in comparison to several separate language-specific models are much more desirable for companies in terms of deployment.  Our approach, NABU, is based on the fact that knowledge graphs are language-agnostic and hence can be used on the encoder side to generate multilingual text. NABU consists of an encoder-decoder architecture which incorporates structural information of RDF triples using an encoding mechanism inspired by . In contrast to recent related work, NABU relies on the use of a reification  %\todo{sure?}  strategy for modeling the graph structure of RDF input. The decoder part  %\todo{do you mean decoder?}  is based on the vanilla Transformer model along with an unsupervised tokenization model.  %which implements  and unigram language model for handling  multilinguality. %\todo{Is the statement below really necessary in here?Would make sense to add that in the details of the approach.}  %Note that NABU follows the same strategy of recent literature on multilingual  models in which a special token is used in the encoder to determine to what target language to translate.  %evaluation We evaluate NABU on the standard benchmarking WebNLG datasets in three settings: monolingual, bilingual and multilingual. For the monolingual setting, we compare NABU with state-of-the-art English approaches and also perform experiments on Russian and German. The goal of the bilingual setting is to analyze the performance of NABU for language families. To achieve this goal, we train and evaluate bilingual models using NABU on English-German and on English-Russian. In the multilingual setting, we compare NABU with a multilingual Transformer model on English, German and Russian. %%%results Our results show that NABU outperforms state-of-the-art approaches on English and achieves 66.21 BLEU. NABU also achieves consistent results across all languages on multilingual settings with 56.04 BLEU. In addition, NABU presents promising results on the bilingual models with 61.99 BLEU.  %\todo{numbers?}  Our findings suggest that NABU is able to generate multilingual text with similar quality to that generated by humans. %conclusion The main contributions of this paper can be summarized as follows:       -Transformer architecture for generating multilingual text from RDF KGs.       The version of NABU used in this paper and also all experimental data are publicly available.~\footnote{https://github.com/dice-group/NABU}.  
"," The RDF-to-text task has recently gained substantial attention due to continuous growth of Linked Data. In contrast to traditional pipeline models, recent studies have focused on neural models, which are now able to convert a set of RDF triples into text in an end-to-end style with promising results. However, English is the only language widely targeted. We address this research gap by presenting NABU, a multilingual graph-based neural model that verbalizes RDF data to German, Russian, and English. NABU is based on an encoder-decoder architecture, uses an encoder inspired by Graph Attention Networks and a Transformer as decoder. Our approach relies on the fact that knowledge graphs are language-agnostic and they hence can be used to generate multilingual text. We evaluate NABU in monolingual and multilingual settings on standard benchmarking WebNLG datasets. Our results show that NABU outperforms state-of-the-art approaches on English with 66.21 BLEU, and achieves consistent results across all languages on the multilingual scenario with 56.04 BLEU. %Moreover, we trained bilingual models for analyzing the capability of NABU to model jointly distinct language families such as English-Russian.  %\todo{Which conclusion did you reach from this training?}",244
"   We are digitally surrounded by computational Language Models  that guide us while writing to reduce the user effort, suggest different options for words/sentences to enhance our style, or fix our grammatical/correctness errors accurately . Many of the keys we press while writing on a keyboard act as part of the inputs to compose new datasets for those models that shape how we communicate with others. Nevertheless, does it happen in the same way when we write code?  Succinctly, yes. According to some recent surveys found in the literature  , the Natural Language Processing  subfield related to programming language includes examples of LMs used in several tasks and contexts. For example, the authors of  used different techniques such as graph-based statistical LMs, probabilistic LMs, or Deep Learning  LMs to suggest code to programmers similarly to auto-completer features in IDEs. LMs were used to generate automated source code based on sample code inputs or pseudo-code and evaluating how this generated code performs . Another exciting application of NLP into source code languages is the automatic translation between different languages. The work reported in  explores different supervised and unsupervised approaches to migrate code between different programming languages to improve interoperability or port codebases written in obsolete or deprecated languages . Another example found is the use of Bayesian networks, attention mechanisms, and pointer networks  to fill a given code portion with missings.  There is a more general understanding of the natural languages閳 different characteristics in the NLP broad field. Since there exist many research fields related to human languages, there is a richer background on existing language characteristics. For example, there is much knowledge on aspects like the minimal representation units of a word in a specific language, the most used words of a language, or if a word is a neologism or not. Programming languages share some syntax similarities with spoken languages. However, it does not have the same restrictions in the sense of common words or neologisms , or other syntax restrictions and features such as punctuation, format, or style. Every programming language has indeed reserved words and symbols to denote different actions, resources, or syntax. However, there is an essential part of the source code that is only limited by the programmer閳ユ獨 imagination, the conventions existing, or the guides for good practices. As  claims,    [...] traditional language models limit the vocabulary to a fixed set of common words. For code, this strong assumption has been shown to have a significant negative effect on predictive performance [...]  In that paper, Karampatsis and Sutton  present how segmenting words into subword units can improve source code modeling. Similarly, other researchers  dug in representing source code vocabulary with a similar emphasis on modeling words using sub-word units and envisioning their importance when using neural networks . Nevertheless, how that word segmentation affect the accuracy or the appropriateness of the code generated or auto-completed in some modern LM using deep learning approaches? That kind of question raises the main goal for this paper: discover what kinds of associations between different modern neural network architectures and tokenization models produce the best results when creating LMs to generate and auto-complete source code.  To pursue that goal, this research aims to conduct experiments combining different deep neural network  architectures with different tokenization and pre-trained models over an existing Python dataset. Using that experimentation, we want to investigate the combinations that improve code generation and auto-completion tasks  while checking the outcomes from those tasks using metrics like accuracy and human assessment.  The rest of the paper is as follows: Section 2 presents the different approaches followed during the research, the DNNs used, the software methods and data employed. Section 3 describes results achieved during the research according to different metrics and tests, while section 4 discusses these findings and the implications of the results as appropriate. Finally, Section 5 presents some conclusions.  
"," In recent years, the use of deep learning in language models gained much attention. Some research projects claim that they can generate text that can be interpreted as human-writing, enabling new possibilities in many application areas. Among the different areas related to language processing, one of the most notable in applying this type of modeling is programming languages. For years, the Machine Learning community has been researching this software engineering area, pursuing goals like applying different approaches to auto-complete, generate, fix, or evaluate code programmed by humans. Considering the increasing popularity of the Deep-Learning-enabled language models approach, we detected a lack of empirical papers that compare different deep learning architectures to create and use language models based on programming code. This paper compares different neural network architectures like AWD-LSTMs, AWD-QRNNs, and Transformer while using transfer learning and different tokenizations to see how they behave in building language models using a Python dataset for code generation and filling mask tasks. Considering the results, we discuss each approach闁炽儲鐛 different strengths and weaknesses and what gaps we find to evaluate the language models or apply them in a real programming context.",245
"    A dominant approach to text generation  is to use autoregressive models learned by maximum likelihood estimation  on supervised data. However, this approach introduces two well-known discrepancies between training and evaluation objectives that lead to undesired generations.  % First, the training loss is negative log-likelihood, whereas the evaluation is based on human judgment of the output quality.  Under model misspecification, MLE tends to over-generalize, assigning large probability mass to both high-quality and low-quality sequences . Therefore, in practice, we must carefully select the  decoding algorithms to produce high-quality outputs.  Second, during training, the autoregressive model conditions on the gold history/prefix; however, at inference time it conditions on model-generated history. This is known as the exposure bias problem . In the worst case, one incorrect prediction can produce a low-probability prefix under the gold data distribution, and errors compound in each of the following steps . In practice, prior work has observed problems such as repetition and hallucination partly due to exposure bias .   We aim to bridge the gap between training and evaluation in this paper. To match training and evaluation objectives, ideally we should maximize output quality given model-generated histories. This corresponds to the reinforcement learning  objective: maximizing the expected reward  over trajectories  induced by the policy .  However, optimizing this objective is notoriously difficult. Prior RL approaches mainly focus on fine-tuning a learned model to optimize sequence-level metrics such as BLEU~, but empirically it remains unclear if RL is beneficial to text generation . % Note that many challenges in RL arise from exploring an exponentially large space of sequences,  with sparse rewards only on those close to the reference. We thus propose to learn from only the reference sequences without interaction . Specifically, we use off-policy policy gradient with importance weighting , where training examples with higher probability under the model are weighted higher.  Further, our reward functions approximate human judgment of the output quality  by estimating how likely a human would have generated a sequence.  We call our algorithm \algoname .    Results on news summarization,  question generation,  and machine translation show that \algoname leads to better model performance than MLE and RL fine-tuning by both task metrics and human-rated quality.  Further, our analysis shows that \algoname learns high-precision models that are less sensitive to decoding algorithms. In addition, it alleviates exposure bias: the output quality does not degrade much as generation length increases.  
","     Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation.     This paradigm leads to       diverse but low-quality samples due to mismatched learning objective and evaluation metric      and  exposure bias due to mismatched history distributions .      To alleviate these problems, we frame text generation as an  reinforcement learning  problem with expert demonstrations ,     where the goal is to maximize quality given model-generated histories.      We propose \algoname :     an easy-to-optimize algorithm that learns from the demonstrations by importance weighting.      Intuitively, \algoname upweights confident tokens and downweights unconfident ones in the reference during training,      avoiding optimization issues faced by prior RL approaches that rely on online data collection.     According to both automatic and human evaluation,     models trained by \algoname outperform those trained by MLE and policy gradient      on summarization, question generation, and machine translation.      Further, our models are less sensitive to decoding algorithms     and alleviate exposure bias.",246
"  \let\thefootnote\relax\footnote{  Corresponding author.}  Recent years have witnessed significant improvements in vision and language communities, which have consequently led to substantial attention in vision-language multi-modality tasks such as visual grounding , image captioning , and visual question answering . Furthermore, as video becomes ubiquitous, as a daily source of information and communication, video-language tasks such as video captioning , video moment retrieval , and video question answering   are emerging as important topics. Among these topics, video QA is especially challenging, as it requires fine-grained understanding of both video and language.     
"," Video Question Answering  requires fine-grained understanding of both video and language modalities to answer the given questions. In this paper, we propose novel training schemes for multiple-choice video question answering with a self-supervised pre-training stage and a supervised contrastive learning in the main stage as an auxiliary learning. In the self-supervised pre-training stage, we transform the original problem format of predicting the correct answer into the one that predicts the relevant question to provide a model with broader contextual inputs without any further dataset or annotation. For contrastive learning in the main stage, we add a masking noise to the input corresponding to the ground-truth answer, and consider the original input of the ground-truth answer as a positive sample, while treating the rest as negative samples. By mapping the positive sample closer to the masked input, we show that the model performance is improved. We further employ locally aligned attention to focus more effectively on the video frames that are particularly relevant to the given corresponding subtitle sentences. We evaluate our proposed model on highly competitive benchmark datasets related to multiple-choice video QA: TVQA, TVQA+, and DramaQA. Experimental results show that our model achieves state-of-the-art performance on all datasets. We also validate our approaches through further analyses.",247
" Neural machine translation  which typically follows the encoder-decoder framework, directly applies a single neural network to transform the source sentence into the target sentence. With tens of millions of trainable parameters in the NMT model, translation tasks are usually data-hungry, and many of them are low-resource or even zero-resource in terms of training data. Following the idea of unsupervised and self-supervised pre-training methods in the NLP area , some works are proposed to improve the NMT model with pre-training, by making full use of the widely available monolingual corpora . Typically, two different branches of pre-training approaches are proposed for NMT:  and .  The  approaches seek to incorporate the sentence representation provided by the pre-trained model, such as BERT, into the NMT model .  These approaches are able to leverage the publicly available pre-trained checkpoints in the website but they need to change the NMT model to fuse the sentence embedding calculated by the pre-trained model. Large-scale parameters of the pre-trained model significantly increase the storage cost and inference time, which makes it hard for this branch of approaches to be directly used in production.  As opposed to  approaches, the  approaches aim to directly pre-train the whole or part of the NMT model with tailored objectives, and then initialize the NMT model with pre-trained parameters . These approaches are more production-ready since they keep the size and structure of the model same as standard NMT systems.  While achieving substantial improvements, these pre-training approaches have two main cons. Firstly, as pointed out by , the artificial symbols like [mask] used by these approaches during pre-training are absent from real data at fine-tuning time, resulting in a pretrain-finetune discrepancy. Secondly, while each pre-training step only involves sentences from the same language, these approaches are unable to make use of the cross-lingual alignment information contained in the source and target monolingual corpus. We argue that, as a cross-lingual sequence generation task, NMT requires a tailored pre-training objective which is capable of making use of cross-lingual alignment signals explicitly, e.g., word-pair information extracted from the source and target monolingual corpus, to improve the performance.  To address the limitations mentioned above, we propose Code-Switching Pre-training  for NMT. We extract the word-pair alignment information from the source and target monolingual corpus automatically, and then apply the extracted alignment information to enhance the pre-training performance. The detailed training process of CSP can be presented in two steps:  1) perform lexicon induction to get translation lexicons by unsupervised word embedding mapping ; 2) randomly replace some words in the input sentence with their translation words in the extracted translation lexicons and train the NMT model to predict the replaced words. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragments based on the context calculated by the encoder. By predicting the sentence fragment which is replaced on the encoder side, CSP is able to either attend to the remaining words in the source language or to the translation words of the replaced fragment in the target language. Therefore, CSP trains the NMT model to: 1) learn how to build the sentence representation for the input sentence as the traditional pre-training methods do; 2) learn how to perform cross-lingual translation with extracted word-pair alignment information. In summary, we mainly make the following contributions:    \footnotetext[1]{To be used in production easily, these models need to be distilled into a student model with the structure and size same as standard NMT systems.} 
","  This paper proposes a new pre-training method, called Code-Switching Pre-training  for Neural Machine Translation . Unlike traditional pre-training method which randomly masks some fragments of the input sentence,  the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the cross-lingual alignment information extracted from the source and target monolingual corpus. Additionally,  we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask].  To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.",248
" With increasingly larger amounts of unstructured text becoming digitally available in many different fields, the need for robust geographically-aware retrieval of information from large textual collections is now more urgent than ever.  Textual data is often deeply geographical and it has been shown that geographic queries make for a large part of all search queries . Toponym resolution is a class of entity linking that focuses specifically on geographical entities. Given a toponym  that has been recognized in text,\footnote{We do not consider toponym detection as part of the toponym resolution task in this paper. There is a large body of research in the natural language processing community that deals with the specific problem of named entity recognition, of which toponym detection is a part.} its aim is to resolve it to its spatial footprint . This step requires an external source of knowledge which usually comes in the shape of a gazetteer, that is, a dictionary of geographical entities with their associated alternative place names and geospatial information. On the other hand, candidate selection is the task of identifying the potential entities that can be referred to by a named entity recognized in text. As the intermediary step between named entity recognition and the downstream task of entity disambiguation, candidate selection is an integral part of entity linking. And yet, it has often been an overlooked component of the entity linking pipeline, even though it has been shown to have a significant impact on the final performance , especially in noisy or non-standard text.  Toponyms are particularly prone to name variations and changes, which can arise from multiple causes, such as regional spelling differences, diachronic spelling variation, and change of the geopolitical status . In toponyms, variation is common not only at a token-level , but also at a character-level , and at both token- and character-level . In addition to these, noisy text often presents other types of character-level variations, such as spelling errors, typographical errors, and OCR errors . The number of potential variations can be very high, and yet candidate selection should ensure that the correct location is provided among the pool of retrieved entities.  In this paper, we present a new and flexible deep learning approach to geographical candidate selection through toponym matching, which is specifically tailored to dealing with these challenges characteristic of noisy scenarios. Our method consists of two main components:  toponym matching, formulated as a binary classification of toponym query-candidate pairs, and  candidate selection, formulated as a ranking task where the aim is to rank the good candidates first while minimizing the presence of noisy candidates. The main contributions of this paper are:       ).       Our method has been designed to be as language-independent as possible. It only relies upon a character tokenizer when processing the string inputs and a reference gazetteer. We have tested its downstream application on datasets from different languages, time periods and origins, from seventeenth century Latin America to nineteenth century Britain and the United States. All codes, datasets, gazetteers and evaluation settings are openly available to support research reproducibility and to foster the use of DeezyMatch in other downstream tasks.\footnote{DeezyMatch codes can be found here: \url{https://github.com/Living-with-machines/DeezyMatch/}. For a more detailed description of the DeezyMatch architecture and functionalities, see . All experiments can be found here: \url{https://github.com/Living-with-machines/LwM_SIGSPATIAL2020_ToponymMatching}. We provide all resources to allow full reproducibility of the results.}  %% SECTION 
"," %   Recognizing toponyms and resolving them to their real-world referents is required for providing advanced semantic access to textual data. This process is often hindered by the high degree of variation in toponyms. Candidate selection is the task of identifying the potential entities that can be referred to by a toponym previously recognized. While it has traditionally received little attention in the research community, it has been shown that candidate selection has a significant impact on downstream tasks , especially in noisy or non-standard text. In this paper, we introduce a flexible deep learning method for candidate selection through toponym matching, using state-of-the-art neural network architectures. We perform an intrinsic toponym matching evaluation based on several new realistic datasets, which cover various challenging scenarios . We report its performance on candidate selection in the context of the downstream task of toponym resolution, both on existing datasets and on a new manually-annotated resource of nineteenth-century English OCR'd text. %",249
" 	% \{-0.3em} 	% General introduction 	Belief tracking  is an important component in task-oriented dialog systems. The system tracks user goals through multiple dialog turns, i.e. infers structured belief states expressed in terms of slots and values , to query an external database . 	Different belief tracking models have been proposed in recent years, either trained independently  or within end-to-end  trainable dialog systems . 	 	% problem 	Existing belief trackers mainly depend on supervised learning with human annotations of belief states for every user utterance. However, collecting these turn-level annotations is labor-intensive and time-consuming, and often requires domain knowledge to identify slots correctly. Building E2E trainable dialog systems, called E2E dialog systems for short, even further magnifies the demand for increased amounts of labeled data .  	 	 	 	% idea 	Notably, there are often easily-available unlabeled dialog data such as between customers and trained human agents accumulated in real-world customer services. 	In this paper, we are interested in reducing the reliance on belief state annotations in building E2E task-oriented dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. 	Intuitively, the dialog data, even unlabeled, can be used to enhance the performance of belief tracking and thus benefit the whole dialog system, because there are cues from user inputs and system responses which reveal the belief states, as shown in Figure . 	 	%The underlying idea is very simple: as the system makes responses based on its belief of user goals, we should be able to use the system response to infer the corresponding belief state.  	%The correlation between belief states and system responses have also been reported in previous works , which shows that learning belief tracking and response generation together is beneficial to both tasks. % mutual information  	 	% proposed model 	Technically, we propose a latent variable model for task-oriented dialogs, called the LAtent BElief State  dialog model. 	The model generally consists of multiple  turns of user inputs  and system responses  which are observations, and belief states  which are latent variables. 	Basically, \modelname{} is a conditional generative model of belief states and system responses given user inputs, i.e. . 	Once built, the model can be used to infer belief states and generate responses. 	More importantly, such latent variable modeling enables us to develop semi-supervised learning on a mix of labeled and unlabeled data under the principled variational learning framework . 	In this manner, we hope that the LABES model can exploit the cues for belief tracking from user inputs and system responses. 	Furthermore, we develop \modelname{}-S2S, which is a specific model instantiation of \modelname{}, employing copy-augmented Seq2Seq  based conditional distributions in implementing .  	 	%To leverage this correlation, we propose the LAtent BElief State dialog model , a conditional generative model that models belief states and system responses jointly given the user inputs.  	%In particular, we represent the structured belief state as discrete latent variables, e.g. a sequence of words defined on the vocabulary space.  	 	%With the recent advances of neural variational inference  , effective methods are proposed to address structured latent representation learning , discrete latent variable modeling  and sequential inference . Inspired by these works, we propose a VI-based scheme to learn the latent belief states sequentially over multiple dialog turns, which is employed under unsupervised scenarios. Thus our model can conduct semi-supervised learning from both labeled and unlabeled dialog data. 	 	We show the advantage of our model compared to other E2E task-oriented dialog models, and demonstrate the effectiveness of our semi-supervised learning scheme on three benchmark task-oriented datasets: CamRest676 , In-Car  and MultiWOZ  across various scales and domains.  	In supervised experiments, \modelname{}-S2S obtains state-of-the-art results on CamRest676 and In-Car, and outperforms all the existing models which do not leverage large pretrained language models on MultiWOZ.  	In utilizing unlabeled dialog data, semi-supervised \modelname{}-S2S significantly outperforms both supervised-only and prior semi-supervised baselines.  	Remarkably, we can reduce the annotation requirements to 50\% without performance loss on MultiWOZ, which is equivalent to saving around 30,000 annotations.  	 	 	
"," 		%濞存粣绠戠槐閬嶆晬鐏炲墽澹岄柟璇″枟閸ㄦ粓鎯冮崚鐞籺ro闁挎稑濂旈幈銊╁绩鐟欏嫭鍠呴悷 		Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. 		In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. 		We propose a probabilistic dialog model, called the LAtent BElief State  model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. 		Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. 		Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES\footnote{Code available at https://github.com/thu-spmi/LABES}. 		In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. 		Remarkably, we can reduce the annotation demands to 50\% without performance loss on MultiWOZ.",250
"  Deep learning has achieved significant successes, but these successes heavily rely on massive annotated data. Few-Shot Learning  is one of the keys to breaking such shackle, and commits to learning new tasks with only a few examples  . FSL has made impressive progress in many areas, such as computer vision  . But the progress of FSL in natural language processing  is much slower.  One of the primary constraints is the lack of a unified benchmark for few-shot NLP, thus new methods cannot be easily compared and iteratively improved.  %Similar to computer vision,  Existing few-shot NLP researches mainly focus on simple N-classification problems, such as  text classification  and  entity relation classification .  However, on one hand, these works often report results on their own constructed few-shot data, which is pretty inefficient in results comparison and thus hinders cumulative progress. On the other hand, these simple N-classification problems cannot reflect the complexity of real-world NLP tasks.  NLP tasks often face the challenges of structure prediction problems, such as sequence labeling  and parsing . More importantly, different NLP tasks are often deeply related to each other, i.e. multi-task problems .  One typical scenario of complex NLP is the Dialogue Language Understanding problem, which includes two sub-tasks: Intent Detection  and Slot Tagging .  As a multi-task problem, these two sub-tasks are proved to strongly promote and depend on each other .  One of the main obstacles in constructing the NLP FSL benchmark comes from the special evaluation paradigm of FSL. Few-shot models are usually first pre-trained on data-rich domains  and then tested on unseen few-shot domains. %where pre-training and test tasks need to be related . Thus, FSL evaluations always need a lot of different domains to conquer the result-randomness from domain selection and limited learning shots.  But it is often hard to gather enough domains for NLP tasks.  To solve this, existing works  construct fake domains from a single dataset. They split all labels into training labels and testing labels.  Then, they construct fake pre-training and testing domains with training and testing labels respectively, so that testing labels are unseen during pre-training. Such simulation can yield plenty of related domains, but lacks reality and only works when the label set is large.  Actually, splitting labels is impractical for many real-world NLP problems.  For example of the Name Entity Recognition, the label sets are often too small to split .  In this paper, we present FewJoint, a novel FSL benchmark for joint multi-task learning, to promote FSL research of the NLP area.  To reflect the real word NLP complexities beyond simple N-classification, we adopt a sophisticated and important NLP problem for the benchmark: Task-oriented Dialogue Language Understanding.  Task-oriented Dialogue is a rising research area that develops dialogue systems to help users to achieve goals, such as booking tickets. Language Understanding is a fundamental module of Task-oriented Dialogue that extracts semantic frames from user utterances . It contains two sub-tasks: Intent Detection and Slot Tagging.  With the Slot Tagging task, our benchmark covers one of the most common structure prediction problems: sequence labeling. Besides, thanks to the natural dependency between Intent Detection and Slot Tagging, our benchmark can embody the multi-task challenge of NLP problems. %Fig  shows an example for few-shot joint language understanding.  To conquer randomness and make an adequate evaluation,  we include 59 different dialogue domains from real industrial API, which is a considerable domain amount compared to all existing few-shot and dialogue data. We also provide a Few-shot Learning platform to ease the experiment set up and comparison.   In summary, our contribution is three-fold:  We present a novel Few-shot learning benchmark with 59 real-world domains, which allows evaluating few-shot models without constructing fake domains.  We propose to reflect real-world NLP complexities by covering the structure prediction problems and multi-task learning problems.  We propose a Few-shot Learning platform to ease comparison and implement of few-shot methods.   [t] 	}; 	 	 	%	   %
"," Few-shot learning  is one of the key future steps in machine learning and has raised a lot of attention. However, in contrast to the rapid development in other domains, such as Computer Vision, the progress of FSL in Nature Language Processing  is much slower.  One of the key reasons for this is the lacking of public benchmarks.  NLP FSL researches always report new results on their own constructed few-shot datasets, which is pretty inefficient in results comparison and thus impedes cumulative progress. In this paper, we present FewJoint, a novel Few-Shot Learning benchmark for NLP.  Different from most NLP FSL research that only focus on simple N-classification problems, our benchmark introduces few-shot joint dialogue language understanding, which additionally covers the structure prediction and multi-task reliance problems.  This allows our benchmark to reflect the real-word NLP complexity beyond simple N-classification.  Our benchmark is used in the few-shot learning contest of SMP2020-ECDT task-1.\footnote{The Eighth China National Conference on Social Media Processing. Link: \url{https://smp2020.aconf.cn/smp.html}}  We also provide a compatible FSL platform to ease experiment set-up.\footnote{The dataset and platform is available at \url{https://github.com/AtmaHou/MetaDialog}}",251
"  Event coreference resolution aims to identify which event mentions in a document refer to the same event . For example, the two event mentions in Figure , departing and leave, refer to the same EndPosition event of Nokia's CEO.  Traditional event coreference resolution methods usually rely on a series of upstream components , such as entity recognition and event detection. Such a pipeline framework, unfortunately, often suffers from the error propagation problem. For instance, the best event detection system in KBP 2017 only achieved 56 F1 , and it will undoubtedly limit the performance of the follow-up event coreference task . Furthermore, most previous approaches use hand-crafted features , which heavily depend on other NLP components  and thus are hard to generalize to new languages/domains/datasets.    In this paper, we propose an End-to-End Event Coreference method --  neural network, which can predict event chains from a raw text in an end-to-end manner. For example, taking the raw text in Figure  as input,  will directly output two event coreference chains, \{departing, leave, goodbye\} and \{rejoin\}. By jointly modeling event detection and event coreference,  neural network does not require any prior components, and the representations/pieces of evidence between different tasks and different decisions can be shared and reinforced. Besides,  are learned in an end-to-end manner, which can inherently resolve the error propagation problem.  End-to-end event coreference, however, is challenging due to the mention diversity and the long-distance coreference. First, event mentions are highly diversified , which may be a variety of syntactic objects, including nouns, verbs, and even adjectives. For example, an EndPosition event can be triggered by departing, leave, goodbye and former. By contrast, mentions in entity coreference are mostly noun phrases . Second, coreferential event mentions commonly appear over long-distance sentences, therefore event coreference is intricately governed by long-distance, semantic-dependent decisions . For example, in Figure  the closest antecedent\footnote{In this paper, antecedents are coreferential mentions that appear earlier in the document.} of the mention goodbye -- leave, is far from it. To resolve the coreference between these two distant, diverse event mentions, a system can only rely on their semantic meanings, i.e., they both describe the same EndPosition event but from different perspectives. By contrast, most of entity mentions' closest antecedents are in the same or immediately preceding sentence , which can be resolved more easily using local and syntactic clues.  To resolve the mention diversity problem and the long-distance coreference problem, this paper further proposes a type-guided mechanism into our  neural network. This mechanism bridges distant, diverse event mentions by exploiting event type information in three folds: 1) type-informed antecedent network which enables  to capture more semantic information of event mentions by predicting coreferential scores and type scores simultaneously; 2) type-refined mention representation which enhances mention representation with type information, therefore even lexically dissimilar mentions can be bridged together, such as the two diverse EndPosition mentions goodbye and departing; 3) type-guided decoding algorithm which can exploit global type consistency for more accurate event chains.  The main contributions of this paper are:  1. We propose an end-to-end neural network for event coreference resolution 閳-  neural network.  can jointly model event detection and event coreference, and learn to automatically extract features from raw text. To the best of our knowledge, this is the first end-to-end neural event coreference model that can achieve state-of-the-art performance.  2. We design a type-guided mechanism for event coreference, which can effectively resolve the mention diversity problem and the long-distance coreference problem in event coreference resolution.  3. We conduct experiments on two standard datasets: KBP 2016 and KBP 2017, which show that  achieves new state-of-the-art performance. And additional ablation experiments verify the effectiveness of the proposed type-guided mechanism.  
","   Traditional event coreference systems usually rely on pipeline framework and hand-crafted features, which often face error propagation problem and have poor generalization ability.   In this paper, we propose an End-to-End Event Coreference approach -- $\text{E}^{3}\text{C}$ neural network, which can jointly model event detection and event coreference resolution tasks, and learn to extract features from raw text automatically.   Furthermore, because event mentions are highly diversified and event coreference is intricately governed by long-distance, semantic-dependent decisions, a type-guided event coreference mechanism is further proposed in our $\text{E}^{3}\text{C}$ neural network.   Experiments show that our method achieves new state-of-the-art performance on two standard datasets.",252
" Sequence labeling assigns each token with a label in a sequence. Tasks such as Named Entity Recognition  , Part-Of-Speech  tagging  and chunking  can all be formulated as sequence labeling tasks. BiLSTM-CRF  is one of the most successful neural sequence labeling architectures. It feeds pretrained  word representations into a single layer bi-directional LSTM  encoder to extract contextual features and then feeds these features into a CRF  decoder layer to produce final predictions. The CRF layer is a linear-chain structure that models the relation between neighboring labels. In the traditional CRF approach, exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are applied for training and prediction respectively.  %The Viterbi algorithm is applied to exactly find the best label sequence in inference and the forward-backward algorithm is applied to compute posterior marginal distributions exactly for each position in training.  In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently.  In practice, we sometimes require very fast sequence labelers for training  and prediction . The BiLSTM encoder and the CRF layer both contain sequential computation and require  time over  input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure , distill larger encoders into smaller ones  or in other settings . The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks. %More recently,  proposed BiLSTM-LAN to replace the CRF layer, which has a lower time complexity, but the network introduces 3 additional LSTM layers that require sequential computations. The CRF layer is still necessary for better accuracy in many tasks, which limits the speed. %  showed such an algorithm can be unfolded as an RNN on grid-structure, we expand the work on the sequence structure and unfold the MFVI algorithm as an RNN as will  In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference  to approximately decode the linear-chain CRF. MFVI iteratively passes messages among neighboring labels to update their distributions locally. Unlike the exact probabilistic inference algorithms, MFVI can be parallelized over different positions in the sequence, achieving time complexity that is constant in  with full parallelization. %Similar to , we show that such an algorithm can be unfolded as an RNN,  Previous work  showed that such an algorithm can be unfolded as an RNN for grid CRF structure. We expand on the work for the linear-chain CRF structure and unfold the algorithm as an RNN which can be connected with the encoder to form an end-to-end neural network that is amenable to parallelization for both training and prediction. We call the unfolded RNN an approximate inference network . In addition to linear-chain CRFs, we also apply AIN to factorized second-order CRF models, which consider relations between more neighboring labels. Our empirical results show that AIN significantly improves the speed and achieves competitive accuracy against the traditional CRF approach on 4 tasks with 15 datasets.    
"," %with pretrained word embeddings and contextual feature extractors such as RNN or CNN  The linear-chain Conditional Random Field  model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However, these algorithms require sequential computation that makes parallelization impossible. In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model. Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction. The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.",253
" Neural Machine Translation  established on the encoder-decoder framework, where the encoder takes a source sentence as input and encodes it into a fixed-length embedding vector, and the decoder generates the translation sentence according to the encoder embedding, has achieved advanced translation performance in recent years . So far, despite the big advance in model architecture, most models keep taking a standard assumption to translate every sentence independently, ignoring the implicit or explicit sentence correlation from document-level contextual clues during translation.  % 1 涓轰粈涔圖ocument鍦∟MT涓噸瑕侊紝璇存槑浣犵殑璇鹃鏈夋剰涔夈 However, document-level information has shown helpful in improving the translation performance from multiple aspects: consistency, disambiguation, and coherence . If translating every sentence is completely independent of document-level context, it will be difficult to keep every sentence translations across the entire document consistent with each other. Moreover, even sentence independent translation may still benefit from document-level clues through effectively disambiguating words by referring to multiple sentence contexts. At last, document-level clues as a kind of global information across the entire text may effectively help generate more coherent translation results compared to the way only adopting local information inside a sentence alone.  % 2 宸叉湁鏂规硶濡備綍鍒╃敤Document锛屾湁浣曚紭缂虹偣銆傝繖閲屽彲浠ョ畝瑕佷粙缁嶅凡鏈夋柟娉曪紝涓嶇敤鍏紡銆% %% 銆愬彲浠ラ厡鎯呬慨鏀瑰垹鍑忋 There have been few recent attempts to introduce the document-level information into the existing standard NMT models. Various existing methods  focus on modeling the context from the surrounding text in addition to the source sentence.  For the more high-level context,  propose a multi-head hierarchical attention machine translation model to capture the word-level and sentence-level information. The cache-based model raised by  uses the dynamic cache and topic cache to capture the inter-sentence connection.  integrate their proposed Hierarchical Modeling of Global Document Context model  into the original Transformer model to improve the document-level translation.  % 3 閽堝宸叉湁鏂规硶鐨勭己鐐癸紝鏈枃鎻愬嚭涓绉峏XX鏂规硶锛岀壒鐐瑰拰浼樼偣鏄粈涔 % 鍙叧娉ㄥ叏灞鎴栬呭眬閮ㄧ殑淇℃伅锛屾病鏈夌患鍚堣冭檻 However, most of the existing document-level NMT methods focus on introducing the information of disambiguating global document or the surrounding sentences but fail to comprehend the relationship among the current sentence, the global document information, and the local document information, let alone the refined global document-level clues.  In this way, our proposed model can focus on the most relevant part of the concerned translation from which exactly encodes the related document-level context.  The empirical results indicate that our proposed method significantly improves the BLEU score compared with a strong Transformer baseline and performs better than other related models for document-level machine translation on multiple tasks.   
"," Standard neural machine translation  is on the assumption of document-level context independent. Most existing document-level NMT methods are satisfied with a smattering sense of brief document-level information, while this work focuses on exploiting detailed document-level context in terms of multiple forms of document embeddings, which is capable of sufficiently modeling deeper and richer document-level context. The proposed document-aware NMT is implemented to enhance the Transformer baseline by introducing both global and local document-level clues on the source end. Experiments show that the proposed method significantly improves the translation performance over strong baselines and other related studies.",254
"  Historically, metrics for evaluating the quality of machine translation  have relied on assessing the similarity between an MT-generated hypothesis and a human-generated reference translation in the target language.  Traditional metrics have focused on basic, lexical-level features such as counting the number of matching n-grams between the MT hypothesis and the reference translation. Metrics such as {  remain popular as a means of evaluating MT systems due to their light-weight and fast computation.   Modern neural approaches to MT result in much higher quality of translation that often deviates from monotonic lexical transfer between languages.  %A single reference translation might not always be sufficient to accommodate the expressiveness of such translations.  For this reason, it has become increasingly evident that we can no longer rely on metrics such as { and fail to adequately differentiate the highest performing MT systems.  %The findings of the Metrics Shared Task highlight that segment-level evaluation and strong neural MT systems are major challenges, with none of the submitted metrics achieving satisfactory levels of correlation with human judgements .  In this paper, we present {rosslingual  Optimized Metric for Evaluation of Translation.}, a PyTorch-based framework for training highly multilingual and adaptable MT evaluation models that can function as metrics. Our framework takes advantage of recent breakthroughs in cross-lingual language modeling  to generate prediction estimates of human judgments such as Direct Assessments  , Human-mediated Translation Edit Rate   and metrics compliant with the Multidimensional Quality Metric framework .   Inspired by recent work on Quality Estimation  that demonstrated that it is possible to achieve high levels of correlation with human judgements even without a reference translation  , we propose a novel approach for incorporating the source-language input into our MT evaluation models. Traditionally only QE models have made use of the source input, whereas MT evaluation metrics rely instead on the reference translation. As in , we show that using a multilingual embedding space allows us to leverage information from all three inputs and demonstrate the value added by the source as input to our MT evaluation models.  To illustrate the effectiveness and flexibility of the { framework and the trained MT evaluation models described in this paper to the research community upon publication.  
"," We present {, Human-mediated Translation Edit Rate and Multidimensional Quality Metrics. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems. %Furthermore, they show promising results towards solving the current challenges of accurate segment-level evaluation and robustness to top performing systems.",255
"   Aspect detection, which is a vital component of aspect-based sentiment analysis , aims at identifying predefined aspect categories  discussed in segments  of online reviews. Table shows an example review about a television from several different aspects, such as Image, Sound, and Ease of Use. With a large number of reviews, automatic aspect detection allows people to efficiently retrieve review segments of aspects they are interested in. It also benefits many downstream tasks, such as review summarization  and recommendation justification .  [!t]     {!}{     {m{19em}c}     \toprule     Sentence & Aspect \\  can leverage annotated labels of aspect categories but suffer from domain adaptation problems . Another research direction consists of unsupervised approaches and has gained a lot of attention in recent years. Early unsupervised systems are dominated by Latent Dirichlet Allocation  based topic models . However, several recent studies have revealed that LDA-based approaches do not perform well for aspect detection and the extracted aspects are of poor quality  . Compared to LDA-based approaches,  deep learning models, such as aspect-based autoencoder  , have shown excellent performance in extracting coherent aspects and identifying aspect categories for review segments. However, these models require some human effort to manually map model discovered aspects to aspects of interest, which may lead to inaccuracies in mapping especially when model discovered aspects are noisy. Another research direction is based on weakly supervised approaches that leverage a small number of aspect representative words  for the fine-grained aspect detection . Although these models outperform unsupervised approaches, they do make use of human annotated data to extract high-quality aspect seed words, which may limit their application. In addition, they are not able to automatically discover new aspects from review corpus.  We focus on the problem of unsupervised aspect detection  since massive amount of reviews are generated every day and many of them are for newer products. It is difficult for humans to efficiently capture new aspects and manually annotate segments for them at scale. Motivated by ABAE, we learn interpretable aspects by mapping aspect embeddings into word embedding space, so that aspects can be interpreted by the nearest words. To learn better representations for both aspects and review segments, we formulate UAD as a self-supervised  representation learning problem and solve it using a contrastive learning algorithm, which is inspired by the  success of self-supervised contrastive learning in visual representations . In addition to the learning algorithm, we also resolve two problems that deteriorate the performance of ABAE, including its self-attention mechanism for segment representations and aspect mapping strategy . Finally, we discover that the quality of aspect detection can be further improved by knowledge distillation . The contributions of this paper are summarized as follows: [leftmargin=*,topsep=0pt,itemsep=1pt,partopsep=1pt, parsep=1pt]           % [!t] %     {!}{ %     {m{20em}c} %     \toprule %     Sentence & Aspect \\\relax %File: formatting-instructions-latex-2021.tex %release 2021.1 \documentclass[letterpaper]{article} % DO NOT CHANGE THIS \usepackage{aaai21}  % DO NOT CHANGE THIS \usepackage{times}  % DO NOT CHANGE THIS \usepackage{helvet} % DO NOT CHANGE THIS \usepackage{courier}  % DO NOT CHANGE THIS \usepackage[hyphens]{url}  % DO NOT CHANGE THIS \usepackage{graphicx} % DO NOT CHANGE THIS \urlstyle{rm} % DO NOT CHANGE THIS \def\UrlFont{\rm}  % DO NOT CHANGE THIS \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT \frenchspacing  % DO NOT CHANGE THIS {8.5in}  % DO NOT CHANGE THIS {11in}  % DO NOT CHANGE THIS  %new added start \usepackage{booktabs} \usepackage{footnote}  \usepackage{amsmath,amssymb,mathrsfs} \usepackage[ruled,linesnumbered]{algorithm2e}  \usepackage{epstopdf} \usepackage{multirow} \usepackage[skip=0pt]{subcaption} \usepackage{soul}  \usepackage{tabularx} \renewcommand\tabularxcolumn[1]{m{#1}}  \usepackage{enumitem}  \renewcommand\vec[1]{\overrightarrow{#1}}   \usepackage{microtype} \usepackage[switch]{lineno}  %new added end    % %Leave this % /Title  % Put your actual complete title  within the parentheses in mixed case % Leave the space between \Title and the beginning parenthesis alone % /Author  % Put your actual complete list of authors  within the parentheses in mixed case. % Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands, % remove them.  % DISALLOWED PACKAGES % \usepackage{authblk} -- This package is specifically forbidden % \usepackage{balance} -- This package is specifically forbidden % \usepackage{color  % \usepackage{CJK} -- This package is specifically forbidden % \usepackage{float} -- This package is specifically forbidden % \usepackage{flushend} -- This package is specifically forbidden % \usepackage{fontenc} -- This package is specifically forbidden % \usepackage{fullpage} -- This package is specifically forbidden % \usepackage{geometry} -- This package is specifically forbidden % \usepackage{grffile} -- This package is specifically forbidden % \usepackage{hyperref} -- This package is specifically forbidden % \usepackage{navigator} -- This package is specifically forbidden %  %  -- This package is specifically forbidden % \usepackage{setspace} -- This package is specifically forbidden % \usepackage{stfloats} -- This package is specifically forbidden % \usepackage{tabu} -- This package is specifically forbidden % \usepackage{titlesec} -- This package is specifically forbidden % \usepackage{tocbibind} -- This package is specifically forbidden % \usepackage{ulem} -- This package is specifically forbidden % \usepackage{wrapfig} -- This package is specifically forbidden % DISALLOWED COMMANDS %  \author{     %Authors     % All authors must be in the same font size and format.     Tian Shi\textsuperscript{\rm 1}, Liuqing Li\textsuperscript{\rm 2}, Ping Wang\textsuperscript{\rm 1}, Chandan K. Reddy\textsuperscript{\rm 1}\\ } \affiliations{     %Afiliations     \textsuperscript{\rm 1}Department of Computer Science, Virginia Tech\\     \textsuperscript{\rm 2}Verizon Media\\     tshi@vt.edu, liuqing.li@verizonmedia.com, ping@vt.edu,      reddy@cs.vt.edu     % See more examples next }  \author {     % Author     Author Name \\ }  \affiliations{     Affiliation \\     Affiliation Line 2 \\     name@example.com } \fi   \author {     % Authors          First Author Name,\textsuperscript{\rm 1}         Second Author Name, \textsuperscript{\rm 2}         Third Author Name \textsuperscript{\rm 1} \\ } \affiliations {     % Affiliations     \textsuperscript{\rm 1} Affiliation 1 \\     \textsuperscript{\rm 2} Affiliation 2 \\     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com } \fi      Unsupervised aspect detection  aims at automatically extracting interpretable aspects and identifying aspect-specific segments  from online reviews. However, recent deep learning based topic models, specifically aspect-based autoencoder, suffer from several problems such as extracting noisy aspects and poorly mapping aspects discovered by models to the aspects of interest. To tackle these challenges, in this paper, we first propose a self-supervised contrastive learning framework and an attention-based model equipped with a novel smooth self-attention  module for the UAD task in order to learn better representations for aspects and review segments. Secondly, we introduce a high-resolution selective mapping  method to efficiently assign aspects discovered by the model to the aspects of interest. We also propose using a knowledge distillation technique to further improve the aspect detection performance. Our methods outperform several recent unsupervised and weakly supervised approaches on publicly available benchmark user review datasets. Aspect interpretation results show that extracted aspects are meaningful, have a good coverage, and can be easily mapped to aspects of interest. Ablation studies and attention weight visualization also demonstrate effectiveness of SSA and the knowledge distillation method.         
"," Unsupervised aspect detection  aims at automatically extracting interpretable aspects and identifying aspect-specific segments  from online reviews. However, recent deep learning based topic models, specifically aspect-based autoencoder, suffer from several problems such as extracting noisy aspects and poorly mapping aspects discovered by models to the aspects of interest. To tackle these challenges, in this paper, we first propose a self-supervised contrastive learning framework and an attention-based model equipped with a novel smooth self-attention  module for the UAD task in order to learn better representations for aspects and review segments. Secondly, we introduce a high-resolution selective mapping  method to efficiently assign aspects discovered by the model to the aspects of interest. We also propose using a knowledge distillation technique to further improve the aspect detection performance. Our methods outperform several recent unsupervised and weakly supervised approaches on publicly available benchmark user review datasets. Aspect interpretation results show that extracted aspects are meaningful, have a good coverage, and can be easily mapped to aspects of interest. Ablation studies and attention weight visualization also demonstrate effectiveness of SSA and the knowledge distillation method.",256
"     There are several recent studies that aim to predict the aspect ratings using deep neural network based models with multi-task learning framework . In this setting, rating predictions for different aspects, which are typically highly correlated and can share the same review encoder, are treated as different tasks. However, these models rely on hand-crafted aspect keywords to aid in rating/sentiment predictions . Thus, their results, especially case studies of reviews, are biased towards pre-defined aspect keywords. In addition, these models only focus on improving the prediction accuracy, however, knowledge discovery  from review corpus still relies on unsupervised  and rule-based methods , which limits applications of current MARP models . In the past few years, model uncertainty of deep neural network classifiers has received increasing attention , because it can identify low-confidence regions of input space and give more reliable predictions. Uncertainty models have also been applied to deep neural networks for text classification . However, few existing uncertainty methods have been used to improve the overall prediction accuracy of multi-task learning models when crowd-sourcing annotation is involved in the MARP task. In this paper, we attempt to tackle the above mentioned issues. The primary contributions of this paper are as follows:  [leftmargin=*,topsep=1pt,itemsep=1pt, partopsep=1pt, parsep=1pt]    The rest of this paper is organized as follows: In Section , we introduce related work of MARP task and uncertainty estimation methods. In Section , we present details of our proposed FEDAR model, AKR method and LEAD uncertainty estimation approach. In Section , we introduce different MARP datasets, baseline methods and implementation details, as well as analyze experimental results. Our discussion concludes in Section.%% %% This is file `sample-sigconf.tex', %% generated with the docstrip utility. %% %% The original source files were: %% %% samples.dtx   %%  %% IMPORTANT NOTICE: %%  %% For the copyright see the source file. %%  %% Any modified versions of this file must be renamed %% with new filenames distinct from sample-sigconf.tex. %%  %% For distribution of the original source see the terms %% for copying and modification in the file samples.dtx. %%  %% This generated file may be distributed as long as the %% original source files, as listed above, are part of the %% same distribution.  %% %% The first command in your LaTeX source must be the \documentclass command. \documentclass[sigconf]{acmart}  \usepackage{amsmath,amssymb,multicol,mathrsfs} \usepackage[ruled,linesnumbered]{algorithm2e} \usepackage{graphicx} \usepackage{balance}  \usepackage{epstopdf} \usepackage{multirow} \usepackage{color,soul}  \usepackage{tabularx} \renewcommand\tabularxcolumn[1]{m{#1}}  \usepackage[normalem]{ulem} \usepackage{enumitem}   \usepackage{flushend} \usepackage{tikz} \usepackage{pgf} \usepackage[eulergreek]{sansmath}  \usepackage{graphicx} \usepackage{subcaption}  \renewcommand\vec[1]{\overrightarrow{#1}}   \usepackage{url}  [1]{>{}  %%%% As of March 2017, [siggraph] is no longer used. Please use sigconf  for SIGGRAPH conferences.  %%%% As of May 2020, [sigchi] and [sigchi-a] are no longer used. Please use sigconf  for SIGCHI conferences.  %%%% Proceedings format for SIGPLAN conferences  % \documentclass[sigplan, anonymous, review]{acmart}  %%%% Proceedings format for conferences using one-column small layout % \documentclass[acmsmall,review]{acmart}  %% %% \BibTeX command to typeset BibTeX logo in the docs \AtBeginDocument{%   \providecommand\BibTeX{{%             %% %% Submission ID. %% Use this when submitting an article to a sponsored event. You'll %% receive a unique submission ID from the organizers %% of the event, and this ID should be used as the parameter to this command. %%  %% %% The majority of ACM publications use numbered citations and %% references.  The command  switches to the %% ""author year"" style. %% %% If you are preparing content for an event %% sponsored by ACM SIGGRAPH, you must use the ""author year"" style of %% citations and references. %% Uncommenting %% the next command will enable that style. %%  %% %% end of the preamble, start of the body of the document source.   %% %% The ""title"" command has an optional parameter, %% allowing the author to define a ""short title"" to be used in page headers. \title{Deliberate Self-Attention Network with Uncertainty Estimation for Multi-Aspect Review Rating Prediction}  %% %% The ""author"" command and its associated commands are used to define %% the authors and their affiliations. %% Of note is the shared affiliation of the first two authors, and the %% ""authornote"" and ""authornotemark"" commands %% used to denote shared contribution to the research. \author{Tian Shi} \affiliation{Virginia Tech}   \author{Ping Wang} \affiliation{Virginia Tech}   \author{Chandan K. Reddy} \affiliation{Virginia Tech}   %% %% By default, the full list of authors will be used in the page %% headers. Often, this list is too long, and will overlap %% other information printed in the page headers. This command allows %% the author to define a more concise list %% of authors' names for this purpose. \renewcommand{  %% %% The abstract is a short summary of the work to be presented in the %% article.   In recent years, several online platforms have seen a rapid increase in the number of review systems that request users to provide aspect-level feedback. Multi-Aspect Rating Prediction , where the goal is to predict the ratings from a review at an individual aspect level, has become a challenging and an imminent problem. To tackle this challenge, we propose a deliberate self-attention deep neural network model, named as FEDAR, for the MARP problem, which can achieve competitive performance while also being able to interpret the predictions made. As opposed to the previous studies, which make use of hand-crafted keywords to determine aspects in sentiment predictions, our model does not suffer from human bias issues since aspect keywords are automatically detected through a self-attention mechanism. FEDAR is equipped with a highway word embedding layer to transfer knowledge from pre-trained word embeddings, an RNN encoder layer with output features enriched by pooling and factorization techniques, and a deliberate self-attention layer. In addition, we also propose an Attention-driven Keywords Ranking  method, which can automatically extract aspect-level sentiment-related keywords from the review corpus based on the attention weights. Since crowdsourcing annotation can be an alternate way to recover missing ratings of reviews, we propose a LEcture-AuDience  strategy to estimate model uncertainty in the context of multi-task learning, so that valuable human resources can focus on the most uncertain predictions. Our extensive set of experiments on different DMSC datasets demonstrate the superiority of the proposed FEDAR and LEAD models. Visualization of aspect-level sentiment keywords demonstrate the interpretability of our model and effectiveness of our AKR method.   %% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm. %% Please copy and paste the code instead of the example below. %%  <ccs2012> <concept> <concept_id>10002951.10003317.10003347.10003353</concept_id> <concept_desc>Information systems~Sentiment analysis</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10002951.10003317.10003347.10003356</concept_id> <concept_desc>Information systems~Clustering and classification</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10002951.10003317.10003347.10003352</concept_id> <concept_desc>Information systems~Information extraction</concept_desc> <concept_significance>300</concept_significance> </concept> </ccs2012>       %% %% Keywords. The author should pick words that accurately describe %% the work being presented. Separate the keywords with commas.   %% A ""teaser"" image appears between the author and affiliation %% information and the body of the document, and typically spans the %% page. %  %    %    %   \Description{Enjoying the baseball game from the third-base %   seats. Ichiro Suzuki preparing to bat.} %    %   %% %% This command processes the author and affiliation and title %% information and builds the first part of the formatted document.         %% %% The acknowledgments section is defined using the ""acks"" environment %% . This ensures the proper %% identification of the section in the article metadata, and the %% consistent spelling of the heading. %  % To Robert, for the bagels and explaining CMYK and color spaces. %   \newpage     \endinput %% %% End of file `sample-sigconf.tex'.  
","  In recent years, several online platforms have seen a rapid increase in the number of review systems that request users to provide aspect-level feedback. Multi-Aspect Rating Prediction , where the goal is to predict the ratings from a review at an individual aspect level, has become a challenging and an imminent problem. To tackle this challenge, we propose a deliberate self-attention deep neural network model, named as FEDAR, for the MARP problem, which can achieve competitive performance while also being able to interpret the predictions made. As opposed to the previous studies, which make use of hand-crafted keywords to determine aspects in sentiment predictions, our model does not suffer from human bias issues since aspect keywords are automatically detected through a self-attention mechanism. FEDAR is equipped with a highway word embedding layer to transfer knowledge from pre-trained word embeddings, an RNN encoder layer with output features enriched by pooling and factorization techniques, and a deliberate self-attention layer. In addition, we also propose an Attention-driven Keywords Ranking  method, which can automatically extract aspect-level sentiment-related keywords from the review corpus based on the attention weights. Since crowdsourcing annotation can be an alternate way to recover missing ratings of reviews, we propose a LEcture-AuDience  strategy to estimate model uncertainty in the context of multi-task learning, so that valuable human resources can focus on the most uncertain predictions. Our extensive set of experiments on different DMSC datasets demonstrate the superiority of the proposed FEDAR and LEAD models. Visualization of aspect-level sentiment keywords demonstrate the interpretability of our model and effectiveness of our AKR method.",257
"  Recent advances in deep learning have led to significant improvement of Neural Machine Translation  .  Particularly, the performance on the sentence-level translation of both low- and high- resource language pairs is dramatically improved .  However, when translating text with long-range dependencies, such as in conversations or documents, the original mode of translating one sentence at a time ignores the discourse phenomena , introducing undesirable behaviors such as inconsistent pronouns across different translated sentences.   Document-level NMT, as a more realistic translation task in these scenarios, has been systematically investigated in the machine translation community.  Most literatures focused on looking back a fixed number of previous source or target sentences as the document-level context .  Some latest works innovatively attempted to either get the most out of the entire document context or dynamically select the suitable context .  Because of the scarcity of document training data, the benefit gained from such an approach, as reflected in BLEU, is usually limited. We therefore elect to pay attention to the context in the previous  sentences only where  is a small number and usually does not cover the entire document.    Almost all of the latest studies chose the standard transformer model as their baseline which translates each sentence in the document with the model trained on the sentence-level data.   The cohesion and consistency are in general poor.   A more reasonable baseline is to train the transformer with the context prepended, and this modification could be simply implemented via data preprocessing.   conducted a detailed analysis of RNN-based NMT models on the topic of whether or not to include the extended context.  Consistency and precision is often viewed as a trade-off of each other. We conduct a detailed analysis of the effect of document context on consistency in transformer architecture accepting multi-sentence input.  When it comes to leveraging the contextual information, the common approach is to model the interaction between the sentence and its context with specially designed attention modules .  Such works tend to include more than one encoder or decoder, with a substantial number of parameters and additional computations.  In our work, we reduce the contextual and regular attention modules into one single encoder and decoder.  Our idea is motivated by the one transformer decoder with the two-stream self-attention .  % In particular, we maintain two different sets of hidden states and employ two different masking matrices to capture the long and short term dependencies.   The contributions of this paper are threefold:   i) we extensively research the performance of the standard transformer in the setting of multi-sentence input and output;  ii) we propose a simple but effective modification to adapting the transformer for document NMT with the aim of ameliorating the effect of error accumulation;  iii) our experiments demonstrate that even the simple baseline can achieve comparable results.  
"," Many document-level neural machine translation  systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity.  However, few attention is paid to the baseline model.  In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation.  Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors.  We examine our approach on the two publicly available document-level datasets.  We can achieve a strong result in BLEU and capture discourse phenomena.",258
" Response generation for dialogue systems has stimulated great interests for researchers recently . The core idea of dialogue generation is to formulate the task as a sequence translation problem and translate the query to a response. One common neural model is the sequence-to-sequence  encoder-decoder framework. Many approaches have been proposed to improve the basic S2S model for better human-computer conversation performance.   Despite their popularity, these approaches assume that each training sample, namely, query-response pair, contributes equally to the model and ignore the consideration of different response quality contrastively. Table depicts some example responses for a particular query in a dialogue dataset. Both of the first and the second response are relevant to the query but the first one is obviously better when considering informativeness and interestingness. The third response is acceptable for the conversation but quite universal, meaning that, it can also be used to answer other queries. Thus, its quality is not as good as that of the first two responses. The fourth response is poor since it directly copies part of the query. Although the fourth response is not acceptable, it is still better when compared with the fifth response, which is completely irrelevant to the query.  [!t]     {!}{     {L{2.5cm}|L{5cm}|L{4cm}}     \Xhline{3\arrayrulewidth}        Query & Response & Quality \\ {gbsn}1. 娑撳娲﹂搹鐣屽姧鐢附娼靛鍫濐樋娑撳秳绌, 閸欘垰銇婇梼鍏呯瘍婵傝姤妫ら幆鍛櫓, 鐏忚鲸鏇熸 & \multirow{2}{*}{Relevant and interesting} \\         & Although rains are inconvenient, sunny day is no better than rainy day since I am afraid of the sun. &  \\         & {UTF8}{gbsn}2. 閺堢拋銊ュ笁娑撳娲︽径! & \multirow{2}{*}{Relevant but simple}. \\         & I hate rainy days! & \\         {UTF8}{gbsn}閸欏牆绱戞慨瀣╃瑓闂嗐劋绨 & {UTF8}{gbsn}3. 娴ｇ姷骞囬崷銊╂綒瀹娑滅箷閺勵垰绠嶅鐑囩吹 & \multirow{2}{*}{Acceptable but universal} \\         It started to rain again & Are you in Qingdao or Guangzhou now? &  \\         & {UTF8}{gbsn}4. 閸, 閸欏牅绗呴梿銊ょ啊 & \multirow{2}{*}{Quiet boring} \\         & Yes, it rains again. & \\         & {UTF8}{gbsn}5. 鏉╂瑦澧嶆径褍顒熼崪宀鍎撴竟顐︽珶閸ヮ厼绶㈡潻  & \multirow{2}{*}{Irrelevant} \\         & The university is near martyrs cemetery. & \\ %\piji{DO NOT show this kind of informaitn}     \Xhline{3\arrayrulewidth}     }                  Some initial attempts have been conducted to consider the quality of the training data. Following the idea of instance weighting,  pre-train a calibration network to calculate the response quality score for each training sample  and update the model with the weighted combination of the sample loss. Similarly,  estimate the instance score based on the corpus-level n-gram co-occurrence and the length of the response. Both of them are simple to implement but they still have some limitations:  The calibration network is only trained on relevant responses and irrelevant responses from other queries and therefore cannot capture the fine-grained response quality, as exemplified in Table;  The instance weighting strategy treats all tokens in the response as equal importance to the query by assigning them with the same quality score, which may erroneously encourage the generation of some uninformative words in the relevant responses .  To tackle the issues mentioned above, we introduce the Contrastive Learning paradigm to model the multi-level fine-grained quality of the responses with respect to the query. Specifically, we develop a Rank-aware Calibration  network aiming for modeling the fine-grained quality and characterizing the response properties  that will affect the conversation experience with a multi-scale response quality score. The rank-aware calibrator adopts the strategies of pointwise regression and pairwise ranking for gauging the quality of the query-response pair. Besides, to address the second limitation aforementioned, we design a more exquisite strategy to consider the different importance of tokens instead of simply scaling the training sample loss with the response-level quality score. Concretely, we propose to conditionally sample a response via Monte-Carlo Rollout for each gold standard response token and deem the quality scores of the sampled responses as the importance of the tokens in the sample loss estimation.  It is also observed that some meaningful words such as ``university'' and ``martyrs cemetery'' in the fifth response in Table are very likely to receive low quality scores due to the irrelevance to the query. Thus, we propose Knowledge Inference  component to explicitly encourage the generation of the informative tokens in the gold standard responses. This component firstly associates the query and the decoder hidden representation with the memories of the informative tokens and then incorporates the summarized memories into each decoding step.  In summary, our contributions are as follows:  ank-aware Calibration  network to construct the multi-level contrastive objectives. We further design a strategy to calibrate the model training with token-level quality information.\\ nowledge Inference  component. \\ \indent  We build a dataset with fine-grained response annotations and conduct extensive evaluations. The experimental results validate the effectiveness of the proposed framework.   Code and the labelled dataset will be public to facilitate the research.   
"," Most of the existing works for dialogue generation are data-driven models trained directly on corpora crawled from websites. They mainly focus on improving the model architecture to produce better responses but pay little attention to considering the quality of the training data contrastively. In this paper, we propose a multi-level contrastive learning paradigm to model the fine-grained quality of the responses with respect to the query. A Rank-aware Calibration  network is designed to construct the multi-level contrastive optimization objectives. Since these objectives are calculated based on the sentence level, which may erroneously encourage/suppress the generation of uninformative/informative words. To tackle this incidental issue, on one hand, we design an exquisite token-level strategy for estimating the instance loss more accurately. On the other hand, we build a Knowledge Inference  component to capture the keyword knowledge from the reference during training and exploit such information to encourage the generation of informative words. We evaluate the proposed model on a carefully annotated dialogue dataset and the results suggest that our model can generate more relevant and diverse responses compared to the baseline models.",259
"  Knowledge Distillation  is a popular model acceleration and compression approach . It assumes that a lightweight network  can learn to generalize in the same way as a large network . To this end, a simple method is to train the student network with predicted probabilities of the teacher network as its targets.  In KD, the student network is a ``copycat'' of the teacher network because the knowledge is learned from the teacher prediction. Rather, a more straightforward way is to transfer the knowledge in parameters between two networks, as the parameters are the sources of the predictions. Such an idea has been recently found to be effective in the pre-training  fine-tuning paradigm . For example, the parameters learned on large-scale unlabeled data can be used as a good start to train a complex network on the target task. However, parameter reuse is not applicable to model acceleration and compression because the teacher and student networks might be of different width and depth\footnote{In a multi-layer neural network, the number of neurons in a hidden layer is referred to as network width, and the number of stacked layers is referred to as network depth.}.  In this paper, we propose   to transfer the parameters of the teacher network to the student network. We design a parameter generator to model the transformation from teacher network parameters to student network parameters, even if they have different sized weight matrices. After that, a fine-tuning process is performed to improve the quality of the transferred parameters. See \fig{fig:compare} for a comparison of KD and WD.  We test the WD method in a well-tuned Transformer-based machine translation system. The experiments are run on three machine translation tasks, including WMT16 English-Roman , NIST12 Chinese-English , and WMT14 English-German . With a similar speedup, the student network trained by WD is 0.51};           };            {background}             ;           ;           ;           ;           ;            {background}             ;             };            {background}             ]  {};                       %% predictions           ;           ;           ;           ;           ;            {background}             ;             ;           ;           ;           ;           ;            {background}             ;              = [rectangle,minimum width=0.3cm,inner sep=0pt]         \tikzstyle{teacher prob} = [prob,fill=ugreen!45]         \tikzstyle{student prob} = [prob,fill=orange!45]         \tikzstyle{ground truth prob} = [prob,fill=lyyblue!45]          \tikzstyle{pgnode} = [circle,fill=lyyblue,minimum size=0.2cm,inner sep=0pt]          % Teacher         };         };         $};          {background}           ]  {};                   %% predictions         ;         ;         ;         ;         ;          {background}           ;           ;         ;         ;         ;         ;          {background}           ;           ;         ]pgmid) {};         ]pgmid) {};         ]pgmid) {};          \draw[-latex,lyyblue]  to ;         \draw[-latex,lyyblue]  to ;         \draw[-latex,lyyblue]  to ;         \draw[-latex,lyyblue]  to ;          {background}           ]  {};                   % Connections         \draw[-latex',red]  .. controls + and + .. ;         \draw[-latex',red]  to ;          \draw[-latex',red]  .. controls + and + .. ;         \draw[-latex',red]  .. controls + and + .. ;         \draw[-latex',red]  .. controls + and + .. ;          \draw[-latex',red]  .. controls + and + .. ;         \draw[-latex',red]  .. controls + and + .. ;          \draw[-latex,densely dashed]  to ;         \draw[-latex,densely dashed]  to ;         \draw[-latex,densely dashed]  to ;          \draw[-latex,densely dashed]  to ;         \draw[-latex,densely dashed]  to ;         \draw[-latex,densely dashed]  to ;                 }            
","   Knowledge distillation has been proven to be effective in model acceleration and compression. It allows a small network to learn to generalize in the same way as a large network. Recent successes in pre-training suggest the effectiveness of transferring model parameters. Inspired by this, we investigate methods of model acceleration and compression in another line of research. We propose  to transfer the knowledge in the large network parameters through a parameter generator. Our experiments on WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks show that weight distillation can train a small network that is 1.88$\sim$2.94$\times$ faster than the large network but with competitive performance. With the same sized small network, weight distillation can outperform knowledge distillation by 0.51$\sim$1.82 BLEU points.",260
" % ==============================================================================  The CLEVR dataset  is a modern 3D incarnation of historically significant shapes-based datasets like SHRDLU , used for demonstrating AI efficacy on language understanding . Although originally aimed at the visual question answering  problem , its versatility has seen its use in diverse ML domains, including extensions to physics simulation engines for language augmented hierarchical reinforcement learning  and causal reasoning .      Parallelly, research interest in geometric learning and GNN  based techniques have seen a dramatic surge in recent deep learning zeitgeist. In this focused paper, we present a library that allows easy integration and application of geometric representation learning on CLEVR dataset tasks - enabling the NLP research community to apply GNN based techniques to their research .  The library has three main  components:  1. Parser: allows extraction of graph structured relationships among objects of the environment -- both for textual questions, and semantic image scene graphs, 2. Embedder: allows generation of latent embeddings using any models or desired backend of choice , 3. Visualizer: provides tools for visualizing structural graphs and latent embeddings.   %  %     : allows extraction of graph structured relationships among objects of the environment -- both for textual questions, and semantic image scene graphs %     : allows generation of latent embeddings using any models or desired backend of choice  %     : provides tools for visualizing structural graphs and latent embeddings %   %Thus, with the release of this library, we hope to enable greater adoption of geometric learning in the NLP community, by lowering initial learning curve and/or rapid prototyping and integration of GNNs in NLP research domains like language grounded RL, visual reasoning, language compositionality  etc. .  % ============================================================================== 
"," The CLEVR dataset has been used extensively in language grounded visual reasoning in  Machine Learning  and Natural Language Processing  domains. We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes and relationships extraction, and construction of structural graph representations for dual modalities. Structural order-invariant representations enable geometric learning and can aid in downstream tasks like language grounding to vision, robotics, compositionality, interpretability, and computational grammar construction. We provide three extensible main components -- parser, embedder, and visualizer that can be tailored to suit specific learning setups. We also provide out-of-the-box functionality for seamless integration with popular deep graph neural network  libraries. Additionally, we discuss downstream usage and applications of the library, and how it accelerates research for the NLP research community\footnote{Code is available at - \url{https://github.com/raeidsaqur/clevr-parser}}.",261
"     Aggressive language detection  which aims to automatically detect abusive, offensive language and hate speech in social media texts, as one of the important applications of Natural Language Processing , has recently received increasing research attention. Yet there are still limited efforts paid for ALD task. Current works mostly treat ALD as a regular text classification by neural networks, e.g., Long-short Term Memory  , Convolutional Neural Networks   or Transformer , with sophisticated features, e.g., pre-trained embeddings .     Nevertheless, social media texts often differ substantially from the written texts, that is, social media texts can be much noisy and contain typos , e.g., abbreviations, letter repetition, etc. Such characteristic of unnormalized texts can greatly hinder the detection of aggressive contents. Taking the examples sentence  in Fig. , the raw unnormalized expressions that carry crucial signals for indicating offensive languages, can be difficult for a detector to give correct prediction when merely seeing the surface forms. However, if these unnormalized contents are transformed into the normalized standard texts, the inferences of the detector can be much easier.          Based on the above observation, in this paper, we propose to improve the ALD task by simultaneously handling the text normalization . A multi-task learning  framework is adopted for the joint training of these two tasks. As depicted in Fig. , first, the shared encoder is expected to learn the underlying common features over two tasks, while the private encoders for ALD and TN learn the task-relevant features, respectively, based on which the decoders can make their own task predictions. To further enhance the capabilities of the shared and private feature representations, respectively, we suggest the adversarial training architecture . Technically, a task discriminator is used for distinguishing the separate learning of ALD and TN tasks.     We conduct experiments on four widely used ALD datasets, including TRAC , HSOL , KTC  and OLI , based the annotated text normalization data, Lexnorm15 .  Results show that the aggressive language detection can benefit much from the joint learning with text normalization. Our model outperforms baseline methods by a large margin, with 64.0\% and 53.6\% F1 score in TRAC-FB and TRAC-TW test sets, respectively, and average 90.5\% F1 score for other three datasets. In-depth analysis is performed for further understanding of how the TN influences the ALD task, as well as the mechanism of our proposed adversarial multi-task learning framework.           
"," Aggressive language detection , detecting the abusive and offensive language in texts, is one of the crucial applications in NLP community. Most existing works treat ALD as regular classification with neural models, while ignoring the inherent conflicts of social media text that they are quite unnormalized and irregular. In this work, we target improving the ALD by jointly performing text normalization , via an adversarial multi-task learning framework. The private encoders for ALD and TN focus on the task-specific features retrieving, respectively, and the shared encoder learns the underlying common features over two tasks. During adversarial training, a task discriminator distinguishes the separate learning of ALD or TN. Experimental results on four ALD datasets show that our model outperforms all baselines under differing settings by large margins, demonstrating the necessity of joint learning the TN with ALD. Further analysis is conducted for a better understanding of our method.",262
"  Deep neural networks  have been proved vulnerable to adversarial attacks, which maliciously craft adversarial examples to fool the victim model . For instance, highly poisonous phrases with minor modification can easily deceive Google's toxic comment detection system . With the broad use of DNN-based natural language processing  systems, such as spam filtering  and malware detection , there is growing concern about their security. As a result, research into textual adversarial attacking becomes increasingly important.  %by perturbing the original input In recent years plenty of adversarial attack models have been proposed .  Nevertheless, few of them work satisfactorily in real-world attack situations. Existing adversarial attack models can be roughly classified into four categories according to the accessibility to the victim model: gradient-based, score-based, decision-based and blind models. First, gradient-based models, also known as white-box models, require full knowledge of the victim model to perform gradient computation . % attack models work in the white-box setting only , where full knowledge of the victim model is required for gradient computation. Unfortunately, we hardly know the architecture of the victim model in real-world attack situations, let alone compute the gradients.  Second, blind models do not need to know anything about the victim model, but their attack performance is usually not good enough, precisely because of complete ignorance about the victim model.  Specifically, existing blind models either implement character-level random perturbations  or conduct sentence-level distracting  and paraphrasing . However, character-level attacks are easy to repulse , and sentence-level attacks cannot guarantee attack validity, i.e, keeping the ground-truth label of the adversarial example the same as original input. More importantly, the attack success rates of most blind models are unsatisfactory. % and adversarial example quality, including grammaticality and language naturality. %  % inclined to craft invalid adversarial examples, which have different ground-truth labels from original input, or   Finally, score- and decision-based attack models seem to be more suitable for real-world adversarial attack situations. They only need to know the output of the victim models -- the former requires prediction scores and the latter just needs the final prediction decision. % which is normally practicable in real-world adversarial attacking situations % Attack models between the above two kinds of models seem to more suitable for the real-world situation of adversarial attacking, where we are usually able to invoke the victim model and obtain its output.  Existing score- and decision-based attack models have achieved great attack performance , but they have a significant problem. To craft an adversarial example, these models have to iteratively make perturbations and query the victim model too many times, e.g., a very recent score-based model needs to query the victim model more than  times on average to generate an adversarial example . % They utilize the victim model output as guidance and iteratively conduct perturbations until finding an adversarial example . % PWWS濞屸剝婀乮teratively % Although achieving good attacking performance, these models usually need to invoke the victim model too many times, e.g., the attack model in  needs to invoke the victim model more than  times on average when attack one instance. It is neither efficient nor practical to invoke the victim model so many times in real-world situations of adversarial attacking.  We argue that the low efficiency of existing score- and decision-based attack models results from that they have no learning ability and simply follow certain fixed optimization rules to attack, e.g., greedy algorithm , genetic algorithm  and particle swarm optimization .  % these model -> these score- and decision-based models? % For each instance, they start to attack from scratch. % And no lessons are learned from previous attacks. %For example,  ?  To solve this problem, we propose to build an attack model possessing learning ability, which can learn lessons from attack history and store them in its parameters so as to improve attack efficiency. % learn the weak sides of data and the victim model% data? % from history so as to launch deadly attacks efficiently. Considering no labeled data are available in adversarial attacking, we design our model following the reinforcement learning paradigm. There are two main operations in our model, including identifying key words in the original sentences that crucially influence the decision of the victim model, and selecting appropriate substitutes to replace them. Our model is aimed at learning an optimal policy under which a series of substitution operations are iteratively conducted to generate adversarial examples.  % The prober is aimed at locating where is the most vulnerable in a sentence, i.e., which word in a sentence is easiest to attack. % The attacker is supposed to find the most fatal attack, i.e., which word should replace the most vulnerable word in the original input. %Our attack model is highly adaptable and can be combined with different word substitution methods.   In experiments, we evaluate our attack model on the benchmark datasets of three typical NLP tasks including sentiment analysis, text classification and natural language inference. The victim models are respective  state-of-the-art models of the datasets, namely ALBERT , XLNet  and RoBERTa , and two open APIs. Since our model can work in both score- and decision-based attack settings, we carry out experiments in the two settings. Experimental results show that our attack model consistently outperforms the baseline methods on all the datasets in terms of both attack success rate and attack efficiency. % within whatever the limit of the number of victim model query times.  We also find our model can bring more robustness improvement to the victim model by adversarial training. % conduct quantitative analyses to exhibit the learning ability of our model.  % 閸滃矁鐦濋弴鎸庡床閺傝纭堕惃鍕波閸氬牊褝绱 % score閸滃畳ecision based閻ㄥ嫭甯归崙鐚寸吹 % 娣囶喗鏁奸悳鍥ㄦЦ閸氾箒顩︽担婊璐熸稉娑擃亪鍣哥憰浣瑰瘹閺嶅浄绱垫稉宥勭稊娑撴椽鍣哥憰浣瑰瘹閺嶅洤鎯傞敍灞惧壈娑斿绗夐弰顖滃閸掝偅妲戠涵&閹存垳婊戦惃鍕侀崹瀣躬鐠囥儲瀵氶弽鍥︾瑐濞屸剝婀侀弰搴㈡▔娴兼ê濞嶉妴  
"," Adversarial attacking aims to fool deep neural networks with adversarial examples. In the field of natural language processing, various textual adversarial attack models have been proposed, varying in the accessibility to the victim model. Among them, the attack models that only require the output of the victim model are more fit for real-world situations of adversarial attacking. However, to achieve high attack performance, these models usually need to query the victim model too many times, which is neither efficient nor viable in practice. To tackle this problem, we propose a reinforcement learning based attack model, which can learn from attack history and launch attacks more efficiently. In experiments, we evaluate our model by attacking several state-of-the-art models on the benchmark datasets of multiple tasks including sentiment analysis, text classification and natural language inference. Experimental results demonstrate that our model consistently achieves both better attack performance and higher efficiency than recently proposed baseline methods. We also find our attack model can bring more robustness improvement to the victim model by adversarial training. All the code and data of this paper will be made public.",263
"  In natural languages, lexical items can often be used in multiple word classes without overt changes in word form. For instance, the word buru in Mundari can be used as a noun to denote `mountain', or as a verb to denote `to heap up' . Known as word class flexibility, this phenomenon is considered one of the most challenging topics in linguistic typology . We present a computational methodology to quantify the regularity in word class flexibility across languages.   There is an extensive literature on how languages vary in word class flexibility, either directly  or through related notions such as word class conversion  . However, existing studies tend to rely on analyses of small sets of lexical items that may not be representative of word class flexibility in the broad lexicon. Critically lacking are systematic analyses of word class flexibility across many languages, and existing typological studies have only focused on qualitative comparisons of word class systems.   We take to our knowledge the first step towards computational quantification of word class flexibility in \NumLanguages languages, taken from the Universal Dependencies project . We focus on lexical items that can be used both as nouns and as verbs, i.e., noun-verb flexibility. This choice is motivated by the fact that the distinction between nouns and verbs is the most stable in word class systems across languages: if a language makes any distinction between word classes at all, it will likely be a distinction between nouns and verbs . However, our understanding of cross-linguistic regularity in noun-verb flexibility is impoverished.  We operationalize word class flexibility as a property of lemmas. We define a lemma as flexible if some of its occurrences are tagged as nouns and others as verbs. Flexible lemmas are sorted into noun dominant lemmas, which occur more frequently as nouns, and verb dominant lemmas that occur more frequently as verbs. Our methodology builds on contextualized word embedding models  to quantify semantic shift between grammatical classes of a lemma, within a single language. This methodology can also help quantify metrics of flexibility in the lexicon across  languages.  We use our methodology to address one of the most fundamental questions in the study of word class flexibility: should this phenomenon be analyzed as a directional word-formation process similar to derivation, or as a form of underspecification? Derived words are commonly argued to have a lower frequency of use and a narrower range in meaning compared to their base . If word class flexibility is a directional process, we should expect that flexible lemmas are subject to more semantic variation in their dominant word class than in their less frequent class. We also test the claim that noun-to-verb flexibility  involves  more  semantic shift  than  verb-to-noun flexibility.  While previous work has explored these questions, it remains challenging to quantify semantic shift and semantic variation, particularly across different languages.  We present a novel probing task that reveals the ability of deep contextualized models to capture semantic information across word classes. Our utilization of deep contextual models predicts human judgment on the spectrum of noun-verb flexible usages including homonymy , polysemy , and word class flexibility. We find that BERT outperforms ELMo and non-contextual word embeddings, and that the upper layers of BERT capture the most semantic information, which resonates with existing probing studies .   
"," Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes , and we apply this method to \NumLanguages languages\footnote{Code and data to reproduce the experimental findings are available at: \url{https://github.com/SPOClab-ca/word-class-flexibility}.}. We find that contextualized embeddings not only capture human judgment of  class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology.",264
"  Coreference resolution is the task of identifying mentions in a document that co-refer to the same entity. It is an important task facilitating many applications such as question answering and text summarization.   proposed the first neural end-to-end architecture for coreference resolution. Most recent state-of-the-art systems use it as a backbone while utilizing better scoring functions, pruning procedures, or pre-trained token representations. Despite this usage, to our knowledge, no in-depth analysis has been done to better understand the inner workings of such an influential system. This understanding is important: for example, 's analysis of the then-best classical coreference systems inspired many important follow-up works . However, it is unknown if observations on such classical feature-based and often highly pipelined systems extend to the current end-to-end models.  In this paper, we empirically analyze the best instantiation of this model family, SpanBERT + c2f-coref, by investigating the interaction between its two components: the mention detector and mention linker. Specifically, we study how the errors in each independently or jointly affect the final clustering.  Using the CoNLL-2012 and PreCo datasets, we highlight the low-precision, high-recall nature of the detector. While traditionally only recall is emphasized for the detector as a design decision , we show huge degradation from noisy mentions and that, perhaps surprisingly, increasing the number of candidates considered by the baseline linker only deteriorates the performance. While some classical coreference pipelines focused on detector precision, it is rarely emphasized for modern end-to-end systems. We hence stress the importance of a precision-recall balance for the detector and demonstrate how pruning hyperparameters, in addition to reducing computational complexity, help control this trade-off. However, we show the difficulty of obtaining a high-precision detector by demonstrating the importance of anaphoricity decisions and the inability of the detector to make such decisions. Finally, we highlight the high potential of the linker and that the remaining errors besides anaphoricity decisions mainly involve pronoun resolution. We hope these findings shed light on the internal mechanism of the mainstream coreference system and lay out an empirical foundation for future research.  
","  Coreference resolution is an important task for discourse-level natural language understanding. However, despite significant recent progress, the quality of current state-of-the-art systems still considerably trails behind human-level performance. Using the CoNLL-2012 and PreCo datasets, we dissect the best instantiation of the mainstream end-to-end coreference resolution model that underlies most current best-performing coreference systems, and empirically analyze the behavior of its two components: the mention detector and mention linker. While the detector traditionally focuses heavily on recall as a design decision, we demonstrate the importance of precision, calling for their balance. However, we point out the difficulty in building a precise detector due to its inability to make important anaphoricity decisions. We also highlight the enormous room for improving the linker and that the rest of its errors mainly involve pronoun resolution. We hope our findings will help future research in building coreference resolution systems.",265
"    Neural machine translation   enables end-to-end training of translation models and is known to give state-of-the-art results for a large variety of language pairs. NMT for high-resource language pairs is straightforward: choose an NMT architecture and implementation, and train a model on all existing data. In contrast, for low-resource language pairs, this does not work well due to the inability of neural networks to generalize from small amounts of data. One reason for this is the strong over-fitting potential of neural models .  There are several solutions that address this issue of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual  transfer learning , pseudo-parallel data generation  , or multi-task learning . On the other hand, model regularization techniques place constraints on the learning of model parameters in order to aid the model to learn robust representations that positively impact model performance. Among existing model regularization methods, dropout  is most commonly used and is known to be effective regardless of the size of data. We thus focus on designing a technique that can complement dropout especially in an extremely low-resource situation.  The most common way to train NMT models is to minimize a softmax cross-entropy loss, i.e., cross-entropy between the softmax distribution and the smoothed label distribution typically represented with a one-hot vector. In other words, the NMT model is trained to produce a softmax distribution that is similar to the label. In high-resource settings, this may never happen due to the diversity of label sequences.  However, in low-resource settings, due to lack of the diversity, there is a high chance of this occurring and over-fitting is said to take place. We consider that a simple manipulation of the softmax distribution may help prevent it.  This paper presents our investigation into   during training NMT models in order to address the over-fitting issue. Softmax tempering is realized by simply dividing the pre-softmax logits with a positive real number greater than 1.0.  This leads to a smoother softmax probability distribution, which is then used to compute the cross-entropy loss. Softmax tempering has been devised and used regularly in knowledge distillation , albeit for different purposes. We regard softmax tempering as a means of deliberately making the softmax distribution noisy during training with the expectation that this will have a positive impact on the final translation quality.    We primarily evaluate the utility of softmax tempering on extremely low-resource settings involving English and 11 languages in the Asian Languages Treebank  . Our experiments reveal that softmax tempering with a reasonably high temperature improves the translation quality. Furthermore, it makes the greedy search performance of the models trained with softmax tempering comparable to or better than the performance of the beam search using the models that are trained without softmax tempering, enabling faster decoding.  We then expand the scope of our study to high-resource settings, taking the WMT 2019 English-to-German translation task, as well as multilingual settings using the ALT data. We also show that softmax tempering improves the performance of NMT models using recurrently stacked layers that heavily share parameters. Furthermore, we clarify the relationship between softmax tempering and dropout, i.e., the most widely used and effective regularization mechanism. Finally, we analyze the impact of softmax tempering  on the softmax distributions and on the gradient flows during training.  
"," Neural machine translation  models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against smoothed gold labels. In low-resource scenarios, NMT models tend to over-fit because the softmax distribution quickly approaches the gold label distribution. To address this issue, we propose to divide the logits by a temperature coefficient, prior to applying softmax, during training. In our experiments on 11 language pairs in the Asian Language Treebank dataset and the WMT 2019 English-to-German translation task, we observed significant improvements in translation quality by up to 3.9 BLEU points. Furthermore, softmax tempering makes the greedy search to be as good as beam search decoding in terms of translation quality, enabling 1.5 to 3.5 times speed-up. We also study the impact of softmax tempering on multilingual NMT and recurrently stacked NMT, both of which aim to reduce the NMT model size by parameter sharing thereby verifying the utility of temperature in developing compact NMT models. Finally, an analysis of softmax entropies and gradients reveal the impact of our method on the internal behavior of NMT models.",266
" Neural text generation is one of the extensively studied tasks of natural language processing , as it forms the basis for dialogue systems, machine translation, and text summarization. However, often monotonous or dull, texts generated from existing methods do not fully reflect the rich diversity and expression in human language. In particular, models tend to overproduce words frequently appearing in the data, while hardly utilizing informative words  . % along with fast-evolving model architectures and pre-training techniques. \dkp{} %  Even pre-training techniques on large corpora fail to resolve the issue.  %\dkpc{Current logic: three causes, and we choose to solve the last  Better logic: one cause - drawback, second cause - drawback, the third cause - directly addressing the model, thus optimal!}  Possible causes for text degeneration have been illuminated, such as a defect specific to model architectures or the discrepancy between training data and a true distribution. Recently, the emphasis has been placed on investigating the flaws in the maximum likelihood objective. Concretely, the likelihood training pays little attention to the top ranks in terms of the target token probabilities, or maximizing likelihood itself does not adequately reflect human language processing. Therefore, with the maximum likelihood-based training, models learn to produce tokens frequently appearing in the data more often.   We argue, however, that the primary reason behind the sub-optimal performance of the likelihood objective is essentially the imbalanced token distribution inherent in natural language. Natural language is extremely skewed in distribution, where the top hundred most frequently-used  words occupy nearly half of the total corpus following the Zipf's law. Training a classifier with the inherently imbalanced data on the maximum likelihood estimation  leads to biased classification boundaries in favor of majority classes. In other words, models play a difficult role in learning with the imbalanced label  distribution. %  %   Although the above might be contributing factors, we found that word distribution itself provides clues to the text degeneration.   %We set our work in line with the last category. However, unlike the previous approaches, we claim that data distribution can provide clues as to why text degenerates.  %Prior studies report a number of main causes that for text degeneration. First, it is attributed to the Attention mechanisms, where \dkp{bulabula}. Another reason lies in the training of the machine over a fixed corpora of which the distribution does not agree with the real-world language distribution. Lastly, maximum-likelihood objective, by which the machine is trained with, has been questioned. Following the maximum-likelihood, little attention is made to the top ranks of the next \dkpc{next of what?} token probabilities. \dkpc{this should be explained crystal clear, since this phenomenon is directly related to our model}. and that it differs from human behavior.  [t]     % [t] %  %  %   We hypothesize that text generation can be enriched by balancing out the training data distribution. To this end, we introduce F-Softmax , Section), which factorizes the probability distribution of the target token into a product of two conditional probabilities of  frequency class, and  token from the target frequency class. It ensures training over balanced data, since the frequency classes are designed to have the distribution close to uniformity, and token distributions within a class are confined to  subsets of vocabularies grouped with similar frequencies. To this end, all unique tokens are assigned to a frequency class prior to the training, by our novel mean efficiency maximization , Section). MefMax evaluates and maximizes the class-labeling performance with the normalized entropy , having the probability distributions to be learned as uniform as possible.  % Athe probability distributions to be learned by the model are as uniform as possible, without introducing any hyperparameter. %            % We propose a factorized softmax that achieves this by introducing the concept of classes and decomposing the output probabilities using the classes. It computes the probability distributions of tokens in a factorized manner; probability distribution of the class and conditional probability distribution of the next token given the class. The probability of the next token is computed within a subset of the vocabulary, rather than full vocabulary. Well structured subsets of vocabulary allows model to benefit from the balanced output distributions. We assign each token to a unique class utilizing our proposed mean efficiency maximization algorithm so that each data distribution to be trained has a distribution that is as uniform as possible.  We conduct extensive performance evaluations on seven relevant metrics that quantify the diversity and quality of generated texts. In terms of the diversity of generated texts, our approach significantly outperforms not only the MLE baseline but also other diversity-promoting alternatives . We also achieve state-of-the-art results on most of the quality performances.   
"," %Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is largely attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose F$^2$-Softmax to enable a balanced training over the tokens with skewed frequency distribution. \dkp{By decomposing the softmax function, F$^2$-Softmax confines probability distribution to subsets of vocabularies which are more uniformly distributed. The subsets are further optimized by our novel mean efficiency maximization , without introducing any hyperparameter.} Significant performance gains across generation quality metrics suggest that our methods achieve human-like diversity in text generation.  Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, F$^2$-Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. F$^2$-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of  frequency class, and  token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies.  Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.  % Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose F$^2$-Softmax for a balanced training even with the skewed frequency distribution. F$^2$-Softmax decomposes a probability distribution of the target token into a product of two conditional probabilities of  frequency class  token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. The subsets are further optimized by our novel mean efficiency maximization . It maximizes the . Significant performance gains across generation quality metrics suggest that our method achieves human-like diversity in text generation, without compromising the quality of generated texts. Significant performance gains on seven relevant metrics suggest the supremacy of our approach improves both diversity and quality in text generation.",267
"  Natural language understanding  is a key component of conversational dialogue systems, converting user's utterances into the corresponding semantic representations for certain narrow domain . As a core task in NLU, slot tagging is usually formulated as a sequence labeling problem.  Recently, motivated by commercial applications like Amazon Alexa, Apple Siri, Google Assistant, and Microsoft Cortana, great interest has been attached to rapid domain transfer and adaptation with only a few samples. Few-shot learning approaches become appealing in this scenario , where a general model is learned from existing domains and transferred to new domains rapidly with merely few examples .  The similarity-based few-shot learning methods have been widely analyzed on classification problems, which classify an item according to its similarity with the representation of each class. These methods learn a domain-general encoder to extract feature vectors for items in existing domains, and utilize the same encoder to obtain the representation of each new class from very few labeled samples . This  scenario has been successfully adopted in the slot tagging task by considering both the word-label similarity and temporal dependency of target labels. Nonetheless, it is still a challenge to devise appropriate word-label similarity metrics for generalization capability.  In this work, a vector projection network is proposed for the few-shot slot tagging task in NLU. To eliminate the impact of unrelated label vectors but with large norm, we exploit projections of contextual word embeddings on each normalized label vector as the word-label similarity. Moreover, the half norm of each label vector is utilized as a threshold, which can help reduce false positive errors.   %It first normalizes the vector representation of each label as a unit vector, and then exploits projections of contextual word embeddings on these unit label vectors as the word-label similarities.  One-shot and five-shot experiments on slot tagging and named entity recognition  tasks show that our method can outperform various few-shot learning baselines, enhance existing advanced methods like TapNet and prototypical network, and achieve state-of-the-art performances.  Our contributions are summarized as follows:        
","  Few-shot slot tagging becomes appealing for rapid domain transfer and adaptation, motivated by the tremendous development of conversational dialogue systems. In this paper, we propose a vector projection network for few-shot slot tagging, which exploits projections of contextual word embeddings on each target label vector as word-label similarities. Essentially, this approach is equivalent to a normalized linear model with an adaptive bias. The contrastive experiment demonstrates that our proposed vector projection based similarity metric can significantly surpass other variants. Specifically, in the five-shot setting on benchmarks SNIPS and NER, our method outperforms the strongest few-shot learning baseline by $6.30$ and $13.79$ points on F$_1$ score, respectively. Our code will be released at \url{https://github.com/sz128/few_shot_slot_tagging_and_NER}.",268
" %  %       Over the last decade an increasing number of people access news online, and use social networking platforms to engage, consume and propagate this content in their social circles. Social networks provide easy means to distribute news and commentary, resulting in a sharp increase in the number of media outlets, representing a wide range of perspectives and ideologies. However, despite this diversity, content is often shared only among people that hold similar beliefs and ideologies, resulting in highly segregated information communities, often referred to as ``echo chambers''.  % To date, most works studying this phenomenon have either focused on the linguistic aspects of biased and polarized content, or on the social aspects connecting users to content providers and the way documents spread. Our main observation in this paper is that modeling both these aspects is needed in order to understand and analyze information communities. %focused on analyzing the interactions between news sources and users in social networks % Our goal in this paper is to formalize the connections between the perspectives expressed in the text, the users who share them, their social interactions and the information communities emerging from these connections. We suggest a novel, minimally supervised, approach for embedding information communities, which allows us to map the news media landscape on politically divisive issues, and capture the ideological biases and perspectives expressed in news content. We analyze the differences between communities based on their position on a continuous conservative-liberal ideological spectrum, and observe the differences in perspectives expressed in documents shared in these communities in three issues--  immigration, gun-control and abortion.   % %Identifying the perspective difference and making it explicit can help strengthen trust in the newly-formed information landscape and ensure that all perspectives are represented. It can also help lay the foundation for the automatic detection of false content and rumors and help identify information echo-chambers in which only a single perspective is highlighted. %ribeiro2018media      To help clarify our objectives, consider two articles on the highly polarized immigration issue. \\ %  Different Perspectives on Immigration  %Difference in frame usage by party. Topic: Immigration, Frame used: Economic % colback=blue!5!white,colframe=blue!75!black   [raster columns=1,raster equal height,size=small, colframe,toprule=0pt, bottomrule=0pt, leftrule=0pt, rightrule=0pt, titlerule=0pt, arc=0pt] [colback=blue!15!white,colframe=blue!75!black,nobeforeafter, title={ Adapted from }~~]   }    [colback=red!15!white,colframe=red!75!black,nobeforeafter, title={ Adapted from }~~]   }       The two articles capture opposite political perspectives, liberal  and conservative . They do not directly contradict each other, rather they focus the discussion on different aspects helping them argue their case. The first emphasizing the contribution of immigrants to the community through tax revenue, and the second emphasizing implication on wages for U.S. workers. This process is known as framing.  % Our goal is to both capture the political perspective associated with articles, and explain that perspective by identifying the framing dimensions in the text, which are associated with that perspective and support it. Previous work by~ studied policy issue framing on news media and suggested 15 broad frames to analyze how issues are framed, which include Economic, Morality and Security, among others. These framing dimensions can help capture ideological splits. For example, by framing the immigration issue using the morality frame or using the security frame, the reader is primed to accept the liberal or conservative perspectives, respectively. However, as shown in Example 1, in some cases this analysis is too coarse grained, as both articles frame the issue using the economic frame, suggesting that a finer grained analysis is needed to capture the differences in perspective. To help resolve this issue, we suggest a data-driven refinement to these frames, described in section, by identifying repeating expressions used in the context of the different frames, and grouping them to form sub-frames, which can separate between different usages of the frame to express different political perspectives . %  Our goal is to capture the ideological perspective associated with documents, identify how issues are framed to support these perspectives, and represent the political meaning of these frames on an ideological spectrum.       %Identifying political perspectives is typically framed as a text-categorization The analysis discussed above is typically framed as text classification, e.g., biased language analysis, political ideology identification or framing analysis. Given the highly dynamic nature of political events and the strategies used to discuss them, these methods would require continuous adaptation. %  Instead, we take a different approach driven by the principal of social homophily, referring to the tendency of individuals to form social ties with others who share their views. This phenomenon was previously used to help overcome language variation issues. In our settings, we follow the observation that the political perspectives and attitudes expressed in the text will be reflected in the behavior of users engaging with it. We identify similar patterns and exploit distant supervision to construct information communities consisting of documents and users,  holding similar views and focusing on similar aspects of the issues. Figure describes an example for the immigration issue.   We define the communities over an information graph, modeling the interaction between news document nodes, users who share them on Twitter, and political influencers, such as politicians, followed-by the sharing users. % %Given a graph connecting Twitter users nodes, via activity-links to news article nodes  , and  via social-links  to politically affiliated users ,  Our algorithm groups users and news-article nodes together into communities, associates political meaning with these communities by observing the social links to politicians, and identifies repeating themes and perspectives by observing how the topic is framed in the news articles associated with each community. %TODO: explain - the embedding space, allowing them to share representation, creates a common language for evaluating the relationship between these elements, connecting frames with political labels, documents .  A community defines a probability distribution, over these elements, to belong to the same cluster. The assignments are overlapping.  The community is represented using its centroid, allowing us to create an embedding for the community, that can be evaluated in the embedding space - by observing its similarity with other elements, such as labels, and perspectives, which  explain that community.  %  % To accomplish that, we define a latent space for embedding users, documents, political influencers, frames and ideology-labels. Intuitively, the embedding space is shaped both by the textual content of documents, the engagement patterns of users with these documents and their social ties to politicians.  We take a community embedding approach, and define communities as a multivariate Gaussian distribution over that latent space. We suggest an EM-style learning approach, augmenting the graph embedding objective, based on first-order graph relations, with a global-view derived from the inferred community assignments.  Unlike related work analyzing community behavior and conflicts, we do not analyze observed community structures, but rather our goal is to construct these communities, as a way to characterize how different issues are discussed and align social information with the text. Recent work by ~ exploits social supervision for detecting political bias in documents. We take a broader view in this work, and aim to characterize the discussion, rather than individual articles.   %TODO - what are our experiments designed todo?  One practical thing is to evaluate document classification, and show that adding communities can help.  More broadly -   sanity check for the communities.  communities properties. We conduct extensive experiments to evaluate the inferred community structures, over three politically divisive issues, namely, immigration, abortion and gun-control. We show that our approach can be used to detect the political ideology of documents, even when little social information is available, as well as characterize cohesive information communities, focusing on different aspects of these issues.   %   % TODO : discuss disconnected docs  % todo: discuss ""beyond binary labels""   % todo : discuss the no-supervision settings - similar to topic models etc.    %Eval:  %Extenral Indicators of sanity - % frame/subframe label correlation+visualization %indicator correlation. % classifier results - subframes are good features, comparable to BERT.    %Internal Sanity check - % took top-K articles for each SF, and evaluated  that they correspond to the definition . % TODO: add example for a couple of paragraphs. %  Sample 10 from each side, for each topic = 60 documents. Compare two lists of SF predicted from the text - GLDA vs. embedding    %Applying SF for analyzing data - %event based table 
"," In this paper we suggest a minimally-supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by~,  into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.  % a method for characterizing the perspectives on news media on several politically divisive issues, such as immigration, gun-control and abortion.",269
"  % Neural machine translation  has made great progress in recent years.  Recently, more and more novel network structures of neural machine translation have been proposed , among which Transformer  achieves the best results. One important difference between Transformer and other translation models is its multi-head attention mechanism.   % The original intention of introducing multi-head attention is to capture different aspects of context information and it indeed improves the performance of NMT models. Some interesting phenomena of the attention heads are discovered recently.  find that only a small subset of heads appears to be important for the translation task and vast majority of heads can be removed without seriously affecting performance.  also find that several heads can be removed from trained transformer models without statistically significant degradation in test performance. It turns out that not all heads are equally important.  We speculate that this can be attributed to the imbalanced training of multi-head attention, as some heads are not trained adequately and contribute little to the model. However, this can be turned into the bottleneck for the whole model. For an analogy, if a soccer player gets used to using the right foot and spares more training opportunities for it, it will be stronger and stronger. As a result, the right foot is further relied on, while the left foot receives less training and gradually turns into the limitation.  In this paper, we firstly empirically confirm the inequality in multi-head attention. Then a new training method with two variants  % \SJ{reconsider the choice of words: stragety, ways, methods... You need two. }  is proposed to avoid the bottleneck and improve the translation performance. Further analyses are also made to verify the assumption.  % 
"," Recent studies show that the attention heads in Transformer are not equal~. We relate this phenomenon to the imbalance training of multi-head attention and the model dependence on specific heads. To tackle this problem, we propose a simple masking method: HeadMask, in two specific ways. Experiments show that translation improvements are achieved on multiple language pairs. Subsequent empirical analyses also support our assumption and confirm the effectiveness of the method.",270
"   Two widely-known formalisms are commonly used to represent the syntactic structure of sentences in human languages: constituent and dependency representations.  Constituent trees, which are commonly used in tasks where span information is crucial, describe the syntax of a sentence in terms of constituents  and their hierarchical order. We can find two kinds of constituent trees: continuous and discontinuous  and , respectively). The latter extend the former by allowing  %the representation of  crossing branches and constituents with gaps in the middle. These are necessary for describing some wh-movement, long-distance extractions, dislocations, cross-serial dependencies and other linguistic phenomena common in free word order languages such as German .  On the other hand, a dependency tree straightforwardly connects each word of a sentence as a dependent of another, which is considered its head word. This structure composed of binary syntactic dependencies is known for representing information closer to semantic relations and can be classified as projective or non-projective  and , respectively). %, being the  Non-projective dependency trees  %are a more complex structure that allows to allow crossing dependencies, and can model the same linguistic phenomena described by discontinuous constituent trees.  Since the information described in a  %regular  constituent tree cannot be fully represented in a  %regular  dependency tree and vice versa ,  %typically there are parsers that are typically parsers are exclusively trained to produce either dependency or constituent structures and, in some cases,  %they are  restricted to the less complex continuous/projective representations. %, supporting just one out of the four syntactic structures described before.    generates continuous and projective structures with a single  model, and the sequence labeling parser of  combines continuous constituents with non-projective dependency structures. In both cases, which are discussed in more detail in Section,  representations are shown to benefit each other in terms of accuracy.}     we propose a novel multitask transition-based parser that can efficiently generate unrestricted constituent and dependency structures  from a single trained model. We design an encoder-decoder neural architecture that is jointly trained across the syntactic information represented in the two formalisms by following a multitask learning strategy . Inspired by  , we model constituent trees as augmented dependency structures  and use two separate task-specific decoders to produce both regular and augmented dependency trees. Each decoder relies on Pointer Networks  and a biaffine classifier  to incrementally generate labelled dependencies from left to right, as proposed by . Finally, the decoding runtime $) and the required memory space of our multi-representational approach remains the same as the single-task dependency parser by , since a single model is trained and the multitask learning strategy has no impact on decoding time, allowing both decoders to be run in parallel.    We test our multi-representational neural model\footnote{Source code available at \url{https://github.com/danifg/MultiPointer}.} on the continuous English and Chinese Penn Treebanks  and on the discontinuous NEGRA  and TIGER  datasets. In all benchmarks, our approach outperforms single-task parsers , which proves that learning across regular dependency trees and constituent information  leads to gains in accuracy in both tasks, obtaining competitive results in all cases and surpassing the current state of the art %by a wide margin  in several datasets.    [t]  \& \& {\tiny ADVP} \& {\tiny ADJP} \& \\  \deproot{2}{} \depedge[edge unit distance=4ex]{2}{1}{ROOT+S\#2} \depedge{2}{3}{VP\#1} \depedge[edge unit distance=3ex]{2}{4}{VP\#1} \depedge[edge unit distance=3ex]{2}{5}{ROOT+S\#2}  %  [theme = simple]  \deproot[edge unit distance=2ex]{4}{} \depedge[edge unit distance=5ex]{4}{1}{nsubj} \depedge[edge unit distance=4ex]{4}{2}{cop} \depedge{4}{3}{advmod} \depedge[edge unit distance=4ex]{4}{5}{punct} \\ % {\tiny a) Continuous constituent tree.}   {\tiny b) Projective augmented dependency tree.}  {\tiny c) Projective dependency tree.}\\  %%%%%%%%%%%%%%%%%%%%%%%%%   %   \deproot[edge unit distance=4ex]{2}{} \depedge[edge unit distance=2ex]{4}{1}{NP\#2} \depedge{4}{3}{NP\#1} \depedge[edge unit distance=4.5ex]{2}{4}{S\#1} \depedge[edge unit distance=4ex]{2}{5}{VROOT\#2}  %  [theme = simple]  \deproot[edge unit distance=2.5ex]{2}{} \depedge[edge unit distance=2ex]{4}{1}{APP} \depedge{4}{3}{DET} \depedge[edge unit distance=4.5ex]{2}{4}{SUBJ} \depedge[edge unit distance=4ex]{2}{5}{punct} \\ % {\tiny d) Discontinuous constituent tree.}   {\tiny e) Non-projective augmented dependency tree.}  {\tiny f) Non-projective dependency tree.}      
"," We propose a transition-based approach that, by training a single model, can efficiently parse any input sentence with both constituent and dependency trees, supporting both continuous/projective and discontinuous/non-projective syntactic structures. To that end, we develop a Pointer Network architecture with two separate task-specific decoders and a common encoder, and follow a multitask learning strategy to jointly train them. The resulting quadratic system, not only becomes the first parser that can jointly produce both unrestricted constituent  and dependency   trees from a single model, but also proves that both syntactic formalisms can benefit from each other during training, achieving state-of-the-art accuracies in several widely-used benchmarks such as the continuous English and Chinese Penn Treebanks, as well as the discontinuous German NEGRA and TIGER datasets.",271
"   %GOAL: introducing rule based% %Traditional solutions for task-oriented dialogue systems decompose the task of building a complete task-oriented dialogue system into several sequential steps, including ,  and ~ . In this paper we focus on the dialogue policy that is a key component in dialogue management; it decides what actions the system should take at each time step according to the context and user feedback.  The aim of dialogue policies in  is to select appropriate actions at each time step according to the current context of the conversation and user feedback~. In early work, dialogue policies were manually designed as a set of rules that map the dialogue context to a corresponding system action~. %That is only feasible when domain is not complex. %, an approach that suffers from limited task scalability and from an inability of easily being updated as user behavior changes.  %When the task domain is not complex and the possible conversation scenarios can be predefined explicitly, the dialogue policy can be represented as a set of rules that map the dialogue context to a corresponding system action .  The ability of rule-based solutions is limited by the domain complexity and task scalability. Moreover, the design and maintenance of these rules require a lot of effort and domain knowledge.   %GOAL: introducing supervised learning and its disadvantages% Due to recent advantages in deep learning and the availability of labeled conversational datasets,  can be employed for dialogue policy training to overcome the disadvantages of rule-based systems. %Dialogue context-action pairs are fed to the model to infer the underlying relation between dialogue context and corresponding dialogue actions with supervised learning methods.  \todo{It looks obvious what is the task from he first sentence the paragraph} The downside of the supervised learning approach is that the dialogues observed in the datasets are unlikely to represent all possible conversation scenarios; in some extreme cases, the required conversational dataset cannot be collected or acquiring it might cost-prohibitive.   %GOAL: introducing RL and its disadvantages% The success of  in other areas holds promises for dialogue ~. Using  techniques, we can train dialogue policies and optimize automatically, from scratch and utilizing interactions with users~.  % Handcrafting complex rules is not essential anymore and the expense and pressure of maintaining the policy over time can be alleviated.  In -based solutions, the dialogue system takes actions that are controlled by the dialogue policy, and user feedback , which is provided when the dialogue is finished, is utilized to adjust the initial policy~.  %These methods assume that the system has access to a  at the end of each dialogue. In practice, reward signals are not always available and may be inconsistent~.  As it is not practical to ask for explicit user feedback for each dialogue during policy training, different strategies have been proposed to design a rule-based user simulator along with a reward function that can approximate the real  which exists only in each user's mind.  Designing an appropriate user simulator and accurate reward function requires strong domain knowledge.  This process has the same disadvantages as rule-based dialog systems~.  The difference is that rule-based approaches to system design meet this problem at the dialogue agent side while rule-based user simulators need to solve it at the environment side.    %To train a dialogue agent with reinforcement learning, we have to handcraft a rule-based user simulator and it will suffer the same problem with rule-based dialogue agent when the task in becoming complex.  %The only difference is that one approach meets this problem at the dialogue agent side  while another one has to solve it at the environment side .  %%GOAL: Describing the bottleneck% If the task is simple and easy to solve, why not just build a rule-based system rather than a user-simulator that is then used with  techniques to train the dialogue system, where more uncontrollable factors are involved?  And if the task domain is complex and hard to solve, is it easier to design and maintain a complicated rule-based user simulator than to build a rule-based dialogue agent? % Training a model-based user simulator~ with real human dialogue dataset is an alternative solution but it is still data-hungry.  %Besides, there is no guarantee that human-designed and model-based simulator can cover all possible dialogue scenarios.  % With respect to the comparison between reinforcement learning and supervised learning, s Supervised learning methods do not suffer from these issues but require labeled conversational data; in some exceptional cases, if the data cannot be collected for privacy reasons,  is the solution. However, collecting labeled data is feasible for many applications~. % Therefore in this work seek to answer the following research question:  focusing purely on advancing -based methods?}   To address this question, we introduce three dialogue  methods which do not require a user simulator. The proposed methods can achieve comparable or even higher performance compared to   methods.  The first method utilizes an action decoder to predict dialogue combinations.  %The sequential decision setup can make use of dependency information between different atomic dialogue actions in the same response.  The second method regards the dialogue  task as a multi-label classification problem.  Unlike previous work, we assign a dense layer to each action label in the action space. % This change provides the dialogue agent with more stable and higher performance.  Based on the second method, we propose an adversarial learning method for dialogue  without utilizing .  To backpropagate the loss from the reward model to the policy model, we utilize the Gumbel-Softmax to connect the policy model and the reward model in our third method.  % We compare our methods with  and adversarial  based dialogue training solutions to show how we can achieve comparable performance without a utilizing costly user simulator.  To summarize, our contributions are: [nosep,leftmargin=*]  performance in dialogue  with fewer efforts and costs compare to existing -based solutions.    
"," %\todo[maybe a more interesting title? like ``rethinking supervised learning and reinforcement learning in dialogue policy learning""] Dialogue policy learning for  has enjoyed great progress recently mostly through employing  methods. However, these approaches have become very sophisticated. It is time to re-evaluate it. Are we really making progress developing dialogue agents only based on ? We demonstrate how ~traditional supervised learning together with ~a simulator-free adversarial learning method can be used to achieve performance comparable to  -based methods.  First, we introduce a simple dialogue action decoder to predict the appropriate actions. Then, the traditional multi-label classification solution for dialogue policy learning is extended by adding dense layers to improve the dialogue agent performance. Finally, we employ the Gumbel-Softmax estimator to alternatively train the dialogue agent and the dialogue reward model without using .  Based on our extensive experimentation, we can conclude the proposed methods can achieve more stable and higher performance with fewer efforts, such as the domain knowledge required to design a user simulator and the intractable parameter tuning in reinforcement learning. Our main goal is not to beat  with supervised learning, but to demonstrate the value of rethinking the role of  and supervised learning in optimizing .",272
"   % What is neural keyphrase generation in general Keyphrases are phrases that summarize and highlight important information in a piece of text. Keyphrase generation  is the task of automatically predicting such keyphrases given the source text. The task can be  easily misunderstood and trivialized as yet another natural language generation task like summarization and translation, failing to recognize one key aspect that distinguishes KPG: the multiplicity of generation targets; for each input sequence, a KPG system is expected to output  keyphrases, each a mini-sequence of multiple word tokens.    % Typically, one source text is associated with multiple keyphrases, % which may either be present in  or absent from the source text. % This property of the task, along with the others, pushes the community to investigate leveraging deep neural networks to handle this task.  % There are quite a few work on KPGen task, people mainly use two popular frameworks: one2one and one2seq % However, in previous literature, the comparison between the two frameworks, the effects of architectural and hyper-parameter choice remain unclear.  % Keyphrase generation is essentially a natural language generation  task. Despite this unique nature, KPG has been essentially ``brute-forced'' into the sequence-to-sequence   framework in the existing literature .%,sun2019divgraphpointer,ye2018kp_semi}.  % Seq2Seq models are encoder-decoder neural networks, where an encoder reads through the source text to form a hidden representation, and a decoder then generates a target sequence  word by word conditioned on the source text representation passed by the encoder. The community has approached the unique challenges with much ingenuity in problem formulation, model design, and evaluation. For example, multiple target phrases have been reformulated by either splitting into one phrase per data point or joining into a single sequence with delimiters , both allowing straightforward applications of existing neural techniques such as Seq2Seq. In accordance with the tremendous success and demonstrated effectiveness of neural approaches, steady progress has been made in the past few years --- at least empirically --- across various domains, including sub-areas where it was previously shown to be rather difficult .  Meanwhile, with the myriad of KPG's unique challenges comes an ever-growing collection of studies that, albeit novel and practical, may quickly proliferate and overwhelm. We are therefore motivated to present this study as --- to the best of our knowledge --- the first systematic investigation on such challenges as well as the effect of interplay among their solutions. We hope this study can serve as a practical guide to help researchers to gain a more holistic view on the task, and to profit from the empirical results of our investigations on a variety of topics in KPG including model design, evaluation, and hyper-parameter selection. %data processing,   % Based on their training paradigms, most keyphrase generation models introduced in prior works fall into two categories, namely \onetoone and \onetoseq . % Models have achieved improved performance on texts of various types, including scientific publications , news articles , and forum postings .  % However, we are unaware of any existing systematic and comprehensive empirical analysis on neural keyphrase generation, particularly on examining effects of the fundamental factors shared in various model designs.   [t!]      comparison between \onetoone  and \onetoseq  paradigms on the same data point. Bottom: demonstration of the decoding process for \onetoone  and \onetoseq  models. \onetoseq can apply both beam search  and greedy decoding .}     %Note when greedy decoding is used, beam width is set to 1 .}     %         % In this empirical study, we do exhaustive experiments and provide comprehensive analysis. % To facilitate future research in the community and clarifying,  % In this work, we present a comprehensive empirical study of neural keyphrase generation with extensive experiments, aiming to characterize key factors in keyphrase generation models, quantitatively analyze their impacts on model performance, and compare a wide range of baseline variants.  % We hope this study serves as a practical guide to help researchers on architecture, methods, and hyper-parameter selection. % We also hope to provide new insights to the community. % Based on extensive experiments, we provide comprehensive analyses on a number of factors that affect the training and generalization performance of keyphrase generation models. % Thus our contributions are: The rest of the paper is organized as follows. We first enumerate specific challenges in KPG due to the multiplicity of its target, and describe general setups for the experiments. We subsequently present experimental results and discussions to answer three main questions:\\ 1. How well do KPG models generalize to various testing distributions?\\ 2. Does the order of target keyphrases matter while training \onetoseq ?\\ 3. Are larger training data helpful? How to better make use of them?  %  %        
"," Recent years have seen a flourishing of neural keyphrase generation  works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. % \todo{Among the growing number of neural models competing on this track, we observe that most of them fall into two categories --- \onetoone and \onetoseq --- based on their training paradigms.} % However, there lacks a comprehensive comparison among different model designs, and an investigation on related factors  that may affect a keyphrase generation system's performance. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system's generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic.",273
" The de-facto supervised neural network training paradigm requires a large dataset with annotations.  It is time-consuming, difficult and sometimes even infeasible to collect a large number of data-points  due to task nature. A typical example task is medical diagnosis.  In addition, annotating datasets also is costly, especially in domains where experts are difficult to recruit. In a traditional annotation process, the  human-machine communication bandwidth is narrow. Each label provides  bits per sample for a -class classification problem.  However, humans don't solely rely on such low bandwidth communication to learn. They instead learn through natural language communication, which grounds on abstract concepts and knowledge. Psychologists and philosophers have long posited  natural language explanations  as central, organizing elements to human learning and reasoning. Following this intuition, we explore methods to incorporate natural language explanations in learning paradigms to improve learning algorithm's data efficiency.        earning  w\underline{i}th  \underline{C}ontrastive  % Natural Language \underline{E}xplanations }}}{#1}} [1]{{{{ALICE}}}{#1}}    [1]{[{WX: #1}]} % [1]{{}}     % % File emnlp2020.tex % %% Based on the style files for ACL 2020, which were %% Based on the style files for ACL 2018, NAACL 2018/19, which were %% Based on the style files for ACL-2015, with some improvements %%  taken from the NAACL-2016 style %% Based on the style files for ACL-2014, which were, in turn, %% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009, %% EACL-2009, IJCNLP-2008... %% Based on the style files for EACL 2006 by  %%e.agirre@ehu.es or Sergi.Balari@uab.es %% and that of ACL 08 by Joakim Nivre and Noah Smith  \documentclass[11pt,a4paper]{article} \usepackage[hyperref]{emnlp2020} \usepackage{times} \usepackage{latexsym} \renewcommand{\UrlFont}{\ttfamily  \usepackage{booktabs} \usepackage{multirow} \usepackage{graphicx}  \usepackage{subcaption} \usepackage{comment}  \usepackage{amsmath} \usepackage{amsfonts} \usepackage{amssymb}  \usepackage{algorithm} \usepackage{algorithmic}   \renewcommand{\algorithmicrequire}{Input:} \renewcommand{\algorithmicensure}{Output:}    %  Enter the acl Paper ID here  % % You can expand the titlebox if you need extra space % to show all the authors. Please do not make the titlebox % smaller than 5cm ; we will check this % in the camera-ready version and ask you to change it back.  [1]{{ {\TeX}  \title{ ALICE: Active Learning with Contrastive Natural Language Explanations  }  \author{Weixin Liang \\   Stanford University \\    \\\And   James Zou \\   Stanford University \\    \\\And   Zhou Yu \\   University of California, Davis \\    \\   }        \date{}        % abstract    \footnotetext[1]{Co-supervised project.}     \clearpage  
","  % abstract   \input abstract   % Long papers may consist of up to 8 pages of content, plus unlimited pages for references; final versions of long papers will be given one additional page of content  so that reviewers闁 comments can be taken into account.",274
" 	 	 	Causal explanation detection  aims to detect whether there is a causal explanation in a given message . Linguistically, there are coherence relations in messages which explain how the meaning of different textual units can combine to jointly build a discourse meaning for the larger unit. The explanation is an important relation of coherence which refers to the textual unit  in a message that expresses explanatory coherent semantics . As shown in Figure , M1 can be divided into three discourses, and D2 is the explanation that expresses the reason why it is advantageous for the equipment to operate at these temperatures. CED is important for tasks that require an understanding of textual expression . For example, for question answering, the answers of questions are most likely to be in a group of sentences that contains causal explanations . Furthermore, the summarization of event descriptions can be improved by selecting causally motivated sentences . Therefore, CED is a problem worthy of further study. 	 	 	 	 	The existing methods mostly regard this task as a classification problem . At present, there are mainly two kinds of methods, feature-based methods and neural-based methods, for similar semantic understanding tasks in discourse granularity, such as opinion sentiment classification and discourse parsing . The feature-based methods can extract the feature of the relation between discourses. However, these methods do not deal well with the implicit instances which lack explicit features. For CED, as shown in Figure , D2 lacks explicit features such as , , or the features of tenses, which are not friendly for feature-based methods. The methods based on neural network are mainly Tree-LSTM model  and hierarchical Bi-LSTM model . The Tree-LSTM models learn the relations between words to capture the semantics of discourses more accurately but lack further understanding of the semantics between discourses. The hierarchical Bi-LSTM models can employ sequence structure to implicitly learn the relations between words and discourses. However, previous work shows that compared with Tree-LSTM, Bi-LSTM lacks a direct understanding of the dependency relations between words. Therefore, the method of implicit learning of inter-word relations is not prominent in the tasks related to understanding the semantic relations of messages . Therefore, how to directly learn the relations between words effectively and consider discourse-level correlation to further filter the key information is a valuable point worth studying. 	 	Further analysis, why do the relations between words imply the semantics of the message and its discourses? From the view of computational semantics, the meaning of a text is not only the meaning of words but also the relation, order, and aggregation of the words. In other simple words is that the meaning of a text is partially based on its syntactic structure . In detail, in CED, the core and subsidiary words of discourses contain their basic semantics. For example, as D1 shown in Figure , according to the word order in syntactic structure, we can capture the  of  is . We can understand the basic semantic of D1 which expresses some kind of  is  via root words  and its affiliated words. Additionally, why the correlation and key information at the discourse level are so important to capture the causal explanatory semantics of the message? Through observation, the different discourse has a different status for the explanatory semantics of a message. For example, in M1, combined with D1, D2 expresses the explanatory semantics of , while D3 expresses the semantic of transition. In detail, D1 and D2 are the keys to the explanatory semantics of M1, and if not treated D1, D2, and D3 differently, the transitional semantic of D3 can affect the understanding of the explanatory semantic of M1. Therefore, how to make better use of the information of keywords in the syntactic structure and pay more attention to the discourses that are key to explanatory semantics is a problem to be solved. 	 	To this end, we propose a Pyramid Salient-Aware Networks  which utilizes keywords on the syntactic structure of each discourse and focuses on the key discourses that are critical to explanatory semantics to detect causal explanation of messages. First, what are the keywords in a syntactic structure? From the perspective of syntactic dependency, the root word is the central element that dominates other words, while it is not be dominated by any of the other words, all of which are subordinate to the root word . From that, the root and subsidiary words in the dependency structure are the keywords at the syntax level of each discourse. Specifically, we sample 100 positive sentences from training data to illuminate whether the keywords obtained through the syntactic dependency contain the causal explanatory semantics. And we find that the causal explanatory semantics of more than 80\% sentences be captured by keywords in dependency structure\footnote{Five Ph.D. students majoring in NLP judge whether sentences could be identified as which containing causal explanatory semantics by the root word and its surrounding words in syntactic dependency, and the agreement consistency is 0.8}. Therefore, we extract the root word and its surrounding words on the syntactic dependency of each discourse as its keywords.  	 	Next, we need to consider how to make better use of the information of keywords contained in the syntactic structure. To pay more attention to keywords, the common way is using attention mechanisms to increase the attention weight of them. However, this implicitly learned attention is not very interpretable. Inspired by previous researches , we propose a bottom graph-based word-level salient network which merges the syntactic dependency to capture the salient semantics of discourses contained in their keywords. Finally, how to consider the correlation at the discourse level and pay more attention to the discourses that are key to the explanatory semantics? Inspired by previous work , we propose a top attention-based discourse-level salient network to focus on the key discourses in terms of explanatory semantics. 	 	In summary, the contributions of this paper are as follows:  	 		yramid Salient-Aware Network  to detect causal explanations of messages which can effectively learn the pivotal relations between keywords at word level and further filter the key information at discourse level in terms of explanatory semantics. 		 		 	 	
"," 		Causal explanation analysis  can assist us to understand the reasons behind daily events, which has been found very helpful for understanding the coherence of messages. In this paper, we focus on , an important subtask of causal explanation analysis, which determines whether a causal explanation exists in one message. We design a Pyramid Salient-Aware Network  to detect causal explanations on messages. PSAN can assist in causal explanation detection via capturing the salient semantics of discourses contained in their keywords with a bottom graph-based word-level salient network. Furthermore, PSAN can modify the dominance of discourses via a top attention-based discourse-level salient network to enhance explanatory semantics of messages. The experiments on the commonly used dataset of CEA shows that the PSAN outperforms the state-of-the-art method by 1.8\% F1 value on the  task.",275
"     Event coreference resolution  is a task about determining which event mentions in a document refer to the same real-world event. Event coreference resolution is an important part of NLP systems such as summarization, text-level event extraction, question answering and so on. Besides, compared to considerable research of entity coreference resolution, there is less attention on event coreference resolution. Therefore, event coreference resolution is still a challenging task and the performance should be improved.           Event mentions that refer to the same event can occur both within a document  and across multiple documents . We focus on WD event coreference in this paper because WD event coreference is the basic work of CD event coreference.     The main task of WD event coreference is judging whether a pair of events are coreferential or not.     Figure shows two coreferential event pairs from two documents. The first event pair in D1 is about  event and the second event pair in D2 is about  event.      In order to judge the coreference of a event pair, most approaches for solving event coreference resolution relied on various linguistic properties especially , which contains {spatio-temporal} information of events. For instances, in Figure, the words with red front are events. And the words with blue, green and orange front are participant, time, location of the events respectively.            Although event arguments contain useful information for event coreference resolution, there are two problems for using event arguments information in event coreference resolution. Firstly, it's difficult to extract event arguments accurately due to the diversity of the expression of event arguments. The performance of event argument extraction is only 55.7 in ACE corpus. For instance, in D1, the arguments about  event in the two sentences are the same but are expressed differently. In details, in D1, the participant, time, location of  event are ,  and  in S1, but ,  and  in S2 respectively. Secondly, not every event mention contains all arguments of one event that may make model confused about the coreference of two events in a event pair. For instance, in D2, the  that the location of  event is in S1 but not in S2. Besides, in D2, devoid of event arguments,  event and  event are coreferential in context.      As aforementioned, the arguments of events are difficult to extract. It is also difficult to use arguments to solve all the problems of event coreference resolution even if they are extracted. Thus, the context about event mentions is more important and effective for event coreference resolution. In order to use context information efficiently, we propose a multi-loss neural network model  which doesn't need any argument information to accomplish within-document event coreference resolution task. We propose two sub-models which use context information to detect the coreference of two events in a event pair and train them jointly. One is a classifier which  predicts whether the two events in one pair are coreferential, and another is a scorer which calculates similarity scores between them to assist infer coreference.  	     The final stage about event coreference resolution is event clustering. After all event pairs are predicted and scored, we filter event pairs according to the results of classifier and scorer. Then, we use a dynamic connectivity algorithm to construct a graph for event clustering. Each node in graph is a event mention and each edge between two nodes represent whether the two event are coreferential or not. Finally, all events connected in one graph are considered to be in one event cluster. 	     We evaluate our model on ECB+ corpus and use , ,  and  as measures. The experimental results show that our model achieve a significant improvement compared to the state-of-the-art methods which use event argument features.       
","Event coreference resolution is an important task in Natural Language Processing  and nearly all the existing approaches to this task  rely on event argument information. However, these methods tend to suffer from error propagation from the stage of event argument extraction. Besides, not every event mention contains all arguments of an event and argument information may confuse the model that events have arguments to detect event coreference in real text. Furthermore, the context information of an event is useful to infer coreference between events. Thus, in order to reduce the errors propagated from event argument extraction and use context information effectively, we propose a multi-loss neural network model which does not need any argument information to do the within-document event coreference resolution task and achieve a significant performance than the state-of-the-art methods.",276
" A task-oriented spoken dialogue system usually consists of three modules: input,output and control, shown in Fig.. The input module which consists of automatic speech recognition  and spoken language understanding  extracts semantic-level user dialogue actions from user speech signal. The control module  has two missions. One is to maintain dialogue state, an encoding of the machine's understanding about the conversation. Once the information from the input module is received, the dialogue state is updated by dialogue state tracking . The other is to choose a semantic-level machine dialogue action to response the user, which is called dialogue decision policy.    The output consists of natural language generation  and text-to-speech  synthesis, which convert dialogue action to audio. Dialogue management is an important part of a dialogue system. Nevertheless, there are inevitable ASR and SLU errors which make it hard to track true dialogue state and make decision. In recent statistical dialogue system, the distribution of dialogue state, i.e. belief state, is tracked. A well-founded theory for belief tracking and decision making is offered by partially observable Markov Decision Process  framework.  Previous DST algorithms can be divided into three families: hand-crafted rules, generative models, and discriminative models. Recently, since the Dialog State Tracking Challenges  have provided labelled dialog state tracking data and a common evaluation framework and test-bed, a variety of machine learning methods for DST have been proposed. These methods rely strictly on set of labelled off-line data. Since the labelled data are off-line, the learning process of these supervised learning methods is independent on the dialogue policy module. The key issues of these supervised learning methods are poor generalization and over-tuning. Due to the lack of labels, these approaches can not be easily used for on-line update of DST.   This work marks first step towards employing the deep reinforcement learning  method into dialogue state tracking  module. The performance of the DST module is optimized during the conversation between the user and the dialogue system. We call the DRL-based DST module as the tracking agent. In order to bound the search space of the tracking agent, we propose a companion teaching framework . Furthermore, under this framework, we can train tracking agent and dialogue policy agent jointly with respective deep reinforcement learning  algorithms in order to make these two agents adaptive to each other. % And there are two main types of DST systems in the current dialogue system. One is the semantic-based dialogue state tracking system, and the other is the text-based dialogue state tracking system that implicitly or explicitly removes spoken language understanding  module. In this paper, we explain the proposed tracking agent framework based on the semantic-based dialogue state tracking  system.  The paper has two main contributions:     The rest of the paper is organized as follows. Section  gives an overview of related work. In Section , the framework of on-line DST are presented. The implementation detail is represented in Section . In Section , the joint training process is introduced. Section  presents experiments conducted to evaluate the proposed framework, followed by the conclusion in Section .  
"," Dialogue state tracking  is a crucial module in dialogue management. It is usually cast as a supervised training problem, which is not convenient for on-line optimization. In this paper, a novel companion teaching based deep reinforcement learning  framework for on-line DST optimization is proposed. To the best of our knowledge, this is the first effort to optimize the DST module within DRL framework for on-line task-oriented spoken dialogue systems. In addition, dialogue policy can be further jointly updated. Experiments show that on-line DST optimization can effectively improve the dialogue manager performance while keeping the flexibility of using predefined policy. Joint training of both DST and policy can further improve the performance.",277
"   {T}{he} task-oriented spoken dialogue system  aims to assist a human user in accomplishing a specific task . The dialogue management is a core part of SDS. There are two main missions in dialogue management: dialogue belief state tracking  and dialogue decision-making . In this work, we only focus on devising a policy that chooses which dialogue action to respond to the user.   The sequential system decision-making process can be abstracted into a partially observable Markov decision process . Under this framework, reinforcement learning approaches can be used for automated policy optimization.   In the past few years, there are many deep reinforcement learning  algorithms閿 which use neural networks  as function approximators, investigated for dialogue policy .  Most of these approaches focus on dialogue policy optimization in a single dialogue task. However, in real-life scenarios, a dialogue agent can be asked by many users for different dialogue tasks, e.g., Apple Siri can support many dialogue tasks .  In the multi-task setup, the traditional DRL-based approaches have to train an individual policy for each dialogue task.  It means that each dialogue policy has its independent model parameters, whose scale will increase proportionally with the number of the tasks. One solution is to train a { and {  In this paper, we propose  the Structured Actor-Critic  Reinforcement Learning for Universal Dialogue Management  . With the scalability of the GNN , a single set of parameters can be used for different dialogue tasks. That makes it possible to train a generic policy among multiple dialogue tasks. To tackle the efficiency problem, we deploy an advanced off-policy actor-critic algorithm, which combines decoupled acting and learning with a novel off-policy correction method called V-trace. Combining the improved optimization algorithm with the structured dialogue policy, we can make the generic policy learning process more stable and efficient than the original GNN-based dialogue policy .  We evaluate the performance of STRAC on PyDial benchmark, which includes six environments and three dialogue domains. Results show that our unified dialogue agent STRAC gets the best performance on most of the 18 tasks of the benchmark.  
"," Traditional dialogue policy needs to be trained independently for each dialogue task. In this work, we aim to solve a collection of independent dialogue tasks using a unified dialogue agent. The unified policy is parallelly trained using the conversation data from all of the distributed dialogue tasks. However, there are two key challenges: the design of a unified dialogue model to adapt to different dialogue tasks;  finding a robust reinforcement learning method to keep the efficiency and the stability of the training process. Here we propose a novel structured actor-critic approach to implement structured deep reinforcement learning , which not only can learn parallelly from data of different dialogue tasks\footnote{In the experimental setup of this work, each dialogue task has only one dialogue domain.} but also achieves stable and sample-efficient learning. We demonstrate the effectiveness of the proposed approach on 18 tasks of PyDial benchmark. The results show that our method is able to achieve state-of-the-art performance.",278
"  %When building a dialogue system, complex tasks that require more information exchange are often more challenging. One example is to handle the restaurant reservation consultation in multiple areas during a single conversation. Specifically, this type of task that needs to complete some subtasks  in order to finish the conversation is called the composite task.  Composite tasks are different from multi-domain dialogue tasks. The latter is often mentioned in papers that focusing on transfer learning. In most case, multi-domain dialogue tasks involve only one domain in a single dialogue, and the performance of this one domain model is tested on different domains in order to highlight its transferability. On the contrary, composite dialogue tasks may involve multiple domains in a single dialogue, and the agent must complete all subtasks  in order to get positive feedback.  Consider the process of completing a composite task . An agent first chooses a subtask , then make a sequence of decisions to gather related information  until all information required by users are provided and these subtasks are completed, and then choose the next subtask  to complete. The state-action space will increase with the number of subtasks. Thus, dialogue policy learning for the composite task needs more exploration, and it needs to take more dialogue turn between agent and user to complete a composite task. The sparse reward problem is further magnified.   Solving composite tasks using the same method as the one solving single domain tasks may hit obstacles. The complexity of the composite task makes it hard for an agent to learn an acceptable strategy. While hierarchical deep reinforcement learning  shows its promising power, by introducing the framework of options over Markov Decision Process , the original task can be decomposed into two parts: deciding which subtask to solve and how to solve one subtask, thus simplifying the problem.  However, in previous works, multi-layer perceptrons  are often used in DQN to estimate the Q-value. MLPs use the concatenation of the flatten dialogue state as its inputs. In this way, it cannot capture the structural information of the semantic slots in that state easily, which results in low sampling efficiency. In our work, we propose ComNet, which makes use of the Graph Neural Network  to better leverage the graph structure in the observations  and being coherent with the HDRL method.  Our main contributions are three-fold: 1. We propose a new framework ComNet combining HDRL and GNN to solve the composite tasks while achieving sample efficiency. 2. We test ComNet based on PyDial  benchmark and show that our result over-performed the vanilla HDRL systems and is more robust to noise in the environment. 3. We test the transferability of our framework and prove that under our framework, an efficient and accurate transfer is possible.  
"," Dialogue policy training for composite tasks, such as restaurant reservation in multiple places, is a practically important and challenging problem. Recently, hierarchical deep reinforcement learning  methods have achieved good performance in composite tasks. However, in vanilla HDRL, both top-level and low-level policies are all represented by multi-layer perceptrons  which take the concatenation of all observations from the environment as the input for predicting actions. Thus, traditional HDRL approach often suffers from low sampling efficiency and poor transferability. In this paper, we address these problems by utilizing the flexibility of graph neural networks . A novel ComNet is proposed to model the structure of a hierarchical agent. The performance of ComNet is tested on composited tasks of the PyDial benchmark. Experiments show that ComNet outperforms vanilla HDRL systems with performance close to the upper bound. It not only achieves sample efficiency but also is more robust to noise while maintaining the transferability to other composite tasks.",279
"  Relation extraction  aims to identify the semantic relations between named entities in text. While previous work  focuses on extracting relations within a sentence, a.k.a.~-level RE, recent studies  have escalated it to the  level, since a large amount of relations between entities usually span across multiple sentences in the real world. According to an analysis on Wikipedia corpus , at least 40.7\% of relations can only be extracted on the document level.  Compared with sentence-level RE, document-level RE requires more complex reasoning, such as logical reasoning, coreference reasoning and common-sense reasoning. A document often contains many entities, and some entities have multiple mentions under the same phrase of alias. To identify the relations between entities appearing in different sentences, document-level RE models must be capable of modeling the complex interactions between multiple entities and synthesizing the context information of multiple mentions.   Figure shows an example of document-level RE. Assume that one wants to extract the relation between ``Surfers Riverwalk"" in S11 and ``Queensland"" in S1. One has to find that ``Surfers Riverwalk"" contains ``Pacific Fair"" , and  ``Pacific Fair""  is located in ``Queensland"" . This chain of interactions helps infer the inter-sentential relation ``located in"" between ``Surfers Riverwalk"" and ``Queensland"".    %==================== 
"," Relation extraction  aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions.",280
"  Dialogue state tracker is a core part of the task-oriented dialogue system, which records the dialogue state. The dialogue state consists of a set of  triples, where the specific value represents the user goal, e.g., . The dialogue system responds to the user just based on the dialogue state. Thus, in order to make the dialogue process natural and fluent, it is essential to extract the dialogue state from the dialogue context accurately. However, the paucity of annotated data is the main challenge in this field. In this work, we solve a key problem that how to learn from the unlabeled data in DST task. We design a dual learning framework for DST task, where the dialogue state tracker is the primal agent and the dual agent is the utterance generator. Within the dual learning framework, these two primal-dual agents help to update each other through external reward signals and reconstruction errors by using unlabeled data. It only needs a few of labeled dialogue data to warm up these two primal-dual agents.  However, there are two main challenges when combining dual learning framework with previous dialogue state tracking  methods:  How to represent dialogue state under dual learning framework? Dual learning method is first proposed in the neural machine translation  task. The outputs of the primal-dual agents in NMT task are both sequential natural languages. However, in DST task, the output of the dialogue state tracker consists of isolated domain-slot-value triples. The traditional DST task is formulated as a classification problem with the given ontology, where all the possible values of the corresponding slot are listed. Under this problem definition, the previous classification methods just choose the right value for each slot. The recent innovated tracker TRADE directly generates the values slot by slot using copy mechanism from dialogue context. However, these tracker methods get slot values independently. During the dual learning loop, it is hard to get reward signal from these independent slot values. The reward signal from dual utterance generator is also hard to allocate to these isolated value generation processes. Since the relations of the predicted values are not modeled and they are assumed to be independent with each other, it would face serious reward sparse problem. In this work, we reformulate the dialogue state tracking task as a sequential generation task. The whole dialogue state is represented by a sequence with structured information. For example, the state  can be represented as ``\textless\textgreater  \textless\textgreater  \textless\textgreater   \textless\textgreater  \textless\textgreater  \textless\textgreater  \textless\textgreater''.  [tb]             Is it reasonable that generating the whole dialogue context from dialogue state? The intuitive dual task of the state tracker is dialogue context generation. However, in MultiWOZ 2.1 dataset, the dialogue context has more than 10 turns on average and the average length of each sentence is over 10 tokens. It is very difficult in generating accurately a dialogue context with a dialogue state. Because the dialogue context is too long, it is hard to guarantee that the generated dialogue context contains the same semantics with the given state. In this work, we simplify the dual task into a user utterance generation task which ignores the specific values of the given state. The input of the dual task is composed of two parts , and its output is the delexicalized user utterance. The delexicalized script is copied from the released code \footnote{https://github.com/ConvLab/ConvLab}. The system utterance and user utterance can be lexicalized respectively according to the given turn state. We get a new pseudo-labeled dialogue turn. In order to produce multi-turn pseudo-labeled data, we sample a labeled dialogue data and combine it with the pseudo-labeled dialogue turn, where the dialogue turn directly adds to the end of the sampled dialogue context and the turn state covers into the label of the sampled state. Finally, we get a new dialogue context and pseudo label of the state, as the intuitive dual-task does.  The main contributions of this paper are summarized as follows:        
"," In task-oriented multi-turn dialogue systems, dialogue state refers to a compact representation of the user goal in the context of dialogue history. Dialogue state tracking  is to estimate the dialogue state at each turn. Due to the dependency on complicated dialogue history contexts, DST data annotation is more expensive than single-sentence language understanding, which makes the task more challenging. In this work, we formulate DST as a sequence generation problem and propose a novel dual-learning framework to make full use of unlabeled data. In the dual-learning framework, there are two agents: the primal tracker agent  and the dual utterance generator agent . Compared with traditional supervised learning framework, dual learning can iteratively update both agents through the reconstruction error and reward signal respectively without labeled data. Reward sparsity problem is hard to solve in previous DST methods. In this work, the reformulation of DST as a sequence generation model effectively alleviates this problem. We call this primal tracker agent dual-DST. Experimental results on MultiWOZ2.1 dataset show that the proposed dual-DST works very well, especially when labelled data is limited. It achieves comparable performance to the system where labeled data is fully used.",281
"  % P1 intro {T}{ask-oriented} dialog system aims for users to achieve goals such as finding attractions or booking restaurants. Developing such a system typically requires the following dialog components to construct a pipeline as illustrated in \fig : natural language understanding  to extract user's intents and slot-values , dialog state tracking  to update belief states , querying database, dialog policy  to decide the system's next action , and natural language generation  to generate system responses . Although recent advances in neural approaches in the natural language domain have greatly improved the performance of individual dialog components, errors in each component are accumulated in the pipelined system, resulting in degradation of overall performance. Therefore, designing an effective architecture and optimizing the entire dialog system in an end-to-end fashion are still challenging.  % P2 % e2e Recently, several end-to-end  neural dialog systems have proposed . % Modularized % seq2seq Sequence-to-sequence approaches directly generate system responses given user utterance inputs, but they have limitations that querying the external database is unavailable , and system actions are not interpretable . % RL in e2e Moreover, a few previous researchers have investigated dialog policy optimization by reinforcement learning in end-to-end neural task-oriented dialog systems .  % GPT-2 Meanwhile, recent approaches that transfer general linguistic knowledge from large pre-trained language model, GPT-2 , to goal-oriented dialog have shown remarkable improvements . They employed the GPT-2 backbone as it is, and fine-tuned the model to auto-regressively generate dialog states, system actions, and responses in a sequence. Although leveraging the rich knowledge allows the models to generate more natural and appropriate responses, reinforcement learning on transformer-based architectures has been reported as unstable , and learning dialog policy on those models has not been explored yet.  % P4 our approaches    In this paper, we present an end-to-end trainable neural dialog system with reinforcement learning for multi-domain task-completion tasks, SUMBT+LaRL, which consists of two components:  an extended version of SUMBT for a word-level dialog state tracker and  LaRL for a word-level policy model.  In addition to SUMBT that updates belief states employing the slot-utterance matching mechanism, SUMBT+ predicts domains and user-intents from the user utterance.  Then given the predictions by SUMBT+, the LaRL models a categorical latent system action spaces and generates system responses. In our training framework, we emphasize the importance of separately pre-train SUMBT+ and LaRL and fine-tune the entire model in an end-to-end fashion. Then, the trained dialog policy is further optimized by off-line reinforcement learning using REINFORCE algorithm to succeed dialog tasks. During reinforcement training, the policy gradients at latent actions decouple the discourse-level decision making and language generation by the decoder, enabling stable and effective reinforcement learning. We further propose new success  criteria in which the system has to respond to more requestable slots and calculate the match performance using the belief state estimated by SUMBT+.  We demonstrated the efficacy of the proposed system on MultiWOZ2.0, implementing on ConvLab platform for user simulator-based evaluations. Our extensive experimental results on both corpus and simulator-based evaluation shows the effectiveness of the proposed pretraining and end-to-end fine-tuning framework as well as reinforcement learning on the latent action space. From the results and qualitative analysis of simulated dialog examples, we also present the discrepancy problem of corpus and automatic evaluations, the limitations of off-line reinforcement learning for dialog systems, and the needs of advanced reward design and success criteria. Our model achieved the new state-of-the-art success rate in the end-to-end corpus-based evaluation on the MultiWOZ2.0, as well as outperformed the challenge winner of the 8th dialog system technology challenge  challenge in the simulator-based evaluation.  % P5 contribution summary In summary, the main contributions of this paper are three-fold:         % section intro   \sect 2 briefly reviews end-to-end multi-domain task-completion dialog systems and the DSTC8 Challenge. \sect 3 explains the detailed architecture of the proposed SUMBT+LaRL and training procedures. Related work is described in \sect 4 and experimental results are presented in \sect 5.  %\newpage %\clearpage 
"," The recent advent of neural approaches for developing each dialog component in task-oriented dialog systems has remarkably improved, yet optimizing the overall system performance remains a challenge. In this paper, we propose an end-to-end trainable neural dialog system with reinforcement learning, named SUMBT+LaRL. The SUMBT+ estimates user-acts as well as dialog belief states, and the LaRL models latent system action spaces and generates responses given the estimated contexts. We experimentally demonstrate that the training framework in which the SUMBT+ and LaRL are separately pretrained and then the entire system is fine-tuned significantly increases dialog success rates. We propose new success criteria for reinforcement learning to the end-to-end dialog system as well as provide experimental analysis on a different result aspect depending on the success criteria and evaluation methods. Consequently, our model achieved the new state-of-the-art success rate of 85.4\% on corpus-based evaluation, and a comparable success rate of 81.40\% on simulator-based evaluation provided by the DSTC8 challenge.  %The recent advent of neural approaches for developing each dialog component in task-oriented dialog systems has remarkably improved, yet optimizing the overall system performance remains a challenge. In this paper, we propose an end-to-end trainable neural dialog system with reinforcement learning, named SUMBT+LaRL. The SUMBT+ estimates user-acts as well as dialog belief states, and the LaRL models latent system action spaces and generates responses given the estimated contexts. We experimentally demonstrate that the training framework in which the SUMBT+ and LaRL are separately pretrained and then the entire system is fine-tuned significantly increases dialog success rates. We propose new success criteria for reinforcement learning to the end-to-end dialog system as well as provide experimental analysis on a different result aspect depending on the success criteria and evaluation methods. Consequently, our model achieved the new state-of-the-art success rate of 85.4% on corpus-based evaluation, and a comparable success rate of 81.40% on simulator-based evaluation provided by the DSTC8 challenge.",282
"   The massive rise in user-generated web content, alongside with the freedom of speech in social media and anonymity of the users has brought about an increase in online offensive content and anti-social behavior. The consequences of such behavior on genuine users of the social media have become a serious concern for researchers in Natural Language Processing and related fields in recent years.  The shared task number 6 at SemEval 2019, OffensEval , proposes to model the task of offensive language identification hierarchically, which means identifying the offensive content, whether it is targeted, and if so, the target of the offense. In OffensEval, offensive language is defined as ``any form of non-acceptable language  or a targeted offense, which can be veiled or direct'' which includes ``insults, threats, and posts containing profane language or swear words'' .  We have participated in the first two subtasks  of OffensEval with the proposed approach of a deep model consisting of a Recurrent Neural Network  for word-level and Convolutional Neural Network  for character-level processing. Character-level processing is beneficial, as offensive comments are likely to follow unorthodox writing styles, contain obfuscated words, or have irregular word separation which leads to tokenization issues . We also experimented with two other methods, a Support Vector Machine  with TFIDF and count features and another SVM with BERT  -encoded sentences as input, both with lower performances comparing with the deep model.  After overviewing the related work in section , we discuss the methodology and the data in details in section , and the results in section . In section , we analyze the results and conclude the paper in section .   
"," This paper presents the models submitted by Ghmerti team for subtasks A and B of the OffensEval shared task at SemEval 2019. OffensEval addresses the problem of identifying and categorizing offensive language in social media in three subtasks; whether or not a content is offensive , whether it is targeted  towards an individual, a group, or other entities . The proposed approach includes character-level Convolutional Neural Network, word-level Recurrent Neural Network, and some preprocessing. The performance achieved by the proposed model for subtask A is 77.93\% macro-averaged F\textsubscript{1}-score.",283
"  A wide range of Natural Language Processing  tasks, such as Machine Translation , speech recognition, information retrieval, data mining, and creating text resources for low-resource languages benefit from the upstream task of language identification. The Cuneiform Language Identification  task in VarDial 2019  tries to address the problem of identifying languages and dialects of the texts written in cuneiform symbols.   Identifying languages and dialects of the cuneiform texts is a difficult task, since such languages lack resources and also there is the problem of tokenization. Although there are some work addressing the problem of tokenization in some of these languages or dialects, there is not any universal method or tool available for tokenization of cuneiform texts, as such a task depends on the rules of that language, simply because cuneiform writing system is a syllabic as well as a logographic one.  As a result, all the endeavors in this paper are based on character-level features. This work investigates different machine learning methods which are proven to be effective in text classification and compares them by their obtained F\textsubscript{1}-score, accuracy, and training time.  In this paper, we first review the literature of language identification and the work on languages written using cuneiform writing system in , introduce the models used to tackle the problem of identifying such languages and dialects in , describe the training data in , and discuss the results in .  % You can begin with a brief description of the task and an overview of your approach.   % We would like to ensure that future readers of your paper can find the relevant task description, data and results. So, we ask that you cite the shared task report paper  in your introduction.  
"," Identification of the languages written using cuneiform symbols is a difficult task due to the lack of resources and the problem of tokenization. The Cuneiform Language Identification task in VarDial 2019 addresses the problem of identifying seven languages and dialects written in cuneiform; Sumerian and six dialects of Akkadian language: Old Babylonian, Middle Babylonian Peripheral, Standard Babylonian, Neo-Babylonian, Late Babylonian, and Neo-Assyrian. This paper describes the approaches taken by  \tt{SharifCL} -score of 72.10\%.",284
"    In recent years, there has been a growing interest in hierarchical multi-label classification  which can be applied in a wide range of applications such as International Patent Classification , product annotation  and advertising recommendation . In the common flat classification problem, each input sample is only associated with a single label from a set of disjoint labels. However, in HMC problem, the labels are organized in the form of a tree or a Directed Acyclic Graph  and each input sample is usually associated with multiple labels, which made it more challenging.  The most straight-forward approach in dealing with HMC problem is to convert it to a flat multi-label classification problem by simply ignoring the relevance between labels . The main disadvantage in doing so is the loss of the useful hierarchical information. Alternatively, the local approach  is designed to perform multi-label classification, where the classifications are carried out at each level of the label hierarchy . The overall classification results are then generated based on these local predictions. While hierarchical information can be better utilized in local approaches,  misclassifications are easily propagated to the next levels . Global approaches are proposed to learn a single global model for all labels to reduce the model size and consider the entire label hierarchy at once . These global classifiers are typically built on flat classifiers with modifications made to integrate the hierarchical information of labels  into the model. Recently, more algorithms which combine the local and global approaches are proposed .  All algorithms introduced above only focus on the design of hierarchical classifier while ignoring the hierarchical features which may be extracted and they are important in HMC as well.  and  consider hierarchical feature extraction in their work. However, the extraction process is designed and fulfilled by applying the typical attention mechanism over the whole text. Since in HMC problem the text may be associated with multiple labels at each hierarchy level, the features extracted from typical attention may be diluted.  We believe it is reasonable to hypothesize that a label-based attention, where information extraction is performed based on different labels at different hierarchical levels, would allow the model to be more interpretable and have an overall better performance in accuracy. Given the above motivations, we propose LA-HCN --- a HMTC model with a label-based attention to facilitate label-based hierarchical feature extraction, where we introduce the concept and mechanism of component which is an intermediate representation that helps bridge the latent association between the words and the labels for label-based attention.     Main contributions of this work:        
"," Hierarchical multi-label text classification  has been gaining popularity in recent years thanks to its applicability to a plethora of real-world applications. The existing HMTC algorithms largely focus on the design of classifiers, such as the local, global, or a combination of them. However, very few studies have focused on hierarchical feature extraction and explore the association between the hierarchical labels and the text. In this paper, we propose a Label-based Attention for Hierarchical Mutlti-label Text Classification Neural Network  , where the novel label-based attention module is designed to hierarchically extract important information from the text based on the labels from different hierarchy levels. Besides, hierarchical information is shared across levels while preserving the hierarchical label-based information. Separate local and global document embeddings are obtained and used to facilitate the respective local and global classifications. In our experiments, LA-HCN outperforms other state-of-the-art neural network-based HMTC algorithms on four public HMTC datasets. The ablation study also demonstrates the effectiveness of the proposed label-based attention module as well as the novel local and global embeddings and classifications. By visualizing the learned attention , we find that LA-HCN is able to extract meaningful information corresponding to the different labels which provides explainability that may be helpful for the human analyst.",285
"  Multi-task learning is the problem of minimizing the average error across  tasks, as measured on held-out samples, and motivated by the observation that sometimes learning a single model with partially shared parameters performs better than  single-task models. In the learning-to-learn setting, we worry about our error on a task . Both of these settings apply to randomly initialized base learners, as well as architectures pre-trained on yet another task. In learning-to-learn, the new task  is assumed to come from an ambiguity set defined by the  tasks.  Unsurprisingly, most approaches to multi-task learning minimize the average loss across the training samples available for these tasks. This does not always lead to the best solution, however, since the relations between loss and error may differ across tasks. Several off- and online methods for normalizing these relations have been proposed , but even with this, minimizing average loss across tasks has two disadvantages:  Performance on outlier tasks may be very poor ; and  in the learning-to-learn setting, minimizing average loss is only optimal if the task selection is unbiased .   Minimizing the worst-case loss across tasks instead of the average loss, in theory solves these two problems, which is why this approach is popular in algorithmic fairness  and domain adaptation under covariate shift assumptions . In some multi-task settings, it is possible to directly modify the loss that is minimized in multi-task learning , but this is for example not possible in the common approach to multi-task learning where each batch is sampled from one of  tasks at random . We present a more general approach to multi-task learning with worst-case-aware loss minimization, instead relying on automated curriculum learning .    We present an automated curriculum learning approach to robust multi-task transfer learning. Our approach is general and parameterizes a family of worst-case-aware objectives, with minimax and loss-proportional minimization at the two extremes. In a series of experiments on the GLUE multi-task benchmark , we show that several of these objectives lead to better performance on the benchmark itself, but more importantly, also lead to much better  generalization to other out-of-domain data sets. %Finally, we show that the shared models learned using worst-case-aware curriculum learning also perform better in learning-to-learn settings.     
"," Multi-task transfer learning based on pre-trained language encoders achieves state-of-the-art performance across a range of tasks. Standard approaches implicitly assume the tasks, for which we have training data, are equally representative of the tasks we are interested in, an assumption which is often hard to justify. This paper presents a more agnostic approach to multi-task transfer learning, %, relying on $\alpha$-ball  which uses automated curriculum learning to minimize a new family of worst-case-aware losses across tasks. Not only do these losses lead to better performance on outlier tasks; they also lead to better performance in zero-shot and few-shot transfer settings.",286
" There is a wide of range of existing natural language processing  toolkits such as CoreNLP , UDPipe , FLAIR , spaCy{State-of-the-art} \\{Performance} & [c]{@{}c@{}}{All Chinese} \\{Fundamental Tasks} & [c]{@{}c@{}}{Multi-task} \\{Learning} \\ \hline       %      CoreNLP        & Java                 &              &                  &                   &            \\        %      spaCy          & Python               &              &                  &                   &            \\       % CoreNLP        & Java                 &              &                  &                   &            \\        LTP       & C++                  &              &                               & $            In this paper, we introduce , a PyTorch-based Chinese natural language processing toolkit for NLP, which built in the SOTA pre-trained model. As shown in Figure, given Chinese corpus,  produces comprehensive analysis results, including lexical analysis, syntactic parsing, and semantic parsing. In addition,  is user-friendly where the fundamental tasks API and visualization tool are provided. %in-depth result analysis. As shown in Table, compared to the existing widely-used NLP toolkits,   has the following advantages:             The existing NLP toolkit for the Chinese language all adopts independent models for each task, which ignores the shared knowledge across tasks.         %     Considering that pipelined approaches usually suffer from error propagation due to their independent models in Chinese NLP tasks,          To alleviate this issue, we propose to use the multi-task framework  to take advantage of the shared knowledge across all tasks.         Meanwhile, multi-task learning with shared encoder for all five tasks can greatly reduce the occupied memory and improve the speed, which makes   more efficient, reducing the need for hardware.          In addition, to enable the multi-task learning enhancing each subtask performance, we follow  to adopt the distillation method single-task models teach a multi-task         model, helping the multi-task model surpass its all single-task teachers.         %                 %   We adopt the multi-task learning with shared encoder for all five tasks, which can greatly reduce the occupied space, making   more efficient.         %    In addition, to enable the multi-task learning enhancing each subtask performance, we follow  to adopt the distillation method single-task models teach a multi-task         %  model, helping the multi-task model surpass its all single-task teachers.         %             supports all Chinese NLP tasks including lexical analysis , syntactic parsing, and semantic parsing .         To the best of our knowledge, this is the first neural Chinese toolkits that supports all Chinese fundamental NLP tasks.      works with users閳 custom modules. Users can easily add a new pre-trained model with a configuration file. We have made all task training configuration file open-sourced. Hence, you can change the pretrained model to any BERT-like model supported by Transformers  easily by changing the config.       First, we provide all fundamental tasks API, which is convenient for users to use the toolkit without the need for any knowledge.   Second, we provide a visualization tool, so that users can view the processing results directly.    We evaluate          on a total of five Chinese NLP tasks, and 閾夸苟d that it achieves state-of-the-art or competitive performance at each task.     is fully open-sourced and can support all Chinese fundamental NLP tasks. We hope  can facilitate Chinese NLP research and applications.  
","   We introduce N-LTP, an open-source Python Chinese natural language processing toolkit supporting five basic tasks: Chinese word segmentation, part-of-speech tagging, named entity recognition, dependency parsing, and semantic dependency parsing. N-LTP adopts the multi-task framework with the pre-trained model to capture the shared knowledge across all Chinese relevant tasks. In addition, we propose to use knowledge distillation where single-task models teach a multi-task model, helping the multi-task model surpass its single-task teachers.   Finally, we provide fundamental tasks API and a visualization tool to make users easier to use and view the processing results directly.   To the best of our knowledge, this is the first toolkit to support all Chinese NLP fundamental tasks.   Source code, documentation, and pre-trained models are available at \url{https://github.com/HIT-SCIR/ltp}.",287
" Building robust task-oriented dialogue systems is challenging due to complex system design and limited availability of human-annotated data. A dialogue agent is expected to learn dialogue reasoning, decision making, and language generation, which require a large amount of training data.  However, collecting and annotating data for training a dialogue system is time-intensive and not transferable among domains. One possible workaround is to leverage the pre-trained language model to reduce human supervision.   Recent progress in pre-training language models has been shown to be promising in alleviating the data scarcity problem.  Such models are typically pre-trained on large-scale plain text with self-supervised objectives, e.g., language modeling and language denoising. Fine tuning pre-trained language models improves a wide range of natural language processing applications, notably machine translation, and personalized dialogue response generation. However, adapting pre-trained language models to task-oriented dialogue systems is not trivial.   Current state-of-the-art  approaches in task-oriented dialogue rely on several tasks-specific modules, such as State Operation Predictor for dialogue state tracking, and CopyNet for end-to-end dialogue task completion.   Such modules are usually absent in the pre-training stage. Therefore, tasks-specific architecture modifications are required in order to adapt pre-trained language models to different dialogue tasks.    In this work, we aim to simplify the process of transferring the prior knowledge of pre-trained language models for improving task-oriented dialogue systems.  We propose Minimalist Transfer Learning , a simple yet effective transfer learning framework that allows to plug-and-play pre-trained sequence-to-sequence  models and jointly learn dialogue state tracking  and dialogue response generation. Unlike previous approaches, which use a copy mechanism to ``carryover'' the previous dialogue states and generate new dialogue states, we introduce Levenshtein belief spans  which models the difference between old states and new states. In practice, MinTL first decodes the  for updating the previous dialogue state; then, the updated state is used to search the external knowledge base; and finally, a response decoder decodes response by conditioning on the dialogue context and knowledge base match result.   MinTL is easy to set up by using different pre-trained seq2seq backbones.  We conduct extensive experiments on both DST and end-to-end dialogue response generation tasks with two pre-trained seq2seq models, such as  T5 and BART. The experimental result on a large-scale task-oriented dialogue benchmark MultiWOZ suggests that our proposed method significantly improves SOTA performance in both the full data and simulated low resource setting. Our contributions are summarized as follows:         framework that efficiently leverages pre-trained language models for task-oriented dialogue without any ad hoc module.     -based systems achieve competitive results compared to the SOTA.     [!t]               
"," In this paper, we propose Minimalist Transfer Learning  to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation.  Unlike previous approaches, which use a copy mechanism to ``carryover'' the old dialogue states to the new one, we introduce Levenshtein belief spans , that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5~ and BART~, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20\% training data, and 3) $Lev$ greatly improves the inference efficiency\footnote{Code available in \url{https://github.com/zlinao/MinTL}}.",288
"   Recent advancements in the area of generative modeling have helped increase the fluency of generative models. However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data .  One reason could be that the generation task is typically construed as an end-to-end system. This is in contrast to traditional approaches, which incorporate a sequence of steps in the NLG system, including content determination, sentence planning, and surface realization . A review of literature from psycholinguistics and cognitive science also provides strong empirical evidence that the human language production process is not a monolith .       Prior approaches have indeed incorporated content planning into the NLG system, for example data-to-text generation problems  as well as classic works that include planning, based on speech acts  .  Our work closely follows these prior approaches, with one crucial difference: our planners are not based on dialogue acts or speech acts.   Consider the example in Fig.. An input utterance by Person B, a statement , followed by a question , can be effectively responded to using plans, learned and generated, prior to the realization phase. The realization output can then include the mention of , consistent with the generated plan .   Dialogue acts  , by their nature, encompass a wide variety of realized output, and hence cannot sufficiently constrain the language model during the generation process. Research has addressed this issue by adapting existing taxonomies  towards their own goals . We instead use an adapted and extended form of lexical-conceptual structures  to help constrain the realization output more effectively .  Our work makes the following contributions: \\ % [nolistsep,noitemsep] %%[nosep,wide]           
"," Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation  are typically construed as end-to-end architectures that do not adequately model human generation processes.    To investigate, we decouple generation into two separate phases: planning and realization. In the planning phase, we train two planners to generate plans for response utterances. The realization phase uses response plans to produce an appropriate response. Through rigorous evaluations, both automated and human, we demonstrate that decoupling the process into planning and realization performs better than an end-to-end approach.",289
"   A metaphor is a figurative form of expression that compares a word or a phrase to an object or an action to which it is not literally applicable but helps explain an idea or suggest a likeness or analogy between them. Metaphors have been used extensively in all types of literature and writings, especially in poetry and songs to communicate complex feelings, emotions, and visuals present in the text to readers effectively. Metaphors are ubiquitous in natural language and help in structuring our understanding of the world even without our conscious realization of its presence. Given the prevalence and significance of metaphorical language, effective detection of metaphors plays an essential role in many natural language processing applications, for example, language understanding, information extraction, sentiment analysis, etc.  However, automated detection of metaphorical phrases is a difficult problem primarily due to three reasons. First, there is a subjective component involved: the metaphoricity of expression may vary across humans. Second, metaphors can be domain and context dependent. And third, there is a lack of annotated data, which is required to train supervised machine learning algorithms to facilitate automated detection accurately.   Most of the previous approaches for detection of metaphorical phrases, have either relied on manual and lexical detection which requires heavily handcrafted features built from linguistic resources, that are costly to obtain and greatly limits their applicability or have used supervised machine learning based algorithms with limited forms of linguistic context, for example using only the subject verb objects triplets . Although these techniques automate the detection of metaphorical phrases, however, the prediction accuracies are not as good as the prediction accuracies of these techniques in other text classification tasks.  Inspired by recent works in the field of NLP and transfer learning, in this paper, we present an end-to-end method composed of deep contextualized word embeddings, bidirectional LSTMs and multi-head attention mechanism to address some of the limitations aforementioned. Our method is notable in the sense that unlike many existing approaches, it requires only the raw text sequences as input and does not depend on any complex or fine-tuned feature pipelines.  
"," %% Text of abstract Metaphors are ubiquitous in natural language, and their detection plays an essential role in many natural language processing tasks, such as language understanding, sentiment analysis, etc. Most existing approaches for metaphor detection rely on complex, hand-crafted and fine-tuned feature pipelines, which greatly limit their applicability. In this work, we present an end-to-end method composed of deep contextualized word embeddings, bidirectional LSTMs and multi-head attention mechanism to address the task of automatic metaphor detection. Our method, unlike many other existing approaches, requires only the raw text sequences as input features to detect the metaphoricity of a phrase. We compare the performance of our method against the existing baselines on two benchmark datasets, TroFi, and MOH-X respectively. Experimental evaluations confirm the effectiveness of our approach.",290
"  For text editing, the sequence-to-sequence  framework has been applied to text simplification , punctuation restoration , grammatical error correction , machine translation post-editing , and etc. We observe that current inference methods can be roughly grouped into two categories: End-to-end   and Tagging . For models from both categories, the encoders extract and encode information from the source text sequence. Yet, the goal of the decoders is different for End2end and Tagging. Upon receiving the encoder's hidden states that comprise the source text information, the decoder of End2end directly decodes the hidden states and generates the completely edited target text sequence. But, the decoder of Tagging produces a sequence of editing operations, such as deletion and insertion, that is later applied to the source text to yield the edited text via a realization step . The mechanisms of End2end and Tagging are illustrated in Figure .     
","  In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a new inference method, Recurrence, that iteratively performs editing actions, significantly narrowing the problem space. In each iteration, encoding the partially edited text, Recurrence decodes the latent representation, generates an action of short, fixed-length, and applies the action to complete a single edit. For a comprehensive comparison, we introduce three types of text editing tasks: Arithmetic Operators Restoration , Arithmetic Equation Simplification , Arithmetic Equation Correction . Extensive experiments on these tasks with varying difficulties demonstrate that Recurrence achieves improvements over conventional inference methods.",291
" There is a broad consensus among grammar formalisms that the composition of form and meaning in natural language is a resource-sensitive process, with the words making up a phrase contributing exactly once to the resulting whole. The sentence ``the Mad Hatter offered'' is ill-formed because of a  of grammatical material, ``offer'' being a ditransitive verb; ``the Cheshire Cat grinned Alice a cup of tea'' on the other hand is ill-formed because of an  of material, which the intransitive verb ``grin'' cannot accommodate.  Given the resource-sensitive nature of language, it comes as no surprise that Linear Logic ,  in particular its intuitionistic version ILL, plays a central role in current logic-based grammar formalisms. Abstract Categorial Grammars and Lambda Grammars use ILL ``as-is'' to characterize an abstract level of grammatical structure from which surface form and semantic interpretation are obtained by means of compositional translations. Modern typelogical grammars in the tradition of the Lambek Calculus, e.g.~Multimodal TLG, Displacement Calculus, Hybrid TLG, refine the type language to account for syntactic aspects  of word order and constituency; ILL here is the target logic for semantic interpretation, reached by a homomorphism relating types and derivations of the syntactic calculus to their semantic counterparts.  A common feature of the aforementioned formalisms is their adoption of the parsing-as-deduction method: determining whether a phrase is syntactically well-formed is seen as the outcome of a process of logical deduction. This logical deduction automatically gives rise to a program for meaning composition, thanks to the remarkable correspondence between logical proof and computation known as the Curry-Howard isomorphism, a natural manifestation of the syntax-semantics interface. The Curry-Howard -terms associated with derivations are neutral with respect to the particular semantic theory one wants to adopt, accommodating both the truth-conditional view of formal semantics and the vector-based distributional view, among others.  Despite their formal appeal, grammars based on variants of linear logic have fallen out of favour within the NLP community, owing to a scarcity of large-scale datasets, but also due to difficulties in aligning them with the established high-performance neural toolkit. Seeking to bridge the gap between formal theory and applied practice, we focus on the  of linear logic, a lean graphical calculus that does away with the bureaucratic symbol-manipulation overhead characteristic of conventional prooftheoretic presentations . Integrating proof nets with recent advances in neural processing, we propose a novel approach to linear logic proof search that eliminates issues commonly associated with higher-order types and hypothetical reasoning, while greatly reducing the computational costs of structure manipulation, backtracking and iterative processing that burden standard parsing techniques .  Our proposed methodology relies on two key components. The first is an encoder/decoder-based supertagger that converts raw text sentences into linear logic judgements by dynamically constructing contextual type assignments, one primitive symbol at a time. The second is a bi-modal encoder that contextualizes the generated judgement in conjunction with the input sentence. The contextualized representations are fed into a Sinkhorn layer, tasked with finding the valid permutation that brings primitive symbol occurrences into alignment. The architecture induced is trained on labeled data, and assumes the role of a formally grounded yet highly accurate parser, which transforms raw text sentences into linear logic proofs and computational terms of the simply typed linear -calculus, further decorated with dependency annotations that allow reconstruction of the underlying dependency graph .  
"," Linear logic and the linear $\lambda$-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on {\AE}thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear $\lambda$-calculus with an accuracy of as high as $70\%$.",292
"  Autoregressive language models are functions that estimate a probability distribution over the next word in a sequence from past words, . This requires capturing statistical dependencies between words over short timescales, where syntactic information likely dominates , as well as long timescales, where semantic and narrative information likely dominate . Because this probability distribution grows exponentially with sequence length, some approaches simplify the problem by ignoring long-range dependencies. Classical -gram models, for example, assume word  is independent of all but the last  words, with typical  . Hidden Markov models  assume that the influence of previous words decays exponentially with distance from the current word .  In contrast, neural network language models such as recurrent  and transformer networks  include longer-range interactions, but simplify the problem by working in lower-dimensional representational spaces. Attention-based networks combine position and content-based information in a small number of attention heads to flexibly capture different types of dependencies within a sequence . Gated recurrent neural networks  compress information about past words into a fixed-length state vector . The influence each word has on this state vector tends to decay exponentially over time. However, each element of the state vector can have a different exponential time constant, or ``timescale'' , enabling gated RNNs like the long short-term memory  network to flexibly learn many different types of temporal relationships . Stacked LSTM networks reduce to a single layer , showing that network depth has an insignificant influence on how the LSTM captures temporal relationships. %Yet in both types of networks this flexibility comes at a cost, since the models must learn the shape of these dependencies from the data. %\ahcomment{rewrote this again substantially. think it's quite a bit better now?} Yet in all these networks the shape of the temporal dependencies must be learned directly from the data. This seems particularly problematic for very long-range dependencies, which are only sparsely informative .  This raises two related questions: what should the temporal dependencies in a language model look like? And how can that information be incorporated into a neural network language model?  To answer the first question, we look to empirical and theoretical work that has explored the dependency statistics of natural language.  quantified temporal dependencies in English and French language corpora by measuring the mutual information between tokens as a function of the distance between them. They observed that mutual information decays as a power law, i.e.\  for constant . This behavior is common to hierarchically structured natural languages  as well as sequences generated from probabilistic context-free grammars  . %While the precise shape of this dependency function may vary between languages and corpora, we shall take as a given that its mathematical form follows a power law.  Now to the second question: if temporal dependencies in natural language follow a power law, how can this information be incorporated into neural network language models? To our knowledge, little work has explored how to control the temporal dependencies learned in attention-based models. However, many approaches have been proposed for controlling gated RNNs, including updating different groups of units at different intervals , gating units across layers , and explicitly controlling the input and forget gates that determine how information is stored and removed from memory . Yet none of these proposals incorporate a specific shape of temporal dependencies based on the known statistics of natural language.  %\vvcomment{Rewrote that last sentence -- claiming that it's unclear how to relate them to this theoretical stuff seems a bit odd since we directly control the input/forget gates to relate them to the theory :D} \ahcomment{LOVE IT}%Yet it is unclear how to relate these largely practical modifications to theoretical properties of how these models capture temporal dependencies.  %\ahcomment{SHOULD WE SAY SOMETHING ABOUT TRANSFORMERS HERE? I'D LIKE TO, BUT I'M NOT EXACTLY SURE WHAT!} %\vvcomment{I'm tempted to claim at the top of the paragraph  that nobody's really thought through how to measure, let alone control, how transformers are encoding temporal dependencies between words. So they are not a good candidate for incorporating this information because we need a way to measure how a specific, controllable mechanism or group of mechanismin the model encode temporal dependencies between words. Then we can say that some thought has been put into this for RNNs...etc.}  %\ahcomment{good suggestion!}  In this work, we build on the framework of  to develop a theory for how the memory mechanism in LSTM language models can capture temporal dependencies that follow a power law. This relies on defining the  of an individual LSTM unit based on how the unit retains and forgets information. We show that this theory predicts the distribution of unit timescales for LSTM %We show that this theory predicts specific characteristics--the distribution of timescales across LSTM units--of  models trained on both natural English  and formal languages . Further, we show that forcing models to follow this theoretical distribution improves language modeling performance. These results highlight the importance of combining theoretical modeling with an understanding of how language models capture temporal dependencies over multiple scales. %dependencies over multiple timescales.   %Effective language models should capture the statistical properties of natural language, including information that varies at multiple timescales. For example, syntactic effects evolve at the timescale of words, whereas semantics, emotions, and narratives can evolve at much longer timescales of tens to hundreds or thousands of words. The importance of long timescale information is evident in results showing that neural networks have outperformed classical n-gram models on many language modeling benchmarks . This difference is attributed to these networks' ability to capture long timescale dependencies that that are impossible for n-gram models. Yet it is difficult to interpret how neural language models represent information at different timescales, and unclear how these timescale representations should be controlled to yield better or more interpretable models.  %One popular architecture for neural language models is recurrent neural networks, in particular Long Short-Term Memory  . Efforts to interpret the representations learned by LSTMs using probing tasks have shown that LSTM language models are capable of learning both short timescale information about word order ~, and long timescale semantic information~. Other methods have attempted to interpret the timescale of LSTM representations using predictive models of brain responses to natural language~. Yet the question of how and where information about different timescales is maintained in LSTM representations still does not have a satisfying answer.  %One alternative to interpreting representations in existing models is to construct language models in which different layers or groups of units are explicitly constrained to operate at different timescales. Several approaches have been proposed for building such explicitly multi-timescale models, including updating different groups of units at different intervals , gating units across layers , and including explicit control of the input and forget gates that determine how information is stored and removed from memory . These approaches ease interpretation by controlling the timescales represented by different units. Yet this raises a new concern: unlike standard LSTMs, explicitly multi-timescale models are unable to flexibly learn the statistics of natural language. This can decrease the performance of these models  and diminish their utility. Thus, when constructing explicitly multi-timescale language models it is important to consider which timescales are present in natural language.  % quantified the distribution of timescales in natural language by measuring the mutual information between tokens as a function of the distance between them. They observed that mutual information decays as a power law, which is common to many hierarchical structures . It would be desirable for a language model to retain temporal information that mimics these statistics. However, it is not clear how to attain power law using LSTMs, which are fundamentally designed to decay information exponentially across time .  %In this work, we present a method to control the timescales of information represented by each unit of an LSTM language model, resulting in interpretable multi-timescale representations. Building on the theoretical grounding of , we quantify the timescale represented in each unit using forget gate activations. We use this framework to analyze an existing LSTM language model  and show how different layers of the model retain information across time. Next, we use this framework to construct explicitly multi-timescale language models where the timescale of each LSTM unit is controlled by setting the forget and input gate biases. To determine the distribution of timescales within this model we used a prior that mimics the power law statistical properties of natural language  through a combination of exponential timescales. Finally, we show that this prior creates interpretable representations in which long and short timescale information is selectively routed into different parts of the network.   
","  Language models must capture statistical dependencies between words at timescales ranging from very short to very long. %, but how much information is needed for each timescale? Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. %However, it is unclear how power law decay of information should manifest in neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory  language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency  words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models.   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % EMNLP version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %Although neural language models are effective at capturing statistics of natural language, their representations are challenging to interpret. In particular, it is unclear how these models retain information over multiple timescales.  %In this work, we construct explicitly multi-timescale language models by manipulating the input and forget gate biases in a long short-term memory  network. %%In this work, we quantify and control the timescale of each unit in a LSTM language model via the the input and forget gate biases.  %The distribution of timescales is selected to approximate power law statistics of natural language through a combination of exponentially decaying memory cells. %%We then design a prior based on statistical properties of natural language and construct a multi-timescale LSTM language model.  %We then empirically analyze the timescale of information routed through each part of the model using word ablation experiments and forget gate visualizations. %%Next, we propose word ablation experiments and forget gate visualizations to interpret the timescale of information routing through the different parts of a model.  %These experiments show that the multi-timescale model successfully learns representations at the desired timescales, and that the distribution includes longer timescales than a standard LSTM.  %%Moreover, it outperforms the standard model on the language modeling task on the Penn Treebank and WikiText-2 datasets, especially on rare words. \ahcomment{change last sentence to point about interpretability} %Further, information about high-, mid-, and low-frequency words is routed preferentially through units with the appropriate timescales. %Thus we show how to construct language models with interpretable representations of different information timescales.  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %Shivangi's version %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Language models should ideally capture the statistical properties of natural language varying over multiple timescales. However, representations within language models  are challenging to interpret. Hence, it is unclear how different layers of an LSTM LM retain information over different timescales.  % % In this paper, we propose a mechanism to interpret and control the timescale of information routing through an LSTM unit. We observed that a standard LSTM LM favors representations of short timescale information . We then introduce a prior based on the statistical properties of language to control the distribution of the timescales across LSTM units to achieve an effective multi-timescale language model. In addition to this, we present a word ablation experiment and forget gate visualization to interpret the timescale of information routing through the different parts of the model. % % The proposed model learns representations of both short as well as long-timescale information. It also achieves better prediction performance than a standard LSTM LM on Penn Treebank and WikiText-2 datasets, especially on rare words.",293
"          The advancement in the field of Computer Vision ~ and Natural Language Processing ~ over the last decade, has introduced several interesting machine learning techniques. %problems more convenient.          The problems such as object detection~, segmentation~, and  image classification~ in CV, and machine translation~, question answering~, biomedical and clinical text mining~ , speech recognition~ in NLP, are being solved much more efficiently than ever before. This has facilitated the researchers to indulge into solving interdisciplinary problems that demand knowledge of both the fields.                   Visual Question Answering ~ has emerged as one such problem. In VQA, the task is poised as questions being asked with respect to an image, where the machine needs to learn and generate answers of such questions based on the learned features of the input image. In contrast to the typical CV tasks which largely focus on %have singular and          solving problems such as %restricted problems ~ and Inception-Resnet-v2~, respectively. We fuse the representations together and pass it to the specific answer prediction model at the leaf node. For the task of question classification in the root node, we propose a question segregation technique. We use Support Vector Machine ~ as the classifier with hand-engineered and word frequency-based features for QS. We use the machine learning technique for QS, as the rule-based strategy suffers from the problem of defining too many rules that may not extend to other datasets~. The following examples from RAdiology Data ~ show the difficulty of the rule-based approach in the medical domain.                                         Evidence of hemorrhage in the kidneys? \\ { `Yes/No'              Is the spleen present?  \\ { `Others'                                    Careful analysis of the question reveals that the first example expects a descriptive type answer that is to list out the facts that indicate kidney hemorrhage , while the second example expects to confirm the presence/absence of spleen . The presence of such anomalies in the question acts as a hindrance in the formation of robust rules for the classification of questions into their correct type.                  We perform all our experiments in the RAD and  ImageCLEF2018 VQA-Med 2018  datasets, as they perfectly capture the problem statement that we intend to solve. Detailed discussion on the dataset can be found in Section. Experimental evaluation demonstrates promising results, showing the effectiveness of our proposed approach. %'s efficiency.          Additionally, error analysis of the system's outputs %error analysis          shows the future direction in this research area by addressing different kinds of errors.          The organization of this paper is as follows. We first discuss the related work in VQA. Then we present the details of the methodologies that we implemented to solve our specific problem. In particular, we explain our proposed HQS-VQA models in detail. Basically, we discussed the technique used for the question segregation module and the VQA components used to generate the query-specific answers. Details of experiments along with the evaluation results and necessary analysis are reported. %We then perform the experiments and show the results with qualitative and quantitative analysis.          Finally, we conclude and provide the future directions of our work.                 The motivation behind our work are stemmed from the following facts: %of the medical visual question answering are listed as follows:            "", ``{x-ray}"", etc.) and reports for the patients are easily accessible with the increase in the use of medical portals. But the patient still needs to visit a medical expert to fully understand those reports and get answers to their queries. This process is both time-consuming and cost-sensitive. %involves investment of time and money, even to get simple queries answered.          On the other hand, clinicians also find an efficient VQA system very useful to understand the results of a complex medical image. They may use such a system as a second opinion just to boost their self-confidence in the understanding of some specific aspects of such medical images. Although it is possible to search queries on search engines, the search results may be inaccurate, spurious, vague, and, or enormous. In the case of medical reports such inaccurate, spurious, or vague results could lead to serious after-effects. In this context, the VQA in the medical domain is getting attention as an important research problem trying to provide the answer to end-user queries related to medical images.   ' or `No' as the answer. Whereas, the queries from clinicians and medical experts are expected to be more problem-specific, which requires elaborate answers. Again, a skilled trainee is expected to ask more specific and sophisticated questions, while queries from beginners are likely to be simple and straightforward. For example, a naive trainee may inquire about the presence of any abnormality in the image, whereas a senior trainee may identify the abnormality of `intraventricular hemorrhage' from the image, and want to understand more about the grade and effect of the hemorrhage. They can then draw inferences from the acquired data for effective treatment. [!ht]             {-0.9in}{-1.2in}               m{0.45\linewidth}  m{0.2\linewidth}}                 { & {{*}{} & Is this a cyst in the left lung? &  No \\                  & Has the left lung collapsed? & Yes \\                  & Where is the nodule? & Below the 7th rib in the right lung \\                  & What are the densities in both mid-lung fields? & pleural plaques                 \\ \hline                                                                                                                    
","        {        Visual Question Answering in Medical domain  plays an important role in providing medical assistance to the end-users. These users are expected to raise either a straightforward question with a Yes/No answer or a challenging question that requires a detailed and descriptive answer. The existing techniques in VQA-Med fail to distinguish between the different question types sometimes complicates the simpler problems, or over-simplifies the complicated ones. It is certainly true that for different question types, several distinct systems can lead to confusion and discomfort for the end-users. To address this issue, we propose a hierarchical deep multi-modal network that analyzes and classifies end-user questions/queries and then incorporates a query-specific approach for answer prediction. We refer our proposed approach as Hierarchical Question Segregation based Visual Question Answering, in short HQS-VQA.      %   We first use the Support Vector Machine  with the hand-engineered features to classify the questions into yes/no and descriptive types.    % Then, based on the question types, we employ different strategies to provide the answer. The Yes/No type questions are treated as a binary classification problem. We generate the answer from a fixed vocabulary for the descriptive type question.     Our contributions are three-fold, viz. firstly, we propose a question segregation  technique for VQA-Med; secondly, we integrate the QS model to the hierarchical deep multi-modal neural network to generate proper answers to the queries related to medical images; and thirdly, we study the impact of QS in Medical-VQA by comparing the performance of the proposed model with QS and a model without QS. We evaluate the performance of our proposed model on two benchmark datasets, viz. RAD and CLEF18. Experimental results show that our proposed HQS-VQA technique outperforms the baseline models with significant margins. We also conduct a detailed quantitative and qualitative analysis of the obtained results and discover potential causes of errors and their solutions.         }",294
"  The following instructions are directed to authors of papers submitted to EMNLP 2020 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format  version of their papers. The proceedings are designed for printing on A4 paper.   
"," This document contains the instructions for preparing a manuscript for the proceedings of EMNLP 2020. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.",295
"  The rapid development of science and technology in the world has created a vast amount of data. In particular, the growth of social networks continuously creates a huge amount of comments and posts which are valuable sources to exploit and analyze in the digital era. Text classification is a prerequisite for such works as analyzing user opinion in the network environment, filtering and removing malicious information, and detecting criminal risk. With great potential, text classification has attracted much attention from experts in the natural language processing community worldwide. In English, we easily search for a range of text classification publications in many fields. However, relatively few researches have been done on Vietnamese text. Most published articles focus on binary classification. However, a large amount of information today requires analysis in many more aspects . The lack of knowledge and techniques for the Vietnamese language makes us decide to conduct this research to classify multi-class text for Vietnamese social media datasets. These datasets are provided from the VLSP share-task and publications on text classification. In particular, there are various social media textual datasets such as UIT-VSMEC for emotion recognition  and UIT-VSFC for students' feedback classification  and HSD-VLSP for hate speech detection . These are the datasets with multi-label and imbalance between the labels that have been published recently. They are suitable for the requirements that we would like to study.  The emergence of deep neural networks  and word embeddings have made text classification more efficient. Pre-trained word embeddings accurately capture semantics to assist deep learning models improve the efficiency of classification. In this study, we implement deep learning models such as CNN , LSTM  and their variants to solve classification problems. Besides, we implement the BERT model , which is a state-of-the-art model in many natural language processing tasks in recent years. BERT is trained through the transformer閳ユ獨 two-dimensional context . BERT is in contrast to previous deep learning models that looked at a text sequence from left to right or combined left-to-right and right-to-left training. To improve the word representation, we create a normalized words dictionary, which helps recognize words included in pre-trained embedding but is not represented due to misspellings.  As a result, CNN model combined with fastText's pre-trained embedding , has been remarkably performance on Vietnamese social media datasets. Our study also proves the efficiency of BERT on Vietnamese students' feedback dataset. Besides, we combine single models to increase the efficiency of the classification. As a result, our ensemble model accomplishes higher results than the single model. Compared to previous studies done on the datasets, our models achieve better results.   
"," Text classification is a popular topic of natural language processing, which has currently attracted numerous research efforts worldwide. The significant increase of data in social media requires the vast attention of researchers to analyze such data. There are various studies in this field in many languages but limited to the Vietnamese language. Therefore, this study aims to classify Vietnamese texts on social media from three different Vietnamese benchmark datasets. Advanced deep learning models are used and optimized in this study, including CNN, LSTM, and their variants. We also implement the BERT, which has never been applied to the datasets. Our experiments find a suitable model for classification tasks on each specific dataset. To take advantage of single models, we propose an ensemble model, combining the highest-performance models. Our single models reach positive results on each dataset. Moreover, our ensemble model achieves the best performance on all three datasets. We reach 86.96\% of F1-score for the HSD-VLSP dataset, 65.79\% of F1-score for the UIT-VSMEC dataset, 92.79\% and 89.70\% for sentiments and topics on the UIT-VSFC dataset, respectively. Therefore, our models achieve better performances as compared to previous studies on these datasets.",296
"  The Transformer model has achieved the state-of-the-art performance on various natural language preprocessing  tasks, originally in neural machine translation , and recently in massive multilingual machine translation , crosslingual pretraining , and many other tasks. There has been a growing interest in increasing the model capacity of Transformers, which demonstrates improved performance on various sequence modeling and generation tasks .   %However, there are still two challengee  Training Transformers with increased or variable depth is still an open problem. Depending on the position of layer norm sub-layer, backpropagating gradients through multiple layers may suffer from gradient vanishing . In addition, performance does not always improve by simply stacking up layers . When used for multilingual or multi-task pretraining, such as multilingual machine translation, crosslingual language modeling, etc., the simplicity of using one shared Transformer network for all languages  is appealing. However, how to share model capacity among languages  so as to facilitate positive transfer while mitigating negative transfer has not been well explored.   %how to determine which part of the model to share     In this work, we present a novel approach to train deep Transformers, in which the layers to be used  and the effective depth are not static, but learnt based on the underlying task. Concretely, we model the decision to use each layer as a latent variable, whose distribution is jointly learnt with the rest of the Transformer parameters. %inference network  which acts as layer weighting during training, and  %\asa{I would say: 'At training time we approximate the discrete choice with a Gumbel-Softmax  distribution. The `soft weights' sampled from this distribution also act as gradient normalization for each layer, and this allows us to train very deep Transformers  without using regular layer normalization layers. At inference time, the learnt discrete choice can be used to directly derive a compact model by pruning layers with low probability, but we have the choice of leaving the learned layer selection probabilities as soft weights.} At training time we approximate the discrete choice with a Gumbel-Softmax  distribution. The `soft weights' sampled from this distribution also act as gradient normalization for each layer, and this allows us to train very deep Transformers  without using regular layer normalization layers. At inference time, the learnt discrete choice can be used to directly derive a compact model by pruning layers with low probability, but we have the choice of leaving the learned layer selection probabilities as soft weights. %At inference time, the learnt discrete choice can be used to directly derive a compact model by pruning layers with low probability. Such dynamic ``soft weighting"" also acts as gradient normalization for each layer, and the proposed approach can be used to train very deep Transformer  without using regular layer normalization layers.  By evaluating on WMT'16 English-German machine translation  and masked language modeling  tasks , we show that we can successfully train deeper Transformer  and outperform existing approaches in terms of quality and training stability. %is learnt jointly with the rest of Transfor weighs each layer during training  contribution of  to learn the contribution from each layer by  %automatically learn which Transformer layer to use for each language in multilingual machine translation task. Our approach allows training deep Transformer without vanishing or exploding gradient. As a result, it enables using increased model capacity during training, and at inference time pruning to a more compact model using the layer selection policies. %Our approach learns layer selection optimized for improving translation quality for each language in a single Transformer network. At inference time, the learnt sub-network  %Our contributions are as follows:   We show this approach can be extended to learn task-specific sub-networks by learning different layer selection probabilities for each language pair in multilingual machine translation. %learning multiple ``views"" of layer selection in a shared Transformer.  This result contributes to the growing interest of learning efficient architectures for multi-task and transfer learning in natural language understanding and generation . %\asa{selfish plug to cite BERT and PALs here?}  %Particularly, deep models are trained on the WMT'16 English-German machine translation task, and the crosslingual masked language modeling task , with 64-layer and 96-layer encoders/decoders, respectively.      The main contributions of this paper are as follows. We present a probabilistic framework to learn which layers to select in the Transformer architecture. Based on this framework, we propose a novel method to train one shared Transformer network for multilingual machine translation with different layer selection probabilities for each language pair. The proposed method alleviates the vanishing gradient issue and enables stable training of deep Transformers. We conduct experiments on several tasks to evaluate the proposed approach: WMT'16 English-German machine translation, masked language modeling, and multilingual many-to-one as well as one-to-many machine translation with diverse languages.                 % First, we present a principled approach to utilize layers in the Transformer architecture. Our method learns both  weighting and  selection of Transformer layers. \xian{Is this sentence clear?} %Our method learns latent layers which can be used by both  weighting and  selection in one training without additional distillation steps \asa{This isn't that clear to me?}.  % Second, the proposed method improves vanishing gradient issue and thus enables stable training of deep Transformers. We evaluate deep models with latent layers for the WMT'16 English-German machine translation task  task, and a 96-layer model for crosslingual masked language modeling .  % Third, we show a variant of this approach to learn language-aware layer selections and automatically determine which layers to share in a multilingual Transformer.  % We evaluated the effectiveness of this approach in  machine translation and  masked language modeling with multiple languages. Our work contributes to the growing interest of learning efficient architectures for multi-task and transfer learning in natural language understanding and generation,                                                   %Our results contributes to the, opens the potentials of    %The rest of the paper is organized as follows. We present relevant background in Sections  and present our approach in Section . We present experiments and evaluation results in Section  and an analysis of the proposed approach in Section .    
"," %We propose two approaches to improve Transformer for multilingual tasks such as multilingual machine translation, XLM-R, etc. We propose a new method to adaptively learn which layers to share in a multilingual Transformer. Our approach increased the depth of the decoder with stable training. Besides achieving superior quality, the learnt layer selection leads to a compact architecture with reduced inference cost.   %We present a principled approach to utilize layers in Transformers based on task distribution. Our method uses variational inference to learn optimal weighting of each layer. This approach enables training very deep Transformers without vanishing gradients. At inference time, the learnt layer selection posterior can be used for pruning to derive a compact model with reduced depth. We demonstrate the quality improvement from this method in multilingual sequence modeling, specifically multilingual machine translation and crosslingual masked language modeling , compared to strong baselines such as existing layer drop and wide models with similar number of parameters. Further analysis reveals such data-driven layer selection learns both specialization layers as well as common layers shared across languages.       The Transformer model has achieved state-of-the-art performance in many sequence modeling tasks. However, how to leverage model capacity with large or variable depths is still an open challenge. We present a probabilistic framework to automatically learn which layer to use by learning the posterior distributions of layer selection. As an extension of this framework, we propose a novel method to train one shared Transformer network for multilingual machine translation with different layer selection posteriors for each language pair. The proposed method alleviates the vanishing gradient issue and enables stable training of deep Transformers . We evaluate on WMT English-German machine translation and masked language modeling tasks, where our method outperforms existing approaches for training deeper Transformers. Experiments on multilingual machine translation demonstrate that this approach can effectively leverage increased model capacity and bring universal improvement for both many-to-one and one-to-many translation with diverse language pairs.   % In particular, we show that this approach can learn adaptive sub-networks by learning multiple ``views"" of layer selections in one shared Transformer. Our approach enables training deep Transformers  without vanishing gradients.  %training with latent layers.   %For multilingual machine translation we can learn a sub-network for each language pair by using different 'layer selection' probabilities per-language.      %\asa{bring universal improvement -> improves performance?}  %\xian{Here I want to express we can improve both average performance as well as performance for individual languages pairs; commenting discussion for now since I want to have a sharable version while we continue iterating}  % to leverage increased model capacity, which also can be pruned to its ``effective depth"" to derive a compact model.  .     %We present a principled approach to improve the training of Transformers by learning to both ``soft weight"" and ``hard select"" each layer via variational inference.     %This approach allows us to train very deep Transformers without layer normalization. At inference time, the learnt layer selection posterior can be used for pruning to derive a compact model with reduced depth. We evaluate the proposed approach on machine translation and masked languaage modeling tasks, where we can train deeper Transformer up to  100 layers with improved quality. On WMT En-De, and multilingual translation.   %We demonstrate the quality improvement from this method in several sequence modeling tasks such as machine translation , specifically multilingual machine translation and crosslingual masked language modeling , improving on strong baselines such as layer drop as well as wide models with a similar number of parameters. Analyzing the data-driven layer selection, we find bottom layers are shared across all languages, while top layers specialize to a particular language.",297
" In recent years, Transformers  have defined state-of-the-art performance on a variety of NLP tasks, including machine translation  and language modeling. While large Transformer models can learn uniquely rich representations, they are also highly overparameterized . Several studies have therefore attempted to prune Transformers during or after training while retaining as much performance as possible . Some methods have been fairly successful, achieving compression ratios up to 10 depending on the downstream task.  Looking beyond task performance, however, it remains unclear how widely-used pruning methods affect a model's learned representations. For example, a pruned Transformer may translate text at the same BLEU, but does pruning affect the model in ways unaccounted for by this metric?  Motivated by this question, we apply recent analysis techniques to study the representations of increasingly sparse Transformers trained on MT. We perform magnitude pruning in an iterative, lottery-ticket fashion to identify Transformers at competitive sparsities with no drop in task performance . We examine the internal structures of our models as sparsity increases, specifically addressing the following questions:     Using iterative magnitude pruning , we train an En-De Transformer that retains 99.4\% of BLEU at 66.4\% sparsity. During IMP, we obtain eight Transformer models at varying levels of sparsity, along with the original unpruned model. We probe these models' representations for learned linguistic knowledge on eighteen auxiliary syntactic and semantic tasks . We then perform an unsupervised comparison of the representations and attention distributions between dense and sparse models, adopting metrics posed in . Our key conclusions are as follows:        
"," Recent work on the lottery ticket hypothesis has produced highly sparse Transformers for NMT while maintaining BLEU. However, it is unclear how such pruning techniques affect a model's learned representations. By probing Transformers with more and more low-magnitude weights pruned away, we find that complex semantic information is first to be degraded. Analysis of internal activations reveals that higher layers diverge most over the course of pruning, gradually becoming less complex than their dense counterparts. Meanwhile, early layers of sparse models begin to perform more encoding. Attention mechanisms remain remarkably consistent as sparsity increases.",298
"  In rule-based machine translation , a linguist formalises linguistic knowledge into lexicons and grammar rules, which is used by the system to analyse sentences in the source language and translate them. While this approach does not require any parallel corpora for training and grants control over the translations created by the system, the process of encoding linguistic knowledge requires a great amount of expert time. Notable examples of RBMT systems are the original, rule-based Systran , Lucy LT  and the Apertium platform .  Instead, corpus-based machine translation  systems learn to translate from examples, usually in the form of sentence-level aligned corpora. On the one hand, this approach is generally computationally more expensive and offers limited control over the generated translations. Furthermore, it is not feasible for language pairs that have limited to no available parallel resources. On the other hand, if parallel resources are available, it boasts a much higher coverage of the targeted language pair. Examples of corpus-based MT paradigms are phrase-based statistical machine translation   and neural machine translation  .  In this work, we focused on leveraging RBMT knowledge for improving the performance of NMT systems in an under-resourced scenario. Namely, we used the information provided by Lucy LT, an RBMT system where the linguistic knowledge is formalised by human linguists as computational grammars, monolingual and bilingual lexicons. Grammars are collections of transformations to annotated trees. Monolingual lexicons are collections of lexical entries, where each lexical entry is a set of feature-value pairs containing morphological, syntactic and semantic information. Bilingual lexicon entries include source-target lexical correspondences and, optionally, contextual conditions and actions. The Lucy LT system divides the translation process into three sequential phases: analysis, transfer, and generation. During the analysis phase, the source sentence is morphologically analysed using a lexicon that identifies each surface form and all its plausible morphological readings. Next, the Lucy LT chart parser together with an analysis grammar consisting of augmented syntactic rules extracts the underlying syntax tree structure and annotates it. The transfer and generation grammars are then applied in succession on that tree, which undergoes multiple annotations and transformations that add information about the equivalences in the target language and adapt the source language structures to the appropriate ones in the target language. Finally, the terminal nodes of the generation tree are assembled into the translated sentence. We focused on the analysis phase, with a special interest for two of the features used: the morphological category  and the inflexion class  or classes of the lexical entries.   %%% NE/TERM Additionally, we focused on two language phenomena that are easily addressable when using RBMT but present a challenge when using corpus-based MT: named entities and terminological expressions.  A named entity  is a word or a sequence of words that unequivocally refer to a real-world object, such as proper nouns, toponyms, numbers or dates. In the context of MT, NEs present different challenges. For example, if an English sentence starts with the word Smith, we do not know a priori if we are dealing with the name of a profession, that will have to be translated, or a proper noun that may have to be left untranslated, or maybe transliterated to a different script. A second issue may arise when using subword units: while word-level models may accidentally preserve an out-of-vocabulary NE, the subword level model will generate a  translation for it. NEs are one of the main out-of-vocabulary word classes, which often cause translation problems that seriously affect the meaning of the sentence .  Similarly, a terminological expression can consist of a single word or a sequence of words that may have a different meaning depending on the context or domain they appear. Hence, the translation for the term might be different depending on the context or domain. Moreover, different contexts and domains may impose additional restrictions on the language used, such as different modes or the use of active or passive voice, and the presence of particular terminology may suggest that a translation is not acceptable even if the meaning of the source sentence is preserved. Accurate terminology translation is crucial to produce adequate translations .  In this work we extend and further analyse the injection of morphological information technique that we proposed in a previous word  and we propose an approach to NEs and terminology that does not rely on any particular technology and can be applied to any MT approach using any kind of resource to detect and translate the NEs and terminological expressions.  To test our proposed approach, we focused on English-Spanish , English-Basque, English-Irish and English-Simplified Chinese language pairs in an under-resourced scenario, using corpora with around one million parallel entries per language pair and domain. Additional test sets that contain several examples of terms, NEs and rich morphology have also been selected and used to further explore the performance of the proposed approaches. Results suggest that, while obtaining results that are not statistically significantly different than the baseline in several scenarios, the proposed approaches show appropriate behaviours such as keeping the passive voice characteristic of some domains.   %Results suggested that adding morphological information to the source language is as effective as using subword units in this particular setting.   
"," Rule-based machine translation is a machine translation paradigm where linguistic knowledge is encoded by an expert in the form of rules that translate text from source to target language. While this approach grants extensive control over the output of the system, the cost of formalising the needed linguistic knowledge is much higher than training a corpus-based system, where a machine learning approach is used to automatically learn to translate from examples. In this paper, we describe different approaches to leverage the information contained in rule-based machine translation systems to improve a corpus-based one, namely, a neural machine translation model, with a focus on a low-resource scenario. Three different kinds of information were used: morphological information, named entities and terminology. In addition to evaluating the general performance of the system, we systematically analysed the performance of the proposed approaches when dealing with the targeted phenomena. Our results suggest that the proposed models have limited ability to learn from external information, and most approaches do not significantly alter the results of the automatic evaluation, but our preliminary qualitative evaluation shows that in certain cases the hypothesis generated by our system exhibit favourable behaviour such as keeping the use of passive voice. %Our results suggest that adding morphological information to the source language is as effective as using subword units in this particular setting.",299
